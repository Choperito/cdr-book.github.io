<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Capítulo 26 Clasificador \(k\)-vecinos más próximos | Fundamentos de ciencia de datos con R</title>
<meta name="author" content="Gema Fernández-Avilés y José-María Montero">
<meta name="description" content="Ramón A. Carrasco\(^{a}\), Itzcóatl Bueno\(^{a,b}\) y José-María Montero\(^{c}\) \(^{a}\)Universidad Complutense de Madrid\(^{b}\)Instituto Nacional de Estadística\(^{c}\)Universidad de...">
<meta name="generator" content="bookdown 0.37 with bs4_book()">
<meta property="og:title" content="Capítulo 26 Clasificador \(k\)-vecinos más próximos | Fundamentos de ciencia de datos con R">
<meta property="og:type" content="book">
<meta property="og:image" content="/img/cover.png">
<meta property="og:description" content="Ramón A. Carrasco\(^{a}\), Itzcóatl Bueno\(^{a,b}\) y José-María Montero\(^{c}\) \(^{a}\)Universidad Complutense de Madrid\(^{b}\)Instituto Nacional de Estadística\(^{c}\)Universidad de...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Capítulo 26 Clasificador \(k\)-vecinos más próximos | Fundamentos de ciencia de datos con R">
<meta name="twitter:description" content="Ramón A. Carrasco\(^{a}\), Itzcóatl Bueno\(^{a,b}\) y José-María Montero\(^{c}\) \(^{a}\)Universidad Complutense de Madrid\(^{b}\)Instituto Nacional de Estadística\(^{c}\)Universidad de...">
<meta name="twitter:image" content="/img/cover.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><link href="libs/tabwid-1.1.3/tabwid.css" rel="stylesheet">
<script src="libs/tabwid-1.1.3/tabwid.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css">
<link rel="stylesheet" href="bs4_book.css">
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Fundamentos de ciencia de datos con <strong>R</strong></a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Buscar" aria-label="Buscar">
</form>

      <nav aria-label="Contenido"><h2>Contenido</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Prefacio</a></li>
<li><a class="" href="pr%C3%B3logo-by-julia-silge.html">Prólogo (by Julia Silge)</a></li>
<li><a class="" href="pr%C3%B3logo-por-yanina-bellini.html">Prólogo (por Yanina Bellini)</a></li>
<li class="book-part">Ciencia, datos, software… y científicos</li>
<li><a class="" href="ciencia-datos.html"><span class="header-section-number">1</span> ¿Es la ciencia de datos una ciencia?</a></li>
<li><a class="" href="metodologia.html"><span class="header-section-number">2</span> Metodología en ciencia de datos</a></li>
<li><a class="" href="ch-110003.html"><span class="header-section-number">3</span> R para ciencia de datos</a></li>
<li><a class="" href="cap-etica.html"><span class="header-section-number">4</span> Ética en la ciencia de datos</a></li>
<li class="book-part">Bienvenidos a la jungla de datos</li>
<li><a class="" href="datos-sql.html"><span class="header-section-number">5</span> Gestión de bases de datos relacionales</a></li>
<li><a class="" href="cap-nosql.html"><span class="header-section-number">6</span> Gestión de bases de datos NoSQL</a></li>
<li><a class="" href="DGDQM.html"><span class="header-section-number">7</span> Gobierno, gestión y calidad del dato</a></li>
<li><a class="" href="cap-130009.html"><span class="header-section-number">8</span> Integración y limpieza de datos</a></li>
<li><a class="" href="chap-feature.html"><span class="header-section-number">9</span> Selección y transformación de variables</a></li>
<li><a class="" href="chap-herramientas.html"><span class="header-section-number">10</span> Herramientas para el análisis en ciencia de datos</a></li>
<li><a class="" href="cap-120006-aed.html"><span class="header-section-number">11</span> Análisis exploratorio de datos</a></li>
<li class="book-part">Fundamentos de estadística</li>
<li><a class="" href="Funda-probab.html"><span class="header-section-number">12</span> Probabilidad</a></li>
<li><a class="" href="Fundainfer.html"><span class="header-section-number">13</span> Inferencia estadística</a></li>
<li><a class="" href="muestreo.html"><span class="header-section-number">14</span> Muestreo y remuestreo</a></li>
<li class="book-part">Modelización estadística</li>
<li><a class="" href="cap-lm.html"><span class="header-section-number">15</span> Modelización lineal</a></li>
<li><a class="" href="cap-glm.html"><span class="header-section-number">16</span> Modelos lineales generalizados</a></li>
<li><a class="" href="cap-gam.html"><span class="header-section-number">17</span> Modelos aditivos generalizados</a></li>
<li><a class="" href="cap-mxm.html"><span class="header-section-number">18</span> Modelos mixtos</a></li>
<li><a class="" href="cap-sparse.html"><span class="header-section-number">19</span> Modelos \(\textit{sparse}\) y métodos penalizados de regresión</a></li>
<li><a class="" href="cap-series-temp.html"><span class="header-section-number">20</span> Modelización de series temporales</a></li>
<li><a class="" href="cap-discriminante.html"><span class="header-section-number">21</span> Análisis discriminante</a></li>
<li><a class="" href="cap-conjunto.html"><span class="header-section-number">22</span> Análisis conjunto</a></li>
<li><a class="" href="tablas-contingencia.html"><span class="header-section-number">23</span> Análisis de tablas de contingencia</a></li>
<li class="book-part">Machine learning supervisado</li>
<li><a class="" href="cap-arboles.html"><span class="header-section-number">24</span> Árboles de clasificación y regresión</a></li>
<li><a class="" href="cap-svm.html"><span class="header-section-number">25</span> Máquinas de vector soporte</a></li>
<li><a class="active" href="cap-knn.html"><span class="header-section-number">26</span> Clasificador \(k\)-vecinos más próximos</a></li>
<li><a class="" href="cap-naive-bayes.html"><span class="header-section-number">27</span> Naive Bayes</a></li>
<li><a class="" href="cap-bagg-rf.html"><span class="header-section-number">28</span> Métodos ensamblados: \(\bf \textit {bagging}\) y \(\bf \textit{random}\) \(\bf \textit{forest}\)</a></li>
<li><a class="" href="cap-boosting-xgboost.html"><span class="header-section-number">29</span> \(\bf \textit{Boosting}\) y el algoritmo XGBoost</a></li>
<li class="book-part">Machine learning no supervisado</li>
<li><a class="" href="cap-cluster.html"><span class="header-section-number">30</span> Análisis clúster: clusterización jerárquica</a></li>
<li><a class="" href="no-jerarquico.html"><span class="header-section-number">31</span> Análisis clúster: clusterización no jerárquica</a></li>
<li><a class="" href="acp.html"><span class="header-section-number">32</span> Análisis de componentes principales</a></li>
<li><a class="" href="af.html"><span class="header-section-number">33</span> Análisis factorial</a></li>
<li><a class="" href="mds.html"><span class="header-section-number">34</span> Escalamiento multidimensional</a></li>
<li><a class="" href="correspondencias.html"><span class="header-section-number">35</span> Análisis de correspondencias</a></li>
<li class="book-part">Deep learning</li>
<li><a class="" href="capNN.html"><span class="header-section-number">36</span> Redes neuronales artificiales</a></li>
<li><a class="" href="cap-redes-convol.html"><span class="header-section-number">37</span> Redes neuronales convolucionales</a></li>
<li class="book-part">Ciencia de datos de texto y redes</li>
<li><a class="" href="mineria-textos.html"><span class="header-section-number">38</span> Minería de textos</a></li>
<li><a class="" href="grafos.html"><span class="header-section-number">39</span> Análisis de grafos y redes sociales</a></li>
<li class="book-part">Ciencia de datos espaciales</li>
<li><a class="" href="datos-espaciales.html"><span class="header-section-number">40</span> Trabajando con datos espaciales</a></li>
<li><a class="" href="geo.html"><span class="header-section-number">41</span> Geoestadística</a></li>
<li><a class="" href="cap-econom-esp.html"><span class="header-section-number">42</span> Modelos econométricos espaciales</a></li>
<li><a class="" href="cap-pp.html"><span class="header-section-number">43</span> Procesos de puntos</a></li>
<li class="book-part">Comunica y colabora</li>
<li><a class="" href="cap-120007-informes.html"><span class="header-section-number">44</span> Informes reproducibles con R Markdown y Quarto</a></li>
<li><a class="" href="shiny.html"><span class="header-section-number">45</span> Creación de aplicaciones web interactivas con Shiny</a></li>
<li><a class="" href="github.html"><span class="header-section-number">46</span> Git y GitHub R</a></li>
<li><a class="" href="geoproces.html"><span class="header-section-number">47</span> Geoprocesamiento en nube</a></li>
<li class="book-part">Casos de estudio en ciencia de datos</li>
<li><a class="" href="cap-crimen.html"><span class="header-section-number">48</span> Análisis de una red criminal</a></li>
<li><a class="" href="cap-publicidad.html"><span class="header-section-number">49</span> Optimización de inversiones publicitarias</a></li>
<li><a class="" href="cap-twitter.html"><span class="header-section-number">50</span> ¿Cómo tuitea Elon Musk?</a></li>
<li><a class="" href="cap-periodismo.html"><span class="header-section-number">51</span> Análisis electoral: de RStudio a su periódico favorito</a></li>
<li><a class="" href="paro-clm.html"><span class="header-section-number">52</span> El impacto de las crisis financiera y de la COVID-19 en el paro de CLM</a></li>
<li><a class="" href="cap-rfm.html"><span class="header-section-number">53</span> Segmentación de clientes en el comercio minorista</a></li>
<li><a class="" href="cap-medicina.html"><span class="header-section-number">54</span> Análisis de datos en medicina</a></li>
<li><a class="" href="cap-futbol.html"><span class="header-section-number">55</span> Messi y Ronaldo: dos ídolos desde la perspectiva de los datos</a></li>
<li><a class="" href="cambioclimatico.html"><span class="header-section-number">56</span> Una nota sobre el cambio climático</a></li>
<li><a class="" href="cap-sist-exp.html"><span class="header-section-number">57</span> Implementación de un sistema experto en el ámbito pediátrico</a></li>
<li><a class="" href="cap-ree.html"><span class="header-section-number">58</span> Predicción de consumo eléctrico con redes neuronales artificiales</a></li>
<li><a class="" href="nlp-textil.html"><span class="header-section-number">59</span> El procesamiento del lenguaje natural para tendencias de moda en textil</a></li>
<li><a class="" href="cap-fraude.html"><span class="header-section-number">60</span> Detección de fraude de tarjetas de crédito</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="info-session.html"><span class="header-section-number">A</span> Información de la sesión</a></li>
<li><a class="" href="referencias.html">Referencias</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="cap-knn" class="section level1" number="26">
<h1>
<span class="header-section-number">Capítulo 26</span> Clasificador <span class="math inline">\(k\)</span>-vecinos más próximos<a class="anchor" aria-label="anchor" href="#cap-knn"><i class="fas fa-link"></i></a>
</h1>
<p><em>Ramón A. Carrasco</em><span class="math inline">\(^{a}\)</span>, <em>Itzcóatl Bueno</em><span class="math inline">\(^{a,b}\)</span> y <em>José-María Montero</em><span class="math inline">\(^{c}\)</span></p>
<p><span class="math inline">\(^{a}\)</span>Universidad Complutense de Madrid<br><span class="math inline">\(^{b}\)</span>Instituto Nacional de Estadística<br><span class="math inline">\(^{c}\)</span>Universidad de Castilla-La Mancha</p>
<div id="introducción-12" class="section level2" number="26.1">
<h2>
<span class="header-section-number">26.1</span> Introducción<a class="anchor" aria-label="anchor" href="#introducci%C3%B3n-12"><i class="fas fa-link"></i></a>
</h2>
<p>El <em>k</em>-vecinos más próximos (KNN, por sus siglas en inglés <em>k-nearest neighbors</em>) es un algoritmo de aprendizaje no paramétrico. Los algoritmos no paramétricos no presuponen la forma concreta del modelo a entrenar, lo que les dota de una enorme flexibilidad. Sin embargo, como contrapartida, necesitan más datos de entrenamiento, lo que se traduce en que son más lentos que los algoritmos paramétricos. Al contrario que otros algoritmos de aprendizaje que permiten deshacerse de los datos de entrenamiento una vez se entrena el modelo, el modelo KNN guarda las observaciones de entrenamiento en la memoria. Esto es, al incorporar una nueva observación <span class="math inline">\({\bf{x}}_j\)</span>, el algoritmo KNN encuentra las <span class="math inline">\(k\)</span> observaciones del conjunto de datos de entrenamiento más parecidas a la nueva y proporciona como salida (predicción) la clase de la variable respuesta a la que pertenecen la mayor parte de las <em>k</em> observaciones (cuando se trata de un problema de clasificación) o el valor medio de la variable respuesta en dichas <span class="math inline">\(k\)</span> observaciones (en el caso de que el problema sea de regresión).</p>
<p>El número de casos (<span class="math inline">\(k\)</span>) a utilizar para clasificar las nuevas observaciones es un parámetro crucial en este algoritmo <span class="citation">(<a href="referencias.html#ref-james2013introduction">G. James et al., 2013</a>)</span>. Si, por ejemplo, <span class="math inline">\(k=3\)</span>, el modelo KNN utiliza las tres observaciones más parecidas (vecinas) al nuevo caso para clasificarlo. Es recomendable probar distintos valores de <span class="math inline">\(k\)</span> para conseguir el mejor ajuste del modelo. No obstante, es conveniente evitar valores extremos de <span class="math inline">\(k\)</span> porque valores muy bajos de <span class="math inline">\(k\)</span> aumentarán la variabilidad y conducirán a clasificaciones erróneas, mientras que valores muy elevados de <span class="math inline">\(k\)</span> harán que el algoritmo sea computacionalmente costoso y, además, tampoco llevarán a buenas clasificaciones (por ejemplo, en el caso de regresión, si el valor de <span class="math inline">\(k\)</span> está cerca del número de observaciones, la predicción para cualquier observación nueva siempre será un valor muy cercano a la media de la variable respuesta en el conjunto de datos de entrenamiento). Además, también se recomienda establecer valores impares de <span class="math inline">\(k\)</span> para evitar, en el caso clasificatorio, puntos muertos estadísticos (empate entre categorías de la variable respuesta, por lo que no se puede decidir cuál es la mayoritaria).</p>
<p>A modo de ejemplo, en la Fig. <a href="cap-knn.html#fig:knn-ejemplo">26.1</a>, tanto con <span class="math inline">\(k=3\)</span> como con <span class="math inline">\(k=7\)</span> la nueva observación se clasifica en la categoría roja, puesto que esta es la mayoritaria en ambos casos.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:knn-ejemplo"></span>
<img src="img/knn.png" alt="Ejemplo de $k$-vecinos más próximos." width="60%"><p class="caption">
Figura 26.1: Ejemplo de <span class="math inline">\(k\)</span>-vecinos más próximos.
</p>
</div>
<p>La escala de las variables puede desvirtuar el resultado del modelo KNN. Por ello, el conjunto de datos debe escalarse para que las variables con unidades de medida grandes no tengan más importancia en el cálculo de la similitud entre las observaciones que las que tienen magnitudes menores. De esta manera desaparece el efecto “escala” o “unidades de medida” y se estandariza la varianza.</p>
<p>Pese a que el modelo KNN es fácil de entender y generalmente preciso, almacenar el conjunto de datos de entrenamiento, así como calcular la distancia entre cada nueva observación a clasificar y las observaciones del conjunto de datos, exige elevados recursos computacionales. Esto implica que cuanto mayor es la cantidad de observaciones en el conjunto de datos, mayor es el tiempo de ejecución para obtener una única predicción, lo cual puede dar lugar a tiempos de procesamiento grandes. Por este motivo, no se recomienda el uso del algoritmo KNN con conjuntos de datos muy grandes. Otra desventaja a tener en cuenta es la dificultad de aplicar KNN a conjuntos de datos con un gran número de variables, puesto que calcular las distancias entre observaciones con múltiples dimensiones también incrementa la necesidad de recursos computacionales y podría dificultar la obtención de clasificaciones y predicciones precisas (en el caso de que la variable respuesta sea numérica).</p>
</div>
<div id="decisiones-a-tener-en-cuenta" class="section level2" number="26.2">
<h2>
<span class="header-section-number">26.2</span> Decisiones a tener en cuenta<a class="anchor" aria-label="anchor" href="#decisiones-a-tener-en-cuenta"><i class="fas fa-link"></i></a>
</h2>
<p>La elección de la función de distancia, así como el número de vecinos, <span class="math inline">\(k\)</span>, son decisiones que debe tomar el investigador antes de ejecutar el algoritmo. Este último es el hiperparámetro del modelo, cuyo valor devuelve la función <code><a href="https://rdrr.io/pkg/caret/man/modelLookup.html">modelLookup()</a></code> de <code>caret</code>:</p>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb376-1"><a href="cap-knn.html#cb376-1" tabindex="-1"></a><span class="fu">modelLookup</span>(<span class="st">"knn"</span>)</span>
<span id="cb376-2"><a href="cap-knn.html#cb376-2" tabindex="-1"></a>  model parameter      label forReg forClass probModel</span>
<span id="cb376-3"><a href="cap-knn.html#cb376-3" tabindex="-1"></a><span class="dv">1</span>   knn         k <span class="co">#Neighbors   TRUE     TRUE      TRUE</span></span></code></pre></div>
<div id="función-de-distancia-a-utilizar" class="section level3" number="26.2.1">
<h3>
<span class="header-section-number">26.2.1</span> Función de distancia a utilizar<a class="anchor" aria-label="anchor" href="#funci%C3%B3n-de-distancia-a-utilizar"><i class="fas fa-link"></i></a>
</h3>
<p>El grado de “vecindad” entre dos observaciones con predictores cuantitativos se mide a través de una función de distancia. En el proceso de entrenamiento del algoritmo KNN, generalmente se utiliza la distancia euclídea, mostrada en la ecuación <a href="cap-knn.html#eq:dist-eu">(26.1)</a>, o la distancia de Manhattan, que puede verse en la ecuación <a href="cap-knn.html#eq:dist-man">(26.2)</a>. En caso de que haya predictores cuantitativos y cualitativos (o solo cualitativos) el coeficiente de Gower es una buena opción, como muestra la ecuación <a href="cap-knn.html#eq:dist-Gow">(26.3)</a>. No obstante, también pueden utilizarse otras funciones de distancia. Un buen número de ellas puede verse en el Cap. <a href="cap-cluster.html#cap-cluster">30</a>.</p>
<p>Sean dos observaciones <span class="math inline">\(p\)</span>-dimensionales, <span class="math inline">\({\bf{x}}_r\)</span> y <span class="math inline">\({\bf{x}}_s\)</span>:</p>
<ul>
<li>
<strong>Distancia euclídea.</strong> Se define como:</li>
</ul>
<p><span class="math display" id="eq:dist-eu">\[\begin{equation}
\tag{26.1}
    d_{e}({\bf x}_r;{\bf{x}}_{s})=\sqrt{\sum_{k=1}^{p}\left(  x_{rk}-x_{sk}\right)  ^{2}}.
\end{equation}\]</span></p>
<p>
</p>
<p>Ignora las unidades de medida de las variables y, en consecuencia, aunque es invariante a los cambios de origen, no lo es a los cambios de escala. También ignora las relaciones entre ellas. Resulta de utilidad con variables cuantitativas incorrelacionadas y medidas en las mismas unidades.</p>
<ul>
<li>
<strong>Distancia Manhattan</strong> o <strong><em>city block.</em></strong> Se define como:</li>
</ul>
<p><span class="math display" id="eq:dist-man">\[\begin{equation}
\tag{26.2}
d_{MAN}({\bf x}_r;{\bf{x}}_{s})=\sum_{k=1}^{p}\left\vert x_{rk}-x_{sk}\right\vert.
\end{equation}\]</span></p>
<p></p>
<p>Le afectan las diferentes escalas de las variables y es menos sensible que la distancia euclídea a los valores extremos. Por ello, es recomendable cuando las variables son cuantitativas, con las mismas unidades de medida (o escaladas), sin relaciones entre ellas y con valores extremos.</p>
<ul>
<li>Si no todas las variables son cuantitativas, se utiliza la <strong>medida de similaridad de Gower</strong>: </li>
</ul>
<p><span class="math display" id="eq:dist-Gow">\[\begin{equation}
\tag{26.3}
S_{rs}({\bf x}_r;{\bf{x}}_{s})=\frac{\sum_{k=1}^{p}s_{rs}}{\sum_{k=1}^{p}w_{rs}},
\end{equation}\]</span></p>
<p>donde <span class="math inline">\(w_{rs}\)</span> vale siempre la unidad, salvo para variables binarias si los dos elementos presentan el valor cero. En cuanto al valor de <span class="math inline">\(S_{rs}\)</span>, se distinguen tres casos:</p>
<pre><code>-   Variables cualitativas de más de dos niveles: 1 si ambos elementos son iguales en la *k*-ésima variable; 0 si son diferentes.

-   Variables dicotómicas: 1 si la variable considerada está presente en ambos elementos; 0 en los demás casos.

-   Variables cuantitativas: $1-\frac {|x_{ik}-x_{jk}|}{R_k}$, donde $R$ es el rango de la variable *k*.</code></pre>
<p>No es recomendable cuando las variables cuantitativas sean muy asimétricas. En este caso, hay dos procedimientos aproximados: <span class="math inline">\((i)\)</span> calcular medidas separadas para las variables cuantitativas y cualitativas y combinarlas estableciendo algún tipo de ponderación (véase Cap. <a href="cap-cluster.html#cap-cluster">30</a>); <span class="math inline">\((ii)\)</span> pasar las variables cuantitativas a cualitativas y utilizar las medidas propuestas para este tipo de variables (véase también Cap. <a href="cap-cluster.html#cap-cluster">30</a>).</p>
</div>
<div id="número-de-vecinos-k-seleccionados" class="section level3" number="26.2.2">
<h3>
<span class="header-section-number">26.2.2</span> Número de vecinos (<em>k</em>) seleccionados<a class="anchor" aria-label="anchor" href="#n%C3%BAmero-de-vecinos-k-seleccionados"><i class="fas fa-link"></i></a>
</h3>
<p>Como ya se ha avanzado, la elección del número de vecinos (<span class="math inline">\(k\)</span>) que intervienen en el ajuste del algoritmo es determinante para su rendimiento. Si se escogen demasiado pocos vecinos, se producirá sobreajuste en el modelo. En el caso extremo en el que solo se utilizase un vecino (<span class="math inline">\(k=1\)</span>), la predicción se basará únicamente en la observación con la menor distancia al elemento a clasificar. Por otro lado, un número alto de vecinos hace que el modelo no ajuste bien al tener en cuenta un vecindario demasiado grande. En este sentido, en el caso extremo de considerar como vecinas todas las observaciones disponibles (<span class="math inline">\(k=n\)</span>), sea cual sea la nueva observación, la predicción siempre será el
valor medio de la variable respuesta en el conjunto de datos (en el caso de la regresión) o la clase mayoritaria de la variable respuesta en dicho conjunto (en el caso de la clasificación).</p>
<p>No existe una regla general para la elección óptima de <span class="math inline">\(k\)</span>, puesto que en gran medida dependerá del conjunto de datos utilizado. Cuando el conjunto de datos tiene pocas variables que no aportan información, valores pequeños de <span class="math inline">\(k\)</span> tienden a funcionar mejor. Cuantas más variables sin importancia se incluyen en el conjunto predictores, mayor deberá ser el valor de <span class="math inline">\(k\)</span> para suavizar su efecto, es decir, para reducir el ruido que incorporan dichas variables, que no aportan información nueva para mejorar el modelo.</p>
</div>
</div>
<div id="procedimiento-con-r-la-función-knn" class="section level2" number="26.3">
<h2>
<span class="header-section-number">26.3</span> Procedimiento con <strong>R</strong>: la función <code>knn()</code><a class="anchor" aria-label="anchor" href="#procedimiento-con-r-la-funci%C3%B3n-knn"><i class="fas fa-link"></i></a>
</h2>
<p>En el paquete <code>class</code> de <strong>R</strong> se encuentra la función <code>knn()</code> que se utiliza para entrenar el modelo <em>k</em>-vecinos más próximos:</p>
<div class="sourceCode" id="cb378"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">knn</span><span class="op">(</span><span class="va">train</span>, <span class="va">test</span>, <span class="va">cl</span>, k <span class="op">=</span> <span class="fl">1</span>, <span class="va">...</span><span class="op">)</span></span></code></pre></div>
<ul>
<li>
<code>train</code>: conjunto de datos con las observaciones de entrenamiento.</li>
<li>
<code>test</code>: conjunto de datos con las observaciones de validación. Un vector se interpreta como una única observación a validar.</li>
<li>
<code>cl</code>: clases de las observaciones de entrenamiento.</li>
<li>
<code>k</code>: número de vecinos a considerar</li>
</ul>
</div>
<div id="aplicación-del-modelo-knn-en-r" class="section level2" number="26.4">
<h2>
<span class="header-section-number">26.4</span> Aplicación del modelo KNN en <strong>R</strong><a class="anchor" aria-label="anchor" href="#aplicaci%C3%B3n-del-modelo-knn-en-r"><i class="fas fa-link"></i></a>
</h2>
<p>Para llevar a cabo la aplicación en la que se centra esta sección se retoma el ejemplo expuesto en los dos capítulos precedentes relativo a la empresa Beauty eSheep, que pretende vender tensiómetros digitales, obteniéndose las predicciones sobre si el cliente comprará o no un tensiómetro en función de una serie de características del cliente (en este caso, todas ellas cuantitativas) que actúan como variables predictoras y que se encuentran, junto con la variable respuesta, en el conjunto de datos <code>dp_entr_NUM</code>, incluido en el paquete <code>CDR</code>, y que se resume en la Tabla <a href="cap-arboles.html#tab:dpentr">24.8</a>.</p>
<p>Dado que todas las variables predictoras son cuantitativas y que tienen distintas escalas de medida (euros, años, unidades, etc.) es necesario indicar en la función <code><a href="https://rdrr.io/pkg/caret/man/trainControl.html">trainControl()</a></code> que se haga un preprocesamiento para estandarizarlas. Además, se define como método de validación (con remuestreo) el de validación cruzada con 10 grupos con repetición (véase Cap. <a href="chap-herramientas.html#chap-herramientas">10</a>).</p>
<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb379-1"><a href="cap-knn.html#cb379-1" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"CDR"</span>)</span>
<span id="cb379-2"><a href="cap-knn.html#cb379-2" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"class"</span>)</span>
<span id="cb379-3"><a href="cap-knn.html#cb379-3" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"caret"</span>)</span>
<span id="cb379-4"><a href="cap-knn.html#cb379-4" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"reshape"</span>)</span>
<span id="cb379-5"><a href="cap-knn.html#cb379-5" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"ggplot2"</span>)</span>
<span id="cb379-6"><a href="cap-knn.html#cb379-6" tabindex="-1"></a></span>
<span id="cb379-7"><a href="cap-knn.html#cb379-7" tabindex="-1"></a><span class="fu">data</span>(dp_entr_NUM)</span>
<span id="cb379-8"><a href="cap-knn.html#cb379-8" tabindex="-1"></a><span class="fu">head</span>(dp_entr_NUM)</span>
<span id="cb379-9"><a href="cap-knn.html#cb379-9" tabindex="-1"></a></span>
<span id="cb379-10"><a href="cap-knn.html#cb379-10" tabindex="-1"></a>  ind_pro11 ind_pro12 ind_pro14 ind_pro15 ind_pro16 ind_pro17 des_nivel_edu.ALTO</span>
<span id="cb379-11"><a href="cap-knn.html#cb379-11" tabindex="-1"></a><span class="dv">1</span>         <span class="dv">1</span>         <span class="dv">0</span>         <span class="dv">1</span>         <span class="dv">1</span>         <span class="dv">1</span>         <span class="dv">0</span>                  <span class="dv">0</span></span>
<span id="cb379-12"><a href="cap-knn.html#cb379-12" tabindex="-1"></a><span class="dv">2</span>         <span class="dv">0</span>         <span class="dv">0</span>         <span class="dv">1</span>         <span class="dv">0</span>         <span class="dv">1</span>         <span class="dv">0</span>                  <span class="dv">0</span></span>
<span id="cb379-13"><a href="cap-knn.html#cb379-13" tabindex="-1"></a><span class="dv">3</span>         <span class="dv">0</span>         <span class="dv">0</span>         <span class="dv">1</span>         <span class="dv">1</span>         <span class="dv">1</span>         <span class="dv">1</span>                  <span class="dv">0</span></span>
<span id="cb379-14"><a href="cap-knn.html#cb379-14" tabindex="-1"></a><span class="dv">4</span>         <span class="dv">0</span>         <span class="dv">1</span>         <span class="dv">1</span>         <span class="dv">0</span>         <span class="dv">0</span>         <span class="dv">0</span>                  <span class="dv">0</span></span>
<span id="cb379-15"><a href="cap-knn.html#cb379-15" tabindex="-1"></a><span class="dv">5</span>         <span class="dv">0</span>         <span class="dv">1</span>         <span class="dv">1</span>         <span class="dv">0</span>         <span class="dv">1</span>         <span class="dv">0</span>                  <span class="dv">0</span></span>
<span id="cb379-16"><a href="cap-knn.html#cb379-16" tabindex="-1"></a><span class="dv">6</span>         <span class="dv">1</span>         <span class="dv">0</span>         <span class="dv">1</span>         <span class="dv">0</span>         <span class="dv">0</span>         <span class="dv">0</span>                  <span class="dv">1</span></span>
<span id="cb379-17"><a href="cap-knn.html#cb379-17" tabindex="-1"></a>  des_nivel_edu.BASICO des_nivel_edu.MEDIO importe_pro11 importe_pro12 importe_pro14</span>
<span id="cb379-18"><a href="cap-knn.html#cb379-18" tabindex="-1"></a><span class="dv">1</span>                    <span class="dv">0</span>                   <span class="dv">1</span>           <span class="dv">157</span>             <span class="dv">0</span>            <span class="dv">40</span></span>
<span id="cb379-19"><a href="cap-knn.html#cb379-19" tabindex="-1"></a><span class="dv">2</span>                    <span class="dv">0</span>                   <span class="dv">1</span>             <span class="dv">0</span>             <span class="dv">0</span>           <span class="dv">240</span></span>
<span id="cb379-20"><a href="cap-knn.html#cb379-20" tabindex="-1"></a><span class="dv">3</span>                    <span class="dv">1</span>                   <span class="dv">0</span>             <span class="dv">0</span>             <span class="dv">0</span>           <span class="dv">425</span></span>
<span id="cb379-21"><a href="cap-knn.html#cb379-21" tabindex="-1"></a><span class="dv">4</span>                    <span class="dv">0</span>                   <span class="dv">1</span>             <span class="dv">0</span>           <span class="dv">120</span>            <span class="dv">60</span></span>
<span id="cb379-22"><a href="cap-knn.html#cb379-22" tabindex="-1"></a><span class="dv">5</span>                    <span class="dv">1</span>                   <span class="dv">0</span>             <span class="dv">0</span>           <span class="dv">120</span>           <span class="dv">133</span></span>
<span id="cb379-23"><a href="cap-knn.html#cb379-23" tabindex="-1"></a><span class="dv">6</span>                    <span class="dv">0</span>                   <span class="dv">0</span>           <span class="dv">115</span>             <span class="dv">0</span>           <span class="dv">220</span></span>
<span id="cb379-24"><a href="cap-knn.html#cb379-24" tabindex="-1"></a>  importe_pro15 importe_pro16 importe_pro17 edad tamano_fam anos_exp ingresos_ano CLS_PRO_pro13</span>
<span id="cb379-25"><a href="cap-knn.html#cb379-25" tabindex="-1"></a><span class="dv">1</span>           <span class="dv">200</span>           <span class="dv">180</span>             <span class="dv">0</span>   <span class="dv">49</span>          <span class="dv">4</span>       <span class="dv">24</span>        <span class="dv">30000</span>             S</span>
<span id="cb379-26"><a href="cap-knn.html#cb379-26" tabindex="-1"></a><span class="dv">2</span>             <span class="dv">0</span>           <span class="dv">180</span>             <span class="dv">0</span>   <span class="dv">38</span>          <span class="dv">2</span>       <span class="dv">12</span>        <span class="dv">53000</span>             N</span>
<span id="cb379-27"><a href="cap-knn.html#cb379-27" tabindex="-1"></a><span class="dv">3</span>           <span class="dv">200</span>           <span class="dv">180</span>           <span class="dv">300</span>   <span class="dv">61</span>          <span class="dv">4</span>       <span class="dv">37</span>       <span class="dv">172000</span>             S</span>
<span id="cb379-28"><a href="cap-knn.html#cb379-28" tabindex="-1"></a><span class="dv">4</span>             <span class="dv">0</span>             <span class="dv">0</span>             <span class="dv">0</span>   <span class="dv">47</span>          <span class="dv">3</span>       <span class="dv">21</span>        <span class="dv">38000</span>             N</span>
<span id="cb379-29"><a href="cap-knn.html#cb379-29" tabindex="-1"></a><span class="dv">5</span>             <span class="dv">0</span>           <span class="dv">180</span>             <span class="dv">0</span>   <span class="dv">34</span>          <span class="dv">1</span>       <span class="dv">10</span>        <span class="dv">38000</span>             N</span>
<span id="cb379-30"><a href="cap-knn.html#cb379-30" tabindex="-1"></a><span class="dv">6</span>             <span class="dv">0</span>             <span class="dv">0</span>             <span class="dv">0</span>   <span class="dv">43</span>          <span class="dv">2</span>       <span class="dv">18</span>        <span class="dv">60000</span>             N</span></code></pre></div>
<div class="sourceCode" id="cb380"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Definimos un método de remuestreo</span></span>
<span><span class="va">cv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/trainControl.html">trainControl</a></span><span class="op">(</span></span>
<span>  method <span class="op">=</span> <span class="st">"repeatedcv"</span>,</span>
<span>  number <span class="op">=</span> <span class="fl">10</span>,</span>
<span>  repeats <span class="op">=</span> <span class="fl">5</span>,</span>
<span>  classProbs <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  preProcOptions <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="st">"center"</span><span class="op">)</span>,</span>
<span>  summaryFunction <span class="op">=</span> <span class="va">twoClassSummary</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>En cuanto a la selección del hiperparámetro <span class="math inline">\(k\)</span>, en el paquete <code>caret</code> de <strong>R</strong> se puede definir una red de posibles valores sobre los que evaluar el modelo KNN, determinándose automáticamente el valor que mejor rendimiento proporciona. A continuación se definen los posibles valores de <span class="math inline">\(k\)</span> que se quieren evaluar.</p>
<div class="sourceCode" id="cb381"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Definimos la red de posibles valores del hiperparámetro</span></span>
<span><span class="va">hyper_grid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/expand.grid.html">expand.grid</a></span><span class="op">(</span>k <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span>,<span class="fl">15</span>,<span class="fl">20</span>,<span class="fl">30</span>,<span class="fl">50</span>,<span class="fl">75</span>,<span class="fl">100</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Una vez que se han definido tanto el método de validación repetida con remuestreo como la red de posibles valores del hiperparámetro se procede a entrenar el modelo:<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;En la salida del modelo, &lt;code&gt;Sens&lt;/code&gt; (sensibilidad) responde a la pregunta: ¿cuando en realidad es un SÍ, cuál es el porcentaje de aciertos del clasificador? Su expresión es &lt;span class="math inline"&gt;\(Sensibilidad=\frac{VP}{VP+FN}\)&lt;/span&gt;, donde &lt;span class="math inline"&gt;\(VP\)&lt;/span&gt; es un verdadero positivo (un cliente que compró el tensiómetro digital es clasificado como comprador) y &lt;span class="math inline"&gt;\(FN\)&lt;/span&gt; un falso negativo (un cliente que lo compró es clasificado como no que no lo compró) (véase Cap. &lt;a href="chap-herramientas.html#chap-herramientas"&gt;10&lt;/a&gt; para mejor entendimiento de estos conceptos). Por consiguiente, la sensibilidad es una medida de la probabilidad de que un cliente que compró el tensiómetro sea clasificado correctamente. &lt;code&gt;Spec&lt;/code&gt; (especificidad) es el porcentaje de verdaderos negativos respecto de todo lo que debería haber sido clasificado como negativo. Su expresión es &lt;span class="math inline"&gt;\(\frac{FP}{FP+VN}\)&lt;/span&gt;, donde &lt;span class="math inline"&gt;\(FP\)&lt;/span&gt; es un falso positivo (un cliente no compró el tensiómetro y, sin embargo, fue clasificado como comprador) y &lt;span class="math inline"&gt;\(VN\)&lt;/span&gt; es un verdadero negativo (un cliente no compró el tensiómetro y fue clasificado correctamente). Su complementario, la 1-especificidad (&lt;span class="math inline"&gt;\(\frac{FP}{FP+VN}\)&lt;/span&gt;) es, básicamente, una medida de la frecuencia (relativa) con la que se producirá una falsa alarma, o la frecuencia con la que un caso real negativo (no compra del tensiómetro) se clasifique como positivo (compra del producto). La curva ROC es la que surge de graficar la sensibilidad (tasa de verdaderos positivos) frente a la tasa de falsos positivos (1-especificidad).&lt;/p&gt;'><sup>192</sup></a></p>
<div class="sourceCode" id="cb382"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">101</span><span class="op">)</span></span>
<span><span class="co"># Se entrena el modelo ajustando el hiperparámetro óptimo</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/train.html">train</a></span><span class="op">(</span></span>
<span>  <span class="va">CLS_PRO_pro13</span> <span class="op">~</span> <span class="va">.</span>,</span>
<span>  data <span class="op">=</span> <span class="va">dp_entr_NUM</span>,</span>
<span>  method <span class="op">=</span> <span class="st">"knn"</span>,</span>
<span>  trControl <span class="op">=</span> <span class="va">cv</span>,</span>
<span>  tuneGrid <span class="op">=</span> <span class="va">hyper_grid</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"ROC"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb383"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb383-1"><a href="cap-knn.html#cb383-1" tabindex="-1"></a><span class="co"># se muestra la salida del modelo</span></span>
<span id="cb383-2"><a href="cap-knn.html#cb383-2" tabindex="-1"></a>model</span>
<span id="cb383-3"><a href="cap-knn.html#cb383-3" tabindex="-1"></a></span>
<span id="cb383-4"><a href="cap-knn.html#cb383-4" tabindex="-1"></a>k<span class="sc">-</span>Nearest Neighbors</span>
<span id="cb383-5"><a href="cap-knn.html#cb383-5" tabindex="-1"></a></span>
<span id="cb383-6"><a href="cap-knn.html#cb383-6" tabindex="-1"></a><span class="dv">558</span> samples</span>
<span id="cb383-7"><a href="cap-knn.html#cb383-7" tabindex="-1"></a> <span class="dv">17</span> predictor</span>
<span id="cb383-8"><a href="cap-knn.html#cb383-8" tabindex="-1"></a>  <span class="dv">2</span> classes<span class="sc">:</span> <span class="st">'S'</span>, <span class="st">'N'</span></span>
<span id="cb383-9"><a href="cap-knn.html#cb383-9" tabindex="-1"></a></span>
<span id="cb383-10"><a href="cap-knn.html#cb383-10" tabindex="-1"></a>No pre<span class="sc">-</span>processing</span>
<span id="cb383-11"><a href="cap-knn.html#cb383-11" tabindex="-1"></a>Resampling<span class="sc">:</span> Cross<span class="sc">-</span><span class="fu">Validated</span> (<span class="dv">10</span> fold, repeated <span class="dv">5</span> times)</span>
<span id="cb383-12"><a href="cap-knn.html#cb383-12" tabindex="-1"></a>Summary of sample sizes<span class="sc">:</span> <span class="dv">502</span>, <span class="dv">502</span>, <span class="dv">502</span>, <span class="dv">503</span>, <span class="dv">503</span>, <span class="dv">502</span>, ...</span>
<span id="cb383-13"><a href="cap-knn.html#cb383-13" tabindex="-1"></a>Resampling results across tuning parameters<span class="sc">:</span></span>
<span id="cb383-14"><a href="cap-knn.html#cb383-14" tabindex="-1"></a></span>
<span id="cb383-15"><a href="cap-knn.html#cb383-15" tabindex="-1"></a>  k    ROC        Sens       Spec</span>
<span id="cb383-16"><a href="cap-knn.html#cb383-16" tabindex="-1"></a>    <span class="dv">1</span>  <span class="fl">0.6584524</span>  <span class="fl">0.6466402</span>  <span class="fl">0.6702646</span></span>
<span id="cb383-17"><a href="cap-knn.html#cb383-17" tabindex="-1"></a>    <span class="dv">2</span>  <span class="fl">0.6769109</span>  <span class="fl">0.6179101</span>  <span class="fl">0.6131217</span></span>
<span id="cb383-18"><a href="cap-knn.html#cb383-18" tabindex="-1"></a>    <span class="dv">3</span>  <span class="fl">0.6828893</span>  <span class="fl">0.6216402</span>  <span class="fl">0.6496561</span></span>
<span id="cb383-19"><a href="cap-knn.html#cb383-19" tabindex="-1"></a>    <span class="dv">4</span>  <span class="fl">0.6851087</span>  <span class="fl">0.6404233</span>  <span class="fl">0.6394709</span></span>
<span id="cb383-20"><a href="cap-knn.html#cb383-20" tabindex="-1"></a>    <span class="dv">5</span>  <span class="fl">0.6951129</span>  <span class="fl">0.6540212</span>  <span class="fl">0.6666138</span></span>
<span id="cb383-21"><a href="cap-knn.html#cb383-21" tabindex="-1"></a>    <span class="dv">6</span>  <span class="fl">0.6914664</span>  <span class="fl">0.6216402</span>  <span class="fl">0.6543386</span></span>
<span id="cb383-22"><a href="cap-knn.html#cb383-22" tabindex="-1"></a>    <span class="dv">7</span>  <span class="fl">0.6982592</span>  <span class="fl">0.6252381</span>  <span class="fl">0.6953439</span></span>
<span id="cb383-23"><a href="cap-knn.html#cb383-23" tabindex="-1"></a>    <span class="dv">8</span>  <span class="fl">0.6974556</span>  <span class="fl">0.6281481</span>  <span class="fl">0.6960053</span></span>
<span id="cb383-24"><a href="cap-knn.html#cb383-24" tabindex="-1"></a>    <span class="dv">9</span>  <span class="fl">0.6992229</span>  <span class="fl">0.6159524</span>  <span class="fl">0.7117725</span></span>
<span id="cb383-25"><a href="cap-knn.html#cb383-25" tabindex="-1"></a>   <span class="dv">10</span>  <span class="fl">0.6994133</span>  <span class="fl">0.6037037</span>  <span class="fl">0.7052910</span></span>
<span id="cb383-26"><a href="cap-knn.html#cb383-26" tabindex="-1"></a>   <span class="dv">15</span>  <span class="fl">0.6875879</span>  <span class="fl">0.5749206</span>  <span class="fl">0.7232011</span></span>
<span id="cb383-27"><a href="cap-knn.html#cb383-27" tabindex="-1"></a>   <span class="dv">20</span>  <span class="fl">0.6731477</span>  <span class="fl">0.5722751</span>  <span class="fl">0.7010582</span></span>
<span id="cb383-28"><a href="cap-knn.html#cb383-28" tabindex="-1"></a>   <span class="dv">30</span>  <span class="fl">0.6752986</span>  <span class="fl">0.5529630</span>  <span class="fl">0.7024603</span></span>
<span id="cb383-29"><a href="cap-knn.html#cb383-29" tabindex="-1"></a>   <span class="dv">50</span>  <span class="fl">0.6890259</span>  <span class="fl">0.5163757</span>  <span class="fl">0.7605556</span></span>
<span id="cb383-30"><a href="cap-knn.html#cb383-30" tabindex="-1"></a>   <span class="dv">75</span>  <span class="fl">0.6852886</span>  <span class="fl">0.5092593</span>  <span class="fl">0.7670106</span></span>
<span id="cb383-31"><a href="cap-knn.html#cb383-31" tabindex="-1"></a>  <span class="dv">100</span>  <span class="fl">0.6719378</span>  <span class="fl">0.5049471</span>  <span class="fl">0.7820106</span></span>
<span id="cb383-32"><a href="cap-knn.html#cb383-32" tabindex="-1"></a></span>
<span id="cb383-33"><a href="cap-knn.html#cb383-33" tabindex="-1"></a>ROC was used to select the optimal model using the largest value.</span>
<span id="cb383-34"><a href="cap-knn.html#cb383-34" tabindex="-1"></a>The final value used <span class="cf">for</span> the model was k <span class="ot">=</span> <span class="fl">10.</span></span></code></pre></div>
<p>En los resultados mostrados, para cada valor de <span class="math inline">\(k\)</span> se toman 10 muestras con 500 observaciones en el conjunto de entrenamiento y 50 en el de test. Dichas 500 y 50 observaciones van rotando de modelo a modelo para tener 10 distintos. Los valores <code>ROC</code>, <code>Sens</code> y <code>Spec</code> que aparecen en los resultados son la media de los obtenidos en esos 10 modelos.</p>
<p>Tanto en los resultados mostrados como en la Fig. <a href="cap-knn.html#fig:006-002-102KNNKCHOOSING3">26.2</a>, se observa que el número óptimo de vecinos es <span class="math inline">\(k=10\)</span>, número de vecinos para el cual se alcanza el rendimiento óptimo del modelo (el mayor valor del área bajo la curva ROC, cuya sigla es AUC). Una vez determinado el número de vecinos óptimos, el modelo seleccionado utiliza todos los datos (en este caso <span class="math inline">\(550=500+50\)</span>) para clasificar una nueva observación.</p>
<div class="sourceCode" id="cb384"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/unlist.html">unlist</a></span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">bestTune</span><span class="op">)</span>,col<span class="op">=</span><span class="st">"red"</span>,linetype<span class="op">=</span><span class="st">"dashed"</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_light</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:006-002-102KNNKCHOOSING3"></span>
<img src="img/knn-tune-k.png" alt="Número óptimo de vecinos ($k$)." width="60%"><p class="caption">
Figura 26.2: Número óptimo de vecinos (<span class="math inline">\(k\)</span>).
</p>
</div>
<p>El <em>box plot</em> de los resultados obtenidos durante el proceso de validación cruzada (Fig. <a href="cap-knn.html#fig:006-002-102KNNRESULTS3">26.3</a>) muestra que el AUC del modelo oscila entre un 60% y un 85% aproximadamente.</p>
<div class="sourceCode" id="cb385"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/reshape/man/melt-24.html">melt</a></span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">resample</span><span class="op">[</span>,<span class="op">-</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">variable</span>, y <span class="op">=</span> <span class="va">value</span>, fill<span class="op">=</span><span class="va">variable</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>   <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_boxplot.html">geom_boxplot</a></span><span class="op">(</span>show.legend<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:006-002-102KNNRESULTS3"></span>
<img src="img/knn-boxplot.png" alt="Resultados obtenidos durante el proceso de validación cruzada." width="60%"><p class="caption">
Figura 26.3: Resultados obtenidos durante el proceso de validación cruzada.
</p>
</div>
<p>El modelo se resiente en su rendimiento al tener dificultades en clasificar correctamente la clase positiva; más concretamente se puede observar en la Fig. <a href="cap-knn.html#fig:006-002-102KNNRESULTS3">26.3</a> que la sensibilidad oscila entre el 40% y el 75%, resultados ligeramente peores que los que obtienen al clasificar observaciones de la clase negativa, los cuales oscilan entre el 50% y el 85%.</p>
<p>La matriz de confusión para <span class="math inline">\(k=10\)</span>, así como las medidas que se derivan de ella, se presentan en los siguientes resultados:</p>
<div class="sourceCode" id="cb386"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">101</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/caret/man/confusionMatrix.html">confusionMatrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span>, <span class="va">dp_entr_NUM</span><span class="op">$</span><span class="va">CLS_PRO_pro13</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb387"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb387-1"><a href="cap-knn.html#cb387-1" tabindex="-1"></a>Confusion Matrix and Statistics</span>
<span id="cb387-2"><a href="cap-knn.html#cb387-2" tabindex="-1"></a></span>
<span id="cb387-3"><a href="cap-knn.html#cb387-3" tabindex="-1"></a>          Reference</span>
<span id="cb387-4"><a href="cap-knn.html#cb387-4" tabindex="-1"></a>Prediction   S   N</span>
<span id="cb387-5"><a href="cap-knn.html#cb387-5" tabindex="-1"></a>         S <span class="dv">186</span>  <span class="dv">65</span></span>
<span id="cb387-6"><a href="cap-knn.html#cb387-6" tabindex="-1"></a>         N  <span class="dv">93</span> <span class="dv">214</span></span>
<span id="cb387-7"><a href="cap-knn.html#cb387-7" tabindex="-1"></a>                                          </span>
<span id="cb387-8"><a href="cap-knn.html#cb387-8" tabindex="-1"></a>               Accuracy <span class="sc">:</span> <span class="fl">0.7168</span>          </span>
<span id="cb387-9"><a href="cap-knn.html#cb387-9" tabindex="-1"></a>                 <span class="dv">95</span>% CI <span class="sc">:</span> (<span class="fl">0.6775</span>, <span class="fl">0.7539</span>)</span>
<span id="cb387-10"><a href="cap-knn.html#cb387-10" tabindex="-1"></a>    No Information Rate <span class="sc">:</span> <span class="fl">0.5</span>             </span>
<span id="cb387-11"><a href="cap-knn.html#cb387-11" tabindex="-1"></a>    P<span class="sc">-</span>Value [Acc <span class="sc">&gt;</span> NIR] <span class="sc">:</span> <span class="er">&lt;</span> <span class="fl">2e-16</span>         </span>
<span id="cb387-12"><a href="cap-knn.html#cb387-12" tabindex="-1"></a>                                          </span>
<span id="cb387-13"><a href="cap-knn.html#cb387-13" tabindex="-1"></a>                  Kappa <span class="sc">:</span> <span class="fl">0.4337</span>          </span>
<span id="cb387-14"><a href="cap-knn.html#cb387-14" tabindex="-1"></a>                                          </span>
<span id="cb387-15"><a href="cap-knn.html#cb387-15" tabindex="-1"></a> Mcnemar<span class="st">'s Test P-Value : 0.03171         </span></span>
<span id="cb387-16"><a href="cap-knn.html#cb387-16" tabindex="-1"></a><span class="st">                                          </span></span>
<span id="cb387-17"><a href="cap-knn.html#cb387-17" tabindex="-1"></a><span class="st">            Sensitivity : 0.6667          </span></span>
<span id="cb387-18"><a href="cap-knn.html#cb387-18" tabindex="-1"></a><span class="st">            Specificity : 0.7670          </span></span>
<span id="cb387-19"><a href="cap-knn.html#cb387-19" tabindex="-1"></a><span class="st">         Pos Pred Value : 0.7410          </span></span>
<span id="cb387-20"><a href="cap-knn.html#cb387-20" tabindex="-1"></a><span class="st">         Neg Pred Value : 0.6971          </span></span>
<span id="cb387-21"><a href="cap-knn.html#cb387-21" tabindex="-1"></a><span class="st">             Prevalence : 0.5000          </span></span>
<span id="cb387-22"><a href="cap-knn.html#cb387-22" tabindex="-1"></a><span class="st">         Detection Rate : 0.3333          </span></span>
<span id="cb387-23"><a href="cap-knn.html#cb387-23" tabindex="-1"></a><span class="st">   Detection Prevalence : 0.4498          </span></span>
<span id="cb387-24"><a href="cap-knn.html#cb387-24" tabindex="-1"></a><span class="st">      Balanced Accuracy : 0.7168          </span></span>
<span id="cb387-25"><a href="cap-knn.html#cb387-25" tabindex="-1"></a><span class="st">                                          </span></span>
<span id="cb387-26"><a href="cap-knn.html#cb387-26" tabindex="-1"></a><span class="st">       '</span>Positive<span class="st">' Class : S  </span></span></code></pre></div>
<p>donde la clase positiva es la compra del tensiómetro digital y la negativa, la no compra de dicho producto.</p>
<p>El valor predictivo positivo muestra que el modelo clasifica a 251 clientes como compradores del tensiómetro digital, si bien solo el 74,10% de estas clasificaciones es correcto. El valor predictivo negativo indica que de los 307 clientes que el modelo predijo como no compradores, tan solo el 69,71% realmente lo son (214 de ellos). Calculando ahora los porcentajes por columnas, la sensibilidad indica que el modelo ha sido capaz de clasificar correctamente al 66,67 % de los clientes compradores del tensiómetro, esto es, a 186 de los 279 que figuran como compradores en el conjunto de entrenamiento. Por otro lado, la especificidad indica que el 76,7% de los clientes que en el conjunto de entrenamiento pertenecían a la clase negativa (no compradora del tensiómetro) ha sido clasificado correctamente (214 de 279).</p>
<div id="resumen-25" class="section level3 unnumbered infobox_resume">
<h3>Resumen<a class="anchor" aria-label="anchor" href="#resumen-25"><i class="fas fa-link"></i></a>
</h3>
<p>En este capítulo se introduce al lector en el algoritmo de aprendizaje supervisado conocido como <em>k</em>-vecinos más próximos, destacando:</p>
<ul>
<li><p>las decisiones a tener en cuenta antes de entrenar el modelo,</p></li>
<li><p>las distancias más utilizadas en el entrenamiento del modelo,</p></li>
<li><p>las ventajas y desventajas del número de vecinos a tener en cuenta en labores de clasificación o prediccion.</p></li>
</ul>
</div>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="cap-svm.html"><span class="header-section-number">25</span> Máquinas de vector soporte</a></div>
<div class="next"><a href="cap-naive-bayes.html"><span class="header-section-number">27</span> Naive Bayes</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="Índice del capítulo"><h2>Índice del capítulo</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#cap-knn"><span class="header-section-number">26</span> Clasificador \(k\)-vecinos más próximos</a></li>
<li><a class="nav-link" href="#introducci%C3%B3n-12"><span class="header-section-number">26.1</span> Introducción</a></li>
<li>
<a class="nav-link" href="#decisiones-a-tener-en-cuenta"><span class="header-section-number">26.2</span> Decisiones a tener en cuenta</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#funci%C3%B3n-de-distancia-a-utilizar"><span class="header-section-number">26.2.1</span> Función de distancia a utilizar</a></li>
<li><a class="nav-link" href="#n%C3%BAmero-de-vecinos-k-seleccionados"><span class="header-section-number">26.2.2</span> Número de vecinos (k) seleccionados</a></li>
</ul>
</li>
<li><a class="nav-link" href="#procedimiento-con-r-la-funci%C3%B3n-knn"><span class="header-section-number">26.3</span> Procedimiento con R: la función knn()</a></li>
<li>
<a class="nav-link" href="#aplicaci%C3%B3n-del-modelo-knn-en-r"><span class="header-section-number">26.4</span> Aplicación del modelo KNN en R</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#resumen-25">Resumen</a></li></ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Fundamentos de ciencia de datos con <strong>R</strong></strong>" coordinado por <a href="https://blog.uclm.es/gemafaviles/" class="text-light">Gema Fernández-Avilés y José-María Montero</a>. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>Este libro ha sido generado con el paquete de R <a class="text-light" href="https://bookdown.org">bookdown</a>.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
