<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Capítulo 28 Métodos ensamblados: bagging y random forest | Fundamentos de ciencia de datos con R</title>
<meta name="author" content="Gema Fernández-Avilés y José-María Montero">
<meta name="description" content="Ramón A. Carrasco\(^{a}\) e Itzcóatl Bueno\(^{b,a}\) \(^{a}\)Universidad Complutense de Madrid \(^{b}\)Instituto Nacional de Estadística  28.1 Introducción a los métodos ensamblados Puede ocurrir...">
<meta name="generator" content="bookdown 0.28 with bs4_book()">
<meta property="og:title" content="Capítulo 28 Métodos ensamblados: bagging y random forest | Fundamentos de ciencia de datos con R">
<meta property="og:type" content="book">
<meta property="og:image" content="/img/cover.png">
<meta property="og:description" content="Ramón A. Carrasco\(^{a}\) e Itzcóatl Bueno\(^{b,a}\) \(^{a}\)Universidad Complutense de Madrid \(^{b}\)Instituto Nacional de Estadística  28.1 Introducción a los métodos ensamblados Puede ocurrir...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Capítulo 28 Métodos ensamblados: bagging y random forest | Fundamentos de ciencia de datos con R">
<meta name="twitter:description" content="Ramón A. Carrasco\(^{a}\) e Itzcóatl Bueno\(^{b,a}\) \(^{a}\)Universidad Complutense de Madrid \(^{b}\)Instituto Nacional de Estadística  28.1 Introducción a los métodos ensamblados Puede ocurrir...">
<meta name="twitter:image" content="/img/cover.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.2/transition.js"></script><script src="libs/bs3compat-0.4.2/tabs.js"></script><script src="libs/bs3compat-0.4.2/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><link href="libs/tabwid-1.1.0/tabwid.css" rel="stylesheet">
<link href="libs/tabwid-1.1.0/scrool.css" rel="stylesheet">
<script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css">
<link rel="stylesheet" href="bs4_book.css">
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Fundamentos de ciencia de datos con R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Buscar" aria-label="Buscar">
</form>

      <nav aria-label="Contenido"><h2>Contenido</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Prefacio</a></li>
<li class="book-part">Ciencia, datos, software… y científicos</li>
<li><a class="" href="ciencia-datos.html"><span class="header-section-number">1</span> ¿Es la ciencia de datos una ciencia?</a></li>
<li><a class="" href="metodologia.html"><span class="header-section-number">2</span> Metodología en ciencia de datos</a></li>
<li><a class="" href="ch-110003.html"><span class="header-section-number">3</span> R para ciencia de datos</a></li>
<li><a class="" href="cap-etica.html"><span class="header-section-number">4</span> Ética en la ciencia de datos</a></li>
<li class="book-part">Bienvenidos a la jungla de datos</li>
<li><a class="" href="datos-sql.html"><span class="header-section-number">5</span> Gestión de bases de datos relacionales</a></li>
<li><a class="" href="cap-nosql.html"><span class="header-section-number">6</span> Gestión de bases de datos NoSQL</a></li>
<li><a class="" href="DGDQM.html"><span class="header-section-number">7</span> Gobierno, gestión y calidad del dato</a></li>
<li><a class="" href="id_130009.html"><span class="header-section-number">8</span> Integración y limpieza de datos</a></li>
<li><a class="" href="chap-feature.html"><span class="header-section-number">9</span> Selección y transformación de variables</a></li>
<li><a class="" href="chap-herramientas.html"><span class="header-section-number">10</span> Herramientas para el análisis en ciencia de datos</a></li>
<li><a class="" href="id_120006-aed.html"><span class="header-section-number">11</span> Análisis exploratorio de datos</a></li>
<li class="book-part">Fundamentos de estadística</li>
<li><a class="" href="Funda-probab.html"><span class="header-section-number">12</span> Probabilidad</a></li>
<li><a class="" href="Fundainfer.html"><span class="header-section-number">13</span> Inferencia estadística</a></li>
<li><a class="" href="muestreo.html"><span class="header-section-number">14</span> Muestreo y remuestreo</a></li>
<li class="book-part">Modelización estadística</li>
<li><a class="" href="cap-lm.html"><span class="header-section-number">15</span> Modelización lineal</a></li>
<li><a class="" href="cap-glm.html"><span class="header-section-number">16</span> Modelos lineales generalizados</a></li>
<li><a class="" href="cap-gam.html"><span class="header-section-number">17</span> Modelos aditivos generalizados</a></li>
<li><a class="" href="cap-mxm.html"><span class="header-section-number">18</span> Modelos mixtos</a></li>
<li><a class="" href="cap-sparse.html"><span class="header-section-number">19</span> Modelos sparse y métodos penalizados de regresión</a></li>
<li><a class="" href="cap-series-temp.html"><span class="header-section-number">20</span> Modelización de series temporales</a></li>
<li><a class="" href="cap-discriminante.html"><span class="header-section-number">21</span> Análisis discriminante</a></li>
<li><a class="" href="cap-conjunto.html"><span class="header-section-number">22</span> Análisis conjunto</a></li>
<li><a class="" href="tablas-contingencia.html"><span class="header-section-number">23</span> Análisis de tablas de contingencia</a></li>
<li class="book-part">Machine learning supervisado</li>
<li><a class="" href="cap-arboles.html"><span class="header-section-number">24</span> Árboles de clasificación y regresión</a></li>
<li><a class="" href="cap-svm.html"><span class="header-section-number">25</span> Máquinas de vector soporte</a></li>
<li><a class="" href="cap-knn.html"><span class="header-section-number">26</span> Clasificador k-vecinos más próximos</a></li>
<li><a class="" href="cap-naive-bayes.html"><span class="header-section-number">27</span> Naive Bayes</a></li>
<li><a class="active" href="cap-bagg-rf.html"><span class="header-section-number">28</span> Métodos ensamblados: bagging y random forest</a></li>
<li><a class="" href="cap-boosting-xgboost.html"><span class="header-section-number">29</span> Boosting y el algoritmo XGBoost</a></li>
<li class="book-part">Machine learning no supervisado</li>
<li><a class="" href="jerarquico.html"><span class="header-section-number">30</span> Análisis cluster: clusterización jerárquica</a></li>
<li><a class="" href="no-jerarquico.html"><span class="header-section-number">31</span> Análisis cluster: clusterización no jerárquica</a></li>
<li><a class="" href="acp.html"><span class="header-section-number">32</span> Análisis de componentes principales</a></li>
<li><a class="" href="an%C3%A1lisis-factorial.html"><span class="header-section-number">33</span> Análisis factorial</a></li>
<li><a class="" href="escalamiento-multidimensional.html"><span class="header-section-number">34</span> Escalamiento multidimensional</a></li>
<li><a class="" href="correspondencias.html"><span class="header-section-number">35</span> Análisis de correspondencias</a></li>
<li class="book-part">Deep learning</li>
<li><a class="" href="capNN.html"><span class="header-section-number">36</span> Redes neuronales artificiales</a></li>
<li><a class="" href="cap-redes-convol.html"><span class="header-section-number">37</span> Redes neuronales convolucionales</a></li>
<li class="book-part">Ciencia de datos de texto y redes</li>
<li><a class="" href="mineria-textos.html"><span class="header-section-number">38</span> Minería de textos</a></li>
<li><a class="" href="grafos.html"><span class="header-section-number">39</span> Análisis de grafos y redes sociales</a></li>
<li class="book-part">Ciencia de datos espaciales</li>
<li><a class="" href="datos-espaciales.html"><span class="header-section-number">40</span> Trabajando con datos espaciales</a></li>
<li><a class="" href="geo.html"><span class="header-section-number">41</span> Geoestadística</a></li>
<li><a class="" href="cap-econom-esp.html"><span class="header-section-number">42</span> Modelos econométricos espaciales</a></li>
<li><a class="" href="cap-pp.html"><span class="header-section-number">43</span> Procesos de puntos</a></li>
<li class="book-part">Comunica y colabora</li>
<li><a class="" href="id_120007-informes.html"><span class="header-section-number">44</span> Informes reproducibles con R Markdown y Quarto</a></li>
<li><a class="" href="shiny.html"><span class="header-section-number">45</span> Creación de aplicaciones web interactivas con Shiny</a></li>
<li><a class="" href="github.html"><span class="header-section-number">46</span> Git y GitHub R</a></li>
<li><a class="" href="geoproces.html"><span class="header-section-number">47</span> Geoprocesamiento en nube</a></li>
<li class="book-part">Casos de estudio en ciencia de datos</li>
<li><a class="" href="cap-crimen.html"><span class="header-section-number">48</span> Análisis de una red criminal</a></li>
<li><a class="" href="cap-publicidad.html"><span class="header-section-number">49</span> Optimización de inversiones publicitarias</a></li>
<li><a class="" href="cap-twitter.html"><span class="header-section-number">50</span> ¿Cómo twitea Elon Musk?</a></li>
<li><a class="" href="cap-periodismo.html"><span class="header-section-number">51</span> Análisis electoral: de Rstudio a su periódico</a></li>
<li><a class="" href="paro-clm.html"><span class="header-section-number">52</span> Crisis: impacto en el paro de Castilla-La Mancha</a></li>
<li><a class="" href="cap-rfm.html"><span class="header-section-number">53</span> Segmentación de clientes en el comerico minorista</a></li>
<li><a class="" href="cap-medicina.html"><span class="header-section-number">54</span> Análisis de datos en medicina</a></li>
<li><a class="" href="cap-futbol.html"><span class="header-section-number">55</span> Messi y Ronaldo: dos ídolos desde la perspectiva de los datos</a></li>
<li><a class="" href="cambioclimatico.html"><span class="header-section-number">56</span> Un dato sobre el cambio climático</a></li>
<li><a class="" href="cap-ree.html"><span class="header-section-number">57</span> Predicción de consumo eléctrico con redes neuronales</a></li>
<li><a class="" href="cap-sist-exp.html"><span class="header-section-number">58</span> Implementación de un sistema experto en el ámbito pediátrico</a></li>
<li><a class="" href="nlp-textil.html"><span class="header-section-number">59</span> El procesamiento del lenguaje natural para tendencias de moda en textil</a></li>
<li><a class="" href="cap-fraude.html"><span class="header-section-number">60</span> Detección de fraude de tarjetas de crédito</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="info-session.html"><span class="header-section-number">A</span> Información de la sesión</a></li>
<li><a class="" href="referncias.html">Referncias</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="cap-bagg-rf" class="section level1" number="28">
<h1>
<span class="header-section-number">Capítulo 28</span> Métodos ensamblados: bagging y random forest<a class="anchor" aria-label="anchor" href="#cap-bagg-rf"><i class="fas fa-link"></i></a>
</h1>
<p><em>Ramón A. Carrasco</em><span class="math inline">\(^{a}\)</span> e <em>Itzcóatl Bueno</em><span class="math inline">\(^{b,a}\)</span></p>
<p><span class="math inline">\(^{a}\)</span>Universidad Complutense de Madrid
<span class="math inline">\(^{b}\)</span>Instituto Nacional de Estadística</p>
<div id="introducción-a-los-métodos-ensamblados" class="section level2" number="28.1">
<h2>
<span class="header-section-number">28.1</span> Introducción a los métodos ensamblados<a class="anchor" aria-label="anchor" href="#introducci%C3%B3n-a-los-m%C3%A9todos-ensamblados"><i class="fas fa-link"></i></a>
</h2>
<p>Puede ocurrir que ninguno de los algoritmos hasta ahora presentados (Caps. <a href="cap-arboles.html#cap-arboles">24</a>, <a href="cap-svm.html#cap-svm">25</a>, <a href="cap-knn.html#cap-knn">26</a> y <a href="cap-naive-bayes.html#cap-naive-bayes">27</a>) proporcionen resultados convincentes para el problema que se quiere modelar. El <em>aprendizaje ensamblado</em> <span class="citation">(<a href="referncias.html#ref-zhou2012ensemble">Zhou 2012</a>)</span> es un paradigma que, como muestra la Fig. <a href="cap-bagg-rf.html#fig:metamodel">28.1</a>, en lugar de entrenar un modelo muy preciso, se centra en entrenar un gran número de modelos con menor precisión, y después combinar sus predicciones para obtener un metamodelo de una precisión más alta.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:metamodel"></span>
<img src="img/metamodelo.png" alt="Esquema de un metamodelo." width="70%"><p class="caption">
Figura 28.1: Esquema de un metamodelo.
</p>
</div>
<p>A los modelos de menor precisión se les suele nombrar como algoritmos “débiles”, es decir, algoritmos con menor capacidad de aprender patrones complejos en los datos.Por tanto, generalmente, son rápidos tanto en tiempo de entrenamiento como de procesamiento. Existen dos paradigmas de aprendizaje ensamblado: el <em>bagging</em> y el <em>boosting</em> (Cap. <a href="cap-boosting-xgboost.html#cap-boosting-xgboost">29</a>).</p>
</div>
<div id="bagging" class="section level2" number="28.2">
<h2>
<span class="header-section-number">28.2</span> Bagging<a class="anchor" aria-label="anchor" href="#bagging"><i class="fas fa-link"></i></a>
</h2>
<p>En lugar de buscar la división más eficiente en cada capa, como ocurre en el árbol de decisión, una alternativa sería construir un metamodelo combinando los resultados de múltiples árboles de decisión. Esta técnica se conoce como <em>bagging</em> y consiste en construir varios árboles utilizando una selección aleatoria de los datos que se utilizan para cada árbol y, finalmente, combinar la predicción de cada uno de ellos a través de la media (en el caso de regresión) o mediante un sistema de votación (en el caso de un problema de clasificación).</p>
<p>La principal característica del <em>bagging</em> es el llamado muestreo bootstrap. La idea básica del bootstrap es que la inferencia sobre una población se haga a partir de una muestra, tomando el papel de población y se remuestree, permitiendo comparar valor poblacional y el valor muestral. En el <em>bagging</em>, el objetivo de este remuestreo es que cada árbol esté entrenado con una muestra única, y por tanto, generen respuestas únicas,esto es modelos débiles distintos. Para ello, debe existir aleatoriedad y variación en cada árbol que conforme el modelo final, puesto que no tendría sentido construir varios árboles idénticos. Como se ha comentado, este problema queda resuelto por el muestreo bootstrap, el cual extrae una muestra aleatoria de los datos en cada ronda. En el caso del <em>bagging</em>, se extraen distintas muestras de datos para el entrenamiento de cada árbol. Aunque esto no elimina la problemática del sobreajuste, los patrones presentes en el conjunto de datos aparecerán en la mayoría de los árboles entrenados y, por tanto, en la predicción final. Es por ello que el <em>bagging</em> es una técnica de gran eficacia para el tratamiento de los valores atípicos y para la reducción de la varianza que generalmente afecta a un modelo compuesto por un único árbol de decisión.</p>
<div id="rbagging" class="section level3" number="28.2.1">
<h3>
<span class="header-section-number">28.2.1</span> Procedimiento con R: la función <code>bagging()</code><a class="anchor" aria-label="anchor" href="#rbagging"><i class="fas fa-link"></i></a>
</h3>
<p>En el paquete <code>ipred</code> de <strong>R</strong> se encuentra la función <code><a href="https://rdrr.io/pkg/ipred/man/bagging.html">bagging()</a></code> que se utiliza para entrenar un modelo <em>bagging</em>:</p>
<div class="sourceCode" id="cb382"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">bagging</span><span class="op">(</span><span class="va">formula</span>, <span class="va">data</span>, <span class="va">...</span><span class="op">)</span></span></code></pre></div>
<ul>
<li>
<code>formula</code>: Refleja la relación lineal entre la variable dependiente y los predictores <span class="math inline">\(Y \sim X_1 + ... + X_p\)</span>.</li>
<li>
<code>data</code>: Conjunto de datos con el que se entrena el modelo.</li>
<li>
<code>nbagg</code>: Número de replicaciones bootstrap.</li>
<li>
<code>coob</code>: Indica si se debe calcular una estimación del ratio de error de predicción.</li>
</ul>
</div>
<div id="implementando-bagging-en-r" class="section level3" number="28.2.2">
<h3>
<span class="header-section-number">28.2.2</span> Implementando <em>bagging</em> en R<a class="anchor" aria-label="anchor" href="#implementando-bagging-en-r"><i class="fas fa-link"></i></a>
</h3>
<p>Es posible la implementación de un modelo de predicción de agregación bootstrap en R. Para ello, se pueden utilizar múltiples funciones como la ya mencionada en la Sec. <a href="cap-bagg-rf.html#rbagging">28.2.1</a> <code><a href="https://rdrr.io/pkg/ipred/man/bagging.html">bagging()</a></code>. En este ejemplo se utilizan los datos sobre compras de clientes <code>dp_entr</code> del paquete <code>CDR</code>, cuyo objetivo es clasificar a los clientes entre quienes comprarían un nuevo producto y quienes no.</p>
<div class="sourceCode" id="cb383"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"CDR"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"ipred"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/topepo/caret/">"caret"</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="http://had.co.nz/reshape">"reshape"</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://ggplot2.tidyverse.org">"ggplot2"</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"dp_entr"</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb384"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># se fija la semilla aleatoria</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">101</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Se entrena el modelo</span></span>
<span><span class="va">bag_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/ipred/man/bagging.html">bagging</a></span><span class="op">(</span></span>
<span>  formula <span class="op">=</span> <span class="va">CLS_PRO_pro13</span> <span class="op">~</span> <span class="va">.</span>,</span>
<span>  data <span class="op">=</span> <span class="va">dp_entr</span>,</span>
<span>  nbagg <span class="op">=</span> <span class="fl">100</span>,  </span>
<span>  coob <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  control <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/rpart.control.html">rpart.control</a></span><span class="op">(</span>minsplit <span class="op">=</span> <span class="fl">2</span>, cp <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb385-1"><a href="cap-bagg-rf.html#cb385-1" tabindex="-1"></a>bag_model</span>
<span id="cb385-2"><a href="cap-bagg-rf.html#cb385-2" tabindex="-1"></a></span>
<span id="cb385-3"><a href="cap-bagg-rf.html#cb385-3" tabindex="-1"></a>Bagging classification trees with <span class="dv">100</span> bootstrap replications </span>
<span id="cb385-4"><a href="cap-bagg-rf.html#cb385-4" tabindex="-1"></a></span>
<span id="cb385-5"><a href="cap-bagg-rf.html#cb385-5" tabindex="-1"></a>Call<span class="sc">:</span> <span class="fu">bagging.data.frame</span>(<span class="at">formula =</span> CLS_PRO_pro13 <span class="sc">~</span> ., <span class="at">data =</span> dp_entr, </span>
<span id="cb385-6"><a href="cap-bagg-rf.html#cb385-6" tabindex="-1"></a>    <span class="at">nbagg =</span> <span class="dv">100</span>, <span class="at">coob =</span> <span class="cn">TRUE</span>, <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">minsplit =</span> <span class="dv">2</span>, </span>
<span id="cb385-7"><a href="cap-bagg-rf.html#cb385-7" tabindex="-1"></a>        <span class="at">cp =</span> <span class="dv">0</span>))</span>
<span id="cb385-8"><a href="cap-bagg-rf.html#cb385-8" tabindex="-1"></a></span>
<span id="cb385-9"><a href="cap-bagg-rf.html#cb385-9" tabindex="-1"></a>Out<span class="sc">-</span>of<span class="sc">-</span>bag estimate of misclassification error<span class="sc">:</span>  <span class="fl">0.1416</span> </span></code></pre></div>
<p>El error de clasificación de este modelo es del 14,16%, o lo que es equivalente, el modelo tiene una precisión del 85,84%. Desafortunadamente, <code><a href="https://rdrr.io/pkg/ipred/man/bagging.html">bagging()</a></code> no selecciona el número óptimo de replicaciones reduciendo el error de clasificación. Para seleccionar el número de replicaciones que minimice el error, se puede graficar la curva de error por número de replicaciones como en la Fig. <a href="cap-bagg-rf.html#fig:bagg-plot">28.2</a>. Se itera el modelo variando los valores del hiperparámetro <code>nbagg</code> (en este ejemplo entre 10 y 150, incrementándose de cinco en cinco). Se observa que el error mínimo (13,79%) se obtiene al establecer el hiperparámetro igual a 60.</p>
<div class="sourceCode" id="cb386"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">missclass</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">)</span> <span class="co"># vector vacio para recopilar el error en cada iteración</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">n</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">10</span>,<span class="fl">150</span>,<span class="fl">5</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span> <span class="co"># valores a probar para nbagg</span></span>
<span>  <span class="co"># se establece la semilla aleatoria</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">101</span><span class="op">)</span></span>
<span>  <span class="co"># se entrena el modelo</span></span>
<span>  <span class="va">bag_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/ipred/man/bagging.html">bagging</a></span><span class="op">(</span></span>
<span>  formula <span class="op">=</span> <span class="va">CLS_PRO_pro13</span> <span class="op">~</span> <span class="va">.</span>,</span>
<span>  data <span class="op">=</span> <span class="va">dp_entr</span>,</span>
<span>  nbagg <span class="op">=</span> <span class="va">n</span>,  </span>
<span>  coob <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  control <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/rpart.control.html">rpart.control</a></span><span class="op">(</span>minsplit <span class="op">=</span> <span class="fl">2</span>, cp <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span>  <span class="co"># se agrega el error de esta iteración</span></span>
<span>  <span class="va">missclass</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">missclass</span>, <span class="va">bag_model</span><span class="op">$</span><span class="va">err</span><span class="op">)</span> <span class="co"># se agrega el error de esta iteración</span></span>
<span><span class="op">}</span></span></code></pre></div>
<div class="sourceCode" id="cb387"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">10</span>,<span class="fl">150</span>,<span class="fl">5</span><span class="op">)</span>,<span class="va">missclass</span>,type <span class="op">=</span> <span class="st">"l"</span>,xlab <span class="op">=</span> <span class="st">"Número de árboles"</span>, ylab<span class="op">=</span><span class="st">"Missclassification error"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:bagg-plot"></span>
<img src="img/bagging_missclass.png" alt="Número de replicaciones vs Error de clasificación." width="60%"><p class="caption">
Figura 28.2: Número de replicaciones vs Error de clasificación.
</p>
</div>
<p>La función <code><a href="https://rdrr.io/pkg/caret/man/train.html">train()</a></code> del paquete <code>caret</code> es otro método para entrenar un algoritmo de <em>bagging</em> en R. Para ello, el argumento <code>method</code> debe tomar el valor <code>"treebag"</code>. Sin embargo, este algoritmo no incluye hiperparámetros a optimizar. Dado que se ha obtenido recursivamente el número óptimo de replicaciones, se puede entrenar el modelo con el valor obtenido y comprobar que el error dado coincide. Se observa que si se entrena un modelo <em>bagging</em> con 60 replicaciones, la precisión del modelo es del 86,93%. Esto es aproximadamente el resultado obtenido anteriormente en el que para 60 replicaciones el modelo tenía un error de clasificación del 13,79%.</p>
<div class="sourceCode" id="cb388"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">101</span><span class="op">)</span></span>
<span><span class="va">model_bag</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/train.html">train</a></span><span class="op">(</span></span>
<span>  <span class="va">CLS_PRO_pro13</span> <span class="op">~</span> <span class="va">.</span>,</span>
<span>  data <span class="op">=</span> <span class="va">dp_entr</span>,</span>
<span>  method <span class="op">=</span> <span class="st">"treebag"</span>,</span>
<span>  trControl <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/trainControl.html">trainControl</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"cv"</span>, number <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>,</span>
<span>  nbagg <span class="op">=</span> <span class="fl">60</span>, </span>
<span>  control <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/rpart.control.html">rpart.control</a></span><span class="op">(</span>minsplit <span class="op">=</span> <span class="fl">2</span>, cp <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb389"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb389-1"><a href="cap-bagg-rf.html#cb389-1" tabindex="-1"></a>model_bag</span>
<span id="cb389-2"><a href="cap-bagg-rf.html#cb389-2" tabindex="-1"></a></span>
<span id="cb389-3"><a href="cap-bagg-rf.html#cb389-3" tabindex="-1"></a>Bagged CART </span>
<span id="cb389-4"><a href="cap-bagg-rf.html#cb389-4" tabindex="-1"></a></span>
<span id="cb389-5"><a href="cap-bagg-rf.html#cb389-5" tabindex="-1"></a><span class="dv">558</span> samples</span>
<span id="cb389-6"><a href="cap-bagg-rf.html#cb389-6" tabindex="-1"></a> <span class="dv">17</span> predictor</span>
<span id="cb389-7"><a href="cap-bagg-rf.html#cb389-7" tabindex="-1"></a>  <span class="dv">2</span> classes<span class="sc">:</span> <span class="st">'S'</span>, <span class="st">'N'</span> </span>
<span id="cb389-8"><a href="cap-bagg-rf.html#cb389-8" tabindex="-1"></a></span>
<span id="cb389-9"><a href="cap-bagg-rf.html#cb389-9" tabindex="-1"></a>No pre<span class="sc">-</span>processing</span>
<span id="cb389-10"><a href="cap-bagg-rf.html#cb389-10" tabindex="-1"></a>Resampling<span class="sc">:</span> Cross<span class="sc">-</span><span class="fu">Validated</span> (<span class="dv">10</span> fold) </span>
<span id="cb389-11"><a href="cap-bagg-rf.html#cb389-11" tabindex="-1"></a>Summary of sample sizes<span class="sc">:</span> <span class="dv">502</span>, <span class="dv">502</span>, <span class="dv">502</span>, <span class="dv">503</span>, <span class="dv">503</span>, <span class="dv">502</span>, ... </span>
<span id="cb389-12"><a href="cap-bagg-rf.html#cb389-12" tabindex="-1"></a>Resampling results<span class="sc">:</span></span>
<span id="cb389-13"><a href="cap-bagg-rf.html#cb389-13" tabindex="-1"></a></span>
<span id="cb389-14"><a href="cap-bagg-rf.html#cb389-14" tabindex="-1"></a>  Accuracy   Kappa    </span>
<span id="cb389-15"><a href="cap-bagg-rf.html#cb389-15" tabindex="-1"></a>  <span class="fl">0.8692532</span>  <span class="fl">0.7385449</span></span></code></pre></div>
</div>
<div id="interpretación-de-variables-en-el-bagging" class="section level3" number="28.2.3">
<h3>
<span class="header-section-number">28.2.3</span> Interpretación de variables en el <em>bagging</em><a class="anchor" aria-label="anchor" href="#interpretaci%C3%B3n-de-variables-en-el-bagging"><i class="fas fa-link"></i></a>
</h3>
<p>Una de las principales desventajas de los algoritmos ensamblados (incluido el <em>bagging</em>) es que mientras que los modelos base son interpretables, el metamodelo resultante no lo es. Pese a esto, aún es posible hacer inferencia de cómo cada una de las variables influye en el modelo entrenado. La manera de medir la importancia de las variables incluidas en un árbol es registrar para cada variable la reducción de la función de pérdida que se le atribuye en cada partición. Dado que una variable puede utilizarse varias veces para dividir el árbol, la importancia total de esa variable será la suma de la reducción de la función de pérdida que se le atribuya por todas las particiones en las que intervenga. Este proceso es similar para el <em>bagging</em>. En este caso, para cada árbol se calcula la reducción de la función de pérdida en todas las divisiones. Tras esto, se agrega esta medida en todos los árboles que forman el metamodelo. El paquete <code>ipred</code>, en el que se encuentra la función <code><a href="https://rdrr.io/pkg/ipred/man/bagging.html">bagging()</a></code>, no captura la información requerida para calcular la importancia de las variables. Sin embargo, el paquete <code>caret</code> si lo hace y se puede construir un gráfico de importancia utilizando la función <code><a href="https://rdrr.io/pkg/vip/man/vip.html">vip()</a></code> del paquete <code>vip</code>.</p>
<div class="sourceCode" id="cb390"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/koalaverse/vip/">"vip"</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/vip/man/vip.html">vip</a></span><span class="op">(</span><span class="va">model_bag</span>, num_features <span class="op">=</span> <span class="fl">15</span>,</span>
<span>    aesthetics <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>color <span class="op">=</span> <span class="st">"skyblue"</span>, fill <span class="op">=</span> <span class="st">"skyblue"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:BAGGINGVIP"></span>
<img src="img/model_bag_imp.png" alt="Importancia de las variables incluidas en el modelo bagging." width="60%"><p class="caption">
Figura 28.3: Importancia de las variables incluidas en el modelo bagging.
</p>
</div>
<p>La Fig. <a href="cap-bagg-rf.html#fig:BAGGINGVIP">28.3</a> muestra que las variables más importantes en el modelo <em>bagging</em> entrenado para predecir si un cliente comprará o no el <em>tensiómetro digital</em> son: si ha comprado la <em>depiladora eléctrica</em>, cuánto importe ha gastado en ese producto, si ha comprado el <em>estimulador muscular</em> y si ha comprado el <em>smartchwatch fitness</em>.</p>
</div>
</div>
<div id="random-forest" class="section level2" number="28.3">
<h2>
<span class="header-section-number">28.3</span> Random Forest<a class="anchor" aria-label="anchor" href="#random-forest"><i class="fas fa-link"></i></a>
</h2>
<p>El <em>bagging</em> es el paradigma tras el algoritmo de <em>random forest</em>. Este algoritmo fue desarrollado por primera vez por <span class="citation">(<a href="referncias.html#ref-ho1995random">T. K. Ho 1995</a>)</span>. Sin embargo, fueron <span class="citation">(<a href="referncias.html#ref-cutler1999fast">Cutler and Zhao 1999</a>)</span> y <span class="citation">(<a href="referncias.html#ref-breiman2001random">Breiman 2001</a>)</span> quienes desarrollaron una versión extendida del modelo y registraron <strong>Random Forest</strong> como marca comercial. Este algoritmo básico de <em>bagging</em> funciona del siguiente modo: a partir del conjunto de datos de entrenamiento se generan <span class="math inline">\(K\)</span> muestras aleatorias <span class="math inline">\(\mathbb{S}_{k}\)</span>, se entrena un modelo de árbol de decisión (<span class="math inline">\(f_k\)</span>) utilizando la muestra <span class="math inline">\(\mathbb{S}_{k}\)</span> como conjunto de entrenamiento. Tras el entrenamiento, se dispone de <span class="math inline">\(K\)</span> árboles de decisión, como se observa en la Fig. <a href="cap-bagg-rf.html#fig:ejemplo-rf">28.4</a>. La predicción de una nueva observación <span class="math inline">\(x\)</span> se obtiene como la media de las <span class="math inline">\(K\)</span> predicciones:</p>
<p><span class="math display">\[\begin{equation}
y\leftarrow\hat{f}(x)=\frac{1}{K}\sum^{K}_{k=1}f_{k}(x)
\end{equation}\]</span></p>
<p>En el caso de regresión, o por la votación por mayoría en el caso de clasificación.</p>
<p>Tanto el <em>bagging</em> como el <em>random forest</em> desarrollan múltiples árboles y utilizan el muestreo bootstrap para la aleatorización de los datos. Sin embargo, el <em>random forest</em> establece una limitación artificial a la selección de variables al no considerar todas en cada árbol.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:ejemplo-rf"></span>
<img src="img/randomforest.png" alt="Ejemplo de Random Forest." width="60%"><p class="caption">
Figura 28.4: Ejemplo de Random Forest.
</p>
</div>
<p>El <em>bagging</em> considera las mismas variables para construir cada árbol con el objetivo de minimizar su entropía, y, por tanto, todos los árboles suelen tener un aspecto similar. Esto lleva a que las predicciones dadas por los árboles estén altamente correlacionadas. El modelo <em>random forest</em> evita este problema estableciendo la obligación, en cada división, de utilizar un subconjunto de las variables. Esto proporciona a algunas variables mayor probabilidad de ser seleccionadas, y al generar árboles únicos y no correlacionados se consigue una estructura de decisión final más fiable.</p>
<p>En general, es mejor que el <em>random forest</em> esté formado por una gran cantidad de árboles (por lo menos 100) para suavizar el impacto de valores atípicos. Sin embargo, la tasa de efectividad disminuye a medida que se incorporan más árboles. Llegado a cierto punto, los nuevos árboles no aportan una mejora significativa al modelo, pero si incrementan los tiempos de procesamiento.</p>
<p>El modelo <em>random forest</em> es rápido de entrenar y es una buena técnica para obtener un modelo de referencia. Aunque estos modelos funcionan bien en la interpretación de patrones complejos y son versátiles, otras técnicas, como por ejemplo el <em>gradient boosting</em> (Cap. <a href="cap-boosting-xgboost.html#cap-boosting-xgboost">29</a>), proporcionan una mayor precisión en las predicciones en muchos casos.</p>
<p>Estos modelos se han vuelto populares porque tienden a proporcionar un muy buen rendimiento con los parámetros predeterminados en las distintas implementaciones. En efecto, a pesar de tener muchos hiperparámetros que pueden ser ajustados, los valores por defecto de dichos hiperparámetros tienden a ofrecer buenos resultados en la predicción. Los hiperparámetros más importantes que hay que ajustar al entrenar un modelo <em>random forest</em> son: el número de árboles (<span class="math inline">\(K\)</span>), el número de variables incluidos en el subconjunto aleatorio en cada división (<code>mtry</code>), la complejidad de cada árbol, el esquema de muestreo y la regla de división a utilizar durante la construcción del árbol.</p>
<div id="número-de-árboles-k" class="section level3" number="28.3.1">
<h3>
<span class="header-section-number">28.3.1</span> Número de árboles (<span class="math inline">\(K\)</span>)<a class="anchor" aria-label="anchor" href="#n%C3%BAmero-de-%C3%A1rboles-k"><i class="fas fa-link"></i></a>
</h3>
<p>El primer hiperparámetro a ajustar es el número de árboles que componen el modelo de <em>random forest</em>. Su valor debe ser lo suficientemente grande como para que la tasa de error se estabilice. La regla general es que el valor mínimo de árboles sea igual a 10 veces el número de variables incluidas en el modelo. Sin embargo, cuando se tienen en cuenta otros hiperparámetros para optimizar, es posible que el número de árboles se vea afectado. El tiempo de procesamiento aumenta linealmente con la cantidad de árboles incluidos, pero cuantos más se incluyan, se obtendrán estimaciones de error más estables.</p>
</div>
<div id="número-de-variables-a-considerar-mtry" class="section level3" number="28.3.2">
<h3>
<span class="header-section-number">28.3.2</span> Número de variables a considerar (<code>mtry</code>)<a class="anchor" aria-label="anchor" href="#n%C3%BAmero-de-variables-a-considerar-mtry"><i class="fas fa-link"></i></a>
</h3>
<p><code>mtry</code> se refiere al hiperparámetro encargado de controlar la aleatorización de variables utilizadas para las particiones de los árboles. Este hiperparámetro ayuda a equilibrar la baja correlación del árbol con los demás, y una razonable fuerza predictiva. Existe un valor predeterminado para este hiperparámetro el cual se puede utilizar en caso de no querer o no poder ajustarlo. En el caso de la regresión, se determina que <span class="math inline">\(mtry=\frac{p}{3}\)</span> siendo <span class="math inline">\(p\)</span> el número de variables incluidas en el modelo. Y en problemas de clasificación, el valor predeterminado es <span class="math inline">\(mtry=\sqrt p\)</span>. Cuando hay pocas variables relevantes, es decir, los datos son muy ruidosos, tiende a funcionar mejor que el valor de <code>mtry</code> sea alto, pues hace que sea más probable seleccionar esas variables. En cambio, cuando muchas variables son importantes, funciona mejor un valor bajo de <code>mtry</code>.</p>
</div>
<div id="complejidad-de-los-árboles" class="section level3" number="28.3.3">
<h3>
<span class="header-section-number">28.3.3</span> Complejidad de los árboles<a class="anchor" aria-label="anchor" href="#complejidad-de-los-%C3%A1rboles"><i class="fas fa-link"></i></a>
</h3>
<p>Un modelo <em>random forest</em> se construye con árboles de decisión a los que se les puede controlar su profundidad y su complejidad como se vio en el Cap. <a href="cap-arboles.html#cap-arboles">24</a>. Esto se puede hacer ajustando los hiperparámetros de profundidad máxima permitida, tamaño del nodo o la cantidad máxima de nodos terminales.</p>
<p>El tamaño del nodo es el hiperparámetro más común para controlar la complejidad del árbol y la mayoría de las implementaciones usan los valores predeterminados de 1 para árboles de clasificación y 5 para los árboles de regresión, dado que estos valores tienden a producir buenos resultados. Si se quiere controlar el tiempo de procesamiento, se pueden conseguir reducciones significativas del tiempo aumentando el tamaño del nodo impactando de manera marginal en la estimación del error.</p>
</div>
<div id="esquema-de-muestreo" class="section level3" number="28.3.4">
<h3>
<span class="header-section-number">28.3.4</span> Esquema de muestreo<a class="anchor" aria-label="anchor" href="#esquema-de-muestreo"><i class="fas fa-link"></i></a>
</h3>
<p>Por defecto, el <em>random forest</em> tiene como esquema de muestreo el bootstrapping, explicado anteriormente, en el cual todas las observaciones se muestrean con reemplazo. Todas las replicaciones de bootstrap tienen el mismo tamaño que el conjunto de datos de entrenamiento. Sin embargo, el esquema de muestreo se puede ajustar tanto en el tamaño de la muestra como en el diseño muestral (con o sin reposición). El hiperparámetro de tamaño de muestra determina cuántas observaciones se extraen para el entrenamiento de cada árbol. Cuanto menor sea el tamaño muestral, menor será la correlación entre los árboles, lo cual puede llevar a mejores resultados de precisión en la predicción. La forma de determinar el tamaño muestral óptimo puede hallarse evaluando algunos valores que oscilen entre el 25% y el 100%, y en el caso de que haya variables no balanceadas respecto a los valores de las categóricas se puede intentar muestrear sin reposición.</p>
</div>
<div id="regla-de-división" class="section level3" number="28.3.5">
<h3>
<span class="header-section-number">28.3.5</span> Regla de división<a class="anchor" aria-label="anchor" href="#regla-de-divisi%C3%B3n"><i class="fas fa-link"></i></a>
</h3>
<p>Por defecto, la regla de división que utilizan los árboles de decisión que conforman un <em>random forest</em> es la que se presentó en el Cap. <a href="cap-arboles.html#cap-arboles">24</a>. Esto es, en el caso de regresión seleccionar la división que minimiza la desviación típica <span class="math inline">\((\sigma)\)</span>; y en el caso de clasificación la división que minimiza la impureza de Gini o la entropía.</p>
</div>
<div id="procedimiento-con-r-la-función-randomforest" class="section level3" number="28.3.6">
<h3>
<span class="header-section-number">28.3.6</span> Procedimiento con R: la función <code>randomForest()</code><a class="anchor" aria-label="anchor" href="#procedimiento-con-r-la-funci%C3%B3n-randomforest"><i class="fas fa-link"></i></a>
</h3>
<p>En el paquete <code>randomForest</code> de <strong>R</strong> se encuentra la función <code><a href="https://rdrr.io/pkg/randomForest/man/randomForest.html">randomForest()</a></code> que se utiliza para entrenar un modelo de este tipo:</p>
<div class="sourceCode" id="cb391"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/randomForest/man/randomForest.html">randomForest</a></span><span class="op">(</span><span class="va">formula</span>, data<span class="op">=</span><span class="va">...</span>, <span class="va">...</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/randomForest/man/randomForest.html">randomForest</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, <span class="va">xtest</span>, <span class="va">ytest</span>, ntree<span class="op">=</span><span class="fl">500</span>, <span class="va">mtry</span>, <span class="va">...</span><span class="op">)</span></span></code></pre></div>
<ul>
<li>
<code>formula</code>: Refleja la relación entre la variable dependiente <span class="math inline">\(Y\)</span> y los predictores tal que <span class="math inline">\(Y \sim X_1 + ... + X_p\)</span>.</li>
<li>
<code>data</code>: Conjunto de datos con el que entrenar el árbol de acuerdo a la fórmula indicada.</li>
<li>
<code>x</code>: Conjunto de datos de entrenamiento que contiene los predictores</li>
<li>
<code>y</code>: Vector respuesta con las clases o valores de la variable respuesta.</li>
<li>
<code>xtest</code>: Conjunto de datos que contiene los predictores del conjunto de datos de validación.</li>
<li>
<code>ytest</code>: Variable respuesta del conjunto de datos de validación.</li>
<li>
<code>ntree</code>: Número de árboles a construir en el modelo.</li>
<li>
<code>mtry</code>: Número de variables muestreadas aleatoriamente como candidatas en cada partición.</li>
</ul>
</div>
<div id="aplicación-del-modelo-random-forest-en-r" class="section level3" number="28.3.7">
<h3>
<span class="header-section-number">28.3.7</span> Aplicación del modelo <em>random forest</em> en <strong>R</strong><a class="anchor" aria-label="anchor" href="#aplicaci%C3%B3n-del-modelo-random-forest-en-r"><i class="fas fa-link"></i></a>
</h3>
<p>En esta sección se aplica el modelo <em>random forest</em> al ejemplo de datos de retail incluido en el paquete <code>CDR</code>. Se carga el paquete y con ello, los datos <code>dp_entr</code>. Se busca predecir si un cliente va a comprar o no el nuevo producto de acuerdo a los productos que ha consumido, el importe que gasta en ellos y otras características como, por ejemplo, su nivel educativo.</p>
<div class="sourceCode" id="cb392"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"CDR"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/">"randomForest"</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/topepo/caret/">"caret"</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="http://had.co.nz/reshape">"reshape"</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://ggplot2.tidyverse.org">"ggplot2"</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">dp_entr</span><span class="op">)</span></span></code></pre></div>
<p>Este algoritmo al estar basado en árboles de clasificación tiene los mismos requisitos para el entrenamiento que tenían dichos árboles, así se construye el modelo usando el conjunto de datos de entrenamiento.</p>
<div class="sourceCode" id="cb393"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># se fija la semilla aleatoria</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">101</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># se entrena el modelo</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/train.html">train</a></span><span class="op">(</span><span class="va">CLS_PRO_pro13</span><span class="op">~</span><span class="va">.</span>, data<span class="op">=</span><span class="va">dp_entr_NUM</span>, </span>
<span>             method<span class="op">=</span><span class="st">"rf"</span>, metric<span class="op">=</span><span class="st">"Accuracy"</span>, ntree<span class="op">=</span><span class="fl">500</span>,</span>
<span>             trControl<span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/caret/man/trainControl.html">trainControl</a></span><span class="op">(</span>method<span class="op">=</span><span class="st">"cv"</span>, </span>
<span>                                    number<span class="op">=</span><span class="fl">10</span>, </span>
<span>                                    classProbs <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb394"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb394-1"><a href="cap-bagg-rf.html#cb394-1" tabindex="-1"></a>model</span>
<span id="cb394-2"><a href="cap-bagg-rf.html#cb394-2" tabindex="-1"></a></span>
<span id="cb394-3"><a href="cap-bagg-rf.html#cb394-3" tabindex="-1"></a>Random Forest </span>
<span id="cb394-4"><a href="cap-bagg-rf.html#cb394-4" tabindex="-1"></a></span>
<span id="cb394-5"><a href="cap-bagg-rf.html#cb394-5" tabindex="-1"></a><span class="dv">558</span> samples</span>
<span id="cb394-6"><a href="cap-bagg-rf.html#cb394-6" tabindex="-1"></a> <span class="dv">19</span> predictor</span>
<span id="cb394-7"><a href="cap-bagg-rf.html#cb394-7" tabindex="-1"></a>  <span class="dv">2</span> classes<span class="sc">:</span> <span class="st">'S'</span>, <span class="st">'N'</span> </span>
<span id="cb394-8"><a href="cap-bagg-rf.html#cb394-8" tabindex="-1"></a></span>
<span id="cb394-9"><a href="cap-bagg-rf.html#cb394-9" tabindex="-1"></a>No pre<span class="sc">-</span>processing</span>
<span id="cb394-10"><a href="cap-bagg-rf.html#cb394-10" tabindex="-1"></a>Resampling<span class="sc">:</span> Cross<span class="sc">-</span><span class="fu">Validated</span> (<span class="dv">10</span> fold) </span>
<span id="cb394-11"><a href="cap-bagg-rf.html#cb394-11" tabindex="-1"></a>Summary of sample sizes<span class="sc">:</span> <span class="dv">502</span>, <span class="dv">502</span>, <span class="dv">502</span>, <span class="dv">503</span>, <span class="dv">503</span>, <span class="dv">502</span>, ... </span>
<span id="cb394-12"><a href="cap-bagg-rf.html#cb394-12" tabindex="-1"></a>Resampling results across tuning parameters<span class="sc">:</span></span>
<span id="cb394-13"><a href="cap-bagg-rf.html#cb394-13" tabindex="-1"></a></span>
<span id="cb394-14"><a href="cap-bagg-rf.html#cb394-14" tabindex="-1"></a>  mtry  Accuracy   Kappa    </span>
<span id="cb394-15"><a href="cap-bagg-rf.html#cb394-15" tabindex="-1"></a>   <span class="dv">2</span>    <span class="fl">0.8602922</span>  <span class="fl">0.7206238</span></span>
<span id="cb394-16"><a href="cap-bagg-rf.html#cb394-16" tabindex="-1"></a>  <span class="dv">10</span>    <span class="fl">0.8620455</span>  <span class="fl">0.7241029</span></span>
<span id="cb394-17"><a href="cap-bagg-rf.html#cb394-17" tabindex="-1"></a>  <span class="dv">19</span>    <span class="fl">0.8620130</span>  <span class="fl">0.7240248</span></span>
<span id="cb394-18"><a href="cap-bagg-rf.html#cb394-18" tabindex="-1"></a></span>
<span id="cb394-19"><a href="cap-bagg-rf.html#cb394-19" tabindex="-1"></a>Accuracy was used to select the optimal model using the largest value.</span>
<span id="cb394-20"><a href="cap-bagg-rf.html#cb394-20" tabindex="-1"></a>The final value used <span class="cf">for</span> the model was mtry <span class="ot">=</span> <span class="fl">10.</span></span></code></pre></div>
<p>Los resultados de la validación cruzada se pueden ver en el siguiente boxplot. Se observa como la precisión oscila entre el 80% y el 95%. Además, se puede ver en el resultado del modelo que el hiperparámetro <span class="math inline">\(mtry\)</span> se ha ajustado a 10 variables.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:RFRESULTS"></span>
<img src="img/rfboxplot.png" alt="Resultados del modelo random forest durante el proceso de validación cruzada." width="60%"><p class="caption">
Figura 28.5: Resultados del modelo random forest durante el proceso de validación cruzada.
</p>
</div>
<p>Finalmente, aunque el <em>random forest</em> generado está compuesto por 500 árboles, se puede acceder a cualquiera de ellos para estudiarlos en profundidad. Para ello, es necesario instalar el paquete <code>reprtree</code> desde el repositorio <a href="https://github.com/araastat/reprtree" class="uri">https://github.com/araastat/reprtree</a>.</p>
<div class="sourceCode" id="cb395"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://devtools.r-lib.org/">"devtools"</a></span><span class="op">)</span></span>
<span><span class="kw">if</span><span class="op">(</span><span class="op">!</span><span class="op">(</span><span class="st">'reprtree'</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/utils/installed.packages.html">installed.packages</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="fu">devtools</span><span class="fu">::</span><span class="fu"><a href="https://remotes.r-lib.org/reference/install_github.html">install_github</a></span><span class="op">(</span><span class="st">'araastat/reprtree'</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Se pueden observar las decisiones que se toman en el árbol de forma tabulada, indicando qué variable se utiliza para la partición, cuál es el valor que decide la división, indicando si es un nodo terminal (<code>-1</code>) o no (<code>1</code>) y la predicción del nodo, el cual es <code>NA</code> si no es un nodo terminal.</p>
<div class="sourceCode" id="cb396"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb396-1"><a href="cap-bagg-rf.html#cb396-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb396-2"><a href="cap-bagg-rf.html#cb396-2" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(CLS_PRO_pro13<span class="sc">~</span>., <span class="at">data =</span> dp_entr_NUM, <span class="at">ntree=</span><span class="dv">500</span>,</span>
<span id="cb396-3"><a href="cap-bagg-rf.html#cb396-3" tabindex="-1"></a>                   <span class="at">mtry=</span><span class="fu">unlist</span>(model<span class="sc">$</span>bestTune))</span>
<span id="cb396-4"><a href="cap-bagg-rf.html#cb396-4" tabindex="-1"></a></span>
<span id="cb396-5"><a href="cap-bagg-rf.html#cb396-5" tabindex="-1"></a><span class="co"># se observa el árbol número 205</span></span>
<span id="cb396-6"><a href="cap-bagg-rf.html#cb396-6" tabindex="-1"></a>tree205 <span class="ot">&lt;-</span> <span class="fu">getTree</span>(rf, <span class="dv">205</span>, <span class="at">labelVar=</span><span class="cn">TRUE</span>)</span>
<span id="cb396-7"><a href="cap-bagg-rf.html#cb396-7" tabindex="-1"></a></span>
<span id="cb396-8"><a href="cap-bagg-rf.html#cb396-8" tabindex="-1"></a><span class="fu">head</span>(tree205[,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)])</span>
<span id="cb396-9"><a href="cap-bagg-rf.html#cb396-9" tabindex="-1"></a>      split var split point status prediction</span>
<span id="cb396-10"><a href="cap-bagg-rf.html#cb396-10" tabindex="-1"></a><span class="dv">1</span> importe_pro15         <span class="dv">100</span>      <span class="dv">1</span>       <span class="sc">&lt;</span><span class="cn">NA</span><span class="sc">&gt;</span></span>
<span id="cb396-11"><a href="cap-bagg-rf.html#cb396-11" tabindex="-1"></a><span class="dv">2</span> importe_pro12          <span class="dv">60</span>      <span class="dv">1</span>       <span class="sc">&lt;</span><span class="cn">NA</span><span class="sc">&gt;</span></span>
<span id="cb396-12"><a href="cap-bagg-rf.html#cb396-12" tabindex="-1"></a><span class="dv">3</span> importe_pro16          <span class="dv">90</span>      <span class="dv">1</span>       <span class="sc">&lt;</span><span class="cn">NA</span><span class="sc">&gt;</span></span>
<span id="cb396-13"><a href="cap-bagg-rf.html#cb396-13" tabindex="-1"></a><span class="dv">4</span>  ingresos_ano      <span class="dv">156500</span>      <span class="dv">1</span>       <span class="sc">&lt;</span><span class="cn">NA</span><span class="sc">&gt;</span></span>
<span id="cb396-14"><a href="cap-bagg-rf.html#cb396-14" tabindex="-1"></a><span class="dv">5</span> importe_pro17         <span class="dv">150</span>      <span class="dv">1</span>       <span class="sc">&lt;</span><span class="cn">NA</span><span class="sc">&gt;</span></span>
<span id="cb396-15"><a href="cap-bagg-rf.html#cb396-15" tabindex="-1"></a><span class="dv">6</span>      anos_exp          <span class="dv">33</span>      <span class="dv">1</span>       <span class="sc">&lt;</span><span class="cn">NA</span><span class="sc">&gt;</span></span>
<span id="cb396-16"><a href="cap-bagg-rf.html#cb396-16" tabindex="-1"></a>  </span>
<span id="cb396-17"><a href="cap-bagg-rf.html#cb396-17" tabindex="-1"></a><span class="fu">tail</span>(tree205[,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)])</span>
<span id="cb396-18"><a href="cap-bagg-rf.html#cb396-18" tabindex="-1"></a>               split var split point status prediction</span>
<span id="cb396-19"><a href="cap-bagg-rf.html#cb396-19" tabindex="-1"></a><span class="dv">120</span>                 <span class="sc">&lt;</span><span class="cn">NA</span><span class="sc">&gt;</span>         <span class="fl">0.0</span>     <span class="sc">-</span><span class="dv">1</span>          N</span>
<span id="cb396-20"><a href="cap-bagg-rf.html#cb396-20" tabindex="-1"></a><span class="dv">121</span>                 <span class="sc">&lt;</span><span class="cn">NA</span><span class="sc">&gt;</span>         <span class="fl">0.0</span>     <span class="sc">-</span><span class="dv">1</span>          S</span>
<span id="cb396-21"><a href="cap-bagg-rf.html#cb396-21" tabindex="-1"></a><span class="dv">122</span> des_nivel_edu.BASICO         <span class="fl">0.5</span>      <span class="dv">1</span>       <span class="sc">&lt;</span><span class="cn">NA</span><span class="sc">&gt;</span></span>
<span id="cb396-22"><a href="cap-bagg-rf.html#cb396-22" tabindex="-1"></a><span class="dv">123</span>                 <span class="sc">&lt;</span><span class="cn">NA</span><span class="sc">&gt;</span>         <span class="fl">0.0</span>     <span class="sc">-</span><span class="dv">1</span>          S</span>
<span id="cb396-23"><a href="cap-bagg-rf.html#cb396-23" tabindex="-1"></a><span class="dv">124</span>                 <span class="sc">&lt;</span><span class="cn">NA</span><span class="sc">&gt;</span>         <span class="fl">0.0</span>     <span class="sc">-</span><span class="dv">1</span>          S</span>
<span id="cb396-24"><a href="cap-bagg-rf.html#cb396-24" tabindex="-1"></a><span class="dv">125</span>                 <span class="sc">&lt;</span><span class="cn">NA</span><span class="sc">&gt;</span>         <span class="fl">0.0</span>     <span class="sc">-</span><span class="dv">1</span>          N</span></code></pre></div>
<p>Este árbol se muestra en la Fig. <a href="cap-bagg-rf.html#fig:tree-plot">28.6</a>.</p>
<div class="sourceCode" id="cb397"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"reprtree"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/reprtree/man/plot.getTree.html">plot.getTree</a></span><span class="op">(</span><span class="va">rf</span>, k<span class="op">=</span><span class="fl">205</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:tree-plot"></span>
<img src="img/rf_tree205.png" alt="Árbol número 205 del random forest entrenado." width="70%"><p class="caption">
Figura 28.6: Árbol número 205 del random forest entrenado.
</p>
</div>
<p>Sin embargo, el método por el que se representa gráficamente no es muy claro y puede llevar a confusión o dificultar la interpretación del árbol. Si se desea estudiar hasta cierto nivel del árbol, se puede incluir el argumento <code>depth</code> como en el ejemplo abajo mostrado, y que representa el mismo árbol con una profundidad de 5 ramas en la Fig. <a href="cap-bagg-rf.html#fig:tree-plot2">28.7</a>.</p>
<div class="sourceCode" id="cb398"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/reprtree/man/plot.getTree.html">plot.getTree</a></span><span class="op">(</span><span class="va">rf</span>, k<span class="op">=</span><span class="fl">205</span>, depth <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:tree-plot2"></span>
<img src="img/rf_tree205_depth5.png" alt="Árbol número 205 del random forest entrenado hasta la capa 5." width="70%"><p class="caption">
Figura 28.7: Árbol número 205 del random forest entrenado hasta la capa 5.
</p>
</div>
<div id="aplicación-del-modelo-random-forest-con-ajuste-automático" class="section level4" number="28.3.7.1">
<h4>
<span class="header-section-number">28.3.7.1</span> Aplicación del modelo <em>random forest</em> con ajuste automático<a class="anchor" aria-label="anchor" href="#aplicaci%C3%B3n-del-modelo-random-forest-con-ajuste-autom%C3%A1tico"><i class="fas fa-link"></i></a>
</h4>
<p>En este segundo ejemplo, se pretende mejorar la precisión del modelo anterior. Para ello, se ajusta de forma automática los hiperparámetros de dicho algoritmo. De los mencionados anteriormente, solo se va a ajustar automáticamente <code>mtry</code>, que es el único incluido el método <code>rf</code>.</p>
<div class="sourceCode" id="cb399"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb399-1"><a href="cap-bagg-rf.html#cb399-1" tabindex="-1"></a><span class="fu">modelLookup</span>(<span class="st">"rf"</span>)</span>
<span id="cb399-2"><a href="cap-bagg-rf.html#cb399-2" tabindex="-1"></a>  model parameter                         label forReg forClass probModel</span>
<span id="cb399-3"><a href="cap-bagg-rf.html#cb399-3" tabindex="-1"></a><span class="dv">1</span>    rf      mtry <span class="co">#Randomly Selected Predictors   TRUE     TRUE      TRUE</span></span></code></pre></div>
<p>Para ajustar el número de árboles y el resto de hiperparámetros, se puede iterar el modelo y probar distintos valores. En una red de opciones se incluyen los valores a probar para el hiperparámetro <code>mtry</code>.</p>
<div class="sourceCode" id="cb400"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Se especifica un rango de valores posibles de mtry</span></span>
<span><span class="va">tuneGrid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/expand.grid.html">expand.grid</a></span><span class="op">(</span>mtry <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">18</span><span class="op">)</span></span></code></pre></div>
<p>A continuación, se entrena el modelo para que se ajuste al valor de <code>mtry</code> que maximice el rendimiento predictivo del modelo.</p>
<div class="sourceCode" id="cb401"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># se fija la semilla aleatoria</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">101</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># se entrena el modelo</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/train.html">train</a></span><span class="op">(</span><span class="va">CLS_PRO_pro13</span> <span class="op">~</span> <span class="va">.</span>, data<span class="op">=</span><span class="va">dp_entr_NUM</span>, </span>
<span>               method <span class="op">=</span> <span class="st">"rf"</span>, metric <span class="op">=</span> <span class="st">"Accuracy"</span>,</span>
<span>               tuneGrid <span class="op">=</span> <span class="va">tuneGrid</span>,</span>
<span>               trControl <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/trainControl.html">trainControl</a></span><span class="op">(</span>classProbs <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb402"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb402-1"><a href="cap-bagg-rf.html#cb402-1" tabindex="-1"></a>model</span>
<span id="cb402-2"><a href="cap-bagg-rf.html#cb402-2" tabindex="-1"></a></span>
<span id="cb402-3"><a href="cap-bagg-rf.html#cb402-3" tabindex="-1"></a>Random Forest </span>
<span id="cb402-4"><a href="cap-bagg-rf.html#cb402-4" tabindex="-1"></a></span>
<span id="cb402-5"><a href="cap-bagg-rf.html#cb402-5" tabindex="-1"></a><span class="dv">558</span> samples</span>
<span id="cb402-6"><a href="cap-bagg-rf.html#cb402-6" tabindex="-1"></a> <span class="dv">19</span> predictor</span>
<span id="cb402-7"><a href="cap-bagg-rf.html#cb402-7" tabindex="-1"></a>  <span class="dv">2</span> classes<span class="sc">:</span> <span class="st">'S'</span>, <span class="st">'N'</span> </span>
<span id="cb402-8"><a href="cap-bagg-rf.html#cb402-8" tabindex="-1"></a></span>
<span id="cb402-9"><a href="cap-bagg-rf.html#cb402-9" tabindex="-1"></a>No pre<span class="sc">-</span>processing</span>
<span id="cb402-10"><a href="cap-bagg-rf.html#cb402-10" tabindex="-1"></a>Resampling<span class="sc">:</span> <span class="fu">Bootstrapped</span> (<span class="dv">25</span> reps) </span>
<span id="cb402-11"><a href="cap-bagg-rf.html#cb402-11" tabindex="-1"></a>Summary of sample sizes<span class="sc">:</span> <span class="dv">558</span>, <span class="dv">558</span>, <span class="dv">558</span>, <span class="dv">558</span>, <span class="dv">558</span>, <span class="dv">558</span>, ... </span>
<span id="cb402-12"><a href="cap-bagg-rf.html#cb402-12" tabindex="-1"></a>Resampling results across tuning parameters<span class="sc">:</span></span>
<span id="cb402-13"><a href="cap-bagg-rf.html#cb402-13" tabindex="-1"></a></span>
<span id="cb402-14"><a href="cap-bagg-rf.html#cb402-14" tabindex="-1"></a>  mtry  Accuracy   Kappa    </span>
<span id="cb402-15"><a href="cap-bagg-rf.html#cb402-15" tabindex="-1"></a>   <span class="dv">1</span>    <span class="fl">0.8641354</span>  <span class="fl">0.7283098</span></span>
<span id="cb402-16"><a href="cap-bagg-rf.html#cb402-16" tabindex="-1"></a>   <span class="dv">2</span>    <span class="fl">0.8650087</span>  <span class="fl">0.7298376</span></span>
<span id="cb402-17"><a href="cap-bagg-rf.html#cb402-17" tabindex="-1"></a>   <span class="dv">3</span>    <span class="fl">0.8629614</span>  <span class="fl">0.7256812</span></span>
<span id="cb402-18"><a href="cap-bagg-rf.html#cb402-18" tabindex="-1"></a>   <span class="dv">4</span>    <span class="fl">0.8635609</span>  <span class="fl">0.7268514</span></span>
<span id="cb402-19"><a href="cap-bagg-rf.html#cb402-19" tabindex="-1"></a>   <span class="dv">5</span>    <span class="fl">0.8639559</span>  <span class="fl">0.7276250</span></span>
<span id="cb402-20"><a href="cap-bagg-rf.html#cb402-20" tabindex="-1"></a>   <span class="dv">6</span>    <span class="fl">0.8612659</span>  <span class="fl">0.7222420</span></span>
<span id="cb402-21"><a href="cap-bagg-rf.html#cb402-21" tabindex="-1"></a>   <span class="dv">7</span>    <span class="fl">0.8604934</span>  <span class="fl">0.7206476</span></span>
<span id="cb402-22"><a href="cap-bagg-rf.html#cb402-22" tabindex="-1"></a>   <span class="dv">8</span>    <span class="fl">0.8610116</span>  <span class="fl">0.7216937</span></span>
<span id="cb402-23"><a href="cap-bagg-rf.html#cb402-23" tabindex="-1"></a>   <span class="dv">9</span>    <span class="fl">0.8590645</span>  <span class="fl">0.7177882</span></span>
<span id="cb402-24"><a href="cap-bagg-rf.html#cb402-24" tabindex="-1"></a>  <span class="dv">10</span>    <span class="fl">0.8589073</span>  <span class="fl">0.7174718</span></span>
<span id="cb402-25"><a href="cap-bagg-rf.html#cb402-25" tabindex="-1"></a>  <span class="dv">11</span>    <span class="fl">0.8607248</span>  <span class="fl">0.7211179</span></span>
<span id="cb402-26"><a href="cap-bagg-rf.html#cb402-26" tabindex="-1"></a>  <span class="dv">12</span>    <span class="fl">0.8583609</span>  <span class="fl">0.7163903</span></span>
<span id="cb402-27"><a href="cap-bagg-rf.html#cb402-27" tabindex="-1"></a>  <span class="dv">13</span>    <span class="fl">0.8587296</span>  <span class="fl">0.7170933</span></span>
<span id="cb402-28"><a href="cap-bagg-rf.html#cb402-28" tabindex="-1"></a>  <span class="dv">14</span>    <span class="fl">0.8587384</span>  <span class="fl">0.7171642</span></span>
<span id="cb402-29"><a href="cap-bagg-rf.html#cb402-29" tabindex="-1"></a>  <span class="dv">15</span>    <span class="fl">0.8583195</span>  <span class="fl">0.7163106</span></span>
<span id="cb402-30"><a href="cap-bagg-rf.html#cb402-30" tabindex="-1"></a>  <span class="dv">16</span>    <span class="fl">0.8585407</span>  <span class="fl">0.7167355</span></span>
<span id="cb402-31"><a href="cap-bagg-rf.html#cb402-31" tabindex="-1"></a>  <span class="dv">17</span>    <span class="fl">0.8573597</span>  <span class="fl">0.7144030</span></span>
<span id="cb402-32"><a href="cap-bagg-rf.html#cb402-32" tabindex="-1"></a>  <span class="dv">18</span>    <span class="fl">0.8581404</span>  <span class="fl">0.7159558</span></span>
<span id="cb402-33"><a href="cap-bagg-rf.html#cb402-33" tabindex="-1"></a></span>
<span id="cb402-34"><a href="cap-bagg-rf.html#cb402-34" tabindex="-1"></a>Accuracy was used to select the optimal model using the largest value.</span>
<span id="cb402-35"><a href="cap-bagg-rf.html#cb402-35" tabindex="-1"></a>The final value used <span class="cf">for</span> the model was mtry <span class="ot">=</span> <span class="fl">2.</span></span></code></pre></div>
<p>Mientras que en el ejemplo anterior el algoritmo sólo probó tres valores de <code>mtry</code>, esta vez se realiza una prueba exhaustiva de valores. En el primer ejemplo, el valor del hiperparámetro era <code>mtry=10</code>, pero ahora se ha reajustado a <code>mtry=2</code>. Esto es equivalente a decir que 2 variables seleccionadas en cada partición son suficientes, y que no son necesarias 10 como en el ejemplo anterior. Finalmente, se puede observar en la Fig. <a href="cap-bagg-rf.html#fig:rfresults2">28.8</a> los resultados obtenidos durante la validación cruzada. Se observa cómo no sólo la precisión es mayor que en el ejemplo anterior, sino que además los resultados tienen menos dispersión.</p>
<div class="sourceCode" id="cb403"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/reshape/man/melt-24.html">melt</a></span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">resample</span><span class="op">[</span>,<span class="op">-</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">variable</span>, y <span class="op">=</span> <span class="va">value</span>, fill<span class="op">=</span><span class="va">variable</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_boxplot.html">geom_boxplot</a></span><span class="op">(</span>show.legend<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:rfresults2"></span>
<img src="img/rftunedboxplot.png" alt="Resultados obtenidos por el random forest con ajuste automático durante el proceso de validación cruzada." width="60%"><p class="caption">
Figura 28.8: Resultados obtenidos por el random forest con ajuste automático durante el proceso de validación cruzada.
</p>
</div>
</div>
</div>
<div id="resumen-25" class="section level3 unnumbered infobox_resume">
<h3>Resumen<a class="anchor" aria-label="anchor" href="#resumen-25"><i class="fas fa-link"></i></a>
</h3>
<p>En este capítulo se introduce al lector en el <em>bagging</em> y el algoritmo de aprendizaje supervisado conocido como <em>random forest</em>, en concreto:</p>
<ul>
<li>Se presenta el concepto de aprendizaje ensamblado, y se profundiza en uno de sus paradigmas: el <em>bagging</em>.</li>
<li>Se implementa el <em>bagging</em> en <code>R</code> a través de un caso de clasificación binaria.</li>
<li>Se expone cómo medir la importancia de las variables incluidas en un modelo <em>bagging</em> para facilitar su interpretación.</li>
<li>Se explica el modelo <em>random forest</em>, fundamentado en los árboles decisión y en el <em>bagging</em>. Así como los hiperparámetros más importantes para ajustar el modelo de mayor precisión.</li>
<li>Se presenta un ejemplo de clasificación binaria utilizando el modelo <em>random forest</em> en <code>R</code>.</li>
</ul>
</div>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="cap-naive-bayes.html"><span class="header-section-number">27</span> Naive Bayes</a></div>
<div class="next"><a href="cap-boosting-xgboost.html"><span class="header-section-number">29</span> Boosting y el algoritmo XGBoost</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="Índice capítulo"><h2>Índice capítulo</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#cap-bagg-rf"><span class="header-section-number">28</span> Métodos ensamblados: bagging y random forest</a></li>
<li><a class="nav-link" href="#introducci%C3%B3n-a-los-m%C3%A9todos-ensamblados"><span class="header-section-number">28.1</span> Introducción a los métodos ensamblados</a></li>
<li>
<a class="nav-link" href="#bagging"><span class="header-section-number">28.2</span> Bagging</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#rbagging"><span class="header-section-number">28.2.1</span> Procedimiento con R: la función bagging()</a></li>
<li><a class="nav-link" href="#implementando-bagging-en-r"><span class="header-section-number">28.2.2</span> Implementando bagging en R</a></li>
<li><a class="nav-link" href="#interpretaci%C3%B3n-de-variables-en-el-bagging"><span class="header-section-number">28.2.3</span> Interpretación de variables en el bagging</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#random-forest"><span class="header-section-number">28.3</span> Random Forest</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#n%C3%BAmero-de-%C3%A1rboles-k"><span class="header-section-number">28.3.1</span> Número de árboles (\(K\))</a></li>
<li><a class="nav-link" href="#n%C3%BAmero-de-variables-a-considerar-mtry"><span class="header-section-number">28.3.2</span> Número de variables a considerar (mtry)</a></li>
<li><a class="nav-link" href="#complejidad-de-los-%C3%A1rboles"><span class="header-section-number">28.3.3</span> Complejidad de los árboles</a></li>
<li><a class="nav-link" href="#esquema-de-muestreo"><span class="header-section-number">28.3.4</span> Esquema de muestreo</a></li>
<li><a class="nav-link" href="#regla-de-divisi%C3%B3n"><span class="header-section-number">28.3.5</span> Regla de división</a></li>
<li><a class="nav-link" href="#procedimiento-con-r-la-funci%C3%B3n-randomforest"><span class="header-section-number">28.3.6</span> Procedimiento con R: la función randomForest()</a></li>
<li><a class="nav-link" href="#aplicaci%C3%B3n-del-modelo-random-forest-en-r"><span class="header-section-number">28.3.7</span> Aplicación del modelo random forest en R</a></li>
<li><a class="nav-link" href="#resumen-25">Resumen</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Fundamentos de ciencia de datos con R</strong>" coordinado por <a href="https://blog.uclm.es/gemafaviles/" class="text-light">Gema Fernández-Avilés y José-María Montero</a>. Generado por última vez el día 2023-06-16.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>Este libro ha sido generado con el paquete de R <a class="text-light" href="https://bookdown.org">bookdown</a>.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
