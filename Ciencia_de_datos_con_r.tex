% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsthm} %% math
\usepackage{fvextra}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}



% %%  ------------------  Tabla 
% Tabla en lugar de cuadro
\gappto\captionsspanish{\renewcommand{\tablename}{Tabla}
          \renewcommand{\listtablename}{Índice de tablas}}



%% -------------------- tamaño ------------- inicialmente 170 x 240

%\usepackage[paperwidth = 170mm,
%			paperheight = 240mm, top=3.25cm, bottom=2.5cm, left=2.5cm, right=2cm]{geometry}

\usepackage[paperwidth = 190mm,
			paperheight = 250mm, top=3cm, bottom=2.5cm, left=2.5cm, right=2cm]{geometry}


\usepackage[a4, center, cam]{crop}

\makeatother


%% ---------------- blocks -----------

\usepackage{tcolorbox}

\newtcolorbox{infobox}{
  colback=pink,
  colframe=blue,
  coltext=black,
  boxsep=5pt,
  arc=4pt}
  
  
\newtcolorbox{infobox_resume} {
 colback=yellow!45!white,
  colframe=red,
  coltext=black,
  boxsep=5pt,
  arc=4pt}
  
  
  %% ---------------- índice de contenidos -----------

 \usepackage{makeidx}
 \makeindex

  
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}
\nocite{*}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Ciencia de datos con R},
  pdfauthor={Gema Fernández-Avilés y José-María Montero},
  colorlinks=true,
  linkcolor={red},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}

\title{Ciencia de datos con R}
\author{Gema Fernández-Avilés y José-María Montero}
\date{2022-12-22}

\usepackage{amsthm}
\newtheorem{theorem}{Teorema}[chapter]
\newtheorem{lemma}{Lema}[chapter]
\newtheorem{corollary}{Corolario}[chapter]
\newtheorem{proposition}{Proposición}[chapter]
\newtheorem{conjecture}{Conjetura}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definición}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Ejemplo}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Ejercicio}[chapter]
\theoremstyle{definition}
\newtheorem{hypothesis}{Hipótesis}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Observación}
\newtheorem*{solution}{Solución}
\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{normas-cdr-book-v0.2.0}{%
\chapter{Normas CDR-book-v0.2.0}\label{normas-cdr-book-v0.2.0}}

\hypertarget{notas-importantes}{%
\section{Notas importantes}\label{notas-importantes}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  La carpeta que acabamos de compartir \texttt{book-cdr-v0.2.0-autores} contiene la versión con la que se ha generado tanto el PDF como el gitbook. Por favor, \textcolor{red}{tomad este proyecto como definitivo (por ahora) y modificad vuestros capítulos sobre esta versión que os compartimos}.
\item
  A algunos autores os hemos pedido que, una vez comprobado el código y generado el capítulo, \textbf{formateéis las chunks con \texttt{eval=FALSE} (para que no ejecute) y luego incluyáis el resultado, plot, etc}. Otras veces las habéis puesto vosotros directamente. \textbf{Esto se agradece y siempre que podáis, hacedlo, please}, pues como supondréis compilar todo y con tantos paquetes distintos ha dado bastantes errores.
\item
  No llaméis directamente al paquete \texttt{tidyverse}, pues tenemos/tengo conflictos, empezó todo con \texttt{filter()} y luego se añadieron más funciones. Por ahora está solucionado.
\item
  Revisad bien el PDF y el gitbook (\url{https://cdr-book.github.io/prefacio.html}) y comprobad que está todo y bien. Importante, github distingue mayúsculas y minúsculas, luego los nombres las figuras deben coincidir. Es decir, si la figura se llama ``portada-cdr.jpg'', y ponemos ``portada-cdr.JPG'', aunque se genere el HTML y el PDF, como github.io distingue mayúsculas y minúsculas no generaría la imagen.
\item
  Ejecutad desde el PDF/HTML el código y comprobar que se generan perfectamente los resultados esperados.
\item
  Ya están etiquetados todos los capítulos, así que ya se pueden hacer referencias cruzadas, cosa que conviene para dar continuidad al libro.
\item
  Según vayáis revisando, etiquetad también todas las chunks, con un indicativo del capítulo para que no haya duplicidades. Al compilar todo había errores y al no estar etiquetas las chunks ha sido muy difícil ver donde estaba el error.
\item
  Es importante hacer referencias cruzadas en los capítulos/secciones y eliminar duplicidades. Todos nos hemos pasado de la estimación inicial de páginas. El formato del PDF ahora mismo es \(19 \times 25\) en vez de \(17 \times 24\) que era el original. Puede ser una opción si McGraw lo acepta (lo han sugerido ellos pero ahora lo tienen que aprobar). La extensión es de 853 págs. en vez de 700 págs\ldots{}
\item
  Comprobad que los datos están en el paquete del libro, \texttt{CDR}, con su descripción correspondiente y que en el capítulo se llaman directamente desde la librería, no desde la carpeta \texttt{data/file.csv}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"remotes"}\NormalTok{)}
\FunctionTok{install\_github}\NormalTok{(}\StringTok{"cdr{-}book/CDR"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"CDR"}\NormalTok{)}
\FunctionTok{data}\NormalTok{(}\AttributeTok{package =} \StringTok{"CDR"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\item
  Las referencias bibliográficas están revisadas (pues había duplicidades) e incluidas en el archivo \texttt{index.Rmd}
\item
  Generad las palabras clave con el comando \texttt{\textbackslash{}index\{\}} de latex.
\end{enumerate}

\hypertarget{notas-relativas-a-r}{%
\section{Notas relativas a R}\label{notas-relativas-a-r}}

\begin{itemize}
\item
  Para el estilo del código, estamos siguiendo \texttt{styler}. \url{https://www.tidyverse.org/blog/2017/12/styler-1.0.0/}
\item
  Un paquete en el texto irá con la comilla invertida: \texttt{CDR} y citado la primera vez..
\item
  Una función en el texto irá con la comilla invertida y con paréntesis: \texttt{data()}.
\item
  Las referencias a \textbf{R} en el texto irán en negrita: \textbf{R}
\item
  Todos los objetos, variables, argumentos, etc. de R con la comilla invertida. Por ejemplo:
\end{itemize}

\ldots incluye la función \texttt{preProcess(x,\ method)}, que se puede integrar en el entrenamiento
(función \texttt{train()}) para estimar\ldots{}

A continuación, se muestra su uso con un ejemplo del dataset \texttt{Madrid\_Sale}, incluido en el paquete de \textbf{R} \texttt{Idealista18}.

\begin{itemize}
\tightlist
\item
  \textbf{Nombres de las variables y objetos} que se crean:

  \begin{itemize}
  \tightlist
  \item
    unidos por guión bajo: \texttt{reg\_log} en vez de \texttt{reg.log}.
  \item
    informativos y lo más cortos posibles: \texttt{med\_pib\_filtro} en vez de \texttt{media\_pib\_filtro}
  \end{itemize}
\item
  Cuando se cargan los datos de un paquete, especificarlo en la chunk. Por ejemplo:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"gapmaminder"}\NormalTok{)}
\FunctionTok{data}\NormalTok{(}\StringTok{"gapmaminder"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{notas-de-estilo}{%
\section{Notas de estilo}\label{notas-de-estilo}}

\hypertarget{tuxedtulos}{%
\subsection{Títulos}\label{tuxedtulos}}

\begin{itemize}
\item
  Solo la primera letra en mayúscula. Ejemplos:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Redes neuronales artificiales}
  \item
    \textbf{Gobierno y gestión de calidad de datos}
  \end{itemize}
\item
  Lo suficientemente informativos y cortos para que quepan en el encabezado.
\item
  Mejor en español, en la medida de lo posible y entre paréntesis en inglés. Ejemplo:

  \begin{itemize}
  \tightlist
  \item
    Selección de variables (\emph{Feature Selection})
    y no al revés.
  \end{itemize}
\end{itemize}

\hypertarget{figuras}{%
\subsection{Figuras}\label{figuras}}

\begin{itemize}
\item
  Tienen que verse bien la leyenda o el texto que contenga y ser de calidad.
\item
  Deben tener su \emph{caption} informativa, por ejemplo, fig.cap=`Comparativa de metodologías de medición.'\} y estar etiquetadas para luego referenciarlas.
\item
  Referenciarlas en el texto con ``Fig.''. Ejemplo: La Fig. XX muestra \ldots{}
\item
  Dibujarlas a color (el libro es a color).
\item
  Se puede utilizar la librería \texttt{patchwork} (por ejemplo) para formatear los cuadros (ver \url{https://patchwork.data-imaginist.com/})
\item
  Incluid las figuras estáticas, contenidas en la carpeta \textbf{img}, mejor con esta opción
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\StringTok{"img/Comparativa\_modelos.PNG"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tablas}{%
\subsection{Tablas}\label{tablas}}

\begin{itemize}
\item
  Las tablas hay que generarlas en el texto con Rmardown. Si la tabla es muy grande igual es conveniente con Latex. Me ha pasado un autor que ha hecho bastantes tablas en sus capítulos es siguiente enlace, muy útil y que funciona muy bien. \url{https://www.tablesgenerator.com/}
\item
  Todas tienen que tener su título y estar etiquetadas para luego referenciarlas.
\item
  Tienen que estar citadas en el texto. La Tabla xx muestra\ldots{}
\end{itemize}

\hypertarget{otros}{%
\subsection{Otros}\label{otros}}

\begin{itemize}
\item
  Escribid siempre en impersonal y repasad tildes, etc.
\item
  Al hacer referencia a capítulos y secciones escribir: Cap. XX, Sec. XX.
\item
  ``Formatead'' el resultado del PDF controlando figuras y texto y sabiendo que todos los capítulos empiezan en página impar.
\item
  En el cuadro resumen, color amarillo, poned \textbf{Resumen} (sólo la primera en mayúscula y en negrita, más un intro) y en el cuadro de notas, \textbf{Nota} (sólo la primera en mayúscula y en negrita, más un intro).
\end{itemize}

\begin{infobox_resume}
\textbf{Resumen}

Aquí el texto

\end{infobox_resume}

\begin{infobox}
\textbf{Nota}

Aquí el texto

\end{infobox}

\begin{itemize}
\item
  Palabras inglesas en cursiva. Ejemplo: valores \emph{missing}
\item
  Evitad ``como ya se ha mencionado,'' y decid, ``como se ha mencionado en el Cap. XX''
\item
  Minúscula depués de : en español. Por ejemplo,

  \begin{itemize}
  \tightlist
  \item
    \texttt{formula}: refleja el\ldots{}
  \end{itemize}
\item
  Conceptos importantes en \textbf{negrita}.
\item
  Recordad que se puden utilizar los entornos disponibles para ejemplos, ejercicios, etc\ldots{}
\end{itemize}

\begin{example}
\protect\hypertarget{exm:ex-tablas5-cdr}{}\label{exm:ex-tablas5-cdr}En un municipio se desea investigar si el desempleo es o no independiente del sexo del desempleado. Se seleccionan aleatoriamente 100 varones y 100 mujeres y se les pregunta por su situación laboral (trabajando; en paro).
\end{example}

\begin{longtable}[]{@{}llccc@{}}
\toprule()
& & Situación & laboral & \\
\midrule()
\endhead
& & Trabajando & En paro & \textbf{Total} \\
Sexo & Varón & \emph{N}\textsubscript{11} & \emph{N}\textsubscript{12} & \textbf{100} \\
& Mujer & \emph{N}\textsubscript{21} & \emph{N}\textsubscript{22} & \textbf{100} \\
& \textbf{Total} & \emph{N}\textsubscript{.1} & \emph{N}\textsubscript{.2} & \textbf{200} \\
\bottomrule()
\end{longtable}

\hypertarget{prefacio}{%
\chapter*{Prefacio}\label{prefacio}}
\addcontentsline{toc}{chapter}{Prefacio}

\begin{infobox}
\textbf{Nota}: Este libro está publicado por \href{https://www.mheducation.com/}{McGraw Hill}.
Las copias físicas están disponibles en \href{https://www.mheducation.com/}{McGraw Hill}.
La versión online de este libro se puede leer de forma gratuita en \url{https://cdr-book.github.io/} y tiene la \textbf{licencia FALTA}.
Si tiene algún comentario, no dude en contactar con los editores/autores. ¡Gracias!

\end{infobox}

\begin{center}\includegraphics[width=0.5\linewidth]{img/portada-cdr} \end{center}

\hypertarget{los-autores}{%
\section*{Los autores}\label{los-autores}}
\addcontentsline{toc}{section}{Los autores}

\textbf{TO DO}

Aquí listado de nombre y afiliaciones por orden alfabético?

\begin{itemize}
\item
  \texttt{Pedro\ Albarracín\ García.}
  FDS a DXC Technology Company.
\item
  \texttt{Jose-Luis\ Alfaro\ Navarro.}
  Universidad de Castilla-La Mancha.
\item
  \texttt{Eusebio\ Angulo\ Sánchez-Herrera.}
  Universidad de Castilla-La Mancha
\item
  \texttt{Borja\ Andrino.} El País.
\item
  \texttt{Itzcóatl\ Bueno\ García.}
  Instituto Nacional de Estadística
\item
  \texttt{Ismael\ Caballero\ Muñoz-Reja}.
  Universidad de Catilla-La Mancha. DQ-Team
\item
  \texttt{Miguel\ Camacho-Collados.}
  Policía Nacional
\item
  \texttt{Ramón\ Carrasco\ González.}
  Universidad Complutense de Madrid
\item
  \texttt{Víctor\ Casero\ Alonso.}
  Universidad de Catilla-La Mancha
\item
  \texttt{Coro\ Chasco\ Yrigoyen.}
  Universidad Autónoma de Madrid
\item
  \texttt{Ángela\ Celis\ Sánchez.}
  Universidad de Castilla-La Mancha
\item
  \texttt{Luz\ Congosto\ Martínez.}
  Univesidad Carlos III de Madrid
\item
  \texttt{María\ Durbán\ Reguera.}
  Universidad de Carlos III de Madrid
\item
  \texttt{José\ Luis\ Espinosa\ Aranda.}
  Ubotica
\item
  \texttt{Gema\ Fernández-Avilés\ Calderón.}
  Universidad de Castilla-La Mancha
\item
  \texttt{Jaime\ Fierro\ Martín.}
  Analitycae
\item
  \texttt{M.\ Carmen\ García\ Centeno.}
  Universidad San Pablo-CEU
\item
  \texttt{José\ J.\ Galán\ Hernández.}
  Universidad Complutense de Madrid
\item
  \texttt{Rocío\ González\ Martínez.}
  Analitycae
\item
  \texttt{Aurora\ González\ Vidal.}
  Universidad de Murcia
\item
  \texttt{Isidro\ Hidalgo\ Arellano.}
  Junta de Comunidades de Castilla-La Mancha
\item
  \texttt{Ángel\ Jiménez\ Rojas.}
  Junta de Comunidades de Castilla-La Mancha
\end{itemize}

\texttt{Jesús\ Lago\ Millas.} Scout Analyst

\begin{itemize}
\item
  \texttt{Federico\ Liberatore.} Cardiff University
\item
  \texttt{Emilio\ López\ Cano.}
  Universidad Rey Juan Carlos
\item
  \texttt{Bilal\ Laouah.} People Analytics
\item
  \texttt{María\ Lozano\ Zahonero.} Università di Roma Tor Vergata
\item
  \texttt{Ambrosio\ Nguema.} Inditex
\item
  \texttt{Jorge\ Mateu\ Mahiques.} Universidad Jaime I de Castellón
\item
  \texttt{José-María\ Montero\ Lorenzo.} Universidad de Castilla-La Mancha
\item
  \texttt{Leticia\ Meseguer\ Santamaría.} Universidad de Catilla-La Mancha
\item
  \texttt{Román\ Mínguez\ Salido.}
  Universidad de Castilla-La Mancha
\item
  \texttt{Mehdi\ Moradi.}
  Umea University
\item
  \texttt{José\ Ángel\ Olivas\ Varela.}
  Universidad de Castilla-La Mancha
\item
  \texttt{Arturo\ Peralta\ Martín-Palomino.}
  Universidad Internacional de la Rioja
\item
  \texttt{Ricardo\ Pérez\ del\ Castillo.}
  Universidad de Catilla-La Mancha
\item
  \texttt{Lara\ Quijano-Sánchez.}
  Universidad Autónoma de Madrid
\item
  \texttt{Dominic\ Royé.}
  Universidad de Santiago de Compostela
\item
  \texttt{Carlos\ Real\ Ugena.}
  Deloitte
\item
  \texttt{José\ M.\ Sanz\ Candales.}
  Red Eléctrica Española
\item
  \texttt{Cristina\ Sánchez\ Figueroa.}
  Universidad Nacional a Distancia
\item
  \texttt{Jorge\ Velasco\ López.}
  Instituto Nacional de Estadística
\item
  \texttt{Manuel\ Vargas\ Vargas.}
  Universidad de Catilla-La Mancha
\item
  \texttt{Noelia\ Vállez\ Enano.}
  Universidad de Catilla-La Mancha
\item
  \texttt{Andrés\ Vallone.}
  Universidad Católica de Chile
\end{itemize}

\hypertarget{por-quuxe9-este-libro}{%
\section*{¿Por qué este libro?}\label{por-quuxe9-este-libro}}
\addcontentsline{toc}{section}{¿Por qué este libro?}

\textbf{TO DO}

\hypertarget{caracteruxedsticas}{%
\section*{Características}\label{caracteruxedsticas}}
\addcontentsline{toc}{section}{Características}

\textbf{TO DO}

Partes, capítulos, casos de uso, R, académicos y profesionales, \ldots{}

\hypertarget{el-paquete-cdr}{%
\section*{\texorpdfstring{El paquete \texttt{CDR}}{El paquete CDR}}\label{el-paquete-cdr}}
\addcontentsline{toc}{section}{El paquete \texttt{CDR}}

\begin{center}\includegraphics[width=0.25\linewidth]{img/LogoCDR_transparente} \end{center}

El paquete \texttt{CDR} contiene la mayoría de conjuntos de datos utilizados en este libro que no están disponibles en otros paquetes. Para instalarlo use la función \texttt{install\_github()}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# este comando solo necesita ser ejecutado una vez}
\NormalTok{remotes}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"cdr{-}book/CDR"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

La lista de todos los conjuntos de datos puede obtenerse haciendo \texttt{data()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(CDR)}
\FunctionTok{data}\NormalTok{(}\AttributeTok{package =} \StringTok{"CDR"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{a-quiuxe9n-va-dirigido}{%
\section*{¿A quién va dirigido?}\label{a-quiuxe9n-va-dirigido}}
\addcontentsline{toc}{section}{¿A quién va dirigido?}

\textbf{TO DO}

\hypertarget{agradecimientos}{%
\section*{Agradecimientos}\label{agradecimientos}}
\addcontentsline{toc}{section}{Agradecimientos}

\textbf{TO DO}

\hypertarget{informaciuxf3n-del-software}{%
\section*{Información del software}\label{informaciuxf3n-del-software}}
\addcontentsline{toc}{section}{Información del software}

\textbf{TO DO}

\hypertarget{part-ciencia-datos-software-y-cientuxedficos}{%
\part{Ciencia, datos, software\ldots{} y científicos}\label{part-ciencia-datos-software-y-cientuxedficos}}

\hypertarget{ciencia-datos}{%
\chapter{¿Es la ciencia de datos una ciencia?}\label{ciencia-datos}}

\emph{Gema Fernández-Avilés} y \emph{Bilal Loual}

\hypertarget{ciencia}{%
\section{¿Qué se entiende por ciencia?}\label{ciencia}}

No es posible determinar si la ciencia de datos es una ciencia sin previamente consensuar la definición de ciencia. La palabra \textbf{ciencia} \emph{`Scientia'} proviene del verbo \textbf{saber} \emph{`scire'}. Por tanto, en su origen, el saber y la ciencia se consideraban exactamente lo mismo. En este sentido, \citet{Bunge_2004} distingue dos categorías fundamentales de conocimiento: el conocimiento vulgar y el científico. El primero se adquiere de manera cotidiana a partir de percepciones y sensaciones individuales, apoyándose en la evidencia y el sentido común. Este tipo de conocimiento constituye la base sobre la que se sustenta el conocimiento científico y éste se enriquece al verificar, con la ayuda de un método, la validez de las observaciones realizadas. Se adquiere, por tanto, de forma consciente, deliberada y metódica, pudiendo ser sometido a prueba y, llegado el caso, podría ser superado, \citet{Montero_1997}.

\index{ciencia} \index{saber} \index{método} \index{método!científico}

Una definición generalmente aceptada de ciencia es al propuesta por \citet{Blaug_1980}: ``\emph{ciencia es el cuerpo de proposiciones sintéticas acerca del mundo real que es susceptible, al menos en principio, de falsaciones por medio de la observación empírica, ya que excluye la posibilidad de que ciertos acontecimientos se produzca. Así pues, la ciencia se caracteriza por su método de formulación de proposiciones contrastables, y no por su contenido, ni por su pretensión de certeza en el conocimiento; si alguna certeza proporciona la ciencia, ésta será más bien la certeza de la ignorancia.}''

Si bien esta definición está sesgada hacia el enfoque popperiano, introduce la palabra clave de la discusión planteada: el \textbf{método}, y más concretamente, el \textbf{método científico}.

La palabra método procede del latín \emph{`methŏdus'}, y éste del griego \(\mu \varepsilon \theta o \delta 0 \varsigma\), que quiere decir \textbf{camino o sendero que conduce a un fin}. Es, pues, un procedimiento para conseguir algo, y como el fin que busca la ciencia es la verdad, el método científico será el camino mediante el cual las ciencias encuentren sus respectivas verdades. El método científico es, por tanto, el conjunto de procedimientos por los cuales se adquieren conocimientos rigurosos, ciertos y seguros acerca de un objeto. El método científico ha de ser recorrido siguiendo cuatro fases \citep{cancelo_1991}: (i) Un previo inventario de los fenómenos o de los hechos significativos no rutinarios; (ii) el planteamiento de un tema problemático que hace necesaria una explicación; (iii) la ideación de conjeturas tendentes a darla y (iv) el tratamiento de las diversas hipótesis hasta que sólo una se mantenga.

Por tanto, aunque no hay acuerdo a la hora de dar una definición exacta de ciencia, sí hay un mayor acuerdo a la hora de aceptar el método científico como elemento que define la frontera de la ciencia: ``\emph{El método científico y la finalidad a la cual se aplica (conocimiento objetivo del mundo) constituyen la entera diferencia que existe entre la ciencia y la no-ciencia {[}\ldots{]}. El método científico es un rasgo característico de la ciencia, tanto de la pura como de la aplicada: donde no hay método científico no hay ciencia}'', \citet{Bunge_2004}. ``\emph{Una ciencia es una disciplina que utiliza el método científico con la finalidad de hallar estructuras generales (leyes).}'' \citet{Bunge_2004}.

Este desplazamiento de la idea central alrededor de la cual se articula el concepto de ciencia es el que permite pasar a discutir acerca de si cabe hablar o no de Ciencias de datos como ciencia.

\hypertarget{quuxe9-diablos-es-la-ciencia-de-datos}{%
\section{¿Qué diablos es la ciencia de datos?}\label{quuxe9-diablos-es-la-ciencia-de-datos}}

\index{Tukey}
\index{ciencia de datos}
\index{análisis de datos}

La ciencia de datos es una disciplina emergente donde, a diferencia de otros saberes como, por ejemplo, las ciencias matemáticas, el \emph{corpus} o la acumulación de conocimiento se ha generado en un lapso de tiempo relativamente corto y de una forma muy intensiva, y no a lo largo de siglos de historia. Su inicio data de la década de los 70, aunque en 1962 el estadítsico John W. Tukey precedió al término ``ciencia de Datos'' en su artículo ``\emph{The Future of Data Analysis}'' al explicar una evolución de la estadística matemática. En éste, definió por primera vez el análisis de datos como: ``\emph{Procedimientos para analizar datos, técnicas para interpretar los resultados de dichos procedimientos, formas de planificar la recopilación de datos para hacer su análisis más fácil, más preciso o acertado, y toda la maquinaria y los resultados de las estadísticas matemáticas que se aplican al análisis de datos}'', \citep{tukey_1962}. A partir de este momento, toda una serie de acontecimientos fueron consolidando el termino de ciencia de datos como una nueva disciplina. Una breve descripción de los acontecimientos se muestran en la Fig. (\ref{fig:time-line-ds}).

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/time-line-ds3} 

}

\caption{Linea del tiempo de la ciencia de datos.}\label{fig:time-line-ds}
\end{figure}

La ciencia de datos implica la limpieza, la agregación y la manipulación de datos, adquiridos de la web, de teléfonos inteligentes, de clientes, de pacientes, de sensores, de encuestas,\ldots{} entre otras fuentes, para realizar la modelización y el análisis de datos avanzado que ayuden a identificar patrones, tendencias, comportamientos y, por tanto, a la toma de decisiones. El volumen acelerado de fuentes de datos y, posteriormente, de datos, ha hecho que la ciencia de datos sea uno de los campos de más rápido crecimiento en todas las industrias. Como resultado, no sorprende que el científico de datos surgiera como una nueva profesión que ayudaría a comprender y a analizar los volúmenes masivos de datos que se acumulaban en ese momento y que fuese calificado como el ``trabajo más sexy del siglo XXI'' según \citet{hbr_2012}.

\index{estadística} \index{matemática} \index{informática} \index{programación} La ciencia de datos es, por tanto, una disciplina relativamente nueva que combina la Estadística, la Matemática, la Informática y la Programación, para obtener valor de los datos. Se utiliza en una amplia variedad de campos, como la Astronomía, la Medicina, la Economía, el Marketing, las Finanzas, la Biología, la Industria, etc. Esta naturaleza transdisciplinaria de la ciencia de datos añade cierta complejidad a su caracterización, pues siendo una única disciplina, subsume en su ejercicio otras disciplinas como la ciencia Matemática y Estadística y la ciencia de la Computación, que a su vez son aplicadas a un amplio rango de dominios de manera integral. La ciencia de datos se sirve de los métodos formales de la Matemática y de las aplicaciones prácticas e ingenieriles de las Ciencias de la Computación para la generación de conocimiento y para la resolución de problemas prácticos en múltiples campos. Esta ubicuidad hace que la ciencia de datos esté situada transversalmente entre los saberes de primer orden. En otras palabras, la ciencia de datos va adoptando los paradigmas, modelos, teorías o constructos propios del campo sustantivo en el que se ejerce, de forma que para resolver alguna problemática sobre personas, puede recurrirse al \emph{corpus} relativo de la Psicología o de la Sociología y para profundizar sobre alguna condición de salud, podrá hacerse lo propio con la Medicina o la Biología, por mencionar algunos ejemplos.

\hypertarget{lo-cientuxedfico-de-la-ciencia-de-datos}{%
\section{Lo científico de la ciencia de datos}\label{lo-cientuxedfico-de-la-ciencia-de-datos}}

En la Sección \ref{ciencia} se manifestó que un aspecto fundamental de la ciencia es que utiliza el método científico con la finalidad de hallar estructuras generales (principios y leyes) con capacidad predictiva y comprobable. Es por ello que el marco general de la \textbf{metodología científica} ha sido bien fundamentado a lo largo de las últimas décadas a través de las contribuciones de diferentes teóricos de la ciencia (\citet{Diez_Moulines_1999}, \citet{chalmers_2000}, \citet{Bunge_2004}). Por otra parte, la ciencia se clasifica según el objeto de estudio en \citep{Bunge_2018}: empíricas y formales\footnote{Las ciencias empíricas emplean una aproximación hipotético-inductivo-deductiva y amplían el conocimiento. Las ciencias formales se fundamentan en la deducción y explicitan el saber, pero no lo amplían. Ejemplo de ciencia empírica lo es la Biología, mientras que la Lógica lo es de la ciencia formal. \index{ciencia!empírica} \index{ciencia!formal}}. Dado que la ciencia de datos se aplica a diferentes campos, puede tomar las características de las ciencias empíricas y de las formales.

Si se analiza el conjunto de saberes científicos se aprecia que tienen en común una serie de características \citep{Bunge_2018}. La pregunta fundamental en este punto, por tanto, es: ¿Comparte la ciencia de datos estas características? De ser satisfechas conferirían a la ciencia de datos el estatuto de ciencia que comparten otros saberes científicos:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\item
  La actividad científica es \textbf{metódica}. Es decir, se caracteriza por proceder de manera ordenada y planificada. Esta estructuración le otorga solidez y consistencia. En ciencia de datos también se actúa de manera metódica, a través de diferentes metodologías, como lo son \emph{Knowledge Discovery in Databases} (KDD), \emph{Sample, Explore, Modify, Model, Assess} (SEMMA) y \emph{CRoss-Industry Standard Process for Data Mining} (CRISP-DM), tal y como se expone en el capítulo @ref\{metodología\}.
\item
  El conocimiento científico \textbf{se fundamenta en hechos}. Por lo común, los científicos disponen de diferentes instrumentos para observar y registrar la realidad sobre la que conjeturan. Esta labor también es realizada por los científicos de datos, quienes cuentan con un número amplio de instrumentos y metodologías para la recolección de datos. Tal es el caso de los cuestionarios, escalas psicométricas o datos transaccionales producidos por diferentes tecnologías.
\item
  El saber científico es \textbf{falsable}. Ésto implica que las afirmaciones científicas pueden ser contrastadas, precisamente, a través de los hechos. En ciencia de datos, esto también sucede, ya que los resultados a los que se llega no están ligados a la subjetividad del analista, sino a la objetividad de los datos.
\item
  La ciencia es una actividad que \textbf{trasciende los hechos}. Es decir, la ciencia parte de evidencias empíricas que tienden a ser superadas, puesto que la explotación de las mismas suele generar nuevas evidencias que a su vez pueden contribuir a crear nuevos marcos teóricos explicativos o a ampliar los existentes. La ciencia de datos puede ejercerse en el mismo sentido. Por ejemplo, la construcción de un recomendador como Netflix, parte de ciertos datos, pero su uso genera nuevos datos de comportamiento que pueden ser empleados para optimizar su sustrato algorítmico.
\item
  La investigación científica se caracteriza también por ser una \textbf{actividad analítica}. Es decir, tiende a descomponer los problemas en sus partes constitutivas. Obsérvese que la consecuencia de ello es que no se pueda hablar de una ciencia general, sino de especializaciones. Naturalmente, la especialización también existe en esta disciplina, por eso, cuando la ciencia de datos se aplica intensivamente en Recursos Humanos, por ejemplo, es posible hablar de \emph{Human Resource Analytics}.
\item
  La ciencia es \textbf{comunicable} y para ello se sirve de sistemas representacionales lógico-formales. Este atributo también se aprecia en la ciencia de datos, puesto que los hallazgos tienden a ser compartidos a través de diferentes estrategias, entre ellas, la visualización de datos.
\end{enumerate}

La ciencia, sin embargo, no sólo puede describirse en términos de características, sino también \textbf{funcionalmente} (\citet{Hempel_2005}). De hecho, las características anteriormente citadas son las que posibilitan las funciones \textbf{descriptiva}, \textbf{explicativa} y \textbf{predictiva}.

\begin{itemize}
\item
  La primera, la \textbf{descriptiva}, permite recabar información sobre el suceso que se analiza para tratar de conocerlo en mayor profundidad y detalle. En ciencia de datos, usualmente, una de las primeras tareas consiste en describir el conjunto de datos para conocer en detalle sus características, es decir, el número de variables, el número de observaciones, los valores nulos, etc., (esta parte se conoce como ``Entendimiento de los datos'' en la metodología CRIPS-DM, ver Sección \ref{met-crisp-dm}).
\item
  La segunda, la \textbf{explicativa}, determina cómo se relacionan los fenómenos que se observan. Por ejemplo, cuando un científico de datos emplea un modelo de regresión lineal, lo que hace es establecer una relación explicativa entre la variable dependiente y las independientes a través de los coeficientes, (esta parte se conoce como ``Validación''
  en la metodología CRIPS-DM, ver Sección \ref{met-crisp-dm}).
\item
  La tercera, la \textbf{predictiva}, permite anticipar ciertos eventos en el tiempo. Tal es el caso de los científicos de datos que ejercen su labor en el ámbito comercial y emplean, por ejemplo, el análisis de series temporales para pronosticar las ventas futuras y poder realizar una planificación del aprovisionamiento de \emph{stocks} con mayor eficiencia,
  (esta parte está incluida en la fase de ``Modelado'' en la metodología CRIPS-DM, ver Sección \ref{met-crisp-dm}).
\end{itemize}

De todo lo expuesto hasta aquí, se puede sostener, sin lugar a dudas, que la ciencia de datos emplea el método científico y comparte las principales funciones de la ciencia. Ahora bien, la ciencia de datos no puede entenderse plenamente sin presuponer las otras disciplinas en las que se aplica. Por tanto, uno de los interrogantes que deberán resolver los futuros profesionales es si la ciencia de datos es un saber de primer orden que lidia directamente con la realidad, como la Física o la Química o si, por el contrario, es un saber de segundo orden, es decir, una suerte de disciplina que se sirve de otros saberes para desplegarlos y actualizarlos.

\begin{infobox_resume}
\textbf{RESUMEN}

Para determinar si la ciencia de datos es una ciencia en primer lugar se debe
consensuar la definición de ciencia, que va íntimamente ligada a la definición
de método científico.
Por otra parte, las ciencias tienen en común una serie de características,
que deben ser satisfechas por la ciencia de datos para adquirir el estatus de ciencia.

Del análisis anterior, se concluye que la ciencia de datos emplea el método científico y comparte las principales funciones de la ciencia, dejando la respuesta abierta al investigador.

\end{infobox_resume}

\hypertarget{metodologuxeda}{%
\chapter{Metodología en ciencia de datos}\label{metodologuxeda}}

\emph{Gema Fernández-Avilés y Ramón A. Carrasco}

\hypertarget{preliminares}{%
\section{Preliminares}\label{preliminares}}

En el Capítulo \ref{ciencia-datos} se ha puesto de manifiesto que el
método científico es el elemento que define la frontera de la ciencia.
\citet{Bunge_2018} al hablar del método científico, lo define como: ``\emph{Un
procedimiento para tratar un conjunto de problemas. Cada clase de
problemas requiere un conjunto de métodos o técnicas especiales. Los
problemas del conocimiento, a diferencia de los del lenguaje o los de la
acción, requieren la invención o la aplicación de procedimientos
especiales adecuados para los varios estadios del tratamiento de los
problemas\ldots{}}''. De acuerdo con su \textbf{concepción del método}, \citet{Bunge_2018}
destaca ocho operaciones necesarias en la aplicación de este:
\index{metodología}

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\item
  Enunciar preguntas bien formuladas y verosímilmente fecundas.
\item
  Arbitrar conjeturas, fundadas y contrastables con la experiencia,
  para contestar las preguntas.
\item
  Derivar consecuencias lógicas de las conjeturas.
\item
  Arbitrar técnicas para someter las conjeturas a contrastación.
\item
  Someter a contrastación esas técnicas para comprobar su relevancia y
  la fe que merecen.
\item
  Llevar a cabo la contrastación e interpretar sus resultados.
\item
  Estimar la pretensión de verdad de las conjeturas y la fidelidad
  de las técnicas.
\item
  Determinar los dominios en los cuales valen las conjeturas y las
  técnicas, y formular los nuevos problemas originados por la
  investigación.
\end{enumerate}

A su vez, sugiere una serie de reglas para la ejecución ordenada de las
operaciones anteriores:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\item
  Formular el problema con precisión y, al principio, específicamente.
\item
  Proponer conjeturas bien definidas y fundadas de algún modo, y no
  suposiciones que no comprometan en concreto ni tampoco con
  ocurrencias sin fundamento visible.
\item
  Someter las hipótesis a contrastación dura, no laxa.
\item
  No declarar verdadera una hipótesis satisfactoriamente confirmada;
  considerarla, en el mejor de los casos, como parcialmente
  verdadera.
\item
  Preguntarse por qué la respuesta es como es y no de otra manera.
\end{enumerate}

Sin embargo, de acuerdo con \citet{Montero_1997}, estas reglas no son
definitivas ni infalibles y necesitan de ulterior perfeccionamiento, que
se llevará a cabo a lo largo de la investigación científica. Además, las
reglas del método científico no son autosuficientes, necesitan apoyarse
en la inteligencia y creatividad humanas.

En resumen, es el tratamiento sistemático de los problemas de la forma
descrita, y no la certeza de los resultados obtenidos o la utilización
de las técnicas muy concretas y específicas, el que garantiza el
carácter científico de las conclusiones, \citet{cancelo_1991}. Y, la ciencia de
los datos, como no podía ser de otra forma, proporciona una serie de
metodologías que guían el trabajo de los científicos de datos. Las
principales metodologías se presentan a continuación.

\hypertarget{principales-metodologuxedas-en-ciencia-de-datos}{%
\section{Principales metodologías en ciencia de datos}\label{principales-metodologuxedas-en-ciencia-de-datos}}

\index{metodología!KDD} \index{metodología!SEMMA}
\index{metodología!CRISP-DM}

En un proyecto de ciencia de datos es muy importante la metodología,
pues proporciona al científico de datos una estrategia y un marco sobre
el que trabajar. Desde finales del siglo XX se han ido proponiendo
diversas metodologías enfocadas a la resolución de problemas concretos
de negocio mediante el uso de los datos que hoy podrían englobarse en el
paraguas común de la ciencia de datos. Destacan tres metodologías: (i)
\textbf{KDD}, propuesta por \citet{fayyad_1996} e inspirada en un trabajo previo de
\citet{brachman_1994}, fue la primera metodología aceptada por la comunidad
científica. Se trata del primer intento serio de sistematizar el proceso
conocido hoy día como ciencia de Datos y en aquellos tiempos como
conocimiento basado en bases de datos, pues se centraba en la minería de
datos. (ii) \textbf{SEMMA}, cuyas letras coinciden con la inicial de las
etapas de las que consta en inglés (Sample, Explore, Modify, Model, and
Assess) fue desarrollada y mantenida por el Instituto SAS en 2012. Se
define como el proceso de selección, exploración y modelización de
grandes bases de datos para descubrir patrones de negocio desconocidos.
Y (iii) \textbf{CRISP-DM}, planteada inicialmente en 1996 y publicada
formalmente en \citet{chapman_2000}, ha sido mantenida durante varios años por
la compañía SPSS, posteriormente adquirida por IBM que se ha encargado
de mantener y refinar la metodología hasta tiempo actuales. Esta
metodología define una secuencia flexible de seis fases que permiten la
construcción e implementación de un modelo de minería de datos para ser
utilizado en un entorno real, ayudando a apoyar decisiones de negocios.
Esta metodología, la más utilizada en la actualidad (\citet{shafique_2014};
\citet{azevedo_2008}, entre otros), se describe en la siguiente sección.

\hypertarget{met-crisp-dm}{%
\section{CRISP-DM para ciencia de datos}\label{met-crisp-dm}}

La metodología CRISP-DM está basada en seis etapas, representadas en la Fig. \ref(fig:crisp-dm) que no han variado
desde su formulación en 2000 y en una serie de funciones que se han sido
refinando en el tiempo en el tiempo \href{https://www.ibm.com/docs/es/spss-modeler/saas?topic=dm-crisp-help-overview}{CRISP-DM
(2021)}.
Las etapas del CRISP-DM de manera esquemática son:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Entendimiento del negocio}. Fundamental para el éxito del mismo.
  Consta de cuatro fases:

  \begin{itemize}
  \item
    \emph{Determinación de los objetivos de negocio}, consensuados
    previamente con la organización. Es importante fijar los
    correspondientes KPIs (Key Performance Indicators) que permitan
    medir fidedignamente dichos objetivos.
  \item
    \emph{Evaluación de la situación actual}, inventariar las fuentes de
    datos que estarán disponibles, los recursos materiales y humanos
    que se dispondrán, los factores de riesgo y el plan de
    contingencia para los mismos.
  \item
    \emph{Determinación de los objetivos del proyecto}, que debe
    traducirse al correspondiente rendimiento de los modelos (por
    ejemplo, cual debe de ser su nivel de precisión).
  \item
    \emph{Plan del proyecto}, con sus procesos a realizar y recursos
    asignados.
  \end{itemize}
\item
  \textbf{Comprensión de los datos}. Es necesario el completo entendimiento
  de la materia prima (los datos). Está formada por cuatro fases que
  giran en torno a los datos:

  \begin{itemize}
  \item
    \emph{Recopilación}, tanto de datos internos como externos a la
    organización si son necesarios, incluso se generan datos
    adicionales, el etiquetado de casos no clasificados con
    anterioridad.
  \item
    \emph{Descripción}, especificando aspectos como la cantidad de datos
    disponibles, anticipando posibles problemas de rendimiento,
    tipología de las variables (numéricas, categóricas, booleanas,
    etc), codificación de las mismas (especialmente para las
    categóricas), etc.
  \item
    \emph{Exploración}, a tavés del AED (Análisis Exploratorio de Datos).
    Esta tarea ayuda a formular hipótesis sobre los datos y dirige
    las posteriores etapas de preparación y modelado.
  \item
    \emph{Verificación de la calidad}, buscando problemas como valores
    perdidos, errores de datos (por ejemplo, tipográficos), errores
    de las mediciones (datos que son correctos en su introducción
    pero sus unidades de medida no lo son), incoherencias en la
    codificación (especialmente para variables categóricas).
  \end{itemize}
\item
  \textbf{Preparación de los datos}. Esta etapa suele ser la más costosa en
  tiempo y esfuerzo del proyecto (frecuentemente más de 60\%). Consta
  de cinco fases:

  \begin{itemize}
  \item
    \emph{Selección}, se toman decisiones sobre los casos o filas a
    seleccionar y sobre los atributos (variables) o columnas a
    incluir.
  \item
    \emph{Limpieza}, si en la subfase de verificación de la calidad de
    los datos se han detectado problemas ahora hay que subsanarlos.
    Los valores perdidos se pueden excluir o interpolar; los errores
    de datos, se pueden corregir con algún esquema lógico o
    manualmente; incoherencias en la codificación, se podría
    recodificar sustituyendo los datos originales.
  \item
    \emph{Construcción}, a partir de los ya disponibles, nuevos atributos
    (variables) o columnas y nuevas filas o registros.
  \item
    \emph{Integración}, necesaria para para construir un concepto de
    negocio unificado (por ejemplo, el concepto de cliente) si se
    han usado diversas fuentes (tiquet de compra y registos de
    cliente). La fusión de
    columnas con algunas claves en común (\emph{join}), adición de filas
    con las columnas en común (union), agrupación, etc, se utilizan frecuentemente.
  \item
    \emph{Formateo}, orientada a las necesidades de los posteriores
    modelos a usar. La conversión de variables
    categóricas a numéricas (usando técnicas de \emph{one hot encoding}),
    la normalización (usando normalizaciones min-max o z-score),
    etc, son tareas comunes en esta etapa.
  \end{itemize}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\setcounter{enumi}{3}
\item
  \textbf{Modelado}, se trata de que los modelos ingieran dichos datos y
  aprendan de ellos a resolver el problema de negocio planteado. Las
  fases de las que consta esta etapa son:

  \begin{itemize}
  \item
    \emph{Selección de técnicas de modelado}, por ejemplo, si se va a usar
    Machine Learning supervisado o no supervisado, y tipo de
    algoritmos a usar más especificadamente dentro de cada una de
    esta técnicas. Por supuesto, se tienen en cuenta los requisitos
    fijado en la primera fase, la cantidad y tipo de datos de los
    que disponemos, los requisitos específicos de cada modelo, etc.
  \item
    \emph{Generación de un diseño de comprobación}, a través de medidas y
    criterios de bondad del modelo (curva ROC, AIC, \(R^2\),
    Matriz de confusión, etc).
  \item
    \emph{Generación de los modelos}, se suelen generar diversos modelos
    entrenándolos oportunamente para seleccionar posteriormente el
    más adecuado.
  \item
    \emph{Evaluación del modelo}, en base a los modelos generados y al
    plan de pruebas especificado.
  \end{itemize}
\item
  \textbf{Evaluación}. Se debe comprobar que el modelo final generado
  cumple las expectativas de negocio especificadas en la primera fase.
  Hay que hacer hincapié en este aspecto ya que suele confundir en la
  práctica esta fase de evaluación con la subfase de la anterior etapa
  de evaluación del modelo. Ahora la validación es desde el punto de
  vista de negocio. Así, por ejemplo, cabe plantearse si con el modelo
  elegido se pueden alcanzar las metas de negocio especificadas y
  medidas con los correspondientes KPIs. Tras esta evaluación de los
  resultados del modelo se abre un proceso de revisión que nos permita
  valorar si finalmente el modelo cumple las expectativas o tenemos
  que volver a etapas anteriores.
\item
  \textbf{Implementación}. El conocimiento obtenido con el modelado es
  puesto en valor en esta fase de cara a satisfacer los objetivos de
  negocio planteados en el proyecto. Este despliegue depende mucho
  del tipo de proyecto que se esté realizando aunque generalmente
  incluye las actividades siguientes:

  \begin{itemize}
  \item
    \emph{Planificación del despliegue} del modelado y/o el conocimiento
    obtenido.
  \item
    \emph{Planificación del control y del mantenimiento}, así, por ejemplo.
    hay que verificar que el modelo está cumpliendo con las
    expectativas para las que se ha desarrollado, chequear si el
    modelo hay que reentrenarlo o sustituirlo por otro modelo, etc.
  \item
    \emph{Creación del informe final}, que se puede usar para comunicar
    los resultados del proyecto y pasos siguientes.
  \item
    \emph{Revisión final del proyecto}, donde se establecen las
    conclusiones finales y se formaliza las lecciones aprendidas
    para incorporarlas a futuros proyectos de ciencia de datos.
  \end{itemize}
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/crisp-dm2} 

}

\caption{Etapas de la metodología CRISP-DM.}\label{fig:crisp-dm}
\end{figure}

Para concluir, subrayar que aunque son varias las metodologías
propuestas, existe cierto consenso en que CRISP-DM es la metodología más
completa y la más desarrollada y, además, puede ser implementada (como
todas las propuestas en la literatura) usando el lenguaje \textbf{R} como
principal herramienta para la ciencia de Datos.

\begin{infobox_resume}
\textbf{RESUMEN}

El método científico un elemento clave en la definición de ciencia.
\citet{Bunge_2018} establece una serie de rellas y características para
la correcta aplicacion de la metodología. Así como la ciencia se
nutre del método científico la ciencia de datos se alimenta de
su propipa metodología. Son varías las metodologías propuestas,
destacando el CRISP-DM como la más aceptada y utilizada por las
emprsas y científicos. CRISP-DP se basa en la organización flexible
de seis pilares: entendimiento del negocio, compresión de los datos,
preparación de los datos, modelado, evaluación e implementación.

\end{infobox_resume}

\hypertarget{ch-110003}{%
\chapter{Ciencia de datos con R}\label{ch-110003}}

\emph{Emilio L. Cano}

\hypertarget{introducciuxf3n}{%
\section{Introducción}\label{introducciuxf3n}}

\index{R}
\index{Internet of Things}
\index{Big Data}

El análisis estadístico de datos es una tarea fundamental
en la transformación digital de las empresas y organizaciones.
Siempre ha estado ahí, pero
en la actualidad la disponibilidad de datos, la cantidad de los mismos,
y la velocidad con la que se requieren resultados, está haciendo
necesario el capacitar a los profesionales
para el análisis de datos con nuevas herramientas. Nuevas tendencias (muchas veces
malinterpretadas) como \emph{Big Data}, Industria 4.0, \emph{Internet of Things} (IoT),
o \emph{Data Science}, aumentan el interés por parte de las empresas, los profesionales
y los investigadores en estas técnicas.

El tratamiento de datos y su análisis requiere el uso de
software avanzado. Aunque algunas tareas se puede realizar
eficazmente con programas de hoja de cálculo
como Excel (por ejemplo, son una buena herramienta para mecanizar y almacenar datos),
se debería
utilizar software especializado para el análisis de datos. Existen distintos
paquetes estadísticos comerciales, como SPSS, Statgraphics, Stata, JMP o Minitab. En los últimos
años se ha abierto camino como alternativa el \href{http://www.r-project.org}{software estadístico y lenguaje de
programación \textbf{R}} \citep{R-base}. Hay otras alternativas que en su mayoría, o son parciales referidas a un
ámbito concreto, o son más lenguajes de programación que software estadístico, como Python.
\textbf{R} es software libre, pero su gratuidad solo es una
de sus ventajas (free as in \emph{free beer}, and free as
in \emph{free speech}), como se verá a lo largo del libro. El gran inconveniente
es la curva de aprendizaje: no es tan fácil de aprender y usar como un
software de ventanas, ya que el uso de \textbf{R} se basa en expresiones que
hay que ejecutar desde scripts.

\textbf{R} es un
sistema para \textbf{computación estadística}: software de \textbf{análisis de datos} y
\textbf{lenguaje de programación}. Ha sido ampliamente utilizado en investigación y docencia,
y actualmente también
en las empresas y organismos públicos. Es la evolución del trabajo de los laboratorios Bell con el
lenguaje S \citep{venables2002modern},
llevado al mundo del software libre por Ross Ihaka y Robert Gentleman en los años 90 \citep{ihaka1996r}.
La version R 1.0.0 se publicó el 29 de febrero de 2000.

Uno de los aspectos más espectaculares de \textbf{R} es la cantidad de \textbf{paquetes}
disponibles. Un paquete (\emph{package}) de R es un componente con funcionalidad adicional que se puede
instalar en el sistema para ser utilizado por \textbf{R}. En el momento de
compilar este libro el número de paquetes disponibles en el repositorio oficial
es de 18974.

Una vez conocido el mundo de R se plantea la siguiente pregunta, ¿y por qué
utilizar \textbf{R}? Es imposible dar un único motivo, a continuación se enumeran
algunas de las ventajas que tiene utilizar \textbf{R}:

\begin{itemize}
\tightlist
\item
  Es Free and Open Source Software (FOSS). Gratis y libre. En inglés se suele decir \emph{free as in free beer, and free as in free speech}.
\item
  Amplia comunidad de usuarios que proporciona recursos.
\item
  Es multiplataforma.
\item
  Se usa cada vez en más empresas e instituciones.
\item
  Es posible obtener soporte comercial, por ejemplo a través de Posit Software PBC\footnote{\url{https://posit.co}, antes RStudio PBC}.
\item
  Se ha alcanzado una masa crítica de usuarios que lo hace confiable.
\item
  Es extensible (desde pequeñas funciones, hasta paquetes).
\item
  Se puede implementar la innovación inmediatamente. En software comercial hay que esperar a nuevas versiones, en el mejor de los casos.
\item
  Posee características de ``investigación reproducible''. Veremos más adelante
  qué implica este enfoque. En contextos distintos a la investigación, podemos hablar de informes reproducibles y trazabilidad del análisis.
\end{itemize}

Por otra parte, el uso de \textbf{R} en las empresas está \textbf{creciendo exponencialmente} debido principalmente a la necesidad de analizar y visualizar datos con herramientas potentes para explotar todo su potencial. Grandes empresas de todos los sectores llevan tiempo utilizándolo, si bien la popularización del software y su conocimiento entre los nuevos titulados está facilitando que empresas de todo tipo y tamaño aprovechen esta herramienta en su estrategia digital. Así, además de la \textbf{visualización y presentación efectiva} de los datos, equipos bien formados pueden descubrir relaciones entre variables clave, realizar \textbf{predicciones}, tomar mejores decisiones o \textbf{mejorar sus procesos} gracias al análisis avanzado de datos más allá de la hoja de cálculo.

\hypertarget{110003-bases}{%
\section{\texorpdfstring{La sesión de \textbf{R}}{La sesión de R}}\label{110003-bases}}

R es una aplicación de análisis estadístico
y representación gráfica de datos, y además un lenguaje de programación.
\textbf{R} es \textbf{interactivo}, en el sentido de que
responde a través de un \emph{intérprete} a las \textbf{entradas} que recibe
a través de la \textbf{consola}.

El interfaz de usuario de R (R GUI, \emph{Graphical User Interface}) cumple las funciones básicas para
interactuar con \textbf{R}, pero es muy pobre a la hora de trabajar con él. En su lugar,
es más conveniento utilizar el entorno de desarrollo \href{http://www.rstudio.org}{RStudio Desktop}, (o su versión en la nube \url{https://posit.cloud/})
que es como un
\emph{envoltorio} del sistema R con más funcionalidades y ayudas, pero
manteniendo el mismo nivel de interacción: consola y scripts (archivos de código)\footnote{Lo importante
  es seguir un estilo consistente en cuanto a nombres de objetos, espacios
  en blanco y distribución del código en el script, véase por ejemplo
  la \href{http://adv-r.had.co.nz/Style.html}{guía de estilo de Hadley Wickam}
  \citep{wickham2015advanced}.},\footnote{Una cosa muy importante en \textbf{R}, es que las expresiones son \textbf{sensibles a mayúsculas},
  y por tanto los objetos \texttt{datos} y \texttt{Datos} son distintos.}. Al igual que \textbf{R}, RStudio es una aplicación de software libre, pero en este caso desarrollada y mantenida por una compañía privada, Posit PBC.

\hypertarget{instalaciuxf3n-de-r}{%
\section{\texorpdfstring{Instalación de \textbf{R}}{Instalación de R}}\label{instalaciuxf3n-de-r}}

Durante todo el libro se utiliza el interfaz RStudio. Pero RStudio
es solo un ``envoltorio'' de \textbf{R}, por lo que previamente hay que tener instalado
en el ordenador el sistema ``base'' de \textbf{R}. \textbf{R} está disponible para sistemas
Windows, Mac y Linux. Por cuestiones de espacio no se incluyen detalles en este libro, pero la instalación es sencilla siguiendo las instrucciones en sus correspondientes websites:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Instalación de \textbf{R}: \url{http://www.r-project.org}
\item
  Instalación de RStudio: \url{http://rstudio.com}
\end{enumerate}

Para completar la instalación de \textbf{R}, se muestra cómo instalar\footnote{Una vez instalado un paquete no hay que volver a instalarlo.} algunos paquetes de ejemplo mediante expresiones en la consola o script con la función \texttt{install.packages()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\AttributeTok{pkgs =} \StringTok{"tidyverse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Una vez instalado el paquete, se cargará con la instrucción \texttt{library("nombre\_paquete")} en la sesión de R donde queramos utilizarlo.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A veces resulta útil usar directamente
la función que se va a utilizar en vez de cargar todo el paquete. Esto se hace con el operador \texttt{::}, es decir, \texttt{nombre\_paquete::funcion()}. La siguiente expresión serviría para usar
la función \texttt{select()} del paquete \texttt{dplyr} sin cargar el paquete entero.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{110003-proyectos}{%
\section{Trabajar con proyectos de RStudio}\label{110003-proyectos}}

La manera más eficiente de trabajar con \textbf{R}, es mediante proyectos
de RStudio. Esto permite abstraerse de los detalles de la sesión de R
(espacio de trabajo, directorio de trabajo, \emph{Environment}) ya que al abrir
un proyecto, estará todo preparado para seguir el trabajo donde se dejó,
o empezar de cero si se acaba de crear.
Para crear un proyecto de RStudio, desplegamos el menú de proyectos a la derecha en la barra de herramientas y seleccionamos ``New Project\ldots{}'' También podemos hacerlo en el menú ``File/New Project\ldots{}''.

Es aconsejable crear siempre una estructura
de carpetas que permita tenerlo todo organizado desde el principio,
porque al final los proyectos crecen. La estructura perfecta no existe,
y depende del proyecto en particular. Las siguientes carpetas pueden ser
útiles en un amplio abanico de proyectos, y
las tres primeras se pueden usar prácticamente en cualquier proyecto:

\begin{itemize}
\tightlist
\item
  \includegraphics[width=1em,height=1em]{Ciencia_de_datos_con_r_files/figure-latex/fa-icon-816289cbefb2f9f4bc7e08a916884bf2.pdf} \texttt{datos}: en esta carpeta tendremos los archivos de
  datos, tanto aquellos orígenes de datos que queramos importar, como los
  que podamos guardar desde un script.
\item
  \includegraphics[width=1em,height=1em]{Ciencia_de_datos_con_r_files/figure-latex/fa-icon-816289cbefb2f9f4bc7e08a916884bf2.pdf} \texttt{R}: para los scripts. Es posible que solamente tengamos un script
  en nuestro proyecto, pero si tuviéramos más los podemos meter en esta carpeta.
\item
  \includegraphics[width=1em,height=1em]{Ciencia_de_datos_con_r_files/figure-latex/fa-icon-816289cbefb2f9f4bc7e08a916884bf2.pdf} \texttt{informes}: aquí podemos guardar los archivos Quarto o R Markdown
  que usemos para generar informes o presentaciones.
\item
  \includegraphics[width=1em,height=1em]{Ciencia_de_datos_con_r_files/figure-latex/fa-icon-816289cbefb2f9f4bc7e08a916884bf2.pdf} \texttt{img}: si en nuestro proyecto vamos a tener imágenes
  de cualquier tipo, es una buena idea tenerlas en una carpeta aparte.
\item
  \includegraphics[width=1em,height=1em]{Ciencia_de_datos_con_r_files/figure-latex/fa-icon-816289cbefb2f9f4bc7e08a916884bf2.pdf} \texttt{test}: si queremos separar los scripts en los que
  hacemos pruebas pero no queremos mezclarlos con los ``buenos'' en la carpeta \texttt{scripts}.
\item
  \includegraphics[width=1em,height=1em]{Ciencia_de_datos_con_r_files/figure-latex/fa-icon-841ecb1734e7cbe7af1b18addbe114b1.pdf} \texttt{aux}, \texttt{tmp}, \texttt{util}, \texttt{notas}, doc, \ldots:
  este tipo de carpetas vienen bien cuando
  hay información que está relacionada o es útil para un proyecto, pero no son
  del proyecto de análisis de datos en sí. Por ejemplo, unas especificaciones de
  un producto o servicio, un artículo científico, fotografías de una fábrica,
  comunicaciones con clientes, etc.
\item
  \includegraphics[width=1em,height=1em]{Ciencia_de_datos_con_r_files/figure-latex/fa-icon-841ecb1734e7cbe7af1b18addbe114b1.pdf} \texttt{ejercicios}, \texttt{practicas}, \ldots: si nuestro proyecto
  forma parte de una asignatura, curso, o similar.
\end{itemize}

\begin{infobox}
Un aspecto importante cuando se trabaja en proyectos colaborativos es el control de versiones. Este tema se aborda en el Capítulo \ref{github}.

\end{infobox}

\hypertarget{tratamiento-de-datos-con-r}{%
\section{\texorpdfstring{Tratamiento de datos con \textbf{R}}{Tratamiento de datos con R}}\label{tratamiento-de-datos-con-r}}

En este apartado se van a empezar a utilizar expresiones de R. Muchas de las expresiones que usamos son llamadas a funciones. Por motivos de espacio no se incluyen mayores explicaciones de las mismas, pero se anima al lector a explorar la ayuda de cada una de ellas para comprender mejor su funcionamiento. La ayuda de cualquier función se puede obtener en la consola usando la expresión \texttt{?funcion}, donde \texttt{funcion} es el nombre de la función u objeto del que se quiere obtener ayuda.

\hypertarget{110003-estructuras}{%
\subsection{Estructuras y tipos de datos}\label{110003-estructuras}}

Las estructuras y tipos de datos más frecuentes con las que se trabaja en R son:

\begin{itemize}
\tightlist
\item
  \textbf{Tablas de datos}: es una colección de variables numéricas y/o atributos
  organizadas en columnas,
  en la que cada fila se corresponde con algún elemento en el que se han
  observado las características que representan las variables. La forma más común
  es el \texttt{data.frame}. Cada columna del \texttt{data.frame} es en realidad otra estructura de datos, en concreto, un \texttt{vector}. Un ejemplo de data.frame es el conjunto de datos \texttt{tempmin\_data} del paquete \texttt{CDR} que se analiza en el Capítulo \ref{datos-espaciales}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(CDR)}
\FunctionTok{head}\NormalTok{(tempmin\_data)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, ]}
\CommentTok{\#\textgreater{}         fecha indicativo tmin  longitud  latitud}
\CommentTok{\#\textgreater{} 1: 2021{-}01{-}06      4358X {-}4.7 {-}5.880556 38.95556}
\CommentTok{\#\textgreater{} 2: 2021{-}01{-}06      4220X {-}7.0 {-}4.616389 39.08861}
\CommentTok{\#\textgreater{} 3: 2021{-}01{-}06      6106X  4.7 {-}4.748333 37.02944}
\end{Highlighting}
\end{Shaded}

Un \texttt{data.frame} es un objeto de datos en dos dimensiones, en el que las filas
son la dimensión 1, y las columnas la dimensión 2. Podemos ``extraer'' los datos
de un \texttt{data.frame} por filas, por columnas, o por celdas. Para extraer una
de las variables del \texttt{data.frame} se utiliza utilizar el operador \texttt{\$} después del
nombre del data.frame, y a continuación el nombre de la variable.

El operador \texttt{\textless{}-} asigna al ``símbolo'' que hay a su izquierda, el resultado
de la expresión que hay a su derecha, y lo guarda con ese nombre
en el espacio de trabajo. Por ejemplo, la siguiente expresión extrae todas las filas de la
columna \texttt{tmin} o, dicho de otra forma, el vector con todas las temperaturas mínimas registradas y lo guarda en el objeto \texttt{temp\_min}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{temp\_min }\OtherTok{\textless{}{-}}\NormalTok{ tempmin\_data}\SpecialCharTok{$}\NormalTok{tmin}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Vectores y matrices}: Ya se ha visto que una columna de una tabla de datos es un vector. También se pueden crear vectores con la función \texttt{c} y los elementos del vector separados por comas. A modo de ejemplo, la primera de las siguientes expresiones crea un vector llamado \texttt{dimensiones} con dos cadenas de texto, y la segunda crea una matriz numérica llamada \texttt{coordenadas} a partir de las columnas del data.frame \texttt{tempmin\_data}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nombres }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"longitud"}\NormalTok{, }\StringTok{"latitud"}\NormalTok{)}
\NormalTok{coordenadas }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(tempmin\_data[, }\DecValTok{4}\SpecialCharTok{:}\DecValTok{5}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

Nótese que la extracción de valores de un data.frame o de una matriz se puede realizar también por sus índices de filas y columnas. En este caso se extraen todas las filas de las columnas 4 y 5.

\begin{itemize}
\tightlist
\item
  \textbf{Factor:} es un tipo especial de vector para representar variables de tipo atributo o categóricas. En general, la característica podrá tomar un reducido
  número de valores diferentes, identificados con etiquetas (\emph{labels}) y que llamaremos
  \textbf{niveles} del factor (\emph{levels}).
\end{itemize}

Un ejemplo es el dataset \texttt{dp\_entr} del paquete \texttt{CDR} que se analiza en el Capítulo @ref(cap\_arboles). La columna \texttt{ind\_pro11} es un indicador que toma los valores \texttt{S} y \texttt{N}, mientras que \texttt{des\_nivel\_edu} toma tres posibles valores.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dp\_entr[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{17}\NormalTok{)]}
\CommentTok{\#\textgreater{}     ind\_pro11 des\_nivel\_edu}
\CommentTok{\#\textgreater{} 1           S         MEDIO}
\CommentTok{\#\textgreater{} 497         N         MEDIO}
\CommentTok{\#\textgreater{} 265         N        BASICO}
\CommentTok{\#\textgreater{} 534         N         MEDIO}
\CommentTok{\#\textgreater{} 415         N        BASICO}
\FunctionTok{levels}\NormalTok{(dp\_entr}\SpecialCharTok{$}\NormalTok{des\_nivel\_edu)}
\CommentTok{\#\textgreater{} [1] "ALTO"   "BASICO" "MEDIO"}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Listas}: son estructuras de datos que contienen una
  colección de elementos indexados, que pueden además tener un
  nombre.
  Pueden ser heterogéneas
  en el sentido de que cada elemento de la lista puede ser
  de cualquier tipo.
\end{itemize}

A modo de ejemplo, se muestran los nombres del objeto \texttt{tempmax\_data} del paquete \texttt{CDR}, que contiene 6 elementos de distintas clases.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(tempmax\_data)}
\CommentTok{\#\textgreater{} [1] "ESP"             "ESP\_utm"         "grd\_sf"          "grd\_sp"         }
\CommentTok{\#\textgreater{} [5] "temp\_max\_utm\_sf" "temp\_max\_utm\_sp"}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Fechas}: son un tipo de datos especial que algunas veces provoca problemas
  al compartir datos entre programas. El \texttt{data.frame} \texttt{tempmin\_data}tiene la columna \texttt{fecha} que puede convertirse de manera inmediata a tipo fecha (\texttt{Date}) porque viene en un formato estándar. Véase la ayuda de \texttt{strptime} para especificar otros formatos.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tempmin\_data}\SpecialCharTok{$}\NormalTok{fecha }\OtherTok{\textless{}{-}} \FunctionTok{as.Date}\NormalTok{(tempmin\_data}\SpecialCharTok{$}\NormalTok{fecha)}
\FunctionTok{class}\NormalTok{(tempmin\_data}\SpecialCharTok{$}\NormalTok{fecha)}
\CommentTok{\#\textgreater{} [1] "Date"}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Cadenas de texto}: son estructuras de datos que aparecen como vector de caracteres. La columna \texttt{indicativo} del conjunto de datos \texttt{tempmin\_data} es un ejemplo de este tipo de datos.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(tempmin\_data}\SpecialCharTok{$}\NormalTok{indicativo)}
\CommentTok{\#\textgreater{} [1] "4358X" "4220X" "6106X" "9698U" "4410X" "1331A"}
\end{Highlighting}
\end{Shaded}

\hypertarget{110003-importacion}{%
\subsection{Importación de datos}\label{110003-importacion}}

En el apartado anterior se han utilizado tablas de datos que están incluidas en el un paquete de R. Pero lo habitual es que se tenga que importar datos para su análisis de fuentes externas, como ficheros. A continuación se describen las formas más habituales\footnote{Para poder reproducir los ejemplos, se debe tener una carpeta \texttt{data} en el directorio de trabajo.}.

\begin{itemize}
\tightlist
\item
  \textbf{Excel}: Sin duda una forma muy popular de organizar los datos en ficheros es
  mediante \textbf{hojas de cálculo como Microsoft Excel}.
  Hay varios paquetes con los que se puede trabajar con archivos de Excel. En este libro se utiliza
  el paquete \texttt{readxl} del \texttt{tidyverse}. Con la siguiente expresión se puede descargar un archivo Excel de ejemplo.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{download.file}\NormalTok{(}
  \AttributeTok{url =} \StringTok{"http://emilio.lcano.com/b/adr/p/datos/RRHH.xlsx"}\NormalTok{,}
  \AttributeTok{destfile =} \StringTok{"data/RRHH.xlsx"}\NormalTok{,}
  \AttributeTok{mode =} \StringTok{"wb"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Una vez el archivo está en el directorio de trabajo de la sesión de R, se puede importar su contenido al espacio de trabajo con la siguiente expresión:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rrhh }\OtherTok{\textless{}{-}}\NormalTok{ readxl}\SpecialCharTok{::}\FunctionTok{read\_excel}\NormalTok{(}\StringTok{"data/RRHH.xlsx"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Texto}: Los \textbf{archivos de texto} son el formato más universal para compartir datos. Es también
  muy común que el equipamiento o el software genere datos en formato de texto. Estos
  archivos suelen tener extensión \texttt{.csv} (\emph{comma separated values}) o \texttt{.txt}, aunque
  puede tener cualquier otro o incluso no tener extensión. Con la siguiente expresión se puede descargar un archivo csv de ejemplo.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{download.file}\NormalTok{(}
  \AttributeTok{url =} \StringTok{"http://emilio.lcano.com/b/adr/p/datos/ejDatos.csv"}\NormalTok{,}
  \AttributeTok{destfile =} \StringTok{"data/ejDatos.csv"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Si el archivo tiene extensión \texttt{.csv} como este vendrá
ya con una especificación muy concreta y se pueden usar directamente las funciones
\texttt{read.csv} o \texttt{read.csv2} para tener la tabla de datos en el espacio de trabajo.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{merma }\OtherTok{\textless{}{-}} \FunctionTok{read.csv2}\NormalTok{(}\StringTok{"data/ejDatos.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

La función genérica de R para importar datos de texto es \texttt{read.table}, y
se puede importar cualquier especificación cambiando los argumentos adecuados.
Por ejemplo, la siguiente expresión tendría el mismo resultado que se ha
obtenido con la función \texttt{read.csv2}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{merma }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}
  \AttributeTok{file =} \StringTok{"data/ejDatos.csv"}\NormalTok{,}
  \AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{sep =} \StringTok{";"}\NormalTok{,}
  \AttributeTok{dec =} \StringTok{","}\NormalTok{,}
  \AttributeTok{fileEncoding =} \StringTok{"utf{-}8"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{infobox}
Para saber cómo importar datos desde sistemas gestores de bases de datos ver el
Capítulo \ref{datos-sql}.

\end{infobox}

\hypertarget{exportaciuxf3n-y-archivos-de-datos-de-r}{%
\subsection{Exportación y archivos de datos de R}\label{exportaciuxf3n-y-archivos-de-datos-de-r}}

En algunos proyectos es necesario guardar algunos datos que se han ido creando o
transformando, bien para compartir con otras partes interesadas, bien para
ser utilizados en el mismo u otros proyectos.
Para exportar los datos en excel, se utiliza la función \texttt{write.xlsx} del paquete \texttt{openxlsx} (si no está instalado, lo instalamos de la forma habitual).
Si lo que queremos es exportarlo a texto, podemos utilizar los equivalentes
a las funciones de importación \texttt{write.csv}, \texttt{write.csv2} o \texttt{write.table}.

La siguiente expresión exporta la tabla de datos \texttt{tempmin\_data} a ficheros Excel y csv (formato en inglés).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{openxlsx}\SpecialCharTok{::}\FunctionTok{write.xlsx}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ tempmin\_data,}
  \AttributeTok{file =} \StringTok{"data/temp\_min\_Filomena.csv"}
\NormalTok{)}
\FunctionTok{write.csv}\NormalTok{(}\AttributeTok{x =}\NormalTok{ tempmin\_data, }\AttributeTok{file =} \StringTok{"data/temp\_min\_Filomena.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Tambien se pueden guardar los datos en formato ``nativo'' de R:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Archivos \texttt{.RData}: Almacenan un espacio de trabajo entero, y por tanto
  se pueden guardar varios objetos en el mismo archivo. Al importarlo,
  los objetos estarán en el espacio de trabajo con su nombre original.
  Se guardan con la función \texttt{save} y se restauran con la función \texttt{load}, como en el siguiente ejemplo.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{save}\NormalTok{(tempmin\_data, tempmax\_data,}
  \AttributeTok{file =} \StringTok{"data/datos\_temperaturas.RData"}
\NormalTok{)}
\FunctionTok{load}\NormalTok{(}\StringTok{"data/datos\_temperaturas.RData"}\NormalTok{) }\CommentTok{\# carga de nuevo el objeto}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Archivos \texttt{.rds}: Almacenan un único objeto en un archivo. Para importarlos,
  hay que asignar el resultado al nombre que queramos. Se guardan con la
  función \texttt{writeRDS} y se restauran con la función \texttt{readRDS}, como en el siguiente ejemplo.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{saveRDS}\NormalTok{(}
  \AttributeTok{object =}\NormalTok{ tempmin\_data,}
  \AttributeTok{file =} \StringTok{"data/datos\_temperaturas.rds"}
\NormalTok{)}
\NormalTok{nuevo\_objeto }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\AttributeTok{file =} \StringTok{"data/datos\_temperaturas.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

El paquete \texttt{haven} de R base y otros paquetes especializados pueden
exportar datos a otros formatos de archivo, que no da tiempo a tratar
en detalle en este capítulo. Por otra parte, el paquete \texttt{rvest}, que forma parte
del tidyverse, se puede utilizar para obtener datos de páginas web y otras fuentes de Internet, lo que se suele llamar \emph{web scraping}. Por ejemplo, supongamos que queremos importar la tabla con los datos de comunidades y ciudades autónomas del enlace \url{https://es.wikipedia.org/wiki/Anexo:Comunidades_y_ciudades_autónomas_de_España}.
Las siguientes expresiones importan esta tabla al data.frame \texttt{ccaa\_wiki}.

\textbf{GEMA DICE: Emilio, hay que poner un enlace alternativo o partirlo para que no se salga}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rvest)}
\NormalTok{url }\OtherTok{\textless{}{-}} \StringTok{"https://es.wikipedia.org/wiki/Anexo:Comunidades\_y\_ciudades\_autónomas\_de\_España"}
\NormalTok{ccaa\_wiki }\OtherTok{\textless{}{-}}\NormalTok{ url }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{read\_html}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{html\_node}\NormalTok{(}\AttributeTok{xpath =} \StringTok{"/html/body/div[3]/div[3]/div[5]/div[1]/table"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{html\_table}\NormalTok{(}\AttributeTok{fill =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Alternativas, por ejemplo:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{url }\OtherTok{\textless{}{-}} \StringTok{"https://es.wikipedia.org/wiki/Anexo:Tabla\_peri\%C3\%B3dica"}
\NormalTok{url }\OtherTok{\textless{}{-}} \StringTok{"https://es.wikipedia.org/wiki/Anexo:Pa\%C3\%ADses"}
\end{Highlighting}
\end{Shaded}

La ruta o ``xpath'' se puede obtener usando las herramientas de desarrollo del navegador, y puede que una vez importada la tabla se requiera algún post-procesamiento antes de poder analizar los datos.

\hypertarget{110003-tidyverse}{%
\section{\texorpdfstring{Organización de datos con el \emph{tidyverse}}{Organización de datos con el tidyverse}}\label{110003-tidyverse}}

\hypertarget{el-tidyverse-y-su-flujo-de-trabajo}{%
\subsection{\texorpdfstring{El \emph{tidyverse} y su flujo de trabajo}{El tidyverse y su flujo de trabajo}}\label{el-tidyverse-y-su-flujo-de-trabajo}}

El \emph{tidyverse} es, según se define en su propia \href{http://tidyverse.org}{página web},
``\emph{an opinionated collection of R packages designed for data science}''. Es decir,
un conjunto de paquetes de R ``opinables'' diseñados para ciencia de datos.
Las principales ventajas (opinables) de utilizar el \emph{tidyverse} son tres:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Se utiliza una gramática, estructuras de datos y filosofía de diseño común.
\item
  El flujo de trabajo es más fluido y, una vez se comprenden las ideas principales, más intuitivo.
\item
  Para la mayoría de las operaciones, es computacionalmente más eficiente.
\end{enumerate}

Uno de los paquetes más populares del \texttt{tidyverse} es \texttt{ggplot2}, que proporciona una ``gramática de gráficos''
\citep{Wickham2016}, y es pieza clave en el \texttt{tidyverse} actual, junto con los paquetes \texttt{dplyr} y \texttt{tidyr}.
El flujo de trabajo propuesto por el \emph{tidyverse} se describe en el libro
``R for Data Science'' \citep{wickham2016r}, y se sintetiza en la figura \ref{fig:flujods}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/data-science} 

}

\caption{Flujo de trabajo en Data Science (fuente: R for Data Science, ver bibliografía)}\label{fig:flujods}
\end{figure}

Además del mencionado libro, la web del tidyverse (\url{http://tidyverse.org}) contiene
toda la documentación de los paquetes, incluidos artículos para tareas concretas.
que merece la pena leer alguna vez. Además, están disponibles las famosas \emph{cheatsheets}
que están también enlazadas en esa documentación, y en la ayuda de RStudio (menú
\emph{Help/Cheatsheets}).

Dentro del flujo de trabajo de la figura \ref{fig:flujods}, ya se ha tratado
la primera etapa (\emph{Import}) en el apartado \ref{110003-importacion}. Es importante
señalar que, al utilizar las funciones del \emph{tidyverse}, los datos se organizan
en objetos de clase \texttt{tibble}, que es una extensión del \texttt{data.frame} de R base.
Las principales diferencias son:

\begin{itemize}
\tightlist
\item
  Permite una representación compacta en la consola al mostrar la tabla de datos.
\item
  La selección con corchetes simples de una única variable siempre devuelven
  otro \texttt{tibble} (a diferencia de un \texttt{data.frame}, que devuelve un vector).
\end{itemize}

Se puede forzar a que una tabla de datos sea de un tipo u otro con las funciones \texttt{as.data.frame} (de tibble a data.frame) y \texttt{as\_tibble} (de data.frame a tibble).

Siguiendo con el esquema de la figura \ref{fig:flujods}, en este apartado se verán las etapas \emph{Tidy} (organizar) y \emph{Transform} (transformar), mientras que la visualización
(\emph{Visualise}), modelización (\emph{Model}) y comunicación (\emph{Communicate}) se verán
en otros capítulos del libro.
Una de las características de la forma en que están programados los paquetes del
\emph{tidyverse} es que se puede trabajar\footnote{Existe una guía de estilo del \emph{tidyverse}, que se puede consultar en \url{https://style.tidyverse.org}. Hay incluso una serie de Addins en RStudio para
  comprobar y aplicar esta guía de estilo a través del paquete \texttt{styler}.} con \emph{pipes}.

El \emph{pipe} es básicamente un operador compuesto de dos caracteres, \texttt{\textbar{}\textgreater{}},
que se puede obtener con el atajo de teclado \texttt{CTRL+MAYUS+M}. El operador
se pone en medio de dos expresiones de R. Llamemos \texttt{lado\_izquierdo}
y \texttt{lado\_derecho} a las expresiones que se ponen a izquierda y derecha
del \emph{pipe}. Entonces se utiliza de la siguiente manera:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lado\_izquierdo }\SpecialCharTok{|\textgreater{}}\NormalTok{ lado\_derecho}
\end{Highlighting}
\end{Shaded}

\begin{infobox}
El operador \emph{nativo} de R \texttt{\textbar{}\textgreater{}} apareció en la versión R-4.1.0. Hay un operador alternativo que proviene del paquete \{magrittr\}, \texttt{\%\textgreater{}\%}, que había que usar antes de esta versión, y mucha literatura y documentación está escrita usándolo. Hay diferencias, pero a los efectos de este capítulo ambos operadores se pueden utilizar indistintamente.

\end{infobox}

La expresión \texttt{lado\_izquierdo} debe producir un valor, que puede
ser cualquier objeto de R.
La expresión \texttt{lado\_derecho} debe ser una función, que tomará como primer
argumento el valor producido en la parte izquierda.
Si se desea guardar el resultado final, se debe asignar el resultado
a algún nombre de objeto para que se almacene en el espacio de trabajo.
La siguiente expresión sería un ejemplo de uso.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nombre\_objeto }\OtherTok{\textless{}{-}}\NormalTok{ lado\_izquierdo }\SpecialCharTok{|\textgreater{}}
\NormalTok{  lado\_derecho}
\end{Highlighting}
\end{Shaded}

No obstante la ventaja de usar los pipes es que se pueden encadenar, de forma que el resultado de cada operación pasa a la siguiente expresión del \emph{pipeline}, como en el siguiente ejemplo:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{contam\_mad }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{colnames}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{length}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] 12}
\end{Highlighting}
\end{Shaded}

\hypertarget{transformaciuxf3n-de-datos-con-dplyr}{%
\subsection{\texorpdfstring{Transformación de datos con \texttt{dplyr}}{Transformación de datos con dplyr}}\label{transformaciuxf3n-de-datos-con-dplyr}}

En la gramática del \emph{tidyverse}, dentro del paquete \texttt{dplyr} se dispone de una
serie de ``verbos'' (funciones) para una sola tabla, y que se pueden
agrupar en tres categorías: Para trabajar con filas, para trabajar con
columnas, y para resumir datos.

\hypertarget{operaciones-con-filas}{%
\subsubsection{Operaciones con filas}\label{operaciones-con-filas}}

Los verbos definidos para estas operaciones son:

\begin{itemize}
\tightlist
\item
  \textbf{\texttt{filter()}}: elige filas en función de los valores de la columna.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pm10 }\OtherTok{\textless{}{-}}\NormalTok{ contam\_mad }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(nom\_abv }\SpecialCharTok{==} \StringTok{"PM10"}\NormalTok{) }\CommentTok{\# filtramos por NOx}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{\texttt{arrange()}}: cambia el orden de las filas por algún criterio.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{zonas }\OtherTok{\textless{}{-}}\NormalTok{ contam\_mad }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(zona), daily\_mean)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  \textbf{\texttt{slice()}}: selecciona por el índice de la fila. También hay una serie de funciones
  ``asistentes'' (\emph{helpers}) para obtener los índices que se utilizan con
  frecuencia. Por ejemplo:

  \begin{itemize}
  \tightlist
  \item
    \texttt{slice\_head()} y \texttt{slice\_tail()} obtienen las primeras y últimas filas
    respectivamente (por defecto, una). Se puede especificar \texttt{n} o \texttt{prop}
    (proporción de filas)
  \item
    \texttt{slice\_sample()} obtiene una muestra aleatoria de \texttt{n} filas (o proporción \texttt{p})
  \item
    \texttt{slice\_min()}, \texttt{slice\_max()} obtienen las filas que contienen los menores o mayores
    valores respectivamente de la variable indicada en el argumento \texttt{order\_by}. Si no
    se especifica \texttt{n} o \texttt{prop}, solo los que coinciden con el mínimo o máximo. Nótese
    que puede haber más de una fila que cumpla la condición.
  \end{itemize}
\end{itemize}

Véase el resultado de los siguientes ejemplos:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pm10 }\SpecialCharTok{|\textgreater{}} \FunctionTok{slice}\NormalTok{(}\DecValTok{10}\SpecialCharTok{:}\DecValTok{15}\NormalTok{) }\CommentTok{\# extrae filas desde la 10 a la 15}
\NormalTok{pm10 }\SpecialCharTok{|\textgreater{}} \FunctionTok{slice\_tail}\NormalTok{(}\AttributeTok{n =} \DecValTok{3}\NormalTok{) }\CommentTok{\# extrae las tres últimas filas}
\NormalTok{pm10 }\SpecialCharTok{|\textgreater{}} \FunctionTok{slice\_max}\NormalTok{(}\AttributeTok{order\_by =}\NormalTok{ daily\_mean) }\CommentTok{\# día con mayor valor de PM10}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{) }\CommentTok{\# Para que la muestra aleatoria sea reproducible}
\NormalTok{pm10 }\SpecialCharTok{|\textgreater{}} \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{4}\NormalTok{) }\CommentTok{\# muestra 4 registros}
\end{Highlighting}
\end{Shaded}

\hypertarget{operaciones-con-columnas}{%
\subsubsection{Operaciones con columnas}\label{operaciones-con-columnas}}

Los verbos definidos para estas operaciones son:

\begin{itemize}
\tightlist
\item
  \textbf{\texttt{select()}}: indica cuando una columna es incluida o no.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pm10 }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(longitud, latitud, daily\_mean, tipo)}
\NormalTok{pm10 }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.numeric))}
\NormalTok{pm10 }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(id}\SpecialCharTok{:}\NormalTok{latitud))}
\end{Highlighting}
\end{Shaded}

En cuanto a la \textbf{modificación} de datos tenemos múltiples posibilidades.

\begin{itemize}
\item
  \textbf{\texttt{rename()}}: cambia el nombre de la columna.
\item
  \textbf{\texttt{mutate()}}: cambia los valores de las columnas y crea nuevas columnas. \texttt{transmute} funciona igual que \texttt{mutate}, pero la tabla de datos
  resultante solo contiene las nuevas columnas creadas.
\item
  \textbf{\texttt{relocate()}}: cambia el orden de las columnas.
\item
  \textbf{\texttt{summarise()}}: Resume todos los datos en una sola fila usando alguna función de resumen, por ejemplo la media, desviación típica, etc.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pm10 }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{zona\_calidad\_aire =}\NormalTok{ zona)}
\NormalTok{pm10 }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{relocate}\NormalTok{(fecha, }\AttributeTok{.before =}\NormalTok{ estaciones)}
\NormalTok{pm10\_na }\OtherTok{\textless{}{-}}\NormalTok{ pm10 }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{isna =} \FunctionTok{is.na}\NormalTok{(daily\_mean))}
\NormalTok{pm10\_media }\OtherTok{\textless{}{-}}\NormalTok{ pm10 }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{Media =} \FunctionTok{mean}\NormalTok{(daily\_mean, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

En este punto es importante señalar que dentro de la función \texttt{mutate}
se puede usar cualquier función vectorizada para transformar las
variables. Por ejemplo, se podría transformar una columna con las funciones
\texttt{as.xxx} que se vieron en el apartado \ref{110003-estructuras}, aplicar formatos
a fechas o usar funciones del paquete \texttt{lubridate} para trabajar con este
tipo de datos. A medida que se avance en el libro irán apareciendo aplicaciones
que ahora quizás no sean tan evidentes.

\hypertarget{operaciones-de-resumen-y-agrupaciuxf3n}{%
\subsubsection{Operaciones de resumen y agrupación}\label{operaciones-de-resumen-y-agrupaciuxf3n}}

La primera operación de resumen que puede surgir es
``contar'' filas. La función \texttt{tally()} devuelve el número de filas totales
de un \texttt{data.frame}. La función \texttt{count()} nos puede dar también este número,
pero si le pasamos como argumento además alguna variable, nos da el número
de filas para cada valor diferente de dicha/s variables. Estos recuentos se
pueden añadir a la tabla de datos con las funciones \texttt{add\_count()} y \texttt{add\_tally()},
lo que permite calcular frecuencias absolutas y relativas fácilmente.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pm10 }\SpecialCharTok{|\textgreater{}} \FunctionTok{tally}\NormalTok{()}
\CommentTok{\#\textgreater{}       n}
\CommentTok{\#\textgreater{} 1 53794}
\NormalTok{pm10 }\SpecialCharTok{|\textgreater{}} \FunctionTok{count}\NormalTok{(zona)}
\CommentTok{\#\textgreater{}            zona     n}
\CommentTok{\#\textgreater{} 1: Interior M30 20690}
\CommentTok{\#\textgreater{} 2:      Noreste 12414}
\CommentTok{\#\textgreater{} 3:     Noroeste  4138}
\CommentTok{\#\textgreater{} 4:      Sureste  8276}
\CommentTok{\#\textgreater{} 5:     Suroeste  8276}
\end{Highlighting}
\end{Shaded}

La función \texttt{summarise()} (o , equivalentemente, \texttt{summarize()}) aplica alguna
función de resumen a la/s variable/s que se especifiquen.
El paquete \texttt{dplyr} tiene algunas funciones de resumen adicionales, como
\texttt{n()} (número de filas), \texttt{n\_distinct()} (número de filas con valores distintos),
\texttt{first()}, \texttt{last()}, \texttt{nth()} (primero, último, y enésimo, respectivamente).

En muchas ocasiones las operaciones de análisis se realizan en grupos
definidos por alguna variable de agrupación. La función \texttt{group\_by()}
``prepara'' la tabla de datos para realizar operaciones de este tipo.
Una vez agrupados los datos, se pueden añadir operaciones de
resumen como las vistas anteriormente. A veces hay que ``desagrupar'' los
datos, para lo que utilizaremos la función \texttt{ungroup()}.

A continuación se muestra una expresión un poco más compleja que las anteriores.
En el conjunto de datos \texttt{contam\_mad} del paquete \texttt{CDR}, se filtra por el
nombre de contaminante ``NOx''. Después se agrupan los datos por zona y se
calculan algunos estadísticos resumen para cada zona.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contam\_mad }\SpecialCharTok{|\textgreater{}}  
  \FunctionTok{filter}\NormalTok{(nom\_abv }\SpecialCharTok{==} \StringTok{"NOx"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# filtramos por PM10}
  \FunctionTok{group\_by}\NormalTok{(zona) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarize}\NormalTok{(}
    \AttributeTok{min =} \FunctionTok{min}\NormalTok{(daily\_mean, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q1 =} \FunctionTok{quantile}\NormalTok{(daily\_mean, }\FloatTok{0.25}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{median =} \FunctionTok{median}\NormalTok{(daily\_mean, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(daily\_mean, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q3 =} \FunctionTok{quantile}\NormalTok{(daily\_mean, }\FloatTok{0.75}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{max =} \FunctionTok{max}\NormalTok{(daily\_mean, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  )}
\CommentTok{\#\textgreater{} A tibble: 5 × 7}
\NormalTok{    zona            min    q1 median  mean    q3   max}
    \SpecialCharTok{\textless{}}\NormalTok{chr}\SpecialCharTok{\textgreater{}}         \ErrorTok{\textless{}}\NormalTok{dbl}\SpecialCharTok{\textgreater{}} \ErrorTok{\textless{}}\NormalTok{dbl}\SpecialCharTok{\textgreater{}}  \ErrorTok{\textless{}}\NormalTok{dbl}\SpecialCharTok{\textgreater{}} \ErrorTok{\textless{}}\NormalTok{dbl}\SpecialCharTok{\textgreater{}} \ErrorTok{\textless{}}\NormalTok{dbl}\SpecialCharTok{\textgreater{}} \ErrorTok{\textless{}}\NormalTok{dbl}\SpecialCharTok{\textgreater{}}
\CommentTok{\#\textgreater{} 1 Interior M30 0.0833  32.4   54.1  72.9  90.0  759.}
\CommentTok{\#\textgreater{} 2 Noreste      1       23.8   39.6  56.2  68.9  516.}
\CommentTok{\#\textgreater{} 3 Noroeste     0       12.0   20.3  29.7  34.5  352.}
\CommentTok{\#\textgreater{} 4 Sureste      0       29.1   45.4  64.6  77.2  453 }
\CommentTok{\#\textgreater{} 5 Suroeste     0.667   33.5   59.6  90.5 114.   666.}
\end{Highlighting}
\end{Shaded}

\hypertarget{combinaciuxf3n-de-datos}{%
\subsection{Combinación de datos}\label{combinaciuxf3n-de-datos}}

En el apartado anterior se han tratado los ``verbos'' de una tabla.
Es muy común que haya que combinar datos de distintas
tablas, para lo cual se utilizan lo que el \emph{tidyverse}
considera \emph{two tables verbs}. En esencia, para combinar
tablas que contienen información relacionada, hay que
saber cuáles son las columnas que se refieren a lo mismo,
y de esta forma se hacen uniones (\emph{joins}) utilizando esas
columnas. Hay cuatro tipos de uniones que se pueden realizar,
usando las siguientes funciones:

\begin{itemize}
\tightlist
\item
  \texttt{inner\_join()}: Se incluyen las filas de ambas tablas para las que coinciden las variables de unión.
\item
  \texttt{left\_join()}: Se incluyen todas las filas de la primer tabla y solo las de la segunda donde hay coincidencias.
\item
  \texttt{right\_join()}: Se incluyen todas las filas de la segunda tabla y solo las de la primera donde hay coincidencias.
\item
  \texttt{full\_join()}: Se incluyen todas las filas de las dos tablas.
\end{itemize}

Las funciones requieren como argumentos dos tablas de datos y
la especificación de las columnas coincidentes. Si no se especifica,
hace las uniones por todas las columnas coincidentes en ambas tablas. Para
las filas que solo están en una de las tablas, se añaden valores \texttt{NA}
donde no haya coincidencias.

A modo de ejemplo, las siguientes expresiones unen dos datasets para combinar datos de municipios con su renta. En el Capítulo \ref{datacleaning} se verán estas uniones en la práctica.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sf)}
\NormalTok{munis\_renta }\OtherTok{\textless{}{-}}\NormalTok{ municipios }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{left\_join}\NormalTok{(renta\_municipio\_data) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(name, cpro, cmun, }\StringTok{\textasciigrave{}}\AttributeTok{2019}\StringTok{\textasciigrave{}}\NormalTok{)}
\CommentTok{\#\textgreater{} Joining, by = "codigo\_ine"}
\end{Highlighting}
\end{Shaded}

Otra forma de unir tablas es simplemente añadiendo columnas (que tengan el mismo
número de filas) o filas (que tengan el mismo número de columnas). Para ello se usan
las funciones \texttt{bind\_cols()} y \texttt{bind\_rows()} respectivamente.
Una forma conveniente de añadir nuevas filas o columnas son las funciones
\texttt{add\_row()} y \texttt{add\_column()}. Se pueden añadir antes o después de una fila/columna
especificada, y pasando los valores como pares ``variable = valor''.

Como comentario final del paquete \texttt{dplyr}, una característica importante es que
se pueden usar las funciones vistas sobre
tablas de una base de datos, sin necesidad de utilizar sentencias SQL y con
la ventaja de que las operaciones se realizan en el motor de la base de datos.

\hypertarget{reorganizaciuxf3n-de-datos}{%
\subsection{Reorganización de datos}\label{reorganizaciuxf3n-de-datos}}

A lo largo del capítulo se ha visto la importancia de disponer los
datos de forma rectangular, de forma que tengamos una columna para
cada variable, y una fila para cada observación. Algunas veces es
conveniente reorganizar los datos más ``a lo ancho'' o más
``a lo largo'' de lo que los tenemos.

Para estas operaciones se utilizan las funciones \texttt{pivot\_longer()} y
\texttt{pivot\_wider()} del paquete del \emph{tidyverse} \texttt{tidyr} de la siguiente forma:

\begin{itemize}
\item
  \texttt{pivot\_longer()}: el argumento \texttt{names\_to} asigna el nombre
  de la nueva variable que va a indicar de qué columna vienen los datos; y el
  argumento \texttt{values\_to} asigna el nombre de la nueva variable que va a contener
  el valor de la tabla original.
\item
  \texttt{pivot\_wider()}: el argumento \texttt{names\_from} indica el nombre
  de la variable que contiene los nombres de las nuevas columnas a crear a lo ancho; y el argumento \texttt{values\_from} indica el nombre de la variable que contiene los
  valores en la tabla original. Las observaciones deben estar identificadas de forma
  única por varias variables. Si no es el caso, se puede aplicar una función
  al estilo de las tablas dinámicas de las hojas de cálculo con el argumento \texttt{values\_fn}.
\end{itemize}

\begin{infobox}
Las funciones \texttt{pivot\_longer()} y \texttt{pivot\_wider()} admiten otros argumentos
\texttt{names\_xx} y \texttt{values\_xx} para personalizar la forma de reestructurar los datos.
En la mayoría de las
ocasiones será suficiente con las explicadas (\texttt{xx\_from} y \texttt{xx\_to}), consulta la ayuda
de las funciones en caso necesario, o el {[}artículo sobre \emph{pivoting}{]}(\url{https://tidyr.tidyverse.org/articles/pivot.html}.

\end{infobox}

A modo de ejemplo, el conjunto de datos \texttt{contam\_mad} tiene los datos ``mezclados'' de varias variables medioambientales en la columna \texttt{daily\_mean}. La columna \texttt{nom\_abv} contiene el parámetro al que se refiere la columna de datos. Entonces interesa ``extender'' la tabla para tener cada parámetro en una columna, de forma que se pueda hacer un análisis de datos adecuado, como en el siguiente código.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyr)}
\NormalTok{extendida }\OtherTok{\textless{}{-}}\NormalTok{ contam\_mad }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pivot\_wider}\NormalTok{(}
    \AttributeTok{names\_from =} \StringTok{"nom\_abv"}\NormalTok{,}
    \AttributeTok{values\_from =} \StringTok{"daily\_mean"}\NormalTok{,}
    \AttributeTok{values\_fn =}\NormalTok{ mean}
\NormalTok{  )}
\FunctionTok{colnames}\NormalTok{(extendida)}
\CommentTok{\#\textgreater{}  [1] "estaciones" "id"         "id\_name"    "longitud"}
\CommentTok{\#\textgreater{}  [5] "latitud"    "nom\_mag"    "ud\_med"     "fecha"}
\CommentTok{\#\textgreater{}  [9] "zona"       "tipo"       "BEN"        "SO2"}
\CommentTok{\#\textgreater{} [13] "NO2"        "EBE"        "CO"         "NO"}
\CommentTok{\#\textgreater{} [17] "PM10"       "PM2.5"      "TOL"        "NOx"}
\end{Highlighting}
\end{Shaded}

Se deja como ejercicio volver a obtener la tabla original usando la función \texttt{pivot\_longer()} a partir del objeto \texttt{extendida}.

El paquete \texttt{tidyr} también contiene funciones para reorganizar las columnas
de la tabla uniendo columnas con la función \texttt{unite()}, o separando una columna
en dos o más con la función \texttt{separate()}. Véase la ayuda de las funciones para
saber más.

Para terminar este apartado de reorganización de datos, veamos una
primera aproximación al tratamiento de valores perdidos, que se tratará en el
Capítulo \ref{datacleaning}. En \textbf{R}, un valor
perdido se representa por el valor especial \texttt{NA} (\emph{not available}). Brevemente,
las funciones más utilizadas en este campo son:

\begin{itemize}
\item
  \texttt{drop\_na()} del paquete \texttt{tidyr} permite
  eliminar las filas que tienen valores perdidos en ciertas variables
  (o en cualquiera si no se especifica ninguna).
\item
  \texttt{replace\_na()} susituye los valores perdidos en cada variable por el valor
  especificado.
\item
  \texttt{fill()} permite ``rellenar'' valores perdidos con los últimos encontrados.
\end{itemize}

Los datos de contaminación a menudo tienen muchos valores perdidos. La siguiente expresión elimina las filas del conjunto de datos \texttt{contam\_mad} y después cuenta las filas.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contam\_mad }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{drop\_na}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \CommentTok{\# omitimos los NAs para el análisis}
  \FunctionTok{count}\NormalTok{()}
\NormalTok{n}
\CommentTok{\#\textgreater{} 1: 505773}
\end{Highlighting}
\end{Shaded}

\begin{infobox_resume}

\textbf{RESUMEN}

\begin{itemize}
\tightlist
\item
  \textbf{R} es software libre y gratuito, mantenido por una enorme comunidad.
\item
  La forma de interactuar con \textbf{R} es mediante expresiones, que se escriben en \emph{scripts}, y al ejecutarlas se obtienen los resultados.
\item
  La sesión de \textbf{R} se inicia en un directorio de trabajo.
\item
  Los objetos de datos que se vayan a usar deben estar en el espacio de trabajo.
\item
  RStudio es un ``envoltorio'' de \textbf{R}, y por tanto \textbf{R} tiene que estar instalado
  en el sistema para poder usar RStudio.
\item
  Los paquetes se instalan una sola vez, y lo podemos hacer desde el panel
  \emph{Packages} o con la función \texttt{install.packages} en la consola.
\item
  Para acceder a las funciones de los paquetes, deben cargarse antes con
  la función \texttt{library} o usar la sintaxis explícita \texttt{paquete::funcion()}
\item
  La tabla de datos o \texttt{data.frame} es la estructura más adecuada para análisis de datos.
\item
  Cada característica estudiada es un \texttt{vector}, que normalmente se almacena como una
  columna de la tabla de datos.
\item
  Los elementos de un vector son todos del mismo tipo.
\item
  Las variables categóricas se representan mediante objetos de tipo \texttt{factor}, con niveles y etiquetas.
\item
  Las listas contienen colecciones de objetos que pueden ser de distintos tipos.
\item
  El paquete \texttt{readxl} del \texttt{tidyverse} importa archivos de Excel.
\item
  Los ficheros de texto más populares son los \texttt{.csv}.
\item
  La exportación a ficheros de texto se realiza con las funciones \texttt{write.*} de forma
  similar a como se importan. Para guardar archivos de excel usamos el paquete \texttt{openxlsx}.
\item
  Los datos del espacio de trabajo se pueden guardar en archivos \texttt{.RData} o \texttt{.rds}.
\item
  El \emph{tidyverse} es un conjunto de paquetes que facilita las tareas de análisis de datos.
\item
  El operador \emph{pipe}, \texttt{\textbar{}\textgreater{}} permite ``pasar'' valores a funciones de forma encadenada.
\item
  Las operaciones básicas con una tabla son filtrado, selección y resumen.
\item
  Para crear nuevas columnas en las tablas de datos usamos la función \texttt{mutate}.
\item
  Para combinar tablas con columnas comunes se usan las funciones \texttt{xx\_join}.
\item
  Los datos de una tabla se pueden apilar y extender para tener datos ``anchos'' o ``largos''.
\item
  Cuando tenemos datos perdidos, podemos optar por eliminar las filas o imputar los valores.
\end{itemize}

\end{infobox_resume}

\hypertarget{cap-etica}{%
\chapter{Ética en la ciencia de datos}\label{cap-etica}}

\emph{Mónica Villas}

\hypertarget{por-que-la-uxe9tica-en-la-ciencia-de-datos}{%
\section{¿Por que la ética en la ciencia de datos?}\label{por-que-la-uxe9tica-en-la-ciencia-de-datos}}

Los primeros documentos de ética se remontan a los tiempos de Platón, cuando se comenzó a pensar más allá del día a día y a establecer una relación causa-efecto. Este manual se va a centrar en la ética aplicada, que es la utilización de la ética en la práctica. Algunos ejemplos de ética aplicada pueden ser la ética profesional, la bio-ética, la ética del medio ambiente y así un sinfín de ellas más. Si se refiere al trabajo concreto de un científico de datos, se debe tener en cuenta los pasos concretos desde que se enuncia el problema a resolver hasta que se diseña un algoritmo para resolverlo. El científico de datos tiene que tomar decisiones en cada uno de los pasos del proceso: la recopilación de datos, la transformación, la definición de objetivos a medir, el uso de algoritmos y finalmente la explicación de los resultados. En todos estos pasos, tiene que usar su pensamiento crítico y tomar decisiones que pueden ser correctas o no, en lo que a la ética se refiere. En la parte de automatizar un proceso concreto, y encontrar el algoritmo que ayude a automatizarlo, es otro punto clave en la toma de decisiones. Si este paso no se hace de manera adecuada, la toma de decisiones automáticas puede ``perpetuar'' algunos de los problemas éticos, como por ejemplo los sesgos, o se pueden tener decisiones tomadas por los algoritmos que el propio científico de datos no puede explicar, cómo se verá más adelante. Para ver esto con algo más de detalle, siempre es mejor usar un ejemplo. Si se toma un ejemplo de automatización de contrataciones, en el que un científico de datos necesita desarrollar un algoritmo para que contrate a los mejores profesionales para su compañía, estos serían los pasos a seguir. Primero tiene que entender que significa los mejores profesionales y definir los atributos que los representan. Después, tiene que buscar datos históricos de la compañía, recopilar éstos y estar seguros qué esos datos cumplen con la normativa de privacidad establecida, especialmente si la compañía reside en Europa. Aquí, el científico de datos, debe pensar en temas como la procedencia de los datos, ¿Cuál es la fuente?, ¿a quién pertenecen los datos?, ¿están los datos anonimizados para que no podamos identificar a una persona de manera unívoca? y algunas preguntas similares referidas a la privacidad. Seguidamente, se tendrá que revisar la muestra de datos y asegurar que se tiene una muestra de datos cuyos atributos (edad, profesión, experiencia, raza, género, procedencia geográfica, \ldots) no incluyen sesgos, es decir, no se tiene por ejemplo muchos más casos de personas de color que de raza caucásica , o más personas mayores que menores, o más mujeres que hombres. En definitiva, que la muestra que tiene es suficientemente representativa de la población con la que va a trabajar. Sino, un científico de datos sabe cómo utilizar técnicas para resolver este tipo de problemas que le van a permitir tener una muestra de datos balanceada. Además, se ha de tener cuidado con los datos personales, referentes a las personas como género, edad, raza , \ldots etc dado que la toma de decisiones no puede tener en cuenta estos atributos porque se estaría discriminando y por tanto no actuando de manera ética.
Como se puede ver en este sencillo ejemplo, en su día a día el científico de datos tiene que tomar decisiones, no sólo técnicas, que influyen en el resultado de su trabajo y que pueden afectar a otras personas. Generalmente los científicos de datos suelen ser profesionales que provienen del mundo técnico, de carreras como ingeniería o matemáticas y que en general sus conocimientos de ética son más escasos. De ahí que sea clave su conocimiento de los fundamentos básicos para incorporarlos en su día a día.

Al igual que para los médicos existe un juramento hipocrático no existe una guía común para el científico de datos, donde se describa como debe comportarse. Uno de los documentos que podría servir de base se refiere a las acciones de los profesionales de informática que publica la asociación \href{https://www.acm.org/code-of-ethics/the-code-in-spanish}{ACM.}

\hypertarget{los-principios-uxe9ticos}{%
\section{Los principios éticos}\label{los-principios-uxe9ticos}}

Desde inicios del 2010, el boom de la IA (Inteligencia Artificial) ~ha sido exponencial, debido a la gran cantidad de datos así como la mejora en la computación y la disponibilidad de un gran número de algoritmos. Por ello la demanda de científicos de datos que sean capaces de transformar estos datos en información clave para las empresas, ha crecido mucho en los últimos años. Asimismo, desde 2016, distintos organismos, asociaciones, empresas, gobiernos han publicado numerosos documentos, donde se resalta que es clave la necesidad de principios éticos de la IA. Google, IBM, Amazon publicaron sus principios éticos y otro ejemplo conocido es la publicación del tratado de Asimolar basado en los principios de Asimov. La mayoría de estos documentos están desarrollados por perfiles multidisciplinares: científicos de datos, abogados o expertos en ética, que resaltan la importancia de tener en cuenta los principios éticos en la toma de decisiones automáticas cuando se utiliza la ciencia de datos. Un principio no es ni más ni menos que ``aquello que permite preservar los derechos y libertades de las personas, sin frenar la innovación tecnológica'' \citet{olmeda2022manual}. La mayoría de los principios se pueden agrupar en cuatro grandes categorías: \textbf{autonomía, justicia, evitar daños y generar beneficios.} Algunos ejemplos deprincipios que se pueden agrupar en alguna de estas categorías~ son: \textbf{transparencia, explicabilidad, privacidad, accesibilidad o equidad.} No hay un acuerdo a nivel mundial, sobre cuáles deberían ser los principios claves de la IA, pero sí que se están desarrollando proyectos supranacionales como el de la \href{https://en.unesco.org/artificial-intelligence/ethics}{UNESCO} donde recientemente se ha firmado un acuerdo sobre la ética de la IA, con todos los países miembros.

Uno de estos ejemplos es la regulación europea publicada en Abril de 2021 y que ha sido fruto del trabajo de tres años. Liderado por ungrupo de expertos \href{https://knowww.eu/nodes/5cdeb73471a39a3b75a3b68f\#:~:text=Urs\%20Bergmann\%2C\%20Cecilia\%20Bonefeld\%2DDahl,is\%20Chairing\%20the\%20AI\%20HLEG.}{HLEG AI (High Level Expert Group)} y con un enfoque consultivo, con el que se han realizado distintas publicaciones desde 2018, mientras que se recogían los comentarios de la sociedad civil, instituciones públicas, empresas e instituciones académicas.

La propuesta de regulación, publicada en abril de 2021, ha elegido un enfoque de riesgos, que se centra en clasificar los riesgos en cuatro tipologías:

\begin{itemize}
\item
  \textbf{Riesgos inaceptables}, como el uso de aplicaciones de \emph{social scoring} o para temas judiciales.
\item
  \textbf{Riesgo alto}, como el uso de aplicaciones de contratación o médicas que deberán ser supervisadas por organismos designados antes de su publicación.
\item
  \textbf{Riesgo medio} en las que será necesaria incluir la explicabilidad necesaria para entender la toma de decisiones del algoritmo
\item
  \textbf{Riesgo bajo} para cualquier otro tipo de aplicación
\end{itemize}

En el resto del mundo, aún el progreso en este tipo de regulación está siendo algo más lenta, aunque países como Estados Unidos, que hasta ahora no había puesto el foco en este tipo de regulaciones está empezando a trabajar en ello desde finales del año pasado. Por otro lado, China conocida mundialmente por su falta de respeto a la privacidad, está empezando a dar algún paso en esta área.~ Según un artículo de \href{https://www.forrester.com/blogs/navigate-chinas-new-data-and-ai-regulations}{Forrester}, China está empezando a cambiar su política en este sentido, y como ejemplo en Marzo de 2022 ha lanzado una regulación donde las empresas tienen que informar mejor a los usuarios sobre sus algoritmos de recomendación. En definitiva, parece que la necesidad de la ética para proyectos de ciencia de datos está avanzando poco a poco en todas las geografías, y Europa es por el momento un ejemplo a seguir.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{la-importancia-de-los-sesgos}{%
\section{La importancia de los sesgos}\label{la-importancia-de-los-sesgos}}

En primer lugar, se debería establecer la definición de sesgo, el sesgo es dar un peso desproporcionado a favor o en contra de algo. Si revisamos la bibliografía al respecto se pueden encontrar multitud de tipos de sesgos pero en lo que respecta a la ciencia de datos, los sesgos de los que vamos a hablar son los tipos de sesgos algorítmicos, y algunos de los ejemplos más conocidos. Este sesgo algorítmico sucede, cuando en el proceso del desarrollo de un sistema de inteligencia artificial, se incluyen en alguna parte del proceso. Se puede observar el sesgo en la adquisición de los datos, partiendo de muestras que ya tienen un sesgo o que no están balanceadas, también puede ocurrir este sesgo en base a la selección de los atributos que van a formar parte del modelo, así como en el despliegue del modelo si el contexto en el que se despliega el algoritmo es diferente de donde se despliega como refleja la figura 1 .Los estudios de estos sesgos algorítmicos están enfocados a evitar que se aumenten o perpetúen sesgos de cualquier tipo, teniendo en cuenta que los algoritmos tienen como objetivo generalizar. Como se veía en la sección anterior, mucha de la regulación que se está desarrollando en Europa va enfocado a \textbf{mantener el principio de equidad}, es decir, a no tener decisiones sesgadas.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/sesgosibm} 

}

\caption{Sesgos en el proceso de machine learning - fuente(IBM) }\label{fig:time-line}
\end{figure}

A continuación, se describe algún ejemplo en más detalle de cómo los algoritmos están tomando decisiones e incluyendo sesgos.

El ejemplo de COMPAS (\emph{Correctional Offender Management Profiling for Alternative Sanction}) es una aplicación que da soporte al sistema de justicia americana y que decide si la persona que va a ser juzgada tiene la probabilidad de ser reincidente o no. Cada acusado tenía asignado tres riesgos: Riesgo de reincidir, Riesgo de violencia y Riesgo de aparecer en el juicio. El índice de riesgo se establecía de 1 a 4 como bajo, de 5 a 7 como medio y de 8 a 10 como alto. Si la persona puede ser reincidente espera a que ocurra el juicio en la cárcel, y sino no tiene que ir a la cárcel hasta que secelebre el juicio. Diversos estudios y organizaciones analizaron los datos y no parecía que hubiera un problema de sesgo inicialmente. Sin embargo, la organización PROPUBLICA, recogió datos de unas 12.000~ personas, durante 2013 y 2014 y demostró que la aplicación estaba sesgada. El proceso que se siguió fue el siguiente:

\begin{itemize}
\item
  Partiendo del proceso de asignación de un riesgo se construyó el historial delictivo del acusado
\item
  Para determinar la raza, se usó la clasificación establecida, de negros, blancos, hispanos y asiáticos
\item
  Se revisó la definición de reincidencia, y como se establecían los riesgos en la aplicación de COMPAS
\item
  Solamente analizaron los riesgos para ``reincidencia'' y ``reincidencia violenta''
\item
  Se analizó el índice de reincidencia y de violencia en dos años y su distribución respecto a la raza
\item
  Para comprobar la disparidad entre la raza y el índice de riesgo, se utilizó una regresión logística que consideraba la raza, la edad, la historia criminal, la reincidencia futura, el grado de los cargos y el género.
\item
  Par ver la exactitud del algoritmo se usó un modelo de Cox
\item
  Se utilizó una muestra de unos 7300 acusados (para los que se tenía datos de 2 años) para analizar la tasa de falsos positivos y falsos negativo
\end{itemize}

Las personas de color tenían un índice de riesgo de reincidencia mucho más alto que las personas de raza caucásica. La herramienta predecía bien en el 60\% de los casos estudiados el riesgo de reincidencia, pero sólo en el 20\% de los casos lo hacía de manera correcta en el riesgo de reincidir de manera violenta. Se incluye un resumen de las conclusiones en la siguiente tabla:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6167}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3833}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Casuística en el estudio con datos de 2 años
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Resultados en porcentaje
\end{minipage} \\
\midrule()
\endhead
Los acusados de raza negra se les asignaba un riesgo más alto de reincidencia que los de raza negra & 45\% a los de raza negra

23\% a los de raza caucásica \\
Los acusados de raza blanca se les asignaba un riesgo más bajo de reincidencia que los de raza negra & 48\% a los de raza blanca

28\% a los de raza negra \\
Mayor asignación de riesgo de reincidencia a las personas de color & 77\% más de riesgo de reincidir a las personas de raza negra que a los de raza blanca \\
Se determinó que las variables que tenían mayor importancia para la asignación de riesgo de reincidencia era la edad, la raza y el género & \textless{} 25 años tenía 2.5 veces más de probabilidad de ser asignado un riesgo alto

45\% si además eran de raza negra

Casi un 20\% si la persona era mujer \\
\bottomrule()
\end{longtable}

Los detalles se pueden encontrar en este repositorio de \href{https://github.com/propublica/compas-analysis}{github} que solamente incluye el código en Python.

Hay multitud de ejemplos publicados, respecto al tema de sesgos, una de las mejores referencias sobre sesgos es el libro \citet{o2016weapons}

\hypertarget{es-necesaria-la-explicabilidad}{%
\section{¿Es necesaria la explicabilidad?}\label{es-necesaria-la-explicabilidad}}

La explicabilidad es otro de los principios clave de la IA ética, y además ha sido seleccionado por Europa como uno de los ejes de su propuesta de regulación de IA ética.

XAI (\emph{Explainable AI}) es un término que acuño DARPA en el año 2017 y que lo que hace es agrupar no sólo el concepto de interpretabilidad para los algoritmos de machine learning sino la parte de la psicología que tiene que ver con las explicaciones como se puede ver en la figura 2. No sé trata solamente de entender la toma de decisión del algoritmo sino de explicarlo de manera adecuada dependiendo del tipo de usuarios y estar seguros que la persona que recibe las explicaciones lo ha entendido. Si se toma el ejemplo de un algoritmo que selecciona imágenes cuando contienen un posible tumor, no serán las mismas explicaciones las que necesitará un científico de datos o un médico. Para el científico de datos será mucho más útil revisar los métricas propias del algoritmo (exactitud, precisión, sensibilidad, ..etc) y además saber cuáles de los atributos de entrada del algoritmo han tenido más peso en la decisión. En cambio, al médico lo que le interesará será una explicación menos técnica, más cualitativa, en la que se le explique con detalle por ejemplo porque se seleccionó esa muestra frente a otras, mencionando el tamaño, la forma u características que sean mejor entendidas por el profesional médico.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/xai} 

}

\caption{Explicabilidad según Darpa}\label{fig:time-line2}
\end{figure}

Los algoritmos pueden clasificarse en algoritmos de ``\textbf{caja blanca'' o transparente}, aquellos que son fácilmente interpretables, y algoritmos de \textbf{opacos o de ``caja-negra''} aquellos que no son interpretables y que requieren de herramientas adicionales para ello. Normalmente se tiene que establecer un balance entre la interpretabilidad y la exactitud dado que son métricas con una relación inversamente proporcional. A mayor exactitud menor interpretabilidad y viceversa. Los algoritmos que son más interpretables son aquellos más sencillos como los algoritmos de clasificación, regresión lineal o los árboles de decisiones. Otros como los modelos de \emph{Random forest}, XGboost o algoritmos de \emph{Deep learning} son mucho más exactos pero no son tan interpretables y esto puede presentar ciertos problemas a la hora de usarlos en la toma de decisiones en las compañías, dado que debo ser capaz de explicar la decisión del algoritmo.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/algoritmosexplicabilidad} 

}

\caption{Explicabilidad vs Exactitud}\label{fig:time-line3}
\end{figure}

\hypertarget{sesgos-y-explicabilidad-en-r}{%
\section{Sesgos y explicabilidad en R}\label{sesgos-y-explicabilidad-en-r}}

Para terminar, es muy relevante conocer las herramientas opensource o comerciales disponibles para que se puedan usar en análisis de sesgos o explicabilidad. Las herramientas son relativamente recientes, han ido surgiendo desde el 2018 y van evolucionando rápidamente.

En el caso de las herramientas para detectar el sesgo los proveedores son Microsoft, IBM, Google, Aequitas, Pymetric, Linkedin y el resto Opensource. La mayoría de ellas están abiertas a contribuciones externas y todas ellas utilizan mecanismos para la detección de sesgos, aunque solamente la de Microsft e IBM incluyen algoritmos para la mitigación de estos sesgos.

Referente a las herramientas sobre explicabilidad los proveedores más relevantes son Google, IBM, Oracle, H20 , el resto son opensource. La mayoría de ellas se pueden usar con algoritmos de caja blanca o caja negra. Respecto a los tipos de explicaciones para distintos usuarios sólo una de ellas por el momento, la herramienta de IBM incluye esta funcionalidad. Se puede resaltar también, la facilidad con la que H20 permite elegir el nivel de exactitud y explicabilidad en el momento del diseño del algoritmo. Para un mayor detalle se puede consultar dos tablas comparativas que se incluyen en el \citet{olmeda2022manual}

Recursos en R para equidad:

\begin{itemize}
\tightlist
\item
  \href{https://cran.rproject.org/web/packages/fairness/vignettes/fairness.htmlhttps://kozodoi.me/r/fairness/packages/2020/05/01/fairness-tutorial.html}{Tutorial de fairness (2021):} Este tutorial explica las distintas métricas usadas para medir la equidad (paridad demográfica, paridad proporcional, paridad predictiva\ldots etc) y permite crear la distintas métricas y visualizarlas. El tutorial usa losdatos de COMPAS.
\item
  \href{https://developer.ibm.com/blogs/the-aif360-team-adds-compatibility-with-r/}{Librerias de R} incluidas en IBM fairness360 (2020): Este paquete incluye algoritmos para detectar el sesgo, pero también para mitigarlo.
\item
  \href{https://github.com/matloff/EDFfair}{Libreria EDFFair (2022)} tiene una aproximación distinta, dado que permite al usuario ajustar el nivel de equidad frente a la exactitud, y poder mantener el equilibrio requerido. Esto se explica en detalle en \citet{matloff2022novel}
\end{itemize}

Recursos en R para explicabilidad:

\begin{itemize}
\tightlist
\item
  Algunas de las herramientas más conocidas en explicabilidad, que merecen mención aparte son SHAP y LIME, disponibles en R y en Python y usadas en muchos paquetes comerciales. \textbf{SHAP (2018)} es uno de los métodos más usados para la explicabilidad. Utiliza los valores de Shapley para poder explicar cualquier tipo de modelo, los detalles se pueden encontrar en \citet{aas2021explaining} donde también se proporciona el \href{https://cran.r-project.org/web/packages/shapr/vignettes/understanding_shapr.html}{código}. \textbf{LIME (2017)} es otro de los métodos usados para explicabilidad, trata de ajustar un modelo local alrededor de un punto concreto y lo que hace es estudiar las perturbaciones alrededor de este modelo. Se puede encontrar la explicación detallada en \citet{ribeiro2016should} donde también se proporciona el \href{https://cran.r-project.org/web/packages/lime/index.html}{código}
\end{itemize}

\begin{itemize}
\item
  El artículo \citet{matloff2022novel} hace una recopilación de 27 librerías de R, incluyendo LIME y SHAP y el \href{https://github.com/MI2DataLab/XAI-tools/blob/master/README.md}{código} en github para usar cada una de ellas.
\item
  \href{https://ema.drwhy.ai}{Dalex(2022)} es un paquete disponible en R de reciente creación para ayudar a crear explicaciones partiendo de un modelo, y también proporciona el \href{https://github.com/ModelOriented/DALEX}{código} necesario.

  Este área está evolucionando mucho en los últimos años, y están surgiendo multitud de técnicas nuevas alrededor de la explicabilidad, que van a permitir entender mejor el proceso de decisión realizada por algoritmos más complejos.
\end{itemize}

\hypertarget{part-bienvenido-a-la-jungla-de-datos}{%
\part{Bienvenido a la jungla de datos}\label{part-bienvenido-a-la-jungla-de-datos}}

\hypertarget{datos-sql}{%
\chapter{Gestión de bases de datos relacionales}\label{datos-sql}}

\emph{Ismael Caballero y Ricardo Pérez del Castillo}

\hypertarget{introducciuxf3n-1}{%
\section{Introducción}\label{introducciuxf3n-1}}

El mundo real en el que estamos inmersos es puramente analógico: está lleno de \textbf{entidades} que se relacionan entre ellas o consigo mismo a través de unos determinados eventos que representan determinados \textbf{hechos}. Tanto \textbf{entidades} como \textbf{hechos} tienen una colección de características observables (normalmente llamados \textbf{atributos}), que pueden ser de interés en el contexto de una determinada aplicación.

Para estas aplicaciones que demandan el uso de datos del mundo real, es preciso realizar \textbf{observaciones} de esos \textbf{atributos} de las \textbf{entidades} y de los \emph{hechos} que son relevantes. Al conjunto de entidades y hechos del mundo real que son relevantes para una aplicación se les conoce como \textbf{Universo del Discurso} \citep{Piattinietal2006}. Es importante capturar la semántica del Universo del Discurso mediante los modelos correspondientes.

Es preciso, por tanto, asimilar que los datos que son de interés para una determinada aplicación deben ser capturados mediante un \textbf{proceso de observación y digitalización} de los valores de los atributos relevantes de las entidades y hechos del mundo real. Durante el proceso de observación se pueden producir errores que pueden derivar en problemas relacionados con la calidad de los datos \citep{price2004semiotic}. Por ejemplo, supóngase que las observaciones requieren una determinada frecuencia mínima de observación en relación con la velocidad en la producción de los hechos; si esta frecuencia no es adecuada, entonces, la cantidad de observaciones realizada será insuficiente para modelar el hecho, llevando a un estado inconsistente entre lo sucedido y lo observado.

Una vez capturados estos datos, pueden ser usados en los procesos de negocio o analizados para realizar una tarea o para producir un conocimiento del mundo real que hasta ahora no se tenía visión o conocimiento del mundo real (siempre teniendo en cuenta que no dejan de ser sino una representación del mismo, y que por tanto hay que tener mucho cuidado al extrapolar los resultados de dichos análisis) \citep{davenport2017competing}.

\hypertarget{concepto-de-base-de-datos}{%
\section{Concepto de Base de datos}\label{concepto-de-base-de-datos}}

Para poder habilitar esos análisis de forma automática mediante las potentes técnicas tratadas en el resto de capítulos de este libro, es necesario poder almacenar los datos en algún lugar como \texttt{repositorios} o \texttt{bases\ de\ datos}, donde se puedan fácilmente añadir, borrar, recuperar o modificar los datos.

De acuerdo con \citet{Piattinietal2006}, una \textbf{base de datos (BD)} es una \textbf{colección o depósito de datos integrados, almacenados en soporte secundario (no volátil) y con redundancia controlada}. En una base de datos, los valores correspondientes a los atributos que han de ser compartidos por diferentes usuarios y aplicaciones deben mantenerse independientes de ellos, y su definición (estructura de la base de datos) única y almacenada junto con los datos, se ha de apoyar en un modelo de datos. Este modelo debe captar las interrelaciones y restricciones existentes en las entidades y hechos del mundo real al que representan. Existen diferentes tipos de modelos que permiten estructurar y representar la semántica de los datos.

En el ámbito de los Sistemas de Información, se han desarrollado programas que dan soporte a todo el proceso de creación y explotación de las bases de datos. A estos programas se les conoce como \textbf{Sistemas Gestores de Bases de Datos (SGBD)}. Como ejemplos de estos SGBD se pueden citar Microsoft Access, Microsoft SQL Server, Oracle Server, MySQL, MariaDB, Informix, MongoDB\ldots{} En cualquier caso, como se verá más adelante en este capítulo, los SGBD que han venido siendo más amplicamente utilizados hasta ahora son los conocidos como \textbf{relacionales (SGBDR)}, aunque con el auge de Big Data y de Machine Learning, esta tendencia está cambiando y empiezan a desplegarse cada vez más SGBD conocidos como \textbf{NoSQL} (véase capítulo \ref{datos-no-sql}). Para evitar confusiones, se hace importante diferenciar entre lo que sería la base de datos propiamente dicha (\textbf{como una colección de datos almacenada en un fichero de datos}), y el software SGBD específico, ya sea relacional o NoSQL: una misma base de datos, con las correspondientes adaptaciones, puede ser gestionada usando diferentes SGBD.
Habitualmente, los tipos de SGBDR más usadas son los que tienen capacidades multiproceso/multiusuario, ya que permiten acceder a datos compartidos mediante el uso de interfaces de datos para ejecutar diferentes tipos de análisis empleando lenguajes de programación más potentes - o versátiles - como R o Python.

\hypertarget{gestiuxf3n-de-los-datos-en-una-base-o-repositorio-de-datos}{%
\subsection{Gestión de los datos en una base o repositorio de datos}\label{gestiuxf3n-de-los-datos-en-una-base-o-repositorio-de-datos}}

Las organizaciones usan datos para sus procesos de negocio. En función de esto es posible diferenciar entre diferentes tipos de datos \citep{Mahanti2019} como los recogidos en la tabla mostrada en Fig. \ref{fig:TablaTiposDatos}:

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/TablaTiposDatos} 

}

\caption{Tipos de datos}\label{fig:TablaTiposDatos}
\end{figure}

{[}\textbf{GEMA!!}: como no he conseguido disminuir el tamaño de la letra en la tabla con RMarkDown, he puesto una imagen con la tabla, creo que ahora queda más o menos bien. PEro el problema es que en el caption aparece la palabra ``figura'' y no ``tabla''{]}

Para poder usar estos datos como parte de los procesos de negocio, es necesario desarrollar aplicaciones que realicen los siguientes cuatro tipo de operaciones (normalmente conocidas como \texttt{operaciones\ CRUD}):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Crear datos (\emph{Create})}, inserta datos en el repositorio de datos.
\item
  \textbf{Leer datos (\emph{Read})}, recupera datos del repositorio para aprovisionar el proceso de negocio, o bien para realizar alguna operación específica.
\item
  \textbf{Actualizar datos (\emph{Update})}, modifica el valor de los atributos correspondientes a los hechos o entidades para actualizarlos a nuevas observaciones
\item
  \textbf{Borrar datos (\emph{Delete})}, elimina en bloque o selectivamente los datos almacenados en el repositorio de los datos.
\end{enumerate}

En cualquier caso, estos procedimientos de inserción, actualización, recuperación, y borrado deben garantizar siempre la seguridad del conjunto de los datos de modo que sólo sean accesibles por aquellos usuarios que estén autorizados a trabajar con ellos teniendo en cuenta el propósito establecido para los datos \citep{Piattinietal2006}.

La forma de implementar estas operaciones depende fuertemente del formato (modelo lógico) en el que estén almacenados los datos. Aunque existen diferentes modelos (estructurados, semi-estructurado, no estructurados), en este capítulo, el enfoque se hace sobre el modelo relacional desarrollado por Codd \citep{codd1970relational}, ya que es el más ampliamente usado en el ámbito empresarial y es el que implementan los SGBDR. Para poder dar soporte a las operaciones CRUD anteriormente citadas en bases de datos relacionales, se desarrolló un lenguaje llamado \texttt{Lenguaje\ Estructurado\ de\ Consulta} ( \textbf{Structured Query Language}, \texttt{SQL}), que se verá en la siguiente sección.

\hypertarget{el-lenguaje-estructurado-de-consulta-sql}{%
\section{El Lenguaje Estructurado de Consulta (SQL)}\label{el-lenguaje-estructurado-de-consulta-sql}}

Los principios de SQL están establecidos en el estándar internacional \href{https://www.iso.org/standard/16662.html}{ISO/IEC 9075:1989} como un mecanismo para identificar y regular las expresiones necesarias que permiten manejar bases de datos relacionales. Al ser un estándar, es importante recordar cada fabricantes de SGBDR como Oracle con su \href{https://www.oracle.com/database/}{Oracle Database Manager Server} o con \href{https://www.mysql.com/products/enterprise/database/}{MySQL}, Microsoft con su \href{https://www.microsoft.com/es-es/sql-server/}{SQL Server}, IBM con su \href{https://www.ibm.com/es-es/db2}{DB2}\ldots{} implementan en sus productos su propia versión del estándar SQL. Y aunque son prácticamente iguales, hay ligeros matices que les permiten diferenciarse de la competencia y que por tanto deben ser conocidos cuando se utilicen los correspondientes productos comerciales. No obstante existen en el mercado algunas soluciones \emph{open source} como \href{https://mariadb.org/}{MariaDB} o \href{https://www.postgresql.org/}{PostgreSQL}. En este capítulo, todos los ejemplos que se han desarrollado trabajan contra un servidor \href{https://www.mysql.com/products/}{MySQL 8}.

SQL tiene diferentes tipos de sentencias o instrucciones que dan soporte a los diferentes aspectos de las interacciones con la base de datos. No obstante, cualquier manual de SQL permitirá tratar en profundidad todos los elementos sintácticos del lenguaje, pero es importante recordar que los detalles específicos de la sintaxis específica dependerá fuertemente del SGBDR empleado. Aunque al final de un libro se añade una plantilla con un resumen de la sintaxis de SQL, se recomienda, para los ejemplos propuestos en este libro el \href{https://dev.mysql.com/doc/refman/8.0/en/sql-statements.html}{manual de referencia de SQL de MySQL}. En las siguientes secciones se da una visión global de dichos grupos de sentencias

\hypertarget{sql-como-lenguaje-de-definiciuxf3n-de-datos-ldd}{%
\subsection{SQL como Lenguaje de Definición de Datos (LDD)}\label{sql-como-lenguaje-de-definiciuxf3n-de-datos-ldd}}

El elemento central del modelo relacional es la \textbf{relación}, cuya implementación se realiza a través de una \textbf{tabla de datos}. Así, SQL se utilizará para definir todos los elementos necesarios para crear y modificar las tablas de datos. Se tienen tres tipos de instrucciones básicas para gestionar las tablas como estructuras de datos:

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Create}}, permite crear un componente de la base de datos, tal como una base de datos, una tabla, una vista,\ldots{} En el siguiente ejemplo, se crea usando instrucciones SQL, primero una base de datos llamada \texttt{Biblioteca} y luego se crea una tabla \texttt{Autor} con seis atributos (\texttt{CodAtutor}, \texttt{Nombre}, \texttt{Apellido1}, \texttt{Apellido2}, \texttt{Pseudonimo}, y \texttt{Nacionalidad}), donde se podrán almacenar valores correspondientes a dichos atributos, conformando así la base de datos:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{       create database Biblioteca}
\NormalTok{        create Table }\FunctionTok{Autor}\NormalTok{ (}
\NormalTok{          CodAutor }\FunctionTok{nvarchar}\NormalTok{ (}\DecValTok{20}\NormalTok{) primary key,}
\NormalTok{          Nombre    }\FunctionTok{nvarchar}\NormalTok{(}\DecValTok{40}\NormalTok{) not null,}
\NormalTok{          Apellido1 }\FunctionTok{nvarchar}\NormalTok{(}\DecValTok{50}\NormalTok{) not null,}
\NormalTok{          Apellido2 }\FunctionTok{nvarchar}\NormalTok{(}\DecValTok{50}\NormalTok{), }
\NormalTok{          Pseudonimo }\FunctionTok{nvarchar}\NormalTok{(}\DecValTok{50}\NormalTok{),}
\NormalTok{          Nacionalidad }\FunctionTok{nvarchar}\NormalTok{ (}\DecValTok{50}\NormalTok{)}
\NormalTok{        );}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Alter}}, permite modificar la estructura de un componente, añadiendo por ejemplo, atributos a una tabla, restricciones a un atributo, o modificando el tipo de datos de algún atributo existente; también para eliminar un atributo de una tabla existente. Siguiendo el ejemplo anterior, con la siguiente instrucción se añade un nuevo atributo \texttt{LocalidadNacimiento} a la tabla \texttt{Autor}:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{   alter table Autor add LocalidadNacimiento }\FunctionTok{nvarchar}\NormalTok{(}\DecValTok{50}\NormalTok{) not null}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Drop}}, esta instrucción sirve para eliminar un componente específico, como por ejemplo una tabla, una vista,\ldots Pero no sirve para eliminar los valores almacenados en una tabla. En el siguiente ejemplo, se eliminan las tablas \texttt{Escribe}, \texttt{Autor} y \texttt{Libro}:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{   drop table Escribe;}
\NormalTok{   drop table Autor;}
\NormalTok{   drop table Libro;}
\end{Highlighting}
\end{Shaded}

\hypertarget{sql-como-lenguaje-de-manipulaciuxf3n-de-datos-lmd}{%
\subsection{SQL como Lenguaje de Manipulación de Datos (LMD)}\label{sql-como-lenguaje-de-manipulaciuxf3n-de-datos-lmd}}

En esta sección se describirán las instrucciones típicas más importantes de SQL para el soporte a las operaciones \textbf{CRUD} anteriormente introducidas. Existen, por tanto, cuatro tipos de sentencia para manipular los datos:

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Create}}: implementada mediante la instrucción \texttt{insert}, sirve para insertar registros (también llamados \textbf{tuplas}) en una base de datos. En los ejemplos siguientes se insertan diversas tuplas en varias tablas, siguiendo el mismo orden en el que se especificaron los atributos cuando se creó la tabla. Así por ejemplo, se crea el código ``\emph{dbrown}'' (como valor para el atributo \texttt{CodAutor}) para ``\emph{Dan Brown}'', y el código ``\emph{cdv}'' (como valor para el atributo \texttt{CodLibro}) para su libro ``\emph{El Código da Vinci}''. El siguiente código SQL refleja las instrucciones necesarias:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    insert into Autor }\FunctionTok{values}\NormalTok{ (}\StringTok{\textquotesingle{}dbrown\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Dan\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Brown\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}EstadoUnidense\textquotesingle{}}\NormalTok{);}
\NormalTok{    insert into Libro }\FunctionTok{values}\NormalTok{ (}\StringTok{\textquotesingle{}cdv\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}El Código da Vinci\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Random House\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}2003{-}04{-}23\textquotesingle{}}\NormalTok{);}
\NormalTok{    insert into Escribe }\FunctionTok{values}\NormalTok{ (}\StringTok{\textquotesingle{}dbrown\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}cdv\textquotesingle{}}\NormalTok{);}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Read}}, implementada mediante la instrucción \texttt{select}, permite hacer consultas a la base de datos. En los siguientes ejemplos se escribe el código que selecciona (1) el nombre y primer apellido de los autores con nacionalidad española ordenados por orden alfabético del Apellido1 y (2) se lista todos los libros que haya escrito el autor ``\emph{Pérez Reverte}'' (cuyo \texttt{CodAutor} es ``\emph{perezreverte}'').
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{   Select Nombre, Apellido1 from Autor where Nacionalidad like }\StringTok{\textquotesingle{}Español\textquotesingle{}}\NormalTok{ order by Apellido1;}

\NormalTok{   Select Libro.Título from Autor, Escribe, Libro }\FunctionTok{where}\NormalTok{ ( }\AttributeTok{Libro.CodLibro =}\NormalTok{ Escribe.CodLibro and }\AttributeTok{Autor.CodAutor =}\NormalTok{ Escribe.CodAutor) }\FunctionTok{and}\NormalTok{ (}\AttributeTok{Escribe.CodAutor =}\StringTok{\textquotesingle{}perezreverte\textquotesingle{}}\NormalTok{);}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Update}} permite actualizar los valores de las tuplas seleccionadas. En el siguiente ejemplo, se actualiza el valor del atributo \texttt{pseudonimo} al valor ``\emph{El Manco de Lepanto}'' para el autor ``\emph{Miguel de Cervantes}'' con \texttt{CodAutor} ``\emph{mcervantes}'':
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{   update Autor set pseudonimo}\OtherTok{=}\StringTok{"El Manco de Lepanto"}\NormalTok{ where CodAutor}\OtherTok{=}\StringTok{\textquotesingle{}mcervantes\textquotesingle{}}\NormalTok{;}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Delete}} cuyo principal objetivo es borrar o eliminar en bloque o de forma selectiva una o varias tuplas o registros de datos de una tabla relacional que cumplan una determinada condición. En el siguiente código SQL se borra la(s) tupla(s) que contiene(n) datos del autor ``\emph{Pérez Reverte}''.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{   delete from Autor where CodAutor }\OtherTok{=}\StringTok{\textquotesingle{}perezreverte\textquotesingle{}}\NormalTok{;}
\end{Highlighting}
\end{Shaded}

\hypertarget{sql-como-lenguaje-de-administraciuxf3n-de-datos-lad}{%
\subsection{SQL como Lenguaje de Administración de Datos (LAD)}\label{sql-como-lenguaje-de-administraciuxf3n-de-datos-lad}}

Finalmente, SQL puede ser usado también para administrar los usuarios de una base de datos. Esto implica crear usuarios de la base de datos y asignarles diferentes tipos de permisos para realizar los diferentes tipos de operaciones vistos anteriormente sobre los distintos componentes de datos. Por ejemplo, se puede usar la siguiente instrucción para crear un usuario llamado \texttt{Ismael.Caballero} que tenga por contraseña \texttt{LibroMDSR}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{   create user }\StringTok{\textquotesingle{}Ismael.Caballero\textquotesingle{}}\NormalTok{ identified by }\StringTok{\textquotesingle{}LibroMDSR\textquotesingle{}}\NormalTok{;}
\end{Highlighting}
\end{Shaded}

y la siguiente instrucción se usa para asignar al usuario \texttt{Ismael.Caballero} los permisos necesarios para el acceso, lectura, selección, inserción, actualización y borrado de los valores de la base de datos \texttt{Biblioteca} así como para poder modificar la estructura de los componentes de la base de datos \texttt{Biblioteca} creada anteriormente:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{   GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, ALTER ON biblioteca.}\SpecialCharTok{*}\NormalTok{  TO }\StringTok{\textquotesingle{}Ismael.Caballero\textquotesingle{}}\SpecialCharTok{@}\StringTok{\textquotesingle{}\%\textquotesingle{}}\NormalTok{; }
\NormalTok{   FLUSH PRIVILEGES;}
\end{Highlighting}
\end{Shaded}

Recuérdese que los ejemplos mostrados han sido realizados para MySQL 8, aunque la sintaxis no debería ser muy diferente para ser usadas en otros SGBDR.

\hypertarget{usando-bases-de-datos-desde-r}{%
\section{Usando bases de datos desde R}\label{usando-bases-de-datos-desde-r}}

Como el propósito de este libro es aprender los fundamentos de la Ciencia de Datos usando el software R, en las siguientes secciones se explicarán cómo implementar las operaciones CRUD usando sentencias de paquetes específicos de R. Como en este capítulo se ha usado MySQL, se ha usado el \emph{driver} específico de \href{https://cran.r-project.org/web/packages/RMySQL/index.html}{\texttt{RMySQL}}. En caso de que se usase otro sistema gestor de bases de datos se tendría que recurrir al paquete específico que contuviera el driver correspondiente. Por tanto, en las siguientes secciones se explicará cómo conectarse a una base de datos usando las funciones correspondientes y cómo se implementan las operaciones CRUD con funciones del paquete \texttt{RMySQL}.

\hypertarget{conexiuxf3n-a-una-base-de-datos}{%
\subsection{Conexión a una base de datos}\label{conexiuxf3n-a-una-base-de-datos}}

Antes de poder realizar ninguna operación con la base de datos gestionadas por MySQL es preciso instalar y cargar los paquetes necesarios:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"RMySQL"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(RMySQL)}

\FunctionTok{library}\NormalTok{(DBI)}
\end{Highlighting}
\end{Shaded}

Para desarrollar las explicaciones, se ha usado una base de datos llamada \texttt{classimodels} que ha sido implementada para MySQL v8.0 y desplegada en un servidor con dirección IP 172.20.48.118 que está escuchando en el puerto 3306. El usuario que se conectará a la base de datos será \texttt{Ismael.Caballero} siendo su contraseña \texttt{MdsR.2022}. Otros usuarios tendrían que modificar los parámetros correspondientes para realizar las conexiones a sus propias bases de datos. Se almacenarán todos estos datos en variables para hacer más sencillo el mantenimiento de los chunks. Con \texttt{dbConect()} se realiza la conexión. Con \texttt{summary()} o con \texttt{dbGetInfo()} se pueden mostrar los resultados de la conexión en caso de que ésta se haya realizado con éxito.
Una vez terminada todas las tareas con la base de datos debería desconectarse mediante la instrucción \texttt{dbDisconnect()}. En esta chunk, se deja comentada para poder hacer el resto de los ejemplos del capítulo.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{usuario }\OtherTok{\textless{}{-}} \StringTok{"Ismael.Caballero"}
\NormalTok{passwd }\OtherTok{\textless{}{-}} \StringTok{"MdsR.2022\#"}
\NormalTok{nombrebd }\OtherTok{\textless{}{-}} \StringTok{"classicmodels"}

\NormalTok{servidor }\OtherTok{\textless{}{-}} \StringTok{"172.20.48.118"}
\NormalTok{puerto }\OtherTok{\textless{}{-}} \DecValTok{3306}

\NormalTok{mibbdd }\OtherTok{\textless{}{-}} \FunctionTok{dbConnect}\NormalTok{(}\FunctionTok{MySQL}\NormalTok{(),}
  \AttributeTok{user =}\NormalTok{ usuario, }\AttributeTok{password =}\NormalTok{ passwd, }\AttributeTok{dbname =}\NormalTok{ nombrebd,}
  \AttributeTok{host =}\NormalTok{ servidor, }\AttributeTok{port =}\NormalTok{ puerto}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(mibbdd)}
\FunctionTok{dbGetInfo}\NormalTok{(mibbdd)}
\FunctionTok{dbDisconnect}\NormalTok{(mibdd)}
\end{Highlighting}
\end{Shaded}

\hypertarget{operaciones-de-lectura-selecciuxf3n-read-de-datos}{%
\subsection{\texorpdfstring{Operaciones de lectura / selección (\emph{read}) de datos}{Operaciones de lectura / selección (read) de datos}}\label{operaciones-de-lectura-selecciuxf3n-read-de-datos}}

Una de las operaciones más frecuentes en cualquier tipo de bases de datos son las de lecturas o consultas (\emph{read}), implementadas en SQL con las sentencias de tipo \texttt{select}. El driver \texttt{RMySQL} ofrece distintas alternativas para realizar consultas de selección en \textbf{R Software}. La elección de la mejor operación dependerá de la complejidad de las consultas que se quiera realizar. Se listan a continuación:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{dbReadTable()}, que permite leer una tabla entera de una base de datos MySQL. Es recomendable usar este método si la tabla no es excesivamente grande. Se pueden almacenar los resultados en un data frame para hacer operaciones con ellos después. La ventaja es que sabiendo manejar data frames en R no se necesita aprender mucho más detalle del lenguaje \href{https://www.w3schools.com/sql/}{SQL} como LMD, pero la desventaja es que se acaba perdiendo parte del potencial expresivo de SQL para hacer algunas cosas más fáciles y eficientes. En el siguiente ejemplo se muestra cómo cargar toda la tabla \texttt{customers} en un data frame llamado \texttt{tblCustomers}, mostrando los resultados con la instrucción \texttt{summary()}:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tblCustomers }\OtherTok{\textless{}{-}} \FunctionTok{dbReadTable}\NormalTok{(mibbdd, }\StringTok{"customers"}\NormalTok{)}
\CommentTok{\# summary (tblCustomers)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \texttt{dbGetQuery()}, que tiene más flexibilidad que \texttt{dbReadTable()} porque permite mediante una sentencia SQL \texttt{select} (véase más información en \href{https://www.w3schools.com/mysql/mysql_select.asp}{el tutorial de SQL en W3C} o en la página oficial de \href{https://dev.mysql.com/doc/refman/5.7/en/select.html}{``select'' sobre MySQL}) particularizar la consulta a la base de datos. Esto puede implicar la selección de atributos específicos o incluso el uso de filtros sobre los atributos seleccionados. Por ejemplo, si se quisieran recuperar el número y el nombre de los clientes de Madrid, se podría personalizar la consulta añadiendo las condiciones correspondientes en la cláusula \texttt{where} como se muestra en siguiente código. Puede obtenerse más información sobre la \href{https://www.mysqltutorial.org/mysql-where/}{cláusula where}. Por comodidad se escribe aparte la consulta SQL en una variable para poder manejar más fácilmente la operativa en \textbf{R Software}. Escribir esta consulta puede ser lo que entrañe más dificultad. A continuación, se ejecuta la consulta con \texttt{dbGetQuery()} y se almacenan los resultados en un data frame para su uso posterior. Nuevamente se comprueba el resultado con la instrucción \texttt{sumary()}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SentenciaSQL\_NombresClientes }\OtherTok{\textless{}{-}} \StringTok{"Select CustomerNumber, CustomerName from customers where city = \textquotesingle{}Madrid\textquotesingle{}"}

\NormalTok{Consulta\_ClientesMadrid }\OtherTok{\textless{}{-}} \FunctionTok{dbGetQuery}\NormalTok{(mibbdd, SentenciaSQL\_NombresClientes)}

\FunctionTok{summary}\NormalTok{(Consulta\_ClientesMadrid)}
\end{Highlighting}
\end{Shaded}

Teniendo los resultados en data frames, ya es posible procesarlos en \textbf{R} como si fuera cualquier otro tipo de datos.

Obsérvese que las instrucciones siguientes serían equivalentes:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dbReadTable}\NormalTok{(mibbdd, }\StringTok{"customers"}\NormalTok{)}
\FunctionTok{dbGetQuery}\NormalTok{(mibbdd, }\StringTok{"select * from customers"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \texttt{dbSendQuery()} combinado con \texttt{dbFetch()}. La principal diferencia entre \texttt{dbSendQuery()} y \texttt{dbGetQuery()} es que la primera no recupera datos de la base de datos, y hay que traerlos explícitamente con la función \texttt{dbFetch()}. En el siguiente fragmento de código se muestra la forma de utilización de ambas funciones con un resultado exactamente igual que en el apartado anterior el anterior. En cualquier caso, es interesante resaltar antes algunos aspectos interesantes:
\end{enumerate}

\textbf{GEMA!! ¿Habría alguna forma de que esto tuviera un nivel de indentación?}

\begin{itemize}
\tightlist
\item
  La información de la consulta generada con \texttt{dbSendQuery()} puede ser mostrada usando la función \texttt{dbGetInfo()}.
\item
  Es posible recordar la consulta SQL que se utilizó en \texttt{dbSendQuery()} mediante la función \texttt{dbGetStatement()}.
\item
  La función \texttt{dbFetch()} tiene dos argumentos: la consulta y el número de registros a recuperar (que se puede consultar con la función ); si se quieren recuperar todos los registros que haya podido producir la consulta, debe pasarse el argumento \texttt{n=-1}.
\item
  Si se quiere saber el número de elementos que se han traído con la función \texttt{dbFetch()} se puede usar la función \texttt{dbGetRowCount()}
\end{itemize}

\textbf{Esto siguiente también necesitaría indentación ..\textgreater{} }

En el siguiente ejemplo se muestra cómo leer los datos de la tabla \texttt{customers} combinando \texttt{dbSendQuery()} con \texttt{dbFetch()}. La sentencia SQL es la misma pero se va a realizar la recuperación de los datos de forma diferente con la función \texttt{dbSendQuery()} (el resultado se almacena en el data frame \texttt{Consulta}). Con \texttt{dbFetch} se recuperan los resultados de las consulta desde la base de datos y se almacenan en la variable \texttt{Consulta\_ClientesMadrid\_condbSendQuery}. Con \texttt{dbGetInfo()} se muestran los metadatos de la consulta.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SentenciaSQL\_NombresClientes }\OtherTok{\textless{}{-}} \StringTok{"Select CustomerNumber, CustomerName from customers where city = \textquotesingle{}Madrid\textquotesingle{}"}

\NormalTok{Consulta }\OtherTok{\textless{}{-}} \FunctionTok{dbSendQuery}\NormalTok{(mibbdd, SentenciaSQL\_NombresClientes)}
\FunctionTok{dbGetInfo}\NormalTok{(Consulta)}
\CommentTok{\#\textgreater{} $statement}
\CommentTok{\#\textgreater{} [1] "Select CustomerNumber, CustomerName from customers where city = \textquotesingle{}Madrid\textquotesingle{}"}
\CommentTok{\#\textgreater{}}
\CommentTok{\#\textgreater{} $isSelect}
\CommentTok{\#\textgreater{} [1] 1}
\CommentTok{\#\textgreater{}}
\CommentTok{\#\textgreater{} $rowsAffected}
\CommentTok{\#\textgreater{} [1] {-}1}
\CommentTok{\#\textgreater{}}
\CommentTok{\#\textgreater{} $rowCount}
\CommentTok{\#\textgreater{} [1] 0}
\CommentTok{\#\textgreater{}}
\CommentTok{\#\textgreater{} $completed}
\CommentTok{\#\textgreater{} [1] 0}
\CommentTok{\#\textgreater{}}
\CommentTok{\#\textgreater{} $fieldDescription}
\CommentTok{\#\textgreater{} $fieldDescription[[1]]}
\CommentTok{\#\textgreater{} NULL}

\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Consulta realizada:"}\NormalTok{, }\FunctionTok{dbGetStatement}\NormalTok{(Consulta)))}

\NormalTok{Consulta\_ClientesMadrid\_condbSendQuery }\OtherTok{\textless{}{-}} \FunctionTok{dbFetch}\NormalTok{(Consulta, }\AttributeTok{n =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Número de elementos devueltos en la consulta"}\NormalTok{, }\FunctionTok{dbGetRowCount}\NormalTok{(Consulta)))}

\FunctionTok{summary}\NormalTok{(Consulta\_ClientesMadrid\_condbSendQuery)}
\end{Highlighting}
\end{Shaded}

Para extraer información de los resultados de la consulta, se puede usar la función \texttt{dbColumnInfo()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dbColumnInfo}\NormalTok{(Consulta)}
\CommentTok{\#\textgreater{} name Sclass type length}
\CommentTok{\#\textgreater{} 1 CustomerNumber integer INTEGER 11}
\CommentTok{\#\textgreater{} 2 CustomerName character VAR\_STRING 200}
\end{Highlighting}
\end{Shaded}

El driver \texttt{RMySQL} no proporciona funciones para conocer directamente el tipo y tamaño de los atributos de una tabla. Obsérvese, que mediante la función \texttt{dbSendQuery()} y \texttt{dbColumnInfo()} se podría obtener esta información haciendo una consulta que incluyera los atributos en los cuales se está interesados. Por ejemplo, para conocer el tipo de datos y tamaño de los atributos de la tabla \texttt{employees} se podría usar el siguiente fragmento de código. Con \texttt{dbColumnInfo()} se muestran los metadatos de implementación (operativos) de los atributos de la tabla \texttt{Employees}. Finalmente con la instrucción \texttt{dbClearResult(ConsultaEmployees)} se podrán limpiar los resutlados de la consulta para optimizar el sistema.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SentenciaSQL\_TablaEmployees }\OtherTok{\textless{}{-}} \StringTok{"Select * from employees"}
\NormalTok{ConsultaEmployees }\OtherTok{\textless{}{-}} \FunctionTok{dbSendQuery}\NormalTok{(mibbdd, SentenciaSQL\_TablaEmployees)}

\FunctionTok{dbColumnInfo}\NormalTok{(ConsultaEmployees)}

\CommentTok{\#\textgreater{} name Sclass type length}
\CommentTok{\#\textgreater{} 1 employeeNumber integer INTEGER 11}
\CommentTok{\#\textgreater{} 2 lastName character VAR\_STRING 200}
\CommentTok{\#\textgreater{} 3 firstName character VAR\_STRING 200}
\CommentTok{\#\textgreater{} 4 extension character VAR\_STRING 40}
\CommentTok{\#\textgreater{} 5 email character VAR\_STRING 400}
\CommentTok{\#\textgreater{} 6 officeCode character VAR\_STRING 40}
\CommentTok{\#\textgreater{} 7 reportsTo integer INTEGER 11}
\CommentTok{\#\textgreater{} 8 jobTitle character VAR\_STRING 200}

\FunctionTok{dbClearResult}\NormalTok{(ConsultaEmployees)}
\CommentTok{\#\textgreater{} [1] TRUE}
\end{Highlighting}
\end{Shaded}

\hypertarget{operaciones-de-inserciuxf3n-create-y-actualizaciuxf3n-update-de-datos}{%
\subsection{\texorpdfstring{Operaciones de inserción (\emph{create}) y actualización (\emph{update}) de datos}{Operaciones de inserción (create) y actualización (update) de datos}}\label{operaciones-de-inserciuxf3n-create-y-actualizaciuxf3n-update-de-datos}}

Antes de almacenar los datos en la base de datos, es necesario crear las estructuras necesarias, que como se ha venido diciendo serán tablas y atributos. Para ello se utilizarán las instrucciones especiales de SQL como lenguaje de definición de datos (LDD), esto incluye instrucciones para crear tablas (\href{https://dev.mysql.com/doc/refman/8.0/en/creating-tables.html}{\texttt{create\ table}}), para modificar tabla (\href{https://dev.mysql.com/doc/refman/8.0/en/alter-table.html}{\texttt{alter\ table}}) o para borrar una tabla (\href{https://dev.mysql.com/doc/refman/8.0/en/drop-table.html}{\texttt{drop\ table}}).

Para poder hacer operaciones con los datos, es preciso crear usuarios y asignarles los privilegios adecuados sobre las tablas y atributos. Esto se consigue utilizando las instrucciones especiales de SQL como lenguaje de administración de datos (LAD), lo que incluye instrucciones para crear usuarios (\href{https://dev.mysql.com/doc/refman/8.0/en/create-user.html}{\texttt{create\ user}}), modificar ciertos aspectos de los usuarios (\href{https://dev.mysql.com/doc/refman/8.0/en/alter-user.html}{\texttt{alter\ user}}), y para borrar usuarios (\href{https://dev.mysql.com/doc/refman/5.6/en/drop-user.html}{\texttt{drop\ user}}).

Entonces, un usuario de la base de datos que tenga privilegios suficientes sobre las estructuras creadas puede crear (\href{https://dev.mysql.com/doc/refman/8.0/en/insert.html}{\texttt{insert}}) o modificar (\href{https://dev.mysql.com/doc/refman/8.0/en/update.html}{\texttt{update}}) registros de datos usando las instrucciones específicas de SQL como lenguaje de manipulación de datos (LMD)-\href{https://dev.mysql.com/doc/refman/5.7/en/grant-tables.html}{véase cómo otorgar privilegios a un usuario para crear tablas}:

No obstante, y dado que el objetivo del libro es R, se deja fuera del alcance de este capítulo el uso de los aspectos DDL, DML y DAL de SQL, y se cubrirán mediante la instrucciones \texttt{dbWriteTable()} de \texttt{RMySQL} los aspectos de inserción y de actualización de los registros. \texttt{dbWriteTable()} se usa por tanto para exportar datos de R a una base de datos MySQL, y puede ser usado para las siguientes acciones, siempre y cuando el usuario que ejecute las acciones tenga suficientes permisos para realizarlas.

\hypertarget{crear-una-nueva-tabla-con-datos}{%
\subsubsection{Crear una nueva tabla con datos}\label{crear-una-nueva-tabla-con-datos}}

Se describe cómo crear una tabla nueva a partir de un data frame que se puebla con datos iniciales. Es importante observar que el data frame tendrá tantas columnas como atributos se pretenda o se necesita que tenga la tabla. Por ejemplo, se va a crear un data frame llamado \texttt{DatosPrueba} con dos columnas, una de tipo numérico llamada \texttt{CodPrueba}, y otra de tipo texto llamada \texttt{DatosPrueba}. En este ejemplo y a modo ilustrativo, los datos serán completamente aleatorios, pero en un caso real, el usuario deberá utilizar los valores adecuados. A continuación se construye el dataframe \texttt{dfDatosPruebas} y mediante \texttt{dbListTables()} se comprueba que la tabla no existe en la conexión a la base de datos. Finalmente con \texttt{dbWriteTable()} se creará la nueva tabla. Es importante tener en cuenta las posibles conexiones simultáneas a la base de datos porque se podrían generar problemas. Finalmente, con \texttt{dbWriteTable()} se podría comprobar si se ha creado correctamente otra tabla.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CodPrueba }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{26}\NormalTok{)}
\NormalTok{NombrePrueba }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(letters[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{26}\NormalTok{])}

\NormalTok{dfDatosPrueba }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(CodPrueba, NombrePrueba)}

\FunctionTok{dbListTables}\NormalTok{(mibbdd)}
\CommentTok{\#\textgreater{} [1] "Autor" "DatosPrueba\_16" "DatosPrueba\_22" "Datos\_Prueba\_01",}
\CommentTok{\#\textgreater{} [5] "Make" "Pelicula" "customers" "employees",}
\CommentTok{\#\textgreater{} [9] "offices" "orderdetails" "orders""payments",}
\CommentTok{\#\textgreater{} [13] "productlines" "products"}

\FunctionTok{dbWriteTable}\NormalTok{(mibbdd, }\StringTok{"DatosPrueba"}\NormalTok{, dfDatosPrueba, }\AttributeTok{overwrite =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{row.names =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] TRUE}
\CommentTok{\#\textgreater{}}
\FunctionTok{dbListTables}\NormalTok{(mibbdd)}
\CommentTok{\#\textgreater{} [1] "Autor" "DatosPrueba" "DatosPrueba\_16" "DatosPrueba\_22",}
\CommentTok{\#\textgreater{} [5] "Datos\_Prueba\_01" "Make" "Pelicula" "customers" ,}
\CommentTok{\#\textgreater{} [9] "employees" "offices" "orderdetails","orders" ,}
\CommentTok{\#\textgreater{} [13] "payments" "productlines" "products"}
\end{Highlighting}
\end{Shaded}

Es interesante pensar en la utilidad de este método para duplicar tablas.

\hypertarget{sobreescribir-una-tabla-existente-con-datos-actualizados}{%
\subsubsection{Sobreescribir una tabla existente con datos actualizados}\label{sobreescribir-una-tabla-existente-con-datos-actualizados}}

Cuando se trata de actualizar algunos valores de los atributos de la tabla o de añadir nuevos registros a la tabla, la operación será básicamente la misma que antes, pero se tendrá primero que leer la tabla y convertirla en un data frame para actualizar los valores correspondientemente en el data frame con los valores adecuados o añadir los nuevos valores (en este caso añadimos una nueva fila) y finalmente volver a utilizar el comando \texttt{dbWriteTable()} añadiendo el parámetro \texttt{overwrite\ =\ TRUE} para sobrescribir la tabla y asegurándonos que se añade el parámetro \texttt{row.names\ =FALSE}. En el siguiente ejemplo se actualizan los valores de una tupla específica.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dfDatosPrueba }\OtherTok{\textless{}{-}} \FunctionTok{dbReadTable}\NormalTok{(mibbdd, }\StringTok{"DatosPrueba"}\NormalTok{)}

\NormalTok{dfDatosPrueba}\SpecialCharTok{$}\NormalTok{NombrePrueba[}\DecValTok{25}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"en un lugar de la mancha"}

\FunctionTok{dbWriteTable}\NormalTok{(mibbdd, }\StringTok{"DatosPrueba"}\NormalTok{, dfDatosPrueba, }\AttributeTok{overwrite =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{row.names =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] TRUE}

\NormalTok{dfDatosPruebaModificado }\OtherTok{\textless{}{-}} \FunctionTok{dbReadTable}\NormalTok{(mibbdd, }\StringTok{"DatosPrueba"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{auxf1adir-nuevos-registros-a-una-tabla}{%
\subsubsection{Añadir nuevos registros a una tabla}\label{auxf1adir-nuevos-registros-a-una-tabla}}

Existen dos estrategias para añadir registros a una tabla.

La primera es utilizando la técnica de la sobreescritura descrita anteriormente. Para ello, se procede como antes: se carga la tabla en un data frame (en este caso \texttt{dfDatosPrueba}), se añaden nuevas filas (registros) al data frame (cargadas previamente en el data frame \texttt{dfNuevoRegistro}) usando \texttt{rbind()}, y a continuación se sobreescribe la tabla usando \texttt{dbWriteTable()}. En este fragmento de código se muestra cómo se implementa esta estrategia. En el siguiente ejemplo se muestra cómo añadir nuevos registros a una tabla sobreescribiéndola completamente.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dfDatosPrueba }\OtherTok{\textless{}{-}} \FunctionTok{dbReadTable}\NormalTok{(mibbdd, }\StringTok{"DatosPrueba"}\NormalTok{)}

\NormalTok{dfNuevoRegistro }\OtherTok{\textless{}{-}} \FunctionTok{as.list}\NormalTok{(dfDatosPrueba)}

\NormalTok{dfNuevoRegistro}\SpecialCharTok{$}\NormalTok{CodPrueba }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{27}\NormalTok{)}
\NormalTok{dfNuevoRegistro}\SpecialCharTok{$}\NormalTok{NombrePrueba }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Un Valor Nuevo"}\NormalTok{)}

\NormalTok{dfDatosPrueba }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(dfDatosPrueba, dfNuevoRegistro)}

\FunctionTok{dbWriteTable}\NormalTok{(mibbdd, }\StringTok{"DatosPrueba"}\NormalTok{, dfDatosPrueba, }\AttributeTok{overwrite =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{row.names =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] TRUE}
\end{Highlighting}
\end{Shaded}

La opción anterior puede ser interesante si la tabla no tiene muchos registros y el coste computacional no es muy grande. Pero si se tienen muchos registros es preferible usar otra estrategia para añadir un nuevo registro a la tabla. En esta ocasión, se puede hacer creando un data frame compatible con la estructura de la tabla, y ejecutar la instrucción \texttt{dbWriteTable()} asegurando que se pone el parámetro \texttt{append\ =\ TRUE}. Esto añadirá el nuevo registro al final de la tabla. El siguiente fragmento de código muestra cómo realizar esta operación.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dfDatosPruebaNuevos }\OtherTok{\textless{}{-}} \FunctionTok{as.list}\NormalTok{(dfDatosPrueba)}

\NormalTok{dfDatosPruebaNuevos}\SpecialCharTok{$}\NormalTok{CodPrueba }\OtherTok{\textless{}{-}} \DecValTok{28}
\NormalTok{dfDatosPruebaNuevos}\SpecialCharTok{$}\NormalTok{NombrePrueba }\OtherTok{\textless{}{-}} \StringTok{"Otro valor nuevo"}

\NormalTok{dfDatosPruebaNuevos }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(dfDatosPruebaNuevos)}

\FunctionTok{dbWriteTable}\NormalTok{(mibbdd, }\StringTok{"DatosPrueba"}\NormalTok{, dfDatosPruebaNuevos, }\AttributeTok{append =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{row.names =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] TRUE}
\end{Highlighting}
\end{Shaded}

\hypertarget{inserciuxf3n-con-consulta-sql-usando-la-instrucciuxf3n-dbsendquery}{%
\subsubsection{\texorpdfstring{Inserción con consulta SQL usando la instrucción \texttt{dbSendQuery()}}{Inserción con consulta SQL usando la instrucción dbSendQuery()}}\label{inserciuxf3n-con-consulta-sql-usando-la-instrucciuxf3n-dbsendquery}}

Una última forma de insertar valores en una tabla es mediante la instrucción \texttt{dbSendQuery} utilizando una consulta de inserción \texttt{insert}. En el siguiente ejemplo se muestra cómo insertar tuplas o registros mediante \texttt{dbSendQuery()}, en este caso, se añaden datos completamente aleatorios.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SentenciaSQL\_Insercion }\OtherTok{\textless{}{-}} \StringTok{"insert into DatosPrueba value (29, \textquotesingle{}Una tercera forma\textquotesingle{})"}

\FunctionTok{dbSendQuery}\NormalTok{(mibbdd, SentenciaSQL\_Insercion)}
\CommentTok{\#\textgreater{} \textless{}MySQLResult:{-}365007472,0,23\textgreater{}}
\end{Highlighting}
\end{Shaded}

\hypertarget{operaciones-de-borrado-de-datos-delete}{%
\subsection{\texorpdfstring{Operaciones de Borrado de datos (\emph{delete})}{Operaciones de Borrado de datos (delete)}}\label{operaciones-de-borrado-de-datos-delete}}

Finalmente, se describen las operaciones de borrado. Análogamente a como se hacían las operaciones de inserción, se pueden hacer de dos formas:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Borrado de valores usando \texttt{dbWriteTable()} con sobreescritura}: Esto implica extraer todos los datos de la tabla, borra el registro o los registros correspondientes y luego sobreescribir nuevamente la tabla en la base de datos mediante la instrucción \texttt{dbWriteTable()} con la opción \texttt{overwrite\ =\ TRUE}. El siguiente fragmento de código muestra cómo hacerlo, y finalmente para ver los resultados se pueden usar la función \texttt{summary()}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{dfDatosPrueba }\OtherTok{\textless{}{-}} \FunctionTok{dbReadTable}\NormalTok{(mibbdd, }\StringTok{"DatosPrueba"}\NormalTok{)}

\NormalTok{dfDatosPrueba }\OtherTok{\textless{}{-}}\NormalTok{ dfDatosPrueba[dfDatosPrueba}\SpecialCharTok{$}\NormalTok{CodPrueba }\SpecialCharTok{\textless{}} \DecValTok{25}\NormalTok{, ]}

\NormalTok{dfNuevoRegistro}\SpecialCharTok{$}\NormalTok{CodPrueba }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{101}\NormalTok{)}
\NormalTok{dfNuevoRegistro}\SpecialCharTok{$}\NormalTok{NombrePrueba }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Un Valor Nuevo"}\NormalTok{)}

\NormalTok{dfDatosPrueba }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(dfDatosPrueba, dfNuevoRegistro)}

\FunctionTok{dbWriteTable}\NormalTok{(mibbdd, }\StringTok{"DatosPrueba"}\NormalTok{, dfDatosPrueba, }\AttributeTok{overwrite =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{row.names =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] TRUE}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Borrado de registros con consulta SQL en \texttt{dbSendQuery()}}: Es posible utilizar una sentencia SQL de borrado \href{https://dev.mysql.com/doc/refman/8.0/en/delete.html}{delete} con la instrucción \texttt{dbSendQuery()} para borrar registros de la base de datos. El siguiente fragmento de código muestra cómo hacerlo.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Se usa una sentencia SQL de borrado. El criterio de borrado es completamente aleatorio a efectos ilustrativos.}
\NormalTok{SentenciaSQL\_Eliminación }\OtherTok{\textless{}{-}} \StringTok{"delete from DatosPrueba where CodPrueba \textgreater{} 10"}
\FunctionTok{dbSendQuery}\NormalTok{(mibbdd, SentenciaSQL\_Eliminación)}
\CommentTok{\#\textgreater{} \textless{}MySQLResult:1,0,30\textgreater{}}
\end{Highlighting}
\end{Shaded}

\hypertarget{borrado-de-toda-la-tabla-con-consulta-sql-en-dbsendquery}{%
\subsubsection{\texorpdfstring{Borrado de toda la tabla con consulta SQL en \texttt{dbSendQuery}}{Borrado de toda la tabla con consulta SQL en dbSendQuery}}\label{borrado-de-toda-la-tabla-con-consulta-sql-en-dbsendquery}}

Finalmente si fuera necesario eliminar toda la tabla se podría usar una sentencia \href{https://dev.mysql.com/doc/refman/8.0/en/drop-table.html}{drop table}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dbSendQuery}\NormalTok{(mibbdd, }\StringTok{"drop table DatosPrueba"}\NormalTok{)}
\CommentTok{\#\textgreater{} \textless{}MySQLResult:0,0,31\textgreater{}}
\FunctionTok{dbDisconnect}\NormalTok{(mibbdd)}
\CommentTok{\#\textgreater{} [1] TRUE}
\end{Highlighting}
\end{Shaded}

\begin{infobox_resume}

\textbf{Resumen}

En este capítulo se ha presentado los fundamentos de las bases de datos relacionales. Es importante tener presente los siguientes aspectos:

\begin{itemize}
\tightlist
\item
  Los datos en las bases de datos se corresponden a valores de atributos relevantes de entidades del mundo real.
\item
  Los datos de una base de datos son una percepción u observación del mundo real.
\item
  Los datos son las materias primas de los procesos de negocio.
\item
  Los sistemas de Información dan soporte a los procesos de negocio.
\item
  Los datos son un elemento fundamental de los sistemas de información.
\item
  Existen diferentes vistas o percepciones de los datos que se van refinando a medida que se implementan las bases de datos. Estas vistas se representan mediante diferentes modelos: conceptual, lógico y físico.
\item
  Cada uno de los modelos anteriores utiliza lenguajes que tienen capacidades descriptivas (y por tanto, limitaciones diferentes).
\item
  SQL (\textbf{Structured Query Language}) es el lenguaje más comúnmente utilizado en operaciones sobre el modelo físico de bases de datos relacionales.
\item
  SQL se puede utilizar como Lenguaje de Definición de Datos (LDD), como Lenguaje de Manipulación de Datos (LMD), y como Lenguaje de Administración de Datos (LAD).
\item
  La sintaxis de SQL depende fuertemente del Sistema Gestor de Bases de Datos Relacionales que lo implemente.
\item
  \textbf{R Software}, a través del driver específico, permite manejar bases de datos implementando las operaciones CRUD.
\end{itemize}

\end{infobox_resume}

\hypertarget{datos-no-sql}{%
\chapter{Gesitón de bases de datos NoSQL}\label{datos-no-sql}}

\emph{Ricardo Pérez-Castillo e Ismael Caballero}

\hypertarget{introducciuxf3n-al-big-data}{%
\section{Introducción al Big Data}\label{introducciuxf3n-al-big-data}}

Para pensar en Big Data, se tiene que saber que actualmente se vive en la era de la información, con un teléfono móvil en cada bolsillo, un ordenador portátil en cada mochila y grandes sistemas de tecnología funcionando diariamente mandando datos y datos cada segundo. Se ve claramente que el mundo tiene más datos que nunca, pero esto no es todo, ya que día a día crece aún más \citep{Lopez2012}.

En particular, el volumen de datos disponibles para las empresas aumentó drásticamente desde 2004. En 2004, la cantidad total de datos almacenados en Internet fue de 1 petabyte (equivalente a 100 años de todo el contenido de televisión). Para 2011 la cantidad total de información almacenada en todo el mundo fue de 1 zettabyte (1 millón de petabytes o 36 millones de años de video de alta definición {[}HD{]}). Para el año 2015, alcanzó 7.9 zettabytes (o 7.9 millones de petabytes) y para 2020 se disparó a 35 zettabytes (o 35 millones de petabytes). Este gran volumen de datos y su crecimiento continuo y exponencial, supera las capacidades de las herramientas de datos tradicionales para capturar, almacenar, administrar y analizar los datos \citep{Kalyvas2014}. Por este motivo se hace necesario el uso de métodos, técnicas y herramientas de gestión de datos nuevas. Este espacio es el que cubre Big Data.

Big data es un término abstracto que en cierta medida se ha puesto de moda en diferentes ámbitos: desde los negocios, marketing, social media, y en diferentes ingenierías como informática, sistemas de información, almacenamiento y recuperación de datos, etc. Big data es un término que hace referencia al gran volumen de datos (tanto estructurados como no estructurados) y que inundan los negocios día a día. Pero no es la cantidad de datos lo más relevante. Lo que realmente importa es lo que las organizaciones hacen con los datos. Los grandes volúmenes de datos grandes se pueden analizar en busca de ideas que conducen a mejores decisiones y movimientos comerciales estratégicos \citep{SAS2017}.

Cuando se acumulan grandes volúmenes de datos se plantea la necesidad de ver qué se puede hacer con ellos. Esto redunda en la necesidad de gestionar los datos con una finalidad organizativa, a disponer de tecnología y metodologías específicas. La misma gestión de datos nos lleva a generar información que sea relevante en el contexto de la organización, es decir, a generar conocimiento para la acción, y que sea aplicable por ejemplo a la toma de decisiones, al diseño de acciones para la organización o a la elaboración de planes estratégicos \citep{Alsina2017}.

Por lo tanto, cuando se habla de datos masivos, se está hablando también de gestión de la información y de generación de conocimiento para la acción. Este campo científico es el que da las pautas metodológicas para gestionar grandes volúmenes de datos con el fin de crear valor mediante una serie de procesos y procedimientos. Pero, además, hemos de tener en cuenta que debemos contar con la tecnología, para capturar datos, procesarlos, analizarlos e interpretarlos de manera rápida y eficiente \citep[\citet{GomezGarcia2015}]{Alsina2017}.

Al final, del conjunto de definiciones previas, se puede concluir que Big Data es el \textbf{\emph{``conjunto de datos masivos heterogéneos que supera la capacidad del software habitual para ser capturados, gestionados y procesados en un tiempo razonable''}}. Esta definición tiene en cuenta tres de las V's del Big Data (presentada en la sección \ref{VsBigData}): volumen, variabilidad y velocidad. Así, cuando se habla de Big Data nos referimos a conjuntos de datos o combinaciones de conjuntos de datos cuyo tamaño (Volumen), complejidad (Variabilidad) y velocidad de crecimiento (Velocidad) dificultan su captura, gestión, procesamiento o análisis mediante tecnologías y herramientas convencionales, tales como bases de datos relacionales y estadísticas convencionales o paquetes de visualización, dentro del tiempo necesario para que sean útiles.

\hypertarget{VsBigData}{%
\section{Las V's del Big Data}\label{VsBigData}}

Originalmente se han definido las tres \textbf{V's} del Big Data como: \emph{Volumen}, \emph{Variedad} y \emph{Velocidad}. Los cuales son tres aspectos clave que deben considerar los entornos Big Data.

El \textbf{volumen} se refiere a la cantidad de datos que son generados cada segundo, minuto y día en nuestro entorno. Es la característica más asociada al Big Data, ya que hace referencia a las cantidades masivas de datos que se almacenan con la finalidad de procesar dicha información, transformando los datos en acciones. La importancia del volumen de datos ya se trató en las secciones anteriores. Cada vez las personas están más conectadas al mundo digital por lo que se genera más y más datos. Para algunas empresas, el estar en el mundo digital es algo obligatorio, por lo que la cantidad de datos generados es aún mayor. Por ejemplo, una empresa que vende sus productos únicamente a través de un canal online, le convendría implantar tecnología Big Data para procesar toda aquella información que recoge su página web rastreando todas las acciones que lleva a cabo el cliente; conocer donde cliquea más veces, cuántas veces ha pasado por el carrito de la compra, cuáles son los productos más vistos, las páginas más visitadas, etc.

La \textbf{velocidad} se refiere a los datos en movimiento por las constantes interconexiones que realizamos, es decir, a la rapidez en la que son creados, almacenados y procesados en tiempo real. Para los procesos en los que el tiempo resulta fundamental, tales como la detección de fraude en una transacción bancaria o la monitorización de un evento en redes sociales, estos tipos de datos deben estudiarse en tiempo real para que resulten útiles para el negocio y se consigan conclusiones efectivas. El procesamiento de los datos masivos se debe hacer en el mínimo tiempo posible, y en algunas ocasiones en tiempo real. Es un tema especialmente delicado cuando se deben tomar decisiones en contextos críticos; como por ejemplo, en catástrofes naturales o pandemias, o cuando se efectúa el seguimiento de una campaña analizando comentarios de los actores a quienes esta va dirigida, para ir reorientándola en función de la retroalimentaciones que fluye en redes sociales \citep{Alsina2017}.

La \textbf{variedad} se refiere a las formas, tipos y fuentes en las que se registran los datos. Estos datos pueden ser datos estructurados y fáciles de gestionar como son las bases de datos, o datos no estructurados, entre los que se incluyen documentos de texto, correos electrónicos, datos de sensores, audios, vídeos o imágenes que tenemos en nuestro dispositivo móvil, hasta publicaciones en nuestros perfiles de redes sociales, artículos que leemos en blogs, las secuencias de click que hacemos en una misma página, formularios de registro e infinidad de acciones más que realizamos desde nuestro Smartphone, Tablet y ordenador. Estos últimos datos requieren de una herramienta específica, debido a que el tratamiento de la información es totalmente diferente con respecto a los datos estructurados. Para ello, las empresas necesitan integrar, observar y procesar datos que son recogidos a través de múltiples fuentes de información con herramientas apropiadas.

Después, otros autores han ido incorporando progresivamente otras V's que añaden otros aspectos clave para ser considerados. De esta forma, se se habla de V's en Big Data no se puedepasar por alto la principal característica del análisis de datos que es la V de \textbf{valor} de los datos. Así pues, en los últimos trabajos se habla de las 7 V's del Big Data \citep{IIC2016}: \textbf{veracidad}, \textbf{viabilidad}, \textbf{visualización}, y \textbf{valor} de los datos.

\hypertarget{fuentes-de-datos-en-entornos-big-data}{%
\section{Fuentes de Datos en entornos Big Data}\label{fuentes-de-datos-en-entornos-big-data}}

En esta sección se muestran los diferentes tipos de datos y flujos de datos, como son: estructurados, no estructurados o semiestructurados.

\begin{itemize}
\tightlist
\item
  \textbf{Datos estructurados}: Son aquellos datos que tienen Longitud y formato: como las fechas, los números o las cadenas de caracteres. En esta categoría entran los que se compilan en los censos de población, los diferentes tipos de encuestas, los datos de transacciones bancarias, las compras en tiendas online, etc.
\item
  \textbf{Datos no estructurados}: Son los datos que carecen de un formato determinado y no pueden ser almacenados en una tabla. Pueden ser de tipo texto (los que generan los usuarios de foros, redes sociales, documentos de Word), y los de tipo no-texto (cualquier fichero de imagen, audio, vídeo). Dentro de esta categoría están: archivos de audio, video, fotografías, texto, correos electrónicos, etc. Conviene saber que este tipo de datos no tiene campos fijos y normalmente se tiene poco control sobre ellos. Su manipulación requiere herramientas como Hadoop (la más popular) y/o bases de datos \textbf{\emph{NoSQL}}, entre otras.
\item
  \textbf{Datos semiestructurados}: Poseen organización interna o marcadores que facilita el tratamiento de sus elementos. No pertenecen a bases de datos relacionales. En este caso estaríamos hablando de documentos XML, HTML o los datos almacenados en bases de datos \textbf{\emph{NoSQL}}. Los cuales tienen una cierta estructura, aunque sin llegar a estar totalmente estructurados. Además, de acuerdo con otras fuentes, también podemos considerar en esta clasificación datos Multi-estructurados o híbridos, entre los que pondríamos datos de mercados emergentes, \emph{e-commerce}, datos meteorológicos.
\end{itemize}

\hypertarget{bases-de-datos-relacionales-vs.-nosql}{%
\section{Bases de datos Relacionales vs.~NoSQL}\label{bases-de-datos-relacionales-vs.-nosql}}

Lo que hace que Big Data sea tan útil para muchas empresas es el hecho de que proporciona respuestas a muchas preguntas que las empresas ni siquiera sabían que tenían. En otras palabras, proporciona un punto de referencia. Con una cantidad tan grande de información, los datos pueden ser moldeados o probados de cualquier manera que la empresa considere adecuada. Al hacerlo, las organizaciones son capaces de identificar los problemas de una forma más comprensible.

La recopilación de grandes cantidades de datos y la búsqueda de tendencias dentro de los datos permiten que las empresas se muevan mucho más rápidamente, sin problemas y de manera eficiente. También les permite eliminar las áreas problemáticas antes de que los problemas acaben con sus beneficios o su reputación.

El análisis de Big Data ayuda a las organizaciones a aprovechar sus datos y utilizarlos para identificar nuevas oportunidades. Eso, a su vez, conduce a movimientos de negocios más inteligentes, operaciones más eficientes, mayores ganancias y clientes más satisfechos Las empresas con más éxito con Big Data consiguen valor de las siguientes formas:

\begin{itemize}
\tightlist
\item
  \textbf{Reducción de coste.} Las grandes tecnologías de datos, como Hadoop y el análisis basado en la nube, aportan importantes ventajas en términos de costes cuando se trata de almacenar grandes cantidades de datos, además de identificar maneras más eficientes de hacer negocios.
\item
  \textbf{Más rápida y mejor toma de decisiones.} Con la velocidad de Hadoop y la analítica en memoria, combinada con la capacidad de analizar nuevas fuentes de datos, las empresas pueden analizar la información inmediatamente y tomar decisiones basadas en lo que han aprendido.
\item
  \textbf{Nuevos productos y servicios.} Con la capacidad de medir las necesidades de los clientes y la satisfacción a través de análisis viene el poder de dar a los clientes lo que quieren. Con la analítica de Big Data, más empresas están creando nuevos productos para satisfacer las necesidades de los clientes.
\end{itemize}

Existen ciertas diferencias entre las fuentes de datos tradicionales y las nuevas fuentes de datos que considera Big Data como se resumen en la Tabla \ref{tab:tab1}.

\begin{longtable}[]{@{}cc@{}}
\caption{\label{tab:tab1}. Diferencias entre tecnologías tradicionales y tecnologías Big Data.}\tabularnewline
\toprule()
Tecnologías Tradicionales & Tecnologías Big Data \\
\midrule()
\endfirsthead
\toprule()
Tecnologías Tradicionales & Tecnologías Big Data \\
\midrule()
\endhead
Bases de datos Relacionales & Bases de datos Relacionales + \emph{NoSQL} \\
Consultas & Consultas, Caputras y Procesamientos \\
Datos Internos & Datos Heterogéneos \\
Ámbito de la Informática & Todos los ámbitos \\
\bottomrule()
\end{longtable}

En primer lugar, la tecnología tradicional de almacenamiento y gestión de datos (desde final de los años 80) ha sido las bases de datos relacionales. Aunque las bases de datos relacionales no son ni mucho menos una tecnología en desuso, los entornos Big Data consideran otras tecnologías como por ejemplo las bases de datos \textbf{\emph{NoSQL}}, las cuales son bases de datos no relacionales optimizadas para modelos de datos sin esquema y de desempeño escalable. También son ampliamente conocidas por su facilidad de desarrollo, baja latencia y resiliencia. Utilizan una variedad de modelos de datos, como los almacenes de valor clave en memoria, de gráficos, de documentos y en columnas. Esta página incluye recursos que le servirán para comenzar a usar las bases de datos \textbf{\emph{NoSQL}} \citep{AWS2018}.

Otra gran diferencia respecto a las tecnologías tradicionales es que los entornos Big Data, no sólo se centran en la consulta de datos, sino también en su captura y procesamiento (véase Tabla 1). Además, las fuentes de datos pueden proveer datos heterogéneos con formatos heterogéneos. Estas diferencias hacen que un ámbito puramente informático no sea necesario en los entornos Big Data.

\hypertarget{bases-de-datos-nosql}{%
\section{Bases de datos NoSQL}\label{bases-de-datos-nosql}}

\hypertarget{definiciuxf3n-de-bases-de-datos-nosql}{%
\subsection{Definición de bases de datos NoSQL}\label{definiciuxf3n-de-bases-de-datos-nosql}}

El término \textbf{\emph{NoSQL}} se usa la primera vez en 1998 para referirse a una base de datos relacional sin SQL \citep{Strozzi98}. \textbf{\emph{NoSQL}} no significa estar en contra de SQL, y de hecho esto suele ser una falacia encontrada en la literatura. Sin embargo, para determinados problemas hay otras soluciones de almacenamiento más apropiadas.

Hay una gran variedad de sistemas de gestión de bases de datos que no usan SQL como el principal lenguaje de consultas. Los datos almacenados no requieren estructuras fijas como tablas y no se garantiza completamente los \emph{principios ACID} que si deben cumplir las bases de datos relacionales (SQL):

\begin{itemize}
\tightlist
\item
  \textbf{Atomic} (Atomicidad). Las transacciones se ejecutan completamente o no. Si fallan es como si ni siquiera se hubieran intentado ejecutar.
\item
  \textbf{Consistency} (Consistencia). Un sistema consistente garantiza cualquier transacción llevará a la base de datos de un estado válido a otro estado válido. Cualquier dato que se escriba en la base de datos tiene que ser válido de acuerdo con todas las reglas de integridad definidas en el modelo.
\item
  \textbf{Isolation} (Aislamiento). Cuando varias transacciones se ejecutan en paralelo, cada una de ellas ve el sistema de la misma manera a como lo vería si se ejecutaran de forma aislada o secuencial.
\item
  \textbf{Durability} (Durabilidad). El resultado de las transacciones es un cambio en el estado del sistema persistente. Si apagamos la máquina y la arrancamos de nuevo el cambio producido por la transacción aún está presente.
\end{itemize}

Los sistemas \textbf{\emph{NoSQL}} se denominan a veces \emph{``no sólo SQL''} para subrayar el hecho de que también pueden soportar lenguajes de consulta de tipo SQL.

Se puede decir que la aparición del término \textbf{\emph{NoSQL}} aparece con la llegada de la web 2.0 ya que hasta ese momento sólo subían contenido a la red aquellas empresas que tenían un portal, pero con la llegada de aplicaciones como \emph{Facebook}, \emph{Twitter} o \emph{Youtube}, entre otras, cualquier usuario podía subir contenido, provocando así un crecimiento exponencial de los datos \citep{Acens2014}.

Es en este momento cuando empiezan a aparecer los primeros problemas de la gestión de toda esa información almacenada en bases de datos relacionales. En un principio, para solucionar estos problemas de accesibilidad, las empresas optaron por utilizar un mayor número de máquinas, pero pronto se dieron cuenta de que esto no solucionaba el problema, además de ser una solución muy cara. La otra solución era la creación de sistemas pensados para un uso específico que con el paso del tiempo han dado lugar a soluciones robustas, apareciendo así el movimiento \textbf{\emph{NoSQL}} \citep{Acens2014}.

Por lo tanto, hablar de bases de datos \textbf{\emph{NoSQL}} es hablar de estructuras que permiten almacenar información en aquellas situaciones en las que las bases de datos relacionales generan ciertos problemas debido principalmente a problemas de escalabilidad y rendimiento de las bases de datos relacionales donde se dan cita miles de usuarios concurrentes y con millones de consultas diarias.

Además de lo comentado anteriormente, las bases de datos \textbf{\emph{NoSQL}} son sistemas de almacenamiento de información que no cumplen con el esquema entidad--relación. Tampoco utilizan una estructura de datos en forma de tabla donde se van almacenando los datos, sino que para el almacenamiento hacen uso de otros formatos como clave--valor, mapeo de columnas o grafos.

Las bases de datos relacionales modernas típicamente han mostrado poca eficiencia en determinadas aplicaciones que usan los datos de forma intensiva, incluyendo el indexado de un gran número de documentos, la presentación de páginas en sitios que tienen gran tráfico, y en sitios de streaming audiovisual. Las implementaciones típicas de los (SGBDR) se han afinado o bien para una cantidad pequeña pero frecuente de lecturas y escrituras o para un gran conjunto de transacciones que tiene pocos accesos de escritura. Por otra parte, \textbf{\emph{NoSQL}} puede servir gran cantidad de carga de lecturas y escrituras.

\hypertarget{necesidades-no-cubiertas-por-las-bases-de-datos-relacionales}{%
\subsection{Necesidades no cubiertas por las bases de datos relacionales}\label{necesidades-no-cubiertas-por-las-bases-de-datos-relacionales}}

Las bases de datos no relacionales, también conocidas como bases de datos NoSQL, son principalmente bases de datos distribuidas de código abierto y escalables horizontalmente. Además, son bases de datos que no usan esquemas, por lo que ofrecen una fácil replicación y API sencilla para el acceso a un gran volumen de datos.

La necesidad de este tipo de bases de datos surge porque hay fuentes de datos que son difíciles de modelar en bases de datos relacionales. Algunos ejemplos son texto (datos no estructurados), procesado en \emph{streaming} (flujo continuo de datos), bases de datos científicas (estructuras multidimensionales).

Además de estos tipos de fuentes de datos, las características de las nuevas aplicaciones de internet tales como redes sociales, juegos online, etc., hacen que las bases de datos no relacionales sean necesarias para conseguir mayor velocidad, escalabilidad, independencia de la localización, disponibilidad y mejor gestión de todos los tipos de datos.

\begin{itemize}
\tightlist
\item
  \textbf{Velocidad}. Para demostrar la importancia de la velocidad en internet, sirva como ejemplo como dos grandes compañías monetizan la velocidad de acceso: Amazon tiene estudiado que cuando el tiempo de respuesta disminuye 100ms los ingresos se compensan en un 1\%. Por otra parte, Yahoo asegura que el tráfico aumenta en un 9\% cuando el rendimiento mejora en 400ms. De ahí la importancia de este factor para la evolución hacia bases de datos NoSQL, y de hecho este factor entraña una de las conocidas \textbf{V's} del Big Data.
\item
  \textbf{Escalabilidad}. Al principio la web se consideró una interfaz más, pero no es sólo eso; se ha convertido en un elemento generador y consumidor de datos (fundamentalmente semiestructurados y no estructurados). En el contexto actual, las compañías necesitan mantener la respuesta rápida, aunque se incremente cualquiera de los siguientes elementos: (i) el número de usuarios simultáneos; (ii) el volumen de datos; (iii) se ejecutan en clústeres de máquinas baratas. Además, la arquitectura de los sistemas NoSQL permite: (i) escalar sin disminuir el rendimiento; (ii) añadir nodos sobre la marcha sin interrupciones del servicio; (iii) no generan cuellos de botella. La Figura \ref{fig:FigNoSQL1} muestra una representación comparativa de la escalabilidad de bases de datos NoSQL frente a relacionales. Como se observa, aunque las bases de datos tienen un mejor desempeño para volúmenes de datos reducidos, este se reduce drásticamente para grandes volúmenes de datos. Mientras, el rendimiento de las bases de datos NoSQL tiende a ser constante, por lo que escalan mejor para datos masivos.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{img/NoSQL1} 

}

\caption{Comparativa de escalabilidad en bases de datos relacionales y NoSQL (adaptada de [@LO2017]).}\label{fig:FigNoSQL1}
\end{figure}

\begin{itemize}
\tightlist
\item
  \textbf{Independencia de la localización}. La globalización del mercado en \emph{World Wide Web} obliga a dar servicio rápido y en todas partes del mundo. Las bases de datos no relacionales son distribuidas de acuerdo con diferentes arquitecturas como ``principal-secundario'', o \emph{``peer-to-peer''}.
\item
  \textbf{Disponibilidad}. Similar a la independencia de la localización, la disponibilidad en el mercado WWW es uno de los factores más críticos, ya que se espera una disponibilidad de servicio 24x7. Es decir, hay que pasar de una alta disponibilidad a la disponibilidad continua, cuyas características son: (i) diseño sin principal-secundario; (ii) \emph{Multi-data center}; (iii) disponibilidad \emph{cloud}; (iv) Copias de datos y funcionalidad en múltiples localizaciones.
\item
  \textbf{Gestión de todos los tipos de datos}. Un factor clave en las bases de datos no relacionales es la necesidad de manejar nuevos tipos de datos, tanto estructurados, como no estructurados y semiestructurados; y todo esto sin perder el enfoque de un almacenamiento eficiente. En ese sentido, a menudo, las bases de datos \textbf{\emph{NoSQL}} están altamente optimizadas para las operaciones recuperar y agregar, y normalmente no ofrecen mucho más que la funcionalidad de almacenar los registros (p.ej. almacenamiento clave-valor). La pérdida de flexibilidad en tiempo de ejecución, comparado con los sistemas SQL clásicos, se ve compensada por ganancias significativas en escalabilidad y rendimiento cuando se trata con ciertos modelos de datos.
\end{itemize}

\hypertarget{tipos-de-almacenamiento-en-bases-de-datos-nosql}{%
\subsection{Tipos de almacenamiento en bases de datos NoSQL}\label{tipos-de-almacenamiento-en-bases-de-datos-nosql}}

Se pueden distinguir al menos 4 tipos de bases de datos \textbf{\emph{NoSQL}} \citep{Hecht2011}: clave-valor, documental, en grafo, orientadas a columnas.

\begin{itemize}
\tightlist
\item
  \textbf{Almacenamiento clave-valor}. Los datos se almacenan de forma similar a los mapas o diccionarios de datos, donde se accede al dato a partir de una clave única. Los valores (datos) son aislados e independientes entre ellos, y no son interpretados por el sistema. Pueden ser variables simples como enteros o caracteres, u objetos. Por otro lado, este sistema de almacenamiento carece de una estructura de datos clara y establecida, por lo que no requiere un formateo de los datos muy estricto. Son útiles para operaciones simples basadas en las claves. \emph{Apache Cassandra} es la tecnología de almacenamiento clave-valor más reconocida por los usuarios.
\item
  \textbf{Almacenamiento documental}. Este tipo de base de datos almacena datos semi-estructurados. Los datos se llaman documentos, y pueden estar formateados en XML, JSON, Binary JSON o el que acepte la misma base de datos, pero suele ser un formato de texto. Un ejemplo de cómo se usa lo encontramos en un blog: se almacena el autor, la fecha, el título, el resumen y el contenido del post. Todos los documentos tienen una clave única con la que pueden ser accedidos e identificados explícitamente. Estos documentos no son opacos al sistema, por lo que pueden ser interpretados y lanzar consultas sobre ellos (véase la Fig. \ref{fig:FigNoSQL3}). \emph{CouchDB} o \emph{MongoDB} son quizá las más conocidas. Hay que hacer mención especial a \emph{MapReduce}, una tecnología de \emph{Google} inicialmente diseñada para su algoritmo PageRank, que permite seleccionar un subconjunto de datos, agruparlos o reducirlos y cargarlos en otra colección, y a Hadoop que es una tecnología de \emph{Apache} diseñada para almacenar y procesar grandes cantidades de datos. Por ejemplo, \emph{MongoDB} es una base de datos orientada a documentos. Los documentos se guardan en BSON, que es una forma de representar de forma binaria objetos JSON. De esta forma, con el comando insert y pasando un objeto JSON, MongoDB crea automáticamente un documento y lo añade en la base de datos generando un ObjectId para el nuevo documento \citep{Rubenfa2014}. Este objeto está especialmente pensado para garantizar unicidad en entornos distribuidos como MongoDB. El campo está compuesto por 12 bytes. Los cuatro primeros bytes son un \emph{timestamp} con los segundos desde el \emph{epoch} de Unix; los tres siguientes bytes representan el identificador único de la máquina; los dos siguientes el identificador del proceso; y para finalizar los últimos tres bytes, son un campo incremental. En definitiva, los nueve primeros bytes nos garantizan un identificador único por segundo, máquina y proceso. Los tres últimos bytes, nos garantizan que cada segundo podemos insertar 2\textsuperscript{24} = 16.777.216 documentos con un identificador distinto. Dado que el \emph{ObjectId} está compuesto de esa manera, esto proporciona funcionalidades muy útiles. La primera es que nos puede dar una indicación del orden de creación de los documentos. También sirve para obtener la fecha de creación del documento.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{img/NoSQL3} 

}

\caption{Ejemplo representativo de base de datos NoSQL documental. Adaptado de [@Sanchez2017].}\label{fig:FigNoSQL3}
\end{figure}

\begin{itemize}
\tightlist
\item
  \textbf{Almacenamiento en grafo}. Este tipo de almacenamiento maneja datos semi-estructurados y está basado en teoría de grafos. En estas bases de datos se establece que la información son los nodos y las relaciones entre la información son las aristas (algo similar al modelo relacional). Su mayor uso se contempla en casos de relacionar grandes cantidades de datos que pueden ser muy variables. Por ejemplo, los nodos pueden contener objetos, variables y atributos diferentes en unos y otros. Las operaciones de consulta con \emph{join} se sustituyen por recorridos a través del grafo, y se guarda una lista de adyacencias entre los nodos. Encontramos un ejemplo en las redes sociales: en \emph{Facebook} cada nodo se considera un usuario, que puede tener aristas de amistad con otros usuarios, o aristas de publicación con nodos de contenidos. Soluciones como \emph{Neo4J} y \emph{GraphDB28} son las más conocidas dentro de las bases de datos en grafo.
\item
  \textbf{Almacenamiento orientado a columnas}. El almacenamiento orientado a columnas es similar al documental. Su modelo de datos es definido como ``un mapa de datos multidimensional poco denso, distribuido y persistente'' \citep{Hecht2011}. Se orienta a almacenar datos con tendencia a escalar horizontalmente, por lo que permite guardar diferentes atributos y objetos bajo una misma clave. A diferencia del documental y el clave-valor, en este caso se pueden almacenar varios atributos y objetos, pero no serán interpretables directamente por el sistema. Permite agrupar columnas en familias y guardar la información cronológicamente, mejorando el rendimiento. Esta tecnología se acostumbra a usar en casos con 100 o más atributos por clave. Su precursor es BigTable de Google, pero han aparecido nuevas soluciones como \emph{HBase} o \emph{HyperTable}.
\end{itemize}

\hypertarget{limitaciones-de-las-bases-de-datos-nosql}{%
\subsection{Limitaciones de las bases de datos NoSQL}\label{limitaciones-de-las-bases-de-datos-nosql}}

A pesar de las ventajas de las bases de datos \textbf{\emph{NoSQL}}, estas tienen una serie de limitaciones, tanto técnicas como de carácter no tecnológico. En cuanto a limitaciones técnicas se encuentran: Cómo modelar correctamente los datos para maximizar las capacidades; nivel bajo de seguridad; no soporta transacciones; falta de madurez en \emph{Business Intelligence}; problemas de compatibilidad ya resueltos en los modelos relacionales, entre otros.

Además existen otras limitaciones de carácter no tecnológico, como puede ser: falta de expertos; resistencia al cambio; disponibilidad del vendedor; el código abierto puede implicar problema de soporte para las empresas.

\hypertarget{ejemplo-de-integraciuxf3n-de-una-base-de-datos-nosql-y-anuxe1lisis-de-datos-en-r}{%
\section{Ejemplo de integración de una base de datos NoSQL y análisis de datos en R}\label{ejemplo-de-integraciuxf3n-de-una-base-de-datos-nosql-y-anuxe1lisis-de-datos-en-r}}

En esta sección se verá como R puede ser utilizado para conectarnos a una base de datos \textbf{\emph{NoSQL}}. En particular \textbf{\emph{MongoDB}}. En la sección \ref{introMongo} se presenta una introducción a \emph{MongoDB}. En la sección \ref{paquetesCaso} se explican los paquetes de \textbf{R} utilizados para acceder a \emph{MongoDB}. La sección \ref{conexionMongo} explica como conectarse a una base de datos \textbf{\emph{MongoDB}} remota. Las secciones \ref{consultaViajes} y \ref{analisisViajes} realizan consultas y análisis sobre una colección de viajes realizados por los usuarios de un servicio de bicicletas compartidas con sede en la ciudad de \emph{Nueva York}.

\hypertarget{introMongo}{%
\subsection{Introducción a MongoDB}\label{introMongo}}

\textbf{\emph{MongoDB}} (de la palabra en inglés \emph{``humongous''} que significa \emph{enorme}) es un sistema de base de datos \textbf{\emph{NoSQL}} orientado a documentos, desarrollado bajo el concepto de código abierto. \textbf{\emph{MongoDB}} forma parte de la nueva familia de sistemas de base de datos NoSQL. En lugar de guardar los datos en tablas como se hace en las bases de datos relacionales, \textbf{\emph{MongoDB}} guarda estructuras de datos en documentos similares a JSON con un esquema dinámico (\textbf{\emph{MongoDB}} utiliza una especificación llamada BSON), haciendo que la integración de los datos en ciertas aplicaciones sea más fácil y rápida. \textbf{\emph{MongoDB}} soporta la búsqueda por campos, consultas de rangos y expresiones regulares. Las consultas pueden devolver un campo específico del documento, pero también puede ser una función JavaScript definida por el usuario. Cualquier campo en un documento de MongoDB puede ser indexado, al igual que es posible hacer índices secundarios. El concepto de índices en MongoDB es similar a los encontrados en base de datos relacionales. Tecnológicamente, \textbf{\emph{MongoDB}} es una base de datos multiplataforma y orientada a documentos que brinda alta rendimiento, alta disponibilidad y escalabilidad fácil.

MongoDB trabaja en concepto de colección y documento. La Tabla \ref{tab:tab2} muestra la relación de la terminología de bases de datos relacional respecto a MongoDB.

\begin{longtable}[]{@{}cc@{}}
\caption{\label{tab:tab2}. Diferencias entre terminología en bases de datos relacionales y MongoDB.}\tabularnewline
\toprule()
Bases de datos relacionales & MongoDB \\
\midrule()
\endfirsthead
\toprule()
Bases de datos relacionales & MongoDB \\
\midrule()
\endhead
Bases de datos & Bases de datos \\
Tabla & Colección \\
Tupla o Fila & Documento \\
Tabla Join & Documentos embebidos \\
Primary Key & Por defecto \emph{key\_id}, gestionada por MongoDB \\
\bottomrule()
\end{longtable}

Asimismo, \textbf{\emph{MongoDB}} proporciona una función \textbf{\emph{MapReduce}} que puede ser utilizada para el procesamiento por lotes de datos y operaciones de agregación. El marco de trabajo de agregación permite realizar operaciones similares a las que se obtienen con el comando SQL \emph{``GROUP BY''}. El marco de trabajo de agregación está construido como un pipeline en el que los datos van pasando a través de diferentes etapas en los cuales estos datos son modificados, agregados, filtrados y formateados hasta obtener el resultado deseado (véase ejemplo esquemático en Figura \ref{fig:FigNoSQL6}. Todo este procesado es capaz de utilizar índices si existieran y se produce en memoria.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{img/NoSQL6} 

}

\caption{Ejemplo esquemático del pipeline de agregación en MongoDB. Adaptado de [@Morgan2015]}\label{fig:FigNoSQL6}
\end{figure}

\hypertarget{paquetesCaso}{%
\subsection{Plataforma tecnológica para el caso práctico}\label{paquetesCaso}}

Para la realización del caso práctico se utilizará \textbf{\emph{Atlas}}, un servicio en la nube gratuito para manegar bases de datos \emph{MongoDB}. \textbf{\emph{MongoDB Atlas}} es fácil de configurar y tiene conjuntos de datos de muestra para ejemplos de \textbf{R} con \emph{MongoDB}. Puede cargar conjuntos de datos de muestra usando el ``\ldots{}'' junto al botón de colecciones en la página de su clúster. No obstante, aunque se puede crear un clúster específico en Atlas, en este ejemplo práctico se parte de uno ya creado.

\begin{itemize}
\item
  Adicionalmente, a modo de apoyo se recomienda utilizar un cliente \emph{MongoDB} para conectarse a la base de datos e inspeccionar los datos contenidos. Esto será extemadamente útil para realizar las consultas. Puede considerarse \href{https://robomongo.org/download}{Robo 3T}. Además, si se crea el propio cluster en \emph{Atlas}, este tiene una interfaz amigable para inspeccionar los datos.
\item
  Además de estas funciones, existe documentación de las colecciones y la información contenida en esta \href{https://docs.atlas.mongodb.com/sample-data/sample-training/}{base de datos de ejemplo}.
\item
  Para la resolución de ejercicios puede consultar el \href{https://docs.mongodb.com/manual/}{Manual de MongoDB} que contiene ejemplos y explicación de la sintáxis de \emph{MongoDB}
\end{itemize}

\hypertarget{paquetes-r-utilizados}{%
\subsubsection{Paquetes R utilizados}\label{paquetes-r-utilizados}}

El controlador R MongoDB preferido, \texttt{mongolite}, es rápido y tiene una sintaxis similar a la del shell MongoDB. \texttt{Mongolite} es la que se utilizará en los siguientes ejemplos. Los otros paquetes enumerados aquí no han estado tan activos en Github recientemente. Los paquetes más populares para conectar \emph{MongoDB} y \textbf{R} son:

\begin{itemize}
\item
  \texttt{mongolite}: un controlador R MongoDB más reciente, \emph{mongolite} puede realizar varias operaciones como indexación, canalizaciones de agregación, cifrado TLS y autenticación SASL, entre otras. Está basado en el paquete jsonlite para R y mongo-c-driver. Podemos instalar mongolite desde CRAN o desde RStudio. \texttt{RMongo}: RMongo fue el primer controlador R MongoDB con una sencilla interfaz \textbf{R} MongoDB. Tiene una sintaxis como la del shell MongoDB. RMongo ha quedado obsoleto a partir de ahora.
\item
  \texttt{rmongodb}: rmongodb tiene funciones para crear pipelines, manejar objetos BSON, etc. Su sintaxis es muy compleja en comparación con mongolite. Al igual que \texttt{RMongo}, rmongodb ha quedado obsoleto y no está disponible ni se mantiene en CRAN.
\end{itemize}

Para poder usar el paquete mongolite debemos cargarlo previamente con el comando siguiente, además de importar la librería previamente.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mongolite)}
\end{Highlighting}
\end{Shaded}

\hypertarget{conexionMongo}{%
\subsection{Conexión y acceso a MongoDB desde R}\label{conexionMongo}}

La variable \emph{cadena\_conexion} representa la cadena de conexión a \textbf{MongoDB} en \textbf{Atlas}. Se podría sustituir por otro servidor o clúster en Atlas si se desea, o un servidor local.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cadena\_conexion }\OtherTok{\textless{}{-}} \StringTok{"mongodb+srv://user01:user01@cluster0.mcblc3z.mongodb.net/test"}
\end{Highlighting}
\end{Shaded}

Se establecen en opciones seguridad la no validación de certificados SSL, para evitar que que exista un error de conexión a Atlas.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{opciones\_conexion }\OtherTok{\textless{}{-}} \FunctionTok{ssl\_options}\NormalTok{(}\AttributeTok{weak\_cert\_validation =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{infobox}
\textbf{NOTA}

En entornos reales de producción, está desaconsejado enviar esta comprobación por razones de seguridad.

\end{infobox}

Después de establecer la conexión a MongoDB se recupera la colección \emph{trips} usando la función \textbf{\emph{mongo()}} en código R para obtener la colección de viajes de la base de datos \emph{sample\_training}. Esta recopilación contiene datos de viajes realizados por los usuarios de un servicio de bicicletas compartidas con sede en la ciudad de \emph{Nueva York}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{viajes }\OtherTok{\textless{}{-}} \FunctionTok{mongo}\NormalTok{(}\AttributeTok{collection =} \StringTok{"trips"}\NormalTok{, }\AttributeTok{db =} \StringTok{"sample\_training"}\NormalTok{, }\AttributeTok{url =}\NormalTok{ cadena\_conexion, }\AttributeTok{options =}\NormalTok{ opciones\_conexion)}
\end{Highlighting}
\end{Shaded}

Se puede verificar que el código ahora esté conectado a la colección verificando el número total de documentos en esta base de datos. Para hacerlo, use la función \texttt{count()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{viajes}\SpecialCharTok{$}\FunctionTok{count}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] 10000}
\end{Highlighting}
\end{Shaded}

\hypertarget{consultaViajes}{%
\subsection{Obtención de datos en R desde MongoDB}\label{consultaViajes}}

Ahora que hay una conexión establecida con la base de datos, se pueden leer los datos de la misma para ser procesados por \textbf{R}. Para mostrar cómo recuperar datos de \emph{MongoDB} y mostrar los mismos se continua con la colección anterior \emph{trips\_collection}, mostrada en la sección anterior. Se puede usar la interfaz de usuario de \textbf{\emph{MongoDB Atlas}} para ver los documentos de \emph{trip\_collection} o directamnte para visualizarlos. Para ello se puede otener cualquier documento de muestra de la colección usando la función {[}\$iterate().\$one(){]} \href{https://jeroen.github.io/mongolite/query-data.html\#iterating}{\texttt{iterate()} y \texttt{one()}} para examinar la estructura de los datos de esta colección.

Una vez se conoce la estructura de los documentos, se pueden realizar consultas más avanzadas, como buscar los 3 viajes más largos a partir de los datos de recopilación de viajes, y luego enumerar la duración en orden descendente. La consulta propuesta utiliza operadores de \href{https://jeroen.github.io/mongolite/query-data.html\#sort-and-limit}{clasificación y límite} para producir este conjunto de resultados.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{viajes}\SpecialCharTok{$}\FunctionTok{find}\NormalTok{(}\AttributeTok{sort =} \StringTok{\textquotesingle{}\{"tripduration" : {-}1\}\textquotesingle{}}\NormalTok{, }\AttributeTok{limit =} \DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{}   tripduration start station id          start station name end station id}
\CommentTok{\#\textgreater{} 1       326222              391         Clark St \& Henry St            310}
\CommentTok{\#\textgreater{} 2       279620             3165 Central Park West \& W 72 St           3019}
\CommentTok{\#\textgreater{} 3       173357             3155     Lexington Ave \& E 63 St           3083}
\CommentTok{\#\textgreater{}           end station name bikeid   usertype birth year}
\CommentTok{\#\textgreater{} 1      State St \& Smith St  18591 Subscriber       1979}
\CommentTok{\#\textgreater{} 2        NYCBS Depot {-} DEL  17547   Customer           }
\CommentTok{\#\textgreater{} 3 Bushwick Ave \& Powers St  15881   Customer           }
\CommentTok{\#\textgreater{}   start station location.type start station location.coordinates}
\CommentTok{\#\textgreater{} 1                       Point                {-}73.99345, 40.69760}
\CommentTok{\#\textgreater{} 2                       Point                {-}73.97621, 40.77579}
\CommentTok{\#\textgreater{} 3                       Point                {-}73.96649, 40.76440}
\CommentTok{\#\textgreater{}   end station location.type end station location.coordinates}
\CommentTok{\#\textgreater{} 1                     Point              {-}73.98913, 40.68927}
\CommentTok{\#\textgreater{} 2                     Point              {-}73.98193, 40.71663}
\CommentTok{\#\textgreater{} 3                     Point              {-}73.94100, 40.71248}
\CommentTok{\#\textgreater{}            start time           stop time}
\CommentTok{\#\textgreater{} 1 2016{-}01{-}01 01:58:20 2016{-}01{-}04 20:35:23}
\CommentTok{\#\textgreater{} 2 2016{-}01{-}02 17:07:26 2016{-}01{-}05 22:47:46}
\CommentTok{\#\textgreater{} 3 2016{-}01{-}02 15:25:36 2016{-}01{-}04 15:34:53}
\end{Highlighting}
\end{Shaded}

\hypertarget{analisisViajes}{%
\subsection{Analizando datos de MongoDB en R}\label{analisisViajes}}

Para analizar datos de \emph{MongoDB} con \textbf{R} con más detalle, puede usar el \href{https://docs.mongodb.com/manual/aggregation/}{marco de agregación de MongoDB}. Este marco permite a los operadores crear canalizaciones de agregación que ayudan a obtener los datos exactos con una sola consulta.

Imagínese que se desea verificar cuántos suscriptores realizaron viajes de una duración \textgreater{} 240 segundos y regresaron a la misma estación donde comenzaron. La consulta usa MongoDB \href{https://docs.mongodb.com/manual/reference/operator/query/expr/}{\$expr} para comparar dos campos en el mismo documento.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{query }\OtherTok{\textless{}{-}}\NormalTok{ viajes}\SpecialCharTok{$}\FunctionTok{find}\NormalTok{(}\StringTok{\textquotesingle{}\{"usertype":"Subscriber","tripduration":\{"$gt":240\},"$expr": \{"$eq": ["$start station name","$end station name"]\}\}\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Combinando estos operadores con código \emph{R}, también se puede ver qué tipo de usuarios son más comunes: \emph{suscriptores} o \emph{clientes únicos}. Para ello, se puede \href{https://docs.mongodb.com/manual/reference/operator/aggregation/group/}{agrupar} usuarios por el campo tipo de usuario (\emph{usertype}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tipos\_usuario }\OtherTok{\textless{}{-}}\NormalTok{ viajes}\SpecialCharTok{$}\FunctionTok{aggregate}\NormalTok{(}\StringTok{\textquotesingle{}[\{"$group":\{"\_id":"$usertype", "Count": \{"$sum":1\}\}\}]\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Para comparar los resultados, se pueden visualizar los datos (ver \ref{fig:visualizar-suscriptores-agregados}.
Es conveniente convertir los datos obtenidos de \texttt{mongolite} en un \href{https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/data.frame}{marco de datos} y, por ejemplo, usar el paquete \texttt{ggplot2}, para trazar estos datos.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}

\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(tipos\_usuario)}

\FunctionTok{ggplot}\NormalTok{(df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{reorder}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{\_id}\StringTok{\textasciigrave{}}\NormalTok{, Count), }\AttributeTok{y =}\NormalTok{ Count)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{, }\AttributeTok{color =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"green"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ Count)) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Tipo de usuario"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/visualizar-suscriptores-agregados-1} 

}

\caption{Suscripción por tipo de usuario.}\label{fig:visualizar-suscriptores-agregados}
\end{figure}

\begin{infobox_resume}
\textbf{RESUMEN}

En este capítulo se ha presentado el concepto de Big Data, porque surje y que aporta respecto a soluciones previas. En particular se ha discutido que son las bases de datos NoSQL y cuales son sus diferencias con las bases de datos relacionales (más tradicionales). Posteriormente se ha explicado la importancia de los métodos de integración de datos, y como ejemplo se mostró la integración de datos en \textbf{R} desde \emph{MongoDB}, explicando cómo poder acceder a este tipo de datos y como analizarlos para el caso concreto de una base de datos documental como \emph{MongoDB}.

\end{infobox_resume}

\hypertarget{DGDQM}{%
\chapter{Gobierno y gestión de calidad de datos}\label{DGDQM}}

\emph{Ismael Caballero, Ricardo Pérez del Castillo y Fernando Gualo}

\hypertarget{introducciuxf3n-2}{%
\section{Introducción}\label{introducciuxf3n-2}}

Los datos se han convertido en un elemento vital para el desarrollo económico de las organizaciones, ya que permiten una mayor eficiencia en el uso de los recursos y un aumento de su productividad.
Tanto es así, que la Unión Europea establece a través de la \href{https://ec.europa.eu/info/strategy/priorities-2019-2024/europe-fit-digital-age/european-data-strategy_es}{Estrategia Europea de Datos} que en 2030, se establecerá un Espacio Único Europeo de Datos para fomentar un ecosistema con nuevos productos y servicios basados en los datos.
Para ello, en esta Estrategia Europea de Datos -que prevé un incremento del 530\% del volumen global de datos- se reclama la necesidad de implantar \textbf{mecanismos de gobierno de datos} a través de políticas y directrices consensuadas a alto nivel para alcanzar los objetivos de la \textbf{estrategia organizacional} y satisfacer aspectos regulatorios tanto genéricos (como las leyes europeas GPDR o Data Governance Act, o las españolas ENS o ENI) como aspectos sectoriales específicos (como Solvencia II para el sector seguros, o Basilea para el sector financiero).

Estos mecanismos de gobierno de datos deben abordar aspectos verticales relacionados con la adquisición, tenencia, compartición, uso y explotación de los datos en los procesos de negocio, abordando a la vez aspectos transversales relacionados con su gestión: calidad de los datos, aspectos éticos y privacidad, interoperabilidad, gestión del conocimiento y el control sobre los activos de datos a través de las políticas correspondientes, y despliegue de estructuras organizativas con una conveniente separación de los roles de gobierno de datos de los de gestión de datos \citep{ISO2017}.
Por tanto puede decirse que el \textbf{gobierno de datos} marca la dirección de cómo la organiación debe realizar la \textbf{gestión de datos} para alcanzar los objetivos establecidos en la(s) estrategia(s) de datos de la organización.
Esto se consigue mediante la definición de una serie de políticas de datos.

\hypertarget{concepto-de-gobierno-de-datos}{%
\section{Concepto de Gobierno de datos}\label{concepto-de-gobierno-de-datos}}

El gobierno de los datos se ha convertido en un habilitador de la economía de los datos \citep{Engels2019, Weber2009}, así como también en un pilar básico para la mejora de la transparencia y eficiencia de las administraciones públicas \citep{OECD2019, Osimo2020, OsorioSanabria2020}.
Aunque existen algunas aproximaciones académicas y profesionales al gobierno de datos, no hay una definición consensuada de este concepto que permita aunar las distintas visiones.
No obstante, la definición más aceptada de gobierno de datos es la propuesta por DAMA en DMBoK2 \citep{DAMA2017}: ``\emph{Es la colección de prácticas y procesos que ayudan a asegurar la gestión formal de los activos de datos dentro de una organización mediante el ejercicio de autoridad, control y toma de decisiones compartidas (planificadas, monitorizadas y forzadas}''.
Teniendo en cuenta los matices que introduce, es también interesante la lectura de la propuesta por \citet{soares2015chief}, que define \textbf{Gobierno de datos} como \emph{``la formulación de políticas para optimizar, securizar, y potenciar los datos como activos organizacionales mediante la alineación de los objetivos de diferentes funciones organizacionales; por su naturaleza, el gobierno de datos requiere cooperación interdepartamental para entregar oportuna y fielmente datos con el máximo valor para la toma de decisiones en la organización''}.

De alguna manera se podría entender que gobernar los datos implica \textbf{el diseño, implementación y mantenimiento de un sistema de gobierno de datos}.
Es posible afirmar que el gobierno de datos tiene tres características destacables \citep{Caballeroetal2022}:

\begin{itemize}
\tightlist
\item
  \textbf{Está dirigido por el valor de los datos}, pues el principal objetivo del gobierno de los datos es asegurar que los datos son tratados como activos de datos y que la gestión y uso que se hace de ellos permite alcanzar el valor organizacional que se espera de ellos. Por tanto, todas las acciones están encaminadas a la obtención de este valor organizacional.
\item
  \textbf{Está centrado en la arquitectura empresarial}, para poder gobernar los datos adecuadamente, es preciso revisar o incorporar ciertos componentes a la arquitectura empresarial tal que o bien den el soporte adecuado o bien forme parte del resultado del gobernar los datos.
\item
  \textbf{Es iterativo e incremental}, pues para alcanzar un estado en el que se considere que los activos de datos están perfectamente gobernados es preciso trazar una hoja de ruta que contemple a través de la ejecución de diferentes proyectos relacionados entre sí el desarrollo y la puesta en valor de los artefactos típicos de un sistema de gobierno de datos (véase sección \ref{artefactosDG}). Esto sólo se puede conseguir en incrementos relevantes (p.ej. la creación de más componentes del sistema de gobierno de datos o la inclusión de nuevos datos a ser gobernados en el alcance de gobierno de datos) para que la organización avance hacia la optimización del valor de los datos.
\end{itemize}

Un aspecto interesante es que a medida que se avanza en estas hojas de ruta, más sensible se vuelve la organización hacia la importancia de los datos, más aprende la organización a gestionar y gobernar los datos, y más amplio es el alcance del gobierno de datos: en definitiva se puede decir que más madura se vuelve la organización en lo que se refiere al gobierno y a la gestión de los datos.

\hypertarget{beneficiosDG}{%
\subsection{Beneficios del Gobierno de Datos}\label{beneficiosDG}}

Cuando se desarrolla un sistema de gobierno de datos, se espera conseguir con cada incremento uno o una combinación de los siguientes tipos de beneficios \citep{ISACA2019}:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  \textbf{Alineamiento estratégico}: Optimización del valor organizacional de los datos mediante el alineamiento con la estrategia organizacional;
\item
  \textbf{Realización de beneficios}: Aseguramiento de que los datos son entregados en condiciones aceptables a los diferentes consumidores de datos.
\item
  \textbf{Optimización de Riesgos}: Evitación o minimización dentro del apetito de riesgo de la organiación de los riesgos relacionados con la adquisición, uso y explotación de los datos, asegurando el cumplimiento de la normativa interna y regulatoria; y
\item
  \textbf{Optimización de Recursos}: Optimización de las capacidades de los recursos humanos y tecnológicos necesarios y utilizados para dar un soporte más eficiente a las distintas operaciones que involucran a los datos, minimizando el desperdicio de recursos al gestionar, usar y explotar los datos.
\end{enumerate}

Estos beneficios deben especificarse como parte de la estrategia de datos de la organización.
Así por ejemplo, una organización que considere realizar un alineamiento estratégico y una optimización de riesgos, estará desarrollando una \textbf{estrategia defensiva} que debería implementarse a través de un \textbf{gobierno técnico}; por otro lado, si una organización quiere por ejemplo maximizar la realización de beneficios, podría considerarse que estaría trazando una \textbf{estrategia ofensiva} que se podría materializar mediante un \textbf{gobierno para el valor}.

\hypertarget{artefactosDG}{%
\subsection{Artefactos de un sistema de Gobierno de Datos}\label{artefactosDG}}

Para poder obtener los beneficios descritos anteriormente, las organizaciones deben realizar esfuerzos para implantar los mecanismos de gobierno de datos reclamados en la Estrategia Europea de Datos particularizándolos a su realidad y en función de su madurez.
Estos mecanismos implican el desarrollo de un \textbf{sistema de gobierno de datos}, que involucra la creación o mantenimiento de forma interrelacionada y sujeto a las restricciones correspondientes de una serie de artefactos.
Dependiendo de si se tiene un gobierno técnico o un gobierno para el valor, la creación y uso de los artefactos será más o menos intensiva.
Estos artefactos son los siguientes \citep{Caballeroetal2022}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Procesos de Gestión de Datos, Gestión de Calidad de Datos y Gobierno de Datos}, que se refieren al diseño y posterior particularización e implantación de las buenas prácticas relacionadas con las tareas típicas de los datos a nivel de las correspondientes disciplinas. Es posible obtener descripciones genéricas de estos proceso en diferentes modelos de referencias de procesos, tales como \textbf{Data Maturity Model (DMM)}\citep{mecca_data_2014}, \textbf{The Data Management Capability Assessment Model (DCAM)}\citep{Council2020} o el \textbf{Modelo Alarcos de Mejora de Datos (MAMD)}\citep{MAMD2020}
\item
  \textbf{Estructuras organizacionales}, que deben recoger las cadenas de responsabilidades y rendición de cuentas, haciendo una adecuada separación entre las responsabilidades propias del gobierno de datos así como las responsabilidades propias de gestión de datos y las de gestión de calidad de datos. Los roles que deben asumir estas responsabilidades son típicamente el de \emph{Chief Data Officer} \citep{soares2015chief, Treder2020} con un punto de vista más ejecutivo/estratégico y los de \emph{data stewards} \citep{plotkin2020data} desde una perspectiva más táctica/operativa.
\item
  \textbf{Principios, políticas y marcos de referencia}, que deberían incluir todos los principios rectores en los que se basará el uso de los datos (tales como los \emph{Generaly Accepted Information Principles} listados en \citet{ladley2019}), las directrices o políticas y los controles correspondientes asociados necesarios para modelar y gestionar el valor de los datos, el riesgo a asumir y las restricciones a considerar según se describe en ISO/IEC 38505-2 \citep{ISO2018}.
\item
  \textbf{Datos}, que se refiere tanto a los datos que se deben gobernar como a las descripciones necesarias a través de los metadatos correspondientes. Para la parte de datos es fundamental poder establecer una adecuada \textbf{arquitectura de datos} con los correspondientes modelos que recojan la semántica del entorno de la organización y refleje el conocimiento de cómo ésta usa los datos para desarrollar su actividad organizacional y/o económica. Para dar soporte al uso correspondiente deben generarse y mantenerse los metadatos correspondientes, que pueden ser de varios tipos \citep{DAMA2017}: (1) \emph{metadatos de negocio} recogidos típicamente en glosario de negocio y que describen la relación del dato con el negocio; (2) \emph{metadatos técnicos} recogidos habitualmente en los \emph{catálogos de datos} que describen detalles técnicos de los datos; y (3) \emph{metadatos operacionales} recogidos típicamente en los diccionarios de datos que recogen aspectos relacionados con el procesamiento y acceso a los datos. Es importante que todos estos metadatos estén reconciliados convenientemente entre ellos ya que su visión conjunta permitirá conocer una descripción adecuada de los datos que se pretenden gobernar, y si está descripción es suficiente como para poder usar los datos con la suficiente garantías de éxitos.
\item
  \textbf{Cultura, ética y comportamiento}, cuyo objetivo es identificar aquellos aspectos culturales y éticos que deben regular la forma en la que la organización abordará las tareas relacionadas con los datos para que estos tengan el valor organizacional deseado \citep{harrison_data_2019}.
\item
  \textbf{Personas, habilidades y competencias}, que trata de organizar los roles que deben asumir las diferentes responsabilidades relacionadas con los diferentes procesos; también debe enfocarse en asegurar que esos roles tienen los conocimientos, habilidades y competencias necesarias para abordar las tareas asociadas mediante los programas formativos correspondientes; finalmente, este artefacto incluye asegurar que la organización tiene planes de contingencia ante la eventual rotación funcional de los recursos humanos dedicados a las responsabilidades relacionadas con los datos \citep{plotkin2020data}.
\item
  \textbf{Servicios, infraestructuras y aplicaciones} que aborda todo lo relacionado con las tecnologías y sistemas de información para dar soporte a las diferentes actividades de los procesos de gestión de datos, gestión de calidad de datos y gobierno de datos
\end{enumerate}

\hypertarget{marcos-y-metodologuxedas-existentes-de-gobierno-de-datos}{%
\section{Marcos y metodologías existentes de Gobierno de Datos}\label{marcos-y-metodologuxedas-existentes-de-gobierno-de-datos}}

En la literatura, tanto académica como profesional, existen algunas propuestas de creación de sistemas de gobierno de datos.
Es interesante resaltar que en el ámbito académico se han desarrollado algunas revisiones sistemáticas de literatura científica para identificar los componentes del gobierno de datos, aunque de forma general, y salvo algunas referencias, se mantienen desconectados de las propuestas profesionales.
También es importante mencionar que salvo COBIT 2019, la inmensa mayoría de estos marcos no identifican explícitamente el concepto de ``\textbf{sistema de gobierno de datos}'', sino que se establece bajo un paraguas más genérico de ``\textbf{gobierno de datos}''.
En cualquier caso, la idea es la misma.
En los siguientes párrafos se resumen los aspectos más importantes de los marcos más relevantes, que pueden servir para que el lector encuentre el que mejor se adapta a su circunstancia personal:

\begin{itemize}
\tightlist
\item
  \citet{Abraham2019} analizan la literatura para identificar los elementos de un marco de trabajo teórico que clasifican en torno a cuatro áreas del gobierno de datos: (1) alcance organizacional (aspectos intra e inter organizacionales), (2) alcance de los datos (datos tradicionales vs big data), (3) alcance del dominio (calidad de datos, seguridad de datos, arquitectura de datos, ciclo de vida, metadatos, almacenamiento e infraestructura de datos), y (4) Mecanismos de gobierno (estructurales, procedimentales y relacionales).
\item
  \citet{AlRuithe2019} identifican a través de una revisión sistemática de literatura identifican las áreas o retos del gobierno de datos donde merece la pena llevarse a cabo investigación. Éstas son tecnología (seguridad, privacidad, disponibilidad, rendimiento, clasificación de los datos, migración de datos), legalidad, y aspectos organizacionales o del negocio.
\item
  \citet{Brous2016} derivan, basándose en una revisión sistemática de la literatura, los principios para desarrollar de forma efectiva estrategias y aproximaciones para el gobierno de datos, que agrupan en torno a cuatro conceptos fundamentales: (1) organización, (2) alineamiento, (3) cumplimiento y (4) entendimiento común de los datos.
\item
  \citet{carruthers2020chief} identifican los posibles elementos que deben contemplarse en la transformación digital, la cual debe apoyarse en el gobierno de datos. Estos elementos (personas, datos, procesos, tecnologías) son representados mediante un triángulo, en el que los datos están en el centro de este. En la continuación de la obra de estos autores presentada en \citet{Jackson2019}, se propone un modelo de transformación, convenientemente soportado en el gobierno de datos.
\item
  DCAM \citep{Council2020} es un modelo de referencia para la evaluación de la capacidad de gestión de datos desarrollado por EDM Council. El modelo tiene ocho componentes agrupados en cuatro niveles (1) Fundamentos (Estrategia de Datos y Casos de negocio; Programas de Gestión de Datos y financiación), (2) Ejecución (Arquitectura de datos y de negocio; Arquitectura de datos y de tecnología; Gestión de Calidad de datos; Gobierno de Datos), (3) Colaboración (Entorno de Control de Datos) y (4) Formalización del diseño e implementación de las actividades analíticas.
\item
  DMBoK \citep{DAMA2017} es un marco de referencia de procesos desarrollado por DAMA que posiciona el gobierno de datos como la función que guía el resto de las acciones relacionadas con la gestión de datos. Identifica una serie de elementos que deben generarse a partir del gobierno de datos: estrategia de gobierno de datos; estrategia de datos; hoja de ruta del gobierno de datos; principios de gobierno de datos, políticas de gobierno de datos, procesos; marco operativo de gobierno de datos; hoja de ruta y guía de implementación; plan de operaciones; glosario de términos; plan de operaciones; cuadro de mandos de gobierno de datos; etc.
\item
  El Data Management Maturity Model (DMM) \citep{mecca_data_2014} es un modelo desarrollado por el SEI. Introduce un modelo de referencia para la evaluación de la madurez de las prácticas de gestión de los activos organizacionales de gestión de datos y sus correspondientes actividades a través del ciclo de vida de estos. Este modelo se puede usar de forma no prescriptiva para guiar a las organizaciones en la implantación de las mejores prácticas de gobierno y gestión de datos.
\item
  \citet{Eryurek2021} identifica los ``ingredientes'' propios de un sistema de gobierno de datos (Herramientas; personas y procesos; cultura de datos) así como las áreas en las que debería enfocarse el gobierno de datos a lo largo del ciclo de vida de los datos (descubrimiento y curación de datos; gestión de datos; políticas de privacidad, seguridad y acceso
\item
  COBIT 2019 \citep{ISACA2019} identifica para el sistema de gobierno de tecnologías y de información los siguientes elementos: procesos; estructuras organizacionales; principios, políticas y marcos de referencia; información; cultura, ética y comportamiento; personas, habilidades y competencias; servicios, infraestructuras y aplicaciones.
\item
  ISO 38505-1 \citep{ISO2017} e ISO 38505-2 \citep{ISO2018} muestran los aspectos claves del gobierno de datos (valor de los datos, riesgo, y restricciones) e introducen seis principios (responsabilidad, estrategia, adquisición, rendimiento, cumplimiento y comportamiento humano) junto con unas guías de desarrollo teniendo en cuenta una serie de factores que determinan su implementación. Identifica una serie de procesos (evaluar, dirigir, monitorizar) como áreas propias de actuación del gobierno de datos, distinguiéndolos y estableciendo las relaciones correspondientes con las operaciones propias de la gestión de datos. Sin embargo, no describe actividades propiamente dicho para la creación de sistemas de gobierno de datos.
\item
  \citet{Khatri2010} aduce que el gobierno de datos implica tomar decisiones sobre activos claves de datos en varios dominios de decisión (principios de datos, calidad de datos, metadatos, acceso a datos y ciclo de vida de los datos).
\item
  \citet{janssen2020data} explora las capacidades de gobierno de datos que las organizaciones dirigidas por datos necesitan para extraer el máximo beneficio de los sistemas algorítmicos basados en Big Data (\emph{Big Data Algorithmic Systems}) y propone un marco para el gobierno de datos para optimizar estos sistemas.
\item
  \citet{ladley2019} presenta un marco de gobierno de datos basado en cinco pilares: compromiso, estrategia, arquitectura y diseño, implementación y operación, y por último, cambio.
\item
  \citet{Lillie2019} estudian la literatura existente para identificar los aspectos más interesantes sobre (1) el alcance y los constructos más importantes del gobierno y gestión de datos, (2) las capacidades ágiles que son requeridas en el gobierno y la gestión de datos y (3) la necesidad de las organizaciones africanas, incluyendo las administraciones públicas, para establecer capacidades ágiles en las funciones de gobierno y gestión de datos.
\item
  En el \emph{Proceso Unificado de Gobierno de Datos} de IBM \citep{soares2010ibm} se identifican cinco ``\emph{ingredientes claves}'' en el gobierno de los datos que deberían ser cubiertos por cualquier marco: (1) Fuerte respaldo por parte de la organización con soporte de las TI, (2) Centrarse en los elementos de datos críticos, (3) Énfasis en los artefactos de datos, (4) Alineación en torno a métricas y aplicación de políticas, y (5) celebración de las victorias rápidas conseguidas como hitos en una hoja de ruta a largo plazo.
\item
  La OECD recoge en su informe sobre gobierno de datos para administraciones públicas \citep{OECD2019} las mejores prácticas que diferentes administraciones de distintos países han seguido para lograr la transparencia de las prácticas de gobierno y para incrementar el valor de los datos de la ciudadanía de cara a una mejor prestación de servicios públicos.
\item
  \citet{Treder2020} identifica algunos componentes específicos que debería tener un sistema de gobierno de datos (cadenas de valor; estrategia de dato; procesos de datos; descripción de los roles y sus responsabilidades; gestión del equipo de la oficina del dato), así como las áreas en las que debe enfocarse el gobierno de datos (casos de negocio; aspectos éticos y cumplimiento; gestión y análisis de datos).
\end{itemize}

\hypertarget{modelo-alarcos-de-mejora-de-datos-mamd}{%
\subsection{Modelo Alarcos de Mejora de Datos (MAMD)}\label{modelo-alarcos-de-mejora-de-datos-mamd}}

A modo de ejemplo, se van a introducir más detalles sobre un marco de referencia basado en estándares internacionales ISO: el \textbf{Modelo Alarcos de Mejora de Datos (MAMD) v3.0} \citep{MAMD2020}.
MAMDv3.0 es un marco de trabajo que se usa para la evaluación y mejora de la capacidad de los procesos de la organización relacionados con la gestión, la gestión de la calidad de los datos y el gobierno de datos.
Tiene dos componentes principales:

\begin{itemize}
\tightlist
\item
  El \textbf{modelo de referencia de procesos (MRP)}, que contiene una descripción de los procesos de gestión de datos, de gestión de calidad de datos y de gobierno de datos. Este MRP está alineado con los principales estándares en el área (ISO 8000-61\citep{ISO8000-61}, e ISO/IEC 38505-2 \citep{ISO2018}), así como con las buenas prácticas de otros modelos como DAMA, DMM o COBIT 2019. la Fig.\ref{fig:mamd-mrp} representa el modelo de referencia de procesos
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/mamd_mrp} 

}

\caption{Modelo de Referencia de Procesos de MAMD}\label{fig:mamd-mrp}
\end{figure}

\begin{itemize}
\tightlist
\item
  El \textbf{modelo de evaluación de procesos (MEP)}, sigue las directrices de evaluación y niveles de capacidad y madurez descritas por ISO/IEC 33000 y adaptadas a la evaluación de procesos de datos conforme al propuesto en ISO 8000-62 \citep{ISO8000-62} la Fig. \ref{fig:mamd-modelomadurez} representa el modelo de madurez incluidos.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/mamd_modelomadurez} 

}

\caption{Modelo de Madurez de MAMD}\label{fig:mamd-modelomadurez}
\end{figure}

\hypertarget{gestiuxf3n-de-calidad-de-datos}{%
\section{Gestión de calidad de datos}\label{gestiuxf3n-de-calidad-de-datos}}

Datos con niveles inadecuados de calidad, acaban teniendo un impacto negativo para las organizaciones, bien en términos económicos, bien en términos de reputación \citep{redman2016getting}.Por eso es importante que las organizaciones cuiden del nivel de calidad de sus datos, y se aseguren que dichos niveles permanecen dentro de los niveles permitidos para que la ejecución de los procesos de negocio se haga dentro del apetito de riesgo de la organización.

Se dice que un conjunto de datos tiene calidad cuando sirven para el propósito para el que fueron recogidos (\emph{fitness for use}) \citep{Strongetal1997}.
Para determinar si un conjunto de datos tiene calidad suficiente para dicho propósito, es preciso identificar y seleccionar un conjunto de criterios (llamados en la literatura \textbf{dimensiones} \citep{Wang1998}, o \textbf{características de calidad de datos} \citep{ISO25012}) que van a permitir representar los requisitos de calidad de datos que tienen los usuarios relevantes para el contexto de uso de dichos datos.
Al conjunto de dimensiones o características de calidad de datos seleccionadas se les llama \textbf{modelo de calidad de datos}.
Por tanto, puede decirse, que para determinar la calidad de los datos debe elegirse un modelo de calidad de datos.
En la tabla mostrada en la Fig.
\ref{fig:TablaCaracteristicasISO25012} se introduce una descripción de las características de calidad de datos incluidas en el estándar ISO/IEC 25012.
Como puede observarse estas características se clasifican en dos grandes bloques: \textbf{Inherentes} y \textbf{Dependientes del Sistema}.
Las \textbf{inherentes} se refieren al grado con el que las características de calidad de los datos tienen un potencial intrínseco para satisfacer las necesidades establecidas y necesarias cuando los datos son utilizados bajo condiciones específicas; las \textbf{dependientes del Sistema}, por otro lado, permiten determinar el grado con el que la calidad de datos es alcanzada y preservada a través de un sistema informático cuando los datos son utilizados bajo condiciones específicas.
Se puede ampliar más información sobre este aspectos en \citet{caballeroetal2017}.

Para ilustrar el significado de algunas de estas características (p.ej. \textbf{exactitud}, \textbf{completitud}, o \textbf{consistencia}), a continuación se introducen algunos ejemplos:

\begin{itemize}
\tightlist
\item
  Como ejemplo de niveles inadecuados de \textbf{exactitud sintáctica} -en este caso- podría ponerse el hecho de un atributo \texttt{Nombre} de la entidad \texttt{Persona} toma un valor ``\emph{Marja}'' (no existente en los datos de referencia de nombre) en lugar de ``\emph{María}'' (que sí que está incluido).
\item
  El hecho de que un atributo \texttt{Nombre} de la entidad \texttt{Persona} toma el valor de ``\emph{George}'' en vez de ``\emph{Jorge}'' para almacenar datos de la Persona llamada realmente ``\emph{Jorge}'' es un ejemplo de nivel inadecuado de \textbf{exactitud semántica}. Ambos valores son sintácticamente correctos, pero \texttt{George} es otra persona distinta a \texttt{Jorge}, y quien guardó los datos, simplemente se equivocó de persona.
\item
  Supóngase que para una determinada aplicación, se necesitan recoger valores para los siguientes atributos de una entidad Persona \texttt{DNI}, \texttt{Nombre}, \texttt{Apellido1}, y \texttt{Apellido2} para ser usada adecuadamente en un contexto de uso. En caso de faltar alguno de ellos (falta de completitud), podría ocurrir que los datos de la persona no fueran usables; inculso, podrían faltar algunos atributos más como por ejemplo \texttt{email}, pero si no son relevantes para la aplicación, daría igual no tenerlo.
\item
  Un ejemplo de falta de consistencia podría darse cuando el valor del atributo \texttt{FechaNacimiento} de la entidad Persona no es anterior o igual a la fecha de hoy.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/TablaCaracteristicasISO25012} 

}

\caption{Tipos de datos}\label{fig:TablaCaracteristicasISO25012}
\end{figure}

{[}\textbf{GEMA!!}: como no he conseguido disminuir el tamaño de la letra en la tabla con RMarkDown, he puesto una imagen con la tabla, creo que ahora queda más o menos bien.
Una de las ``consecuencias'' es que en el caption aparece la palabra ``figura'' y no ``tabla''{]}

Diferentes autores han proporcionado diferentes mecanismos para medir y evaluar la calidad de los datos usando las dimensiones o características de calidad seleccionadas.
Aunque para dar soporte a este proceso, se han propuesto numerosas metodologías de evaluación \citep{batini2016data}, el principal problema de estas contribuciones es que, típicamente, se han realizado \emph{ad hoc} y no permiten ni generalizar los resultados obtenidos ni compararlos con los obtenidos por otras organizaciones \citep{loshin2011master}.
Para paliar estos problemas, se han desarrollado estándares que recogen los conocimientos y principios básicos comunes para medir y evaluar la calidad de los datos.
Ejemplos de estos estándares pueden ser la mencionada ISO/IEC 25012 \citep{ISO25012}, o como ISO 8000-8 \citep{ISO8000-8} que recogen características de calidad, ISO/IEC 25024 \citep{ISO25024} que recoge aspectos específicos de cómo llevar a cabo las mediciones de las características, o ISO/IEC 25040 \citep{ISO25040} que proporciona una metodología de evaluación de calidad del software que puede ser adaptada a la evaluación rigurosa y sistemática de la calidad de los datos.

Es preciso en este punto introducir la principal diferencia entre \textbf{medir} y \textbf{evaluar} la calidad: medir consiste en determinar la cantidad de calidad de datos que tiene un conjunto de datos; mientras que evaluar implica determinar si, de acuerdo al apetito de riesgo de la organización, la cantidad de calidad de datos medida es suficiente y adecuada para usar los datos en el contexto de uso establecido por los datos.
En cualquier caso, la evaluación de calidad de datos requiere primero medir la calidad.
Y para medir la calidad, primero deben definirse procedimientos de medición que ayuden a conseguir información no sólo de la cantidad de calidad de datos que tiene un dataset, sino también de cómo esta cantidad de calidad de datos medida representa aspectos concretos de la calidad.
En ese sentido, ISO 25024 \citep{ISO25024} proporciona una serie de propiedades medibles para cada una de las características presentadas en la Tabla representada en la Fig.
\ref{fig:TablaCaracteristicasISO25012}; además para cada una de estas propiedades medibles, el estándar proporciona un método de medición genérico, que permitirá, convenientemente particularizado medir dichas propiedades y luego agruparlas para determinar el valor de la característica de calidad de datos.
En la Fig.\ref{fig:propiedadesISO25024} se muestran las propiedades medibles para las características de calidad identificadas como inherentes.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/propiedadesISO25024} 

}

\caption{Propiedades de las características inherentes de calidad de datos}\label{fig:propiedadesISO25024}
\end{figure}

Una de las ventajas de usar estas propiedades medibles es que es posible identificar mejor qué está causando que los datos no tengan los niveles de calidad adecuados y por tanto actuar directamente sobre ellas.

Así por ejemplo para medir el grado de exactitud de un conjunto de datos, habría que medir las propiedades ``\emph{Exactitud Sintáctica}'', ``\emph{Exactitud Semántica}'' y ``\emph{Rango de Exactitud}'', y luego hacer algún tipo de agrupación de los resultados que tuviera en cuenta la importancia o peso relativo de cada una de estas propiedades a la hora de evaluar la Exactitud.
Supóngase, por ejemplo que fuera significativo usar una media ponderada para realizar la agregación de las mediciones de las propiedades medibles, representando \(\alpha\), \(\beta\), \(\gamma\) coeficientes representando el peso relativo de cada una de las propiedades. Con esto se podría usar la fórmula mostrada en la ecuación \eqref{eq:exactitud} para el cálculo de la \textbf{exactitud}.

\begin{equation} \label{eq:exactitud}
Exactitud = \alpha \times ExactitudSemántica + \beta \times ExactitudSintática + \gamma \times RangoExactitud
\end{equation}

A la hora de medir las propiedades medibles correspondientes a las características de calidad de datos, es interesante tener en cuenta que ISO 25024 proporciona procedimientos de medición que, típicamente, están basados en la comprobación de si se violan las reglas de negocio que regulan la adecuación al uso de los datos en un contexto determinado \citep{loshin2002rule}.
Así, ISO 25024 proporciona métodos de medición de tipo ratio, que suelen corresponderse con la fórmula mostrada en la ecuación \eqref{eq:MedidaPM}):

\begin{equation} \label{eq:MedidaPM}
Medida \ PropiedadDQ  = \frac{Número \ Registro \ Violando \ Reglas \ de \ Negocio \ Específicas}{Número \ Total \ de \ Registros}
\end{equation}

A la hora de implementar esta fórmula como método de medición, uno de los ejercicios más difíciles es recolectar y validar las reglas de negocio específicas que rigen la validez de los datos.
Siguiendo con uno de los ejemplos anteriores, se va a medir la Exactitud Sintáctica usando la fórmula que propone ISO 25024 y que se representa en la ecuación \eqref{eq:MedidaExactitudSintactica}):

\begin{equation} \label{eq:MedidaExactitudSintactica}
ExactitudSintática = \frac{Número \ Registros \ que \ Son \ Semánticamente \ Correctos}{Número \ Registros \ Totales}
\end{equation}

Si se tuviera un ejemplo en el que se pretende medir el nivel de exactitud sintáctica del atributo \texttt{DNI} se podría usar la siguiente regla de negocio ``\emph{el DNI tiene que seguir la especificación para DNI o para el NIE, correspondiente con la expresión regular} \texttt{(d\{8\})({[}A-Z{]})}''.
Para realizar la validación de la reglas de negocio habría que escribir algún tipo de programa que permitiera validar cuántos registros cumplen la regla de negocio establecido; tras ralizar la comprobación de la regla, hay que contar el número de registros que no lo han cumplido y el número de registros totales para poder generar el valor correspondientes de la medida de la propiedad.
Como se verá en la siguiente subsección, en algunos entornos académicos y profesionales, alternativamente a la escritura de este programa, se utilizan técnicas de perfilado de datos para realizar mediciones.
En muchos casos, esto se consigue asumiendo de forma implícita un enunciado y validación de las reglas de negocio que rigen cuando los datos son adecuados para un contexto de uso.

Para ilustrar cómo se realiza la medición de una característica de calidad, supóngase que una organización, en base a su apetito de riesgo para un determinado proceso de negocio, considera que para una determinada aplicación, puede asignar los siguientes pesos a los coeficientes \(\alpha\) = 0,4, \(\beta\) = 0,4, y \(\gamma\) = 0,2 (de la fórmula de la ecuación \eqref{eq:exactitud}) y que además, ha obtenido como resultados de la medición de las propiedades medibles de la Exactitud, los siguientes valores: para la \texttt{ExactitudSemántica}, 80; para la \texttt{ExactitudSintática}, 60; y para el \texttt{RangoExactitud}, 70.
Entonces, el grado de exactitud del dataset sería 70.

Una vez que se han realizado estas mediciones, el siguiente paso es realizar la evaluación propiamente dicha.
Esto consisten en comparar los resultados obtenidos en las mediciones con unos valores umbrales mínimos de aceptación deseables especificados por los usuarios de los datos a través de los requisitos de calidad de datos; es importante resaltar que estos valores umbrales dependen fuertemente del apetito de riesgo de la organización a la hora de utilizar estos datos \citep{redman2016getting}.
Siguiendo el ejemplo anterior de la exactitud, suponga el lector que una organización considera que si no se alcanza un valor de al menos 75\% para el uso pretendido de los datos, entonces, éstos no deberían ser usados.
En el ejemplo, y como se ha obtenido un resultado de 70\% para la medición, entonces, como se requiere un valor de al menos 75\%, se puede concluir que \textbf{los datos no tienen el nivel adecuado de calidad para el uso pretendido, y por tanto no deben usarse}.
Esto no significa que los datos no puedan usarse en otro contexto en el que por ejemplo el valor umbral se estableciese en 65\% en función del apetito de riesgo de la organización.

En algunos contextos de uso, como se verá posteriormente en el capítulo \ref{datacleaning}, antes de poder usar los datos se realiza un proceso de preparación de datos que tiene como objetivo determinar y adecuar los niveles de calidad al uso que se pretende mediante un proceso de evaluación y mejora de los datos - típicamente llamado \textbf{limpieza de los datos}- .
En estos contextos, la aproximación presentada basada en características de calidad de datos y propiedades medibles no se suele usar, recurriéndose a una evaluación y limpieza más estadística.
Se pierde entonces de alguna manera la capacidad de establecer una dirección más efectiva y sobre todo alineada a las necesidades reales de la organización de las operaciones de evaluación y limpieza de datos.

Finalmente, es interesante mencionar que, basándose en el estándar ISO/IEC 25012 \citep{ISO25012} e ISO/IEC 25024 \citep{ISO25024} es posible certificar el nivel de calidad de datos de un repositorio de datos.
\citep{Gualoetal2021} recoge experiencias de medición, evaluación y certificación de calidad de datos.

\hypertarget{mediciuxf3n-de-calidad-de-datos-vs-perfilado-de-datos}{%
\subsection{Medición de calidad de datos vs perfilado de datos}\label{mediciuxf3n-de-calidad-de-datos-vs-perfilado-de-datos}}

En esta subsección se plantea el \textbf{perfilado de datos} como una técnica para realizar la medición de las propiedades medibles de las características de calidad de datos.
\citet{abedjan2015profiling} clasifica los tipos de \textbf{data profiling} en las siguientes categorías:

\begin{itemize}
\item
  \textbf{Perfilado de columna simple}, que implicaría tareas de identificación de cardinalidades, identificación de patrones y tipos de datos, distribución de valores de datos, clasificación de dominios.
\item
  \textbf{Perfilado de columnas múltiples}, que implicaría tareas de correlación y reglas de asociación, identificación de clusters y outliers, elaboración de resúmenes de datos y bocetos.
\item
  \textbf{Perfilado de dependencias}, que a su vez implica:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Detección de reglas de unicidad} tales como la identificación de claves, identificación de condiciones e identificación de sinónimos.
  \item
    \textbf{Detección de dependencias de inclusión}, que puede abarcar el descubrimiento de claves ajenas o la identificación de dependencias condicionales de inclusión.
  \item
    \textbf{Dependencias funcionales}, como pueden ser las dependencias condicionales.
  \end{itemize}
\end{itemize}

En \textbf{R Software} se puede utilizar el paquete \texttt{dlookr} creado por Choonghyun Ryu y descrito en \url{https://github.com/choonghyunryu/dlookr} o \href{https://choonghyunryu.github.io/dlookr/articles/introduce.html}{aquí} y que contiene algunas funciones interesantes que pueden ayudar a realizar determinadas tareas de perfilado.
Por ejemplo, la función \texttt{overview()} da información general sobre un conjunto de datos; resulta muy interesante la función \texttt{diagnose()} que proporciona información realizando un perfilado de los valores únicos y los valores únicos de un conjunto de valores.
En el siguiente fragmento se demuestra el tipo de información proporcionada por \texttt{diagnose(idealista18::Madrid\_POIS\$City\_Center)}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dlookr)}
\FunctionTok{diagnose}\NormalTok{(idealista18}\SpecialCharTok{::}\NormalTok{Madrid\_POIS}\SpecialCharTok{$}\NormalTok{City\_Center)}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 6}
\CommentTok{\#\textgreater{}   variables types   missing\_count missing\_percent unique\_count unique\_rate}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}     \textless{}chr\textgreater{}           \textless{}int\textgreater{}           \textless{}dbl\textgreater{}        \textless{}int\textgreater{}       \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Lon       numeric             0               0            1           1}
\CommentTok{\#\textgreater{} 2 Lat       numeric             0               0            1           1}
\end{Highlighting}
\end{Shaded}

Si estas funciones de perfilado proporcionan información suficiente y adecuada, se puede para computar las mediciones de las propiedades medibles.
Por ejemplo, se puede utilizar el resultado de la columna \texttt{missing\ count} para calcular el grado de completitud de las variables \texttt{longitud}y \texttt{latitud}, que se pueden establecer en 100\% al ser \texttt{missing\ count} para las dos variables.
Incluso se pueden utilizar funciones como \texttt{plot\_na\_pareto()} para visualizar un gráfico de Pareto mostrando las variables que no tienen valores nulos.

Finalmente es interesante mencionar que el paquete \texttt{dlook} incluye funciones como \texttt{diagnose\_paged\_report()} que permiten elaborar informes que contienen información sobre las estructuras de datos del paquete, avisos, descripción de las variables, valores perdidos, valores únicos de las variables categóricas y numéricas, distribuciones de valores ceros y negativos, posibles outliers, \ldots{} El siguiente chunk explica cómo crear un informe de 15 páginas en formato PDF con toda esa información sobre la variable \texttt{idealista18::Madrid\_POIS\$Metro} (\textbf{OJO LO DEJO COMENTADO PORQUE ME DABA FALLO AL COMPILAR}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# diagnose\_paged\_report(idealista18::Madrid\_POIS$Metro)}
\end{Highlighting}
\end{Shaded}

En ocasiones, y retomando la idea de las reglas de negocio, se puede utilizar este tipo de informes para derivar algunas reglas de negocio sobre los datos que se pueden usar en la medición de las propiedades medibles seleccionadas de las características de calidad de datos que mejor representan los requisitos de calidad de los datos de los usuarios.
Teniendo en cuenta que los valores almacenados en una base de datos provienen del esfuerzo realizado durante un tiempo indeterminado, las reglas de negocio encontradas deberían responder a los esfuerzos continuados a lo largo del tiempo por la organización para mantenerse actualizado.
Pero en ocasiones ha podido ocurrir que estos mantenimientos no se hayan realizado de la forma más rigurosa ni trazable posible, con lo que sería altamente recomendable validar con el personal del negocio si las reglas halladas o inferidas a partir de estos informes realmente se corresponden con lo que el negocio necesita.
A modo resumen puede decirse que la información proporcionada por el perfilado puede usarse en primera instancia para derivar reglas de negocio que no se conocen a partir del estado actual de los datos; y, en segunda instancia para recoger información que se puede emplear durante el proceso de medición de determinadas características de calidad de datos.
En el capítulo \ref{datacleaning} se profundizará en el proceso de estudio de dos caracteríticas de calidad de datos: completitud y consistencia.

\hypertarget{mejora-de-datos}{%
\subsection{Mejora de datos}\label{mejora-de-datos}}

Si los datos no tienen el nivel de calidad que se necesita de ellos para una tarea, es preciso mejorar su calidad para que no se arruinen los proceso de negocio.
Para ello, a partir de los resultados de las mediciones, los analistas de calidad de datos deben determinar las cuasas raíces de esos niveles inadecuados de calidad de datos.
\citet{Strongetal1997_Potholes} identifican diez posibles obstáculos que pueden hacer que los datos podrían no tengan esos niveles adecuados de calidad:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Múltiples fuentes de datos producen diferentes valores para el mismo atributo de la misma entidad.
\item
  La realización de juicios subjetivos en la producción de los datos, puede llevar a valores diferentes.
\item
  Errores Sistemáticos en la producción de Información llevan a la pérdida de información.
\item
  Grandes volúmenes de información almacenada dificultan su acceso en tiempo razonable.
\item
  Sistemas heterogéneos distribuidos llevan a definiciones inconsistentes, formatos y valores.
\item
  La información no numérica es difícil de indexar.
\item
  Análisis automatizado de los contenidos en colecciones de información pueden no producir resultados adecuados. 8.A medida que las necesidades de los usuarios para la realización de tareas en entornos organizacionales cambian, la información que es relevante y útil cambia
\item
  Un acceso fácil a la información puede entrar en conflicto con los requisitos de seguridad, confidencialidad y privacidad.
\item
  La falta de recursos de computación limita el acceso a los datos en circunstancias favorables.
\end{enumerate}

En función de la naturaleza del problema detectado, las acciones correctivas pueden ser de distinta naturaleza:

\begin{itemize}
\item
  \textbf{Corrección de causas sistemáticas}.
  Si se observa que los problemas se suceden de forma sistemática y repetida, entonces las acciones de mejora de datos deben estar orientada a eliminar esas causas sistemáticas, como las descritas anteriormente en \citet{Strongetal1997_Potholes}.
  Por ejemplo: si los errores de calidad de datos se deben a que un proceso de negocio que está mal diseñado, entonces hay que rediseñarlo; si las causas se deben a que hay personas desempeñando ciertos roles que no tienen los conocimientos o habilidades adecuadas, entonces, hay que darle la formación adecuada; o si se deben a que hay software (por ejemplo, procesos ETL) que fallan, entonces hay que realizar el mantenimiento correctivo correspondientes.
\item
  \textbf{Corrección de errores debidos a causas aleatorias}.
  Si no es posible identificar cuáles son las causas raíces, porque son completamente desconocidas o aleatorias, entonces, no queda más remedio que actuar sobre los valores de los datos, cambiándolos para asegurarse que se cumplen las reglas de negocio que están establecidas.
  A este proceso se le suele llamar \emph{data cleansing}.
  \citet{ilyas2019data} identifican diversas técnicas de limpieza de datos (que pueden incluir operaciones de \textbf{imputaciones de datos} - véase la sección \ref{imputacion} del capítulo \ref{datacleaning}, de normalización, o de estandarización): esto implica realizar limpieza basadas en reglas de negocio, deduplicación de datos, transformación de datos, o limpieza guiadas por machine learning.
  En este caso, sería posible utilizar algunas funciones del paquete \texttt{dlookr} relacionadas con la transformación de los datos tales como \texttt{imputate\_na()} que genera valores estadísticamente significativos para el caso de los valores faltantes o \texttt{imputate\_outlier()} que genera valores estadísticamente significativos para el caso de outliers (valores con niveles adecuados de precisión o de consistencia).
\end{itemize}

Es importante resaltar que en caso de que los errores se deban a causas sistemáticas, suele ser preciso incluir también acciones de limpieza de datos para poder devolver el estado de la base de datos a un estado coherente.

Tanto para el caso de la evaluación, como para la limpieza, siempre resulta conveniente (si no necesario en la mayoría de los casos), tener recopiladas y validadas un conjunto de reglas de negocio que se puedan usar en la evaluación y medición de la calidad de los datos \citep{ilyas2019data}.
En este sentido, en \citep{Caballeroetal2022br4dq} se proporciona una metodología para la identificación de las reglas de negocio y de cómo deben clasificarse para medir y evaluar determinadas dimensiones de calidad de datos.

\begin{infobox_resume}

\textbf{RESUMEN}

En este capítulo se ha presentado los fundamentos del gobierno de datos.
Es importante tener en cuenta los siguientes aspectos:

\begin{itemize}
\tightlist
\item
  El gobierno de datos tiene como objetivo asegurar que los datos que se usan y gestionan en las organiaciones están alineadas a las estrategias de datos de la organización, maximizando así el valor organizacional de los datos.
\item
  Gobernar los datos implica el diseño, implementación y mantenimiento de un sistema de gobierno de datos
\item
  Un sistema de gobierno de datos tiene siete tipos de componentes: procesos de gestión de datos, gestión de calidad de datos y gobierno de datos; estructuras organizacionales; principios, políticas y marcos de referencia; datos y descripción de los datos; cultura, ética y comportamiento; personas, habilidades y competencias; servicios, infraestructuras y aplicaciones.
\item
  Existen modelos de referencias que pueden ser usados como base para la creación de sistemas de gobierno de datos.
\item
  El gobierno de datos persigue cuatro beneficios básicos para la organización: alineamiento estratégico, realización de beneficios, optimización de riesgos, optimización de recursos.
\item
  La gestión de la calidad de datos es el proceso mediante el cual se garantiza que los datos tengan el nivel de calidad adecuado para las tareas para las que fueron pensados.
\item
  Para evaluar y medir la calidad se necesitan criterios que ayuden a medir la calidad de los datos; estos criterios se llaman características o dimensiones de calidad de datos.
\item
  La evaluación y medición de calidad de datos requiere la identificación y clasificación de las reglas de negocio que rigen la validez de los datos
\item
  Las técnicas y herramientas de perfilado de datos se pueden utilizar como base para la realización de las mediciones de calidad de datos; y en ocasiones para la identificación de reglas de negocio a partir de los datos.
\item
  Cuando los datos no tienen calidad, a partir de las mediciones y evaluaciones realizadas, deben investigarse cuáles son las posibles causas. Si las causas son sistemáticas, entonces hay que enfocar el problema desde un punto de vista organizacional; si las causas son aleatorias, se pueden usar las técnicas de limpieza de datos vistas.
\end{itemize}

\end{infobox_resume}

\hypertarget{130009}{%
\chapter{Integración y limpieza de datos}\label{130009}}

\emph{Jorge Velasco López}

\hypertarget{introducciuxf3n-3}{%
\section{Introducción}\label{introducciuxf3n-3}}

En proyectos de modelado predictivo, como la clasificación o la regresión, generalmente es necesario realizar un preprocesamiento previo de los datos. Esto se debe a razones, \citep{brownlee2020data}, tales como que (i) algunos algoritmos de aprendizaje automático requieren determinados requisitos de datos (por ejemplo, que sean numéricos); (ii) es necesario corregir el ruido estadístico y los errores en los datos; (iii) no pueden extraerse de los datos directamente las relaciones no lineales complejas.

En consecuencia, para asegurar niveles adecuados de calidad y permitir descubrir la estructura subyacente del caso de uso a los algoritmos de aprendizaje, se deben procesar los datos en bruto con anterioridad a poder usarse para probar y evaluar un modelo de \emph{machine learning}. Este paso, en un proyecto de modelado predictivo, se conoce como \emph{preparación de datos}, o bien \emph{preprocesamiento de datos}.
\index{datos!preprocesamiento}

Cada proceso de preparación es muy específico para cada conjunto de datos, para los objetivos del proyecto y para los algoritmos que se utilizarán para modelar los datos. Sin embargo, hay una serie tareas comunes que suelen requerirse en esta fase de preparación de datos, como la \textbf{integración de los datos} (combinación de datos de distintas fuentes) y la \textbf{limpieza de los datos} (identificar y corregir posibles errores en los datos como las causas de niveles inadecuados de calidad).

Otras tareas que típicamente se incluyen en el proceso de preparación de datos son: 1) la selección de variables (\emph{feature selection}); 2) transformaciones de datos para cambiar la escala o distribución de variables; 3) transformación de variables (\emph{feature engineering}); 4) reducción de dimensionalidad (creación de proyecciones compactas de los datos) y 5) otro tipo de tratamientos.

Este capítulo se centra en la parte de limpieza de datos y en la integración de datos. Los datos recogidos en su origen se denominan \textbf{datos sin procesar} o \textbf{brutos} y se recopilan en el contexto de un problema que desea resolver. Si se requiere recuperar los datos de diversas fuentes para obtener vistas unificadas se habla de integración de datos. Aunque los datos están típicamente representados en formato matricial, las variables pueden ser de distinto tipo de datos (numéricas, rangos, porcentajes, categóricas, binarias, etc.). Además, debe tenerse en cuenta que muchos algoritmos de aprendizaje automático fundamentalmente solo operan con datos numéricos.

La colección de paquetes \texttt{Tidyverse} \citep{wickham2017package} dispone de una serie de herramientas destinadas a facilitar la manipulación, importación, exploración y visualización de datos y se utiliza exhaustivamente en ciencia de datos. Está compuesto de los siguientes paquetes: \texttt{readr,\ dplyr,\ ggplot2,\ tibble,\ tidyr,\ purr,\ stringr} y \texttt{forcats}, que ya se han introducido en buena medida en anteriores capítulos. La mayoría de las funciones del paquete \texttt{tidyverse} crean objetos \emph{tibble}, que son compatibles con los \texttt{data.frame} por ser objetos muy similares (tabulares, organizados en filas y columnas) aunque hay alguna pequeña diferencia.

El paquete \texttt{caret} \citep{kuhn2008building}, por su parte, proporciona una interfaz unificada que simplifica el proceso de modelado. Para ello, emplea la mayoría de los métodos de aprendizaje estadístico implementados en \textbf{R Software}, además de numerosas funciones para realizar varias tareas del proceso de modelado: \emph{feature selection}, \emph{data splitting}, validación del modelo, etc.
En relación al preprocesamiento de los datos, incluye la función \texttt{preProcess(x,\ method)}, que se puede integrar en el entrenamiento (función \texttt{train()}) para estimar los parámetros de las transformaciones a partir de la muestra de entrenamiento y posteriormente aplicarlas automáticamente al hacer nuevas predicciones. El parámetro \texttt{method} permite establecer una lista de procesamientos, para imputación, creación y transformación de variables explicativas, selección de predictores y extracción de componentes. El paquete \texttt{caret} también incluye una serie de funciones auxiliares, como \texttt{rfe()} y \texttt{dummyVars()}.

Las principales utilidades de \texttt{tidyverse} en relación con la manipulación de ficheros y variables se han ilustrado en el capítulo \ref{ch-110003}. En este capítulo, se revisarán los problemas de calidad de las variables, se observará cómo manejar valores atípicos en variables cuantitativas y el tratamiento de valores \emph{missing}, para lo que se usará el paquete de \texttt{caret}. Además, se dará una introducción al manejo del ruido estadístico con el paquete de \textbf{R} \texttt{stat}.
Como fuente de datos, se utilizará el set de datos de \texttt{Gapminder}, que contiene información demográfica de todos los países desde 1952, como la cantidad de población (variable \texttt{pop}), expectativa de vida (\texttt{lifeExp}) e ingresos per cápita (\texttt{gdpPercap}), asi como el país (\texttt{country}), continente (\texttt{continent}) y año de referencia (\texttt{year}).
Por último, se dará una visión de los procesos de integración de datos y de la correcta asignación del tipo de dato, para lo que se usará el set de datos de la empresa de comercio electrónico ``Beauty eSheep''. Tanto el set de datos de \texttt{Gapminder}como éste, están incluidos en el paquete \texttt{CDR}.

\hypertarget{problemas-de-calidad-de-datos}{%
\section{Problemas de calidad de datos}\label{problemas-de-calidad-de-datos}}

\index{datos!problemas de calidad}

Es habitual que en el tratamiento de algunas de las variables numéricas, se presenten problemas de calidad de datos (véase capítulo \ref{DGDQM}).
Para ilustrarlo en el set de datos \texttt{Gapminder}, podríamos sospechar que la variable numérica \texttt{gdpPercapita}, estuviera en una medida (moneda) distinta para determinados continentes en determinadas condiciones. Por ejemplo, para los países que tienen un valor de \texttt{lifeExp} menor de 60. Para verificar esa hipótesis, se podría hacer uso de funciones ad-hoc o manipulaciones en los datos que permitan obtener un \emph{subset} de los datos de interés bajo determinadas condiciones, o bien crear variables derivadas que faciliten esta tarea. En este ejemplo, se podría agrupar por continente y año y se podría presentar en orden descendiente un resumen de los principales descriptivos \footnote{Como se puede comprobar, una simple combinación de \texttt{mutate()} y la condición lógica \texttt{ifelse} facilita el filtrado solo allí donde se necesita, en este caso en el momento de crear la variable derivada de \texttt{pib\_filtro}.
  Si se confirmara la hipótesis, deberían realizarse las oportunas correcciones para las filas afectadas. En nuestro ejemplo se observa que para el continente africano en la década de los 50 presentan valores bajos de PIB promedio, consistente con la evidencia y por tanto no serían necesarias correcciones posteriores.}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder\_cons }\OtherTok{\textless{}{-}}\NormalTok{ gapminder }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{pib\_filtro =} \FunctionTok{ifelse}\NormalTok{(lifeExp }\SpecialCharTok{\textless{}} \DecValTok{60}\NormalTok{, gdpPercap }\SpecialCharTok{*}\NormalTok{ pop }\SpecialCharTok{/} \DecValTok{10}\SpecialCharTok{\^{}}\DecValTok{9}\NormalTok{, }\ConstantTok{NA}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(continent, year) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarize}\NormalTok{(}
    \AttributeTok{med\_gdpPercap =} \FunctionTok{mean}\NormalTok{(gdpPercap),}
    \AttributeTok{dt\_gdpPercap =} \FunctionTok{sd}\NormalTok{(gdpPercap),}
    \AttributeTok{media\_pob =} \FunctionTok{mean}\NormalTok{(pop),}
    \AttributeTok{media\_pib\_filtro =} \FunctionTok{mean}\NormalTok{(pib\_filtro),}
    \AttributeTok{dt\_pib\_filtro =} \FunctionTok{sd}\NormalTok{(pib\_filtro)}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(media\_pib\_filtro))}
\CommentTok{\# Para obtener un tibble desagrupado habría que añadir la función ungroup()}
\FunctionTok{head}\NormalTok{(gapminder\_cons, }\AttributeTok{n =} \DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 7}
\CommentTok{\#\textgreater{} \# Groups:   continent [1]}
\CommentTok{\#\textgreater{}   continent  year med\_gdpPercap dt\_gdpPercap media\_pob media\_pib\_filtro dt\_pib\textasciitilde{}1}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}     \textless{}int\textgreater{}         \textless{}dbl\textgreater{}        \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}            \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Africa     1957         1385.        1135.  5093033.             7.36     14.5}
\CommentTok{\#\textgreater{} 2 Africa     1952         1253.         983.  4570010.             5.99     11.4}
\CommentTok{\#\textgreater{} \# ... with abbreviated variable name 1: dt\_pib\_filtro}
\end{Highlighting}
\end{Shaded}

Para el caso de variables cualitativas, siguiendo el ejemplo con el mismo set de datos, podría ocurrir que hubiera algún valor de la variable \texttt{continent} con espacios en blanco y con problemas de mayúsculas y minúsculas. A continuación, se presenta una posible actuación para este ejemplo.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{continent }\OtherTok{\textless{}{-}} \FunctionTok{as.character}\NormalTok{(gapminder}\SpecialCharTok{$}\NormalTok{continent) }\CommentTok{\# Casteo de la variable a tipo carácter}
\NormalTok{continent }\OtherTok{\textless{}{-}} \FunctionTok{str\_trim}\NormalTok{(continent) }\CommentTok{\# Eliminación de espacios en blanco}
\NormalTok{gapminder}\SpecialCharTok{$}\NormalTok{continent }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{str\_to\_upper}\NormalTok{(continent)) }\CommentTok{\# Convertir a mayúsculas}
\FunctionTok{levels}\NormalTok{(gapminder}\SpecialCharTok{$}\NormalTok{continent) }\CommentTok{\# Previsualizacion de datos corregidos}
\end{Highlighting}
\end{Shaded}

La función \texttt{mutate()} también puede servir para cambiar el tipo de dato, como en el siguiente ejemplo, donde se utiliza conjuntamente con la función \texttt{across()} para abarcar todas las columnas tipo \texttt{factor} del \texttt{data.frame}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Convertir todos los factores en tipo carácter}
\NormalTok{Gapminder\_conv }\OtherTok{\textless{}{-}}\NormalTok{ gapminder }\SpecialCharTok{|\textgreater{}} \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.factor), as.character))}
\end{Highlighting}
\end{Shaded}

Además del uso de las funciones de transformación de ficheros y variables como las presentadas en \texttt{tidyverse}, la \emph{visualización} también puede ayudar en la identificación de problemas de calidad. En la Fig. \ref{fig:boxplot}, se usa la función \texttt{facet\_wrap()} para hacer diagramas de caja agrupados con las variables \texttt{x=continent} y \texttt{y=gdpPercapita}. A continuación, se muestran los puntos con \texttt{geom\_jitter()} y se agrega \texttt{facet\_wrap} como capa con el \texttt{continent} como argumento.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(continent }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"Americas"}\NormalTok{, }\StringTok{"Europe"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(year }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{1952}\NormalTok{, }\DecValTok{1982}\NormalTok{, }\DecValTok{2002}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{factor}\NormalTok{(year), }\AttributeTok{y =}\NormalTok{ gdpPercap, }\AttributeTok{fill =}\NormalTok{ continent)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Año"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{continent, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{45}\NormalTok{, }\AttributeTok{hjust =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/boxplot} 

}

\caption{Visualización boxplot}\label{fig:boxplot}
\end{figure}

\index{outlier}
Por su parte, un \textbf{\emph{outlier}} es un registro que no cumple una serie de reglas de negocio especificadas para ese atributo, es decir, aquel que tiene niveles inadecaudos de exactitud o de consistencia (véase capitulo @(DGDQM)). Una buena forma de identificar los valores atípicos (\emph{outliers}) en variables cuantitativas, es a través de un \textbf{boxplot} como el indicado en la Fig. \ref{fig:boxplot}, ya que en éste se puede identificar como están representados los datos con respecto a sus métricas descriptivas (media, mediana, cuartiles). De alguna forma, se está realizando un perfilado de datos para descubrir reglas de negocio, que son descritas en este caso, bajo un fundamento estadístico. No obstante, como se comentaba en el capitulo @(DGDQM), dichas reglas deberían ser validadas con la gente de negocio para poder tener la certeza de que realmente responden a lo que necesita el negocio.
Por ejemplo, se podría considerar que podrían ser \emph{outliers} algunas observaciones de baja \texttt{pop} y alto \texttt{gdpPercapita}. El análisis exploratorio mostraría que son valores atípicos, pero que no son incorrectos (es decir, tienen niveles adecuados de calidad):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{atipicos }\OtherTok{\textless{}{-}}\NormalTok{ gapminder }\SpecialCharTok{|\textgreater{}} \FunctionTok{filter}\NormalTok{(gapminder}\SpecialCharTok{$}\NormalTok{gdpPercap }\SpecialCharTok{\textgreater{}} \DecValTok{40000}\NormalTok{, gapminder}\SpecialCharTok{$}\NormalTok{pop }\SpecialCharTok{\textless{}} \DecValTok{1500000}\NormalTok{)}
\FunctionTok{head}\NormalTok{(atipicos, }\AttributeTok{n =} \DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 6}
\CommentTok{\#\textgreater{}   country continent  year lifeExp    pop gdpPercap}
\CommentTok{\#\textgreater{}   \textless{}fct\textgreater{}   \textless{}fct\textgreater{}     \textless{}int\textgreater{}   \textless{}dbl\textgreater{}  \textless{}int\textgreater{}     \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 Kuwait  Asia       1952    55.6 160000   108382.}
\CommentTok{\#\textgreater{} 2 Kuwait  Asia       1957    58.0 212846   113523.}
\end{Highlighting}
\end{Shaded}

\hypertarget{niveles-inadecuados-de-completitud-valores-missing}{%
\section{\texorpdfstring{Niveles inadecuados de completitud: Valores \emph{missing}}{Niveles inadecuados de completitud: Valores missing}}\label{niveles-inadecuados-de-completitud-valores-missing}}

\hypertarget{visualizaciuxf3n}{%
\subsection{Visualización}\label{visualizaciuxf3n}}

\index{datos!visualización}
\index{missing!informativos}
\index{missing!aleatorios}
\index{missing}

Los datos pueden faltar por multitud de posibles razones. Sin embargo, estas razones generalmente se agrupan en dos categorías: \textbf{valores \emph{missing} informativos} \citep{kuhn2013applied} y \textbf{valores \emph{missing} aleatorios} \citep{little2019statistical}. Los informativos implican una causa estructural, ya sea por deficiencias en la forma en que se recopilaron los datos o por anomalías en el entorno de observación. Los aleatorios implican que los valores faltantes ocurren independientemente del proceso de recopilación de datos. La categoría que causa los valores nulos determinará cómo se deben manejar. Por ejemplo, se puede dar a los informativos su propia categoría (por ejemplo, ``Ninguno''), ya que su valor único puede afectar el rendimiento predictivo. Mientras que los aleatorios pueden implicar la eliminación o la imputación.
Además, los diferentes algoritmos de aprendizaje automático manejan la falta de información de manera diferente. De hecho, la mayoría de los algoritmos no incorporan mecanismos para manejarlos (por ejemplo, modelos lineales generalizados y derivados, redes neuronales y SVM) y, por lo tanto, requieren que se traten previamente. Solo unos pocos modelos (principalmente basados en árboles) tienen procedimientos incorporados para tratar los valores nulos.

En \textbf{R}, los valores nulos se representan con el símbolo NA (\emph{Not Available}, ``no disponible'') y los valores indefinidos (p.~ej., dividir por cero) se representan con el símbolo NaN (\emph{Not a Number},``no es un número''). La función \texttt{vis\_miss()} en el paquete de \textbf{R} \texttt{visdat} \citep{tierney2017visdat} también permite una fácil visualización de patrones de datos nulos, con opciones de clasificación y agrupación. Los valores nulos se indican mediante una celda negra. Las variables y los patrones NA se han agrupado por filas (es decir, clúster = TRUE) en la Fig. \ref{fig:missing}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# gapminder\_cons se le ha imputado algunos valores missing}
\FunctionTok{vis\_miss}\NormalTok{((gapminder\_cons), }\AttributeTok{cluster =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

También el paquete \texttt{dlookr} permite realizar informes de distribución de valores nulos.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/missing} 

}

\caption{Visualización de valores missing}\label{fig:missing}
\end{figure}

\hypertarget{imputacion}{%
\subsection{Técnicas de Imputación}\label{imputacion}}

\index{imputación}

La gestión de los valores \emph{missing} debe hacerse considerando la problemática que se quiera resolver. Una primera opción a considerar sería excluir los valores \textbf{missing}, si bien se estaría eliminando información. Si se quisiseran filtrar los registros nulos para alguna columna se podría utilizar la función \texttt{is.na()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(pop))}
\CommentTok{\# En el ejemplo, se filtran los casos que no son NA de la variable \textasciigrave{}pop\textasciigrave{}.}
\CommentTok{\# También se puede usar la función \textasciigrave{}drop\_na()\textasciigrave{} del paquete \textasciigrave{}tidyr\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

También se puede optar por cambiar el valor del NA, por ejemplo por un 0, de la siguiente manera:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder\_cons[}\FunctionTok{is.na}\NormalTok{(gapminder\_cons)] }\OtherTok{\textless{}{-}} \DecValTok{0}
\CommentTok{\# También puede usarse la función \textasciigrave{}replace\_na()\textasciigrave{}, que susituye los valores perdidos en cada variable por el valor especificado.}
\end{Highlighting}
\end{Shaded}

No obstante, estas dos últimas, no son acciones recomendadas en primera instancia, porque filtrar registros con valores nulos o introducir valores que podrían no respetar la semántica de los datos puede ocasionar un alto impacto negativo en los niveles globales de calidad de datos del dataset.

Se puede ir más allá de la eliminación de valores \emph{missing} o de la estandarización. A través de diversos métodos se pueden generar valores que, con mayor o menor probabilidad, podrían ser los que realmente correspondieran a estos valores faltantes. A estas técnicas se las conocen como \textbf{técnicas de imputación de valores}. Para imputar valores perdidos se puede usar la función \texttt{preProcess()}del paquete \texttt{caret} o la función \texttt{imputate\_na()} del paquete \texttt{dlookR}. En el primer caso, hay diferentes valores que se le puede pasar al parámetro \emph{method}:

\begin{itemize}
\item
  \texttt{knnImpute}: permite utilizar el algoritmo kNN (k-vecinos más cercanos) para imputar valores perdidos. El algoritmo kNN requiere que se le indique el número de vecinos a utilizar en la predicción a través del parámetro k;
\item
  \texttt{bagImpute}: con este valor se usan varios árboles de decisión para hacer la imputación del valor \emph{missing};
\item
  \texttt{medianImpute}: imputa la mediana para el caso de una variable numérica, que suele ser preferible a imputar la media, puesto que el promedio puede verse afectado por \emph{outliers}. A continuación, se indica cómo funciona cada método de imputación usable la función \texttt{preProcess()}del paquete \texttt{caret}:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Se convierte a data.frame}
\NormalTok{gapminder\_cons }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(gapminder\_cons, }\AttributeTok{stringsAsFactors =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\# Se realiza el preprocesamiento:}
\NormalTok{pre\_knn }\OtherTok{\textless{}{-}} \FunctionTok{preProcess}\NormalTok{(gapminder\_cons, }\AttributeTok{method =} \StringTok{"knnImpute"}\NormalTok{, }\AttributeTok{k =} \DecValTok{2}\NormalTok{)}
\NormalTok{pre\_bag }\OtherTok{\textless{}{-}} \FunctionTok{preProcess}\NormalTok{(gapminder\_cons, }\AttributeTok{method =} \StringTok{"bagImpute"}\NormalTok{)}
\NormalTok{pre\_median }\OtherTok{\textless{}{-}} \FunctionTok{preProcess}\NormalTok{(gapminder\_cons, }\AttributeTok{method =} \StringTok{"medianImpute"}\NormalTok{)}
\CommentTok{\# Se obtienen los datos}
\NormalTok{imputed\_knn }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(pre\_knn, gapminder\_cons)}
\NormalTok{imputed\_bag }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(}
\NormalTok{  pre\_bag,}
\NormalTok{  gapminder\_cons}
\NormalTok{)}
\NormalTok{imputed\_median }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(pre\_median, gapminder\_cons)}
\end{Highlighting}
\end{Shaded}

\hypertarget{mejorando-la-exactitud-y-la-precisiuxf3n-eliminaciuxf3n-del-ruido-estaduxedstico}{%
\section{Mejorando la exactitud y la precisión: eliminación del ruido estadístico}\label{mejorando-la-exactitud-y-la-precisiuxf3n-eliminaciuxf3n-del-ruido-estaduxedstico}}

\index{exactitud} \index{precisión} \index{ruido estadístico}

El \textbf{ruido estadístico} es una variabilidad no explicada dentro de una muestra de datos. Generalmente consiste en errores (errores de medición y errores de muestreo) y residuos (diferencia entre su valor observado y el valor predicho).

\textbf{R} tiene, entre otras, dos funciones útiles para filtrar y reducir el ruido, \textbf{filter()} y \textbf{fft()} del paquete \texttt{stats}. Para explorar el uso del primero, en la Fig. \ref{fig:filterprev} se visualizan los dos conjuntos de datos que se usarán como ejemplos: una señal con ruido y una señal pura superpuesta sobre un fondo exponencial.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/filterprev} 

}

\caption{Señal con ruido}\label{fig:filterprev}
\end{figure}

La función \texttt{stat::filter()}aplica el filtrado lineal a una serie temporal univariante o a cada serie por separado de una serie temporal multivariante. Para estimar la relación señal-ruido, se puede utilizar el máximo de la señal pura y la desviación estándar de la señal con ruido usando 100 puntos divididos uniformemente entre los dos extremos.

La función \texttt{filter()} de \textbf{R} toma la forma general \texttt{filter(x,\ filter)}, donde \texttt{x} es el objeto que se filtra y \texttt{filter} es un objeto que contiene los coeficientes del filtro.

Para crear un filtro de media móvil de siete puntos idénticos, cada uno igual a \(1/7\), se puede usar la función \texttt{rep()}. Con ello se obtienen los valores del artículo original \citep{savitzky1964smoothing} y luego se aplica a la señal con ruido. Con este filtro, la relación señal-ruido mejora de \(4.57\) a \(14.28\) como se puede comprobar en la Fig. \ref{fig:filterpos}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{medmov\_7 }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1} \SpecialCharTok{/} \DecValTok{7}\NormalTok{, }\DecValTok{7}\NormalTok{)}
\CommentTok{\# La aplicación de este filtro a nuestra señal ruidosa devuelve el siguiente resultado}
\NormalTok{señal\_ruido\_medmov\_7 }\OtherTok{\textless{}{-}}\NormalTok{ stats}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(señal\_ruido, medmov\_7)}
\NormalTok{puntuacion2 }\OtherTok{\textless{}{-}} \FunctionTok{max}\NormalTok{(señal\_gauss) }\SpecialCharTok{/} \FunctionTok{sd}\NormalTok{(señal\_ruido\_medmov\_7[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{, }\DecValTok{201}\SpecialCharTok{:}\DecValTok{250}\NormalTok{)], }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/filterpos} 

}

\caption{Señal con tratamiento del ruido}\label{fig:filterpos}
\end{figure}

\hypertarget{integraciuxf3n-de-datos}{%
\section{\texorpdfstring{Integración de datos \index{datos!integración}}{Integración de datos }}\label{integraciuxf3n-de-datos}}

La integración de datos es una combinación de procesos técnicos y de negocio que se utilizan para combinar información proveniente de diferentes fuentes para convertirla en datos fiables y valiosos. En términos generales, se puede decir que consiste en acceder a los datos desde todas las fuentes y localizaciones tanto en entorno local, en la nube o de una combinación de ambos y la integración de datos, de modo que los registros de una fuente de datos mapeen registros en otra. Con ello, se obtenienen las vistas necesarias para el proceso de análisis o de negocio.

En el capítulo \ref{ch-110003} se introducía el concepto de las uniones de transformación y de filtrado. En este se desarrollará mediante un ejemplo práctico, junto a otros procesos como la comprobación de la integridad referencial y de las claves principales (o primary keys -PK-) y otras transformaciones, sobre el set de datos que se usarán en los capítulos correspondientes al \emph{machine learning}, correspondientes a la empresa de comercio electrónico ``Beauty eSheep''. Las tablas principales de su \emph{datawarehouse} son:

\begin{itemize}
\tightlist
\item
  \texttt{dim\_pedido}: Pedidos realizados por los clientes. Con las siguientes columnas: ide\_pedido (factor), des\_pedido (factor).
\item
  \texttt{dim\_fecha\_venta}: Distintas fechas en las que se han producido ventas en la empresa. Columnas: ide\_fecha\_venta (factor), des\_fecha\_venta (factor).
\item
  \texttt{dim\_vendedor}: Proveedores. Columnas: ide\_vendedor (factor), des\_vendedor (factor).
\item
  \texttt{dim\_categoria\_productos}: Catálogo de categorías de productos de la empresa. Columnas: ide\_categoria\_productos (factor), des\_categoria\_productos (factor).
\item
  \texttt{dim\_producto}: Detalle de los distintos productos que conforman el catálogo de ventas de la empresa. Columnas: ide\_producto (factor), des\_producto (factor), ide\_categoria\_productos (factor).
\item
  \texttt{dim\_nivel\_educacion\_cliente}: Niveles de educacación de los clientes. Columnas: ide\_nivel\_educacion\_cliente (tipo factor), des\_nivel\_educacion\_cliente (factor).
\item
  \texttt{dim\_cliente}: Clientes que tiene o ha tenido la empresa. Columnas: ide\_cliente (factor), ide\_cliente\_nif (factor), des\_cliente (factor), num\_edad (entero), num\_tamano\_familiar (entero), num\_anos\_experiencia (entero), num\_ingresos\_ano (doble), ide\_nivel\_educacion\_cliente (factor).
\end{itemize}

Una de las herramientas principales para la integración, son las uniones de transformación, que agregan las columnas de un conjunto \(y\) a otro \(x\) enlazando las filas, según las claves:

\begin{itemize}
\item
  \texttt{inner\_join()}: incluye todas las filas en \(x\) e \(y\).
\item
  \texttt{left\_join()}: incluye todas las filas en \(x\).
\item
  \texttt{right\_join()}: incluye todas las filas en \(y\);
\item
  \texttt{full\_join()}: incluye todas las filas en \(x\) o \(y\). Si una fila en \(x\) enlaza a varias filas de \(y\), se devuelven todas las filas de \(y\), por cada vez que enlace con \(x\).
\end{itemize}

Por otro lado, las uniones de filtrado filtran las filas de \(x\) en función de la presencia o ausencia de coincidencias en \(y\):

\begin{itemize}
\item
  \texttt{semi\_join()} devuelve todas las filas de \(x\) con una coincidencia en \(y\);
\item
  \texttt{anti\_join()} devuelve todas las filas de \(x\) que no tengan una coincidencia en \(y\). La función
\item
  \texttt{nest\_join()} devuelve todas las filas y columnas de \(x\) con una nueva columna anidada, que contiene todas las coincidencias de \(y\). Cuando no hay ninguna coincidencia, la columna de la lista es un tibble de 0 filas.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/integracion-datos-iniciales} 

}

\caption{Proceso de integración de datos iniciales}\label{fig:rmarkdown1}
\end{figure}

En la Fig. \ref{fig:rmarkdown1}, se muestra el proceso de integración de los datos iniciales de ``eSheep''. En el siguiente código se puede ver parte de dicho proceso, donde se aplican los conceptos vistos hasta ahora:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Se hace para todas las dimensiones.}
\CommentTok{\# Por ejemplo, para la tabla fac\_venta: se convierten en factores las variables}
\NormalTok{fac\_venta }\OtherTok{\textless{}{-}}\NormalTok{ fac\_venta }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{((}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"ide"}\NormalTok{))), as.factor)}
\CommentTok{\# Asimismo, se reconvierte la variable ide\_fecha a tipo fecha:}
\NormalTok{fac\_venta}\SpecialCharTok{$}\NormalTok{ide\_fecha }\OtherTok{\textless{}{-}} \FunctionTok{as.Date}\NormalTok{(fac\_venta}\SpecialCharTok{$}\NormalTok{ide\_fecha, }\StringTok{"\%d/\%m/\%Y"}\NormalTok{)}
\ControlFlowTok{if}\NormalTok{ (}\FunctionTok{any}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(fac\_venta}\SpecialCharTok{$}\NormalTok{ide\_fecha))) \{}
  \FunctionTok{warning}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Fechas ide\_fecha erroneas en fac\_venta"}\NormalTok{))}
\NormalTok{\}}
\CommentTok{\# Comprobación de la clave primaria o primary key (pk) para la tabla fac\_venta:}
\ControlFlowTok{if}\NormalTok{ (}\FunctionTok{nlevels}\NormalTok{(fac\_venta}\SpecialCharTok{$}\NormalTok{ide\_venta) }\SpecialCharTok{!=} \FunctionTok{nrow}\NormalTok{(fac\_venta)) \{}
  \FunctionTok{stop}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}
    \StringTok{"Error PK fac\_venta filas: "}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(fac\_venta),}
    \StringTok{" valores distintos"}\NormalTok{, }\FunctionTok{nlevels}\NormalTok{(fac\_venta}\SpecialCharTok{$}\NormalTok{ide\_venta)}
\NormalTok{  ))}
\NormalTok{\}}
\CommentTok{\# Comprobación de la integridad referencial entre fac\_venta y dim\_fecha\_venta}
\NormalTok{df\_error }\OtherTok{\textless{}{-}}\NormalTok{ fac\_venta }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{anti\_join}\NormalTok{(dim\_fecha\_venta)}
\ControlFlowTok{if}\NormalTok{ (}\FunctionTok{nrow}\NormalTok{(df\_error)) \{}
  \FunctionTok{stop}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"No integridad ref fac\_venta vs DIM\_FECHA\_VENTA, n. filas erróneas: "}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(df\_error)))}
\NormalTok{\}}
\CommentTok{\# En este paso se ha usado la función anti\_join, que sirve para identificar los registros de una tabla que no coinciden con la otra tabla.}
\CommentTok{\# A continuación, se construye el tablón con los datos agregados de las compras de los clientes}
\NormalTok{dim\_cliente}\SpecialCharTok{$}\NormalTok{ide\_cliente }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(dim\_cliente}\SpecialCharTok{$}\NormalTok{ide\_cliente)}
\NormalTok{dp\_fac\_cliente\_venta }\OtherTok{\textless{}{-}}\NormalTok{ fac\_venta }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(ide\_cliente, ide\_producto) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{num\_imp =} \FunctionTok{sum}\NormalTok{(num\_imp), }\AttributeTok{ind\_pro =} \FunctionTok{as.logical}\NormalTok{(}\FunctionTok{sum}\NormalTok{(num\_imp))) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{inner\_join}\NormalTok{(dim\_producto) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(ide\_cliente, des\_producto, num\_imp, ind\_pro) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pivot\_wider}\NormalTok{(}
    \AttributeTok{names\_from =}\NormalTok{ des\_producto, }\AttributeTok{names\_sort =}\NormalTok{ T,}
    \AttributeTok{values\_from =} \FunctionTok{c}\NormalTok{(ind\_pro, num\_imp),}
    \AttributeTok{values\_fill =} \FunctionTok{list}\NormalTok{(}\AttributeTok{ind\_pro =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{num\_imp =} \DecValTok{0}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate\_if}\NormalTok{(is.logical, }\FunctionTok{funs}\NormalTok{(}\FunctionTok{factor}\NormalTok{(., }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"FALSE"}\NormalTok{, }\StringTok{"TRUE"}\NormalTok{), }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"N"}\NormalTok{, }\StringTok{"S"}\NormalTok{)))) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{inner\_join}\NormalTok{(dim\_cliente) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{inner\_join}\NormalTok{(dim\_nivel\_educacion\_cliente) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\StringTok{"ide\_nivel\_educacion\_cliente"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(ide\_cliente))}
\CommentTok{\# En la función de pivot\_wider, se puede observar que se usa el parámetro values\_fill, que rellena donde las combinaciones no existen.}
\CommentTok{\# Por su parte,  mutate\_if(is.logical, funs(factor(., levels=c("FALSE","TRUE"), labels=c("N","S")))) sustituye FALSE por N y TRUE por S.}
\end{Highlighting}
\end{Shaded}

Como resultado del proceso de integración, se obtiene la tabla dp\_fac\_cliente\_venta (a partir de la que se obtiene dp\_fac\_cliente\_venta\_pro13), con los campos que se listan a continuación:

\begin{itemize}
\tightlist
\item
  \texttt{ide\_cliente\ (tipo\ factor)}: Código identificador único del cliente.
\item
  \texttt{ind\_pro11-17\ (factor)}: Indicador si el cliente es consumidor de ese producto (`S') o no (`N').
\item
  \texttt{importe\_pro11-17\ (doble)}: Importe neto global consumido por el cliente en ese producto en euros.
\item
  \texttt{ide\_cliente\_nif\ (factor)}: Código identificador oficial único del cliente.
\item
  \texttt{des\_cliente\ (factor)}: Descripción del cliente (apellidos, nombre).
\item
  \texttt{edad\ (entero)}: Edad del cliente.
\item
  \texttt{tamano\_fam\ (entero)}: Número de miembros de la unidad familiar a la que pertenece el cliente.
\item
  \texttt{anos\_exp\ (entero)}: Años de trabajo del cliente.
\item
  \texttt{ingresos\_ano\ (doble)}: Ingresos anuales del cliente en euros.
\item
  \texttt{des\_nivel\_edu\ (factor)}: Descripción del nivel de educación del cliente.
\end{itemize}

En general, los tipos de datos más comunes son los numéricos y los categóricos, aunque cada uno puede subdividirse aún más, como tipo entero, doble y punto flotante para variables numéricas y booleano, ordinal o nominal para variables categóricas. El tipo \texttt{factor} de \textbf{R}, es de tipo categórica con un número finito de valores o niveles.

A partir de \texttt{dp\_fac\_cliente\_venta} y \texttt{dp\_fac\_cliente\_venta\_pro13}, se obtiene \texttt{dp\_entr} (datos para entrenamiento de los modelos en su escala original) y \texttt{dp\_test} (datos para testeo de los modelos en su escala original). A su vez, a partir de estos útimos, se obtienen los equivalentes con variables sólo numéricas (\texttt{dp\_entr\_num} y \texttt{dp\_test\_num}), que son los conjuntos de datos preparados para la fase de modelado. En el próximo capítulo, se puede ver cómo se obtienen y cómo usarlos en el resto del preprocesamiento.

\begin{infobox_resume}

\textbf{RESUMEN}

\begin{itemize}
\tightlist
\item
  En un proyecto de modelado predictivo, como la clasificación o la regresión,
  los datos en bruto normalmente no pueden usarse directamente, sino que deben procesarse
  para asegurar niveles adecuados de calidad y permitir descubrir la estructura subyacente del caso de uso a los algoritmos de aprendizaje.
\item
  Se han visto las principales funciones de manipulación de datos del paquete \texttt{tidyverse} con el propósito de limpieza de datos y el paquete \texttt{caret} para la imputación de valores missing.
\item
  Se ha dado una visión del tratamiento de los datos cuando se presentan problemas de calidad de datos, valores \emph{missing} y problemas de ruido estadístico.
\item
  Se ha realizado un proceso de integración de datos con el ejemplo de ``Beauty eSheep''.
\end{itemize}

\end{infobox_resume}

\hypertarget{120006-aed}{%
\chapter{Análisis exploratorio de datos}\label{120006-aed}}

\emph{Emilio L. Cano}

\hypertarget{introducciuxf3n-4}{%
\section{Introducción}\label{introducciuxf3n-4}}

\index{análisis exploratorio de datos}
\index{visualización}
\index{población}
\index{muestra}
\index{Estadística Descriptiva}
\index{Inferencia Estadística}

El análisis exploratorio de datos (AED), y en particular su visualización,
es el primer análisis que se debe hacer sobre cualquier conjunto de datos.
El AED se realiza mediante dos herramientas: los resúmenes numéricos
y las visualizaciones gráficas.
La ``historia'' que nos esté contando el gráfico de los datos, nos guiará
hacia las técnicas de aprendizaje estadístico más adecuadas. Incluso,
en muchas ocasiones será suficiente el AED para tomar
una decisión sobre el problema en estudio.

\hypertarget{el-cuarterto-de-anscombe}{%
\subsection{El cuarterto de Anscombe}\label{el-cuarterto-de-anscombe}}

\index{cuarteto de Anscombe}

Un ejemplo clásico de la importancia del \textbf{AED} y, concretamente, las representaciones gráficas es el ``Cuarteto de Anscombe'' \citep{Anscombe1973},
el cual está compuesto por 11 filas de 8 variables numéricas que conforman 4 conjuntos de datos (disponibles en el objeto \texttt{anscombe}), con los mismos resúmenes estadísticos pero que propiedades muy distintas, lo que se ve fácilmente cuando se representa en forma gráfica. Si se calcula, por ejemplo, media y la desviación típica de cada variable se observa que son prácticamente iguales. Incluso los coeficientes de correlación de cada \texttt{x} con su \texttt{y} son también prácticamente idénticos.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{anscombe }\SpecialCharTok{|\textgreater{}} \FunctionTok{summarise}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\AttributeTok{.fns =}\NormalTok{ mean))}
\CommentTok{\#\textgreater{}  x1 x2 x3 x4       y1     y2     y3     y4}
\CommentTok{\#\textgreater{}  9  9  9  9    7.5009  7.5009  7.5   7.5009}
\NormalTok{anscombe }\SpecialCharTok{|\textgreater{}} \FunctionTok{summarise}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\AttributeTok{.fns =}\NormalTok{ sd))}
\CommentTok{\#\textgreater{}    x1    x2       x3       x4     y1      y2    y3      y4}
\CommentTok{\#\textgreater{} 3.316  3.316   3.316   3.316   2.031   2.031   2.030   2.030}
\end{Highlighting}
\end{Shaded}

Sin embargo, la Fig. \ref{fig:anscombeuni} muestra que, a pesar de tener medias y desviaciones típicas prácticamente iguales, los datos son muy diferentes.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/anscombeuni-1} 

}

\caption{Representación de las variables del cuarteto de Anscombe}\label{fig:anscombeuni}
\end{figure}

Si en el análisis por separado ya se ve la necesidad de hacer un gráfico,
ésta es más evidente cuando se analizan las variables conjuntamente. La
Figura \ref{fig:anscombelm} muestra los cuatro gráficos que constituyen
``El cuarteto de Anscombe'', y que se puede obtener de la propia ayuda del
conjunto de datos (\texttt{example(anscombe)}). La línea de regresión que se ajusta
es prácticamente la misma, y los coeficientes de correlación entre las variables \emph{X} e \emph{Y} de los cuatro gráficos, idénticos: 0.8163.
Es evidente que la relación entre las variables es muy distinta en cada uno
de los casos, y si no se visualizan los datos para elegir el mejor modelo
de regresión y después interpretarlo, se pueden tomar decisiones
erróneas. El cuarteto de Anscombe es muy ilustrativo, al igual que \emph{The Datasaurus Dozen:} \citep{Matejka2017} en \url{https://www.autodeskresearch.com/publications/samestats}.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/anscombelm-1} 

}

\caption{Los cuatro gráficos que constituyen El cuarteto de Anscombe junto con un ajuste lineal}\label{fig:anscombelm}
\end{figure}

\hypertarget{conceptos-generales}{%
\subsection{Conceptos generales}\label{conceptos-generales}}

Muy brevemente, se presentan una serie de conceptos esenciales para la mejor
comprensión de este manual\footnote{Para un análisis extenso de los conceptos aquí expuestos puede consultarse, por ejemplo, \citet{montero2007}.}. Los datos que se analizan,
provienen de una determinada \textbf{población}, y no son más que
una \textbf{muestra}, es decir, un subconjunto de toda la población.
La \textbf{Estadística Descriptiva} se ocupa del AED en sentido amplio, que se aplica
sobre los datos concretos de la muestra. La
\textbf{Inferencia Estadística} (véase Capítulo \ref{Fundainfer} hace referencia a los métodos mediante los cuales, a través de los datos muestrales, se toman decisiones, se analizan relaciones, o se hacen predicciones sobre la población. Para ello, haremos uso de la \textbf{Probabilidad} (véase Capítulo \ref{Funda-probab}) aplicando el método más adecuado. Además, será muy importante considerar el método de obtención de la muestra (véase Capítulo \ref{muestreo} que, en términos generales, debe ser representativa de la población para que las conclusiones sean válidas.
La Fig. \ref{fig:dogma1} representa la esencia de la Estadística y sus métodos.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/dogma1} 

}

\caption{La esencia de los métodos estadísticos}\label{fig:dogma1}
\end{figure}

Por otra parte, los individuos de una población vendrán clasificados por
un carácter o fenómeno estadístico. Éstos pueden ser de dos tipos:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Cuantitativos} (se pueden medir o contar). Se denomina \textbf{variable} a cualquier fenómeno estadístico que pueda expresarse en valores numéricos. Se clasifican como \textbf{variables discretas}, (se puede \emph{contar} el número de valores que toma) y \textbf{continuas} (pueden tomar cualquier valor en un intervalo dado).
\item
  \textbf{Cualitativos} (no se pueden expresar como un número). Se denomina \textbf{variable cualitativa, atributo o factor} a cualquier fenómeno estadístico que indica una \emph{cualidad} o \emph{atributo}. Éstas pueden tener varios niveles o solo dos (dicotómicas). Si en una variable categórica se pueden \emph{ordenar} las categorías, entonces tenemos \textbf{variables ordinales}.
\end{enumerate}

\hypertarget{componentes-de-un-gruxe1fico-y-su-representaciuxf3n-en-r}{%
\subsection{Componentes de un gráfico y su representación en R}\label{componentes-de-un-gruxe1fico-y-su-representaciuxf3n-en-r}}

De los diferentes sistemas que dispone R para representar gráficos (los ``base'', paquete \texttt{graphics}, y los ``grid'', paquete \texttt{lattice} \citep{lattice}),
este capítulo se centra en el paquete \texttt{ggplot2} \citep{Wickham2016}, que forma parte del \emph{tidyverse}, por su amplio uso y popularidad.

El flujo de trabajo con \texttt{ggplot2} se puede resumir en los siguientes pasos:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Proporcionar una \textbf{tabla de datos} a la función \texttt{ggplot}. Es el primer argumento (\texttt{data}) y
  se puede utilizar el operador \emph{pipe}.
\item
  Proporcionar las \textbf{columnas} de la tabla de datos que serán representadas en el gráfico.
  Este será el segundo argumento (\texttt{mapping}) de la función \texttt{ggplot}, y se
  especifica con la función \texttt{aes} (\emph{aesthetics})
  como una lista de pares \emph{aesthetic = variable}, de forma que el elemento especificado
  como \emph{aesthetic} será ``mapeado'' a los valores de la variable. Esta especificación
  se puede hacer también en las funciones que añaden capas, que se explican a continuación.
  Los \emph{aesthetics} más comunes (y para muchos tipos de gráficos obligatorios) son \texttt{x} e \texttt{y},
  es decir, las variables que se usarán para el eje horizontal y el eje vertical respectivamente.
  Además, se pueden especificar variables para el color, el tamaño, el símbolo de los puntos,
  el tipo de línea, el texto, y otros específicos del tipo de gráfico. Los \emph{aesthetics} se
  pueden especificar también de forma ``fija'' (sin depender de ninguna variable) fuera de
  la función \texttt{aes}.
\item
  Añadir las \textbf{capas} del gráfico con los \emph{geoms}, es decir, los objetos geométricos que representan a cada variable. Esto se indica con el operador \texttt{+}, como si se ``sumasen'' componentes al gráfico mediante funciones \texttt{geom\_xxx}.
\item
  Añadir otras capas al gráfico, por ejemplo, una capa de etiquetas del gráfico
  (función \texttt{labs}), de ejes, para modificar los ejes y leyendas creados por defecto
  (funciones \texttt{scale\_*\_xxx}), para crear nuevas variables a representar basadas en los datos (funciones \texttt{stat\_xxx}).
\item
  Añadir un tema al gráfico, por ejemplo, en blanco y negro, o con especificaciones
  concretas como el posicionamiento de la leyenda.
\item
  Añadir ``facetas'' (\emph{facets}). De esta forma se divide el gráfico en varios subgráficos
  basándose en los valores de una o más variables discretas (normalmente categóricas).
\end{enumerate}

\hypertarget{120006-aeduni}{%
\section{Análisis exploratorio de una característica}\label{120006-aeduni}}

\hypertarget{variables-cualitativas}{%
\subsection{Variables cualitativas}\label{variables-cualitativas}}

El resumen numérico de variables cualitativas se muestra en la tabla de frecuencias,
la cual se puede representar con un gráfico de barras o con un gráfico
de sectores\footnote{El gráfico de sectores no es recomendable ya que proporciona la misma
  información que el gráfico de barras y es mucho más difícil para el ojo
  humano distinguir ángulos que alturas.}. Por ejemplo, el conjunto de datos \texttt{accidentes2020\_data}
disponible en el paquete \texttt{CDR}, describe los datos de accidentes de tráfico en la Ciudad de Madrid registrados por Policía Municipal con víctimas y/o daños al patrimonio. Entre sus variables, contiene la variable cualitativa tipología del accidente \texttt{tipo\_accidente}. Un resumen puede obtenerse tanto con la función \texttt{table()}
como con el paquete \texttt{dplyr}, como se vio en la Sección \ref{110003-tidyverse}:
En variables cualitativas, llamamos a la categoría más frecuente \textbf{moda} de la variable.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(CDR)}
\FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{accidentes2020\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(tipo\_accidente) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop =} \DecValTok{100} \SpecialCharTok{*}\NormalTok{ n }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(n))}
\CommentTok{\#\textgreater{}                  tipo\_accidente    n       prop}
\CommentTok{\#\textgreater{}  1:                      Alcance 7294 22.4936010}
\CommentTok{\#\textgreater{}  2:           Atropello a animal   75  0.2312887}
\CommentTok{\#\textgreater{}  3:          Atropello a persona 2127  6.5593487}
\CommentTok{\#\textgreater{}  4:                        Caída 2118  6.5315940}
\CommentTok{\#\textgreater{}  5: Choque contra obstáculo fijo 4667 14.3923274}
\CommentTok{\#\textgreater{}  6:             Colisión frontal  899  2.7723810}
\CommentTok{\#\textgreater{}  7:      Colisión fronto{-}lateral 8081 24.9205909}
\CommentTok{\#\textgreater{}  8:             Colisión lateral 4386 13.5257656}
\CommentTok{\#\textgreater{}  9:            Colisión múltiple 2231  6.8800691}
\CommentTok{\#\textgreater{} 10:                Despeñamiento    2  0.0061677}
\CommentTok{\#\textgreater{} 11:                         Otro  251  0.7740463}
\CommentTok{\#\textgreater{} 12:        Solo salida de la vía  151  0.4656613}
\CommentTok{\#\textgreater{} 13:                       Vuelco  145  0.4471582}
\end{Highlighting}
\end{Shaded}

Para representar el gráfico de barras con la función \texttt{ggplot()} se añade la función \texttt{geom\_bar()} (ver Fig. \ref{fig:geombar}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\NormalTok{accidentes2020\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ tipo\_accidente), }\AttributeTok{fill =} \StringTok{"pink"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/geombar-1} 

}

\caption{Gráfico de barras con ggplot2}\label{fig:geombar}
\end{figure}

\begin{infobox}
El código anterior es la forma más básica de hacer un gráfico con \texttt{ggplot2}.
Opciones más avanzadas pueden encontrarse en el libro
\emph{R for Data Science} \citep{wickham2016r}.

\end{infobox}

Ya se ha comentado que los gráficos de sectores no se recomiendan a menos que se incluya información numérica. El paquete \texttt{ggstatsplot} realiza gráficos que incluyen análisis estadísticos. Por ejemplo, la función \texttt{ggpiestats()} proporciona un gráfico de sectores con algunos tests estadísticos (ver la ayuda de la función) y podría utilizarse para determinar en qué medida un conjunto de 80 ayuntamientos de distinto signo político presta o no un determinado servicio \texttt{serv} (ver el conjunto de datos en el paquete del libro \texttt{?CDR::ayuntam}). El siguiente código produce el gráfico de la Figura \ref{fig:ggs}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggstatsplot)}
\NormalTok{ayuntam }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggpiestats}\NormalTok{(}\AttributeTok{x =}\NormalTok{ serv)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/ggs-1} 

}

\caption{Gráfico de sectores con tests. Prestación o no de un determinado servicio X}\label{fig:ggs}
\end{figure}

Una alternativa a los gráficos de sectores son los \emph{waffle charts} (gráficos de gofre, o de tableta de chocolate). La siguiente expresión produce el gráfico de la Figura \ref{fig:gofre} usando el paquete \texttt{waffle}. Con el argumento \texttt{use\_glyph} se pueden incluir iconos en vez de cuadrados.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(waffle)}
\NormalTok{freq }\OtherTok{\textless{}{-}}\NormalTok{ ayuntam }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(serv)}
\NormalTok{m }\OtherTok{\textless{}{-}} \FunctionTok{setNames}\NormalTok{(freq}\SpecialCharTok{$}\NormalTok{n, freq}\SpecialCharTok{$}\NormalTok{serv)}
\FunctionTok{waffle}\NormalTok{(m,}
  \AttributeTok{rows =} \DecValTok{4}\NormalTok{, }\AttributeTok{colors =} \FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"green"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/gofre-1} 

}

\caption{Gráfico waffle: Prestación o no de un determinado servicio X por parte de 80 ayuntamientos}\label{fig:gofre}
\end{figure}

\hypertarget{variables-cuantitativas}{%
\subsection{Variables cuantitativas}\label{variables-cuantitativas}}

Los estadísticos descriptivos más importantes que se hallan en un AED
se dividen en tres grandes grupos:

\begin{itemize}
\item
  \textbf{Medidas de posición}, que su vez se divide en (i) central: media (\texttt{mean()}), mediana (\texttt{median()}) y moda y (ii) no central: cuantiles \texttt{quantile()}, mínimo (\texttt{min()}) y el máximo (\texttt{max()}).
\item
  \textbf{Medidas de dispersión}. Las más importantes son: varianza (\texttt{var()}), desviación típica (\texttt{sd()}), rango intercuartílico (\texttt{IQR()}), desviación absoluta mediana (\texttt{mad()}) y coeficiente de Variación (\texttt{sd(x)/mean(x)})
\item
  \textbf{Medidas de forma}: asimetría (\emph{skewness}) y aplanamiento (\emph{kurtosis})
\end{itemize}

La función \texttt{summary()} de \textbf{R} base es una función de las llamadas ``genéricas''
y solo aborda las medidas de posición.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(CDR)}
\FunctionTok{summary}\NormalTok{(renta\_municipio\_data}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{2019}\StringTok{\textasciigrave{}}\NormalTok{)}
\CommentTok{\#\textgreater{}    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA\textquotesingle{}s }
\CommentTok{\#\textgreater{}    4053    9914   11595   12247   13690   32183    5697}
\end{Highlighting}
\end{Shaded}

Sin embargo, los estadíscos descriptivos suelen presentarse juntos ``describiendo'' al conjunto de datos. Existen distintos paquetes, como \texttt{summarytools}, que proporcionan un resumen completo de un vector numérico con la función \texttt{descr()} y de un conjunto de datos completo (ver opciones del paquete.)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(summarytools)}
\NormalTok{renta\_municipio\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{2019}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{descr}\NormalTok{()}

\CommentTok{\#\textgreater{}Descriptive Statistics}
\CommentTok{\#\textgreater{}2019}
\CommentTok{\#\textgreater{}N: 55273}
\CommentTok{\#\textgreater{}                        2019}
\CommentTok{\#\textgreater{}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}             Mean   12246.84}
\CommentTok{\#\textgreater{}          Std.Dev    3562.94}
\CommentTok{\#\textgreater{}              Min    4053.00}
\CommentTok{\#\textgreater{}               Q1    9914.00}
\CommentTok{\#\textgreater{}           Median   11595.00}
\CommentTok{\#\textgreater{}               Q3   13690.50}
\CommentTok{\#\textgreater{}              Max   32183.00}
\CommentTok{\#\textgreater{}              MAD    2742.81}
\CommentTok{\#\textgreater{}              IQR    3776.25}
\CommentTok{\#\textgreater{}               CV       0.29}
\CommentTok{\#\textgreater{}         Skewness       1.82}
\CommentTok{\#\textgreater{}      SE.Skewness       0.01}
\CommentTok{\#\textgreater{}         Kurtosis       5.77}
\CommentTok{\#\textgreater{}          N.Valid   49576.00}
\CommentTok{\#\textgreater{}        Pct.Valid      89.69}
\end{Highlighting}
\end{Shaded}

La representación de la tabla de frecuencias de una variable continua
es el \textbf{histograma}. En el caso de las
variables continuas, para representarlo, se cuenta el número de observaciones (frecuencia) por intervalos (\emph{bins}). Una posible regla sería el método de Sturges\footnote{Este es el método que utiliza por defecto la función \texttt{hist}
  de \textbf{R} base, que además redondea la amplitud del intervalo para facilitar la
  interpretación. Otra regla muy sencilla es tomar como número de intervalos
  entorno a la raíz cuadrada del número total de datos.}, que se puede hallar con la función \texttt{nclass.Sturges()}.

Para obtener tabla de frecuencias de la renta neta media per cápita en 2019
usando el número de intervalos según la regla de Sturges se haría:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{renta\_municipio\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{clases\_sturges\_renta =} \FunctionTok{cut}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{2019}\StringTok{\textasciigrave{}}\NormalTok{,}
    \AttributeTok{breaks =} \FunctionTok{nclass.Sturges}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{2019}\StringTok{\textasciigrave{}}\NormalTok{)}
\NormalTok{  )) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(clases\_sturges\_renta)}
\end{Highlighting}
\end{Shaded}

Sin embargo, esta regla no siempre es la más apropiada,
como se verá en la Sección \ref{no-mentir},
pues debe estudiarse bien la naturaleza de la variable a analizar.

El histograma proporciona mucha información sobre la variable: (i) si es aproximadamente simétrica, (ii) si tiene forma de campana (se parece a la distribución Normal), (iii) si hay valores extremos y cómo son de frecuentes,
y (iv) si puede haber mezcla de poblaciones (más de una moda).

La función \texttt{geom\_histogram()} del paquete \texttt{ggplot2} añade una capa con un
histograma al gráfico. El color de las barras se controla con el \emph{aesthetics} \texttt{fill}, y la altura puede representar las frecuencias absolutas (recuentos) o
relativas (proporciones). El número de intervalos se indica con el argumento
\texttt{bins}, o alternativamente la anchura de intervalo con \texttt{bin\_width}, véase la Fig. \ref{fig:plot-hist-bins}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OtherTok{\textless{}{-}}\NormalTok{ renta\_municipio\_data }\SpecialCharTok{|\textgreater{}}
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{drop\_na}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{2019}\StringTok{\textasciigrave{}}\NormalTok{))}

\NormalTok{h1 }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{+} \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{color =} \StringTok{"yellow"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"pink"}\NormalTok{)}
\NormalTok{h2 }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{+} \FunctionTok{geom\_histogram}\NormalTok{(}
  \AttributeTok{color =} \StringTok{"yellow"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"pink"}\NormalTok{,}
  \AttributeTok{bins =} \FunctionTok{nclass.Sturges}\NormalTok{(renta\_municipio\_data}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{2019}\StringTok{\textasciigrave{}}\NormalTok{)}
\NormalTok{)}
\NormalTok{h3 }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{+} \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{color =} \StringTok{"yellow"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"pink"}\NormalTok{, }\AttributeTok{bins =} \DecValTok{20}\NormalTok{)}

\FunctionTok{library}\NormalTok{(patchwork)}
\NormalTok{h1 }\SpecialCharTok{+}\NormalTok{ h2 }\SpecialCharTok{+}\NormalTok{ h3}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/plot-hist-bins} 

}

\caption{Histogramas de la renta neta media per cápita en 2019 con distintos bins. Iquierda: bins por defecto (n=30); Centro: bins con la regla de Strurges; Derecha: bins = 20}\label{fig:plot-hist-bins}
\end{figure}

Una representación alternativa al histograma es la línea de densidad, que sustituye
las barras por una línea continua, generalmente suavizada. A continuación, se añade
la linea de densidad a uno de los histogramas de la Fig. \ref{fig:plot-hist-bins}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\SpecialCharTok{+} \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =} \FunctionTok{after\_stat}\NormalTok{(density)),}
  \AttributeTok{position =} \StringTok{"identity"}\NormalTok{,}
  \AttributeTok{color =} \StringTok{"yellow"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"pink"}
\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{lwd =} \DecValTok{1}\NormalTok{, }\AttributeTok{colour =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/plot-hist-bins} 

}

\caption{Histograma y linea de densidad de la renta neta per capita española en 2019 }\label{fig:plot-hist-density}
\end{figure}

Otra representación gráfica muy útil de las variables continuas son el
gráfico de \textbf{cajas y bigotes} y el \textbf{diagrama de violín}, que se obtienen fácilmente combinando en \texttt{ggplot()} las capas \texttt{geom\_boxplot()} y \texttt{geom\_violin()} respectivamente (véase Fig. \ref{fig:boxplot-violin}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OtherTok{\textless{}{-}}\NormalTok{ renta\_municipio\_data }\SpecialCharTok{|\textgreater{}}
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{drop\_na}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \DecValTok{0}\NormalTok{, }\AttributeTok{y =} \StringTok{\textasciigrave{}}\AttributeTok{2019}\StringTok{\textasciigrave{}}\NormalTok{))}
\NormalTok{boxplot }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{+} \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{color =} \StringTok{"yellow"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"pink"}\NormalTok{)}
\NormalTok{violin }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{+} \FunctionTok{geom\_violin}\NormalTok{(}\FunctionTok{aes}\NormalTok{(), }\AttributeTok{color =} \StringTok{"yellow"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"pink"}\NormalTok{)}
\NormalTok{boxplot }\SpecialCharTok{+}\NormalTok{ violin}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/plot-violin-zonas} 

}

\caption{Boxplot y violin plot de la renta neta media por persona (€) en 2019}\label{fig:plot-boxplot-violin}
\end{figure}

Otra visualización básica para una variable numérica es la visualización
secuencial de las observaciones, bien a través de puntos (\texttt{geom\_point()}) o a través de líneas (\texttt{geom\_line()}). El orden de las observaciones nos pueden indicar cuándo se ha producido un cambio u otros patrones.

\hypertarget{120006-aedmulti}{%
\section{Análisis exploratorio de varias características}\label{120006-aedmulti}}

En el apartado \ref{120006-aeduni} se ha realizado un AED de
variables aisladas, pero lo usual es incluir las relaciones entre otras variables
dentro del AED. Además, en el caso de las variables numéricas
se pueden obtener algunas medidas de resumen
conjunto que indiquen la relación entre las distintas características.
Las herramientas estadísticas utilizadas son: (i) las tablas de frecuencias conjuntas, que, en el caso de dos atributos, pueden ser tablas de doble entrada,
con un atributo en filas y el otro en columnas, para determinar si existe asociación entre las variables, como se verá en el Capitulo \ref{tablas-contingencia};
(ii) los resúmenes numéricos, como la covarianza, el coeficiente de correlación, coeficientes de asociacion, etc. y (iii) los gráficos en los que se puede representar más de una variable.

\hypertarget{variables-cualitativas-1}{%
\subsection{Variables cualitativas}\label{variables-cualitativas-1}}

El resumen numérico sigue siendo la tabla
de frecuencias, en este caso conjuntas para los distintos niveles
que toman las variables. Para dos atributos, se puede representar en
forma de tabla de doble entrada. Este tipo de tabas se denominan \textbf{tablas de contingencia} (ver Capitulo \ref{tablas-contingencia}).

El resultado de la función \texttt{table()} se puede utilizar dentro de las funciones \texttt{prop.table()} y \texttt{addmargins()} para obtener las frecuencias relativas, añadir las marginales, o ambas cosas. Para el ejemplo de la prestacion de servicio o no por parte de 80 ayuntamientos, \texttt{table()} podría utilizarse para dar respuesta a la siguiente pregunta: ¿La prestación pública del servicio \emph{X} es independiente del signo político del Ayuntamiento o depende de dicho signo?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(ayuntam}\SpecialCharTok{$}\NormalTok{signo\_gob, ayuntam}\SpecialCharTok{$}\NormalTok{serv)}
\CommentTok{\#\textgreater{}             }
\CommentTok{\#\textgreater{}              No Sí}
\CommentTok{\#\textgreater{}   Avanzados  14 28}
\CommentTok{\#\textgreater{}   Ilustrados  6 32}
\end{Highlighting}
\end{Shaded}

Aunque la representación gráfica más habitual siguen siendo los gráficos de barras:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OtherTok{\textless{}{-}}\NormalTok{ ayuntam }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(signo\_gob, }\AttributeTok{fill =}\NormalTok{ serv))}

\NormalTok{frecuencias }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{+} \FunctionTok{geom\_bar}\NormalTok{()}
\NormalTok{proporciones }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{+} \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \FunctionTok{position\_fill}\NormalTok{())}

\NormalTok{frecuencias }\SpecialCharTok{+}\NormalTok{ proporciones}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/barras-ayunt-1} 

}

\caption{Grafico de barras de la prestación pública del servicio *X* por parte de 80 Ayuntamientos de distinto signo político. Iquierda: frecuencias absolutas. Derecha: frecuencias relativas.}\label{fig:barras-ayunt}
\end{figure}

Una visualización interesante de tablas de doble entrada son gráficos en los que
se representan las frecuencias conjuntas por medio de puntos cuyo área es proporcional a la frecuencia. La Fig. \ref{fig:balloonplot-eda-ayunt} muestra gráficamente la tabla de frecuencias conjunta de los atributos \texttt{signo\_gob} y \texttt{serv} del conjunto de datos \texttt{ayuntam}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gplots)}
\FunctionTok{balloonplot}\NormalTok{(}\FunctionTok{table}\NormalTok{(ayuntam}\SpecialCharTok{$}\NormalTok{signo\_gob, ayuntam}\SpecialCharTok{$}\NormalTok{serv))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/balloonplot-eda-ayunt-1} 

}

\caption{Representación gráfica de tabla de frecuencias con la función `ballonplot()`}\label{fig:balloonplot-eda-ayunt}
\end{figure}

Para representar más de dos factores a la vez en un único gráfico, se dispone de los gráficos de mosaico con la función \texttt{mosaicplot()} de R base, o bien el paquete
\texttt{ggmosaic}, que incluye una función \texttt{geom\_mosaic()} para usar en gráficos \texttt{ggplot2}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggmosaic)}
\NormalTok{accidentes2020\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_mosaic}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}
    \AttributeTok{x =} \FunctionTok{product}\NormalTok{(tipo\_accidente, sexo),}
    \AttributeTok{fill =}\NormalTok{ sexo}
\NormalTok{  ))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/mosaico-accidentes-1} \end{center}

En cualquier caso, se pueden representar más variables creando ``subgráficos'' o
facetas (\emph{facets}). Basta con añadir una capa al gráfico \texttt{ggplot2}
con la función \texttt{facet\_wrap} y argumento \texttt{facets} una lista de variables
(categóricas o discretas) para cuyos valores queremos hacer un gráfico
distinto.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{accidentes2020\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(sexo, }\AttributeTok{fill =}\NormalTok{ estado\_meteorológico)) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(tipo\_accidente)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=1\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/acc-3vbles-1} \end{center}

\hypertarget{variables-cuantitativas-1}{%
\subsection{Variables cuantitativas}\label{variables-cuantitativas-1}}

La descripción conjunta de variables numéricas se
puede resumir con el vector de medias (media de cada variable) y la
matriz de varianzas-covarianzas. La covarianza es una medida
del grado de dependencia lineal entre dos variables numéricas.
Si la covarianza es cero, no hay relación lineal (pero podría
ser otro tipo de relación, recuérdese el cuarteto de Anscombe).
Pero la covarianza es una medida que depende mucho de la escala
de las variables, y
es más fácil interpretar el coeficiente de correlación lineal (función \texttt{cor()}), que
está acotado entre -1 y 1. Cuanto más se acerque a 1 en valor absoluto,
más fuerte será la dependencia lineal.

Además, el cálculo de la matriz de correlación puede suponer un punto de partida
en las técnicas de reducción de la dimensionalidad, como en el análisis de componentes principales (ACP). Si se desea
calcular la matriz de correlación del conjunto de datos \texttt{TIC2021},
que presenta las estadísticas de uso de las TIC en la Unión Europea 2021,
para llevar a cabo posteriormente un ACP (ver Capítulo XX) se puede utilizar el paquete \texttt{corrplot}, que proporciona una forma elegante y versátil de representar la matriz de correlaciones\footnote{Una gran cantidad de ejemplos puede verse ejecutado \texttt{example(corrplot)}.}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(corrplot)}
\NormalTok{mcor\_tic }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(TIC2021)}
\FunctionTok{corrplot.mixed}\NormalTok{(mcor\_tic, }\AttributeTok{order =} \StringTok{"AOE"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/correlacion-1} \end{center}

La matriz de correlaciones se puede representar mediante ``mapas de calor''
(\emph{heatmap}), es decir, un cuadrado que representa las filas y columnas de
la matriz de correlaciones (variables) y donde el color de las celdas es
una gradación que depende del valor de las celdas. Un mapa de calor de la matriz de correlaciones guardada anteriormente, \texttt{mcor\_tic}.
puede obtenerse con la expresión \texttt{heatmap(mcor\_tic)}.

En cuanto a los resúmenes gráficos, el \textbf{diagrama de dispersion} es el gráfico más adecuado. La función \texttt{geom\_point()} de \texttt{ggplot2} añade una capa con los puntos (x, y), que ya nos da una idea de la relación entre las variables, y permite interpretar
conjuntamente con el coeficiente de correlación. Se puede añadir una línea de regresión, incluida una banda de confianza, por diversos métodos (función \texttt{geom\_smooth()} por defecto, una curva \emph{loess} o \emph{gam} dependiendo del número de filas).
Alternativamente a los puntos como objeto geométrico, se pueden representar líneas (\texttt{geom\_line()}).

Por ejemplo, antes de llevar un ajuste lineal o de otro tipo con los datos \texttt{airquality} tal y como se hará en los capítulos XX, se podría hacer un AED
previo entre las variables \texttt{Ozone} y \texttt{Temp} con el gráfico de la Figura \ref{fig:scatter-ozono}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airquality }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(Ozone, Temp) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Temp, }\AttributeTok{y =}\NormalTok{ Ozone)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/scatter-ozono-1} 

}

\caption{Gráfico de dispersión del Ozono frente a la Temperatura}\label{fig:scatter-ozono}
\end{figure}

Un caso particular es cuando la variable explicativa es el tiempo. En este caso estaremos hablando de series temporales, y la representación con líneas es más adecuada (véase la Figura \ref{fig:plot-nox-time}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contam\_mad }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(nom\_abv }\SpecialCharTok{==} \StringTok{"NOx"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(fecha, nom\_mag) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{media\_estaciones =} \FunctionTok{mean}\NormalTok{(daily\_mean, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fecha, }\AttributeTok{y =}\NormalTok{ media\_estaciones)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ nom\_mag)) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{linewidth =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{color =} \StringTok{"black"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/TablaTiposDatos} 

}

\caption{Concentración media semanal de NOx en las estaciones de medición de Madrid (enero 2011- marzo 2022)}\label{fig:plot-nox-time}
\end{figure}

\hypertarget{variables-cualitativas-y-cuantitativas}{%
\subsection{Variables cualitativas y cuantitativas}\label{variables-cualitativas-y-cuantitativas}}

Cuando se trabaja en un proyecto de ciencia de datos, lo normal es tener conjuntamente variables cualitativas y cuantitativas.
Para representar conjuntamente ambos tipos de variables
existen múltiples posibilidades, algunas
de las cuales se enumeran a continuación, con el tipo
de gráfico adecuado:

\begin{itemize}
\tightlist
\item
  Una variable numérica y una variable categórica: Gráficos de cajas o de violín
  para cada posible valor de la categórica (Figura \ref{fig:plot-violin-zonas}), o bien gráficos de densidad para cada categoría (Figura \ref{fig:plot-ridges-tipos}).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contam\_mad }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{na.omit}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(nom\_abv }\SpecialCharTok{==} \StringTok{"PM10"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{between}\NormalTok{(fecha, }\AttributeTok{left =} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"2022{-}03{-}10"}\NormalTok{), }\AttributeTok{right =} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"2022{-}03{-}20"}\NormalTok{))) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(zona, daily\_mean)) }\SpecialCharTok{+}
  \FunctionTok{geom\_violin}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{height =} \DecValTok{0}\NormalTok{, }\AttributeTok{width =} \FloatTok{0.01}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ zona, }\AttributeTok{y =}\NormalTok{ daily\_mean, }\AttributeTok{fill =}\NormalTok{ zona)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/plot-violin-zonas} 

}

\caption{Comparación de los niveles de PM10 en las Zonas de la ciudad de Madrid a efectos de Calidad del Aire durante la Calima de marzo de 2022}\label{fig:plot-violin-zonas}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggridges)}
\NormalTok{contam\_mad }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(nom\_abv }\SpecialCharTok{==} \StringTok{"NOx"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ daily\_mean, }\AttributeTok{y =}\NormalTok{ tipo, }\AttributeTok{fill =}\NormalTok{ tipo)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density\_ridges}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/plot-ridges-tipos} 

}

\caption{Comparación de densidades de NOx por tipo de estación de medición}\label{fig:plot-ridges-tipos}
\end{figure}

\begin{itemize}
\tightlist
\item
  Dos variables numéricas y una variable categórica: Gráfico de dispersión
  de las numéricas, y mapeado del color por la categórica.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# periodo del estado de alarma}
\NormalTok{pm10\_nox\_mad }\OtherTok{\textless{}{-}}\NormalTok{ contam\_mad }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{na.omit}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(nom\_abv }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"PM10"}\NormalTok{, }\StringTok{"NOx"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{between}\NormalTok{(fecha, }\AttributeTok{left =} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"2020{-}03{-}14"}\NormalTok{), }\AttributeTok{right =} \FunctionTok{as.Date}\NormalTok{(}\StringTok{"2020{-}06{-}30"}\NormalTok{))) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(estaciones, zona, tipo, nom\_abv, daily\_mean, fecha) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =} \StringTok{"nom\_abv"}\NormalTok{, }\AttributeTok{values\_from =} \StringTok{"daily\_mean"}\NormalTok{, }\AttributeTok{values\_fn =}\NormalTok{ mean)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pm10\_nox\_mad }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ PM10, }\AttributeTok{y =}\NormalTok{ NOx, }\AttributeTok{colour =}\NormalTok{ tipo, }\AttributeTok{size =}\NormalTok{ zona)}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/plot-dispersion-conjunta-alarma} 

}

\caption{Grafíco de dipersión entre las variables, `NOx`, PM110, `zona` y `tipo` durante el estado de alarma en la ciudad de Madrid}\label{fig:plot-dispersion-conjunta-alarma}
\end{figure}

\begin{itemize}
\tightlist
\item
  Más de dos variables numéricas y más de una categórica: gráfico de dispersión
  y mapeado de las otras variables a otros \emph{aesthetics}. Combinación de geometrías
  y \emph{aesthetics} (por ejemplo, añadir puntos con efecto \emph{jitter} a un gráfico de cajas).
  La función \texttt{ggpairs} puede incluir también variables categóricas y mapearlas
  por ejemplo al color, consiguiendo una visualización muy rica.
\end{itemize}

En todos los casos anteriores, se pueden crear ``facetas'' para hacer un gráfico
por cada combinación de variables categóricas, de forma que tengamos un buen número de
variables representadas en un mismo ``lienzo'', como en la Figura \ref{fig:plot-dispersion-ms-conjunta-alarma}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pm10\_nox\_mad }\SpecialCharTok{|\textgreater{}}
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{drop\_na}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ NOx, }\AttributeTok{x =}\NormalTok{ PM10, }\AttributeTok{colour =}\NormalTok{ tipo, }\AttributeTok{shape =}\NormalTok{ zona)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(estaciones))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/plot-dispersion-ms-conjunta-alarma} 

}

\caption{Grafíco de dipersión entre las variables, `NOx`, PM110, `zona` y `tipo` por estación de monitoreo durante el estado de alarma en la ciudad de Madrid}\label{fig:plot-dispersion-ms-conjunta-alarma}
\end{figure}

\begin{infobox_resume}

\textbf{RESUMEN}

\emph{Análisis exploratorio de una característica}

\begin{itemize}
\tightlist
\item
  El análisis exploratorio es una tarea fundamental antes de abordar cualquier otra técnica estadística.
\item
  Las variables categóricas se resumen con tablas de frecuencias y gráficos de barras.
\item
  Las variables discretas se pueden resumir también con tablas de frecuencias y gráficos de barras, pero si hay muchos valores distintos también pueden ser apropiados los histogramas.
\item
  Las variables numéricas se pueden resumir con tablas de frecuencias por intervalos, medidas de posición y de dispersión, histogramas y gráficos de cajas.
\item
  Los gráficos de cajas sirven además para identificar valores atípicos.
\item
  Los gráficos secuenciales pueden proporcionar información sobre tendencias y otros patrones en los datos.
\end{itemize}

\emph{Análisis exploratorio de varias características}

\begin{itemize}
\tightlist
\item
  Las variables cualitativas se pueden resumir con tablas de frecuencias conjuntas y su
  representación gráfica, y con
  combinaciones de gráficos de barras.
\item
  La principal medida conjunta de dos variables numéricas es el coeficiente de correlación. Para más de dos variables se suelen representar en forma de matriz.
\item
  El gráfico de dispersión es la representación básica para dos variables numéricas. Se pueden representar estos gráficos por pares en forma de matriz de gráficos.
\item
  Para añadir más variables, se pueden ``mapear'' variables a \emph{aesthetics} (tamaño, color, etc.), añadiendo más objetos geométricos, o bien añadiendo ``facetas'' (subgráficos) para cada variable o para cada posible valor de un variable cualitativa.
\end{itemize}

\end{infobox_resume}

\hypertarget{chap-feature}{%
\chapter{Feature selection and engineering}\label{chap-feature}}

\textbf{Jorge Velasco López}

\hypertarget{introducciuxf3n-5}{%
\section{Introducción}\label{introducciuxf3n-5}}

Como se indicaba en el capítulo anterior, la preparación de datos en un contexto de un poryecto de modelado predictivo, consiste en la transformación de datos sin procesar en una forma más adecuada para el modelado. Esta preparación puede ser un proceso minuciosamente laborioso e incluye tareas como la integración y limpieza de datos, como se vio en el capítulo anterior. En este capítulo se verán el resto de tareas de preprocesamiento:

\begin{itemize}
\tightlist
\item
  Selección de variables (\emph{feature selection}): identificar aquellas variables de entrada que sean más relevantes para las fases posteriores del proceso de modelado.
\item
  Transformaciones de la escala o distribución de la variable objetivo.
\item
  Transformación de variables (\emph{feature engineering}): derivación de nuevas variables a partir de los datos disponibles.
\item
  Reducción de dimensionalidad: Creación de proyecciones compactas de los datos.
\item
  Otro tipo de tratamientos, como el \emph{split} de datos o el manejo de datos no balanceados.
\end{itemize}

La creación de variables predictoras a partir de los datos en bruto tiene una componente creativa. Requiere además de herramientas adecuadas y de experiencia para encontrar las mejores representaciones, apoyándose en lo posible en el conocimiento que se tenga de los datos durante la exploración de los mismos.
Para seleccionar y configurar la preparación de datos, pueden usarse \textbf{estadísticas descriptivas}, para determinar si las operaciones de escalado pueden ser apropiadas. Las pruebas de hipótesis estadísticas se pueden utilizar para determinar si una variable coincide con una distribución de probabilidad dada. Se pueden usar diagramas para determinar si las variables están relacionadas y si lo están, en qué medida, proporcionando información sobre si una o más variables son redundantes o irrelevantes a la variable objetivo. Tambiés es posible apoyarse en visualizaciones, como \textbf{gráficas de los datos} para identificar si una variable tiene valores atípicos y proporcionar información sobre la distribución de probabilidad que subyace a los datos y así poder decidir si una transformación de la distribución de probabilidad de una variable sería apropiada.

La fase posterior de modelización, también puede informar sobre la selección y configuración de los métodos de preparación de datos. Por ejemplo, la elección de \textbf{algoritmos} puede imponer requisitos y expectativas sobre el tipo y forma de variables de entrada en los datos \citep{boehmke2019hands}. Así, podría ser necesario que las variables tengan una distribución de probabilidad específica, la eliminación de variables de entrada correlacionadas y/o la eliminación de variables que no estén fuertemente relacionadas con la variable objetivo.

Incluso preparando los datos para cumplir con las expectativas del modelo, es posible que no se obtenga todo su potencial, dado que hay representaciones de datos que son mejores que otras. El \emph{feature engineering} ayudará a aumentar la eficacia de un modelo.
Nótese que, para un conjunto determinado de datos y un problema concreto de modelado predictivo de clasificación o regresión, puede ser la primera vez que se realice una modelización y por tanto, sea necesario trabajar los datos de manera innovadora. Tanto es así, que se suele decir que se invierte hasta el 80\% del tiempo de análisis de datos en el proceso de preparación de datos. \index{Paquete Caret}

El paquete \textbf{\texttt{caret}} \citep{kuhn2008building} proporciona una interfaz unificada que simplifica el proceso de modelado empleando la mayoría de los métodos de aprendizaje estadístico implementados en \textbf{R}. Se puede decir que caret es un \emph{meta-engine} (agregador) que permite aplicar casi cualquier algoritmo y se ha elegido como herramienta principal para la parte de preprocesamiento por su amplia difusión y por coherencia en la parte de \emph{machine learning} del presente libro, que también lo utiliza. No obstante, se podrían usar otros, como el paquete \texttt{recipes} incluido en \texttt{tidymodels}. Los \emph{meta-engines} y \texttt{caret} en particular brindan más coherencia en la forma en que especifican las entradas y se extraen las salidas, pero pueden ser menos flexibles que los algoritmos directos. En relación al preprocesamiento de los datos, además de una serie de funciones auxiliares, como \texttt{dummyVars()} y \texttt{rfe()}, incluye la función principal \texttt{preProcess()}, que ya se introdujo en el capítulo anterior y para la que el parámetro \texttt{method} permite establecer una lista de procesamientos:

\begin{itemize}
\tightlist
\item
  \textbf{Imputación}: \texttt{knnImpute}, \texttt{bagImpute} o \texttt{medianImpute}, para imputar valores missing como se vio en el capítulo anterior.
\item
  \textbf{Creación y transformación de variables explicativas}: \texttt{center} (resta la media de los valores), \texttt{scale} (divide los valores por la desviación estándar), \texttt{range} (normaliza los valores), \texttt{BoxCox} (aplica una transformada de Box-Cox siendo los valores positivos), \texttt{YeoJohnson} (aplica una transformada Yeo-Johnson), \texttt{expoTrans} (aplica una transformación de potencia), \texttt{spatialSign} (transforma los datos a un círculo unitario de \texttt{p} dimensiones).
\item
  \textbf{Selección de predictores y extracción de componentes}: \texttt{corr} (correlación), \texttt{nzv} (eliminar atributos con una varianza cercana a cero), \texttt{zv} (eliminar atributos con varianza cero), \texttt{pca} (transformar datos a los componentes principales), \texttt{ica} (transformar datos a los componentes independientes).
\end{itemize}

Es más probable que estas transformaciones sean útiles para algoritmos como el de regresión, métodos basados en instancias (también llamado \emph{memory-based learning} como \texttt{K} vecinos más cercanos-KNN- y aprendizaje de cuantificación vectorial-LVQ-), máquinas de vectores de soporte-SVM- y redes neuronales-NN- y menos probable que sean útiles para métodos basados en árboles y reglas.
En \texttt{caret}, estas transformaciones pueden ser utilizadas de dos maneras:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Independiente}: las transformaciones se pueden modelar a partir de datos de entrenamiento y aplicarse a múltiples conjuntos de datos. El modelo de la transformación se prepara utilizando la función \texttt{preProcess()} y se aplica a un conjunto de datos utilizando la función \texttt{predict()}.
\item
  \textbf{Entrenamiento}: las transformaciones se pueden preparar y aplicar automáticamente durante la evaluación del modelo. Las transformaciones aplicadas durante el entrenamiento se preparan usando \texttt{preProcess()} y se pasan a la función \texttt{train()} a través del argumento \texttt{preProcess}. En esta sección se presentan varios ejemplos de preprocesamiento de datos usando ambos métodos.
\end{enumerate}

Se usará el dataset de \texttt{Madrid\_Sale} (disponibles en el paquete de \textbf{R} \texttt{Idealista18}), con datos inmobiliaros del año 2018 y los datos de la tienda de comercio electrónico ``Beauty eSheep'' que se usa en los capítulos de \emph{machine learning} y que se introdujo en el capítulo anterior.
Las librerías que se usan, además de las menciondas de \texttt{tidyverse}, \texttt{idealista18} y \texttt{caret}, son \texttt{FSelector} (para la selección de atributos mediante métodos \emph{embedded}), \texttt{rsample} (para realizar muestreo), \texttt{gridExtra} (para visualizar gráficos) y \texttt{corrplot} (para visualizar correlaciones).

\hypertarget{feature-selection-selecciuxf3n-de-variables}{%
\section{\texorpdfstring{\emph{Feature Selection} (Selección de variables)}{Feature Selection (Selección de variables)}}\label{feature-selection-selecciuxf3n-de-variables}}

\index{feature selection} \index{selección de variables}

La selección de variables es el conjunto de técnicas para seleccionar el subconjunto de variables de entrada que sea más relevante para la modelización de la predicción de variable objetivo. Esto es importante porque: (i) variables redundantes de entrada pueden distraer o engañar a los algoritmos de aprendizaje, lo que posiblemente signifique un menor rendimiento predictivo; (ii) es deseable desarrollar modelos utilizando únicamente los datos que se requieren para hacer una predicción, tanto por el coste computacional como por la interpretabilidad del modelo utilizado.

Las técnicas de selección generalmente pueden agruparse en las que usan la variable de destino (supervisados) y los que no (no supervisados). Debido a la complejidad de la cuestión, se va a revisar únicamente las técnias supervisadas más relevantes, que se pueden dividir en:

\begin{itemize}
\item
  los de tipo filtro, que puntúan cada variable de entrada y permiten seleccionar un subconjunto \citep{brownlee2020data};
\item
  los métodos \emph{wrapper}, que eligen las variables que dan como resultado el modelo de mejor rendimiento y
\item
  los modelos intrínsecos o \emph{embedded}, que seleccionan variables automáticamente como parte del ajuste del modelo durante el entrenamiento (como algunos modelos de regresión penalizados como \texttt{Lasso} y árboles de decisión y \emph{random forests}).
\end{itemize}

La selección de variables también está relacionada con las técnicas de reducción de dimensionalidad, ya que en ambos métodos se busca reducir el número de variables de entrada para un modelo predictivo. La diferencia es que la reducción de la dimensionalidad crea una proyección de los datos que dan como resultado variables de entrada completamente nuevas. Así, la reducción de la dimensionalidad es una alternativa a la selección de variables.

\hypertarget{muxe9todos-de-selecciuxf3n-tipo-filtro}{%
\subsection{Métodos de selección tipo Filtro}\label{muxe9todos-de-selecciuxf3n-tipo-filtro}}

\index{selección!tipo filtro}

Los \textbf{métodos de selección de variables de filtro} usan técnicas estadísticas para evaluar la relación entre cada variable de entrada (también llamadas predictoras) y la variable de destino (también llamada objetivo o de salida). Las puntuaciones obtenidas se utilizan como base para clasificar y elegir las variables de entrada que se utilizarán en el modelo.

La elección de las técnicas estadísticas depende de los tipos de datos de las variables. Las medidas estadísticas utilizadas en la selección de variables basadas en filtros generalmente calculan una variable de entrada a la vez con la variable de destino. Como tales, se les conoce como medidas estadísticas univariantes.

En función de si la entrada/salida es numérica/categórica, se estaría tratando de un problema de modelado predictivo o de clasificación y las técnicas de selección de filtro serían diferentes. Por ejemplo, si las variables de entrada y salida fueran numéricas, sería un problema de modelado predictivo, para las que se suelen usar técnicas de coeficientes de correlación, como el de \emph{Pearson} para una correlación lineal o métodos basados en rangos para una correlación no lineal (por ejemplo con el Coeficiente de rango de \emph{Spearman}) y el método de Información Mutua. Si ambas fueran categóricas podría usarse la medida de correlación de chi-cuadrado.
Sin embargo, no es habitual tener un conjunto de datos con solo un tipo de variable de entrada. Un enfoque para manejar diferentes tipos de datos de variables de entrada es seleccionar por separado variables de entrada numéricas y variables de entrada categóricas usando las métricas apropiadas.

Uno de los aspectos fundamentales en la selección de variables es comprobar si su varianza es cero o cercana a cero. Si la varianza es cercana a cero, significa que casi todas las observaciones tienen valores similares y por tanto, esas variables podrían ser descartadas, puesto que es muy probable que solo añadan ruido al modelo. Comprobar con \texttt{caret} si las observaciones tienen \textbf{varianza} cero se hace con la función \texttt{nearZeroVar()}. A continuación se muestra su uso con un ejemplo del dataset \texttt{Madrid\_Sale}, incluido en el paquete de \textbf{R} \texttt{Idealista18}.

En el dataset se tienen varias variables numéricas (como las correspondientes al número de baños, metros cuadrados, precio, latitud, longitud, si tiene terraza, etc.). Para comprobar si tienen o no varianza cero se puede ejecutar el siguiente código, que devuelve entre otras la variable \texttt{nzv=FALSE} (\emph{near zero variance}) para casi todas las variables, excepto \texttt{PARKINGSPACEPRICE,\ ISDUPLEX,\ ISSTUDIO,\ ISINTOPFLOOR}y\texttt{BUILTTYPEID\_1} que podrían descartarse para el modelo:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{numeric\_cols }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(Madrid\_Sale, is.numeric)}
\NormalTok{variance }\OtherTok{\textless{}{-}} \FunctionTok{nearZeroVar}\NormalTok{(Madrid\_Sale[numeric\_cols], }\AttributeTok{saveMetrics =}\NormalTok{ T)}
\CommentTok{\# Con el argumento saveMetrics, se guardan los valores que se han utilizado para los cálculos.}
\CommentTok{\# Se muestran los primeros resultados}
\FunctionTok{head}\NormalTok{(variance, }\AttributeTok{n =} \DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{}       freqRatio  percentUnique   zeroVar   nzv}
\CommentTok{\#\textgreater{} PERIOD  2.019617   0.004218742   FALSE   FALSE}
\CommentTok{\#\textgreater{} PRICE   1.076923   2.911986500   FALSE   FALSE}
\end{Highlighting}
\end{Shaded}

Otra de las cuestiones importantes, es la \textbf{correlación entre variables}. Existen varios modelos, como la regresión lineal y la regresión logística para las que una de las bases del modelo es la no colinealidad o multicolinealidad, por lo que no deberían haber variables correlacionadas. \index{correlación}

Para comprobarla, se puede usar la función \texttt{findCorrelation()} de \texttt{caret}a la que se le pasa una matriz de correlaciones, con lo que se obtiene qué variables habría que eliminar en caso necesario. Con el paquete de \texttt{corrplot} se genera la visualización de la Fig. \ref{fig:corr}:

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/corridealista} 

}

\caption{Correlación}\label{fig:corr}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{madrid\_cor }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(Madrid\_Sale[numeric\_cols])}
\FunctionTok{findCorrelation}\NormalTok{(madrid\_cor)}
\CommentTok{\# Se visualiza}
\NormalTok{correlationMatrix }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(madrid\_cor[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{])}
\FunctionTok{corrplot}\NormalTok{(correlationMatrix, }\AttributeTok{method =} \StringTok{"circle"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

En este caso, no habría variables altamente correlacionadas, aunque sí habría que analizar principalmente la relación entre \texttt{CONSTRUCTEDAREA} y \texttt{BATHNUMBER.} Si las hubiera, habría que considerar eliminar determinadas variables. De manera similar, habría que controlar que no haya combinaciones lineales, para lo que se puede usar la función \texttt{findLinearCombos()}.

La colinealidad ocurre cuando los predictores del modelo están relacionados de tal manera que constituyen una combinación lineal entre sí. Aunque no empeoraría el poder predictivo del modelo, sí dificultaría la evaluación de la importancia de los predictores individuales.

Las variables perfectamente correlacionadas son redundantes y es posible que no agreguen valor al modelo y, si se eliminan, serían más rápidas de entrenar. Sin embargo, dos variables que no están fuertemente correlacionadas aún podrían ser importantes para el modelo. En caso de duda, lo más seguro sería entrenar el modelo y observar su rendimiento.

\hypertarget{muxe9todos-de-selecciuxf3n-de-variables-tipo-wrapper}{%
\subsection{\texorpdfstring{Métodos de selección de variables tipo \emph{wrapper}}{Métodos de selección de variables tipo wrapper}}\label{muxe9todos-de-selecciuxf3n-de-variables-tipo-wrapper}}

Otro enfoque consiste en utilizar un método contenedor o \emph{wrapper} \citep{saeys2007review}, que realiza una búsqueda a través de diferentes combinaciones o subconjuntos de variables de entrada para comprobar el efecto que tienen en la precisión del modelo.
\index{selección!tipo wrapper}

Hay varias alternativas:

\begin{itemize}
\tightlist
\item
  Evaluar las variables individualmente y seleccionar las \(n\) principales variables que obtienen una buena precisión. Sin embargo, al probar el modelo repetidamente con solo una de las variables a la vez, se pierde la información de las dependencias entre variables.
\item
  Observar la precisión del modelo para todas las combinaciones de variables posibles. En este sentido, se puede utilizar un algoritmo de búsqueda global estocástica, como los algoritmos genéticos. Aunque efectivos, estos enfoques pueden ser computacionalmente muy costosos, especialmente para grandes conjuntos de datos de entrenamiento y modelos más sofisticados.
\item
  La selección \emph{forward} es un algoritmo simple en el que comienza con una sola variable y agrega secuencialmente la siguiente variable, actualizando el modelo y observando su rendimiento.
\item
  La selección \emph{backward} es el enfoque inverso de la selección \emph{forward}. Se comienza con todas las variables y luego se eliminan iterativamente la variable de menor rendimiento, mientras se actualiza el modelo. Esto ayuda a comprender si la última variable clasificada era realmente útil.
\item
  Un método automático popular para la selección de variables proporcionado por el paquete \texttt{caret}se llama ``Eliminación de variables recursivas'' o RFE.\index{Método RFE}
\end{itemize}

El siguiente ejemplo ilustra la última de las alternativas, el \textbf{método RFE}, en el conjunto de las variables numéricas del \texttt{data.frame} de \texttt{Madrid\_Sale}. Se utiliza un algoritmo \emph{Random Forest} en cada iteración para evaluar el modelo. El algoritmo está configurado para explorar todos los subconjuntos posibles de los atributos seleccionados. En la Fig. \ref{fig:rfe}, se puede comprobar que en 3 atributos se estabiliza el RMSE.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Se toma una muestra con el paquete rsample y se seleccionan las variables de interés}
\NormalTok{Madrid\_Sale\_num }\OtherTok{\textless{}{-}}\NormalTok{ Madrid\_Sale[, }\DecValTok{3}\SpecialCharTok{:}\DecValTok{9}\NormalTok{] }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{UNITPRICE)}
\NormalTok{Madrid\_Sale\_num\_sample }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(Madrid\_Sale\_num), }\AttributeTok{size =} \DecValTok{5000}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{Madrid\_Sale\_num\_sample }\OtherTok{\textless{}{-}}\NormalTok{ Madrid\_Sale\_num[Madrid\_Sale\_num\_sample, ]}
\CommentTok{\# Para asegurar que los resultados sean repetibles}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\CommentTok{\# Definir el control usando una función de selección de random forest}
\NormalTok{control }\OtherTok{\textless{}{-}} \FunctionTok{rfeControl}\NormalTok{(}\AttributeTok{functions =}\NormalTok{ rfFuncs, }\AttributeTok{method =} \StringTok{"cv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{10}\NormalTok{)}
\CommentTok{\# Ejecutar el algoritmo RFE sobre el dataset de Sacramento de solo variables numéricas}
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{rfe}\NormalTok{(Madrid\_Sale\_num\_sample[, }\DecValTok{2}\SpecialCharTok{:}\DecValTok{6}\NormalTok{], Madrid\_Sale\_num\_sample[, }\DecValTok{1}\NormalTok{], }\AttributeTok{sizes =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{), }\AttributeTok{rfeControl =}\NormalTok{ control)}
\CommentTok{\# sumarizar los resultados}
\FunctionTok{print}\NormalTok{(results)}
\CommentTok{\# listar las variables resultantes}
\FunctionTok{predictors}\NormalTok{(results)}
\CommentTok{\# Graficar los resultados}
\FunctionTok{plot}\NormalTok{(results, }\AttributeTok{type =} \FunctionTok{c}\NormalTok{(}\StringTok{"g"}\NormalTok{, }\StringTok{"o"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/rfeidealista} 

}

\caption{Selección RFE}\label{fig:rfe}
\end{figure}

\hypertarget{muxe9todos-de-selecciuxf3n-tipo-embedded}{%
\subsection{\texorpdfstring{Métodos de selección tipo Embedded \index{selección!tipo embedded}}{Métodos de selección tipo Embedded }}\label{muxe9todos-de-selecciuxf3n-tipo-embedded}}

Finalmente, hay algunos algoritmos de aprendizaje automático que realizan la selección automática de funciones como parte del aprendizaje del modelo. Se podría aludir a estas técnicas como \textbf{métodos de selección de variables intrínsecas (o \emph{embedded})}. Esto incluye algoritmos como modelos de regresión penalizados como Lasso y árboles de decisión, incluidos los \emph{random forests}.

El siguiente ejemplo carga el conjunto de datos de \texttt{Madrid\_Sale} con una selección de variables numéricas (\texttt{PRICE,\ CONSTRUCTEDAREA,\ ROOMNUMBER,\ BATHNUMBER,\ HASTERRACE,\ HASLIFT}) y construye un modelo de aprendizaje de cuantificación vectorial o LVQ, \citep{kohonen1995learning}. Luego, se usa la función \texttt{varImp()} para estimar la importancia de la variable, que se muestra en consola y se representa gráficamente. En la Fig. \ref{fig:embeddedidealista} se muestra que entre los atributos analizados, los de \texttt{CONSTRUCTEDAREA,\ BATHNUMBER\ y\ ROOMNUMBER} son los 3 más importantes del conjunto de datos para todas las categorías y \texttt{HASTERRACE} es el menos importante, para la variable objetivo \texttt{PRICE}, transformada como se explicará en los siguientes apartados mediante \emph{binning}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Para asegurar que los resultados sean repetibles}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\CommentTok{\# Preparar el esquema de entrenamiento en este caso "repeated CV"}
\NormalTok{control }\OtherTok{\textless{}{-}} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{10}\NormalTok{, }\AttributeTok{repeats =} \DecValTok{3}\NormalTok{)}
\CommentTok{\# Realizar el *Binning* con un número determinado de bins}
\NormalTok{df3 }\OtherTok{\textless{}{-}}\NormalTok{ Madrid\_Sale\_num\_sample }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{price\_bin =} \FunctionTok{cut}\NormalTok{(PRICE, }\AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{250000}\NormalTok{, }\DecValTok{500000}\NormalTok{, }\DecValTok{750000}\NormalTok{, }\DecValTok{10000000}\NormalTok{), }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"0k\_250k"}\NormalTok{, }\StringTok{"250k\_500k"}\NormalTok{, }\StringTok{"500k\_750k"}\NormalTok{, }\StringTok{"\textgreater{}750k"}\NormalTok{), }\AttributeTok{include.lowest =} \ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{PRICE)}
\CommentTok{\# Entrenar el modelo:}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(price\_bin }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ df3, }\AttributeTok{method =} \StringTok{"lvq"}\NormalTok{, }\AttributeTok{preProcess =} \StringTok{"scale"}\NormalTok{, }\AttributeTok{trControl =}\NormalTok{ control)}
\NormalTok{importancia }\OtherTok{\textless{}{-}} \FunctionTok{varImp}\NormalTok{(model, }\AttributeTok{scale =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# Estimar la importancia de la variable}
\FunctionTok{plot}\NormalTok{(importancia) }\CommentTok{\# Graficar la importancia}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/embeddedidealista} 

}

\caption{Método Embedded}\label{fig:embeddedidealista}
\end{figure}

\hypertarget{transformaciones-de-escala-y-de-la-distribuciuxf3n-de-la-variable-objetivo}{%
\section{Transformaciones de escala y de la distribución de la variable objetivo}\label{transformaciones-de-escala-y-de-la-distribuciuxf3n-de-la-variable-objetivo}}

\hypertarget{31}{%
\subsection{Transformaciones de la variable objetivo}\label{31}}

Aunque no siempre es necesario, la transformación de la variable objetivo puede llevar a una mejora predictiva, especialmente con modelos paramétricos. Por ejemplo, los modelos de regresión lineal ordinarios asumen que los errores de predicción (y por lo tanto la respuesta) se distribuyen normalmente. Pero puede ocurrir que la variable objetivo tenga valores atípicos y la suposición de normalidad no se cumpla.

Para minimizar la asimetría de la variable respuesta, se puede usar una transformación log (u otra). Esta sería una alternativa al uso de la función de pérdida del ``error logarítmico cuadrático medio'' (RMSLE) como medida de evaluación del modelo.

Existen dos enfoques principales para ayudar a corregir las variables objetivo con sesgo positivo:

\begin{itemize}
\tightlist
\item
  Normalizar con una transformación log. Esto transformará la mayoría de las distribuciones sesgadas a la derecha para que sean aproximadamente normales. Sin embargo, hay que considerar el preprocesamiento como la creación de un flujo de trabajo que pueda revisarse y volver a usarse. Para esto, se puede usar el paquete \texttt{recipes} o la función \texttt{caret::preProcess()} \footnote{Si la variable objetivo tiene valores negativos o ceros, una transformación logarítmica producirá \(NaN\) e \(- Inf\), respectivamente. Si los valores de respuesta no positivos son pequeños (por ejemplo, entre \(-0.99\) y \(0\)), se puede aplicar una pequeña compensación como con la función \texttt{log1p()} que agrega 1 al valor antes de aplicar una transformación de registro. Si los datos consisten en valores \(\geq\) \(-1\), se debe usar la transformación de Yeo-Johnson. Por tanto, si se usa una transformación log o Box-Cox, no se deben centrar los datos primero ni realizar ninguna operación que pueda hacer que los datos no sean positivos.}. En la Fig. \ref{fig:log}, se puede ver que tomar logaritmos da buen resultado para normalizar esta distribución.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{respuesta\_log }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(Madrid\_Sale}\SpecialCharTok{$}\NormalTok{PRICE)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/logidealista} 

}

\caption{Variable respuesta log}\label{fig:log}
\end{figure}

\begin{itemize}
\tightlist
\item
  Como segunda opción se puede usar una transformación de Box-Cox, que es más flexible que la transformación logarítmica y se puede encontrar la función adecuada a partir de una familia de transformadas de potencia, que transformarán la variable a lo mas parecido a una distribución normal \citep{sakia1992box} \footnote{En el centro de la transformación de Box-Cox hay un exponente, lambda (\(\lambda\)), que varía de \(-5\) a \(5\). Se consideran todos los valores de \(\lambda\) y se estima el valor óptimo para los datos dados a partir de los datos de entrenamiento. El ``valor óptimo'' es el que resulta en la mejor transformación a una distribución normal aproximada.}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{respuesta\_boxcox }\OtherTok{\textless{}{-}} \FunctionTok{preProcess}\NormalTok{(Madrid\_Sale\_num\_sample, }\AttributeTok{method =} \StringTok{"BoxCox"}\NormalTok{)}
\NormalTok{trainBC }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(respuesta\_boxcox, Madrid\_Sale\_num\_sample)}
\NormalTok{respuesta\_boxcox}
\CommentTok{\#\textgreater{} Created from 5000 samples and 2 variables}
\CommentTok{\#\textgreater{}}
\CommentTok{\#\textgreater{} Pre{-}processing:}
\CommentTok{\#\textgreater{} {-} Box{-}Cox transformation (2)}
\CommentTok{\#\textgreater{} {-} ignored (0)}
\CommentTok{\#\textgreater{}}
\CommentTok{\#\textgreater{} Lambda estimates for Box{-}Cox transformation:}
\CommentTok{\#\textgreater{} {-}0.3, {-}0.3}
\end{Highlighting}
\end{Shaded}

Hay que tener en cuenta que cuando se modela con una variable objetivo transformada, las predicciones también estarán en la escala transformada. Es posible que haya que deshacer (o volver a transformar) los valores pronosticados a su escala original para que los responsables de la toma de decisiones puedan interpretar los resultados más fácilmente.

\begin{infobox}
\textbf{NOTA}

El paquete \texttt{recipes}, incluido en \texttt{tidymodels}, permite desarrollar el modelo de transformación de variables de forma secuencial. La idea detrás del paquete es similar a \texttt{caret::preProcess()}, donde se quiere crear el modelo de preprocesamiento pero aplicarlo más tarde y dentro de cada remuestreo. Hay tres pasos principales para crear y aplicar la transformación de variables con recipes, \emph{recipe} (donde se definen los pasos para crear el plan), \emph{prep} (donde se estiman los parámetros en función de los datos de entrenamiento) y \emph{bake} (donde se aplica el modelo a nuevos datos). Sin embargo, a diferencia de \texttt{caret}, no manejan automáticamente las variables categóricas y se requiere crear variables ficticias manualmente.

\end{infobox}

\hypertarget{escalado-de-datos}{%
\subsection{Escalado de datos}\label{escalado-de-datos}}

También hay que considerar la escala en la que se miden las variables individuales.
Los modelos que incorporan funciones lineales en las variables de entrada, son sensibles a la escala de esas variables. Muchos algoritmos usan estas funciones, algunas de manera más obvia (como Modelos Lineales Generalizados-GLM- y regresión regularizada) que otras (como redes neuronales-NN-, SVM y análisis de componentes principales). Otros ejemplos incluyen algoritmos que utilizan medidas de distancia, como la distancia euclidia (p.~ej., KNN, agrupación de k-medias y agrupación jerárquica).
Para estos modelos, a menudo es aconsejable estandarizar las variables de entrada. Las funciones de estandarización incluyen el centrado y la escala para que las variables numéricas tengan una media cero y una varianza unitaria, lo que proporciona una unidad de medida comparable común en todas las variables.

La función \texttt{preProcess()} de \texttt{caret} acepta dos tipos principales: \texttt{center}, que resta el promedio a los valores (todos tendrán promedio 0); \texttt{scale}, que divide los valores entre la desviación estándar (todos tendrán desviación típica 1) y \texttt{range}, que normaliza los datos, haciendo que éstos tengan un rango de 0 a 1.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{preprocess }\OtherTok{\textless{}{-}} \FunctionTok{preProcess}\NormalTok{(Madrid\_Sale\_num, }\AttributeTok{method =} \StringTok{"center"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(}\FunctionTok{predict}\NormalTok{(preprocess, Madrid\_Sale\_num)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{], }\AttributeTok{n =} \DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{}       PRICE    CONSTRUCTEDAREA     ROOMNUMBER}
\CommentTok{\#\textgreater{} 1 {-}270110.11       {-}54.39716       {-}1.5808996}
\CommentTok{\#\textgreater{} 2 {-}161110.11       {-}47.39716       {-}1.5808996}
\CommentTok{\#\textgreater{} 3  {-}23110.11       {-}26.39716       {-}0.5808996}
\end{Highlighting}
\end{Shaded}

Con esta función y para este caso, \texttt{caret} centra los datos de 6 variables, correspondientes a las variables numéricas seleccionadas. La función \texttt{preProcess()} no está pensada para transformar los datos en el momento, sino para hacer la transformación en el proceso de entrenamiento o de predicción. Para ello, en este caso se pasa el preprocesamiento y los datos a la función \texttt{predict()} devolviendo caret todos los datos con el procesamiento ya aplicado (en este caso, habiendo restado la media).
La combinación de las transformaciones de scale y center puede servir para estandarizar los datos. Los atributos tendrán un valor medio de 0 y una desviación estándar de 1 denominándose estandarización \textbf{z-score}. Para ello, se usaría la sentencia \texttt{method=c("center",\ "scale")}. Otra estandarización ampliamente usada es la \textbf{min-max}.

Cuando se estandariza, hay que hacerlo tanto en los datos de entrenamiento (\emph{train}) como en los de prueba (\emph{test}), para que se basen en la misma media y varianza. En todo caso hay que
tener en cuenta que nunca deben de usarse los datos que se han apartado para el conjunto de \emph{test} en el proceso de estandarización.

\hypertarget{feature-engineering}{%
\section{\texorpdfstring{\emph{Feature engineering}}{Feature engineering}}\label{feature-engineering}}

\index{feature engineering}

La mayoría de los modelos requieren que los predictores tomen forma numérica, con alguna excepción. Por ejemplo, los modelos basados en árboles manejan de manera natural variables numéricas o categóricas. Pero incluso estos modelos pueden mejorarse con el uso del preprocesamiento de variables categóricas. Las siguientes secciones se centran en algunos de los enfoques más comunes para diseñar variables categóricas.

Estos métodos generan nuevas variables, lo que aumenta la precisión del modelo y las predicciones generales. Son de dos tipos, de Agrupamiento o \emph{Binning}, donde se crean agrupaciones (o \emph{bins}) para variables continuas y de Codificación, donde las variables numéricas se forman a partir de variables categóricas.

\hypertarget{binning}{%
\subsection{\texorpdfstring{\emph{Binning} \index{binning}}{Binning }}\label{binning}}

\emph{Binning} es el agrupamiento que se realiza para crear agrupamientos de variables continuas convirtiéndolas en categóricas. Hay dos tipos de agrupamiento, 1) no supervisado (agrupamiento automático o manual) y 2) supervisado, que implica la creación de \emph{bins} para la variable continua mientras se tiene en cuenta también la variable objetivo.
Sin embargo, el \emph{Binning} debe usarse con moderación, ya que puede haber una pérdida en el rendimiento del modelo \citep{kuhn2013applied}. En el ejemplo que se mostraba en apartados anteriores, se hacía \emph{Binning} sobre la variable de \texttt{PRICE} para emplear un método de selección de variables tipo \emph{embedded}. A continuación, se muestra una forma alternativa de hacer el \emph{bin}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df2 }\OtherTok{\textless{}{-}}\NormalTok{ Madrid\_Sale\_num\_sample }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{price\_bin =} \FunctionTok{cut}\NormalTok{(PRICE, }\AttributeTok{breaks =} \DecValTok{4}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{PRICE)}
\FunctionTok{head}\NormalTok{(df2, }\AttributeTok{n =} \DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{}       CONSTRUCTEDAREA ROOMNUMBER BATHNUMBER HASTERRACE HASLIFT}
\CommentTok{\#\textgreater{} 72912              80          3          1          1       0}
\CommentTok{\#\textgreater{} 64401             121          3          2          0       1}
\CommentTok{\#\textgreater{}                 price\_bin}
\CommentTok{\#\textgreater{} 72912 (2.69e+04,2.06e+06]}
\CommentTok{\#\textgreater{} 64401 (2.69e+04,2.06e+06]}
\end{Highlighting}
\end{Shaded}

\hypertarget{codificaciuxf3n}{%
\subsection{\texorpdfstring{Codificación \index{codificación}}{Codificación }}\label{codificaciuxf3n}}

Muchos modelos requieren que todas las variables predictoras sean numéricas. En consecuencia, se puede necesitar transformar las variables categóricas en representaciones numéricas para que estos algoritmos puedan procesarse. La codificación es el proceso en el que se crean variables numéricas a partir de variables categóricas. Es de dos tipos, 1) \textbf{Codificación de etiquetas} (implica asignar a cada etiqueta un entero o valor único según el orden alfabético, que es la codificación más popular y ampliamente utilizada) y 2) \textbf{Codificación \emph{One-hot}} (implica la creación de variables adicionales o \emph{dummies} sobre la base de valores únicos para cada categoría de la variable categórica). Esta última se suele usar cuando se agrupan categorías que ocurren con poca frecuencia y en general, muchos algoritmos trabajan sobre ellas de manera eficiente.
Para crear variables \emph{dummies} con \texttt{caret}, se puede usar la función \texttt{dummyVars()} y aplicar un \texttt{predict()} para obtener los datos resultantes. La estrategia que implementa es crear una columna para cada valor distinto que exista en la variable que se está codificando y, para cada registro, marca con un \(1\) la columna a la que pertenezca dicho registro y deja las demás con \(0\).
Para el dataset de la tienda de comercio electrónico ``Beauty eSheep'', se haría lo siguiente.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Se obtiene el modelo de *one hot encoding*}
\CommentTok{\# se aplica el modelo para obtener un data.frame con las variables ya modificadas}
\CommentTok{\# se debe hacer lo mismo con los de test.}
\NormalTok{dp\_dummies }\OtherTok{\textless{}{-}} \FunctionTok{dummyVars}\NormalTok{(}\StringTok{" \textasciitilde{} ."}\NormalTok{, }\AttributeTok{data =}\NormalTok{ df\_pre\_dummies)}
\NormalTok{df\_pos\_dummies }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(dp\_dummies, }\AttributeTok{newdata =}\NormalTok{ dp\_entr)}
\end{Highlighting}
\end{Shaded}

Adicionalmente, en este mismo ejemplo, se crea un \texttt{data.frame} uniendo estas variables \texttt{df\_pos\_dummies} con el resto de variables del modelo:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Se obtiene la tabla con el resto de variables que no se han transformado: las variables que son numéricas y la clase objetivo}
\NormalTok{df\_pos\_resto }\OtherTok{\textless{}{-}}\NormalTok{ dp\_entr }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{starts\_with}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"des\_"}\NormalTok{, }\StringTok{"ind\_"}\NormalTok{)))}
\CommentTok{\# Se obtiene así el data.frame todas las variables numéricas: las columnas de indicadores, las columnas dummies y el resto de columnas no transformadas (la clase objetivo queda al final siendo un factor):}
\NormalTok{dp\_entr\_num }\OtherTok{\textless{}{-}}\NormalTok{ df\_pos\_ind }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{bind\_cols}\NormalTok{(df\_pos\_dummies, df\_pos\_resto)}
\end{Highlighting}
\end{Shaded}

\hypertarget{reducciuxf3n-de-dimensionalidad}{%
\section{Reducción de dimensionalidad}\label{reducciuxf3n-de-dimensionalidad}}

La reducción de dimensionalidad es un enfoque alternativo para filtrar las variables no informativas sin eliminarlas mediante el \emph{feature selection}, que generalmente se usa para variables numéricas. Como se explica en el capítulo de \textbf{\emph{ref(ACP)}}, el espacio de un conjunto de variables podría reducirse proyectándolo a un subespacio de variables de menor dimensión utilizando componentes principales.

Para aplicar un análisis de componentes principales (PCA por sus siglas en inglés) en \textbf{R} con \texttt{caret}, se indica el valor \texttt{pca} al parámetro method de la función \texttt{preProcess()}. Asimismo, con el parámetro \texttt{thresh} se puede indicar el porcentaje de la variabilidad deseado. Igualmente, se podrían combinar métodos, por ejemplo haciendo \texttt{method=c("center",\ "scale",\ "pca")}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pre\_pca }\OtherTok{\textless{}{-}} \FunctionTok{preProcess}\NormalTok{(Madrid\_Sale\_num, }\AttributeTok{method =} \StringTok{"pca"}\NormalTok{, }\AttributeTok{thresh =} \FloatTok{0.8}\NormalTok{)}
\FunctionTok{predict}\NormalTok{(pre\_pca, Madrid\_Sale\_num)}
\end{Highlighting}
\end{Shaded}

\hypertarget{otras-transformaciones}{%
\section{Otras transformaciones}\label{otras-transformaciones}}

\hypertarget{particionado-de-datos}{%
\subsection{\texorpdfstring{Particionado de datos \index{particionado de datos}}{Particionado de datos }}\label{particionado-de-datos}}

Un objetivo principal del proceso de aprendizaje automático es encontrar el algoritmo que prediga con mayor precisión los valores futuros en función de un conjunto de variables. En otras palabras, un algoritmo que, no solo se ajuste bien a nuestros datos pasados sino, lo que es más importante, que prediga un resultado futuro con precisión. Esto se llama la generalización del algoritmo.

Para proporcionar una comprensión precisa de la generalización del modelo óptimo final, se pueden dividir los datos en conjuntos de datos de entrenamiento y prueba:

\begin{itemize}
\tightlist
\item
  \textbf{Conjunto de entrenamiento (\emph{train})}: estos datos se utilizan para desarrollar conjuntos de funciones, entrenar los algoritmos, ajustar hiperparámetros, comparar modelos y todas las demás actividades necesarias para elegir un modelo final.
\item
  \textbf{Conjunto de prueba (\emph{test})}: habiendo elegido un modelo final, estos datos se utilizan para estimar una evaluación imparcial del rendimiento del modelo, que se puede denominar'' error de generalización''.
\end{itemize}

Dada una cantidad fija de datos, las recomendaciones típicas para dividir los datos en divisiones de prueba de entrenamiento incluyen 60 \% (entrenamiento)--40 \% (\texttt{test}), 70 \%--30 \% u 80 \%--20 \%. Las dos formas más comunes de dividir los datos incluyen el muestreo aleatorio simple y el muestreo estratificado, que explican en detalle en capítulos dedicados de este libro.

\hypertarget{muestreo-aleatorio-simple}{%
\subsubsection{\texorpdfstring{Muestreo aleatorio simple \index{muestreo!aleatorio simple}}{Muestreo aleatorio simple }}\label{muestreo-aleatorio-simple}}

La forma más sencilla de dividir los datos en conjuntos de entrenamiento y \texttt{test} es tomar una muestra aleatoria simple. Esto no controla ningún atributo de datos, como la distribución de su variable objetivo (\(Y\)). Hay varias formas de dividir nuestros datos en \textbf{R}. Por ejemplo, con \texttt{caret} se haría lo siguiente para producir una división de 70 a 30 en los datos:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Usando el paquete caret}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{\# para permitir reproductirlo}
\NormalTok{index }\OtherTok{\textless{}{-}} \FunctionTok{createDataPartition}\NormalTok{(Madrid\_Sale\_num}\SpecialCharTok{$}\NormalTok{PRICE, }\AttributeTok{p =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{list =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{train }\OtherTok{\textless{}{-}}\NormalTok{ Madrid\_Sale\_num[index, ]}
\NormalTok{test }\OtherTok{\textless{}{-}}\NormalTok{ Madrid\_Sale\_num[}\SpecialCharTok{{-}}\NormalTok{index, ]}
\FunctionTok{dim}\NormalTok{(Madrid\_Sale\_num) }\CommentTok{\# 94815}
\FunctionTok{dim}\NormalTok{(train) }\CommentTok{\# 66373}
\FunctionTok{dim}\NormalTok{(test) }\CommentTok{\# 28442}
\end{Highlighting}
\end{Shaded}

\hypertarget{muestreo-estratificado}{%
\subsubsection{\texorpdfstring{Muestreo estratificado \index{muestreo!estratificado}}{Muestreo estratificado }}\label{muestreo-estratificado}}

Si se desea controlar el muestreo para que los conjuntos de entrenamiento y prueba tengan similares distribuciones, se puede usar muestreo estratificado. Esto es habitual en problemas de clasificación donde la variable objetivo puede estar notablemente desbalanceada (por ejemplo, 90\% de las observaciones con respuesta ``Sí'' y 10\% con respuesta ``No''). Sin embargo, también se puede aplicar el muestreo estratificado a problemas de regresión para conjuntos de datos que tienen un tamaño de muestra pequeño y donde la variable objetivo se desvía mucho de la normalidad.

La forma más sencilla de realizar un muestreo estratificado en una variable objetivo es usar el paquete \texttt{rsample}, donde se especifica la variable objetivo para estratificar. Lo siguiente ilustra que en nuestros datos originales la variable del precio con \emph{binning} (\texttt{price\_bin}) tiene una respuesta desbalanceada. Al aplicar el muestreo estratificado, tanto los conjuntos de entrenamiento como de prueba tienen distribuciones de respuesta aproximadamente iguales.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(df3}\SpecialCharTok{$}\NormalTok{price\_bin) }\SpecialCharTok{|\textgreater{}} \FunctionTok{prop.table}\NormalTok{()}
\CommentTok{\#         0k\_250k 250k\_500k 500k\_750k     \textgreater{}750k}
\CommentTok{\#         0.4776    0.2914    0.1132    0.1178}
\NormalTok{split\_strat }\OtherTok{\textless{}{-}} \FunctionTok{initial\_split}\NormalTok{(df3, }\AttributeTok{prop =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{strata =} \StringTok{"price\_bin"}\NormalTok{)}
\NormalTok{train\_strat }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(split\_strat)}
\NormalTok{test\_strat }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(split\_strat)}
\CommentTok{\# Tasa de respuesta consistente entre los datos train y test}
\FunctionTok{table}\NormalTok{(train\_strat}\SpecialCharTok{$}\NormalTok{price\_bin) }\SpecialCharTok{|\textgreater{}} \FunctionTok{prop.table}\NormalTok{()}
\CommentTok{\#   0k\_250k 250k\_500k 500k\_750k     \textgreater{}750k}
\CommentTok{\#   0.4777015 0.2913093 0.1132075 0.1177816}
\FunctionTok{table}\NormalTok{(test\_strat}\SpecialCharTok{$}\NormalTok{price\_bin) }\SpecialCharTok{|\textgreater{}} \FunctionTok{prop.table}\NormalTok{()}
\CommentTok{\#   0k\_250k 250k\_500k 500k\_750k     \textgreater{}750k}
\CommentTok{\#   0.4773635 0.2916112 0.1131824 0.1178429}
\end{Highlighting}
\end{Shaded}

Para el ejemplo que se presenta en este libro para el \emph{machine learning} de la tienda de comercio electrónico ``eSheep'' se obtenía, después de un proceso de integración descrito en el capítulo anterior, el \texttt{data.frame} \texttt{dp\_fac\_cliente\_venta}, a partir del cual se obtenía \texttt{dp\_entr} (datos de entrenamiento de los modelos en su escala original) y \texttt{dp\_test} (datos para prueba de los modelos en su escala original):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Se parte en entrenamiento y test usando muestreo estratificado por la variable objetivo}
\NormalTok{trainIndex }\OtherTok{\textless{}{-}} \FunctionTok{createDataPartition}\NormalTok{(dp\_fac\_cliente\_venta\_pro13[, dep], }\AttributeTok{p =}\NormalTok{ .}\DecValTok{8}\NormalTok{, }\AttributeTok{list =}\NormalTok{ F, }\AttributeTok{times =} \DecValTok{1}\NormalTok{)}
\NormalTok{dp\_entr }\OtherTok{\textless{}{-}}\NormalTok{ dp\_fac\_cliente\_venta\_pro13[trainIndex, ]}
\NormalTok{dp\_test }\OtherTok{\textless{}{-}}\NormalTok{ dp\_fac\_cliente\_venta\_pro13[}\SpecialCharTok{{-}}\NormalTok{trainIndex, ]}
\CommentTok{\# Una selección de variables numéricas da como resultado los data.frames dp\_entr\_num y dp\_test\_num, para los conjuntos de entrenamiento y test, respectivamente.}
\end{Highlighting}
\end{Shaded}

\hypertarget{tuxe9cnicas-para-manejar-datos-no-balanceados}{%
\subsection{\texorpdfstring{Técnicas para manejar datos no balanceados \index{datos!no balanceados}}{Técnicas para manejar datos no balanceados }}\label{tuxe9cnicas-para-manejar-datos-no-balanceados}}

Los datos utilizados en distintas áreas a menudo tienen menos del 1\% de eventos raros pero ``interesantes'' (por ejemplo, estafadores que usan tarjetas de crédito, usuarios que hacen clic en anuncios o servidores dañados que escanean su red). Sin embargo, la mayoría de los algoritmos de aprendizaje automático no funcionan bien con conjuntos de datos desbalanceados \citep{kuhn2013applied}. Hay varias técnicas para lidiar con esto de las que a continuación se introducen algunas:

\begin{itemize}
\tightlist
\item
  \textbf{\emph{Downsampling}} equilibra el conjunto de datos al reducir el tamaño de las clases abundantes para que coincidan con las frecuencias en la clase menos prevalente. Este método se utiliza cuando la cantidad de datos es suficiente. Manteniendo todas las muestras en la clase rara y seleccionando aleatoriamente un número igual de muestras en la clase abundante, se puede recuperar un nuevo conjunto de datos equilibrado para modelado adicional.
\item
  \textbf{\emph{Upsampling}} se usa cuando la cantidad de datos es insuficiente. Trata de equilibrar el conjunto de datos aumentando el tamaño de las muestras más raras. En lugar de deshacerse de las muestras abundantes, se generan nuevas muestras raras mediante la repetición o el \emph{bootstrapping}.
\item
  \textbf{Creación de datos sintéticos} \citep{chawla2002smote}: Esta técnica consiste en balancear el conjunto de entrenamiento generando nuevos registros sintéticos, esto es, inventados de la clase minoritaria. Existen diversos algoritmos que realizan esta tarea siendo uno de los más conocidos la técnica de SMOTE (\emph{Synthetic Minority Oversampling Technique}).
\item
  \textbf{Otras técnicas}, como que el algoritmo implemente mecanismos para dar mayor peso a los casos de la clase minoritaria, etc
\end{itemize}

Por ejemplo, en capítulos posteriores se modelará por ejemplo el algoritmo \textbf{RPART} (algoritmos de árboles de regresión y clasificación) con \emph{downsampling}, para obtener mejora en el rendimiento:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Se especifica que el modelo se entrene con downsampling}
\NormalTok{md\_conf\_model}\SpecialCharTok{$}\NormalTok{fitControl}\SpecialCharTok{$}\NormalTok{sampling }\OtherTok{\textless{}{-}} \StringTok{"down"}
\CommentTok{\#  Se le indica que haga un centrado a los datos}
\NormalTok{md\_conf\_model}\SpecialCharTok{$}\NormalTok{fitControl}\SpecialCharTok{$}\NormalTok{preProcess }\OtherTok{\textless{}{-}} \StringTok{"center"}
\end{Highlighting}
\end{Shaded}

Para todos los modelos de \emph{machine learning} de capítulos posteriores, se hace previamente una parametrización común en esta fase de preparación de los datos y se guarda en la lista \texttt{md\_conf\_model}. Esta lista incluye los elementos \texttt{dep}, \texttt{form}, \texttt{fitControl}, \texttt{positive\_class}, \texttt{metric} y \texttt{seed}, que se conforman así:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# nombre de la columna de la clase objetivo}
\NormalTok{dep }\OtherTok{\textless{}{-}} \FunctionTok{colnames}\NormalTok{(dp\_entr[}\FunctionTok{ncol}\NormalTok{(dp\_ENTR)])}
\CommentTok{\# metrica de validacion}
\NormalTok{metric }\OtherTok{\textless{}{-}} \StringTok{"ROC"}
\CommentTok{\# preparar cv}
\NormalTok{fitControl }\OtherTok{\textless{}{-}} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"cv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{10}\NormalTok{, }\AttributeTok{savePredictions =}\NormalTok{ T, }\AttributeTok{summaryFunction =}\NormalTok{ twoClassSummary, }\AttributeTok{classProbs =}\NormalTok{ T)}
\CommentTok{\# clase positiva el primer nivel del factor}
\NormalTok{positive\_class }\OtherTok{\textless{}{-}} \FunctionTok{levels}\NormalTok{(dp\_entr[, dep])[}\DecValTok{1}\NormalTok{]}
\CommentTok{\# fórmula con la clase objetivo en función del resto}
\NormalTok{form }\OtherTok{\textless{}{-}} \FunctionTok{formula}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(dep, }\StringTok{"\textasciitilde{}."}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Hay que considerar, que no existe una ventaja absoluta de un método de muestreo sobre otro. La aplicación de estos métodos depende del caso de uso al que se aplica y del conjunto de datos en sí. La función de \texttt{caret} para implementar estas técnicas está en \texttt{?caret::trainControl())}, como se ve en el ejemplo anterior.

\hypertarget{muxe9todos-de-remuestreo}{%
\subsection{\texorpdfstring{Métodos de remuestreo \index{remuestreo}}{Métodos de remuestreo }}\label{muxe9todos-de-remuestreo}}

En la sección anterior se indicaba que los datos deben dividirse en conjuntos de entrenamiento y prueba y que no debía usarse el conjunto de prueba para evaluar el rendimiento del modelo durante la fase de entrenamiento. Para evaluar el rendimiento del modelo, una opción sería evaluar una métrica de error basada en los datos de entrenamiento. Desafortunadamente, esto conduce a resultados sesgados, ya que algunos modelos pueden funcionar muy bien con los datos de entrenamiento pero no generalizarse bien a un nuevo conjunto de datos.

Un segundo método es utilizar un enfoque de validación, que implica dividir aún más el conjunto de entrenamiento para crear dos partes: un conjunto de entrenamiento y un conjunto de \textbf{validación.} Entonces se puede entrenar el modelo en el nuevo conjunto de entrenamiento y estimar el rendimiento en el conjunto de validación. Lamentablemente, la validación con un solo conjunto de reserva puede ser muy variable y poco confiable a menos que esté trabajando con conjuntos de datos muy grandes \citep{molinaro2005prediction}.

En el capítulo de ``Técnicas de Modelización estadística avanzadas'' se explicaban los métodos de remuestreo, que brindan un enfoque alternativo al permitir ajustar repetidamente un modelo a partir de los datos de entrenamiento y probar su rendimiento en otras partes. Los dos métodos de remuestreo más utilizados incluyen la validación cruzada de kfold y el \emph{bootstrapping}.

\hypertarget{validaciuxf3n-cruzada-k-fold-kfold-cross-validation}{%
\subsubsection{\texorpdfstring{Validación cruzada k-fold (\emph{kfold cross validation})}{Validación cruzada k-fold (kfold cross validation)}}\label{validaciuxf3n-cruzada-k-fold-kfold-cross-validation}}

\index{valiación cruzada}

La validación cruzada kfold (kfold CV) es un método de remuestreo que divide aleatoriamente los datos de entrenamiento en k grupos (\emph{folds}) de aproximadamente el mismo tamaño. El modelo se ajusta en k menos 1 grupos y el último se usa para calcular el rendimiento del modelo. Este procedimiento se repite k veces; cada vez, un grupo diferente se trata como el conjunto de validación. Este proceso da como resultado k estimaciones del error de generalización. Por lo tanto, la estimación del kfold CV se calcula promediando los errores de prueba k, lo que nos proporciona una aproximación del error de generalización que podríamos esperar en datos nuevos.

En consecuencia, con kfold CV, cada observación en los datos de entrenamiento se mantendrá una vez para ser incluida en el conjunto de prueba, como se ilustra en \ref{fig:26ch10}. En la práctica, normalmente se usa k=5 o k=10. No existe una regla formal en cuanto al tamaño de k; sin embargo, a medida que k aumenta, la diferencia entre el rendimiento estimado y el rendimiento real que se verá en el conjunto de prueba disminuirá. Por otro lado, el uso de k demasiado grande puede introducir cargas computacionales. Además, en \citep{molinaro2005prediction} encontraron que k=10 funcionaba de manera similar a la validación cruzada dejando uno fuera (LOOCV), que es el enfoque más extremo (es decir, establecer k=n).

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/kfold} 

}

\caption{Remuestro k fold cv}\label{fig:26ch10}
\end{figure}

Aunque el uso de \(k\geq 10\) ayuda a minimizar la variabilidad en el rendimiento estimado, el kfold CV tiende a tener una mayor variabilidad que el \emph{bootstrapping} (que se analiza a continuación). \citep{kim2009estimating} demostró que repetir kfold CV puede ayudar a aumentar la precisión del error de generalización estimado.

Para el caso de ``Beauty eSheep'', cuando se entrena el algoritmo se le pasa el parámetro de \texttt{trControl} entre los que está \texttt{fitControl}, que como se veía anteriormente está parametrizado para realizar un kfold cv de 10 \emph{folds}.

\hypertarget{bootstrapping}{%
\subsubsection{\texorpdfstring{\emph{Bootstrapping}}{Bootstrapping}}\label{bootstrapping}}

\index{bootstrapping}

Es una muestra aleatoria de los datos tomados con reemplazo \citep{efron1986bootstrap}. Esto significa que, después de seleccionar un dato para incluirlo en el subconjunto, aún está disponible para una selección posterior. Una muestra bootstrap tiene el mismo tamaño que el conjunto de datos original a partir del cual se construyó. La Fig. \ref{fig:28ch10} proporciona un esquema de muestreo bootstrap donde cada muestra contiene 12 observaciones al igual que en el conjunto de datos original. Además, el muestreo bootstrap contendrá aproximadamente la misma distribución de valores (representados por colores) que el conjunto de datos original.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/bootstrap} 

}

\caption{Remuestro bootstrap}\label{fig:28ch10}
\end{figure}

Dado que las observaciones se replican en \emph{bootstrapping}, tiende a haber menor variabilidad en la medida del error en comparación con kfold CV. Sin embargo, también puede aumentar el sesgo de su estimación de error. Esto puede ser un problema con conjuntos de datos más pequeños. Sin embargo, para la mayoría de los conjuntos de datos de tipo medio o grande (por ejemplo, \(n \geq 1000\)), no ocasiona problemas. Se pueden crear muestras \emph{bootstrap} fácilmente con \texttt{rsample::bootstraps()}, como se ilustra en el fragmento de código a continuación.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{bootstraps}\NormalTok{(df3, }\AttributeTok{times =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Para series temporales, para incorporar el origen móvil y otros procedimientos de remuestreo de series temporales \citep{hyndman2018forecasting} es el recurso más usado, centrado en \textbf{R}.

\hypertarget{ajuste-de-hiperparuxe1metros}{%
\subsection{Ajuste de hiperparámetros}\label{ajuste-de-hiperparuxe1metros}}

\index{hiperparámetros}

Aunque no se trata de transformación de preprocesamiento, sino ya de la fase de modelización, se introduce aquí el concepto de hiperparámetros (también conocidos como parámetros de ajuste), que sirven para para controlar la complejidad de los algoritmos de aprendizaje automático y, por tanto, la compensación entre sesgo y varianza. No todos los algoritmos tienen hiperparámetros (por ejemplo, mínimos cuadrados ordinarios-MCO-). Sin embargo, la mayoría tiene al menos uno. La configuración adecuada de estos hiperparámetros a menudo depende de los datos y el problema en cuestión y no siempre se puede ajustar solo con los datos de entrenamiento. En consecuencia, se requiere un método para identificar la configuración óptima.

Supongamos que los valores de \(k\) más pequeños (por ejemplo 2, 5 o 10) conducen a una varianza alta (pero un sesgo más bajo) y los valores más grandes (por ejemplo, 150) conducen a un sesgo alto (pero una varianza más baja). Entonces, el valor óptimo de k podría ser entre 20 y 50, pero hay que preguntarse por el valor óptimo. Una forma de realizar el ajuste de hiperparámetros es probar los hiperparámetros manualmente hasta que se encuentre la mejor combinación de valores de hiperparámetros que resulten en la mayor precisión predictiva (medida usando k fold CV, por ejemplo), aunque puede ser un trabajo muy tedioso dependiendo de la cantidad de hiperparámetros. Un enfoque alternativo es realizar una búsqueda en cuadrícula, que es un enfoque automatizado para buscar en muchas combinaciones de valores de hiperparámetros.

\hypertarget{evaluaciuxf3n-de-modelos}{%
\subsection{Evaluación de modelos}\label{evaluaciuxf3n-de-modelos}}

De manera similar al punto anterior, aunque no se trata de transformaciones de la fase de preprocesamiento, se introduce aquí el concepto de evaluación de modelos. Tradicionalmente, el rendimiento de los modelos estadísticos se basaba en gran medida en pruebas de bondad de ajuste y evaluación de residuos. Desafortunadamente, pueden derivarse conclusiones equívocas de los modelos predictivos que pasan este tipo de evaluaciones.

En la actualidad, para analizar el rendimiento del modelo, se suele evaluar la precisión predictiva a través de funciones de pérdida (\emph{loss functions}). Las funciones de pérdida son métricas que comparan los valores predichos con el valor real.
Al realizar métodos de remuestreo, se evalúan los valores pronosticados para un conjunto de validación en comparación con el valor objetivo real. Por ejemplo, en la regresión, una forma de medir el error es tomar la diferencia entre el valor real y el predicho para una observación determinada (esta es la definición habitual de residuo en la regresión lineal ordinaria). El error de validación general del modelo se calcula agregando los errores en todo el conjunto de datos de validación.
Es importante considerar el contexto del problema al identificar la métrica de rendimiento a usar. Además, al comparar varios modelos, hay que hacerlo con la misma métrica. Para los \textbf{modelos de predicción} caben destacar las siguientes métricas, todos con objetivo de minimizar menos el \(R^2\) que es de maximizar:
\index{MSE} \index{RSME} \index{Curva ROC} \index{AUC} \index{Gini} \index{entropía}

\begin{itemize}
\tightlist
\item
  El error cuadrático medio (\textbf{MSE}) es el promedio del error cuadrático, siendo ésta (junto a la siguiente) la métrica de error más común utilizada. Siendo \(x\) e \(y\), \(D\) vectores dimensionales, y \(x_i\) el valor de la \(i\) dimensión de \(x\), \(MSE=\sum_{i=1}^{D}(x_i-y_i)^2\)
\item
  \textbf{RMSE}: Raíz del error cuadrático medio. La raíz se toma para que su error esté en las mismas unidades que su variable de respuesta. \(RMSE=\sqrt{MSE}\)
\item
  \textbf{Desviación}: Abreviatura de desviación residual media. En esencia, proporciona un grado en el que un modelo explica la variación en un conjunto de datos cuando se utiliza la estimación de máxima verosimilitud.
\item
  \textbf{MAE}: Error absoluto medio. Similar a MSE pero en lugar de elevar al cuadrado, solo toma la diferencia media absoluta entre los valores reales y predichos. \(MAE=\sum_{i=1}^{D}|x_i-y_i|\)
\item
  \textbf{RMSLE}: Raíz del error logarítmico cuadrático medio. Similar a RMSE pero realiza un log() en los valores reales y predichos antes de calcular la diferencia. \(RMSLE=\sqrt{(log(x_i+1)-log(y_i+1))^2)}\).
\item
  \textbf{\(R^2\)}: Esta es una métrica popular que representa la proporción de la varianza en la variable dependiente que es predecible a partir de la(s) variable(s) independiente(s).
\end{itemize}

Para el caso de la \textbf{clasificación} destacan las siguientes medidas, todos con objetivo de minimizar:

\begin{itemize}
\tightlist
\item
  \textbf{Clasificación errónea}: tasa de clasificación incorrecta en porcentaje.
\item
  \textbf{Error medio por clase}: tasa de error promedio para cada clase.
\item
  \textbf{MSE}: Error cuadrático medio. Es la distancia de 1.0 a la probabilidad sugerida.
\item
  \textbf{Entropía cruzada} (también conocida como pérdida de registro o desviación): similar a MSE pero incorpora un registro de la probabilidad predicha multiplicada por la clase real. En consecuencia, esta métrica penaliza las predicciones en las que se predice una pequeña probabilidad para la clase verdadera.
\item
  \textbf{Índice de Gini}: se utiliza principalmente con métodos basados en árboles y frecuentemente se conoce como una medida de purezax, donde un valor pequeño indica que un nodo contiene predominantemente observaciones de una sola clase.
\end{itemize}

Cuando se aplican modelos de clasificación, a menudo se usa una matriz de confusión para evaluar ciertas medidas de desempeño. Una matriz de confusión (figura \ref{fig:matrizconfusion}) es una matriz que compara niveles categóricos reales (o eventos) con los niveles categóricos predichos. Cuando se predice el nivel correcto, se dice que se ha poducido un \textbf{verdadero positivo} (\(tp\)). Sin embargo, si se predice un nivel o evento que no sucedió, esto se denomina \textbf{falso positivo} (\(fn\)). Por otro lado, cuando no se predijo un nivel o evento y sucede, se denomina \textbf{falso negativo} (\(fn\)) y si se predijo es un \textbf{verdadero negativo} (\(tn\)).

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/matrizconfusion} 

}

\caption{Matriz de confusión}\label{fig:matrizconfusion}
\end{figure}

A partir de éstos, se pueden extraer diferentes niveles de rendimiento para clasificadores binarios como:
\index{exactitud} \index{precisión} \index{sensibilidad} \index{expecificidad}
- Exactitud (\(\frac{tp+tn}{p+n}\)).
- Precisión (\(\frac{tp}{tp+fp}\)).
- Sensibilidad (\(\frac{tp}{p}\)).
- Especificidad (\(\frac{tn}{n}\)).
- \textbf{AUC}: Es el área bajo la curva (\emph{Area Under the Curve}). Un buen clasificador binario tendrá una alta precisión y sensibilidad, por lo que se trata de maximizarlo. Esto significa que el clasificador funciona bien cuando predice que un evento ocurrirá y no ocurrirá, lo que minimiza los falsos positivos y los falsos negativos. Para capturar este equilibrio, a menudo se usa una \textbf{curva ROC} (\emph{Receiver Operating Characteristic}) como la de la Fig. \ref{fig:roc2}.
La curva ROC traza la tasa de falsos positivos a lo largo del eje \(x\) y la tasa de verdaderos positivos a lo largo del eje \(y\). Una línea que es diagonal desde la esquina inferior izquierda hasta la esquina superior derecha representa una suposición aleatoria. Cuanto más alta esté la ROC en la esquina superior izquierda, mayor será el área AUC bajo esta curva.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/roc2} 

}

\caption{Curva ROC}\label{fig:roc2}
\end{figure}

\begin{infobox_resume}
\textbf{RESUMEN}

Se ha dado una visión de las principales transformaciones que se realizan en la fase de preprocesamiento del un proyecto de modelado predictivo, la selección de variables (\emph{feature selection}), las transformaciones de la escala o distribución de la variable objetivo, la transformación de variables (\emph{feature engineering}), la reducción de dimensionalidad y otro tipo de tratamientos.
La creación de variables predictoras a partir de los datos en bruto tiene una componente creativa, que requiere de herramientas adecuadas y de experiencia para
encontrar las mejores representaciones, apoyándose en lo posible en el conocimiento que se tenga de los datos.
Se ha utilizado fundamentalmente el paquete \texttt{caret} y \texttt{rsample} para realizar las tareas de procesamiento sobre dos conjuntos de datos, el de la tienda de ``Beauty eSheep'' y el de \texttt{Madrid\_Sale}.

\end{infobox_resume}

\hypertarget{part-fundamentos-de-estaduxedstica}{%
\part{Fundamentos de estadística}\label{part-fundamentos-de-estaduxedstica}}

\hypertarget{Funda-probab}{%
\chapter{Probabilidad}\label{Funda-probab}}

\emph{Mª Leticia Meseguer Santamaría} y \emph{Manuel Vargas Vargas}

\hypertarget{introducciuxf3n-a-la-probabilidad}{%
\section{Introducción a la probabilidad}\label{introducciuxf3n-a-la-probabilidad}}

La incertidumbre es inevitable en muchos campos científicos, producto de la imposibilidad de predeterminar el resultado de un fenómeno repetido bajo idénticas condiciones, el desconocimiento de todas las causas que pueden influir en él, o una información limitada sobre los condicionantes que rigen su comportamiento. De hecho, gran parte del \emph{``avance científico''} consiste en reducir o controlar el nivel de incertidumbre, bien mejorando el proceso de obtención e interpretación de datos, o estableciendo \emph{``modelos''} que \emph{``expliquen''} los resultados.

Producto de la incertidumbre, las decisiones que se toman (o la validez de los resultados que se obtienen) conllevan un \emph{``riesgo''}, que puede concretarse en enunciados equivocados, modelos con escaso poder predictivo o decisiones con resultados no deseados. Sin embargo, no se prescinde de tomar decisiones en ambientes de incertidumbre, sino que se intenta evaluar y minimizar los riesgos asociados.

Así pues, resulta importante poder \emph{``medir''} la incertidumbre, es decir, cuantificar su magnitud y establecer \emph{``reglas de medida''} que permitan su tratamiento, la estimación de riesgos, y ayuden a la toma de decisiones. La \textbf{teoría de la probabilidad} se puede entender como un ente que proporciona reglas de comportamiento que ayudan en los objetivos anteriores, siendo el campo de aplicación tan amplio que puede cubrir cualquier rama de las ciencias sociales, técnicas y naturales.

El concepto de probabilidad apareció en la antiguedad, asociado a los juegos de azar, y se ha ido refinando y formalizando a lo largo de la historia. Sin embargo, la mayoría de las definiciones tradicionales presentan limitaciones que impiden su uso riguroso en cualquier situación. Aún así, siguen estando en el subconsciente colectivo, de forma que se entienden expresiones como \emph{``es muy probable que llueva mañana''}, \emph{``es improbable que me toque la lotería de Navidad''}, o \emph{``es probable que se obtenga en breve una vacuna contra cierta enfermedad''} cuando responden a conceptualizaciones diferentes y, en muchos casos, vagas e imprecisas.

Aunque sigue habiendo debates filosóficos y epistemológicos sobre el concepto de probabilidad (véase, por ejemplo, \citet{Hajek2016}), su uso generalizado en muchos campos científicos está relacionado más con el desarrollo de su carácter de ``medida'' de la incertidumbre, con un tratamiento matemático que permite su aplicación práctica (véase, por ejemplo, \citet{Ross2012}, \citet{Morin2016} o \citet{Balakrishnanetal2019}). Es este enfoque el que se desarrolla sucintamente en este capítulo.

\hypertarget{probabilidad-elementos-buxe1sicos-definiciuxf3n-y-teoremas}{%
\section{Probabilidad: elementos básicos, definición y teoremas}\label{probabilidad-elementos-buxe1sicos-definiciuxf3n-y-teoremas}}

El desarrollo del concepto de probabilidad, entendido como medida de la incertidumbre sobre la ocurrencia de un evento, precisa de algunos requisitos previos que permitan una aplicación operativa.

En primer lugar, es necesario definir en qué situaciones se puede aplicar. Se entenderá por \textbf{experimento} cualquier acción u observación de la realidad que pueda repetirse varias veces en idénticas condiciones, dando lugar a resultados identificables y conocidos antes de ser realizado. Cuando, dadas las condiciones, se conoce qué resultado se producirá, se dice que el experimento es \textbf{determinista}; en caso contrario, si dadas las condiciones, no se puede saber cuál ocurrirá, el experimento se denomina \textbf{aleatorio} o \textbf{estocástico}. Así pues, sólo se puede hablar de \emph{``probabilidad''} sobre experimentos aleatorios.

Ahora, dado un experimento aleatorio (\(E\)) y el conjunto de posibles resultados (\(\Omega\)), se define una \textbf{probabilidad} como una medida del grado de creencia en la ocurrencia de cada posible resultado, denominado genéricamente \textbf{suceso} \(S \in \Omega\). Como se ve, la definición es muy amplia, por lo que precisa de algún requisito para evitar que una asignación concreta de grados de creencia produzca inconsistencias. Dicho requisito supone el cumplimiento de una estructura (matemática) concreta, que se adopta de forma axiomática. La más conocida, debida a Andrei Kolmogorov se puede formalizar como:

\begin{itemize}
\tightlist
\item
  \textbf{Axiomática de Kolmogorov}: Se considera un experimento aleatorio \(E\), el conjunto de posibles sucesos \(\Omega\), y una función real \(P\) que asigna a cada suceso un número real. Se dice que \(P\) es una medida de probabilidad si cumple:

  \begin{itemize}
  \item
    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \tightlist
    \item
      \(P(S)\geq 0 , \forall S \in \Omega\).
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \setcounter{enumi}{1}
    \tightlist
    \item
      \(P(\Omega)=1\)
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \setcounter{enumi}{2}
    \tightlist
    \item
      Dada una sucesión numerable de sucesos \(\left\{ S_i \right\}\) disjuntos dos a dos \(S_i \cap S_j = \emptyset\ \forall i,j\), la probabilidad del suceso unión es la suma de probabilidades,
      \(P \left( \underset {i}{\cup} S_i \right) = \underset{i}{\sum} P(S_i)\).
    \end{enumerate}
  \end{itemize}
\end{itemize}

Así, una probabilidad es una medida que cumple esta axiomática, asignando a suceso un valor real (entre 0 y 1) que expresa el grado de creencia en la ocurrencia de dicho suceso, entendiendo que 0 indica que se cree que no ocurre nunca y 1, que ocurre seguramente (ver, por ejemplo \citet{deFinetti2017} para su fundamentación).

Algunas consecuencias que se derivan de la axiomática de Kolmogorov de forma inmediata son:
- \(P(\emptyset)=0\)
- Denominando \(\bar S\) al suceso complementario, \(P(\bar S)=1-P(S)\)
- Dados dos sucesos cualesquiera, \(P(S_1 \cup S_2)=P(S_1)+P(S_2)-P(S_1 \cap S_2)\)

Sin embargo, esta definición es formal, en el sentido de que indica qué requisitos debe cumplir para evitar inconsistencias, pero no determina qué valor concreto de probabilidad asignar a cada suceso. Históricamente, se han propuesto varias concepciones para resolver este problema:

\begin{itemize}
\item
  Concepción \textbf{clásica} (o de Laplace): Dado un experimento aleatorio \(E\) con n posibles resultados elementales mutuamente excluyentes e igualmente verosímiles, la probabilidad de un suceso \(S_i\) es:
  \begin{equation}
  P(S_i)=\frac {\text{casos favorables en los que ocurre } S_i}{\text{casos posibles}}=\frac{n_i}{n}=f_i
  \end{equation}
  Por \emph{``igualmente verosímiles''} se entiende que ``no hay razón para afirmar que uno suceda más veces que otro'', conocido como \emph{``principio de razón insuficiente''}. Es fácilmente comprobable que esta regla cumple la axiomática de Kolmogorov, e interpreta la probabilidad como la \emph{``frecuencia''} de ocurrencia de cada suceso. A pesar de sus limitaciones (utiliza la equiprobabilidad de los sucesos elementales para definir las probabilidades y asume un conjunto finito de ellos), su fácil comprensión y utilidad en casos sencillos hace que esta regla sea muy utilizada (e incluso confundida con una ``definición'' de probabilidad).
\item
  Concepción \textbf{frecuentista}: Se consideran n repeticiones de un experimento aleatorio, manteniendo idénticas condiciones. Sea \(n_i\) el número de veces que se presenta el suceso \(S_i\), entonces se le asigna la probabilidad:
  \begin{equation}
  P(S_i)=\underset {n \to \infty}{lim} \frac {n_i}{n} 
  \end{equation}
  Esta concepción, extiende la versión clásica, identificando la probabilidad con la frecuencia relativa de cada resultado cuando el experimento se repite un gran número de veces. Su uso en la práctica se conoce como \textbf{la concepción frecuentista de la probabilidad}.
\end{itemize}

El siguiente paso es formalizar cómo influye la ocurrencia de un suceso sobre la probabilidad de que ocurran otros. Así, dado un suceso \(A\) con \(P(A)>0\), la probabilidad de que ocurra otro, \(B\), condicionado a que ha ocurrido \(A\), \(P(B/A)\), se calcula como:
\begin{equation}
P(B/A)= \frac {P(B \cap A)}{P(A)}
\label{eq:probcondi}
\end{equation}
es decir, la probabilidad de que ocurran simultáneamente ambos dividida entre \(P(A)\) para que \(P(\Omega / A)=1\).

Esta nueva medida se denomina \textbf{probabilidad condicionada}\footnote{Esta definición cumple la axiomática de Kolmogorov y es la forma de introducir ``información'' en la determinación de probabilidades. Su versión para distribuciones de probabilidad es muy utilizado en inferencia estadística.} y permite obtener resultados fundamentales para el cálculo de probabilidades:

\begin{itemize}
\item
  \textbf{Independencia de sucesos}: Dos sucesos, A y B, se dicen independientes si \(P(B/A)=P(A) \Rightarrow P(B \cap A)=P(B)P(A)\).
\item
  \textbf{Teorema de la probabilidad total}: Dado un conjunto de sucesos \(\left \{ A_i \right\}\) disjuntos y cuya unión sea \(\Omega\) (denominado \textbf{partición} de \(\Omega\)), la probabilidad de cualquier suceso es:
  \begin{equation}
  P(B)= \underset {i}{\sum} P(B/A_i)P(A_i)
  \label{eq:probtotal}
  \end{equation}
  Este teorema permite determinar la probabilidad de un suceso \(B\), que puede tener varias causas \(A_i\), mediante la suma de las probabilidades de que aparezca \(B\) condicionada a cada una de las causas ponderada por la probabilidad de cada causa.
\item
  \textbf{Teorema de Bayes}: Dada una partición de \(\Omega\), y un suceso B con \(P(B)>0\), la probabilidad condicionada de cada elemento de la partición a que ha ocurrido B es:
  \begin{equation}
  P(A_i/B)={P(A_i\cap B) \over P(B)} = {P(B/A_i)P(A_i) \over \underset {j}{\sum} P(B/A_j)P(A_j)}
   \end{equation}
  Este teorema, aplicación directa de la definición de probabilidad condicionada y del teorema de la probabilidad total, es un resultado tan importante que su uso ha dado nombre a una rama entera de la estadística, la conocida como \textbf{estadística bayesiana}.\footnote{\(P(A_i)\) reciben el nombre de \textbf{probabilidades a priori}, \(P(B/A_i)\) el de \textbf{verosimilitudes}, y \(P(A_i/B)\), \textbf{probabilidades a posteriori}. El teorema establece cómo se modifican las probabilidades cuando se introduce información en forma de verosimilitudes, siendo muy utilizada su versión para distribuciones de probabilidad.}
\end{itemize}

\hypertarget{variable-aleatoria-y-su-distribuciuxf3n-tipos-de-variables-aleatorias}{%
\section{Variable aleatoria y su distribución: tipos de variables aleatorias}\label{variable-aleatoria-y-su-distribuciuxf3n-tipos-de-variables-aleatorias}}

Una limitación operativa de la probabilidad tal como se ha utilizado hasta ahora es que hace referencia a sucesos y operaciones entre conjuntos, lo que dificulta su tratamiento. Sin embargo, en muchos casos los sucesos están caracterizados por valores numéricos, por lo que podrían ser utilizados en sustitución de los primeros para facilitar los cálculos. A esta idea corresponde la noción de \textbf{variable aleatoria} (v.a.) que es una función que asigna un valor numérico a cada suceso de un experimento aleatorio. Para trabajar con probabilidades sobre números, a cada uno se le asigna la probabilidad de los sucesos que están caracterizados por dicho valor. \footnote{Matemáticamente, una variable aleatoria es una función \(X: \Omega \longrightarrow \mathbb R\) que, para cada valor real, cumple \(X^{-1}(x) \in \Omega\), de forma que se pueda asignar \(P(x)=P \left ( X^{-1}(x) \right )\).}

Dada una v.a. \(X\), su \textbf{función de distribución} asigna a cada número real \(x\) la probabilidad de que la variable tome un valor menor o igual que \(x\), \(F_X (x)= P(X \leq x)\).

Una variable de dice \textbf{discreta} si sólo puede tomar un conjunto finito (o infinito numerable) valores con probabilidad positiva. A ese conjunto de valores y sus probabilidades \(\left\{ x_i \right\} , \ P(X=x_i)\) se le denomina \textbf{función de cuantía}.

Una variable se denomina \textbf{continua}, si su función de distribución es continua, existe la primera derivada y es continua. Como consecuencia, la probabilidad en un valor concreto siempre será cero, \(P(X=x_i)=0\), por lo que sólo habrá probabilidades positivas sobre intervalos. Se denomina \textbf{función de densidad} a la derivada de la función de distribución
\(f(x)=F´(x)=\frac{dF(x)}{dx}\).\footnote{La función de densidad es el equivalente continuo de la función de cuantía, indicando, de forma intuitiva, dónde se ``concentra'' la probabilidad de observar valores de \(X\). De hecho, no es raro que se utilize el término general de ``densidad'' independientemente del tipo de variable que sea.}

Ya que una v.a. \(X\) está caracterizada por su distribución (a través de la función de distribución o por la de cuantía-densidad), se han desarrollado \textbf{modelos de distribución de probabilidad} que permiten modelizar el comportamiento aleatorio de las variables y calcular probabilidades de forma fácil.

\hypertarget{modelos-de-distribuciuxf3n-de-probabilidad}{%
\section{Modelos de distribución de probabilidad}\label{modelos-de-distribuciuxf3n-de-probabilidad}}

Se van a presentar los modelos más habituales en la práctica (para una visión completa, se puede consultar \citet{JohnsonKempKotz2008}). En este punto se presentan algunos modelos de distribuciones de probabilidad según la v.a. sea discreta o continua.

\hypertarget{modelos-discretos}{%
\subsection{Modelos discretos}\label{modelos-discretos}}

Los modelos de distribución discretos más habituales son el Binomial, el Binomial Negativo y el de Poisson. Los dos primeros se asientan sobre el \textbf{fenómeno de Bernoulli}, que, de manera general, consiste en n repeticiones independientes de un experimento dicotómico, es decir, que tiene sólo dos posibles resultados: uno identificado con el \emph{``éxito''} del experimento, cuya probabildad se denota por \(p\), y el otro, con el \emph{``fracaso''}, con probabilidad \(q=(1-p)\).

\begin{itemize}
\tightlist
\item
  \textbf{Distribución Binomial B(n;p)}
\end{itemize}

La distribución binomial es una distribución de probabilidad discreta que mide el número de éxitos en una secuencia de \emph{n ensayos} independientes de Bernoulli con una probabilidad fija de éxito \(p\). Puede tomar los valores \(x=0,1,...,n\) y su función de cuantía es:
\begin{equation}
P(X=x)=\binom{n}{x}p^xq^{n-x} \equiv \frac {n!}{x!(n-x)!} p^xq^{n-x}
\end{equation}
Su media es \(E(X)=\mu=np\) y su varianza, \(Var(X)=\sigma^2=npq\)

La representación gráfica de las funciones de cuantía y distribución se muestra en la fig.~\ref{fig:binomd}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/binomd-1} 

}

\caption{Función de cuantía y de distribución para una variable B(10,0.5)}\label{fig:binomd}
\end{figure}

En el \textbf{caso particular} de que n=1, B(1;p), se denomina \textbf{Distribución Bernouilli, B(p)}.

\begin{itemize}
\tightlist
\item
  \textbf{Distribución Binomial Negativa o de Pascal BN(r,p)}
\end{itemize}

La distribución Binomial Negativa es una serie de ensayos de Bernoulli independientes, con probabilidad constante de éxito \(p\), donde la v.a. \(X\) denota el número de ensayos fracasados (\(x\)) hasta que se produce un número determinado de éxitos (\(r\)). Puede tomar los valores \(x=0,1,...\) y su función de cuantía es:
\begin{equation}
P(X=x)=\binom{x+r-1}{x}q^xp^r \equiv \frac {(x+r-1)!}{(r-1)!x!}q^xp^r
\end{equation}
con media: \(E(X)=r\frac{q}{p}\) y varianza \(Var(X)=r\frac{q}{p^2}\).

La representación gráfica de las funciones de cuantía y distribución se muestra en la fig.~\ref{fig:nbin}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/nbin-1} 

}

\caption{Función de cuantía y de distribución para una variable BN(3,0.35)}\label{fig:nbin}
\end{figure}

En el \textbf{caso particular} de que r=1, BN(1;p), se está ante una \textbf{Distribución Geométrica G(p)}.

\begin{itemize}
\tightlist
\item
  \textbf{Distribución de Poisson \(P(\lambda)\)}
\end{itemize}

Se denominan \textbf{procesos de Poisson} a aquéllos en los que la ocurrencia de un suceso se encuentra distribuida a lo largo de un tiempo (o espacio) dado, cumpliendo que el proceso es estable, con una media de ocurrencias \(\lambda\) por unidad de tiempo, que se presentan de forma aleatoria e independiente. La variable que mide el número \(x\) de ocurrencias puede tomar los valores \(x=0,1,2,...\) y se dice que sigue una distribución de Poisson, con función de cuantía:
\begin{equation}
P(X=x)=\frac{e^\lambda\lambda^x}{x!}
\end{equation}
Su media es \(E(X)=\lambda\) y su varianza, \(Var(X)=\lambda\).

La representación gráfica de las funciones de cuantía y distribución se muestra en la fig.~\ref{fig:poisson}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/poisson-1} 

}

\caption{Función de cuantía y de distribución para una variable P(2.5)}\label{fig:poisson}
\end{figure}

\hypertarget{modelos-continuos}{%
\subsection{Modelos continuos}\label{modelos-continuos}}

Los modelos de distribución para variables continuas más habituales son el Normal, Gamma, Chi-cuadrado, t-student y F-Snedecor.

\begin{itemize}
\tightlist
\item
  \textbf{Distribución Normal \(N(\mu , \sigma)\)}
\end{itemize}

La distribución Normal, de Gauss o gaussiana, tiene una gran importancia debido a que un gran número de fenómenos aleatorios se pueden modelizar a partir de esta distribución (véase, más adelante, el epígrafe \ref{tcl}.

Una v.a. se dice que sigue una distribución normal de parámetros \(\mu\) y \(\sigma\) si puede tomar cualquier valor real y su función de densidad es de la forma:

\begin{equation}
f(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
\end{equation}
Su media es \(E[X]=\mu\) y su varianza, \(Vas(X)=\sigma^2\). Por ello, se suele decir que la normal está caracterizada por su media y su desviación típica.

La gráfica de la función de densidad tiene forma de campana (conocida como \textbf{campana de Gauss}) y es simétrica respecto de la media, y con colas mayores conforme aumenta la desviación típica, como muestra la fig.~\ref{fig:norm}, donde \(\mu=0\) y \(\sigma=1,1.5, \text{y } 2\)

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/norm-1} 

}

\caption{Función de densidad y de distribución para variable Normales}\label{fig:norm}
\end{figure}

Una característica importante de la distribución normal es la
\textbf{propiedad aditiva o reproductiva}, que indica que la combinación lineal de distribuciones normales sigue siendo una distribución normal. Si se consideran \(n\) variables aleatorias independientes con distribuciones \(N(\mu_i,\sigma_i)\) cualquier combinación lineal cumple:
\begin{equation}
\beta_0+\beta_1X_1+...+\beta_nX_n=\beta_0 + \sum_{i=1}^{n}\beta_iX_i \sim N \left( \beta_0+\sum_{i=1}^{n}\beta_i\mu_i, \sqrt{\sum_{i=1}^{n}\beta_i^2\sigma_i^2} \right)
\end{equation}

En particular, si \(X\sim N(\mu,\sigma)\), la variable \(Z=\frac{X-\mu}{\sigma} \sim N(0,1)\), conocida como \textbf{normal estándar} o \textbf{normal tipificada}.\footnote{También es posible interpretar que toda distribución normal es una transformada de la distribución \(Z\), ya que \(X=\mu+\sigma Z\).}

\begin{itemize}
\tightlist
\item
  \textbf{Distribución Gamma \(\Gamma ( \alpha, \beta )\)}
\end{itemize}

La distribución Gamma es útil cuando se medir el tiempo de espera (o la vida útil) hasta que ocurre un número determinado de eventos (\(\alpha\)), suponiendo que \(\beta\) (a veces también representado por \(\theta\) o parámetro de escala) es el tiempo medio entre ocurrencias de un suceso.

Esta distribución toma valores positivos y su función de densidad viene dada por la expresión:
\begin{equation}
f(x) = \frac {1}{\beta^\alpha \Gamma (\alpha)}x^{\alpha-1}e^{-x/\beta}
\end{equation}
donde \(\Gamma( \alpha)= \int_0^\infty x^{\alpha-1}e^{-x}dx=(\alpha -1)!\) si \(\alpha\) es un número natural.

Su media es \(E(X)=\alpha\beta\) y su varianza, \(Var(X)=\alpha\beta^2\). El parámetro \(\alpha\) es conocido como \textbf{parámetro de forma}; si \(\alpha\leq1\) la función de densidad tiene forma de''distribución gamma tiene forma de ``L''; si \(\alpha>1\), la distribución es campaniforme, con asimetría positiva y, conforme va aumentando, el centro de la distribución se desplaza hacia la derecha. \textbf{\(\beta\)} se conoce como ``parámetro de escala'' y determina el alcance de la asimetría positiva. La fig.~\ref{fig:gamma} muestra la representación gráfica de las funciones de densidad y distribución para \(\beta=2\) y \(\alpha=1\) (azul), \(\alpha=2\) (morado), \(\alpha=5\) (rojo) y \(\alpha=10\) (verde):

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/gamma-1} 

}

\caption{Función de densidad y de distribución para variables Gamma}\label{fig:gamma}
\end{figure}

En el \textbf{caso particular} de que \(\alpha=1\), se denomina \textbf{distribución exponencial} de parámetro \(\beta\).

\begin{itemize}
\tightlist
\item
  \textbf{Distribución \(\chi_n ^2\) de Pearson}
\end{itemize}

Sean \(X_1, X_2, ....,X_n\) v.a.i. todas distribuidas según una N(0,1). La suma de sus cuadrados sigue una distribución \(\Gamma (\frac {n}{2} , 2 )\), que se denomina \textbf{distribución Chi-cuadrado}: \footnote{Como se verá en el capítulo siguientes, epígrafe \ref{pobnormales}, esta distribución está aparece en el muestreo de poblaciones normales, en concreto al trabajar con varianzas, lo que justifica su interés.}
\begin{equation}
\sum_{i=1}^n X_i^2 \sim \chi^2_n \equiv \Gamma (\frac {n}{2} , 2 )
\end{equation}

Al parámetro \(n\), se le llama \textbf{grados de libertad}; su media es \(E(X)=n\) y su varianza, \(Var(X)=2n\) y la forma funcional de su densidad y distribución son casos particulares de la Gamma. De hecho, la fig.~\ref{fig:gamma} corresponde a distribuciones \(\chi^2_n\) con \(n=2,4,10, \text{y } 20\).

\begin{itemize}
\tightlist
\item
  \textbf{Distribución \(t-Student\)}
\end{itemize}

La distribución \(t-Student\) es otra distribución de probabilidad que surge en el muestreo de poblaciones normales (véase el epígrafe \ref{pobnormales}), en este caso asociada al uso de medias. Se dice que una v.a. \(X\) sigue una distribución t-student, con n-grados de libertad, si es el cociente entre una distribución normal estándar y la raiz de una \(\chi^2_n\) dividida entre sus grados de libertad \(X \sim \frac{N(0,1)}{\sqrt{\chi^2_n /n}}\)

Su función de densidad viene dada por:
\begin{equation}
f(x)=\frac{\Gamma((n+1)/2)}{\sqrt{n\pi}\Gamma(n/2)}(1+x^2/n)^{-(n+1)/2}
\end{equation}
con media \(E(X)=0\) y varianza, \(Var(X)=\frac{n}{n-2}\), siendo n\textgreater2. Su densidad tiene forma acampanada, simétrica respecto a cero y parecida a la de la Normal, pero con mayor probabilidad en las colas. En la fig.~\ref{fig:student} se muestran las funciones de densidad y distribución para tres \(t-Student\), con 3 (verde), 10 (rojo) y 100 (azul) grados de libertad

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/student-1} 

}

\caption{Función de densidad y de distribución para variables t-Student}\label{fig:student}
\end{figure}

\begin{itemize}
\tightlist
\item
  \textbf{Distribución \(F\) de Snedecor}
\end{itemize}

Este modelo también está asociado al muestreo sobre poblaciones normales, en este caso, a la comparación de varianzas. Se define una distribución F de Snedecor con \((n,m)\) grados de libertad como el cociente de dos distribuciones \(\chi^2\) independientes divididas entre sus grados de libertad, \(F_{n,m} \sim \frac {\chi^2_{n}/n}{\chi^2_{m}/m}\)

La función de densidad \(F_{n,m}\) viene dada por:
\begin{equation}
f(x)=\frac{\Gamma(\frac{n+m}{2})}{\Gamma(\frac{n}{2})\Gamma(\frac{m}{2})}\left(\frac{n}{m}\right)^{\frac{n}{2}}\frac{x^{\frac{n-2}{2}}}{(1+\frac{nx}{m})^{\frac{n+m}{2}}}
\end{equation}
con media \(E(X)=\frac{m}{m-2}\), siendo m\textgreater2 y varianza \(Var(X)=\frac{2m^2(n+m-2)}{n(m-2)^2(m-4)}\), cuando m\textgreater4

La gráfica de la función de densidad es parecida a la de la \(\chi_2\). Así, sólo está definida para el semieje positivo y su apariencia variará según los grados de libertad. La fig.~\ref{fig:FSnedecor} muestra las funciones de densidad y distribución para varias distribuciones F-Snedecor:

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/FSnedecor-1} 

}

\caption{Función de densidad y de distribución para variables F-Snedecor}\label{fig:FSnedecor}
\end{figure}

\hypertarget{tcl}{%
\section{Teorema central del límite}\label{tcl}}

A veces es difícil encontrar la distribución muestral de algunos estadísticos o estimadores, o, incluso, es imposible determinar la distribución de la variable de interés; entonces es útil aplicar algunos teoremas de convergencia, en especial el Teorema Central del Límite (TCL).

El TCL permite, bajo ciertas condiciones, usar la distiribución normal para estudiar otras distribuciones más generales.

Teorema Central del Límite: Si \(X_1,..., X_n\) son v.a.i. e idénticamente distribuidas con media \(\mu\) y desviación típica \(\sigma\), entonces \(\sum_{i=1}^n X_i\) tiene asintóticamente una distribución normal de media \(n\mu\) y desviación típica \(\sqrt{n}\sigma\). Y, por las propiedades de la normal, tenemos que:
\begin{equation}
  \frac{\sum_{i=1}^n X_i-n\mu}{\sqrt{n}\sigma} \underset {n\rightarrow\inf}\longrightarrow N(0,1)
\end{equation}
Señala que la distribución de la suma de n variables aleatorias tiende a una distribución normal, cuando n es muy grande. Es decir, aunque cada uno de los efectos sea raro o difícil de estudiar, si lo que queremos estudiar
es la suma de los mismos sabemos que, bajo ciertas condiciones, esta se comportará como una distribución normal. Así, se explica el hecho constatado de que muchas distribuciones de variables observadas en la naturaleza o en experimentos físicos sean aproximadamente normales, por ejemplo, las medidas del cuerpo humano, altura, peso, longigtud de los dedos, etc.

\hypertarget{distribuciones-de-probabilidad-en-r}{%
\section{\texorpdfstring{Distribuciones de probabilidad en \textbf{R}}{Distribuciones de probabilidad en R}}\label{distribuciones-de-probabilidad-en-r}}

En \textbf{R} están definidas las distribuciones de probabilidad más importantes, de tal forma que se aplica a cada nombre del modelo un determinado prefijo para calcular una función específica, \textbf{``d''} para la función de cuantía o densidad, \textbf{``p''} para la función de distribución, \textbf{``q''} para los cuantiles (o percentiles) y \textbf{``r''} para generar muestras pseudo-aleatorias.

Se va a utilizar el paquete \textbf{Rlab}, por lo que lo primero es instalar este paquete y cargar la librería.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Rlab)}
\end{Highlighting}
\end{Shaded}

En la tabla siguiente se exponen los modelos de distribución vistos, indicando el tipo, con su notación y la función utilizada en R para su cálculo:

Tabla : Funciones de distribución en \textbf{R}

\begin{longtable}[]{@{}cccc@{}}
\toprule()
Distribución & Tipo de modelo & Notación & Función en \textbf{R} \\
\midrule()
\endhead
Binomial & discreto & B(n,p) & binom \\
Bernoulli & discreto & B(p) & bern \\
Binomial negativa & discreto & BN(r;p) & nbinom \\
Geométrica & discreto & G(p) & geom \\
Poisson & discreto & P(\(\lambda\)) & pois \\
& & & \\
Normal & continuo & \(N(\mu,\sigma)\) & norm \\
Gamma & continuo & \(\Gamma(\alpha,\beta)\) & gamma \\
Exponencial & continuo & \(Exp (\beta)\) & exp \\
Chi-cuadrado & continuo & \(\chi^2_n\) grados de libertad & chisq \\
t-student & continuo & \(t_n\) grados de libertad & t \\
F-Snedecor & continuo & \(F_{n,m}\) grados de libertad & f \\
\bottomrule()
\end{longtable}

A continuación se realizarán dos ejemplos con \textbf{R}, uno para modelos discretos y otro para la normal. La adaptación a cualquier otro modelo de probabilidad consiste, básicamente, en sustituir las funciones de \textbf{R}.

\textbf{Ejemplo con distribuciones discretas:}

Un algoritmo de identificación de imágenes tiene acreditada una tasa de error del 20\% en caso de personas y del 5\% para el resto de imágenes, suponiendo el primer tipo el 25\% del total de imágenes.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Si se analizan 10 imágenes de personas, calcular la probabilidad de que identifique correctamente siete.
\end{enumerate}

Denominando \(X\) al número de imágenes de personas correctamente clasificadas, se tiene que \(X \sim B(10,0.8)\). Se pide \(P(X=7)=\)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dbinom}\NormalTok{(}\DecValTok{7}\NormalTok{, }\AttributeTok{size =} \DecValTok{10}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.8}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 0.2013266}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Para el resto de imágenes, calcular la probabilidad de identificar correctamente como mucho 50 hasta que se produce el segundo error.
\end{enumerate}

Denominando \(Y\) al número de imágenes correctamente identificadas hasta el segundo error, se tiene que \(Y \sim BN(2,0.05)\). Se pide \(P(Y \leq 50)\)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnbinom}\NormalTok{(}\DecValTok{50}\NormalTok{, }\AttributeTok{size =} \DecValTok{2}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.05}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 0.7405031}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Históricamente, el número medio diario de imágenes incorrectamente clasificadas es de 7. Calcular la probabilidad de que un día seleccionado al azar clasifique incorrectamente entre 6 y 9 imágenes.
\end{enumerate}

Denominando \(T\) al número de imágenes incorrectamente identificadas en un día, se tiene que \(T \sim P(\lambda = 7)\). Se pide \(P(6 \leq T \leq 9) = P(T \leq 9) - P(T \leq 5)\)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ppois}\NormalTok{(}\DecValTok{9}\NormalTok{, }\DecValTok{7}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{{-}} \FunctionTok{ppois}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 0.5297877}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  Calcular la probabilidad de que en un lote de 20 imágenes del mismo tipo sean todas correctamente clasificadas.
\end{enumerate}

Como no se especifica el tipo de imágenes se identifica, hay que calcular dicha probabilidad condicionada a cada grupo y utilizar el teorema de la probabilidad total: \(P(acierto)=P(acierto/personas)*P(personas)+P(acierto/otras)*P(otras)\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acierto\_personas }\OtherTok{\textless{}{-}} \FunctionTok{dbinom}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{20}\NormalTok{, }\FloatTok{0.2}\NormalTok{)}
\NormalTok{acierto\_otras }\OtherTok{\textless{}{-}} \FunctionTok{dbinom}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{20}\NormalTok{, }\FloatTok{0.05}\NormalTok{)}
\NormalTok{acierto\_total }\OtherTok{\textless{}{-}}\NormalTok{ acierto\_personas }\SpecialCharTok{*} \FloatTok{0.25} \SpecialCharTok{+}\NormalTok{ acierto\_otras }\SpecialCharTok{*} \FloatTok{0.75}
\NormalTok{acierto\_total}
\CommentTok{\#\textgreater{} [1] 0.2717467}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{4}
\tightlist
\item
  Si se han clasificado correctamente las 20 imágenes del lote, calcular la probabilidad de que correspondan a imágenes de personas.
\end{enumerate}

Se pide la probabilidad de que correspondan a personas condicionado a que han sido correctamente clasificadas, por lo que hay que utilizar el teorema de Bayes: \(P(personas/acierto)= \frac {P(acierto/personas)*P(personas)}{P(acierto)}\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acierto\_personas }\SpecialCharTok{*} \FloatTok{0.25} \SpecialCharTok{/}\NormalTok{ acierto\_total}
\CommentTok{\#\textgreater{} [1] 0.01060658}
\end{Highlighting}
\end{Shaded}

\textbf{Ejemplo con una distribución normal}:

Las calificaciones (de 0 a 10) en un curso de estadística siguen una de distribución normal N(6, 1.25), \(X \sim N(6, 1.25)\)

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Calcular la probabilidad de que una persona obtenga una calificación inferior a 4.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\DecValTok{4}\NormalTok{, }\AttributeTok{mean =} \DecValTok{6}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.25}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 0.05479929}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Número probable de personas que obtendrán sobresaliente (9 o más) en un grupo de 60 personas.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{pnorm}\NormalTok{(}\DecValTok{9}\NormalTok{, }\DecValTok{5}\NormalTok{, }\FloatTok{1.25}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{p}
\CommentTok{\#\textgreater{} [1] 0.0006871379}
\CommentTok{\# En un grupo de 60 personas, redondeando a un número entero}
\FunctionTok{round}\NormalTok{(}\DecValTok{60} \SpecialCharTok{*}\NormalTok{ p)}
\CommentTok{\#\textgreater{} [1] 0}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Calcular la nota mínima para estar en el 30\% de personas con mejores calificaciones.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.7}\NormalTok{, }\DecValTok{6}\NormalTok{, }\FloatTok{1.25}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 6.655501}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  En un curso de informática las calificaciones siguen una de distribución normal N(5, 1.75), independientes de las de estadística. Calcular la probabilidad de que una persona matriculada en ambos cursos saque mayor calificación en estadística.
\end{enumerate}

Llamando \(X=C_e-C_i\) a la diferencia entre las calificaciones en estadística (\(C_e\)) y en informatica (\(C_i\)), ambas normales e independientes, la distribución de \(X\) será \(X \sim N \left ( 6-5, \sqrt{1.25^2+1.75^2} \right )\). Se pide \(P(X>0)\), que se representa en la fig.~\ref{fig:graf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{mean =} \DecValTok{1}\NormalTok{, }\AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(}\FloatTok{1.25}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \FloatTok{1.75}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{), }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 0.6790309}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/graf-1} 

}

\caption{P(X>0) representada como el área bajo la densidad de la distribución normal}\label{fig:graf}
\end{figure}

\begin{infobox_resume}
\textbf{RESUMEN}

La teoría de la probabilidad proporciona reglas de comportamiento que permiten la ordenación y toma de decisiones en situaciones donde prevalecen condiciones de incertidumbre.

Los modelos de distribuciones de probabilidad permiten el cálculo en, prácticamente, cualquier estudio de las ciencias sociales, técnicas y naturales.

\end{infobox_resume}

\hypertarget{Fundainfer}{%
\chapter{Inferencia estadística}\label{Fundainfer}}

\emph{Mª Leticia Meseguer Santamaría} y \emph{Manuel Vargas Vargas}

\hypertarget{introinfer}{%
\section{Introducción a la Inferencia Estadística}\label{introinfer}}

Cuando se estudian fenómenos mediante variables, el objetivo estadístico básico es determinar cuáles son las distribuciones probabilísticas que rigen dichas variables o algunas características determinadas por ellas. Es este comportamiento aleatorio el que permite hacer predicciones con unos márgenes de error conocidos, analizar y cuantificar la relación entre variables, evaluar si hipótesis o modelos teóricos son congruentes con los datos disponibles, etc. Así, en la práctica, cuando se estudia una variable, \(X\), lo habitual es que se desconozca su distribución, llamada distribución poblacional, \(F(x)\), pero que se disponga de un conjunto de realizaciones \((x_1,...,x_n)\), también llamado muestra, valores concretos de dicha variable a partir de los cuales ``aproximar'' la distribución desconocida.

La inferencia estadística proporciona las herramientas y técnicas que permiten, a partir de la información muestral, extrapolar resultados a la distribución poblacional con márgenes de error conocidos. Un primer objetivo (más detallado en el Capítulo siguiente, \ref{muestreo}) es analizar qué condiciones debe cumplir la muestra para que su información sea válida y extrapolable a toda la población (es la conocida como \textbf{teoría de muestreo}). Un segundo objetivo es establecer los mecanismos que permitan dicha extrapolación manteniendo controlados los errores de muestreo.

Es habitual que se conozca (o se asuma) que la distribución poblacional \(F(x)\) pertenezca a alguna familia paramétrica, es decir, que se asuma su forma funcional pero que dependa de algunos parámetros (lo más frecuente es que se asuma la normalidad, pero podría ser cualquiera de los modelos paramétricos existentes). Se habla entonces de \textbf{inferencia paramétrica}, ya que se usa la información muestral para determinar los ``mejores'' valores (bajo algún criterio) de los parámetros que rigen la distribución poblacional, existiendo tres planteamientos básicos, estimación puntual (epígrafe \ref{estimpuntual}), por intervalo (epígrafe \ref{estimintervalos}) y contraste de hipótesis (epígrafe \ref{contrhip}).

También hay situaciones en las que la forma funcional de la distribución poblacional es desconocida, o se duda de que la familia paramétrica considerada sea adecuada. En estos casos, bajo el nombre genérico de \textbf{inferencia no paramétrica}, se plantean contrastes que buscan determinar cuándo es posible asumir un modelo concreto de distribución, entre los que destacan, por su frecuente uso, los contrastes de normalidad (epígrafe \ref{contrnormalidad}). Otra alternativa que permite aproximar características poblacionales sin asumir ninguna distribución poblacional concreta es el \textbf{remuestreo}, fundamentalmente el denominado \emph{``bootstrap''}, (tema tratado en el Capítulo \ref{muestreo}).

\hypertarget{mas}{%
\section{Muestreo aleatorio simple}\label{mas}}

Al estudiar una variable poblacional, \(X\), de la que se desconoce su distribución, llamada \textbf{distribución poblacional, \(F(x)\)}, se utiliza la información suministrada por una \textbf{muestra} obtenida por algún método de muestreo probabilístico que garantice que sea representativa de la variable poblacional.

En la mayoría de los casos y técnicas estadísticas se asume que la muestra está obtenida mediante el método básico de muestreo, conocido como \textbf{muestreo aleatorio simple}, consistente en seleccionar totalmente al azar a los individuos de la muestra, por lo que todos tienen la misma probabilidad de formar parte de ella. Si cada individuo sólo puede aparecer una vez en la muestra, se habla de \textbf{muestra sin reemplazamiento}, mientras que se denomina \textbf{muestra con reemplazamiento} en caso contrario. De esta forma, dada una distribución poblacional \(F(x)\),una muestra aleatoria simple (m.a.s.) es una realización de un conjunto de \(n\) variables aleatorias independientes e idénticamente distribuidas \(X=(X_1,...,X_n)\), cuya \textbf{distribución conjunta} es de la forma:
\begin{equation}
F(X_1,...,X_n)=F_{X_1}(x_1)...F_{X_n}(x_n)=F(x_1)...F(x_n)
\label{eq:distrconjunta}
\end{equation}

Una herramienta básica para la inferencia es la \textbf{distribución empírica de la muestra}, definida como:
\begin{equation}
\hat{\mathbb{F}}_n (x)= {1 \over n} \sum_{i=1}^n \mathbb{I}_{(-\infty ,x]}(X_i)
\label{eq:distrempirica}
\end{equation}
donde por \(\mathbb{I}_{(-\infty ,x]}(X_i)\) se indica la función indicadora, que toma el valor 1 si \(X_1 \leq x\) y 0 en caso contrario.\footnote{Es decir, la distribución empírica de la muestra indica, para cada valor \emph{x}, la proporción de elementos de la muestra que toman un valor menor o igual que él.}

La gran ventaja del muestreo aleatorio simple consiste en que, dada una m.a.s. \((X_1,..., X_n)\):
\begin{equation}
\underset {n \rightarrow \infty}{lim} \ E \left [ {\left ( \hat{\mathbb{F}}_n (x)- F(x) \right )^2} \right ] = 0
\label{eq:glivenko}
\end{equation}
conocido como \emph{teorema de Glivenko-Cantelli}. Este resultado es fundamental en inferencia, pues garantiza que el muestreo aleatorio simple produce muestras representativas de la población, ya que, a medida que aumenta el tamaño muestral, la distribución empírica de la muestra se aproxima a la distribución poblacional (véase fig.~\ref{fig:distrempir}). \footnote{Un tema que se abordará en el Capítulo \ref{muestreo} es la determinación del tamaño muestral necesario para que la aproximación tenga un error menor que uno prefijado.} Así, cualquier característica de una distribución poblacional puede ser aproximada por su equivalente en la distribución empírica.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/distrempir-1} 

}

\caption{Distribución empírica para muestras de diferente tamaño de una distribución Normal}\label{fig:distrempir}
\end{figure}

Es muy frecuente que, a efectos de inferencia, no se estudie el comportamiento aleatorio de toda la muestra (su distribución conjunta) sino que interese el comportamiento de una función de la muestra que no dependa de ningún valor desconocido, \(T(X)=T(X_1,...,X_n)\), llamada genéricamente \textbf{estadístico muestral}; dicho comportamiento vendrá determinado por la \textbf{distribución en el muestreo} del estadístico \(T(X)\). El hecho de utilizar una m.a.s. permite establecer resultados de interés sobre los estadísticos o, en algunos casos, incluso obtener la distribución en el muestreo exacta de los estadísticos más usuales (epígrafe \ref{pobnormales}).

Así, dada una variable poblacional \(X\) con varianza finita, y una m.a.s. se define la \textbf{media muestral} como
\begin{equation}
\bar X = \frac {X_1, + ... + X_n}{n}
\label{eq:mediamuestral}
\end{equation}
El hecho de utilizar una m.a.s. garantiza que:
\begin{equation}
E[\bar X] = E[X] \ \text{;} \ Var(\bar X)=\frac{Var(X)}{n}
\end{equation}
Este resultado es muy útil, ya que indica que la variabilidad de la media muestral es más pequeña que la variabilidad de la variable poblacional, siendo inversamente proporcional al tamaño muestral.

Otro estadístico muy utilizado es la \textbf{varianza muestral}, que se define como:
\begin{equation}
S^2 = {\sum_{i=1}^n \left ( X_i - \bar X \right ) ^2 \over n}
\label{eq:varmuestral}
\end{equation}
En este caso, su esperanza es:
\begin{equation}
E[S^2] = \frac {n-1}{n} Var[X]
\end{equation}
que no coincide con la varianza poblacional. Para evitar este hecho, se define la \textbf{cuasivarianza muestral}:
\begin{equation}
S_c^2 = {\sum_{i=1}^n \left ( X_i - \bar X \right ) ^2 \over {n-1}}
\label{eq:cuasivarmuestral}
\end{equation}
estadístico para el que sí se cumple que \(E[S_c ^2] = Var[X]\), ya que existe una relación de proporcionalidad entre ambos estadísticos \(nS^2 = (n-1)S_c ^2\).\footnote{Muchos textos, sobre todo anglosajones no hacen esta distinción, sino que denominan directamente \emph{``varianza muestral''} a la cuasivarianza. En R, por ejemplo, las funciones \emph{var} o \emph{sd} proporcionan la cuasivarianza y cuasidesviación típica muestrales respectivamente, matiz que hay que tener siempre presente.}

\hypertarget{estimpuntual}{%
\section{Estimación puntual}\label{estimpuntual}}

Sea una población caracterizada por una distribución poblacional \(F_\theta (x)\), de una familia paramétrica pero que se desconoce el valor del parámetro \(\theta \in \Theta\), donde \(\Theta\) es el espacio paramétrico (conjunto de posibles valores de \(\theta\)). Dada una m.a.s. \(X=(X_1,...,X_n)\), se considera como \emph{estimador} de \(\theta\) a un estadístico muestral cuyo resultado sea un posible valor del parámetro:
\begin{equation}
\hat{\theta}=T(X)=T(X_1,...,X_n) \in \Theta
(\label{eq:estimador})
\end{equation}
Para cada estimador, se puede definir su \textbf{error cuadrático medio}:
\begin{equation}
ECM_\theta (\hat{\theta})=E_\theta \left[ { \left ( \hat{\theta}-\theta \right ) ^2 } \right]
\label{eq:ecm}
\end{equation}
que proporciona un valor medio del error que se comete al ``aproximar'' el verdadero valor \(\theta\) por el resultado del estimador \(\hat{\theta}\).
Así, el criterio de \emph{``mínimos cuadrados''} propone utilizar aquel estimador que minimiza el error cuadrático medio:
\begin{equation}
\hat{\theta}_{MC}= \underset {\hat{\theta}} {min} E_\theta \left[ {\left ( \hat{\theta}-\theta \right )^2} \right]
\end{equation}
Desarrollando la expresión del ECM (@(eq:ecm)), éste se puede reexpresar como
\begin{equation}
ECM_\theta (\hat{\theta})=Var_\theta (\hat{\theta}) + \left ( E_{\theta}(\hat{\theta}) - \theta \right ) ^2  = Var_\theta (\hat{\theta}) + b_\theta ^2 (\hat{\theta})
\end{equation}
donde \(b_\theta(\hat{\theta}) =\left ( E_{\theta}(\hat{\theta}) - \theta \right )\) se conoce como \textbf{sesgo} del estimador (\emph{bias}, en inglés). Así, el ECM de un estimador depende de su varianza y de su sesgo al cuadrado.

Para determinar, por tanto, el \emph{``mejor''} estimador bajo el criterio de mínimos cuadrados, se podría obtener en dos pasos:

\begin{itemize}
\item
  Utilizar estimadores ``insesgados'', es decir, de sesgo cero, o sea, \(E(\hat{\theta})=\theta\) (el valor medio del estimador coincide con el parámetro).
\item
  De entre los estimadores inesgados, utilizar el de varianza mínima, \(Var(\hat{\theta}_{MC})= \underset {\hat{\theta}} {min} Var(\hat{\theta})\)
\end{itemize}

Queda fuera del objetivo de este capítulo plantear la obtención del estimador de mínimos cuadrados para cualquier distribución poblacional y parámetros, que el lector interesado puede encontrar en cualquier texto teórico de inferencia estadística (\citet{CasellaBerger2007} , \citet{Blais2020} , \citet{Almudevar2021}).

Otro planteamiento para encontrar estimadores puntuales se basa en la función de densidad de la muestra, que depende de ésta y del parámetro que caracteriza a la distribución poblacional:
\begin{equation}
f(x_1,...,x_n;\theta)=f(x_1;\theta)...f(x_n;\theta)=L(\theta;x_1,...,x_n)
\label{eq:verosimilitud}
\end{equation}
Considerando el parámetro como fijo, la función se interpreta como la densidad de la muestra. Sin embargo, si se considera que la muestra está dada, también se puede interpretar como una función del parámetro, que mide la \textbf{verosimilitud} (\emph{likelihood}, en inglés) de cada valor del parámetro en función de la muestra obtenida. Así, el criterio para determinar el \emph{``mejor''} estimador puede ser utilizar aquél que maximiza la función de verosimilitud; se obtiene entonces el conocido como \emph{estimador máximo verosímil}:
\begin{equation}
\hat{\theta}_{MV}= \underset {\theta} {max} L(\theta;x_1,...,x_n)
\end{equation}
Para el cálculo del estimador máximo verosímil no se suele utilizar la función de verosimilitud, sino su logaritmo (que alcanza los máximos y mínimos en los mismos puntos), derivando respecto al parámetro e igualando a cero (ecuación de verosimilitud).

Este método suele proporcionar estimadores con buenas propiedades estadísticas y, en muchos casos, suele conducir al mismo resultado que el método de mínimos cuadrados. \footnote{En las distribuciones usuales es así, salvo que el estimador de máxima verosimilitud no sea insesgado, como es el caso de estimar la varianza en una distibución normal.}. En las distribuciones usuales, es relativamente sencillo obtener la ecuación de verosimilitud y resolverla, por lo que se dispone de estimadores máximo verosímiles conocidos. En modelos más elaborados, la resolución de la ecuación de verosimilitud se puede complicar, hasta el extremo de que haya que recurrir a métodos numéricos de aproximación.

Una alternativa computacionalmente más sencilla es la basada en el conocido como \textbf{método de los momentos}. El planteamiento básico es expresar el parámetro en función de los momentos poblacionales (esperanza, varianza, etc.) y utilizar como estimador la misma función pero de los momentos muestrales (media muestral, varianza muestral, etc.). En las distribuciones más usuales, los parámetros suelen ser momentos poblacionales o tranformadas simples de éstos, por lo que el método de los momentos es muy sencillo. Como contrapartida, es más difícil evaluar las propiedades estadísticas de estos estimadores, salvo que coincidan con los de mínimos cuadrados o de máxima verosimilitud.

En \textbf{R}, la librería \texttt{fdistrplus} dispone de la función \texttt{fitdist()}, que permite la obtención de los estimadores para las distribuciones usuales por diversos métodos, incluidos el de máxima verosimilitud (mle) y el de los momentos (mme).

\hypertarget{estimintervalos}{%
\section{Estimación por intervalos}\label{estimintervalos}}

Dado que todo estimador es una variable aleatoria, su valor concreto, la \emph{``estimación''} de parámetro \(\hat\theta\), depende de la muestra. Esta variación muestral ocasiona incertidumbre sobre la estimación. Una forma de incluir esta variabilidad en la estimación puede consistir en sustituir la estimación puntual por un intervalo de valores en el que se tenga un cierto nivel de confianza de que contenga al verdadero valor del parámetro.

El método más extendido para obtener \textbf{intervalos de confianza} consiste en utilizar un estimador puntual y utilizar su distribución en el muestreo para construir una región que contenga, con cierta probabilidad \((1-\alpha)\), al verdadero valor \(\theta\):
\begin{equation}
IC_{(1-\alpha)}=[LIC , LSC] \ \text{tal que} \ P \left ( LIC \leq \theta \leq LUC  \right ) = (1-\alpha)
\label{eq:interconfianza}
\end{equation}
donde los límites inferior (LIC) y superior (LSC) de confianza dependen de la desviación típica del estimador y de constantes asociadas a su distribución y al nivel de confianza \((1-\alpha)\), llamados \textbf{valores críticos}. En esta ecuación, tanto el LIC como el LSC son variables aleatorias; cuando se utilizan los datos de una muestra, se convierten en valores reales, por lo que no se puede hablar de \emph{``probabilidad de que el parámetro esté dentro del intervalo''}, sino que se habla de \emph{``confianza en que el intervalo contenga al valor del parámetro''}.

En \textbf{R}, la librería \texttt{Rlab} permite obtener los valores críticos de las distribuciones usuales a través de los cuantiles, anteponiendo \emph{q} al nombre de la distribución, recogidos en la tabla XXX-ref. cruzada ``Funciones de distribución en R'' (por ejemplo usando las funciones \texttt{qbinom()}, \texttt{qnorm()}, \texttt{qt()}, \texttt{qf()}, etc.). Igualmente, la librería \texttt{DescTools} dispone de funciones para calcular intervalos de confianza en poblaciones normales para la media (\texttt{MeanCI()}), la diferencia de medias (\texttt{MeanDiffCI()}), la mediana (\texttt{MedianCI()}), cualquier cuantil (\texttt{QuantileCI()}), o la varianza (\texttt{VarCI()}). Por último, en el caso de no conocer la distribución en el muestreo del estimador, se puede recurrir al remuestreo por bootstrap, que se detallará en el capítulo \ref{muestreo}, indicando el método \emph{``boot''} en las funciones anteriores.

\hypertarget{contrhip}{%
\section{Contrastes de hipótesis}\label{contrhip}}

Hay situaciones donde no interesa tanto estimar el valor de un parámetro sino decidir si la información muestral es congruente con algún valor concreto del parámetro. En estos casos, se puede establecer como \textbf{hipótesis} que el parámetro toma un valor concreto y \emph{contrastar} si es verosímil haber obtenido el resultado muestral dado. Este planteamiento se conoce como \textbf{contrastes de significación}.

Así, se establece una hipótesis, históricamente conocida como \textbf{hipótesis nula}, que determina un valor del parámetro:
\begin{equation}
H_0 \equiv \theta = \theta_0
\end{equation}
Con este valor, la distribución muestral del estimador permite obtener la probabilidad de observar un valor tan \emph{``distante''} como el obtenido en la muestra, conocido como \textbf{p-valor}: si es muy pequeño, sería poco probable haber observado ese valor si la hipótesis es cierta, por lo que la evidencia empírica no es congruente con ella; si no es pequeño, sería probable haber observado el valor bajo la hipótesis, por lo que no hay evidencia empírica \emph{``contra''} dicha hipótesis nula.

Cuando por \emph{``distante''} se entiende alejado del valor medio del estimador, mayor o menor, se habla de \textbf{p-valor bilateral} o ``a dos colas''. Cuando se considera \emph{``distante''} como menor que (o mayor que) la media del estimador, se habla de \textbf{p-valor unilateral} (a la izquierda o a la derecha, respectivamente) o ``a una cola''.

Habitualmente, se considera que un p-valor por debajo de 0.05 ya indica que la evidencia empírica no permite asumir como cierta la hipótesis nula, expresándose como que el valor del parámetro es \emph{``significativamente distinto (menor o mayor)''} que \(\theta_0\). También es posible interpretar el p-valor como \emph{``la probabilidad máxima de cometer el error de rechazar la hipótesis nula cuando es cierta''}, abreviado como \emph{``tamaño del error si se rechaza la hipótesis nula''}.

Estos contrastes de significación, originalmente desarrollados por Ronald Fisher, fueron incluidos en un esquema de toma de decisiones por Jerzy Neyman y Egon Pearson, planteando que, de no ser cierta la hipótesis nula, se debe plantear una hipótesis alternativa \(H_1\). La decisión de qué hipótesis resulta más congruente con los datos se basa en la comparación por cociente de las verosimilitudes de la muestra bajo cada una de ellas, decidiendo el rechazo de la hipótesis nula a favor de la alternativa cuando dicho cociente es, en probabilidad, inferior a un valor prefijado, \(\alpha\), conocido como \textbf{nivel de significación}. Dependiendo de la estructura de las hipótesis (simples, si sólo determinan un valor del parámetro, o compuestas, si determinan más de uno; a su vez, unilaterales si los valores son todos menores, o mayores, que uno dado, o bilaterales en caso contrario) la regla de decisión del contrate resulta más o menos compleja de obtener.

Computacionalmente, dada la información muestral, es más fácil calcular el p-valor que plantear el esquema de decisión de Neyman-Pearson, por lo que es la estrategia utilizada en la práctica. Dado el carácter breve e introductorio de este capítulo, no se profundizará más en este esquema de decisión, que puede consultarse en, por ejemplo, \citet{CasellaBerger2007}, \citet{Blais2020} o \citet{Almudevar2021}.

\hypertarget{pobnormales}{%
\section{Inferencia estadística paramétrica sobre poblaciones normales}\label{pobnormales}}

Como consecuenca del TCL (\ref{tcl}), el supuesto de que la distribución poblacional es una normal es el caso más habitual en la práctica, siendo requisito básico en muchísimas técnicas estadísticas. En este caso, las distribuciones muestrales de los estimadores de los parámetros poblacionales, tanto de la media \(\mu\) como de la varianza \(\sigma^2\), son conocidas, lo que facilita la construcción de intervalos de confianza y contrastes de hipótesis.

Así, dada una distribución poblacional normal y una m.a.s. de tamaño n,

\begin{itemize}
\tightlist
\item
  Para estimar la varianza poblacional, \(\sigma ^2\), el estimador máximo verosímil es la varianza muestral \eqref{eq:varmuestral}, que es sesgado. El estimador insesgado es la cuasivarianza muestral \eqref{eq:cuasivarmuestral}. Sus distribuciones en el muestreo son:
  \begin{equation}
  {n S^2 \over \sigma^2} = {(n-1) S_c^2 \over \sigma^2} \sim \chi^2_{n-1}
  \end{equation}
  Así, el intervalo de confianza a nivel \((1-\alpha)\) es:
  \begin{equation}
  IC_{(1-\alpha)} =\left [ {n S^2 \over {\chi^2_{n-1,\alpha /2}}} , {n S^2 \over {\chi^2_{n-1, 1-\alpha /2}}}  \right ]
  \end{equation}
  o, equivalentemente, usando la proprocionalidad entre varianza y cuasivarianza muestrales
  \begin{equation}
  IC_{(1-\alpha)} = \left [ {(n-1) S_c^2 \over {\chi^2_{n-1,\alpha /2}}} , {(n-1) S_c^2 \over {\chi^2_{n-1, 1-\alpha /2}}}  \right ]
  \end{equation}
  donde \(\chi ^2 _{n-1,\alpha / 2}\) representa el cuantil en la distribución.
\end{itemize}

Para el contraste de \(H_0 \equiv \sigma^2 = \sigma_0^2\), la ``distancia'' es \({n S^2 \over \sigma^2_0} = {(n-1) S_c^2 \over \sigma_0^2}\), lo que permite calcular los p-valores mediante una distribución \(\chi^2_{n-1}\).

Para el caso de querer estimar la desviación típica, basta con calcular la raiz cuadrada del estimador de la varianza, o si se busca un intervalo de confianza, la raiz de los extremos del intervalo para la varianza. Los contrastes de hipótesis son equivalentes, ya que \(\sigma^2=\sigma_0^2 \equiv \sigma = \sigma_0\).

\begin{itemize}
\item
  Para estimar el parámetro \(\mu\) se utiliza el estimador media muestral \(\hat{\mu} = \bar X\), en el que coinciden los métodos de mínimos cuadrados, máxima verosimilitud y el de los momentos, siendo insesgado y de varianza mínima.
\item
  Si la varianza poblacional es conocida, la distribución en el muestreo de la media muestral es:
  \begin{equation}
  \bar X \sim N \left ( \mu , {\sigma \over {\sqrt{n}}} \right ) \equiv {\bar X - \mu \over {\sigma / \sqrt n}} \sim N(0,1)
  \end{equation}
  El intervalo de confianza a nivel \((1-\alpha)\) es:
  \begin{equation}
  IC_{(1-\alpha)} =\left [ \bar X - z_{\alpha / 2} {\sigma \over \sqrt n} , \bar X + z_{\alpha / 2} {\sigma \over \sqrt n}  \right ]
  \end{equation}
  donde \(z_{\alpha/2}\) representa el cuantil en una distribución normal estándar.
\end{itemize}

Para el contraste de \(H_0 \equiv \mu = \mu_0\), la ``distancia'' es \(\bar X - \mu_0 \over \sigma / \sqrt n\), lo que permite calcular los p-valores mediante una distribución \(N(\mu , \sigma )\).

\begin{itemize}
\tightlist
\item
  Si la varianza poblacional es desconocida, se sustituye por su estimación, por lo que la distribución en el muestreo de la media muestral es:
  \begin{equation}
  {\bar X - \mu \over {S / \sqrt {n-1}}} \equiv {\bar X - \mu \over {S_c / \sqrt n}} \sim t_{n-1}
  \end{equation}
  El intervalo de confianza a nivel \((1-\alpha)\) es:
  \begin{equation}
  IC_{(1-\alpha)} =\left [ \bar X - t_{n-1,\alpha / 2} {S \over \sqrt {n-1}} , \bar X + t_{n-1, \alpha / 2} {S \over \sqrt {n-1}}  \right ]
  \end{equation}
  o, equivalentemente,
  \begin{equation}
  IC_{(1-\alpha)} = \left [ \bar X - t_{n-1,\alpha / 2} {S_c \over \sqrt n} , \bar X + t_{n-1, \alpha / 2} {S_c \over \sqrt n}  \right ]
  \end{equation}
  donde \(t_{n-1,\alpha/2}\) representa el cuantil de la distribución \emph{t-Student}.
\end{itemize}

Para el contraste de \(H_0 \equiv \mu = \mu_0\), la ``distancia'' es \({\bar X - \mu_0 \over S / \sqrt {n-1}} = {\bar X - \mu_0 \over S_c / \sqrt n}\), lo que permite calcular los p-valores mediante una distribución \(t_{n-1}\).

Cuando el interés es la comparación de dos poblaciones normales independientes, X e Y, a partir de muestras \((X_1,...,X_n)\) y \((Y_1,...,Y_m)\), se modifican los estimadores.

\begin{itemize}
\item
  Para la comparación de las varianzas poblacionales, la distribución muestral del cociente es una distribución F-Snedecor:
  \begin{equation}
  {{{m S_Y^2} \over {(m-1)\sigma_Y^2}} \over {{n S_X^2} \over {(n-1)\sigma_X^2}}} \equiv {S_{cY}^2 / \sigma_Y^2 \over S_{cX} ^2 / \sigma_X^2} \sim F_{m-1,n-1}
  \end{equation}
  que permite calcular intervalos de confianza de forma idéntica a la expuesta a los casos anteriores pero con la distribución F. Un caso muy frecuente es querer contrastar si ambas varianzas poblacionales son iguales (el cociente entre ellas es la unidad).
\item
  Para la comparación de las medias poblacionales, el caso más común es asumir que las varianzas (aunque desconocidas) son iguales, por lo que el estimador es:
  \begin{equation}
  {(\bar X - \bar Y)-(\mu_X - \mu_Y) \over \sqrt{{nS_X^2+mS_Y^2} \over n+m-2} \sqrt{{1\over n} + {1 \over m}}} \sim t_{n+m-2}
  \end{equation}
  si se utiliza la varianza muestral o
  \begin{equation}
  {(\bar X - \bar Y)-(\mu_X - \mu_Y) \over \sqrt{{(n-1)S_{cX}^2+(m-1)S_{cY}^2} \over n+m-2} \sqrt{{1\over n} + {1 \over m}}} \sim t_{n+m-2}
  \end{equation}
  si se utiliza la cuasivarianza muestral.
\end{itemize}

Al ser distribuciones t-Student, los intervalos de confianza y contrastes de hipótesis son similares a los del caso de una única población con las correcciones pertinentes.

\hypertarget{ejemplopobnorm}{%
\section{\texorpdfstring{Inferencia sobre poblaciones normales con \textbf{R}}{Inferencia sobre poblaciones normales con R}}\label{ejemplopobnorm}}

Los datos sobre calidad del aire en la ciudad de Nueva York (\texttt{airquality}) recogen la variable \texttt{Wind} que mide, en mph, la velocidad del viento entre el día 1 de mayo y el 30 de septiembre. Se particionan dichos datos en dos variables \(X=\text{Velocidad del viento hasta el 15 de julio}\) e \(Y=\text{Velocidad del viento desde el 16 de julio}\). Asumiendo que las distribuciones poblacionales son normales, se propone:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Determinar una estimación de la velocidad media y de la desviación típica de ambas variables, usando el método de máxima verosimilitud.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(fitdistrplus)}
\FunctionTok{library}\NormalTok{(DescTools)}
\NormalTok{x }\OtherTok{\textless{}{-}}\NormalTok{ airquality}\SpecialCharTok{$}\NormalTok{Wind[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{76}\NormalTok{]}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ airquality}\SpecialCharTok{$}\NormalTok{Wind[}\DecValTok{77}\SpecialCharTok{:}\DecValTok{153}\NormalTok{] }\CommentTok{\# Se particiona la muestra en los dos períodos}
\NormalTok{mle\_x }\OtherTok{\textless{}{-}} \FunctionTok{fitdist}\NormalTok{(x, }\AttributeTok{distr =} \StringTok{"norm"}\NormalTok{, }\AttributeTok{method =} \StringTok{"mle"}\NormalTok{)}
\NormalTok{mle\_y }\OtherTok{\textless{}{-}} \FunctionTok{fitdist}\NormalTok{(y, }\AttributeTok{distr =} \StringTok{"norm"}\NormalTok{, }\AttributeTok{method =} \StringTok{"mle"}\NormalTok{)}
\NormalTok{mle\_x}
\CommentTok{\#\textgreater{} Fitting of the distribution \textquotesingle{} norm \textquotesingle{} by maximum likelihood }
\CommentTok{\#\textgreater{} Parameters:}
\CommentTok{\#\textgreater{}       estimate Std. Error}
\CommentTok{\#\textgreater{} mean 10.640789  0.4274723}
\CommentTok{\#\textgreater{} sd    3.726618  0.3022685}
\NormalTok{mle\_y}
\CommentTok{\#\textgreater{} Fitting of the distribution \textquotesingle{} norm \textquotesingle{} by maximum likelihood }
\CommentTok{\#\textgreater{} Parameters:}
\CommentTok{\#\textgreater{}      estimate Std. Error}
\CommentTok{\#\textgreater{} mean 9.283117  0.3581657}
\CommentTok{\#\textgreater{} sd   3.142891  0.2532613}
\end{Highlighting}
\end{Shaded}

Es resultado muestra como estimaciones la media y la desviación típica muestrales respectivamente, junto al \emph{``error estandar''}, o desviación típica de los estimadores respectivos.

La orden \texttt{plot(mle\_x)} permite un análisis gráfico de las estimaciones realizadas; por ejemplo si optamos por el primer período, se obtiene la fig.\ref{fig:mle} siguiente:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(mle\_x)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/mle-1} 

}

\caption{Resultados gráficos de la estimación por máxima verosimilitud}\label{fig:mle}
\end{figure}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Establecer un intervalo de confianza para la velocidad media del viento hasta el 15 de julio, con un nivel de confianza del 95\%.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MeanCI}\NormalTok{(x, }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{)}
\CommentTok{\#\textgreater{}      mean    lwr.ci    upr.ci }
\CommentTok{\#\textgreater{} 10.640789  9.783563 11.498016}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Calcular un intervalo de confianza para la desviación típica de la velocidad del viento desde el 16 de julio, con un nivel de confianza del 90\%.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{VarCI}\NormalTok{(y, }\AttributeTok{conf.level =} \FloatTok{0.9}\NormalTok{))}
\CommentTok{\#\textgreater{}      var   lwr.ci   upr.ci }
\CommentTok{\#\textgreater{} 3.163501 2.795147 3.655468}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  ¿Se puede considerar que las varianzas poblacionales en ambos períodos son iguales, con un nivel de significación del 1\%?.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{VarTest}\NormalTok{(x, y, }\AttributeTok{conf.level =} \FloatTok{0.99}\NormalTok{, }\AttributeTok{alternative =} \StringTok{"two.sided"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  F test to compare two variances}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  x and y}
\CommentTok{\#\textgreater{} F = 1.4062, num df = 75, denom df = 76, p{-}value = 0.1406}
\CommentTok{\#\textgreater{} alternative hypothesis: true ratio of variances is not equal to 1}
\CommentTok{\#\textgreater{} 99 percent confidence interval:}
\CommentTok{\#\textgreater{}  0.7727136 2.5616496}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} ratio of variances }
\CommentTok{\#\textgreater{}           1.406197}
\end{Highlighting}
\end{Shaded}

El estadístico de contraste, \emph{F-Snedecor}, arroja un valor de 1.4062, con un p-valor de 0.1406. Como este p-valor no es pequeño, no hay suficiente evidencia empírica como para rechazar la hipótesis nula de igualdad de varianzas.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{4}
\tightlist
\item
  Teniendo en cuenta los resultados del apartado anterior, ¿se puede afirmar que la velocidad media del viento en el primer período es mayor que la del segundo, con un nivel de significación del 1\%?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(x, y, }\AttributeTok{conf.level =} \FloatTok{0.99}\NormalTok{, }\AttributeTok{alternative =} \StringTok{"greater"}\NormalTok{, }\AttributeTok{var.equal =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Two Sample t{-}test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  x and y}
\CommentTok{\#\textgreater{} t = 2.4212, df = 151, p{-}value = 0.008328}
\CommentTok{\#\textgreater{} alternative hypothesis: true difference in means is greater than 0}
\CommentTok{\#\textgreater{} 99 percent confidence interval:}
\CommentTok{\#\textgreater{}  0.03918338        Inf}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} mean of x mean of y }
\CommentTok{\#\textgreater{} 10.640789  9.283117}
\end{Highlighting}
\end{Shaded}

Un p-valor tan bajo (0.008) indica que existe suficiente evidencia empírica como para rechazar la hipótesis nula de igualdad de medias, por lo que sí se podría afirmar, con un nivel de confianza del 99\%, que la velocidad media del viento en el primer período es superior a la del segundo.

\hypertarget{contrnormalidad}{%
\section{Inferencia estadística no paramétrica: contrastes de normalidad}\label{contrnormalidad}}

Hasta ahora, se ha supuesto que la distribución muestral del estimador era ``funcionalmente'' conocida, aunque dependiente de un parámetro (o varios). Sin embargo, hay situaciones donde no se conoce cómo se distribuyen los datos, debiendo decidir qué distribución los ha generado. Es lo que se conoce como \emph{``inferencia estadística no paramétrica''}. En este capítulo no se aborda un planteamiento sistemático de esta rama, sino que se presenta la situación más habitual en la práctica, que es decidir si se puede mantener que una muestra proviene de una distribución normal, supuesto básico en muchas técnicas estadísticas.

Posiblemente el test más potente para contrastar la normalidad sea la prueba de Shapiro-Wilks, que asume como hipótesis nula que los datos están generados por una distribución normal. Un rechazo de esta hipótesis (p-valor muy bajo) debería hacer reflexionar sobre la adecuación de muchas técnicas y la interpretación de los resultados. En \textbf{R}, la función \texttt{shapiro.test()} proporciona dicho contraste de normalidad.

Una alternativa es el uso del test de Kolmogorov-Smirnov, diseñado para comparar las distribuciones de dos muestras, fijando que una de ellas sea la distribución normal (este test puede ser igualmente utilizado para cualquier otra distribución usual). La función \texttt{ks.test()} permite en \textbf{R} obtener los resultados de este contraste.

Como ejemplificación del uso del test de Shapiro-Wilk en \textbf{R}, utilizando los datos sobre calidad del aire en la ciudad de Nueva York del ejemplo anterior (\ref{ejemplopobnorm}), se va a contrastar si se puede asumir que las variables \emph{Temp} y \emph{Wind} están generadas por distribuciones normales:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shapiro.test}\NormalTok{(airquality}\SpecialCharTok{$}\NormalTok{Temp)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Shapiro{-}Wilk normality test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  airquality$Temp}
\CommentTok{\#\textgreater{} W = 0.97617, p{-}value = 0.009319}
\FunctionTok{shapiro.test}\NormalTok{(airquality}\SpecialCharTok{$}\NormalTok{Wind)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Shapiro{-}Wilk normality test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  airquality$Wind}
\CommentTok{\#\textgreater{} W = 0.98575, p{-}value = 0.1178}
\end{Highlighting}
\end{Shaded}

Para la variable \emph{Temp} el p-valor es muy bajo (0.0093), por lo que hay suficiente evidencia empírica como para rechazar que provenga de una distribución normal. Por el contrario, en el caso de la variable \emph{Wind}, el p-valor (0.1178) no es pequeño, por lo que no hay suficiente evidencia como para rechazar que esté generada por una distribución normal. La fig.~\ref{fig:qqplots} siguiente muestra la comparación entre los cuantiles empíricos de ambas variables y los teóricos de una distribución normal.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/qqplots-1} 

}

\caption{Q-Q Plots normales para las variables Temp (izq) y Wind (der)}\label{fig:qqplots}
\end{figure}

\begin{infobox_resume}
\textbf{RESUMEN}

La inferencia estadística permite estimar la distribución poblacional de una variable a partir de la información suministrada por una muestra. A partir de estimadores muestrales, se plantean los métodos de estimación puntual, construcción de intervalos de confianza y resolución de contrastes de significación sobre diversas hipótesis.

Para el caso de poblaciones normales, se han desarrollado las expresiones operativas de los métodos anteriores. Igualmente, se ha presentado un contraste de normalidad que permite decidir cuándo el supuesto de normalidad es adecuado o no.

\end{infobox_resume}

\hypertarget{muestreo}{%
\chapter{Muestreo y remuestreo}\label{muestreo}}

\emph{Mª Leticia Meseguer Santamaría} y \emph{Manuel Vargas Vargas}

\hypertarget{introducciuxf3n-al-muestreo}{%
\section{Introducción al muestreo}\label{introducciuxf3n-al-muestreo}}

Muchas investigaciones científicas abordan el estudio de características de un conjunto de elementos, que se denomina \emph{``población''}. Sin embargo, no siempre es posible estudiar la totalidad del colectivo, por problemas de accesibilidad, confidencialidad, costes económicos o temporales, etc. En estos casos, se recurre a extraer un subconjunto de la población que se pretende que sea representativo de ésta respecto a las características estudiadas. El objetivo es obtener información relevante que pueda extrapolarse al total de la población, proceso denominado genéricamente ``muestreo''.

En otros casos, la conveniencia de muestrear una población está relacionada con la credibilidad de los resultados obtenidos o de las propiedades de los modelos utilizados. Así, muchas técnicas cuantitativas dividen la información disponible en un subconjunto de \emph{``entrenamiento''} o \emph{``estimación''} y otro de \emph{``contraste''} o \emph{``validación''}; una elección inadecuada puede alterar los resultados e invalidar las conclusiones. Por último, caso muy común en ciencias sociales, no se puede acceder a la medición directa de los fenómenos, por lo que se accede a ella mediante encuestas, que precisa de una metodología de muestreo rigurosa y diseñada previamente.

Para fijar terminología, se define \textbf{población} como el conjunto de casos de interés a los que se quiere generalizar los resultados de la investigación, denominados genéricamente \textbf{individuos}; a veces, sólo es posible acceder a una parte de la población, por lo que se utiliza el término \textbf{población objetivo} para el conjunto total y \textbf{población muestreable} al conjunto al que se tiene acceso. Por coherencia, cuando ambos colectivos no coincidan, los resultados deben extrapolarse sólo al último de ellos.

Las características de interés de la población se consideran \emph{``variables''}, en el sentido estadístico del término, por lo que son estudiadas mediante su distribución. En algunos casos, ésta será completamente desconocida, siendo de interés su determinación completa; en otros muchos, se buscará determinar sólo algunos aspectos (medidas de posición, dispersión, etc.) o los parámetros que rigen una distribución funcionalmente conocida. En todo caso, el objetivo del estudio es determinar la distribución estadística de estas variables, conocida como \emph{``distribución poblacional''}; a veces, por simplicidad en el lenguaje, a esta distribución o, incluso, a las variables, se les denomina población (por ejemplo, no es infrecuente encontrar enunciados del tipo \emph{``la población sigue una distribución binomial''}, identificando población con variable, o \emph{``la población es una distribución normal''}, identificando población con distribución).

Se define \textbf{muestra} como un subconjunto de la población, que será utilizado para caracterizar la distribución poblacional y \textbf{unidad muestral} a cada individuo de la muestra. Si la muestra tiene la misma distribución que la población, se dice que es \textbf{representativa}, mientras que en caso contrario se denomina \textbf{sesgada}.

Siempre será preferible un método de muestreo que proporcione muestras representativas, característica ligada a la forma de seleccionar la muestra y al tamaño de ésta. Así, siempre que sea posible, se recomienda utilizar un \textbf{muestreo probabilístico}, basado en la selección aleatoria de la muestra (conociéndose, por tanto, la probabilidad de que cada individuo salga seleccionado), lo que permite extender los resultados a toda la población, cuantificando posibles sesgos y detallando un error máximo dentro de un nivel de confianza seleccionado al inicio del proceso.

No siempre será posible utilizar un muestreo probabilístico, por lo que existe una colección de métodos de muestreo no probabilístico (conveniencia, bola de nieve, cuotas, etc.), que no se detallarán en este capítulo. La característica común a todos ellos es que los resultados obtenidos de la muestra no se deben extrapolar a la población, ya que no está garantizada la representatividad.

En el resto del capítulo se presentarán brevemente los métodos de muestreo probabilístico más usuales. Para mayor detalle, se pueden consultar las referencias \citet{ChaudhuriStenger2005}, \citet{Arnab2017} o \citet{WuThompson2020}.

\hypertarget{muestreo-aleatorio-simple-1}{%
\section{Muestreo aleatorio simple}\label{muestreo-aleatorio-simple-1}}

El método básico de muestreo es el conocido como \textbf{muestreo aleatorio simple (m.a.s.)}, ya introducido en el capítulo anterior (epígrafe \ref{mas}) consistente en seleccionar totalmente al azar a los individuos de la muestra, por lo que todos tienen la misma probabilidad de formar parte de ella. Si cada individuo sólo puede aparecer una vez en la muestra, se habla de \textbf{muestreo sin reemplazamiento}, mientras que se denomina \textbf{muestreo con reemplazamiento} en caso contrario.

Este procedimiento es el que se asume en la inmensa mayoría de las técnicas estadísticas convencionales, pero presenta dos inconvenientes. En primer lugar, presupone que existe un registro, o listado, completo de todos los individuos de la población (lo que no siempre es posible), y puede resultar costosa (en medios, tiempo y dinero) su aplicación práctica. En segundo lugar, presupone que la característica estudiada es homogénea en todos los individuos de la población, es decir, la distribución poblacional es idéntica en todos los individuos. Frecuentemente, esta homogeneidad poblacional no se cumple, por lo que sería necesario abordar otros métodos de muestreo que se expondrán más adelante. En todo caso, si se utiliza un m.a.s., la heterogeneidad induce un aumento de la variabilidad muestral, hecho que debe ser tenido en cuenta en la interpretación de resultados.

Además de la forma de selección, el factor que determina la representatividad de una muestra es su tamaño (véase el teorema de Glivenko-Cantelli del capítulo anterior (\eqref{eq:glivenko}). Al utilizar la información muestral para aproximar las características poblacionales se comete el llamado \textbf{error muestral}; si el tamaño de la muestra está determinado, este margen de error marca el grado de precisión con el que se pueden extrapolar los resultados. Una alternativa es predeterminar un error muestral a cierto nivel de confianza y calcular cuál es el menor tamaño muestral que cumple ese requisito.

El error muestral, \(\epsilon\), depende de la característica poblacional que se quiera analizar (frecuentemente, la media, el total poblacional o la proporción), del estimador utilizado y del nivel de confianza. Si se asume una distribución poblacional normal, la expresión general sería:
\begin{equation}
\epsilon_\alpha = z_{1-\alpha/2}\sigma(\hat \theta)
\label{eq:error}
\end{equation}
expresión que es frecuentemente extrapolada a distribuciones poblacionales no normales.

Para el caso de estimar la media poblacional utilizando la media muestral, sustituyendo en la ecuación anterior \eqref{eq:error}, la relación entre error muestral, nivel de confianza y tamaño muestral sería:
\begin{equation}
\epsilon_\alpha = z_{1-\alpha/2}\sqrt{(1-{n\over N}) {S^2 \over n}}
\label{eq:error2}
\end{equation}
Operando y despejando el tamaño muestral, se obtiene la expresión:
\begin{equation}
n = {z^2_{1-\alpha/2} NS^2 \over N\epsilon^2_\alpha + z^2_{1-\alpha/2}S^2}
\label{eq:tam}
\end{equation}
En algunos casos se considera que se está muestreando una población de tamaño infinito, lo que produce una simplificación de la fórmula de obtención del tamaño muestral:
\begin{equation}
\epsilon_\alpha = z_{1-\alpha/2}\sqrt{S^2 \over n} \Longrightarrow n = {z^2_{1-\alpha/2} S^2 \over \epsilon^2_\alpha}
\label{eq:error3}
\end{equation}
Si se está interesado en estimar el total poblacional, un procedimiento análogo conduce a las ecuaciones:
\begin{equation}
\epsilon_\alpha = z_{1-\alpha/2}\sqrt{N^2(1-{n\over N}) {S^2 \over n}} \Longrightarrow n = {z^2_{1-\alpha/2} N^2S^2 \over \epsilon^2_\alpha + z^2_{1-\alpha/2}NS^2}
\label{eq:tot}
\end{equation}
Por último, si se desea estimar la proporción poblacional, P, de individuos que cumplen algún criterio, se puede particularizar el caso del estimador de la media poblacional sobre una población binomial, por lo que el resultado obtenido es:\\
\begin{equation}
n = {z^2_{1-\alpha/2} NPQ \over (N-1)\epsilon^2_\alpha + z^2_{1-\alpha/2}PQ}
\label{eq:prop}
\end{equation}
siendo \(Q=1-P\).
En la práctica es muy frecuente que se desconozca la varianza poblacional, por lo que se suele recurrir a alguna estimación previa, con lo que se tiene una aproximación al tamaño muestral requerido.

Para ejemplificar el proceso de obtención de una muestra aleatoria simple en \textbf{R}, se usará la librería \emph{samplingbook} y el conjunto de datos ``iris'', correspondiente a las medidas, en centímetros, de largo y ancho de los sépalos y pétalos de 150 flores, equidistribuidas entre las especias \emph{setosa}, \emph{versicolor} y \emph{virginica} (además de ilustrar este epígrafe, también servirá de ejemplo para otros tipos de muestreo, \ref{muestestra}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(samplingbook)}
\NormalTok{datos\_ej }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

En este caso, por simplicidad, se considerará que la población es el conjunto de las 150 flores disponibles, y que se desea una muestra aleatoria simple sin reemplazamiento para determinar la longitud media de los sépalos con un error de 0.3 centímetros al 95\% de confianza. La función *``sample.size.mean'' permite calcular el tamaño de muestra necesario para cumplir estos requisitos.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sd }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(datos\_ej}\SpecialCharTok{$}\NormalTok{Sepal.Length) }\CommentTok{\# Se considera como la desviación típica poblacional}
\NormalTok{N }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(datos\_ej) }\CommentTok{\# Tamaño de la población}
\NormalTok{e }\OtherTok{\textless{}{-}} \FloatTok{0.3} \CommentTok{\# Margen de error prefijado}
\FunctionTok{sample.size.mean}\NormalTok{(e, sd, N, }\AttributeTok{level =} \FloatTok{0.95}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} sample.size.mean object: Sample size for mean estimate}
\CommentTok{\#\textgreater{} With finite population correction: N=150, precision e=0.3 and standard deviation S=0.8281}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Sample size needed: 25}
\end{Highlighting}
\end{Shaded}

Así, basta con una muestra aleatoria simple de tamaño 25 para poder estimar la longitud media de los sépalos con los requisitos dados. Para obtener la muestra concreta, la función \emph{``sample''} proporciona los valores obtenidos (conjunto de 25 valores aleatorios entre 1 y N=150) y permite seleccionar los casos que conforma la muestra:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{196}\NormalTok{) }\CommentTok{\# Fija la semilla de aleatorización para poder reproducir los resultados}
\NormalTok{muestra }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{N, }\DecValTok{25}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# Si se quisiera un muestreo con reemplazo, se utilizaría la sentencia replace=TRUE}
\NormalTok{datos\_muestra }\OtherTok{\textless{}{-}}\NormalTok{ datos\_ej[muestra, ] }\CommentTok{\# Se seleccionan los datos que conforman la muestra}
\FunctionTok{head}\NormalTok{(datos\_muestra)}
\CommentTok{\#\textgreater{}     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species}
\CommentTok{\#\textgreater{} 122          5.6         2.8          4.9         2.0  virginica}
\CommentTok{\#\textgreater{} 133          6.4         2.8          5.6         2.2  virginica}
\CommentTok{\#\textgreater{} 104          6.3         2.9          5.6         1.8  virginica}
\CommentTok{\#\textgreater{} 95           5.6         2.7          4.2         1.3 versicolor}
\CommentTok{\#\textgreater{} 147          6.3         2.5          5.0         1.9  virginica}
\CommentTok{\#\textgreater{} 73           6.3         2.5          4.9         1.5 versicolor}
\end{Highlighting}
\end{Shaded}

Para finalizar, usando la función \emph{``Smean''} se puede obtener la estimación de la media poblacional, así como su error estándar y un intervalo de confianza. Aunque en la práctica el valor poblacional es desconocido, en este ejemplo sí se puede obtener a partir del conjunto de todos los datos, lo que permite comparar la estimación con el verdadero valor buscado.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{Smean}\NormalTok{(datos\_muestra}\SpecialCharTok{$}\NormalTok{Sepal.Length, N, }\AttributeTok{level =} \FloatTok{0.95}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Smean object: Sample mean estimate}
\CommentTok{\#\textgreater{} With finite population correction: N=150}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Mean estimate: 5.952}
\CommentTok{\#\textgreater{} Standard error: 0.1145}
\CommentTok{\#\textgreater{} 95\% confidence interval: [5.7275,6.1765]}
\FunctionTok{mean}\NormalTok{(datos\_ej}\SpecialCharTok{$}\NormalTok{Sepal.Length) }\CommentTok{\# Valor de la media poblacional}
\CommentTok{\#\textgreater{} [1] 5.843333}
\end{Highlighting}
\end{Shaded}

En este ejemplo, el error cometido sería de \(5.952-5.843=0.109\) cm.

Si interesa estimar el total poblacional, basta con multiplicar la estimación de la media por el tamaño poblacional, N. Por último, si se desea estimar una proporción poblacional, el proceso sería idéntico al descrito, pero usando la función \emph{Sprop}.

\hypertarget{muestestra}{%
\section{Muestreo estratificado}\label{muestestra}}

Como se ha comentado en el epígrafe anterior, uno de los supuestos del muestreo aleatorio simple es que todos los elementos de la población tienen la misma distribución poblacional. Sin embargo, es frecuente que haya \emph{``grupos''} de individuos que presenten características diferenciadas en alguna variable respecto a la característica que se quiere estudiar.

En el ejemplo anterior, se ha muestreado para estimar la longitud media de los sépalos de las 150 flores recogidas en los datos. Sin embargo, al calcular la media y la desviación típica agrupando según la especie:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(plyr)}
\NormalTok{estratos }\OtherTok{\textless{}{-}} \FunctionTok{ddply}\NormalTok{(datos\_ej, .(Species), summarize, }\AttributeTok{media.sl =} \FunctionTok{mean}\NormalTok{(Sepal.Length), }\AttributeTok{desv.sl =} \FunctionTok{sd}\NormalTok{(Sepal.Length))}
\NormalTok{estratos}
\CommentTok{\#\textgreater{}      Species media.sl   desv.sl}
\CommentTok{\#\textgreater{} 1     setosa    5.006 0.3524897}
\CommentTok{\#\textgreater{} 2 versicolor    5.936 0.5161711}
\CommentTok{\#\textgreater{} 3  virginica    6.588 0.6358796}
\end{Highlighting}
\end{Shaded}

puede observarse que las tres especies no se comportan igual respecto al parámetro de interés (longitud media de los sépalos).La especie \emph{setosa} presenta unos valores menores de longitud media desviación típica, mientras que la especie \emph{virginica} presenta los valores más elevados.

Si no se considerara la especie, un muestreo aleatorio simple podría sesgar los resultados, por ejemplo, con un predominio de las setosas o de las virginicas. En todo caso, la variabilidad del conjunto de datos es más elevada que en cualquiera de las especies, pues a la variación dentro de cada especie se une la variación entre especies. Este hecho hace aumentar el tamaño muestral necesario para estimar con un margen de erro prefijado.

En general, cuando existen grupos de individuos con un comportamiento más homogéneo dentro del grupo y diferenciado entre grupos, no resulta apropiado aplicar un m.a.s. En estos casos, es recomendable el denominado \textbf{muestreo estratificado}, donde se realiza previamente una partición de la población en \textbf{estratos} y se selecciona una m.a.s. dentro de cada grupo.

La estratificación presenta ventajas, como el aumento de la representatividad de la muestra (se necesita un menor tamaño muestral que en el m.a.s.), la reducción del error muestral (la variabilidad es menor en cada estrato) y el incremento de probabilidad de representación en la muestra de grupos con características diferenciadas. Por el contrario, no siempre resulta evidente la relación entre la variable de estratificación y las de interés.

Una de las decisiones que se han de tomar en el muestreo estratificado es el reparto de tamaño muestral entre los distintos estratos, procedimiento conocido como \textbf{afijación}. Las dos opciones más utilizadas son la \textbf{afijación proporcional}, que reparte el tamaño muestral en función de los tamaños poblacionales de cada estrato, y la \textbf{afijación óptima}, que considera también los diferentes valores de la variabilidad dentro de cada estrato.

Para ejemplificar el proceso de muestreo estratificado en el caso de la base de datos utilizada, se procede a considerar cada especie de iris como un estrato, ya que se ha comprobado que presentan distintas distribuciones poblacionales respecto a la variable \emph{longitud del sépalo}. Como en el epigrafe anterior, se quiere estimar la longitud media de la variable con un margen de error de 0.3 al 95\% de confianza. Dentro de la librería \emph{samplingbook} se puede utilizar la función \emph{stratasize} para determinar el tamaño muestral que cumple estos requisitos

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{stratasize}\NormalTok{(e, }\AttributeTok{Nh =} \FunctionTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{50}\NormalTok{), }\AttributeTok{Sh =}\NormalTok{ estratos[, }\DecValTok{3}\NormalTok{], }\AttributeTok{level =} \FloatTok{0.95}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} stratamean object: Stratified sample size determination}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} type of sample: prop}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} total sample size determinated: 11}
\end{Highlighting}
\end{Shaded}

Como se aprecia, para garantizar al 95\% de confianza un margen de error de 0.3 cm es necesario un tamaño muestral de 11, sensiblemente inferior al requerido con un m.a.s. (25). Una vez determinado el tamaño, el criterio de afijación elegido distribuye la muestra entre los estratos.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{stratasamp}\NormalTok{(}
  \AttributeTok{n =} \DecValTok{11}\NormalTok{, }\AttributeTok{Nh =} \FunctionTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{50}\NormalTok{), }\AttributeTok{Sh =}\NormalTok{ estratos[, }\DecValTok{3}\NormalTok{], }\AttributeTok{type =} \StringTok{"prop"}
\NormalTok{)}
\CommentTok{\#\textgreater{}              }
\CommentTok{\#\textgreater{} Stratum 1 2 3}
\CommentTok{\#\textgreater{} Size    4 4 4}
\FunctionTok{stratasamp}\NormalTok{(}
  \AttributeTok{n =} \DecValTok{11}\NormalTok{, }\AttributeTok{Nh =} \FunctionTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{50}\NormalTok{), }\AttributeTok{Sh =}\NormalTok{ estratos[, }\DecValTok{3}\NormalTok{], }\AttributeTok{type =} \StringTok{"opt"}
\NormalTok{)}
\CommentTok{\#\textgreater{}              }
\CommentTok{\#\textgreater{} Stratum 1 2 3}
\CommentTok{\#\textgreater{} Size    3 4 5}
\end{Highlighting}
\end{Shaded}

Como los tres estratos tienen el mismo tamaño poblacional, la afijación proporcional distribuye la muestra equitativamente; sin embargo, la afijación óptima, al considerar las diferencias en variabilidad, asigna más muestra al estrato con mayor variabilidad y menos muestra al de menor variabilidad (como los tamaños muestrales son necesariamente números enteros, se puede producir una ligera diferencia entre el tamaño muestral calculado globalmente y la suma de los tamaños de cada estrato).

Con la afijación óptima estimada, se procede a la selección de la submuestra en cada estrato (m.a.s.) y a la obtención de los datos que conforman la muestra.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{196}\NormalTok{) }\CommentTok{\# Fija la semilla de aleatorización}
\NormalTok{muestra1 }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{50}\NormalTok{, }\DecValTok{3}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{muestra2 }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{51}\SpecialCharTok{:}\DecValTok{100}\NormalTok{, }\DecValTok{4}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{muestra3 }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{101}\SpecialCharTok{:}\DecValTok{150}\NormalTok{, }\DecValTok{5}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# m.a.s. en cada estrato}
\NormalTok{muestra\_estr }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(muestra1, muestra2, muestra3)}
\NormalTok{datos\_muestra\_estr }\OtherTok{\textless{}{-}}\NormalTok{ datos\_ej[muestra\_estr, ] }\CommentTok{\# Selección de los datos que conforman la muestra}
\NormalTok{datos\_muestra\_estr}
\CommentTok{\#\textgreater{}     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species}
\CommentTok{\#\textgreater{} 5            5.0         3.6          1.4         0.2     setosa}
\CommentTok{\#\textgreater{} 40           5.1         3.4          1.5         0.2     setosa}
\CommentTok{\#\textgreater{} 31           4.8         3.1          1.6         0.2     setosa}
\CommentTok{\#\textgreater{} 90           5.5         2.5          4.0         1.3 versicolor}
\CommentTok{\#\textgreater{} 81           5.5         2.4          3.8         1.1 versicolor}
\CommentTok{\#\textgreater{} 59           6.6         2.9          4.6         1.3 versicolor}
\CommentTok{\#\textgreater{} 58           4.9         2.4          3.3         1.0 versicolor}
\CommentTok{\#\textgreater{} 137          6.3         3.4          5.6         2.4  virginica}
\CommentTok{\#\textgreater{} 136          7.7         3.0          6.1         2.3  virginica}
\CommentTok{\#\textgreater{} 132          7.9         3.8          6.4         2.0  virginica}
\CommentTok{\#\textgreater{} 107          4.9         2.5          4.5         1.7  virginica}
\CommentTok{\#\textgreater{} 121          6.9         3.2          5.7         2.3  virginica}
\end{Highlighting}
\end{Shaded}

Finalmente, usando la función \emph{``Smean''} se obtiene la estimación de la media poblacional, así como su error estándar y un intervalo de confianza, al igual que se hizo con el m.a.s.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{Smean}\NormalTok{(datos\_muestra\_estr}\SpecialCharTok{$}\NormalTok{Sepal.Length, N, }\AttributeTok{level =} \FloatTok{0.95}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Smean object: Sample mean estimate}
\CommentTok{\#\textgreater{} With finite population correction: N=150}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Mean estimate: 5.925}
\CommentTok{\#\textgreater{} Standard error: 0.3108}
\CommentTok{\#\textgreater{} 95\% confidence interval: [5.3158,6.5342]}
\end{Highlighting}
\end{Shaded}

\hypertarget{otros-tipos-de-muestreo-probabiluxedstico}{%
\section{Otros tipos de muestreo probabilístico}\label{otros-tipos-de-muestreo-probabiluxedstico}}

Existen otros métodos de muestreo probabilístico que buscan simplificar la extracción de una muestra representativa, entre los que destacan el \textbf{muestreo por conglomerados} y el \textbf{muestreo sistemático}.

Cuando la población es muy grande, es frecuente que se puedan establecer (o construir a partir de alguna variable) subgrupos, o \textbf{clusters}, que tengan las mismas características que todo el conjunto respecto a la variable de interés. En esos casos, a efectos de estimación, sería equivalente muestrear toda la población o sólo un cluster, con el consiguiente ahorro de tamaño muestral, tiempo y coste. Es el conocido como \textbf{muestreo por conglomerados}.

Por ejemplo, se puede estar interesado en estimar el tiempo medio que el alumnado de E.S.O. dedica a estudiar matemáticas en España. Obtener una muestra para todo el pais puede ser costoso en tiempo, recursos materiales y tamaño muestral; sin embargo, se puede asumir que no existen diferencias entre provincias respecto a esta variable, por lo que sería posible muestrear sólo en una provincia (o pocas). En este caso, el muestreo por conglomerados consistiría en una primera etapa de selección aleatoria de clusters (provincia/s en este ejemplo) y, posteriormente, aplicar un método de muestreo sobre dicha selección (que, a su vez, podría ser un m.a.s. o un muestreo estratificado).

No conviene confundir los conceptos de estrato y cluster, aunque ambos sean subgrupos de la población total. En el primer caso, los individuos de cada estrato son muy homogéneos entre sí y diferenciados del resto de estratos. En el segundo caso, los individuos de cada cluster tienen la misma variabilidad que el conjunto de la población, no habiendo diferencias entre clusters respecto a la variable de interés. Así, la ganancia en el muestreo estratificado proviene de trabajar con menores variabilidades intra-estratos, mientras que en el muestreo por conglomerados proviene de utilizar una subpoblación más pequeña.

En otras situaciones, si se dispone de un marco poblacional (listado completo de los individuos), es posible plantear un mecanismo sencillo de obtención de la muestra. Si se tiene un tamaño poblacional N y se quiere una muestra de tamaño n, se pueden establecer \(k=N/n\) bloques, elegir al azar un número entre 1 y \(k\) (que permite seleccionar el primer elemento de la muestra) y, a partir de esa posición, dar saltos de magnitud \(k\) en el listado para seleccionar el resto de unidades muestrales. Es el método conocido como \textbf{muestreo sistemático} o, más técnicamente, \textbf{muestreo sistemático uniforme de paso k}.

Como ejemplo, supóngase que se quiere obtener una muestra de 50 individuos en una población de 2000. El paso sería \(k=2000/50=40\) unidades y se selecciona aleatoriamente una unidad entre las 40 primeras (supóngase que la número 13); el resto de la muestra se obtendría sumándole el paso a la primera seleccionada (13, 53, 93, 133, 173, y así hasta la 1973).

Por último, los métodos expuestos no son incompatibles, sino que se pueden combinar por etapas, dando lugar a los conocidos como \textbf{muestreos polietápicos''}. Por ejemplo, la Encuesta de Población Activa, elaborada por el Instituto Nacional de Estadística, adopta un muestreo bietápico, en primer lugar, estratificado entre secciones censales y, en segundo lugar, muestreando entre las viviendas familiares de cada sección.

\hypertarget{tuxe9cnicas-de-remuestreo-bootstrap.}{%
\section{Técnicas de remuestreo: Bootstrap.}\label{tuxe9cnicas-de-remuestreo-bootstrap.}}

Cuando se infiere una característica poblacional a partir de una muestra, sólo se dispone del valor concreto que el estadístico toma sobre dicha muestra. Salvo en raras ocasiones, no se dispone de su distribución en el muestreo, o sólo se tiene una aproximación asintótica, por lo que no se pueden evaluar sus propiedades estadísticas con tamaños muestrales no elevados. En otros casos, es la complejidad analítica de muchas técnicas actuales de análisis de datos la que dificulta la determinación de la distribución de las estimaciones de los parámetros.

En estos casos, el método \textbf{bootstrap} propone sustituir la distribución poblacional (desconocida) por una estimación conocida (como puede ser la distribución empírica o una aproximación paramétrica) que, mediante remuestreo, sirva para generar muestras aleatorias a partir de la muestra original. Se obtiene así una distribución de remuestreo, llamada también \textbf{distribución bootstrap}, cuyo comportamiento sobre la estimación aproxima a la de la distribución muestral en torno al parámetro, lo que permite evaluar la precisión de las estimaciones.

El método bootstrap más sencillo, llamado \textbf{bootstrap uniforme} o \textbf{bootstrap naïve}, parte de la aproximación de la distribución poblacional por la distribución empírica de la muestra. Supóngase que se tiene una muestra \(X=(x_1,...,x_n)\) que es utilizada para obtener un estimador \(T(X)=\hat{\theta}\) para un parámetro poblacional \(\theta\). Su distribución empírica (véase \eqref{eq:distrempirica}) es:
\begin{equation}
\hat{\mathbb{F}}_n (x)= {1 \over n} \sum_{i=1}^n \mathbb{I}_{(-\infty ,x]}(X_i)
\end{equation}
donde, para cada valor \emph{x}, se obtiene la proporción de elementos de la muestra que toman un valor menor o igual que \emph{x}. Con esta distribución,

\begin{itemize}
\item
  Se genera una primera muestra \(X^{*1}=(x_1^{*1},...,x_n^{*1})\), obtenida mediante muestreo aleatorio simple con reemplazamiento de la muestra original, que permite evaluar es estadístico \(T^{*1}(X^{*1})=\hat{\theta^{*1}}\).
\item
  Siguiendo el mismo procedimiento, se puede generar un número elevado (B) de muestras bootstrap, \(X^{*1}, ..., X^{*B}\), que permiten obtener el valor del estadístico sobre cada una de ellas \(T^{*1}(X^{*1}), ..., T^{*B}(X^{*B})\)
\item
  Con estos valores, se obtiene la distribución bootstrap. Por ejemplo, se puede utilizar esta distribución para calcular la media bootstrap del estadístico \(T(X)\), \(\bar {T}^*= {1 \over B} \sum_{b=1}^B T^*(X^{*b})\), y cuyo error estándar es:
  \begin{equation}
  \hat{Se}_{boot}= \sqrt{{1 \over {B-1}} \sum_{b=1}^B \left ( T(X^{*b})-\bar{T}^* \right ) ^2}
  \label{eq:error4}
  \end{equation}
  expresión que aproxima al error del estadístico \(T(X)\) para estimar \(\theta\).
\end{itemize}

\textbf{Ejemplo:} Los datos sobre calidad del aire en la ciudad de Nueva York (\emph{airquality}) recogen la variable \emph{``Temp''} que mide la temperatura, en grados Fahreheit, entre el día 1 de mayo y el 30 de septiembre. Como se ha visto en el capítulo anterior, epígrafe \ref{ejemplopobnorm}, no se puede asumir que dicha variable esté generada por una distribución normal, por lo que no se podrían utilizar los intervalos de confianza mostrados en el epígrafe \ref{pobnormales}.
El objetivo es calcular una estimación de la temperatura media y dar un intervalo al 95\% de confianza.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{media }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(airquality}\SpecialCharTok{$}\NormalTok{Temp)}
\end{Highlighting}
\end{Shaded}

En este ejemplo, se utiliza la muestra dara obtener un valor del estadístico \emph{media muestral} (\(\bar X = 77.88\)), estimador insesgado de la media poblacional. Sin embargo, al no conocer la distribución en el muestreo (no se asume ningún tipo de distribución poblacional ni se puede hacer uso de aproximaciones asintóticas), no se podría construir un intervalo de confianza.

Aplicando el método bootstrap (en su versión uniforme), se van a obtener 5000 muestras de tamaño 20, mediante remuestreo con reemplazamiento,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{196}\NormalTok{) }\CommentTok{\# Se fija la semilla para permitir la reproducibilidad}
\NormalTok{B }\OtherTok{\textless{}{-}} \DecValTok{5000} \CommentTok{\# Se fija el número de remuestras}
\NormalTok{muestras\_boot }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(B) }\CommentTok{\# Se almacenan todos los valores del estadístico}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{B) \{}
\NormalTok{  remuestra }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(airquality}\SpecialCharTok{$}\NormalTok{Temp, }\DecValTok{20}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  muestras\_boot[k] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(remuestra)}
\NormalTok{\}}
\NormalTok{media\_boot }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(muestras\_boot)}
\NormalTok{media\_boot }\CommentTok{\# Media de los 5000 valores medios de las remuestras}
\CommentTok{\#\textgreater{} [1] 77.87309}
\NormalTok{desv\_boot }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(muestras\_boot)}
\NormalTok{desv\_boot }\CommentTok{\# Desviación típica de los 5000 valores medios de las remuestras}
\CommentTok{\#\textgreater{} [1] 2.090255}
\end{Highlighting}
\end{Shaded}

Para construir los intervalos de confianza, se calculan los valores críticos al 95\% de confianza a partir de las remuestras tipificadas y los extremos del intervalo:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tip\_boot }\OtherTok{\textless{}{-}}\NormalTok{ (muestras\_boot }\SpecialCharTok{{-}}\NormalTok{ media\_boot) }\SpecialCharTok{/}\NormalTok{ desv\_boot }\CommentTok{\# Remuestras tipificadas}
\NormalTok{val\_crit }\OtherTok{\textless{}{-}} \FunctionTok{quantile}\NormalTok{(tip\_boot, }\FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{))}
\NormalTok{val\_crit }\CommentTok{\# Valores críticos}
\CommentTok{\#\textgreater{}      2.5\%     97.5\% }
\CommentTok{\#\textgreater{} {-}1.972530  1.950437}
\NormalTok{ic\_inf }\OtherTok{\textless{}{-}}\NormalTok{ media\_boot }\SpecialCharTok{{-}}\NormalTok{ val\_crit[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ desv\_boot}
\NormalTok{ic\_sup }\OtherTok{\textless{}{-}}\NormalTok{ media\_boot }\SpecialCharTok{{-}}\NormalTok{ val\_crit[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ desv\_boot}
\NormalTok{ic\_boot }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(ic\_inf, ic\_sup)}
\NormalTok{ic\_boot}
\CommentTok{\#\textgreater{}    97.5\%     2.5\% }
\CommentTok{\#\textgreater{} 73.79618 81.99618}
\end{Highlighting}
\end{Shaded}

Así, con el método bootstrap, no es necesario asumir ninguna distribución en el muestreo. Aún así, la representación gráfica de la distribución empírica de los valores medios remuestreados y los extremos del intervalo de confianza, muestra cómo la distribución de las remuestras sobre el estadístico se asemeja a la distribución del estadístico sobre el parámetro.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/dist-1} 

}

\caption{Distribución empírica de la media remuestreada}\label{fig:dist}
\end{figure}

La libreria \emph{``boot''} de R permite también obtener réplicas de un estadístico sobre una muestra. La función básica de esta librería es \emph{boot}, que permite utilizar distintos métodos de remuestreo. En su estructura más simple, basta con indicar los datos originales, el estadístico que se quiere remuestrear y el número de réplicas.
Así, si se quiere estimar, por ejemplo, la mediana de la población con 1000 remuestras, se puede recurrir a la función \emph{boot}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(boot)}
\NormalTok{estadistico }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, i) \{}
  \FunctionTok{median}\NormalTok{(data[i]) }\CommentTok{\# Se especifica aquí el estadístico remuestreado}
\NormalTok{\}}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{196}\NormalTok{)}
\NormalTok{mediana\_boot }\OtherTok{\textless{}{-}} \FunctionTok{boot}\NormalTok{(airquality}\SpecialCharTok{$}\NormalTok{Temp, estadistico, }\AttributeTok{R =} \DecValTok{1000}\NormalTok{)}
\NormalTok{mediana\_boot}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} ORDINARY NONPARAMETRIC BOOTSTRAP}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} boot(data = airquality$Temp, statistic = estadistico, R = 1000)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Bootstrap Statistics :}
\CommentTok{\#\textgreater{}     original  bias    std. error}
\CommentTok{\#\textgreater{} t1*       79  {-}0.052    1.071655}
\end{Highlighting}
\end{Shaded}

Como resultado, se obtiene el valor del estadístico sobre la muestra original, el sesgo estimado y el error estándar.
Para calcular los intervalos de confianza, basta con utilizar la función \emph{boot.ci} sobre la muestra bootstrap obtenida, indicando el nivel de confianza y el tipo de intervalo (por defecto, \emph{``all''} proporciona todos los intervalos disponibles; en el ejemplo, se usa el método de los percentiles)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boot.ci}\NormalTok{(mediana\_boot, }\AttributeTok{conf =} \FloatTok{0.95}\NormalTok{, }\AttributeTok{type =} \StringTok{"perc"}\NormalTok{)}
\CommentTok{\#\textgreater{} BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS}
\CommentTok{\#\textgreater{} Based on 1000 bootstrap replicates}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} CALL : }
\CommentTok{\#\textgreater{} boot.ci(boot.out = mediana\_boot, conf = 0.95, type = "perc")}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Intervals : }
\CommentTok{\#\textgreater{} Level     Percentile     }
\CommentTok{\#\textgreater{} 95\%   (77, 81 )  }
\CommentTok{\#\textgreater{} Calculations and Intervals on Original Scale}
\end{Highlighting}
\end{Shaded}

La función \emph{boot} permite modificaciones del bootstrap uniforme mediante parámetros adicionales. Por ejemplo, el parámetro \emph{strata} se utiliza para generar remuestreos estratificados cuando la muestra original también lo es. Aunque no se han comentado dado el carácter introductorio de este capítulo, existen otros métodos bootstrap, que se pueden obtener especificándolos mediante el parámetro \emph{sim}. Por defecto, el valor es \emph{``ordinary''}, que corresponde al bootstrap uniforme; otras alternativas pueden ser \emph{``parametric''} para bootstrap paramétrico, \emph{``balanced''}, \emph{``permutation''} o \emph{``antithetic''} para otros métodos más avanzados.

\begin{infobox_resume}
\textbf{RESUMEN}

El muestreo probabilístico busca seleccionar una muestra representativa de una población, que permita inferir la distribución poblacional o alguno de sus parámetros. Las decisiones básicas para un correcto proceso de muestreo son el método utilizado (aleatorio simple, estratificado, polietápico, etc.), que depende de la estructura de la población, y la determinación del tamaño muestral que garantice el margen de error asumible.
La técnica \textbf{bootstrap} de remuestreo permite aproximar la distribución de estadísticos muestrales sin asumir ninguna hipótesis sobre la distribución poblacional, ventaja muy útil para evaluar la precisión de los estimadores en muchísimas técnicas complejas.

\end{infobox_resume}

\hypertarget{part-modelizaciuxf3n-estaduxedstica}{%
\part{Modelización estadística}\label{part-modelizaciuxf3n-estaduxedstica}}

\hypertarget{cap-lm}{%
\chapter{Modelización lineal}\label{cap-lm}}

\emph{Víctor Casero y María Durbán}

\hypertarget{modelizaciuxf3n}{%
\section{Modelización}\label{modelizaciuxf3n}}

Se acude a modelos de regresión \index{modelo de regresión} para intentar explicar la relación entre dos o más variables. Para ello se predefine un modelo que pretende explicar el comportamiento de la variable respuesta \index{variable respuesta} o dependiente, denotada por \(Y\), utilizando la información proporcionada por las variables explicativas \index{variable explicativa}(independientes, predictoras\ldots{} denotadas por \(X_1,\ldots,X_k\)).
Pero dichas variables pueden ser de muy distinto tipo. Si la variable respuesta es continua, más concretamente, si se puede asumir que sigue una distribución normal\index{distribución normal}, y al menos una de las variables explicativas es también continua, se puede acudir a la modelización lineal que se desarrolla en este capítulo. Sin embargo, si la variable respuesta fuese de otro tipo, por ejemplo, dicotómica, la modelización lineal explicada en este Capítulo no sería adecuada. En el Capítulo 17 del modelo lineal generalizado \index{modelo lineal generalizado, GLM} quedará más clara esta distinción.

El primer paso en este proceso de modelización es intentar explicar una variable respuesta, que a partir de aquí se supone continua y con distribución normal, a partir de una sola de las variable explicativas, de forma \emph{lineal}\footnote{La relación entre las variables viene determinada por una combinación \emph{lineal} de los parámetros. En contraposición se encuentran los modelos \emph{no lineales}\index{modelo no lineal}, como por ejemplo: \(Y = \beta_0\cdot e^{\beta_1 X_1}\).}.
Dicho modelo probablemente no será bueno, no explicará bien la variable respuesta, esto es lo lógico si la realidad que se pretende explicar es compleja, pero podría ser suficiente para el propósito del estudio\footnote{Ayudará a ello obtener el coeficiente de \emph{bondad de ajuste} (se verá en la Sección \ref{Bondad}).}.
Por ejemplo, se sabe que el peso de una persona está relacionado con muchos factores, pero uno de los más determinantes es la altura.
Si se recogen datos de pesos y alturas de un conjunto de personas se puede ajustar el modelo y obtener una explicación ``suficiente'', aunque parcial, del peso de una persona a partir de su altura.
Es claro que la inclusión de otras variables en el modelo puede ayudar a ``explicar'' mejor la variable respuesta. Se llega así al denominado \emph{modelo de regresión lineal múltiple}\index{modelo de regresión lineal múltiple} que se puede expresar matemáticamente como:
\begin{equation}
Y_i = \beta_0 + \beta_1 X_{1i} + \ldots + \beta_k X_{ki} + \epsilon_i, \quad \epsilon_i \sim \mathcal{N}(0, \sigma^2),
\label{eq:lm}
\end{equation}
donde

\begin{itemize}
\tightlist
\item
  \(\beta_0\) es el \emph{término independiente}\index{término independiente} o \emph{constante del modelo},
\item
  \(\beta_1, \ldots, \beta_k\) son los \emph{coeficientes de regresión}\index{coeficientes de regresión} o \emph{parámetros} del modelo, que se obtendrán a partir del conocimiento de los datos observados \((x_{1i}, \ldots, x_{k_i},y_i)\) y reflejan la magnitud del efecto \emph{lineal} que las variables explicativas \(X_i\) tienen sobre la variable explicada \(Y\),
\item
  y \(\epsilon_i\) es el término de error del modelo\index{error del modelo}, lo que no es capaz de explicar la parte determinista del mismo: \(\beta_0 + \beta_1 x_{1i} + \ldots + \beta_k x_{ki}\), y que se supone sigue una distribución normal de probabilidad, con media 0 y varianza constante \(\sigma^2\).
\item
  Además se asume que las observaciones son independientes\index{observaciones independientes}.
\end{itemize}

Siguiendo con el ejemplo del peso, añadir alguna variable genética, o el sexo, ayudará a mejorar la ``bondad'' del modelo lineal.
Otro ejemplo consiste en pretender explicar el salario en un determinado sector económico en función de\ldots{} años de experiencia, formación, situación familiar, sexo, etc.
Nótese que entre las variables explicativas sí puede haber variables de distinto tipo, continuas, categóricas, etc\footnote{Conviene mencionar la estrecha relación entre regresión lineal múltiple y ANOVA\index{ANOVA}. Ésta última técnica es un caso particular de la primera, concretamente cuando todas las variables explicativas son categóricas (véase la Sección \ref{anova}).}. Ahora bien, la interpretación de los coeficientes dependerá del tipo de variable al que va asociado, como se verá en los casos prácticos (Sección \ref{Casos}).

\hypertarget{procedimiento-de-modelizaciuxf3n}{%
\section{Procedimiento de modelización}\label{procedimiento-de-modelizaciuxf3n}}

\hypertarget{Bondad}{%
\subsection{Estimación del modelo}\label{Bondad}}

Los datos recogidos u observados sirven para \textbf{especificar} la relación predefinida de antemano, mediante la \textbf{estimación} de los coeficientes \(\beta_i\) que mejor ajustan dicha relación, utilizando el método de \emph{mínimos cuadrados}\index{mínimos cuadrados}.
Además, se hacen los correspondientes contrastes para decidir si cada coeficiente es \textbf{significativamente distinto de 0}, esto es, tiene un \emph{efecto} significativo\index{significativo} sobre la respuesta\footnote{desde el punto de vista estadístico \emph{la influencia/efecto sobre la respuesta no es fruto del azar}.} por lo que tiene sentido mantenerlo en el modelo.
En la práctica el coeficiente es \emph{significativo} si su \textbf{p-valor}\index{p-valor} asociado es lo suficientemente pequeño\footnote{Se acepta mayoritariamente un p-valor inferior a 0.05, lo que supone un nivel de confianza de las estimaciones del 95\%. Pero dicho valor es arbitrario. Podría considerarse un nivel de confianza del 99\% y entonces sería significativo si el p-valor fuese inferior a 0.01. En general, cuanto menor sea el p-valor más confianza se tendrá en las conclusiones. Si está próximo, aunque sea menor de 0.05 ó 0.01, hay que tomar las conclusiones con reservas, igual que si es ligeramente superior.}.
Si no lo es, es oportuna la eliminación de dicha variable explicativa del modelo.
En tal caso se vuelven a estimar los coeficientes de las variables que se mantienen hasta llegar a un modelo con todos los coeficientes significativos, iterando las veces necesarias. No obstante se han desarrollado métodos automáticos de selección de variables, basados en la comparación de la varianza residual haciendo uso del test F, mediante el estadístico AIC (\emph{Criterio de Información de Akaike}), etc. \footnote{Se puede consultar el Capítulo 10 de \citet{faraway2002practical}}.
Junto con los contrastes se pueden obtener los intervalos de confianza de los coeficientes, que si son significativos no contendrán el valor 0.

A la par del contraste de significación de cada coeficiente se obtiene el contraste de significación global del modelo.
La hipótesis nula es que todos los coeficientes \(\beta_1, \ldots, \beta_k\) son 0, dicho de otro modo, que el conocimiento de las variables \(X_1, \ldots, X_k\) no aporta mejor información para explicar los valores de \(Y\) que su simple media de las respuestas.

También se ha de obtener la \textbf{bondad del ajuste}\index{bondad del ajuste} del modelo, típicamente proporcionando el coeficiente de determinación (corregido), \(R^2\)\index{R cuadrado} (adimensional, que toma valores entre 0 y 1).
Cuanto mejor explique el modelo los datos muestrales, más próximo a 1 será el \(R^2\), indicando el 1 una linealidad perfecta de los datos. Por el contrario, un \(R^2\) cercano a 0 indica un mal ajuste de los datos utilizando el modelo de regresión estimado.

Es habitual valorar conjuntamente la significación global del modelo, su bondad de ajuste y la significación de cada uno de los coeficientes, considerando apropiados aquellos modelos que son \emph{significativos} y tienen la ``suficiente'' \emph{bondad}, aunque tengan coeficientes no significativos.

\hypertarget{validaciuxf3n-del-modelo}{%
\subsection{Validación del modelo}\label{validaciuxf3n-del-modelo}}

Aunque el modelo sea significativo se debe \emph{validar}, es decir, comprobar los supuestos estadísticos que subyacen al modelo. Para ello se utilizan los \textbf{residuos}\index{residuos} del modelo, lo que queda sin explicar por la regresión estimada, la diferencia entre el valor observado y el estimado. Matemáticamente,
\[e_i = Y_i - \hat{Y}_i = Y_i - (\hat{\beta}_0 +\hat{\beta}_1 X_{1i} + \ldots + \hat{\beta}_k X_{ki})\]

Los supuestos a comprobar son:

\begin{itemize}
\tightlist
\item
  Los residuos han de tener \textbf{varianza constante} (tienen por definición \textbf{media cero}).
\item
  Los residuos han de seguir la \textbf{distribución} de probabilidad \textbf{Normal}.
\item
  Las observaciones tienen que ser \textbf{independientes}.
\item
  La \textbf{relación} entre la variable respuesta y las explicativas se supone \textbf{lineal}\footnote{Podrían estudiarse relaciones no lineales de las variables, pero lineales en los parámetros, como: \(Y = \beta_0 + \beta_1 X + \beta_2 X^2\).}.
\item
  Las variables explicativas son linealmente independientes: ninguna puede ser explicada como combinación lineal de las otras. Si se diese el caso se tendría el conocido problema de la \emph{multicolinealidad}\index{multicolinealidad} y debería quitarse del modelo la variable explicada por el resto.
\end{itemize}

\hypertarget{interpretaciuxf3n-de-los-coeficientes}{%
\subsection{Interpretación de los coeficientes}\label{interpretaciuxf3n-de-los-coeficientes}}

Una vez validado el modelo se pasan a interpretar los coeficientes significativos.
La regla general de interpretación de cada coeficiente de regresión estimado \(\hat{\beta}_i\) es, el cambio \emph{medio} en el valor de la variable respuesta\ldots{}

\begin{itemize}
\tightlist
\item
  por cada cambio unitario de la variable explicativa, si es \emph{continua}.
\item
  al cambiar el nivel de la variable explicativa, desde uno de los niveles que se toma como referencia, si la variable es \emph{categórica}.
\end{itemize}

\ldots{} y \emph{ceteris paribus}, esto es, manteniendo constante el valor de las demás variables.

Habrá que tener en cuenta las magnitudes de cada variable, porque la influencia real en la respuesta podría ser de poca magnitud (quizá por las unidades o escala utilizada), pero significativa estadísticamente.

\hypertarget{predicciuxf3n}{%
\subsection{Predicción}\label{predicciuxf3n}}

La utilidad del modelo estimado (especificado) queda plasmada en su utilización para \textbf{predecir}\index{predicción del modelo lineal} nuevos valores, \(\hat{Y_i}\), a partir del conocimiento/asignación de los valores de las variables explicativas, \(X_1, \ldots, X_k\). No obstante dichas predicciones corresponden al valor \emph{medio}, como se ha indicado, y se pueden dar sus correspondiente intervalos de confianza, tanto para predicciones individuales como para predicciones de la media.

\hypertarget{procedimiento-con-r-la-funciuxf3n-lm}{%
\section{\texorpdfstring{Procedimiento con R: la función \texttt{lm}}{Procedimiento con R: la función lm}}\label{procedimiento-con-r-la-funciuxf3n-lm}}

R tiene implementada la función \texttt{lm} \index{lm} para ajustar/estimar modelos de regresión lineal múltiple.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(formula, }\AttributeTok{data =}\NormalTok{ ..., ...)}
\end{Highlighting}
\end{Shaded}

El argumento mínimo necesario es \texttt{formula} donde se predeterminará la relación entre las variables respuesta y explicativas de una forma bastante intuitiva:

\begin{itemize}
\tightlist
\item
  \texttt{Y\ \textasciitilde{}\ X}, es la fórmula a utilizar para definir un modelo \emph{simple}
  donde \texttt{Y} denota la variable respuesta y \texttt{X} denota la variable explicativa: \(Y = \beta_0 + \beta_1 X\).
\item
  \texttt{Y\ \textasciitilde{}\ X1\ +\ X2}, define un modelo lineal \emph{múltiple} con 2 variables explicativas: \(Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2\).
\item
  \texttt{Y\ \textasciitilde{}\ X1\ +\ X2\ +\ X3\ -\ 1} elimina del modelo lineal múltiple (de 3 variables) el término independiente, \(\beta_0\).
\item
  \ldots{}
\end{itemize}

Si las variables no se encuentran en el \texttt{Environment} se le debe especificar en \texttt{data\ =} el conjunto de datos donde se encuentran.

Hay que hacer notar que \texttt{R} considera, por defecto, las variables explicativas como cuantitativas. Si se tienen variables categóricas codificadas con números hay que indicarle que la trate como categórica, usando la función \texttt{factor}\footnote{Si no está ya definida como \texttt{factor} en el conjunto de datos.}. De no hacerlo, el procedimiento la consideraría numérica, con el consecuente error de interpretación del coeficiente asociado.

A partir de los datos disponibles de \texttt{Y,\ X1,\ ...}, mediante el \emph{método de mínimos cuadrados}\index{mínimos cuadrados}, la función \texttt{lm} estimará los coeficientes \(\beta_i\) asociados a cada variable \texttt{Xi}, calculará sus errores estándar con el que obtiene sus estadísticos de contraste (de la \emph{t de Student})\footnote{Con los errores estándar también se pueden obtener los intervalos de confianza de los coeficientes} y su significación. En el objeto ``lm'' que se genera también almacena valores ajustados, residuos, etc. que se pueden mostrar a través de funciones genéricas disponibles en \texttt{R}. Algunas de ellas son:

\begin{itemize}
\tightlist
\item
  \texttt{print}: muestra un resumen corto
\item
  \texttt{summary}: resumen completo
\item
  \texttt{coef}: estimaciones de los coeficientes del modelo
\item
  \texttt{confint}: intervalos de confianza de los coeficientes
\item
  \texttt{fitted.values}: valores ajustados del modelo (para cada observación del \texttt{data.frame})
\item
  \texttt{residuals}: residuos del modelo.
\end{itemize}

\hypertarget{Casos}{%
\section{Casos prácticos}\label{Casos}}

En esta sección se van a utilizar los datos \texttt{airquality}\footnote{Conjunto de datos incluido con la instalación \texttt{base} de \texttt{R}. Ejecútese ?airquality}
que consisten en 154 medidas (de 6 variables) de calidad del aire en Nueva
York. Las variables consideradas aquí son:

\begin{itemize}
\tightlist
\item
  \texttt{Ozone}: Media de ozono (en ppb, partes por billón)
\item
  \texttt{Solar.R}: Radiación solar (en Lagleys)
\item
  \texttt{Wind}: Velocidad media del viento (en millas por hora)
\item
  \texttt{Temp}: Temperatura máxima diaria (en grados Fahrenheit)
\end{itemize}

El objetivo es establecer la relación entre la cantidad de ozono en la atmósfera, variable respuesta, y las variables meteorológicas \texttt{Solar.R}, \texttt{Wind} y \texttt{Temp}, variables explicativas.
Los valores disponibles de las cuatro variables permiten considerarlas como variables continuas.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/airQlmsimples-1} 

}

\caption{Gráficos de dispersión de las variables explicativas frente a la variable respuesta.}\label{fig:airQlmsimples}
\end{figure}

Antes de proceder con el ajuste múltiple se pueden realizar los ajustes simples, individuales.
La Figura \ref{fig:airQlmsimples} representa 3 regresiones lineales simples, de cada una de las 3 variables explicativas frente a la variable respuesta \texttt{Ozone}.
Se visualiza en cada gráfico un \emph{diagrama de dispersión}\index{diagrama de dispersión} de sólo dos variables, la explicativa en el eje \(X\) y la respuesta en el eje \(Y\), obteniéndose la popularmente denominada \emph{nube de puntos}.
En dicho gráfico se puede ver si entre las variables hay relación positiva/directa (si a mayores valores de \(X\) se observan mayores valores de \(Y\)) o negativa/inversa y si tienen o no relación lineal.
En cada gráfico se ha añadido la correspondiente recta de regresión lineal (con su \emph{intervalo de confianza}), que podría no ser la más apropiada, como parece que ocurre entre \texttt{Ozone} y \texttt{Wind} y entre \texttt{Ozone} y \texttt{Temp} cuya relación parece más bien no lineal, aunque suficiente (en función del interés del estudio). El código para el obtener el primer gráfico sería:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{ggplot}\NormalTok{(airquality, }\FunctionTok{aes}\NormalTok{(Solar.R, Ozone)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{aspect.ratio =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

En regresión lineal múltiple no es posible visualizar en un sólo gráfico la relación entre la variable Y y varias variables explicativas \(X_i\), salvo si sólo se tienen 2 explicativas que se tendría un gráfico en 3 dimensiones, generalmente difíciles de visualizar.

\hypertarget{estimaciuxf3n-de-los-coeficientes}{%
\subsection{Estimación de los coeficientes}\label{estimaciuxf3n-de-los-coeficientes}}

Se comienza ajustando el modelo lineal para la variable \texttt{Ozone}, intentando explicarla a partir de las variables continuas \texttt{Solar.R}, \texttt{Wind} y \texttt{Temp}. El modelo de regresión lineal múltiple se expresaría matemáticamente como:
\[Ozone = \beta_0 + \beta_1 Solar.R + \beta_2 Wind + \beta_3 Temp  + \epsilon\]

Más adelante se introducirá una variable categórica para enriquecer el análisis. La definición en \texttt{R} del modelo se puede ver como primer argumento de la función \texttt{lm}. El objeto que genera la función \texttt{lm} se guarda bajo el nombre de \texttt{airq.lm}, y a continuación se muestra su resumen con \texttt{summary}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airq.lm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Ozone }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Solar.R }\SpecialCharTok{+}\NormalTok{ Wind }\SpecialCharTok{+}\NormalTok{ Temp, }\AttributeTok{data =}\NormalTok{ airquality)}
\FunctionTok{summary}\NormalTok{(airq.lm)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = Ozone \textasciitilde{} Solar.R + Wind + Temp, data = airquality)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}     Min      1Q  Median      3Q     Max }
\CommentTok{\#\textgreater{} {-}40.485 {-}14.219  {-}3.551  10.097  95.619 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}              Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}64.34208   23.05472  {-}2.791  0.00623 ** }
\CommentTok{\#\textgreater{} Solar.R       0.05982    0.02319   2.580  0.01124 *  }
\CommentTok{\#\textgreater{} Wind         {-}3.33359    0.65441  {-}5.094 1.52e{-}06 ***}
\CommentTok{\#\textgreater{} Temp          1.65209    0.25353   6.516 2.42e{-}09 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 21.18 on 107 degrees of freedom}
\CommentTok{\#\textgreater{}   (42 observations deleted due to missingness)}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.6059, Adjusted R{-}squared:  0.5948 }
\CommentTok{\#\textgreater{} F{-}statistic: 54.83 on 3 and 107 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

En la salida del \texttt{summary} se proporcionan las estimaciones de los coeficientes del modelo (columna \texttt{Estimate}).
El término independiente aparece como \texttt{(Intercept)} y toma el valor \(\beta_0 =\) -64.3421, el coeficiente asociado a \texttt{Solar.R} es \(\beta_1 =\) 0.0598, etc.
También aparecen sus p-valores asociados (columna \texttt{Pr(\textgreater{}\textbar{}t\textbar{})}), los 4 coeficientes son significativos al 5\%.
Interpretando la leyenda \texttt{Signif.\ codes}, a mayor número de asteriscos que aparecen, mayor significación del coeficiente (menor p-valor). Así, los coeficientes de \texttt{Temp} y \texttt{Wind} son más significativos que el de \texttt{Solar.R}.

También se pueden apreciar en la salida (penúltima línea) dos medidas de la \emph{bondad del ajuste} del modelo considerado: el R cuadrado múltiple y el R cuadrado ajustado. Para comparar entre diferentes modelos, se utiliza el R cuadrado ajustado/corregido que tiene en cuenta la composición/complejidad del modelo (número de variables, etc.). Para el ejemplo el \(R^2\)(ajustado) es 0.5948, que se podría considerar suficiente (en función del objetivo del estudio), pero está claro que el modelo no explica suficientemente bien la cantidad de ozono.

Y en la última linea de la salida aparece información sobre el contraste global del modelo: Estadístico de la F, grados de libertad y p-valor asociado. Como se aprecia, el modelo es globalmente significativo (p-valor del orden de \(10^{-16}\)).

\hypertarget{validaciuxf3n}{%
\subsection{Validación}\label{validaciuxf3n}}

Lo anterior carece de \emph{validez} si no se satisfacen las hipótesis del modelo mencionadas arriba, principalmente varianza constante (homocedasticidad) y normalidad. Para ello se realiza el análisis de residuos. R proporciona un análisis gráfico:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggfortify)}
\FunctionTok{autoplot}\NormalTok{(airq.lm) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/residuosOzone-1} 

}

\caption{Gráficos de residuos}\label{fig:residuosOzone}
\end{figure}

Por un lado el gráfico de residuos frente a valores ajustados (fitted) muestra cierta heterocedasticidad (varianza cambiante con el valor en el eje X) y no linealidad (ya apreciable de forma individual en la Figura \ref{fig:airQlmsimples}).
Por su parte, el gráfico Q-Q de cuantiles, que enfrenta los de los residuos frente a los de la distribución normal, muestra que los residuos presentan desviaciones de la normalidad en ambas colas.

Para completar el análisis gráfico se puede acudir a contrastes de hipótesis. El más habitual para contrastar normalidad es el de Shapiro-Wilk\index{contraste de Shapiro-Wilk} implementado en \texttt{R} con la función \texttt{shapiro.test}\footnote{Se pueden encontrar otros contrastes de normalidad en el paquete \texttt{nortest}.}. Para contrastar la homocedasticidad se puede acudir a alguno de los tres implementados para tal fin en el paquete \texttt{lmtest}: el de Breusch-Pagan \texttt{bptest}\index{contraste de Breusch-Pagan}, el de Goldfeld-Quandt \texttt{gqtest} o el de Harrison-McCabe \texttt{hmctest}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shapiro.test}\NormalTok{(airq.lm}\SpecialCharTok{$}\NormalTok{residuals)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Shapiro{-}Wilk normality test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  airq.lm$residuals}
\CommentTok{\#\textgreater{} W = 0.91709, p{-}value = 3.618e{-}06}
\NormalTok{lmtest}\SpecialCharTok{::}\FunctionTok{bptest}\NormalTok{(airq.lm)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  studentized Breusch{-}Pagan test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  airq.lm}
\CommentTok{\#\textgreater{} BP = 5.0554, df = 3, p{-}value = 0.1678}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/hist-1} 

}

\caption{Histogramas de las variables `Ozone` y `log(Ozone)` }\label{fig:hist}
\end{figure}

El contraste de homocedasticidad lleva a no rechazar tal supuesto (p-valor \textgreater{} 0.05), pero el contraste de Shapiro-Wilk confirma la falta de normalidad (p-valor \textless{} 0.05).
A este respecto, en la Figura \ref{fig:hist} se muestra el histograma de la variable \texttt{Ozone} apreciándose que los datos recogidos presentan asimetría incompatible con la normalidad, asumida por defecto para la variable respuesta.
Una posible solución sería el uso de una transformación logarítmica que produce cierta simetría en la variable, y por tanto más cercanía a la normalidad.

Para el análisis de colinealidad se pueden representar gráficos 2 a 2 de las variables explicativas, para visualizar lo correlacionadas que están (Figura \ref{fig:airQpairs}). Pero es una análisis parcial, una de las variables explicativas podría venir explicada por el resto o varias de ellas. En este caso es mejor acudir al cálculo de los \emph{factores de inflación de la varianza}, que se pueden obtener mediante la función \texttt{vif} del paquete \texttt{car}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(GGally)}
\FunctionTok{ggpairs}\NormalTok{(airquality[, }\DecValTok{2}\SpecialCharTok{:}\DecValTok{4}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/airQpairs-1} 

}

\caption{Gráfico de dispersión por pares de las variables explicativas}\label{fig:airQpairs}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{car}\SpecialCharTok{::}\FunctionTok{vif}\NormalTok{(airq.lm)}
\CommentTok{\#\textgreater{}  Solar.R     Wind     Temp }
\CommentTok{\#\textgreater{} 1.095253 1.329070 1.431367}
\end{Highlighting}
\end{Shaded}

Los gráficos de dispersión muestran ausencia de correlación entre \texttt{Solar.R} y \texttt{Wind} mientras que la correlación entre \texttt{Wind} y \texttt{Temp} no parece despreciable, como es palpable en la práctica. Sin embargo los VIF muestran valores inferiores a 5 (valor de referencia marcado habitualmente), por lo que el modelo no presenta multicolinealidad.

\hypertarget{interpretaciuxf3n-de-los-coeficientes-1}{%
\subsection{Interpretación de los coeficientes}\label{interpretaciuxf3n-de-los-coeficientes-1}}

Teniendo en cuenta la expresión del modelo de regresión lineal múltiple \eqref{eq:lm} la interpretación de cada uno de los coeficientes es simple y directa: lo que aumenta/disminuye la respuesta \emph{media} en magnitud por un aumento de 1 unidad en la variable (continua) asociada al coeficiente, y \emph{ceteris paribus}. Así,

\begin{itemize}
\tightlist
\item
  si el valor de la variable \texttt{Temp} aumenta en 1 unidad (1 grado Fahrenheit), independientemente del valor de \texttt{Wind} y \texttt{Solar.R}\footnote{si estos cambian tendrán su correspondiente impacto en la cantidad media de ozono.}), la respuesta media (cantidad media de ozono, en ppb) cambia en magnitud lo que indica su coeficiente: 1.6521, en este caso aumenta por ser positivo el coeficiente.
\item
  al ser el de \texttt{Wind} negativo implica que aumentos de \texttt{Wind} reducen la cantidad media de ozono. La reducción por cada unidad de \texttt{Wind} (mph) es el valor de su coeficiente: -3.3336 (ppb).
\item
  la influencia de \texttt{Solar.R} es positiva como \texttt{Temp} pero de mucha menor magnitud (por las unidades de una y otra).
\end{itemize}

Como se puede ver el impacto sobre la cantidad \emph{media} de ozono depende de la variable y de la magnitud del coeficiente.

Conviene mencionar que las interpretaciones del modelo no deben extrapolarse a valores fuera del rango que toman las variables explicativas, porque en esas regiones podrían darse otros efectos distintos del lineal que presupone el modelo estimado.

\hypertarget{predicciuxf3n-1}{%
\subsection{Predicción}\label{predicciuxf3n-1}}

Aunque el modelo no sea el más adecuado se ilustra en esta sección cómo se pueden obtener predicciones a partir de un modelo estimado con la función \texttt{predict}.
Se deben asignar los valores de interés a las variables explicativas del modelo, con formato \texttt{data.frame}.
Y se podrán obtener predicciones del valor medio o valores individuales de la variable respuesta, junto con sus intervalos de confianza, según se proporcione al argumento \texttt{interval} los valores \texttt{confidence} o \texttt{prediction}, respectivamente.
En el siguiente ejemplo se obtienen predicciones de cantidad de ozono para un par de casos elegidos arbitrariamente (el primero corresponde a \texttt{Solar.R}=50, \texttt{Wind}=5 y \texttt{Temp}=62):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nueva.meteo }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Solar.R =} \FunctionTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DecValTok{300}\NormalTok{),}
  \AttributeTok{Wind =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{17}\NormalTok{),}
  \AttributeTok{Temp =} \FunctionTok{c}\NormalTok{(}\DecValTok{62}\NormalTok{, }\DecValTok{90}\NormalTok{)}
\NormalTok{)}
\FunctionTok{predict}\NormalTok{(airq.lm, }\AttributeTok{newdata =}\NormalTok{ nueva.meteo, }\AttributeTok{interval =} \StringTok{"confidence"}\NormalTok{)}
\CommentTok{\#\textgreater{}        fit      lwr      upr}
\CommentTok{\#\textgreater{} 1 24.41075 11.01412 37.80739}
\CommentTok{\#\textgreater{} 2 45.62141 31.46838 59.77444}
\FunctionTok{predict}\NormalTok{(airq.lm, }\AttributeTok{newdata =}\NormalTok{ nueva.meteo, }\AttributeTok{interval =} \StringTok{"prediction"}\NormalTok{)}
\CommentTok{\#\textgreater{}        fit        lwr      upr}
\CommentTok{\#\textgreater{} 1 24.41075 {-}19.662967 68.48448}
\CommentTok{\#\textgreater{} 2 45.62141   1.311914 89.93090}
\end{Highlighting}
\end{Shaded}

Como se puede observar, la predicción puntual (\texttt{fit}) es la misma,
pero con \texttt{prediction} es el valor predicho para un día específico con esas condiciones meteorológicas, mientras que con \texttt{confidence} es el valor medio esperado para días con esas condiciones.
Por ello las predicciones de valores individuales (\texttt{prediction}) tienen un intervalo de confianza mayor que las predicciones de valores medios (\texttt{confidence}).

\hypertarget{nuevo-ajuste-con-logozone}{%
\subsection{\texorpdfstring{Nuevo ajuste con \texttt{log(Ozone)}}{Nuevo ajuste con log(Ozone)}}\label{nuevo-ajuste-con-logozone}}

Ante los problemas de falta de normalidad de la variable \texttt{Ozone} se ajusta un nuevo modelo con la variable \(\log(Ozone)\) como respuesta.
Se aprovecha para introducir/definir una variable \emph{dicotómica} para explicar su interpretación. Concretamente se usa \texttt{Temp} que se \emph{dicotomiza} entre aquellos valores de \texttt{Temp} por encima o por debajo de su mediana:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mediana }\OtherTok{\textless{}{-}} \FunctionTok{median}\NormalTok{(airquality}\SpecialCharTok{$}\NormalTok{Temp)}
\NormalTok{Temp.f }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(}\FunctionTok{as.numeric}\NormalTok{(airquality}\SpecialCharTok{$}\NormalTok{Temp }\SpecialCharTok{\textgreater{}}\NormalTok{ mediana))}
\NormalTok{lairq.lm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\FunctionTok{log}\NormalTok{(Ozone) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Wind }\SpecialCharTok{+}\NormalTok{ Solar.R }\SpecialCharTok{+}\NormalTok{ Temp.f, }\AttributeTok{data =}\NormalTok{ airquality)}
\FunctionTok{summary}\NormalTok{(lairq.lm)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = log(Ozone) \textasciitilde{} Wind + Solar.R + Temp.f, data = airquality)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}2.55347 {-}0.29689  0.02409  0.37171  1.18373 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}               Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  3.3879872  0.2232099  15.178  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} Wind        {-}0.0885666  0.0161038  {-}5.500 2.61e{-}07 ***}
\CommentTok{\#\textgreater{} Solar.R      0.0030723  0.0005973   5.143 1.23e{-}06 ***}
\CommentTok{\#\textgreater{} Temp.f1      0.6999123  0.1158384   6.042 2.25e{-}08 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 0.5572 on 107 degrees of freedom}
\CommentTok{\#\textgreater{}   (42 observations deleted due to missingness)}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.5972, Adjusted R{-}squared:  0.5859 }
\CommentTok{\#\textgreater{} F{-}statistic: 52.89 on 3 and 107 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

Como se ve, las estimaciones de los coeficientes cambian respecto al modelo anterior,
al cambiar la variable respuesta y una variable explicativa del modelo.
Salen todos los coeficientes significativos (al 5\%), el modelo global también significativo y el \(R^2\)(corregido) es similar al del modelo anterior.
En el Capítulo 18 de modelos aditivos generalizados se verá como se puede modelar la relación entre \texttt{Ozone} y el resto de variables de una forma más satisfactoria.
No obstante, este segundo modelo puede ser útil para ilustrar la relación entre las variables, sin olvidar que se ha de comprobar su validez. Para ello se haría de nuevo el análisis de residuos (que se deja de tarea al lector, al obtenerse de manera idéntica al anterior).
En los gráficos de residuos se observará mayor homocedasticidad, linealidad y normalidad que en el caso anterior.

\hypertarget{coeficientes-de-variables-categuxf3ricas}{%
\subsection{Coeficientes de variables categóricas}\label{coeficientes-de-variables-categuxf3ricas}}

En este apartado se aborda la interpretación de coeficientes asociados a variables categóricas. \texttt{Temp.f} toma valores numéricos 0 y 1, según la temperatura sea menor o mayor a la mediana respectivamente.
En la salida anterior de \texttt{R} sólo aparece \texttt{Temp.f1}.
El 1 final indica que el coeficiente está asociado a la categoría 1 de \texttt{Temp.f}.
Para los cálculos con estas variables categóricas \texttt{R} toma una categoría como referencia (la primera ``alfabéticamente'', si no se especifica expresamente) y presenta un coeficiente para cada una de las restantes categorías representando el cambio desde la categoría de referencia a cada una de ellas (técnicamente utiliza variables \emph{dummys}).
La categoría de referencia está considerada en los cálculos del término independiente del modelo.
Por lo tanto, el coeficiente de \texttt{Temp.f1} indica que, \emph{ceteris paribus},
la cantidad media de ozono de días con temperaturas por encima de la mediana (categoría 1) es
0.6999 ppb mayor que los días con temperaturas inferiores a la mediana (categoría 0).

\hypertarget{comparativa-regresiuxf3n-frente-a-anova}{%
\subsubsection[Comparativa: Regresión frente a ANOVA ]{\texorpdfstring{Comparativa: Regresión frente a ANOVA \protect\hypertarget{anova}{}{}}{Comparativa: Regresión frente a ANOVA }}\label{comparativa-regresiuxf3n-frente-a-anova}}

En la Figura \ref{fig:airQlmsimples} se pueden apreciar regresiones simples \emph{puras} (variable continua frente a variable continua).
Si en el gráfico se enfrentan la variable \texttt{Ozone} con la variable categórica \texttt{Temp.f} no se tendrá una visualización similar\footnote{\texttt{ggplot(airquality,\ aes(Temp.f,\ Ozone))\ +\ \ geom\_point()}.}. En realidad, al incluir un \emph{factor} se está realizando un contraste de la \emph{t de Student} para averiguar si existen diferencias entre cada categoría y la de referencia. Técnicamente tales contrastes dos a dos son equivalentes al contraste ANOVA, aunque éste permite comparar si todas las categorías son iguales, o no. No obstante, el gráfico ayuda a la interpretación del coeficiente pues permite comparar las medias de cada categoría, viendo si aumenta o disminuye de una categoría a otra.

\hypertarget{comentarios-finales}{%
\section{Comentarios finales}\label{comentarios-finales}}

Una buena referencia para ampliar conocimientos sobre este tema utilizando \texttt{R} es \citet{faraway2002practical}.
En capítulos posteriores se abordarán modelizaciones más complejas, como por ejemplo, los modelos lineales generalizados, GLM, (Capítulo 17), los modelos aditivos generalizados, GAM, (Capítulo 18) y los modelos mixtos (Capítulo 19). También se verán modelos \emph{sparse} y métodos penalizados de regresión (Capítulo 20) como la regresión \emph{ridge} que permite manejar la presencia de multicolinealidad.

Queda fuera de este capítulo el tema de la confusión de variables, esto es, aquellas variables que aun no siendo de interés directo en el análisis se quieren obtener resultados independientemente de sus valores.
El ejemplo típico es la variable sexo, esto es, se quiere obtener la influencia de una variable \(X_i\) en la respuesta \(Y\), independientemente del sexo del individuo. La solución ya ha salido: se incluye la variable en el modelo para que \emph{ceteris paribus} se puede interpretar en el análisis.

También podrían haberse considerado interacciones entre variables, que se suelen interpretar como sinergias o antagonismos. Pero dada la limitación de espacio y el carácter introductorio no se ha hecho. Pues la interpretación de dichas interacciones suele ser generalmente compleja. En el Capítulo 3 de \citet{james2013introduction} puede encontrarse un ejemplo.

\begin{infobox_resume}

\hypertarget{resumen}{%
\subsection*{Resumen}\label{resumen}}
\addcontentsline{toc}{subsection}{Resumen}

En este Capítulo se ha introducido el modelo de regresión lineal, en particular:

\begin{itemize}
\tightlist
\item
  Se ha presentado directamente el modelo de regresión lineal múltiple indicando los pasos para realizar el análisis: estimación, validación, interpretación y predicción. La regresión lineal simple es un caso particular que se puede obtener a partir de lo comentado en el Capítulo.
\item
  Se ha mostrado el uso de \texttt{R} para el ajuste de este tipo de modelos.
\item
  Se han presentado diversos casos prácticos, que sirven para ilustrar la interpretación de coeficientes, tanto asociados a variables continuas como a categóricas, la interpretación de las predicciones y el resto de análisis.
\item
  Se han mencionado distintos problemas de modelización que el análisis ayuda a detectar proponiendo a su vez soluciones para solventarlos.
\end{itemize}

\end{infobox_resume}

\hypertarget{cap-glm}{%
\chapter{Modelos lineales generalizados}\label{cap-glm}}

\emph{Víctor Casero y María Durbán}

\hypertarget{motivaciuxf3n}{%
\section{Motivación}\label{motivaciuxf3n}}

Como se ha mencionado en el Capítulo 16 de modelos lineales, el objetivo detrás del uso de modelos es el de intentar explicar una variable en función de otras que se cree que influyen en ella. Por ejemplo, podría interesar \emph{predecir}

\begin{itemize}
\item
  si un empleado abandonará la empresa, o no, en función de los años de experiencia, la formación, etc.
  O si un paciente sufrirá o no un accidente coronario en función de su edad, sexo, nivel de colesterol, etc.
\item
  el número de días que el empleado estará de baja laboral en función de su antigüedad, salario, etc.
  O el número de días que un paciente estará en el hospital en función del motivo de ingreso, su edad, sexo, etc.
\end{itemize}

Estos casos no pueden analizarse (correctamente) con el modelo de regresión lineal múltiple visto en el Capítulo 16, por que la \emph{variable respuesta}\index{variable respuesta}, la que interesa predecir en cada ejemplo, no sigue una distribución de probabilidad normal (supuesto necesario para utilizar la regresión lineal). Concretamente,

\begin{itemize}
\item
  abandonar (o no) la empresa o sufrir (o no) un accidente coronario se puede modelizar mediante una variable \emph{dicotómica/binaria}, típicamente asignando los valores 0 y 1, lo que encaja perfectamente con una distribución de probabilidad de Bernoulli (muy distinta de la normal).
\item
  el número de días\ldots{} de baja, de hospitalización\ldots{} es una variable de tipo \emph{recuento}\index{recuento} (sólo valores positivos) modelizable mediante una variable discreta que podría seguir una distribución de Poisson (que por ser discreta, es también distinta a la normal, aunque podría parecerse).
\end{itemize}

En este Capítulo se mostrará cómo el \textbf{modelo lineal generalizado}\index{modelo lineal generalizado, GLM} es el modelo teórico adecuado que permite abordar el problema de regresión con \emph{variables respuesta con distinto tipo de distribución de probabilidad}. Concretamente permitirá explicar una \emph{función de la media} a partir de los distintos valores que pueden tomar las variables explicativas. Este Capítulo se centra en la regresión logística y la regresión de Poisson, casos particulares de este modelo, que permiten dar respuesta (correcta) a los dos casos planteados. Un buen libro de referencia para este Capítulo y otros es \citet{james2013introduction}.

\hypertarget{modelo-y-sus-componentes}{%
\section{Modelo y sus componentes}\label{modelo-y-sus-componentes}}

El \textbf{Modelo Lineal Generalizado}, \textbf{GLM}\footnote{Generalized Linear Model. \emph{Cuidado}: \emph{general} y \emph{generalizado} no son sinónimos en este contexto.}\index{modelo lineal generalizado, GLM} se puede escribir como:
\[ \mu = g^{-1}(\eta)\]
en el que se tienen los siguientes componentes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\mu=E(Y)\), el componente aleatorio: la \emph{media} de la variable respuesta \(Y\), que puede seguir cualquier distribución de probabilidad de la familia exponencial. Entre ellas están las más habituales: la normal (por tanto el modelo de regresión lineal es un caso particular de GLM), la Bernoulli/binomial (la utilizada en la regresión logística), la Poisson, la gamma, etc.
\item
  \(\eta = X \beta\), el componente sistemático, el \emph{predictor lineal}, la `estructura' que aportan las variables explicativas/predictoras \(X=(X_1, \ldots , X_k)\), que intentan explicar el comportamiento de la variable respuesta, donde \(\beta=(\beta_1, \ldots , \beta_k)\) es el vector de coeficientes (parámetros) a estimar del modelo.
\item
  \(g(\cdot)\), la novedad de los GLM, la denominada \textbf{función de enlace}\index{función de enlace}, que relaciona los dos componentes anteriores. Esta función puede tomar distintas formas como se ve en el próximo apartado.
\end{enumerate}

Por lo tanto, gracias al GLM se podrá intentar explicar la relación entre una variable respuesta, y una o varias variables que pueden `explicarla'. El procedimiento de análisis suele constar de dos partes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  La \textbf{especificación} de la relación o estructura predefinida de antemano, mediante la \emph{estimación} de los \emph{coeficientes} que mejor ajustan dicha relación, utilizando el método de \emph{máxima verosimilitud}\footnote{A diferencia de la regresión lineal múltiple, que utiliza el de \emph{mínimos cuadrados}.}.\index{método de máxima verosimilitud}
  Más adelante se verá cómo se interpretan tales coeficientes y cómo se puede comprobar la adecuación del modelo.
\item
  La utilización del modelo estimado (especificado) para \textbf{predecir}\index{predicción del modelo} nuevas respuestas, según sea el caso: valores, probabilidades de ocurrencia, etc.
\end{enumerate}

Para la correcta aplicación de los GLM es crucial la elección de dichas variables respuesta y explicativas (que podrían ser de distinto tipo: numéricas -continuas o discretas- o categóricas/cualitativas -dicotómicas o politómicas-\footnote{En este contexto, las variables categóricas se denominan \emph{factores}\index{factores} y las numéricas \emph{covariables}\index{covariables}.}) y la distribución de probabilidad más apropiada para la respuesta.
Como muestra sirva percatarse de que una misma variable podría ser explicativa o respuesta, por ejemplo `diabetes' (sí o no), según se tenga interés en explicar la influencia de la diabetes en otra variable, o se pretende estudiar la influencia de otras variables en la diabetes.
También una misma variable podría considerarse y utilizarse como variable de distinto tipo, por ejemplo `edad', como variable discreta (\emph{años cumplidos}) o como categórica ordinal (\emph{grupos de edad}), aunque es una variable continua (puede tomar cualquier valor en un intervalo).

\hypertarget{funciuxf3n-enlace}{%
\subsection{\texorpdfstring{Función enlace \index{función de enlace}}{Función enlace }}\label{funciuxf3n-enlace}}

Cada distribución de probabilidad tiene asociada una función de enlace \emph{canónica}\footnote{Podrían considerarse otras para cada distribución, pero excede el nivel de este Capítulo.}:

\begin{itemize}
\tightlist
\item
  Para la normal es la identidad: \(g(\mu)=\mu=\eta\).
\item
  Para la Bernoulli, es la función logit: \(g(\mu)=logit(\mu)=log (\mu / (1-\mu)) =\eta\).
\item
  Para la Poisson, es el logaritmo: \(g(\mu)=log(\mu)=\eta\).
\item
  Para la Gamma es la inversa: \(g(\mu)=1/\mu=\eta\), \ldots{}
\end{itemize}

Tanto en el caso de la regresión logística como la de Poisson aparece el logaritmo (neperiano) en la función de enlace, lo que conduce a efectos \textbf{multiplicativos}\index{efectos multiplicativos} de los \emph{factores} o \emph{covariables} \(X_i\) sobre la respuesta, como se verá más claramente en la sección \ref{SECCinterp}. Este es un punto que las distingue de la regresión lineal, en la que los efectos son \textbf{aditivos}\index{efectos aditivos}.

\hypertarget{glms-en-r}{%
\subsection{\texorpdfstring{GLMs en \texttt{R}}{GLMs en R}}\label{glms-en-r}}

En el paquete \texttt{stats} (de la distribución `base' de \texttt{R}) se encuentra la función \texttt{glm} que se utiliza para obtener el ajuste:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glm}\NormalTok{(formula, }\AttributeTok{family =}\NormalTok{ ..., }\AttributeTok{data =}\NormalTok{ ..., ...)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{formula}: que refleje el \emph{predictor lineal}: \texttt{Y\ \textasciitilde{}\ X1\ +\ X2\ +\ X3}, etc.
\item
  \texttt{family}: la distribución aleatoria de la variable respuesta: \texttt{gaussian}, \texttt{binomial}, \texttt{poisson} \ldots{} que determina la función de enlace (\texttt{binomial} -\textgreater{} \texttt{logit}, etc. \texttt{?family} o \texttt{?glm} para más detalles).
\end{itemize}

Las `herramientas' utilizadas para \texttt{lm} se pueden usar para \texttt{glm} (aunque algunas interpretaciones varían). Así se pueden usar \texttt{summary} para detectar los predictores importantes, obtener los valores ajustados con \texttt{fitted}, etc.

\hypertarget{regresiuxf3n-loguxedstica}{%
\section{Regresión logística}\label{regresiuxf3n-loguxedstica}}

La regresión logística es el caso más `famoso' de GLM, de gran relevancia en distintos contextos: Medicina, Economía, etc.
Se utiliza cuando la \emph{variable respuesta} es \emph{dicotómica}, del tipo pertenencia, o no, a un determinado grupo (fumadores, enfermedad X, parados, morosos, \ldots).
Habitualmente se considera que toma el valor \(Y=1\) si se pertenece al grupo e \(Y=0\) si NO se pertenece al grupo.
Tal tipo de variable se puede modelizar con una \emph{distribución de Bernoulli} caracterizada por el parámetro \(p\) que representa la probabilidad de pertenencia al grupo.

El objetivo principal suele ser predecir la pertenencia o no al grupo de un nuevo individuo/elemento basado en la información conocida del mismo a través de las \emph{variables explicativas}. Para ello se estimará el modelo, con los datos disponibles, para determinar que variables influyen \emph{significativamente} en la variable respuesta\footnote{desde el punto de vista estadístico \emph{la influencia/efecto no es fruto del azar}.}.

¿Por qué no tiene cabida aquí el uso del modelo de regresión lineal múltiple?\\
Al ajustar un modelo del tipo: \(Y=\beta_0 + \beta_1 X_1 + \ldots + \beta_k X_k,\)
las estimaciones realizadas \(\hat{Y}\) serán números reales que rara vez
coincidirán con 0 ó 1 (que es el valor esperable de la variable respuesta).
Dicho de otro modo, si sólo se tuviese una variable explicativa, al ajustar el modelo de regresión lineal simple, la recta se saldría o no llegaría a los valores posibles de respuesta (0 ó 1), como ocurre en la Figura \ref{fig:dispersionYlogit} (izquierda) obtenida a partir de los datos del ejemplo coronario que se manejará.

El \textbf{modelo de regresión logística múltiple}\index{regresión logística} permite ajustar el modelo
\begin{equation} 
   \log{ \Big(\dfrac{p}{1-p}  \Big)}=\beta_0 + \beta_1 X_1 + \ldots + \beta_k X_k
   \label{eq:eta}
\end{equation}
mediante el cual se pueden obtener las probabilidades \(P[Y = 1] = p\) y \(P[Y = 0] = 1-p\).
Para ello se utiliza una \emph{función de enlace} que transforma el predictor lineal \(\eta=\beta_0 + \beta_1 X_1 + \ldots + \beta_k X_k\) en valores plausibles para una probabilidad.
La función logística definida como
\begin{equation}
  p = \frac{e^\eta}{1+e^\eta}
  \label{eq:logit}
\end{equation}
y representada en la Figura \ref{fig:dispersionYlogit} (derecha), es la más habitual, y de la que recibe su nombre la regresión logística.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/dispersionYlogit-1} 

}

\caption{Gráfico de dispersión del diagnóstico frente a la edad (izquierda) y función logística (derecha).}\label{fig:dispersionYlogit}
\end{figure}

La ventaja del modelo logístico es que la función tiende a cero por la izquierda y a uno por la derecha, por lo que la predicción será
siempre un valor válido para una probabilidad. Como contrapartida, a medida que las probabilidades se
acercan a cero o a uno la relación entre el predictor y la probabilidad es menos lineal, lo que complica la interpretación de los coeficientes.

\hypertarget{procedimiento-de-ajuste}{%
\subsection{Procedimiento de ajuste}\label{procedimiento-de-ajuste}}

A partir de los datos disponibles, tanto de los valores de las variables \emph{predictoras} como de las \emph{respuestas}, recogidas en el estudio

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Se \textbf{estimarán los coeficientes}\index{estimación de coeficientes} \(\beta_i\) del modelo, para especificar la relación entre las variables, comprobando si tales estimaciones, \(\hat\beta_j\), son \textbf{significativas}\footnote{Significativamente distintas de cero: tiene sentido no eliminarlas del predictor lineal.}.\index{significativo}
\item
  Se valorará la eliminación de variables no significativas, atendiendo a la significación global del modelo, etc.
\item
  Se comprobará la adecuación del modelo final obtenido
\item
  Se interpretarán los coeficientes y quedará listo para hacer \textbf{predicciones}\index{predicción} de probabilidades o valores para \textbf{clasificar}\index{clasificación} a los individuos/elementos.
\end{enumerate}

\textbf{Nota:} como se ha mencionado en el Capítulo 16 del modelo lineal, para decir que un coeficiente es \textbf{significativo} se acude a su \textbf{p-valor}\index{p-valor} asociado, concretamente cuando dicho p-valor es lo suficientemente pequeño, como norma general inferior a 0.05 (aunque podrían tomarse otros valores de referencia\footnote{Por ejemplo, en las salidas de \texttt{R} aparecen otros tres niveles de referencia, 0.10, 0.01 y 0.001}).

\hypertarget{adecuaciuxf3n-del-modelo}{%
\subsection{Adecuación del modelo}\label{adecuaciuxf3n-del-modelo}}

Para comprobar si el modelo estimado es adecuado, se compara con el modelo más simple, el que solo incluye el término independiente.
Para ello se pueden utilizar distintos \emph{contrastes basados en la deviance}\footnote{en ocasiones traducida como \emph{Devianza}, aunque es habitual no traducirla.}, una medida que juega el papel de la suma de cuadrados de los residuos.
En el modelo de regresión logística, la \emph{deviance} es menos dos veces el logaritmo de la \emph{verosimilitud}.
La diferencia entre la \emph{deviance} de un modelo más elaborado y el simple se distribuye como una chi-cuadrado, lo que permite contrastar cuál de los dos modelos ajusta mejor los datos.
P-valores bajos indicarán que el modelo ajustado es inadecuado, debiendo investigarse otras posibles variables predictoras, si se incumple la hipótesis de linealidad o existe sobredispersión (por ejemplo por exceso de ceros).

Adicionalmente se puede aplicar el contraste de de la razón de verosimilitud, que también se basará en las \emph{deviance}, las generadas al añadir cada término secuencialmente al modelo. Contrasta si cada variable (añadida en ese orden al modelo que contiene las anteriores) es significativa, lo que ayuda a decidir si mantenerla o eliminarla del modelo.

Para contrastar la bondad de ajuste del modelo es habitual en la literatura el \emph{contraste de Hosmer-Lemeshow}.
Aplicable a modelos con al menos una variable cuantitativa .
Valores bajos del p-valor resultante indicarán falta de ajuste.

Respecto a las medidas del ajuste del modelo, en el modelo de regresión logística no tiene sentido calcular el coeficiente de determinación \(R^2\), pero existen varias alternativas equivalentes para hacernos una idea de la variabilidad explicada por el modelo.
Las tres medidas más habituales son la Pseudo \(R^2\) de McFadden, la \(R^2\) de Cox y Snell (que por construcción no puede alcanzar el 1)
y la \(R^2\) de Nagelkerke (es una corrección de la de Cox y Snell).

\hypertarget{SECCinterp}{%
\subsection{Interpretación de resultados}\label{SECCinterp}}

La interpretación de los coeficientes no es tan directa y sencilla como en el modelo lineal.
A partir de las estimaciones de la regresión lineal \(\eta\), modelo \eqref{eq:eta}, se puede obtener la probabilidad de que un individuo pertenezca al grupo (\(Y = 1\)) utilizando la expresión \eqref{eq:logit}. Alternativamente se pueden obtener los \emph{odds}, con la siguiente expresión que se deduce de \eqref{eq:logit}:
\begin{equation}
\mathit{odds} = \frac{p}{1-p} =  e^\eta = e^{\beta_0 + \beta_1 X_1 + \ldots + \beta_k X_k}, 
\label{eq:odds}
\end{equation}
Los \emph{odds} vienen a ser cuántas veces es posible
pertenecer al grupo (\(Y = 1\)) frente a no pertenecer (\(Y = 0\)).
Por ejemplo, si \(p = 0.75\) el \emph{odds} es \(0.75/0.25=3\). Esto indica que es tres veces más posible pertenecer al grupo que no pertenecer\footnote{Por esto se traduce \emph{odds} como \emph{ventaja}. Pero cuidado con su uso: si \(Y=1\) es tener una enfermedad, no parece muy adecuado hablar de \emph{ventaja}.}.

La ecuación \eqref{eq:odds} se puede expresar equivalentemente como:
\begin{equation}
\mathit{odds}=e^{\beta_0} \cdot e^{\beta_1 X_1} \cdot \ldots \cdot e^{\beta_k X_k}.  
\label{eq:modds1}
\end{equation}
La interpretación de los \(\hat{\beta}_j\) carece de sentido, lo interpretable es \(e^{\hat\beta_j}\), denominado \textbf{odd ratio (OR)}\index{odd ratio, OR}.
Además se ve que el modelo \eqref{eq:modds1} es \emph{multiplicativo} (los términos aparecen multiplicando, no sumando), por lo que obtener \(e^{\hat{\beta}_j}\) inferiores a 1 implican disminución de odds, y por encima de 1, implican aumento, \emph{ceteris paribus}\footnote{Manteniendo constante el valor de las demás variables.}.

Con el modelo de regresión logística se puede proporcionar el llamado \textbf{riesgo relativo}\index{riesgo relativo, RR} (RR, \emph{relative risk}). Es una razón de
probabilidades como los \emph{odds}, pero en vez de comparar la probabilidad de pertenecer a un grupo
o a otro de la variable respuesta, compara las probabilidades de pertenecer al grupo
de referencia \(Y=1\) según los valores del predictor categórico \(X_j\).
El riesgo relativo es similar al OR de la variable \(X_j\) sólo cuando la probabilidad de \(Y=1\) es baja, se suele dar como referencia que sea inferior al 10\%. Por lo tanto, OR y RR no coinciden siempre.

\hypertarget{predicciuxf3n.-curva-roc-y-auc}{%
\subsection{Predicción. Curva ROC y AUC}\label{predicciuxf3n.-curva-roc-y-auc}}

Como se ha mencionado, el uso habitual de la regresión logística es la predicción, no tanto de valores o probabilidades sino de la clasificación en un grupo u otro de futuros individuos/elementos basado en la información conocida de ellos a través de las variables explicativas.
La regla de clasificación en los grupos depende de la elección del punto de corte de la probabilidad, por ejemplo \(\hat{Y}=1\) si \(\hat p>0.7\) y \(\hat{Y}=0\) para el resto.
Para ello se acude al análisis de los casos clasificados correctamente o no por el modelo ajustado: \emph{análisis de sensibilidad y especificidad del modelo}, y su visualización más popular, la \textbf{curva ROC}\index{curva ROC} (\emph{Receiver Operating Characteristic}) y la medición de su \textbf{AUC}\index{AUC} (\emph{area under the curve}).

Tanto la \textbf{sensibilidad}\index{sensibilidad} como la \textbf{especificidad}\index{especificidad} son el porcentaje de elementos de la muestra bien clasificados por el modelo. La \emph{sensibilidad} de los que pertenecen al grupo (\(Y=1\)), la \emph{especificidad} de los que no pertenecen (\(Y=0\)). Los mal clasificados se dividen en \emph{falsos positivos}, cuando el modelo pronostica \(\hat{Y}=1\) pero en realidad es \(Y=0\), y \emph{falsos negativos}, el caso contrario.

Gráficamente, se pueden obtener los valores de la sensibilidad y especificidad para distintos puntos de corte (entre 0 y 1).
La Figura \ref{fig:disc2} (izquierda) muestra este gráfico para uno de los casos prácticos\footnote{La interpretación del gráfico se hace en dicho apartado.}.
Pero la forma más popular de analizar la sensibilidad y especificidad es
mediante la curva ROC que representa la sensibilidad frente al ratio de falsos positivos, esto es \emph{1-especificidad} (Figura \ref{fig:disc2} (derecha)).
Su AUC sirve para comparar distintos modelos de regresión logística (u otros modelos con el mismo fin\footnote{Véase el Capítulo 10 de Feature selection \& engineering.}).
Cuanto mejor poder discriminante tenga el modelo más próximo a 1 será el AUC.
Un clasificador aleatorio presentaría la curva ROC coincidente con la diagonal, con un AUC de 0.5.

\hypertarget{regresiuxf3n-de-poisson}{%
\section{Regresión de Poisson}\label{regresiuxf3n-de-poisson}}

Se utiliza cuando la \emph{variable respuesta} es \emph{discreta} y toma sólo valores positivos, modelizable con la \emph{distribución de Poisson}. Típicamente variables de tipo \emph{número de \ldots{}}, como el ejemplo motivador del principio: número de días de hospitalización.
El parámetro que caracteriza la distribución de Poisson es \(\lambda\) que representa la \emph{media} de la variable aleatoria, y a la vez la varianza (a nivel teórico).

Aquí tampoco tiene cabida el modelo de regresión lineal, principalmente porque las estimaciones realizadas \(\hat{Y}\) podrían arrojar valores negativos.

De nuevo el objetivo suele ser la predicción, en el ejemplo, el número de días que un (nuevo) paciente estará ingresado, basado en la información de sus \emph{variables explicativas}.
Previo a la predicción se estimará e interpretará el modelo, a partir de los datos disponibles, para determinar las variables explicativas que influyen sobre la variable respuesta (significativas). Y de nuevo los efectos serán \emph{multiplicativos} por ser el logaritmo su función de enlace, como se mencionó. Por ello, lo visto anteriormente para la regresión logística es asimilable a la regresión de Poisson.

\hypertarget{casos-pruxe1cticos}{%
\section{Casos prácticos}\label{casos-pruxe1cticos}}

\hypertarget{ejemplos-de-regresiuxf3n-loguxedstica}{%
\subsection{Ejemplos de regresión logística}\label{ejemplos-de-regresiuxf3n-loguxedstica}}

Se manejará el conjunto de datos \texttt{cleveland} descrito en el el paquete del libro \texttt{CDR}. Se quiere explicar la variable \texttt{diag} (dicotómica: 1, ha sufrido el accidente coronario, 0 no lo ha sufrido) a partir de otras variables.

\textbf{Estimación}\\
La variable respuesta \texttt{diag} ya aparece como factor en la base de datos, y se considera un primer modelo con variables explicativas \texttt{edad} y \texttt{dep} (variables continuas):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(CDR)}
\NormalTok{reg.log }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(diag }\SpecialCharTok{\textasciitilde{}}\NormalTok{ edad }\SpecialCharTok{+}\NormalTok{ dep,}
  \AttributeTok{family =} \StringTok{"binomial"}\NormalTok{, }\AttributeTok{data =}\NormalTok{ cleveland}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(reg.log)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = diag \textasciitilde{} edad + dep, family = "binomial", data = cleveland)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}2.3804  {-}0.8905  {-}0.6210   1.0021   1.9644  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}3.08080    0.81939  {-}3.760  0.00017 ***}
\CommentTok{\#\textgreater{} edad         0.03738    0.01476   2.532  0.01134 *  }
\CommentTok{\#\textgreater{} dep          0.86851    0.13791   6.298 3.02e{-}10 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 417.98  on 302  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 350.64  on 300  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 356.64}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 4}
\end{Highlighting}
\end{Shaded}

En la salida se muestran las estimaciones de los coeficientes del modelo (columna \texttt{Estimate}) y su significatividad (columna \texttt{Pr(\textgreater{}\textbar{}z\textbar{})}). En este ejemplo, los dos coeficientes de interés (los correspondientes a \texttt{edad} y \texttt{dep}) son significativos (al 5\% de significación).

Se completa el modelo anterior añadiendo variables \emph{categóricas}, concretamente, \texttt{tdolor} (politómica) y \texttt{sexo} (dicotómica), para poder incluir más adelante la interpretación de los coeficientes asociados a este tipo de variables.
Para su correcta interpretación, se deben introducir en el predictor como variables de tipo \texttt{factor}\footnote{ En este caso ya están definidas como factor en el \texttt{data.frame}.}. Si no, el procedimiento la considera numérica, obteniendo una salida que llevaría a una interpretación errónea.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reg.log2 }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(reg.log, }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{+}\NormalTok{ sexo }\SpecialCharTok{+}\NormalTok{ tdolor)}
\end{Highlighting}
\end{Shaded}

\textbf{Adecuación del modelo}\\
Se realiza el contraste de Hosmer-Lemeshow para comprobar la bondad de ajuste de los modelos:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ResourceSelection)}
\FunctionTok{hoslem.test}\NormalTok{(reg.log}\SpecialCharTok{$}\NormalTok{y, reg.log}\SpecialCharTok{$}\NormalTok{fitted.values)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Hosmer and Lemeshow goodness of fit (GOF) test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  reg.log$y, reg.log$fitted.values}
\CommentTok{\#\textgreater{} X{-}squared = 5.631, df = 8, p{-}value = 0.6885}
\FunctionTok{hoslem.test}\NormalTok{(reg.log2}\SpecialCharTok{$}\NormalTok{y, reg.log2}\SpecialCharTok{$}\NormalTok{fitted.values)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Hosmer and Lemeshow goodness of fit (GOF) test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  reg.log2$y, reg.log2$fitted.values}
\CommentTok{\#\textgreater{} X{-}squared = 4.8171, df = 8, p{-}value = 0.7769}
\end{Highlighting}
\end{Shaded}

Arrojan p-valores `altos' indicando que no hay falta de ajuste.

Para realizar el test de la razón de verosimilitud con R se acude a la función \texttt{anova}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(reg.log2, }\AttributeTok{test =} \StringTok{"Chisq"}\NormalTok{)}
\CommentTok{\#\textgreater{} Analysis of Deviance Table}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Model: binomial, link: logit}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Response: diag}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Terms added sequentially (first to last)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}        Df Deviance Resid. Df Resid. Dev  Pr(\textgreater{}Chi)    }
\CommentTok{\#\textgreater{} NULL                     302     417.98              }
\CommentTok{\#\textgreater{} edad    1   15.447       301     402.54 8.487e{-}05 ***}
\CommentTok{\#\textgreater{} dep     1   51.894       300     350.64 5.858e{-}13 ***}
\CommentTok{\#\textgreater{} sexo    1   23.982       299     326.66 9.726e{-}07 ***}
\CommentTok{\#\textgreater{} tdolor  3   62.153       296     264.51 2.037e{-}13 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

Utilizando la \emph{deviance} de añadir cada término secuencialmente al modelo permite concluir que la inclusión secuencial de tales variables predictoras es significativa respecto al modelo que contiene las anteriores.

Para completar, se podría contrastar también el efecto de una variable comparando la \emph{deviance} del modelo con dicha variable o sin ella:

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# se elimina edad del segundo modelo}
\NormalTok{reg.log3 }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(reg.log2, }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{{-}}\NormalTok{ edad)}
\FunctionTok{anova}\NormalTok{(reg.log3, reg.log2, }\AttributeTok{test =} \StringTok{"Chisq"}\NormalTok{)}
\CommentTok{\#\textgreater{} Analysis of Deviance Table}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Model 1: diag \textasciitilde{} dep + sexo + tdolor}
\CommentTok{\#\textgreater{} Model 2: diag \textasciitilde{} edad + dep + sexo + tdolor}
\CommentTok{\#\textgreater{}   Resid. Df Resid. Dev Df Deviance Pr(\textgreater{}Chi)   }
\CommentTok{\#\textgreater{} 1       297     274.45                        }
\CommentTok{\#\textgreater{} 2       296     264.51  1   9.9488  0.00161 **}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

El resultado indica que la variable \texttt{edad} se considera un predictor significativo en el modelo. Se deja al lector realizar las restantes, que le llevarán a concluir que todos son predictores significativos.

El cálculo de de la medida de bondad de ajuste de la Pseudo \(R^2\) de McFadden se puede obtener así:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{null }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(diag }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }\AttributeTok{family =} \StringTok{"binomial"}\NormalTok{, }\AttributeTok{data =}\NormalTok{ cleveland)}
\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{logLik}\NormalTok{(reg.log) }\SpecialCharTok{/} \FunctionTok{logLik}\NormalTok{(null)}
\CommentTok{\#\textgreater{} \textquotesingle{}log Lik.\textquotesingle{} 0.1611088 (df=3)}
\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{logLik}\NormalTok{(reg.log2) }\SpecialCharTok{/} \FunctionTok{logLik}\NormalTok{(null)}
\CommentTok{\#\textgreater{} \textquotesingle{}log Lik.\textquotesingle{} 0.3671819 (df=7)}
\end{Highlighting}
\end{Shaded}

Para el primer modelo la Pseudo \(R^2\) de McFadden es 0.16, mientras que para el segundo es 0.37.

\textbf{Interpretación de los coeficientes}\\
Se muestran los coeficientes estimados del segundo modelo, y sus correspondientes OR (sus exponenciales) que son los interpretables:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(reg.log2)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} glm(formula = diag \textasciitilde{} edad + dep + sexo + tdolor, family = "binomial", }
\CommentTok{\#\textgreater{}     data = cleveland)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance Residuals: }
\CommentTok{\#\textgreater{}     Min       1Q   Median       3Q      Max  }
\CommentTok{\#\textgreater{} {-}2.5346  {-}0.6604  {-}0.2436   0.6568   2.3975  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error z value Pr(\textgreater{}|z|)    }
\CommentTok{\#\textgreater{} (Intercept) {-}6.67669    1.30016  {-}5.135 2.82e{-}07 ***}
\CommentTok{\#\textgreater{} edad         0.05621    0.01828   3.075   0.0021 ** }
\CommentTok{\#\textgreater{} dep          0.80890    0.16597   4.874 1.10e{-}06 ***}
\CommentTok{\#\textgreater{} sexo1        1.69477    0.36801   4.605 4.12e{-}06 ***}
\CommentTok{\#\textgreater{} tdolor2      0.65668    0.67357   0.975   0.3296    }
\CommentTok{\#\textgreater{} tdolor3      0.19465    0.59654   0.326   0.7442    }
\CommentTok{\#\textgreater{} tdolor4      2.58230    0.57549   4.487 7.22e{-}06 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (Dispersion parameter for binomial family taken to be 1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}     Null deviance: 417.98  on 302  degrees of freedom}
\CommentTok{\#\textgreater{} Residual deviance: 264.51  on 296  degrees of freedom}
\CommentTok{\#\textgreater{} AIC: 278.51}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Number of Fisher Scoring iterations: 5}
\FunctionTok{exp}\NormalTok{(}\FunctionTok{coef}\NormalTok{(reg.log2))[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\CommentTok{\#\textgreater{}      edad       dep     sexo1   tdolor2   tdolor3   tdolor4 }
\CommentTok{\#\textgreater{}  1.057824  2.245432  5.445391  1.928371  1.214880 13.227480}
\end{Highlighting}
\end{Shaded}

Los coeficientes asociados a las variables \emph{continuas} son positivos y significativos, por tanto sus OR son significativamente superiores a 1\footnote{En regresión lineal que un coeficiente sea significativo se interpreta como significativamente distinto de cero, por el hecho de que el modelo considera efectos aditivos de las variables. El modelo de regresión logística es de efectos multiplicativos, como se ha mencionado, y por ello el valor `neutro' es el 1. Así, un coeficiente significativo se interpreta ahora como significativamente distinto de 1.}.
Indican que, \emph{ceteris paribus}

\begin{itemize}
\tightlist
\item
  por cada año más cumplido el odds de padecer la enfermedad aumenta poco más de 1 vez, un 1.0578. ¿Y un aumento de 20 años?\ldots{} En tal caso sería \(e^{\beta_1*20} =\) 3.078\ldots{} En 20 años más que se triplica el odds. Ojo con la interpretación, no se triplica la probabilidad sino el odds.
\item
  por un aumento de una unidad en la variable \emph{dep} más que se duplican los \emph{odds} del paciente, pues su OR es 2.2454.
\end{itemize}

¿Y cómo se interpretan los coeficientes asociados a las variables \emph{categóricas}?
¿qué significa \texttt{sexo1}? ¿ \texttt{tdolor2}\ldots? Representan \emph{el cambio de la categoría de referencia a la mostrada}.\footnote{\texttt{R} en las variables definidas como \texttt{factor} toma como referencia la primera categoría al ordenar los valores de la variable o bien alfabéticamente (a, b, c\ldots) o bien numéricamente de menor a mayor. De ahí que \texttt{sexo=0} sea la categoría de referencia, porque toma los valores 0 y 1, y sólo aparezca en las estimaciones \texttt{sexo1} reflejando el cambio a tal categoría desde la de referencia. Para \texttt{tdolor}, al tomar los valores 1, 2, 3 y 4, la categoría de referencia es \texttt{tdolor=1} y aparecen coeficientes para \texttt{tdolor=2}, \texttt{tdolor=3} y \texttt{tdolor=4}.}

\begin{itemize}
\tightlist
\item
  el de \texttt{sexo1} significa que el odds de padecer la enfermedad es más de 5 veces superior para los hombres (\texttt{sexo=1}) que para las mujeres (\texttt{sexo=0}).
\item
  el de \texttt{tdolor4} que los de este tipo de dolor (asintomáticos) presentan un odds 13 veces superior al de los de \texttt{tdolor=1}.\footnote{\texttt{tdolor2} y \texttt{tdolor3} no son significativos.}
\end{itemize}

Nótese que no se muestra \(e^{\beta_0}\), pues su interpretación carece de sentido práctico, sería el OR de las personas de 0 años, \texttt{dep\ =\ 0}, \texttt{sexo=0} y \texttt{tdolor=1}.

\textbf{Predicciones}\\
Una vez estimado e interpretado el modelo y comprobada su bondad, se puede usar para obtener predicciones.
Para ello se deben asignar valores a las variables explicativas: \texttt{edad}, \texttt{dep}, \texttt{sexo} y \texttt{tdolor} (se han escogido arbitrariamente).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{individuo }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{edad =} \DecValTok{50}\NormalTok{, }\AttributeTok{dep =} \DecValTok{3}\NormalTok{,}
  \AttributeTok{sexo =} \StringTok{"1"}\NormalTok{, }\AttributeTok{tdolor =} \StringTok{"4"} \CommentTok{\# entre comillas por ser factores}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Con la función \texttt{predict} se pueden obtener tanto el valor predicho para el predictor lineal (valor no interpretable), \texttt{eta}, como directamente la probabilidad de que dicho individuo padezca la enfermedad coronaria, \texttt{p}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# (eta \textless{}{-} predict(reg.log2, individuo))}
\NormalTok{(p }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(reg.log2, individuo, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{))}
\CommentTok{\#\textgreater{}         1 }
\CommentTok{\#\textgreater{} 0.9446843}
\end{Highlighting}
\end{Shaded}

Con cualquiera de los valores anteriores se pueden obtener su \emph{odds}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p) }\CommentTok{\# exp(eta)}
\CommentTok{\#\textgreater{}        1 }
\CommentTok{\#\textgreater{} 17.07805}
\end{Highlighting}
\end{Shaded}

Es decir, un individuo con 50 años, \texttt{dep=3}, \texttt{sexo=1}(hombre) y \texttt{tdolor=4}(asintomático) tiene 17 veces más posibilidades de padecer la enfermedad que de no padecerla.

\textbf{Riesgo relativo}\\
Se ha visto que el OR para el nivel `hombre' en el segundo modelo es 5.45. El riesgo relativo de ser hombre para un paciente de 50 años, valor \texttt{dep} de 3 y \texttt{tdolor} 4 (de nuevo valores arbitrarios) es:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hom }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{edad =} \DecValTok{50}\NormalTok{, }\AttributeTok{dep =} \DecValTok{3}\NormalTok{, }\AttributeTok{sexo =} \StringTok{"1"}\NormalTok{, }\AttributeTok{tdolor =} \StringTok{"4"}\NormalTok{)}
\NormalTok{muj }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{edad =} \DecValTok{50}\NormalTok{, }\AttributeTok{dep =} \DecValTok{3}\NormalTok{, }\AttributeTok{sexo =} \StringTok{"0"}\NormalTok{, }\AttributeTok{tdolor =} \StringTok{"4"}\NormalTok{)}
\NormalTok{(ph }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(reg.log2, hom, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{))}
\CommentTok{\#\textgreater{}         1 }
\CommentTok{\#\textgreater{} 0.9446843}
\NormalTok{(pm }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(reg.log2, muj, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{))}
\CommentTok{\#\textgreater{}         1 }
\CommentTok{\#\textgreater{} 0.7582346}
\NormalTok{ph }\SpecialCharTok{/}\NormalTok{ pm }\CommentTok{\# riesgo relativo hombre/mujer}
\CommentTok{\#\textgreater{}      1 }
\CommentTok{\#\textgreater{} 1.2459}
\end{Highlighting}
\end{Shaded}

El riesgo relativo de ser hombre para padecer la enfermedad
cardíaca es \(RR=0.9447/0.7582 = 1.2459\), es
decir, tiene un 24.6\% más de posibilidades de tener la enfermedad que una mujer. En este caso se observa que el OR y el RR no son valores cercanos, debido a que el diagnóstico 1 es frecuente (concretamente lo presentan el 45.9\% de los pacientes de la base de datos)

\textbf{Curva ROC}\\
El \emph{análisis de sensibilidad y especificidad del modelo} y la curva ROC y su AUC para este ejemplo se pueden obtener con el siguiente código.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{library}\NormalTok{(Epi)}
\FunctionTok{ROC}\NormalTok{(}
  \AttributeTok{form =}\NormalTok{ diag }\SpecialCharTok{\textasciitilde{}}\NormalTok{ edad }\SpecialCharTok{+}\NormalTok{ dep }\SpecialCharTok{+}\NormalTok{ sexo }\SpecialCharTok{+}\NormalTok{ tdolor, }\AttributeTok{data =}\NormalTok{ cleveland,}
  \AttributeTok{plot =} \StringTok{"sp"}
\NormalTok{)}
\FunctionTok{ROC}\NormalTok{(}
  \AttributeTok{form =}\NormalTok{ diag }\SpecialCharTok{\textasciitilde{}}\NormalTok{ edad }\SpecialCharTok{+}\NormalTok{ dep }\SpecialCharTok{+}\NormalTok{ sexo }\SpecialCharTok{+}\NormalTok{ tdolor, }\AttributeTok{data =}\NormalTok{ cleveland,}
  \AttributeTok{plot =} \StringTok{"ROC"}\NormalTok{, }\AttributeTok{las =} \DecValTok{1}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/disc2-1} 

}

\caption{Gráfico de sensibilidad y especificidad según puntos de corte de discriminación (izquierda) y curva ROC (derecha) para el segundo modelo de regresión logística}\label{fig:disc2}
\end{figure}

La Figura \ref{fig:disc2} (izquierda) muestra que tomando como punto de corte una probabilidad cercana a 0.45 se obtienen valores de sensibilidad y especificidad alrededor de 0.8. No obstante, se deben evaluar los costes y riesgos de una mala clasificación (por ejemplo, dar tratamiento cuando no hace falta, y no darlo cuando hace falta).

La Figura \ref{fig:disc2} (derecha) representa la curva ROC de \texttt{reg.log2}. La curva está por encima de la diagonal, con lo que es mejor que un clasificador aleatorio. El AUC es de 0.878, con una sensibilidad de 78.4\% y una especificidad de 84.1\%.
Si se realiza el análisis para \texttt{reg.log} se comprobará que el AUC es de 0.759, por lo que el segundo modelo es mejor para discriminar.
Además, el gráfico indica el valor del `punto de corte óptimo', 0.518, el que proporciona el mayor AUC, el citado 0.878, con sus correspondientes valores de sensibilidad, especificidad, etc. También presenta las estimaciones y errores estándar (\emph{s.e.}) de los coeficientes del modelo considerado.

\hypertarget{ejemplo-de-regresiuxf3n-de-poisson}{%
\subsection{Ejemplo de regresión de Poisson}\label{ejemplo-de-regresiuxf3n-de-poisson}}

A partir del mismo conjunto de datos, \texttt{cleveland}, se considera como variable a explicar, variable respuesta, \texttt{dhosp}, el número de días de hospitalización de un paciente.
Y como variables explicativas: \texttt{diag}, \texttt{edad}, \texttt{sexo} y \texttt{tdolor}, que son de distinto tipo, lo que permitirá ilustrar sus distintas interpretaciones.

\textbf{Visualización ilustrativa}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(cleveland, }\FunctionTok{aes}\NormalTok{(dhosp, }\AttributeTok{fill =}\NormalTok{ diag)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{)}
\NormalTok{p2 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(cleveland, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{factor}\NormalTok{(dhosp), }\AttributeTok{y =}\NormalTok{ edad)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{()}
\FunctionTok{library}\NormalTok{(patchwork)}
\NormalTok{p1 }\SpecialCharTok{+}\NormalTok{ p2}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/grafpois-1} 

}

\caption{Gráfico de barras de 'dhosp' separado por 'diag' (izquierda) y gráficos de cajas de 'edad' separados por 'dhosp' (derecha).}\label{fig:grafpois}
\end{figure}

A la vista de los gráficos de la Figura \ref{fig:grafpois}, generados con las sentencias anteriores, \texttt{diag} parece ser una buena variable predictora del número de días de hospitalización, mientras que \texttt{edad} no tiene una influencia tan clara.

\textbf{Ajuste e interpretación}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reg.pois }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(dhosp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ diag }\SpecialCharTok{+}\NormalTok{ edad }\SpecialCharTok{+}\NormalTok{ sexo }\SpecialCharTok{+}\NormalTok{ tdolor,}
  \AttributeTok{data =}\NormalTok{ cleveland, }\AttributeTok{family =} \StringTok{"poisson"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Al especificar \texttt{family\ =\ "poisson"} la función \texttt{glm} selecciona automáticamente la función de enlace canónica apropiada: el logaritmo (efectos multiplicativos).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(reg.pois)}\SpecialCharTok{$}\NormalTok{coef}
\CommentTok{\#\textgreater{}                  Estimate  Std. Error     z value     Pr(\textgreater{}|z|)}
\CommentTok{\#\textgreater{} (Intercept) {-}0.2230728726 0.332125111 {-}0.67165314 5.018045e{-}01}
\CommentTok{\#\textgreater{} diag1        1.0902902179 0.109731744  9.93595999 2.903654e{-}23}
\CommentTok{\#\textgreater{} edad         0.0001026277 0.004877904  0.02103931 9.832143e{-}01}
\CommentTok{\#\textgreater{} sexo1        0.2160286255 0.103297627  2.09132226 3.649919e{-}02}
\CommentTok{\#\textgreater{} tdolor2      0.1408464230 0.205232539  0.68627725 4.925383e{-}01}
\CommentTok{\#\textgreater{} tdolor3      0.1217757202 0.189404790  0.64293897 5.202637e{-}01}
\CommentTok{\#\textgreater{} tdolor4      0.1215388103 0.178729482  0.68001546 4.964947e{-}01}
\FunctionTok{exp}\NormalTok{(}\FunctionTok{coef}\NormalTok{(reg.pois))[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\CommentTok{\#\textgreater{}    diag1     edad    sexo1  tdolor2  tdolor3  tdolor4 }
\CommentTok{\#\textgreater{} 2.975137 1.000103 1.241138 1.151248 1.129501 1.129233}
\end{Highlighting}
\end{Shaded}

De todas las variables introducidas en el modelo sólo \texttt{diag} y \texttt{sexo} son significativas (al 5\%). Se confirma así lo visto en la Figura \ref{fig:grafpois} (derecha), que \texttt{edad} no influye en la respuesta. Al ser ambas variables significativas de tipo dicotómico su interpretación, como ya ocurría en regresión logística, se hace en función del `nivel/grupo de referencia', y \emph{ceteris paribus} (en todos los casos).

\begin{itemize}
\item
  El coeficiente de \texttt{diag} es 1.0903 pero se le debe aplicar la exponencial para tener como unidades de medida días.
  Así, el número medio de días de estancia en el hospital es 2.98 veces mayor con \texttt{diag=1} que con \texttt{diag=0} (grupo tomado como referencia).
\item
  Similar para \texttt{sexo}, un hombre (\texttt{sexo=1}) se espera que esté, en media, 1.24 días por cada día que esté una mujer.
\item
  De la misma manera se interpretarían los coeficientes asociados a \texttt{tdolor}, cada uno de ellos expresaría la diferencia con los pacientes del grupo \texttt{tdolor=1}.
\item
  Aunque \texttt{edad} no es significativa, y la magnitud es ínfima, se da su interpretación, al ser la única variable continua: \emph{ceteris paribus}
  por cada año que aumente la \texttt{edad} el número medio de días en el hospital se ve multiplicado por
  1.0001 (ínfimo)\footnote{aunque el incremento fuese de 20 años (como se hizo en el ejemplo de regresión logística), por cada día que esté hospitalizado el más joven, el de 20 años más estará \(e^{\beta_2*20}=\)
    1.0021 días.}.
\end{itemize}

Para posteriores comparaciones se considera otro modelo en el que se elimina \texttt{tdolor} (por no ser significativa):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reg.pois2 }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(dhosp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ diag }\SpecialCharTok{+}\NormalTok{ sexo }\SpecialCharTok{+}\NormalTok{ edad,}
  \AttributeTok{data =}\NormalTok{ cleveland, }\AttributeTok{family =} \StringTok{"poisson"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Adecuación}

En las salidas obtenidas hay información sobre la \emph{Null} y la \emph{Residual deviance}, con las que se puede realizar un contraste de comparación de modelos (el simple frente al elaborado).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pchisq}\NormalTok{(reg.pois}\SpecialCharTok{$}\NormalTok{deviance, reg.pois}\SpecialCharTok{$}\NormalTok{df.residual, }\AttributeTok{lower.tail =}\NormalTok{ F)}
\CommentTok{\#\textgreater{} [1] 0.1325654}
\FunctionTok{pchisq}\NormalTok{(reg.pois2}\SpecialCharTok{$}\NormalTok{deviance, reg.pois2}\SpecialCharTok{$}\NormalTok{df.residual, }\AttributeTok{lower.tail =}\NormalTok{ F)}
\CommentTok{\#\textgreater{} [1] 0.155157}
\end{Highlighting}
\end{Shaded}

Al ser los p-valores superiores a 0.05 ambos modelos se pueden considerar que explican `mejor' que el modelo nulo.

\textbf{Predicción}

Con los modelos ajustados, y comprobada su adecuación, se pueden predecir valores, que en este caso serán el número \emph{medio} de días de hospitalización.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pacientes }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{diag =} \FunctionTok{c}\NormalTok{(}\StringTok{"1"}\NormalTok{, }\StringTok{"0"}\NormalTok{, }\StringTok{"1"}\NormalTok{, }\StringTok{"0"}\NormalTok{),}
  \AttributeTok{edad =} \FunctionTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{50}\NormalTok{),}
  \AttributeTok{sexo =} \FunctionTok{c}\NormalTok{(}\StringTok{"1"}\NormalTok{, }\StringTok{"1"}\NormalTok{, }\StringTok{"0"}\NormalTok{, }\StringTok{"0"}\NormalTok{),}
  \AttributeTok{tdolor =} \FunctionTok{c}\NormalTok{(}\StringTok{"4"}\NormalTok{, }\StringTok{"4"}\NormalTok{, }\StringTok{"4"}\NormalTok{, }\StringTok{"4"}\NormalTok{)}
\NormalTok{)}
\FunctionTok{predict}\NormalTok{(reg.pois, pacientes, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\CommentTok{\#\textgreater{}         1         2         3         4 }
\CommentTok{\#\textgreater{} 3.3532035 1.1270752 2.7017171 0.9080983}
\end{Highlighting}
\end{Shaded}

Se han escogido arbitrariamente valores de las variables explicativas para 4 \texttt{pacientes}: 2 de ellos con accidente coronario (\texttt{diag=1}), un hombre (\texttt{sexo=1}) y una mujer y otros 2 sin accidente, también un hombre y una mujer, los cuatro con 50 años y asintomáticos (\texttt{tdolor=4}).
La predicción obtenida indica que el hombre con accidente coronario estará hospitalizado más días, en media, que el resto, aunque le sigue de cerca la mujer con accidente coronario.

También se pueden dibujar las predicciones de todo el conjunto de datos, que \texttt{glm} ha guardado como \texttt{fitted.values}. Para el modelo \texttt{reg.pois} sería:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleveland}\SpecialCharTok{$}\NormalTok{hat }\OtherTok{\textless{}{-}}\NormalTok{ reg.pois}\SpecialCharTok{$}\NormalTok{fitted.values}
\FunctionTok{ggplot}\NormalTok{(cleveland, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ edad, }\AttributeTok{y =}\NormalTok{ hat, }\AttributeTok{colour =}\NormalTok{ diag)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Edad"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Días hospitalización"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/predictPois-1} 

}

\caption{Predicciones de todo el conjunto de datos del modelo reg.pois (izquierda) y reg.pois2 (derecha)}\label{fig:predictPois}
\end{figure}

La Figura \ref{fig:predictPois} muestra las predicciones de los 303 individuos, para cada uno de los dos modelos de regresión de Poisson considerados. Cabe recordar que en el segundo se ha eliminado la variable \texttt{tdolor}. A la vista de dichos gráficos se puede concluir que es \texttt{tdolor} la que genera las distintas `lineas horizontales' de puntos en torno a \texttt{hosp=3} por un lado y en torno a \texttt{hosp=1} por otro lado.

\begin{infobox_resume}

\hypertarget{resumen-1}{%
\subsection*{Resumen}\label{resumen-1}}
\addcontentsline{toc}{subsection}{Resumen}

En este Capítulo se ha introducido el modelo de regresión lineal \emph{generalizado}, indicado cuando las respuestas no son gaussianas (normales). En particular:

\begin{itemize}
\tightlist
\item
  Se ha introducido la función de enlace, que juega un papel importante en estos modelos.
\item
  Se describen los casos particulares de regresión logística y de regresión de Poisson.
\item
  Se muestra el uso de \texttt{R} para el ajuste de estos modelos.
\item
  Se ilustra la interpretación de los coeficientes, tanto asociados a variables continuas como a categóricas, y los demás resultados obtenidos con \texttt{R} mediante casos prácticos.
\item
  Se incluye el uso de la regresión logística como clasificador y se dan indicaciones de su análisis mediante la curva ROC y el AUC.
\end{itemize}

\end{infobox_resume}

\hypertarget{cap-gam}{%
\chapter{Modelos aditivos generalizados}\label{cap-gam}}

\emph{María Durbán y Víctor Casero}

\hypertarget{introducciuxf3n-6}{%
\section{Introducción}\label{introducciuxf3n-6}}

Los modelos lineales, o los lineales generalizados (GLM), vistos en
capítulos anteriores son deseables porque son simples de ajustar, se
entienden fácilmente, y se dispone de técnicas para contrastar las
hipótesis del modelo. Sin embargo, cuando los datos no están
relacionados de forma lineal con las variables explicativas no tiene
sentido utilizar los modelos de regresión lineal, se necesitan modelos
que flexibilicen esta relación, que se puede expresar así:
\[Y=\beta_0+f(X)+\varepsilon\]

Puede que la función \(f()\) sea conocida de antemano, como ocurre en
muchos modelos biológicos, donde existe una dependencia de tipo
exponencial, \(f(x)=e^{\beta_0+\beta_1x}.\) En otras ocasiones dicha
función es desconocida y se puede utilizar una aproximación. Por
ejemplo, la muy utilizada regresión polinómica:
\begin{equation}
Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \ldots + \beta_p X^p + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0,\sigma^2)
\label{eq:polinom}
\end{equation}

Sin embargo, la regresión polinómica tiene un gran inconveniente, que no
se realiza a nivel local: cada vez que se cambia un coeficiente del
modelo, el cambio impacta a los valores ajustados en todo el rango de la
variable explicativa.

La alternativa es utilizar \emph{suavizadores}\index{suavizadores} de las variables explicativas. Se define así el \textbf{modelo aditivo}\index{modelo aditivo} (en el caso de variable respuesta Gaussiana) para al menos dos variables explicativas:
\[Y= f(X_{1})+\ldots +f(X_j)+\epsilon.\]
Las funciones \(f\) incluyen también a las funciones lineales vistas en el Capítulo 16.
Los modelos aditivos generalizados, GAM, extienden lo anterior a respuestas no gaussianas, como lo hacen los GLMs respecto de los modelos lineales con respuesta gaussiana (véase el Capítulo 17).

\hypertarget{splines-con-penalizaciones}{%
\section{Splines con penalizaciones}\label{splines-con-penalizaciones}}

Las técnicas de suavizado tienen como objetivo extraer las tendencias (o
señales) subyacentes en los datos. Un suavizador o \emph{smoother} es una
herramienta que representa la tendencia de la variable respuesta \(Y\)
como función de una o más variables predictoras. Tiene menos
variabilidad que \(Y\), por eso se le llama suavizador (la regresión
lineal es una suavizador llevado al extremo). La curva en un punto dado
sólo depende de las observaciones en ese punto y de las observaciones
vecinas. Estás son algunas de las técnicas de regresión no-paramétrica
existentes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Regresión polinomial local con pesos, \emph{lowess}.
\item
  Kernels.
\item
  Splines.
\end{enumerate}

Este libro se centra en el caso de los Splines que son los suavizadores más utilizados y en particular en los splines con penalizaciones (\emph{P-splines})\index{P-splines}. Estos se basan en representar la función \(f\) mediante una base de funciones y añadir una penalización a la hora de estimar el modelo de manera que se pueda controlar la variabilidad de la curva que se quiere estimar. Hay muchas maneras de representar una función a través de una base, una de las mejores opciones es el uso de los B-splines \citep{boor01} por sus buenas propiedades numéricas. La penalización se construye a partir de la derivada de la curva que se quiere penalizar, en general, se utilizan penalizaciones de orden 2 lo que implica que se está penalizando todo aquello que no es lineal en la función, es decir, si la penalización es muy grande la curva estimada sería simplemente una línea recta. La penalización está controlada por un parámetros llamado \textbf{parámetro de suavizado}\index{parámetros de suavizado}.

A la hora de ajustar este tipo de modelos hay dos elecciones importantes:

\begin{itemize}
\tightlist
\item
  Tamaño de la base: la mayoría de los
  autores utilizan como regla:
  \[ \text{número de nodos}=min\{40,\text{valores únicos de }x/4\}\] (por ejemplo si se tienen 100 datos, se elegirían 100/4=25 nodos)
\item
  Parámetro de suavizado: Se elegirá utilizando máxima verosimilitud junto con el resto de parámetros del modelo. La Figura \ref{fig:psplines} muestra el impacto que el parámetro de suavizado tiene en el ajuste final de la curva (los datos corresponden al dataset \texttt{fossil} del paquete \texttt{Semipar}).
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/psplines-1} 

}

\caption{Regresión con P-splines  para diferentes valores del parámetro de suavizado}\label{fig:psplines}
\end{figure}

\hypertarget{aspectos-metodoluxf3gicos}{%
\section{Aspectos metodológicos}\label{aspectos-metodoluxf3gicos}}

Al igual que en el caso de los modelos lineales y los modelos GLM, en los modelos GAM es necesario conocer algunos aspectos metodológicos que son fundamentales llevar a cabo un ajuste correcto de los modelos, y entender los resultados obtenidos en el ajuste. A continuación se muestran los más relevantes

\hypertarget{estimaciuxf3n-de-los-parauxe1metros-del-modelo}{%
\subsection{Estimación de los paraámetros del modelo}\label{estimaciuxf3n-de-los-parauxe1metros-del-modelo}}

La estimación de los modelos GAM se lleva a cabo mediante máxima verosimilitud penalizada. Supóngase el caso de una sola variable explicativa y se quiere ajustar el modelo:
\[Y= f(X)+\epsilon.\]
Como se comentó anteriormente, los modelos GAM tiene como punto de partida la representación básica de funciones, es decir, se busca
transformar el modelo de tal forma que \(f(X)\) sea el producto de una
matriz multiplicada por unos coeficientes, es decir, se elige una \emph{base}\index{base}
(una matriz \(\textbf{B}\)) que permita escribir la función \(f\) como una
combinación lineal de sus elementos (los elementos de esta base son
conocidos)
\[f(X)=\sum_{j=1}^k b_j(X)\beta_j,\]
o en forma matricial:
\[f(X)=\textbf{B}\boldsymbol{\beta}.\]
y los parámetros \(\boldsymbol{\beta}\) se estiman minimizando la siguiente expresión:
\[(\boldsymbol{y}-\boldsymbol{B}\boldsymbol{\theta})^\prime(\boldsymbol{y}-\boldsymbol{B}\boldsymbol{\theta}) + \lambda\boldsymbol{P},\]

Donde \(\boldsymbol{P}\) es la matriz de penalización y \(\lambda\) es el parámetro de suavizado. Dado un parámetro de suavizado, los parámetros estimados vienen dados por:
\[\hat{\boldsymbol{\theta}} = (\boldsymbol{B}^T \boldsymbol{B} +\lambda \boldsymbol{P} )^{-1}\boldsymbol{B}^\prime\boldsymbol{y}\]
y los valores ajustados se obtendrían como: \(\hat{ \boldsymbol{y}}= \underbrace{\boldsymbol{B}(\boldsymbol{B}^T\boldsymbol{B} +\lambda \boldsymbol{P} )^{-1}\boldsymbol{B}^\prime}_{\boldsymbol{H}}\boldsymbol{y}\). La matriz \(\boldsymbol{H}\) juega un papel importante ya que la suma de su diagonal da una idea de cómo de compleja es la curva ajustada, es lo que se llaman los \emph{grados de libertad efectivos}\index{grados de libertad efectivos} (los cuales no se corresponden al número de parámetros ajustados).

\hypertarget{inferencia-sobre-las-funciones-suaves}{%
\subsection{Inferencia sobre las funciones suaves}\label{inferencia-sobre-las-funciones-suaves}}

Si nuestro interés es saber si existe una relación estadísticamente
significativa entre la variable explicativa \(X_j\) y la variable
respuesta \(Y\):
\begin{eqnarray*}
H_0: f_j=0  & \text{ (no efecto)}\\
H_1: f_j\neq 0 & \text{ (efecto)}
\end{eqnarray*}

\begin{itemize}
\item
  Dado que las funciones \(f_j\) dependen de los coeficientes que
  acompañan a las bases de B-splines, el contraste anterior es
  equivalente a:
  \begin{eqnarray*}
  H_0: \theta_j=0 & \forall j \\
  H_1: \theta_j\neq 0 & \text{ para al menos un }j 
  \end{eqnarray*}
  La distribución del estadístico de contraste dependerá de si la variable
  respuesta sigue una distribución Normal o no:
\item
  Si los datos son Normales, el estadístico de contraste sigue un
  distribución \(F\).
\item
  En otro caso sigue una distribución \(\chi^2\).
\end{itemize}

\textbf{Comparación de modelos}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Modelos Anidados. La comparación se basa, al igual que en los GLM,
  en la diferencia en la \emph{deviance}\index{*Deviance*} residual. Si se quieren comparar dos
  modelos \(m_1\) y \(m_2\) (donde \(m_1\subset m_2\)), entonces:

  \begin{itemize}
  \tightlist
  \item
    En el caso de datos Normales:
    \[\frac{(DR(m_1)-DR(m_2))/(df_2-df_1)}{DR(m_2)/(n-df_2)}\approx F_{(df_2-df_1), (n-df_2)}\]
  \end{itemize}

  donde \(DR\) es la deviance residual (Suma de cuadrados residual) y
  \(df\) son los grados de libertad asociados con cada modelo.

  \begin{itemize}
  \tightlist
  \item
    En otro caso: \[ DR^*(m_1)-DR^*(m_2)\approx \chi^2_{df_2-df_1}\]
    donde \(DR^*\) es la deviance residual escalada.
  \end{itemize}
\item
  Modelos no anidados

  En este caso los contrastes anteriores no son válidos y se
  utilizarán criterios basados en el AIC (Criterio de Información de Akaike).
\end{enumerate}

\hypertarget{suavizado-mutidimensional-y-para-datos-no-gaussianos}{%
\subsection{Suavizado mutidimensional y para datos no Gaussianos}\label{suavizado-mutidimensional-y-para-datos-no-gaussianos}}

Para el suavizado en 2 dimensiones (o más) también se necesita una base y una penalización. El modelo sería:
\[\boldsymbol{Y} =  f\left(\boldsymbol{X}_{1},\boldsymbol{X}_{2}\right)+\epsilon_i \]
donde \(f()\) es una función suave de las dos covariables \(\boldsymbol{X}_{1}\) y \(\boldsymbol{X}_{2}\).
Dicha función se aproxima mediante el producto tensorial de las bases de
B-splines marginales para cada una de las covariables. los términos de suavizado multidimensional se pueden combinar con términos unidimensionales y términos lineales.

La extensión de los modelos de suavizado al caso en el que la variable
respuesta es no Gaussiana, se hace de forma similar al caso lineal,
cuando se pasa de un modelo de regresión lineal a un GLM.

\begin{itemize}
\tightlist
\item
  Como en el caso de los GLMs \(g(\boldsymbol{\mu})=\boldsymbol{\eta}=f(\boldsymbol{X})=\boldsymbol{B} \boldsymbol{\beta}\)
\item
  Se añade la penalización a la función de verosimilitud
  \[\ell_p(\beta)=\ell(\beta)+\lambda \beta^T P \beta ,\]
  donde \(\ell(\beta)\) es la log-verosimilitud.
\end{itemize}

\hypertarget{la-funciuxf3n-gam-del-paquete-mgcv}{%
\section{\texorpdfstring{La función \texttt{gam} del paquete \texttt{mgcv}}{La función gam del paquete mgcv}}\label{la-funciuxf3n-gam-del-paquete-mgcv}}

Aunque hay muchas librerías disponibles la principal es \texttt{mgcv} que
implementa una gran variedad de modelos de suavizado a través de la
función \texttt{gam} (generalized additive models). La principal referencia
para esta sección es el libro de \citet{Wood06book}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gam}\NormalTok{(formula, }\AttributeTok{method =} \StringTok{""}\NormalTok{, }\AttributeTok{select =} \StringTok{""}\NormalTok{, }\AttributeTok{family =} \FunctionTok{gaussian}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{formula} es el argumento principal de esta función, es la ecuación
  del modelo: por ejemplo, \texttt{y\ \textasciitilde{}\ x1+x2+s(x3)}.

  \begin{itemize}
  \tightlist
  \item
    Lo primero que se tiene que elegir es la base a utilizar para
    representar las funciones suaves, \texttt{s(x)} (ver \texttt{?s} o
    \texttt{?smooth.terms}), o \texttt{te(x1,x2)} en el caso de suavizado multidimensional. Por defecto son los llamados thin plate splines.\\
    El tipo de base usada se puede modificar utilizando el argumento
    \texttt{bs} dentro de \texttt{s(x,\ bs\ =\ "ps")}, en este caso \texttt{ps} indica el uso de
    B-splines con penalizaciones. En la siguiente tabla se describen
    otras alternativas:
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}ll@{}}
\toprule()
\textbf{\texttt{bs} } & \textbf{Descripción} \\
\midrule()
\endhead
\texttt{"tp"} & Thin Plate Regression Splines \\
\texttt{"ts"} & Thin Plate Regression Splines con regularización \\
\texttt{"\textbackslash{}cr"} & Spline cúbicos de regresión \\
\texttt{"\ crs"} & Spline cúbicos de regresión con regularización \\
\texttt{"\ cc"} & Spline cíclicos \\
\texttt{"ps"} & P-splines \\
\bottomrule()
\end{longtable}

\begin{itemize}
\tightlist
\item
  \texttt{m} orden de la penalización, por defecto es 2.
\item
  \texttt{k} número de nodos para construir la base. El número por defecto
  suele ser demasiado bajo por lo que siempre se recomienda que el
  usuario elija el número.
\item
  \texttt{by} una variable numérica o factor de la misma dimensión de cada
  covariable que permite hacer interacciones entre curvas y variables
\item
  \texttt{id} se utiliza para forzar que diferentes términos suaves utilicen
  la misma base y la misma cantidad de suavizado
\item
  \texttt{method} método para estimar el parámetro de suavizado, se puede
  elegir entre: \texttt{REML}, \texttt{ML}, \texttt{GCV.Cp}, \texttt{GACV.Cp} (en la práctica se
  prefiere ``REML'')
\item
  \texttt{family} permite elegir la distribución de los datos (binomial,
  Poisson, etc.), por defecto asume datos Gaussianos.
\item
  \texttt{select=TRUE} : Contrasta si la variable debe entrar o no en el modelo
\end{itemize}

\hypertarget{casos-pruxe1cticos-1}{%
\section{Casos prácticos}\label{casos-pruxe1cticos-1}}

En este apartado se verán una serie de aplicaciones que permiten
mostrar los diferentes usos de este tipo de modelos.

\hypertarget{modelo-unidimensional-con-fossil}{%
\subsection{\texorpdfstring{Modelo unidimensional con \texttt{fossil}}{Modelo unidimensional con fossil}}\label{modelo-unidimensional-con-fossil}}

Se empezará ilustrando el uso de la función \texttt{gam} con el conjunto de datos \texttt{fossil} del paquete \texttt{SemiPar}.
El objetivo es estimar la relación entre la edad de los fósiles y la proporción de isotopos de estroncio.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/tsfossil-1} 

}

\caption{Edad de fósiles con respecto a la proporción de isótopos de estroncio}\label{fig:tsfossil}
\end{figure}

A la vista de la Figura \ref{fig:tsfossil}, es claro que se necesita
ajustar una curva (y no una línea) para estimar la relación entre ambas
variables. Para ello se utiliza la función \texttt{gam} que devuelve un objeto de tipo \texttt{"gam"}, que se puede usar
con las típicas funciones \texttt{print}, \texttt{summary}, \texttt{fitted}, \texttt{plot}, \texttt{residuals},
etc\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mgcv)}
\NormalTok{fit.gam }\OtherTok{\textless{}{-}} \FunctionTok{gam}\NormalTok{(Y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{s}\NormalTok{(X, }\AttributeTok{k =} \DecValTok{25}\NormalTok{, }\AttributeTok{bs =} \StringTok{"ps"}\NormalTok{), }\AttributeTok{method =} \StringTok{"REML"}\NormalTok{, }\AttributeTok{select =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(fit.gam)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Family: gaussian }
\CommentTok{\#\textgreater{} Link function: identity }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Formula:}
\CommentTok{\#\textgreater{} Y \textasciitilde{} s(X, k = 25, bs = "ps")}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parametric coefficients:}
\CommentTok{\#\textgreater{}              Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) 7.074e+03  2.435e{-}02  290504   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Approximate significance of smooth terms:}
\CommentTok{\#\textgreater{}        edf Ref.df     F p{-}value    }
\CommentTok{\#\textgreater{} s(X) 10.22     24 35.89  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} R{-}sq.(adj) =  0.891   Deviance explained = 90.2\%}
\CommentTok{\#\textgreater{} {-}REML = 23.946  Scale est. = 0.062849  n = 106}
\end{Highlighting}
\end{Shaded}

Como se puede ver se han especificado la variable respuesta (Y, proporción de estroncio) y la variable explicativa (X, edad) mediante un \emph{spline}\index{spline}, \texttt{s()}, de tipo penalizado, \texttt{ps}, con 25 nodos, indicando \texttt{REML} como método para estimar el parámetro de suavizado.

De las 2 partes de la salida anterior, en la primera aparecen los términos
que entran como lineales en el modelo (en este caso solo la ordenada en
el origen) y en la parte de abajo aparecen los términos de suavizado.
Como se indicó anteriormente, dado que se ha usado \texttt{select=TRUE}, se está contrastando si la variable \texttt{edad} debe entrar en el modelo o no. En este caso es
claro que ha de entrar ya que el p-valor de \texttt{s(x)} pequeño y los grados de libertad asociados son aproximadamente 10, lo que indica que está lejos de la linealidad.

La función \texttt{gam.check} devuelve los gráficos de residuos usuales
(residuos frente a valores ajustados, gráficos de cuantiles para
comprobar la normalidad, etc.), pero además da información sobre el
proceso de ajuste del modelo.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gam.check}\NormalTok{(fit.gam)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/fig8-1} 

}

\caption{Gráficos de residuos obtenidos con `gam.check` }\label{fig:fig8}
\end{figure}

\begin{verbatim}
#> 
#> Method: REML   Optimizer: outer newton
#> full convergence after 5 iterations.
#> Gradient range [-4.557319e-06,5.900236e-06]
#> (score 23.94602 & scale 0.06284944).
#> Hessian positive definite, eigenvalue range [4.557347e-06,53.03185].
#> Model rank =  25 / 25 
#> 
#> Basis dimension (k) checking results. Low p-value (k-index<1) may
#> indicate that k is too low, especially if edf is close to k'.
#> 
#>        k'  edf k-index p-value
#> s(X) 24.0 10.2    1.03    0.59
\end{verbatim}

El test que aparece en la parte de abajo está contrastando si el número
de nodos elegido es suficiente. Si el valor de \texttt{k} está muy próximo a
\texttt{edf} entonces se debería reajustar el modelo con más nodos.

Si se quiere dibujar la función suave se utiliza el comando
\texttt{plot()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(fit.gam, }\AttributeTok{shade =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{seWithMean =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{pch =} \DecValTok{19}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{cex =}\NormalTok{ .}\DecValTok{55}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/fig9-1} 

}

\caption{Curva ajustada e intervalo de confianza}\label{fig:fig9}
\end{figure}

\hypertarget{modelo-aditivo-con-airquality}{%
\subsection{\texorpdfstring{Modelo aditivo con \texttt{airquality}}{Modelo aditivo con airquality}}\label{modelo-aditivo-con-airquality}}

En esta sección se analizan de nuevo los datos \texttt{airquality} (ver ?airquality\footnote{conjunto de datos incluido con la instalación base de R.}) que
consisten en 154 medidas de calidad del aire en Nueva
York, de Mayo a Septiembre 1973. El objetivo es establecer la relación entre las variables meteorológicas
y la cantidad de ozono en la atmósfera. Ya se ha analizado la relación en el Capítulo 16 de Modelos lineales, donde los ajustes lineales realizados eran satisfactorios, pero se encontraban problemas en los residuos del modelo, que no daban validez a dicha modelización. Allí se sugería que la relación entre la variable respuesta y alguna explicativa fuese no lineal. Además se consideró la transformación logarítmica de la variable \texttt{Ozone}, que adolece de simetría, y con dicha trasformación se obtenía una distribución más similar a la distribución Normal.

Por lo tanto se va a ajustar el modelo incluyendo las variables explicativas de forma suave, en particular se van a incluir las variables \texttt{Wind}, \texttt{Temp} y \texttt{Solar.R}.
Las variable \texttt{Wind} y \texttt{Temp} tienen sólo 31 y 40 valores únicos respectivamente, aunque el dataset tiene 154 valores, por eso se decide utilizar 10 nodos y no más, mientras que para la variable \texttt{Solar.R} se utilizan 20.

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{airq.gam }\OtherTok{\textless{}{-}} \FunctionTok{gam}\NormalTok{(}
  \FunctionTok{log}\NormalTok{(Ozone) }\SpecialCharTok{\textasciitilde{}} \FunctionTok{s}\NormalTok{(Wind, }\AttributeTok{bs =} \StringTok{"ps"}\NormalTok{, }\AttributeTok{k =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{s}\NormalTok{(Temp, }\AttributeTok{bs =} \StringTok{"ps"}\NormalTok{, }\AttributeTok{k =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{s}\NormalTok{(Solar.R, }\AttributeTok{bs =} \StringTok{"ps"}\NormalTok{, }\AttributeTok{k =} \DecValTok{20}\NormalTok{),}
  \AttributeTok{method =} \StringTok{"REML"}\NormalTok{, }\AttributeTok{select =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{data =}\NormalTok{ airquality, }\AttributeTok{na.action =}\NormalTok{ na.omit}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(airq.gam)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Family: gaussian }
\CommentTok{\#\textgreater{} Link function: identity }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Formula:}
\CommentTok{\#\textgreater{} log(Ozone) \textasciitilde{} s(Wind, bs = "ps", k = 10) + s(Temp, bs = "ps", }
\CommentTok{\#\textgreater{}     k = 10) + s(Solar.R, bs = "ps", k = 20)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parametric coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)  3.41593    0.04586   74.49   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Approximate significance of smooth terms:}
\CommentTok{\#\textgreater{}              edf Ref.df     F  p{-}value    }
\CommentTok{\#\textgreater{} s(Wind)    2.318      9 2.255 3.13e{-}05 ***}
\CommentTok{\#\textgreater{} s(Temp)    1.852      9 6.128  \textless{} 2e{-}16 ***}
\CommentTok{\#\textgreater{} s(Solar.R) 2.145     19 1.397 2.31e{-}06 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} R{-}sq.(adj) =  0.689   Deviance explained = 70.7\%}
\CommentTok{\#\textgreater{} {-}REML = 86.106  Scale est. = 0.23342   n = 111}
\end{Highlighting}
\end{Shaded}

Los resultados indican que todas las variables son significativas (p-valores pequeños), estando la variable \texttt{Temp} próxima a la linealidad. El
\(R^2\) ajustado es 0.68 por lo que el modelo ajusta moderadamente bien
los datos. La Figura \ref{fig:fig18} muestra la tres curvas ajustadas,
incluyendo los llamados \emph{residuos parciales}\index{residuos parciales} que corresponden a, por
ejemplo en el caso del gráfico del viento,
\(log(Ozone)-\hat \beta_0-\hat f(Temp)- \hat f(Solar.R)\), es decir, lo
que queda sin explicar después de haber ajustado los demás términos del
modelo.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(airq.gam, }\AttributeTok{residuals =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{scheme =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/fig18-1} 

}

\caption{curvas estimadas para  `Wind`, `Temp` y `Solar` }\label{fig:fig18}
\end{figure}

\hypertarget{modelo-semiparamuxe9trico-con-onions}{%
\subsection{\texorpdfstring{Modelo semiparamétrico con \texttt{onions}}{Modelo semiparamétrico con onions}}\label{modelo-semiparamuxe9trico-con-onions}}

Es un caso particular del modelo aditivo anterior pues en este tipo de modelos todas
las variables entran de forma lineal excepto una.
\[\boldsymbol{Y}= \beta_0 +\beta_1 \boldsymbol{X}_{1}+\ldots +
\beta_{j-1}\boldsymbol{X}_{j-1}+f(\boldsymbol{X}_j)+\epsilon\]

La forma de ajustar el modelo es exactamente igual a la
anterior. Pero hay un caso que merece especial interés,
cuando en la parte paramétrica se incluye una variable categórica con
dos o más niveles. Al igual que en el caso de regresión lineal se puede
plantear si se quieren ajustar dos rectas paralelas (modelo aditivo) o
no paralelas (modelo con interacción).

Para ilustrar este caso se acude al data.frame \texttt{onions} (librería
\texttt{SemiPar}). Contiene 84 observaciones de un experimento sobre la
producción de un tipo de cebolla en dos localidades (Purnong Landing y Virginia).
El objetivo es relacionar el logaritmo de la producción de cebollas con
la densidad de plantas por metro cuadrado. El modelo lineal básico
sería:
\[ \log(\text{yield}_i) = \beta_0 + \beta_1\text{location}_{ij} + \beta_2 \text{dens}_i + \epsilon_i\]
donde
\[\text{location}_{ij} = 
\left\{\begin{array}{cl}
0 & \mbox{si la observación $i$ es de Purnong Landing} \\
1 & \mbox{si la observación $i$ es de Virginia}
\end{array}\right.\]

Se comienza por ajustar el siguiente modelo: \[
\log(\text{yield}_i) = \beta_0 + \beta_1\text{location}_{ij} + f(\text{dens}_i) + \epsilon_i\]

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mgcv)}
\FunctionTok{library}\NormalTok{(SemiPar)}
\FunctionTok{data}\NormalTok{(onions)}
\CommentTok{\# Se indica a R que la variable location es categórica}
\NormalTok{onions}\SpecialCharTok{$}\NormalTok{location }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(onions}\SpecialCharTok{$}\NormalTok{location)}
\CommentTok{\# Se recodifica la variable}
\FunctionTok{levels}\NormalTok{(onions}\SpecialCharTok{$}\NormalTok{location) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Purnong Landing"}\NormalTok{, }\StringTok{"Virginia"}\NormalTok{)}
\NormalTok{fit1 }\OtherTok{\textless{}{-}} \FunctionTok{gam}\NormalTok{(}\FunctionTok{log}\NormalTok{(yield) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ location }\SpecialCharTok{+} \FunctionTok{s}\NormalTok{(dens, }\AttributeTok{k =} \DecValTok{20}\NormalTok{, }\AttributeTok{bs =} \StringTok{"ps"}\NormalTok{),}
  \AttributeTok{method =} \StringTok{"REML"}\NormalTok{, }\AttributeTok{select =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{data =}\NormalTok{ onions}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(fit1)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Family: gaussian }
\CommentTok{\#\textgreater{} Link function: identity }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Formula:}
\CommentTok{\#\textgreater{} log(yield) \textasciitilde{} location + s(dens, k = 20, bs = "ps")}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parametric coefficients:}
\CommentTok{\#\textgreater{}                  Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)       4.85011    0.01688  287.39   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} locationVirginia {-}0.33284    0.02409  {-}13.82   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Approximate significance of smooth terms:}
\CommentTok{\#\textgreater{}           edf Ref.df     F p{-}value    }
\CommentTok{\#\textgreater{} s(dens) 4.568     19 72.76  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} R{-}sq.(adj) =  0.946   Deviance explained = 94.9\%}
\CommentTok{\#\textgreater{} {-}REML = {-}54.242  Scale est. = 0.011737  n = 84}
\end{Highlighting}
\end{Shaded}

En este caso se ve que en la parte lineal aparecen dos parámetros, ambos significativos, la
ordenada en el origen y el parámetro que corresponde a la variable
categórica \texttt{location}, concretamente el parámetro correspondiente a la
categoría Virginia que es negativo indicando que la producción media para la
otra localidad es mayor que la producción media en Virginia. El término
de suavizado es significativo también.

En este caso si se utiliza la función \texttt{plot.gam}, sólo dibujará una
curva, pues las curvas para las dos localizaciones son paralelas y la
diferencia entre ellas es igual al valor del parámetro de la variable
localización. Para dibujar las curvas para cada localización se utiliza
la función \texttt{plot\_smooth} de la librería \texttt{tidymv}, los argumentos son,
primero el modelo, después la variable con respecto a la que se está
ajustando la curva y por último la variable categórica.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidymv)}
\FunctionTok{plot\_smooths}\NormalTok{(fit1, dens, location)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/fig20-1} 

}

\caption{curvas ajustadas para ambas localidades}\label{fig:fig20}
\end{figure}

Asumir curvas paralelas para ambas localidades implica que el descenso
en producción de cebollas a medida que aumenta la densidad de plantas es
el mismo para ambas localidades, y esto no tienen por qué ser cierto,
para relajar esta hipótesis se puede ajustar un modelo con interacción
(de manera similar a lo que se hace en el caso de regresión lineal):
\[\log(\text{yield}_i) =\beta_0 +  \beta_1\text{location}_{ij} +  f(\text{dens}_i)_{L(j)} + \epsilon_i\]
donde
\[L(j) = 
\left\{\begin{array}{cl}
0 & \mbox{si la $i$-ésima observación es de Purnong Landing} \\
1 & \mbox{si la $i$-ésima observación es de Virginia}
\end{array}\right.\]
Para hacerlo en \texttt{R}, se introduce el argumento \texttt{by=location} dentro de
la curva

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit2 }\OtherTok{\textless{}{-}} \FunctionTok{gam}\NormalTok{(}\FunctionTok{log}\NormalTok{(yield) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ location }\SpecialCharTok{+} \FunctionTok{s}\NormalTok{(dens, }\AttributeTok{k =} \DecValTok{20}\NormalTok{, }\AttributeTok{bs =} \StringTok{"ps"}\NormalTok{, }\AttributeTok{by =}\NormalTok{ location),}
  \AttributeTok{method =} \StringTok{"REML"}\NormalTok{, }\AttributeTok{data =}\NormalTok{ onions}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(fit2)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Family: gaussian }
\CommentTok{\#\textgreater{} Link function: identity }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Formula:}
\CommentTok{\#\textgreater{} log(yield) \textasciitilde{} location + s(dens, k = 20, bs = "ps", by = location)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Parametric coefficients:}
\CommentTok{\#\textgreater{}                  Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept)       4.84415    0.01603  302.19   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} locationVirginia {-}0.33018    0.02270  {-}14.54   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Approximate significance of smooth terms:}
\CommentTok{\#\textgreater{}                                   edf Ref.df     F p{-}value    }
\CommentTok{\#\textgreater{} s(dens):locationPurnong Landing 3.096  3.834 176.9  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} s(dens):locationVirginia        4.742  5.795 153.0  \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} R{-}sq.(adj) =  0.952   Deviance explained = 95.7\%}
\CommentTok{\#\textgreater{} {-}REML = {-}58.541  Scale est. = 0.010446  n = 84}
\end{Highlighting}
\end{Shaded}

Ahora aparecen dos términos suaves, uno para cada localidad, de modo que
estas curvas no tienen por qué ser paralelas, sino que cada una se
ajustará a la forma que tengan los datos. En este caso la Figura
\ref{fig:fig21}, generada de nuevo con \texttt{plot\_smooths}, muestra como las curvas se van alejando a medida que
aumenta la densidad de plantas.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/fig21-1} 

}

\caption{curvas ajustadas para ambas localidades permitiendo que no sean paralelas}\label{fig:fig21}
\end{figure}

Para finalizar se comparan ambos modelos con el criterio AIC.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{AIC}\NormalTok{(fit1)}
\CommentTok{\#\textgreater{} [1] {-}125.2307}
\FunctionTok{AIC}\NormalTok{(fit2)}
\CommentTok{\#\textgreater{} [1] {-}131.2181}
\end{Highlighting}
\end{Shaded}

Dado que el menor valor se alcanza en el segundo modelo, se escogería el
modelo que incluye la interacción entre la variable densidad y la
localidad.

\hypertarget{modelo-aditivo-generalizado-y-multidimensional-con-smacker}{%
\subsection{\texorpdfstring{Modelo aditivo generalizado y multidimensional, con \texttt{smacker}}{Modelo aditivo generalizado y multidimensional, con smacker}}\label{modelo-aditivo-generalizado-y-multidimensional-con-smacker}}

Se van a analizar los datos \texttt{smacker} del paquete \texttt{sm}.
El objetivo es ver cómo influyen las
condiciones del mar (temperatura de agua, etc.) en la ausencia o
presencia de huevos de jurel en el mar cantábrico, además, se utilizará la posición geográfica (latitud y longitud) como covariables para captar el efecto espacial.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/fig27-1} 

}

\caption{Localización donde se midió la ausencia/presecia de huevos de jurel}\label{fig:fig27}
\end{figure}

Dado que la variable respuesta es dicotómica se va a utilizar un modelo
de regresión logística en el que se va a flexibilizar la linealidad de
las variables y además se usará una superficie para estimar el efecto de
la localización como una función en dos dimensiones (en función de la
posición geográfica: latitud y longitud). En este caso en vez de usar el
término \texttt{te()} se usa \texttt{s()} también para el caso de 2 dimensiones. La
diferencia fundamental con \texttt{te()} es que \texttt{s()} asume un suavizado
isotrópico, es decir el mismo parámetro de suavizado para la Latitud y
Longitud. No se debe usar \texttt{s()} para el suavizado en dos dimensiones si
las covariables están medidas en unidades diferentes (en este caso como
sí lo están, se puede usar el suavizado isotrópico)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logit1 }\OtherTok{\textless{}{-}} \FunctionTok{gam}\NormalTok{(}
\NormalTok{  Presence }\SpecialCharTok{\textasciitilde{}} \FunctionTok{s}\NormalTok{(smack.long, smack.lat, }\AttributeTok{k =} \DecValTok{60}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{s}\NormalTok{(ldepth) }\SpecialCharTok{+} \FunctionTok{s}\NormalTok{(Temperature),}
  \AttributeTok{family =}\NormalTok{ binomial, }\AttributeTok{select =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{data =}\NormalTok{ smacker}
\NormalTok{)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{pty =} \StringTok{"s"}\NormalTok{, }\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(logit1, }\AttributeTok{scheme =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/unnamed-chunk-91-1} \end{center}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/unnamed-chunk-91-2} \end{center}

La relación entre la temperatura y la probabilidad de presencia de huevos es
no lineal mientras que es lineal para la profundidad. El \(R^2\) es \(0.4\)
por lo que sería necesario recoger más variables para poder obtener
buenas predicciones.

Si se quisieran obtener las probabilidades predichas se haría con la
función \texttt{predict}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prob }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(logit1, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{infobox_resume}

\hypertarget{resumen-2}{%
\subsection*{Resumen}\label{resumen-2}}
\addcontentsline{toc}{subsection}{Resumen}

En este capítulo se han introducido los modelos aditivos generalizados, en particular:

\begin{itemize}
\tightlist
\item
  Se han presentado distintos aspectos metodológicos relacionados con la estimación e inferencia en modelos GAM.
\item
  Se ha mostrado el uso de \texttt{R} para el ajuste de este tipo de modelos.
\item
  Se han presentado diversos casos prácticos que ilustran la versatilidad de estos modelos para analizar datos complejos.
\end{itemize}

\end{infobox_resume}

\hypertarget{cap-mxm}{%
\chapter{Modelos mixtos}\label{cap-mxm}}

\emph{María Durbán y Víctor Casero}

\hypertarget{conceptos-buxe1sicos}{%
\section{Conceptos básicos}\label{conceptos-buxe1sicos}}

Los modelos mixtos (MMs) para variables de respuesta continuas son
modelos estadísticos en los que los residuos siguen una distribución
Normal pero puede que no sean independientes o no tengan varianza
constante. Son necesarios en muchas situaciones, sobre todo en
experimentos donde se realiza algún tipo de muestreo:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estudios con datos agrupados, como por ejemplo, alumnos en una
  clase, individuos en una ciudad.
\item
  Estudios longitudinales o de medidas repetidas, donde un individuo
  es medido repetidamente a lo largo del tiempo o bajo condiciones
  distintas.
\end{enumerate}

Este tipo de estudios se pueden encontrar en diferentes áreas como la
Medicina, Biología, Ciencias Experimentales y Sociales.

\hypertarget{tipo-y-estructura-de-los-datos}{%
\subsection{Tipo y estructura de los datos}\label{tipo-y-estructura-de-los-datos}}

La estructura de los datos con la que se trabaja es el factor
determinante para saber si se han de utilizar modelos mixtos, y en su
caso, qué tipo de modelo.

\textbf{Datos jerárquicos (o agrupados)}\index{datos jerárquicos}

En este tipo de datos la variable dependiente (de respuesta, de interés)
se mide una sola vez en cada individuo (la unidad de análisis), y los
individuos está agrupados (o anidados) en unidades mayores. Muchos tipos
de datos tienen una estructura jerárquica: alumnos en escuelas, personas
en municipios, pacientes en hospitales, plantas en una parcela\ldots{}

Las jerarquías son una forma de representar la relación de dependencia
que hay entre los individuos y los grupos a los que pertenecen. Por
ejemplo, supóngase que se quiere hacer un estudio sobre el tiempo de
recuperación en pacientes hospitalizados por COVID-19 en diferentes
hospitales. Se tiene la siguiente estructura con dos niveles:

\begin{itemize}
\tightlist
\item
  Muchos individuos en el nivel \(1\) (pacientes)
\item
  Agrupados en unas pocas unidades en el nivel \(2\) (hospitales)
\end{itemize}

\begin{center}\includegraphics[width=0.6\linewidth]{./img/Presentacion1} \end{center}

Las estructuras multinivel pueden aparecer también como consecuencia del
diseño del estudio que se está llevando a cabo. Por ejemplo, una
encuesta sobre el estado de salud puede dar lugar a un diseño a tres
niveles: primero se muestrean regiones, luego distritos y después
individuos.

En cada nivel de la jerarquía se pueden medir variables. Algunas estarán
medidas en su nivel \emph{natural}, por ejemplo, en el nivel del hospital se
podría medir el tamaño, y al nivel de los pacientes se podría medir su
situación socio-económica. Además, se pueden mover las variables de un
nivel a otro mediante agregación o desagregación:

\begin{itemize}
\tightlist
\item
  \textbf{Agregación}:\index{agregación} La variable al nivel más bajo se mueve a un nivel
  más alto, por ejemplo, se pueden asociar a cada hospital la media
  del nivel socioeconómico de sus pacientes.
\item
  \textbf{Desagregación}:\index{desagregación} Mover las variables a un nivel más bajo, por
  ejemplo, asignarle a cada paciente una variable que indique el
  tamaño de su hospital de referencia.
\end{itemize}

\textbf{Medidas repetidas y datos longitudinales}

En este tipo de datos la variable dependiente se mide más de una vez a
un mismo individuo \citep{Singer03}. Por ejemplo, se miden los niveles de
glucosa de un enfermo antes y después de haberle inyectado insulina.
Este tipo de datos también puede ser considerados como datos multinivel
(o jerárquicos) donde el Nivel 2 representa a los individuos y el Nivel
1 representa a las diferentes medidas tomadas. Dado que las medidas se
toman a un mismo individuo, es probable que dichas medidas no sean
independientes, por lo que utilizar un modelo lineal ordinario no sería
apropiado.

\begin{center}\includegraphics[width=0.6\linewidth]{./img/Presentacion3} \end{center}

Por \textbf{datos longitudinales}\index{datos longitudinales}, se entienden datos en los que la variable
dependiente se ha medido en distintos instantes de tiempo en cada una de
las unidades de análisis. En algunos casos, cuando la variable
dependiente se mide a lo largo del tiempo, puede ser difícil identificar
si los datos son medidas repetidas o datos longitudinales. Desde el
punto de vista del análisis de los datos mediante MMs esta distinción no
es un elemento crítico. Lo importante es que en ambos tipos de datos la
variable dependiente se ha medido repetidas veces en la misma unidad de
análisis, y que por tanto las observaciones estarán correlacionadas.

\hypertarget{efectos-fijos-o-aleatorios}{%
\subsection{¿Efectos fijos o aleatorios?}\label{efectos-fijos-o-aleatorios}}

En un modelo mixto la clave se encuentra en la distinción entre efectos
fijos y aleatorios \citep{Snijers03}. Esto es importante porque la inferencia y
el análisis de ambos efectos es distinta.

Los \textbf{efectos fijos}\index{efectos fijos} son variables en las cuales el investigador ha
incluido sólo los niveles (o tratamientos) que son de su interés. Por
ejemplo, en un experimento se puede estar interesados en comparar dos
grupos, uno al que se le aplica un tratamiento y otro de control. En
este caso, el objetivo del estudio compara los grupos y no interesa
generalizar los resultados a otros tratamientos que podrían haber sido
incluidos. Otro ejemplo sería el caso en el que se hace un encuesta y se
eligen 10 ciudades. Si sólo interesan los resultados para esas 10
ciudades y no se quieren generalizar los resultados al resto de ciudades
que podrían haber sido seleccionadas, la variable `ciudad' será un
efecto fijo. Si se eligen las ciudades de forma aleatoria de una
población grande de ciudades se consideraría la variable `ciudad' como
un \textbf{efecto aleatorio}\index{efectos aleatorios}.

Una cantidad se considera aleatoria cuando cambia sobre las unidades de
una población. Cuando un efecto en un modelo estadístico es considerado
aleatorio, se está asumiendo que se quieren extraer conclusiones sobre
la población de la cual se han elegido las unidades observadas, y no se
tiene interés en esas unidades en particular. En este contexto se habla
de \emph{intercambiabilidad}\index{intercambiabilidad}, en el sentido de que se podría cambiar una
unidad de la muestra por otra de la población y sería indiferente. Este
es el caso de los factores de agrupamiento o diseño, como son los
bloques en un experimento agrícola, o los días cuando un experimento se
lleva a cabo en días distintos, o un técnico de laboratorio cuando hay
varios haciendo el experimento; también lo serían los sujetos en un
diseño de medidas repetidas o las localizaciones donde se recogen
muestras en un río, si el objetivo es generalizar a todo el río.

Los métodos estándar utilizados para construir tests e intervalos de
confianza para los efectos fijos, no son válidos para los efectos
aleatorios, pues los efectos observados son sólo una muestra de todos
los posibles efectos.

La clave para distinguir, estadísticamente hablando, entre efectos fijos
y aleatorios es si los niveles de la variable se pueden interpretar como
extraídos de una población con una cierta distribución de probabilidad.
En el caso de un efecto fijo, normalmente interesará comparar los
resultados de la variable dependiente para los distintos niveles de la
variable explicativa, es decir, interesará la diferencia entre las
medias. En el caso de efectos aleatorios, no interesa específicamente
comparar si las medias son distintas, sino cómo el efecto aleatorio
explica la variabilidad en la variable dependiente. Por lo tanto, para
que un efecto pueda considerarse aleatorio, es necesario que la variable
dependiente presente cierta variabilidad no explicada asociada con las
unidades del efecto aleatorio.

La siguiente tabla puede ayudar ayudar a determinar si un efecto es fijo
o aleatorio

\begin{center}\includegraphics[width=0.6\linewidth]{./img/efectos} \end{center}

Por ejemplo, en un estudio sobre satisfacción en el trabajo (variable
dependiente) de los empleados (unidades observadas) de un cierto número
de empresas (efecto aleatorio), si el nivel de satisfacción de los
empleados de unas empresas es mayor que el de otras y el investigador no
lo tiene en cuenta, habrá una cierta variabilidad residual asociada con
el efecto `empresa'. Si esta variabilidad fuera próxima a cero, no sería
necesario incluir el efecto aleatorio asociado con la empresa.

\textbf{¿Por qué hay que utilizar modelos mixtos?}

Cuando las observaciones están agrupadas en niveles o siguen una cierta
jerarquía, las unidades se ven afectados por el grupo al que pertenecen.
Las jerarquías (o niveles) permiten representar la relación de
dependencia entre los individuos y los grupos a los que pertenecen. Los
alumnos que están en una misma escuela se parecen más entre sí que si se
hubieran seleccionado aleatoriamente de entre toda la población de
alumnos. Los modelos mixtos permiten tener en cuenta que las
observaciones no son independientes.

\hypertarget{formulaciuxf3n-del-modelo-con-efectos-aleatorios-o-modelos-mixtos}{%
\section{Formulación del modelo con efectos aleatorios o modelos mixtos}\label{formulaciuxf3n-del-modelo-con-efectos-aleatorios-o-modelos-mixtos}}

El nombre de \emph{modelos mixtos lineales}\index{modelos mixtos lineales} viene del hecho de que estos
modelos son lineales en los parámetros, y en las covariables, y pueden
implicar efectos fijos o aleatorios. Son, por lo tanto, una extensión de
los modelos lineales de regresión.

\hypertarget{formulaciuxf3n-general}{%
\subsection{Formulación general}\label{formulaciuxf3n-general}}

La formulación general de un modelo mixto tiene la siguiente forma:
\[ y = X \beta + Z u + \epsilon, \quad u\sim N(0, G), \quad \epsilon \sim N(0, R)\]
donde

\begin{itemize}
\tightlist
\item
  \(X\) es una matriz \(n \times k\) (\(k\) es el número de efectos fijos)
\item
  \(Z\) es una matriz \(n \times p\) (\(p\) es el número de efectos
  aleatorios)
\item
  \(\beta\) es el vector de efectos fijos y \(u\) el de efectos aleatorios
\item
  \(G\) es la matriz de varianzas-covarianzas de los efectos aleatorios,
  con dimensión \(p \times p\)
\item
  \(R\) es la matriz de varianzas-covarianzas del error
\end{itemize}

\hypertarget{estimaciuxf3n-de-beta-y-u}{%
\subsubsection{\texorpdfstring{Estimación de \(\beta\) y \(u\)}{Estimación de \textbackslash beta y u}}\label{estimaciuxf3n-de-beta-y-u}}

Se hace mediante las llamadas \textbf{ecuaciones de Henderson}\index{ecuaciones de Henderson} \citep{Henderson1953}. Permiten
obtener el mejor estimador lineal insesgado de \(X\beta\) y el mejor
predictor lineal insesgado de \(u\). Se obtienen maximizando la densidad
conjunta de \(y\) y \(u\):
\[f(y, u) = f(y | u)f(u), \quad y | u \sim
    N(X\beta + Zu, R)\quad u\sim N(0, G).\]
Derivando con respecto a \(\beta\) y \(u\) se obtienen las ecuaciones de
Henderson cuya solución es:
\begin{eqnarray*}
\hat \beta = \left ( X^\prime V^{-1} X \right )^{-1}X^\prime V^{-1} y \\ 
\hat u = G Z^\prime V^{-1} (y -X \hat \beta),
\end{eqnarray*}
donde \(V = Z G Z^\prime + R\). Pero, \(V\) depende de los parámetros de la
varianza en el modelo que forman parte de \(G\) y \(R\) y que es necesario
estimar, como se muestra a continuación.

\hypertarget{estimaciuxf3n-de-los-componentes-de-la-varianza}{%
\subsubsection{Estimación de los componentes de la varianza}\label{estimaciuxf3n-de-los-componentes-de-la-varianza}}

Los métodos más comunes para la estimación de los parámetros de las
matrices de covarianza son: Máxima verosimilitud (ML) o Máxima
verosimilitid restringida (REML). No existe una solución cerrada para los estimadores, y se hace de forma
numérica o mediante algoritmos iterativos. REML tiene en cuenta los grados de libertad utilizados para estimar los
efectos fijos en el modelo. Si \(n\) es pequeño, REML dará mejores
estimaciones que ML, si \(n\) es grande, no habrá prácticamente ninguna
diferencia. El método preferido es REML.

\hypertarget{inferencia-y-selecciuxf3n-del-modelo}{%
\subsection{Inferencia y selección del modelo}\label{inferencia-y-selecciuxf3n-del-modelo}}

\hypertarget{contrastes-de-hipuxf3tesis-para-los-efectos-fijos-beta}{%
\subsubsection{\texorpdfstring{Contrastes de hipótesis para los efectos fijos, \(\beta\)}{Contrastes de hipótesis para los efectos fijos, \textbackslash beta}}\label{contrastes-de-hipuxf3tesis-para-los-efectos-fijos-beta}}

Utilizando la distribución aproximada:
\[\hat \beta \sim N \left ( \beta, \underbrace{(X^\prime \hat V^{-1} X )^{-1}}_{Var(\hat \beta)} \right )\]

\begin{itemize}
\tightlist
\item
  Si se contrastan parámetros individuales se acudirá al t-test para
  un solo efecto,
\item
  Si se contrastan un conjunto de parámetros se acudirá al F-test para
  más de un efecto,
\item
  También se pueden comparar modelos usando el test de la razón de
  verosimilitud, LRT por sus siglas en inglés (ojo! hay que utilizar ML para estimar los parámetros de la varianza)
  \begin{equation} 
  LRT = -2\left [ ln(l_{H_0})- ln(l_{H_1}) \right ]\approx \chi^2_{df}.
  \label{eq:LRT}
  \end{equation}
\end{itemize}

\hypertarget{contrastes-de-hipuxf3tesis-para-los-paruxe1metros-de-varianza}{%
\subsubsection{Contrastes de hipótesis para los parámetros de varianza}\label{contrastes-de-hipuxf3tesis-para-los-paruxe1metros-de-varianza}}

Al usar el test LRT \eqref{eq:LRT} se ha de tener en cuenta
que la distribución asintótica del estadístico del test depende de si el
valor del parámetro bajo la hipótesis nula (\(H_0\)) está en la frontera del
espacio paramétrico (es decir si se está testando si el parámetro de
varianza es cero o no):

\begin{itemize}
\item
  \textbf{Caso 1}: El valor de los parámetros de varianza bajo la \(H_0\) no
  están en la frontera del espacio paramétrico (por ejemplo, al
  contrastar que los parámetros de varianza de dos efectos aleatorios
  son iguales o no). En ese caso se utiliza el test normalmente.
\item
  \textbf{Caso 2}: El valor de los parámetros de varianza bajo la \(H_0\)
  están en la frontera del espacio paramétrico (por ejemplo, si se
  quiere contrastar que la varianza del efecto aleatorio es cero o
  no). La distribución asintótica del estadístico del test es una
  mixtura entre \(\chi^2_p\) y \(\chi^2_{p-1}\),
  concretamente \(0.5 \chi^2_p + 0.5\chi^2_{p-1}\),
  donde \(p\) es el número de parámetros de la varianza que se hacen
  cero bajo la \(H_0\).
\end{itemize}

\hypertarget{diagnosis-del-modelo}{%
\subsection{Diagnosis del modelo}\label{diagnosis-del-modelo}}

En el caso de modelos mixtos se ha de verificar la hipótesis de
normalidad tanto para los residuos al nivel más bajo (medidas repetidas
en los raíles) como para los efectos aleatorios (raíles), y también las
de independencia (en su caso).

En el caso de los modelos mixtos, se utilizan los residuos
condicionales, estos son la diferencia entre los valores observados y el
valor predicho condicional:
\[\hat\epsilon = y-X \hat \beta -Z \hat u. \]
Estos residuos tienden a estar correlados y sus varianzas pueden cambiar
de un grupo a otro, aunque en el verdadero modelo los residuos sean
incorrelados y con varianza constante. Para solucionar este problema se
pueden escalar los residuos por sus desviaciones estándar (o las
estimaciones de éstas), dando lugar a los \emph{residuos estandarizados}\index{residuos estandarizados} (si
las desviaciones estándar son conocidas), o a los \emph{residuos studentizados}\index{residuos studentizados} (si son desconocidas y se utilizan estimaciones de las
mismas). Con estos residuos se hace un análisis similar al caso de
modelos de regresión lineal.

Además se tendrá que comprobar la hipótesis de normalidad de los efectos
aleatorios

\hypertarget{funciones-de-r-para-ajustar-modelos-mixtos}{%
\section{\texorpdfstring{Funciones de \texttt{R} para ajustar modelos mixtos}{Funciones de R para ajustar modelos mixtos}}\label{funciones-de-r-para-ajustar-modelos-mixtos}}

Hay varios paquetes de \texttt{R} para el ajuste de modelos mixtos. Los más usados son \texttt{nlme} y \texttt{lme4}. El segundo es una versión del primero que
incluye modelos más generales, y mejora los gráficos. A continuación se
describe la función principal del paquete \texttt{lme4}.

\hypertarget{la-funciuxf3n-lmer}{%
\subsection{\texorpdfstring{La función \texttt{lmer()}}{La función lmer()}}\label{la-funciuxf3n-lmer}}

Esta función permite el uso de efectos aleatorios anidados y de errores
correlados o heterocedásticos dentro de los grupos. En general para
definir un modelo mixto se necesita especificar la estructura de la
media y de la parte aleatoria del modelo, incluidos los factores de
agrupamiento, así como la estructura de correlación (si la hay).

También se puede especificar el método de estimación: `'REML'' o `'ML''.

La parte aleatoria del modelo se incluye entre paréntesis en la ecuación y \(|\) separa las variables de agrupamiento de las
predictoras, si no hay variables predictoras para la parte aleatoria se pondría un 1.

La función \texttt{VarCorr()} aplicada a un objeto \texttt{lmer} da información sobre la estructura de componentes
de varianza.

\hypertarget{caso-pruxe1ctico}{%
\section{Caso práctico}\label{caso-pruxe1ctico}}

En esta sección se verá como construir diferentes modelos con efectos aleatorios según a qué nivel estén medidas las variables explicativas y se terminará dando una guía sobre como construir estos modelos en la práctica. Los datos con los que se va a trabajar se encuentran en el dataframe \texttt{Hsb82} del paquete \texttt{mlmRev} y provienen de un estudio titulado \emph{High School and Beyond}. Los datos corresponden a 7185 estudiantes repartidos en 160 escuelas,
el número de alumnos por escuela varía entre 14 y 67.
La variable de interés, \texttt{mAch} es el nivel estandarizado alcanzado en
matemáticas. Una cuestión inicial que se puede plantear es si el nivel socioeconómico (\texttt{cses})
del alumno predice las diferencias en el nivel de matemáticas. Para ello
se ajustaría el modelo:
\[y_j = \beta_0 + \beta_1x_j + \epsilon_j, \]
este modelo ignora que los alumnos provienen de distintos centros (por
eso solo aparece el subíndice j que es el que representa a las unidades
de nivel más bajo, en este caso a los alumnos).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(mlmRev)}
\NormalTok{multi0 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mAch }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cses, }\AttributeTok{data =}\NormalTok{ Hsb82)}
\FunctionTok{summary}\NormalTok{(multi0)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lm(formula = mAch \textasciitilde{} cses, data = Hsb82)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residuals:}
\CommentTok{\#\textgreater{}      Min       1Q   Median       3Q      Max }
\CommentTok{\#\textgreater{} {-}17.8660  {-}5.1165   0.2966   5.3880  14.8705 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}             Estimate Std. Error t value Pr(\textgreater{}|t|)    }
\CommentTok{\#\textgreater{} (Intercept) 12.74785    0.07933  160.69   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} cses         2.19117    0.12010   18.24   \textless{}2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Residual standard error: 6.725 on 7183 degrees of freedom}
\CommentTok{\#\textgreater{} Multiple R{-}squared:  0.04429,    Adjusted R{-}squared:  0.04415 }
\CommentTok{\#\textgreater{} F{-}statistic: 332.8 on 1 and 7183 DF,  p{-}value: \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

La ordenada en el origen es 12.75 y la pendiente 2.19, lo que indica que
por cada unidad que aumenta el nivel socio-económico, la puntuación del
test aumenta en 2.19 unidades, además se puede ver que el coeficiente es
significativo.

Supóngase que ocurre esta situación:

\begin{center}\includegraphics[width=0.6\linewidth]{./img/rectas1} \end{center}

Los alumnos de la escuela A sacan, en promedio, mejores notas que las
que le asignaría el modelo ajustado, con la escuela B ocurre lo
contrario. El gráfico indica que la ordenada en el origen no debería ser
la misma para todos los centros, sino que debería ser distinta para
distintos centros. Es decir, el valor predicho debe ajustarse hacia
arriba o abajo, eso se puede conseguir permitiendo que cada escuela
tenga su propia ordenada en el origen:
\[y_{ij} = \beta_{0i} + \beta_1x_{ij} + \epsilon_{ij}\]

Este modelo es similar al anterior añadiendo el subíndice \(i\) para
identificar el centro al que pertenece cada alumno. En realidad, se
utiliza una variable categórica con tantas categorías como escuelas.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multi1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mAch }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cses }\SpecialCharTok{+}\NormalTok{ school, }\AttributeTok{data =}\NormalTok{ Hsb82)}
\end{Highlighting}
\end{Shaded}

Se está considerando a las escuelas como un efecto fijo y no aleatorio,
es decir, implícitamente se está suponiendo que solo interesan estas
escuelas en particular.

Las situación se pueden complicar más, es posible que el efecto del nivel
socio-económico sea distinto para cada centro, es decir, que un aumento
de una unidad en ese nivel puede dar lugar a un aumento distinto en la
nota del test en cada centro. En la Figura \ref{fig:rectas2} se ve como la
pendiente de la recta para la escuela C es distinta a las dos
anteriores.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{./img/rectas2} 

}

\caption{Ilustración de posibles escenarios para tres escuelas}\label{fig:rectas2}
\end{figure}

El modelo que permite tener en cuenta esta situación es:
\[y_{ij} = \beta_{0i} + \beta_{1i}x_{ij} + \epsilon_{ij}\]
donde aparece ahora el sub-índice \(i\) también en la pendiente, lo que indica que cada centro
tiene una pendiente diferente.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multi2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mAch }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cses }\SpecialCharTok{*}\NormalTok{ school, }\AttributeTok{data =}\NormalTok{ Hsb82)}
\end{Highlighting}
\end{Shaded}

El código anterior generaría 159 coeficientes más que son los que se incluirían con
la interacción. Pero no interesan en estas escuelas en
concreto, sino la población de la que estas escuelas son una muestra.

Con un modelo con efectos aleatorios se pueden contestar a preguntas como: ¿Cuales
son las causas de esta variabilidad? ¿Qué variables pueden explicarla?

\hypertarget{modelo-con-ordenada-en-el-origen-aleatoria}{%
\subsection{Modelo con ordenada en el origen aleatoria}\label{modelo-con-ordenada-en-el-origen-aleatoria}}

Es el modelo mixto más sencillo. Se considera que los datos tienen
una estructura con dos niveles, los alumnos están en el nivel 1 y están
agrupados en escuelas, nivel 2. Se va a empezar suponiendo que no se
dispone de ninguna variable explicativa, y que por lo tanto nuestro
único interés es la diferencia entre las notas medias del test de
matemáticas entre los distintos centros.

Los dos niveles del modelo son:
\[\text{Nivel 1: } y_{ij} = \mu_{i} + \epsilon_{ij}\]

\begin{itemize}
\tightlist
\item
  El subíndice \(j\) corresponde a individuos y el \(i\) a escuelas, si se
  considera a las escuelas como un efecto aleatorio,
\item
  \(\beta_{0i}\) (la media de cada escuela) vendría dada por:
  \[\text{Nivel 2: } \mu_{i} = \beta_{0} + u_{i},\]
\item
  \(\beta_{0}\) es la media de todos los alumnos,
\item
  \(u_{i}\) es la desviación de la media de la escuela \(i\) de la media
  de todas las escuelas.
\end{itemize}

Poniendo las dos ecuaciones juntas:
\begin{equation}
y_{ij} = \beta_{0} + u_{i} + \epsilon_{ij}, \quad i = 1, \ldots, m, \quad j = 1, \ldots n_m
\label{eq:b0aleat}
\end{equation}

\begin{itemize}
\tightlist
\item
  La media de \(y\) para el grupo \(i\) es \(\beta_0 + u_i\),
\item
  Los residuos a nivel individual \(\epsilon_{ij}\) son la diferencia
  entre el valor de la variable respuesta del individuo \(j\) y la media
  del grupo al que pertenece,
\item
  \(u_i\sim N( 0, \sigma^2_i )\), \(\epsilon_{ij}\sim N(0, \sigma^2 )\), y
  ambos son independientes, es decir, las observaciones que provienen
  de distintas escuelas son independientes.
\end{itemize}

En el ejemplo de las escuelas:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lme4)}
\NormalTok{Modelo0 }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(mAch }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ school), }\AttributeTok{data =}\NormalTok{ Hsb82)}
\NormalTok{Modelo0}
\CommentTok{\#\textgreater{} Linear mixed model fit by REML [\textquotesingle{}lmerMod\textquotesingle{}]}
\CommentTok{\#\textgreater{} Formula: mAch \textasciitilde{} 1 + (1 | school)}
\CommentTok{\#\textgreater{}    Data: Hsb82}
\CommentTok{\#\textgreater{} REML criterion at convergence: 47116.79}
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Groups   Name        Std.Dev.}
\CommentTok{\#\textgreater{}  school   (Intercept) 2.935   }
\CommentTok{\#\textgreater{}  Residual             6.257   }
\CommentTok{\#\textgreater{} Number of obs: 7185, groups:  school, 160}
\CommentTok{\#\textgreater{} Fixed Effects:}
\CommentTok{\#\textgreater{} (Intercept)  }
\CommentTok{\#\textgreater{}       12.64}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  La media total estimada es 12.64,
\item
  La media para la escuela \(i\) es: 12.64\(+ \hat u_i\), donde \(\hat u_i\)
  es el efecto aleatorio de la escuela.
\end{itemize}

Para obtener los valores predichos de los efectos aleatorios se utiliza
la función \texttt{ranef()}.

El siguiente
gráfico permite ver los efectos aleatorios junto con sus intervalos de
confianza (las escuelas han sido ordenadas atendiendo a su media para
apreciar mejor la variabilidad entre las mismas).

Se dibujan los efectos aleatorios para ver si siguen una distribución
Normal, para eso se ha de ajustar el modelo con la función \texttt{lmer()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lattice)}
\FunctionTok{qqmath}\NormalTok{(}\FunctionTok{ranef}\NormalTok{(Modelo0, }\AttributeTok{condVar =} \ConstantTok{TRUE}\NormalTok{))}\SpecialCharTok{$}\NormalTok{school}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/unnamed-chunk-102-1} \end{center}

Una primera aproximación para contrastar si hay o no diferencias entre
los grupos sería calcular el intervalo de confianza para \(\sigma_u\):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confint}\NormalTok{(Modelo0)}
\CommentTok{\#\textgreater{}                 2.5 \%    97.5 \%}
\CommentTok{\#\textgreater{} .sig01       2.594729  3.315880}
\CommentTok{\#\textgreater{} .sigma       6.154803  6.361786}
\CommentTok{\#\textgreater{} (Intercept) 12.156289 13.117121}
\end{Highlighting}
\end{Shaded}

el intervalo no contiene al cero, pero la forma más correcta de hacerlo
sería utilizando el test LRT \eqref{eq:LRT} para:
\[\begin{array}{ll}
 H_0: \quad \sigma^2_u = 0 \Rightarrow y_{ij} = \beta_0 + \epsilon_{ij} \\
 H_1: \quad \sigma^2_u\neq 0 \Rightarrow y_{ij} = \beta_0 + u_i + \epsilon_{ij}.
\end{array}\]
El resultado del test en este caso se compara con el valor de una
mixtura de distribuciones Chi-cuadrado \(0.5 \chi^2_0 + 0.5\chi^2_1\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Modelo\_NULL }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mAch }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, }\AttributeTok{data =}\NormalTok{ Hsb82)}
\NormalTok{test }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{2} \SpecialCharTok{*} \FunctionTok{logLik}\NormalTok{(Modelo\_NULL, }\AttributeTok{REML =}\NormalTok{ T) }\SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*} \FunctionTok{logLik}\NormalTok{(Modelo0, }\AttributeTok{REML =}\NormalTok{ T)}
\FunctionTok{mean}\NormalTok{(}\FunctionTok{pchisq}\NormalTok{(test, }\AttributeTok{df =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{lower.tail =}\NormalTok{ F))}
\CommentTok{\#\textgreater{} [1] 9.320673e{-}217}
\end{Highlighting}
\end{Shaded}

Conclusión: el efecto aleatorio es necesario en el modelo.

El siguiente paso sería introducir las variables explicativas, ya estén
al nivel 1 o al 2.

\hypertarget{variables-explicativas-en-el-nivel-1-individuos}{%
\subsubsection{\texorpdfstring{Variables explicativas en el Nivel \(1\) (individuos)}{Variables explicativas en el Nivel 1 (individuos)}}\label{variables-explicativas-en-el-nivel-1-individuos}}

Como la variable explicativa está medida al Nivel 1, se introduce en la
ecuación del Nivel 1:

\begin{itemize}
\tightlist
\item
  \(\text{Nivel 1:}\quad y_{ij} = \mu_{i} + \beta_1x_{ij} + \epsilon_{ij}\)
\item
  \(\text{Nivel 2:} \quad \mu_{i} = \beta_0 + u_i\)
\end{itemize}

Si \(x\) es una variable continua, este modelo asume que la pendiente de
la recta es la misma para todas las escuelas (por eso \(\beta_1\) no lleva
el subíndice \(i\)). Poniendo las dos ecuaciones juntas:
\[y_{ij} = \underbrace{\beta_0 + \beta_1x_{ij}}_{\text{efectos fijos}} + 
\underbrace{u_{i} + \epsilon_{ij}}_{\text{efectos aleatorios}}\]

En este modelo, la relación global entre \(y\) y \(x\) viene representada
por la línea recta con ordenada en el origen \(\beta_0\) y pendiente
\(\beta_1\). Sin embargo, la ordenada en el origen para una determinada
escuela \(i\) viene dada por \(\beta_0 + u_i\). Será mayor o menor que que la
ordenada en el origen global \(\beta_0\) por una cantidad \(u_i\). Aunque la
ordenada en el origen varía de grupo a grupo, la pendiente es la misma
para todos los grupos. Todas las líneas rectas ajustadas para cada grupo
son paralelas.

En el ejemplo de las escuelas, se introduce como variable explicativa
\texttt{cses} (nivel socioeconómico centrado):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Modelo1 }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(mAch }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cses }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ school), }\AttributeTok{data =}\NormalTok{ Hsb82)}
\NormalTok{Modelo1}
\CommentTok{\#\textgreater{} Linear mixed model fit by REML [\textquotesingle{}lmerMod\textquotesingle{}]}
\CommentTok{\#\textgreater{} Formula: mAch \textasciitilde{} cses + (1 | school)}
\CommentTok{\#\textgreater{}    Data: Hsb82}
\CommentTok{\#\textgreater{} REML criterion at convergence: 46724}
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Groups   Name        Std.Dev.}
\CommentTok{\#\textgreater{}  school   (Intercept) 2.945   }
\CommentTok{\#\textgreater{}  Residual             6.084   }
\CommentTok{\#\textgreater{} Number of obs: 7185, groups:  school, 160}
\CommentTok{\#\textgreater{} Fixed Effects:}
\CommentTok{\#\textgreater{} (Intercept)         cses  }
\CommentTok{\#\textgreater{}      12.636        2.191}
\end{Highlighting}
\end{Shaded}

Ahora se tienen dos efectos fijos:
\[\begin{array}{l}
 \hat \beta_0 = 12.64\\
 \hat \beta_1 = 2.19\\
\end{array}\]

\(\hat \beta_0\) es la nota media para alumnos con nivel socieconómico
medio (la variable está centrada). La recta media vendría dada por:
\[E[y | cses] = 12.64 + 2.19~cses\]

Para comprobar si la variable \texttt{cses} es significativa se utiliza el
test LRT \eqref{eq:LRT}. Primero se tienen que
ajustar de nuevo los modelos que se quieren comparar usando máxima
verosimilitud (en vez de REML). Si se utiliza la función \texttt{lmer()} para
ajustar el modelo no es necesario reajustar con ML pues la función anova
lo hará automáticamente, mientras que si se usa la función \texttt{lme()} sí será necesario hacerlo.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(Modelo0, Modelo1)}
\CommentTok{\#\textgreater{} Data: Hsb82}
\CommentTok{\#\textgreater{} Models:}
\CommentTok{\#\textgreater{} Modelo0: mAch \textasciitilde{} 1 + (1 | school)}
\CommentTok{\#\textgreater{} Modelo1: mAch \textasciitilde{} cses + (1 | school)}
\CommentTok{\#\textgreater{}         npar   AIC   BIC logLik deviance Chisq Df Pr(\textgreater{}Chisq)    }
\CommentTok{\#\textgreater{} Modelo0    3 47122 47142 {-}23558    47116                        }
\CommentTok{\#\textgreater{} Modelo1    4 46728 46756 {-}23360    46720 395.4  1  \textless{} 2.2e{-}16 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

Por lo tanto, el nivel socioeconómico afecta a los resultados escolares.
Comparado con el modelo sin la variable explicativa (Modelo0), la
inclusión del nivel socio-económico (Modelo1), ha reducido la
variabilidad a nivel del alumno en un 2.8\% ((6.257
- 6.084)/6.257 = 0.028)

\hypertarget{variables-explicativas-en-el-nivel-2-grupos}{%
\subsubsection{\texorpdfstring{Variables explicativas en el Nivel \(2\) (grupos)}{Variables explicativas en el Nivel 2 (grupos)}}\label{variables-explicativas-en-el-nivel-2-grupos}}

Si las variable explicativas son medidas al Nivel 2:
\[\begin{array}{l}
 \text{Nivel 1:}\quad y_{ij} = \mu_{i} + \epsilon_{ij} \\
 \text{Nivel 2:}\quad \mu_{i} = \beta_0 + \beta_{2}s_{i} + u_{i}\\
 \\
y_{ij} = \underbrace{\beta_0 + \beta_2s_{i}}_{\text{efectos fijos}}+\underbrace{u_{i} + \epsilon_{ij}}_{\text{efectos aleatorios}}\\
\end{array}\]

En nuestro ejemplo, la variable utilizada es \texttt{sector} (público o
privado):
\[mAch = \beta_0 + \beta_2~sector + u_{i} + \epsilon_{ij}\]

Se ajusta el modelo usando la función \texttt{lmer()}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Modelo2 }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(mAch }\SpecialCharTok{\textasciitilde{}}\NormalTok{ sector }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ school), }\AttributeTok{data =}\NormalTok{ Hsb82)}
\NormalTok{Modelo2}
\CommentTok{\#\textgreater{} Linear mixed model fit by REML [\textquotesingle{}lmerMod\textquotesingle{}]}
\CommentTok{\#\textgreater{} Formula: mAch \textasciitilde{} sector + (1 | school)}
\CommentTok{\#\textgreater{}    Data: Hsb82}
\CommentTok{\#\textgreater{} REML criterion at convergence: 47080.13}
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Groups   Name        Std.Dev.}
\CommentTok{\#\textgreater{}  school   (Intercept) 2.584   }
\CommentTok{\#\textgreater{}  Residual             6.257   }
\CommentTok{\#\textgreater{} Number of obs: 7185, groups:  school, 160}
\CommentTok{\#\textgreater{} Fixed Effects:}
\CommentTok{\#\textgreater{}    (Intercept)  sectorCatholic  }
\CommentTok{\#\textgreater{}         11.393           2.805}
\end{Highlighting}
\end{Shaded}

\[E[y | sector] = 11.39 + 2.8~sector\]
o equivalentemente
\[\begin{array}{l}
E[y | sector = 0] = 11.39\\
E[y | sector = 1] = 11.39 + 2.8 = 14.19\\
\end{array}\]
La nota de un alumno en una escuela privada se espera que sea 2.8 unidades
mayor que la de un alumno en una escuela pública (se puede generalizar
puse se asume que las escuelas son un efecto aleatorio). La varianza del
efecto aleatorio de nivel 2 \(\sigma^2_u\) ha
descendido:\((2.935^2-2.584^2)/2.935^2 =\) \(0.22\), es decir que se ha reducido en
un \(22 \%\) la variabilidad no explicada entre los centros al introducir
la variable sector.

Para contrastar si la variable sector es significativa se usa de nuevo el test LRT:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(Modelo0, Modelo2)}
\CommentTok{\#\textgreater{} Data: Hsb82}
\CommentTok{\#\textgreater{} Models:}
\CommentTok{\#\textgreater{} Modelo0: mAch \textasciitilde{} 1 + (1 | school)}
\CommentTok{\#\textgreater{} Modelo2: mAch \textasciitilde{} sector + (1 | school)}
\CommentTok{\#\textgreater{}         npar   AIC   BIC logLik deviance  Chisq Df Pr(\textgreater{}Chisq)    }
\CommentTok{\#\textgreater{} Modelo0    3 47122 47142 {-}23558    47116                         }
\CommentTok{\#\textgreater{} Modelo2    4 47087 47115 {-}23540    47079 36.705  1  1.374e{-}09 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

Por lo tanto, el hecho de que la escuela sea pública o privada influye
en el resultado académico de los alumnos.

\hypertarget{modelo-con-pendiente-aleatoria}{%
\subsection{Modelo con pendiente aleatoria}\label{modelo-con-pendiente-aleatoria}}

En este tipo de modelos se supone que la relación entre la variable
respuesta y las variables explicativas va a ser distinta para las
distintas unidades de nivel 2, es decir, la relación puede cambiar de un
centro a otro. Por ejemplo, el efecto del nivel socioeconómico en las
notas puede ser distinto en distintos centros, de modo que se puede
relajar el modelo anterior, en el que la pendiente era la misma para
todos los grupos, permitiendo que la pendiente varíe aleatoriamente
entre los grupos.
\[\begin{array}{ll}
 \text{Nivel 1:} &\quad y_{ij} = \mu_i + \beta_{1i}x_{ij} + \epsilon_{ij} \\
 \text{Nivel 2:} & \quad \mu_i = \beta_0 + u_{i}\\
& \quad \beta_{1i} = \beta_{1} + v_{i} \\
\end{array}\]
Poniendo las dos ecuaciones juntas:
\[y_{ij} = \underbrace{\beta_0 + \beta_{1}x_{ij}}_{\text{efectos
 fijos}} + 
\underbrace{u_{i} + v_{i}x_{ij} + \epsilon_{ij}}_{\text{efectos aleatorios}}, \quad  \left (\begin{array}{c} u_{i}\\ v_{i}\\
  \end{array}\right ) \sim N(0, G_{i}) \quad G_{i} = \left
 ( \begin{array}{cc}
 \sigma^2_{u} &\\
 \sigma_{uv}& \sigma^2_{v}\\
 \end{array}\right ), \]

donde \(\sigma_{uv}\) es la covarianza entre las ordenadas en el
origen de los grupos y las pendientes. Un valor positivo de la
covarianza implica que grupos con un valor del efecto de grupo \(u_i\)
elevado, tienden a tener valores elevados de \(v_i\), o equivalentemente,
centros con ordenada en el origen alta, tienen pendiente alta.

El modelo en \texttt{R} sería:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Modelo3 }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(mAch }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cses }\SpecialCharTok{+}\NormalTok{ (cses }\SpecialCharTok{|}\NormalTok{ school), }\AttributeTok{data =}\NormalTok{ Hsb82)}
\NormalTok{Modelo3}
\CommentTok{\#\textgreater{} Linear mixed model fit by REML [\textquotesingle{}lmerMod\textquotesingle{}]}
\CommentTok{\#\textgreater{} Formula: mAch \textasciitilde{} cses + (cses | school)}
\CommentTok{\#\textgreater{}    Data: Hsb82}
\CommentTok{\#\textgreater{} REML criterion at convergence: 46714.23}
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Groups   Name        Std.Dev. Corr}
\CommentTok{\#\textgreater{}  school   (Intercept) 2.9464       }
\CommentTok{\#\textgreater{}           cses        0.8331   0.02}
\CommentTok{\#\textgreater{}  Residual             6.0581       }
\CommentTok{\#\textgreater{} Number of obs: 7185, groups:  school, 160}
\CommentTok{\#\textgreater{} Fixed Effects:}
\CommentTok{\#\textgreater{} (Intercept)         cses  }
\CommentTok{\#\textgreater{}      12.636        2.193}
\end{Highlighting}
\end{Shaded}

El efecto del nivel socieconómico en la escuela \(i\) se estima como
\(2.19 + \hat u_i\), y la varianza de las pendientes entre escuelas es
\(0.833^2 = 0.694\). Para la \emph{escuela promedio} se predice un aumento de
\(2.19\) en la puntuación cuando el nivel socioeconómico aumenta en una
unidad.

Ahora se tienen los siguientes parámetros de la varianza:
\[\hat \sigma^2_{u} = 8.68\quad \hat \sigma^2_{v} = 0.694\quad 
\hat \sigma_{uv} = \rho \sigma_{u} \sigma_{v} = 0.051\quad 
\hat \sigma^2 = 36.7\]
La varianza de ordenada en el origen estimada, \(8.68\) se interpreta como
la variabilidad entre las escuelas para un nivel socioeconómico medio.

El parámetro de covarianza estimado es \(\sigma_{uv}=0.051\), por lo que se puede plantear si es necesario o no.

Para comprobarlo, el contraste de hipótesis sería: En este caso
\[H_0:
\sigma_{uv} = 0 \quad \text{ y } \quad H_1: \sigma_{uv}\neq 0\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Modelo3}\FloatTok{.1} \OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(ses }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cses }\SpecialCharTok{+}\NormalTok{ (cses }\SpecialCharTok{||}\NormalTok{ school), }\AttributeTok{data =}\NormalTok{ Hsb82)}
\end{Highlighting}
\end{Shaded}

Cuando se quiere que haya un efecto aleatorio para la ordenada en el
origen y para la pendiente pero que sean incorrelados en la función
solo hay que poner doble barra en vez
de simple.

En este caso no es necesario utilizar la mixtura de \(\chi^2\) para \(H_0:\)
\(\sigma_{uv} = 0\), pues \(\sigma_{uv}\) puede tomar cualquier valor.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(Modelo3}\FloatTok{.1}\NormalTok{, Modelo3)}
\CommentTok{\#\textgreater{} Data: Hsb82}
\CommentTok{\#\textgreater{} Models:}
\CommentTok{\#\textgreater{} Modelo3.1: ses \textasciitilde{} cses + ((1 | school) + (0 + cses | school))}
\CommentTok{\#\textgreater{} Modelo3: mAch \textasciitilde{} cses + (cses | school)}
\CommentTok{\#\textgreater{}           npar     AIC     BIC logLik deviance Chisq Df Pr(\textgreater{}Chisq)}
\CommentTok{\#\textgreater{} Modelo3.1    5 {-}226164 {-}226129 113087  {-}226174                    }
\CommentTok{\#\textgreater{} Modelo3      6   46723   46764 {-}23355    46711     0  1          1}
\end{Highlighting}
\end{Shaded}

Por lo tanto se puede suponer que la covarianza es 0.

El siguiente paso sería contrastar si es necesario que las rectas tengan
pendientes diferentes, es decir, \(H_0\): \(\sigma^2_v = 0\), \(H_1\):
\(\sigma^2_v>0\), en este caso sí se necesita la aproximación:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{2} \SpecialCharTok{*} \FunctionTok{logLik}\NormalTok{(Modelo1, }\AttributeTok{REML =}\NormalTok{ T) }\SpecialCharTok{+}
  \DecValTok{2} \SpecialCharTok{*} \FunctionTok{logLik}\NormalTok{(Modelo3}\FloatTok{.1}\NormalTok{, }\AttributeTok{REML =}\NormalTok{ T)}
\FunctionTok{mean}\NormalTok{(}\FunctionTok{pchisq}\NormalTok{(test, }\AttributeTok{df =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{lower.tail =}\NormalTok{ F))}
\CommentTok{\#\textgreater{} [1] 0}
\end{Highlighting}
\end{Shaded}

Por lo tanto la pendiente es diferente en las distintas escuelas.

Además, se puede usar algún criterio de información para comparar los
modelos:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{AIC}\NormalTok{(}\FunctionTok{logLik}\NormalTok{(Modelo3))}
\CommentTok{\#\textgreater{} [1] 46726.23}
\FunctionTok{AIC}\NormalTok{(}\FunctionTok{logLik}\NormalTok{(Modelo3}\FloatTok{.1}\NormalTok{))}
\CommentTok{\#\textgreater{} [1] {-}226113.6}
\FunctionTok{AIC}\NormalTok{(}\FunctionTok{logLik}\NormalTok{(Modelo1))}
\CommentTok{\#\textgreater{} [1] 46732}
\end{Highlighting}
\end{Shaded}

A veces la covariable media a nivel 2 (a nivel de grupo, en este caso
escuelas) puede explicar tanto la variabilidad de la ordenada en el
origen como la pendiente:
\[\begin{array}{ll}
  \text{Nivel 1:} &\quad y_{ij}=\mu_i+\beta_{1i}x_{ij}+\epsilon_{ij} \\
 \text{Nivel 2:} & \quad \mu_i=\beta_0+\beta_{2i}s_i+u_{i}\\
& \quad \beta_{1i}=\beta_{1}+\beta_{3i}s_i +v_{i} \\
\end{array}\]
\[y_{ij} = \underbrace{\beta_0 +\beta_1x_{ij} \beta_2s_{i}+\beta_3 x_{ij}:s_i}_{\text{efectos fijos}} + 
\underbrace{u_{i} +v_ix_{ij}+ \epsilon_{ij}}_{\text{efectos aleatorios}}\]
Al introducir la variable medida al nivel 2, la parte fija se modifica
(con respecto al Modelo 3) pero no la parte aleatoria:

\begin{itemize}
\tightlist
\item
  \(\beta_2\) indica si los centros privados son diferentes de los
  públicos en cuanto a su nota media.
\item
  \(\beta_3\) indica si los centros privados difieren de los públicos en
  cuanto a la relación entre el nivel socio-económico y la puntuación.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Modelo4 }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(mAch }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cses }\SpecialCharTok{*}\NormalTok{ sector }\SpecialCharTok{+}\NormalTok{ (cses }\SpecialCharTok{||}\NormalTok{ school),}
  \AttributeTok{data =}\NormalTok{ Hsb82}
\NormalTok{)}
\NormalTok{Modelo4}
\CommentTok{\#\textgreater{} Linear mixed model fit by REML [\textquotesingle{}lmerMod\textquotesingle{}]}
\CommentTok{\#\textgreater{} Formula: mAch \textasciitilde{} cses * sector + ((1 | school) + (0 + cses | school))}
\CommentTok{\#\textgreater{}    Data: Hsb82}
\CommentTok{\#\textgreater{} REML criterion at convergence: 46648.85}
\CommentTok{\#\textgreater{} Random effects:}
\CommentTok{\#\textgreater{}  Groups   Name        Std.Dev.}
\CommentTok{\#\textgreater{}  school   (Intercept) 2.5971  }
\CommentTok{\#\textgreater{}  school.1 cses        0.5182  }
\CommentTok{\#\textgreater{}  Residual             6.0580  }
\CommentTok{\#\textgreater{} Number of obs: 7185, groups:  school, 160}
\CommentTok{\#\textgreater{} Fixed Effects:}
\CommentTok{\#\textgreater{}         (Intercept)                 cses       sectorCatholic  }
\CommentTok{\#\textgreater{}              11.393                2.784                2.805  }
\CommentTok{\#\textgreater{} cses:sectorCatholic  }
\CommentTok{\#\textgreater{}              {-}1.346}
\end{Highlighting}
\end{Shaded}

Los centros privados tienen una nota media más alta que los públicos
(2.81), y tienen una pendiente más suave que la de los centros públicos
(-1.35). En un colegio privado la mejora de la nota con respecto al
nivel socio-económico es más lenta que un colegio público.

\hypertarget{cuxf3mo-construir-el-modelo-en-la-pruxe1ctica}{%
\subsection{¿Cómo construir el modelo en la práctica?}\label{cuxf3mo-construir-el-modelo-en-la-pruxe1ctica}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Se ajusta el modelo con todos los efectos fijos y aleatorios
  posibles
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Modelo5 }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(mAch }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cses }\SpecialCharTok{*}\NormalTok{ sector }\SpecialCharTok{+}\NormalTok{ (cses }\SpecialCharTok{|}\NormalTok{ school), }\AttributeTok{data =}\NormalTok{ Hsb82)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Se contrasta qué efectos aleatorios son significativos, sin mover
  los efectos fijos
\end{enumerate}

Primero se ve si la covarianza entre efectos fijos y aleatorios es cero
o no:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Se ajusta el modelo con covarianza = 0}
\NormalTok{Modelo5}\FloatTok{.1} \OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(ses }\SpecialCharTok{\textasciitilde{}}\NormalTok{ cses }\SpecialCharTok{*}\NormalTok{ sector }\SpecialCharTok{+}\NormalTok{ (cses }\SpecialCharTok{||}\NormalTok{ school),}
  \AttributeTok{data =}\NormalTok{ Hsb82}
\NormalTok{)}
\FunctionTok{anova}\NormalTok{(Modelo5}\FloatTok{.1}\NormalTok{, Modelo5)}
\CommentTok{\#\textgreater{} Data: Hsb82}
\CommentTok{\#\textgreater{} Models:}
\CommentTok{\#\textgreater{} Modelo5.1: ses \textasciitilde{} cses * sector + ((1 | school) + (0 + cses | school))}
\CommentTok{\#\textgreater{} Modelo5: mAch \textasciitilde{} cses * sector + (cses | school)}
\CommentTok{\#\textgreater{}           npar     AIC     BIC logLik deviance Chisq Df Pr(\textgreater{}Chisq)}
\CommentTok{\#\textgreater{} Modelo5.1    7 {-}220069 {-}220021 110042  {-}220083                    }
\CommentTok{\#\textgreater{} Modelo5      8   46650   46705 {-}23317    46634     0  1          1}
\end{Highlighting}
\end{Shaded}

Como lo es, no se tendría que contrastar nada más, los efectos
aleatorios son los que se han incluido en el Modelo5. Si no hubiera sido
significativa se continuaría testando si la pendiente aleatoria es
significativa y si la ordenada en el origen lo es.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Una vez elegidos los efectos aleatorios (que se mantienen),
  se eligen los efectos fijos:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Modelo6 }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(Modelo5, . }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{{-}}\NormalTok{ cses}\SpecialCharTok{:}\NormalTok{sector)}
\FunctionTok{anova}\NormalTok{(Modelo6, Modelo5)}
\CommentTok{\#\textgreater{} Data: Hsb82}
\CommentTok{\#\textgreater{} Models:}
\CommentTok{\#\textgreater{} Modelo6: mAch \textasciitilde{} cses + sector + (cses | school)}
\CommentTok{\#\textgreater{} Modelo5: mAch \textasciitilde{} cses * sector + (cses | school)}
\CommentTok{\#\textgreater{}         npar   AIC   BIC logLik deviance  Chisq Df Pr(\textgreater{}Chisq)    }
\CommentTok{\#\textgreater{} Modelo6    7 46678 46726 {-}23332    46664                         }
\CommentTok{\#\textgreater{} Modelo5    8 46650 46705 {-}23317    46634 29.983  1  4.358e{-}08 ***}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}.\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

Como la interacción es significativa ya no se tendría que hacer ningún
test más y este sería el modelo final

\begin{infobox_resume}

\hypertarget{resumen-3}{%
\subsection*{Resumen}\label{resumen-3}}
\addcontentsline{toc}{subsection}{Resumen}

En este capítulo se han introducido los modelos mixtos o modelos con efectos aleatorios, en particular:

\begin{itemize}
\tightlist
\item
  Se han dado las claves para distinguir entre efectos fijos y aleatorios.
\item
  Se ha presentado la formulación del modelo y se han dado las claves para llevar a cabo la estimación del mismo.
\item
  Se han explicado las etapas del proceso a seguir para el ajuste de este tipo de modelos.
\item
  Se ha mostrado el uso de \texttt{R} para ajustar estos modelos.
\item
  Se ha ilustrado el análisis de modelos multinivel como caso particular de un modelo con efectos aleatorios.
\end{itemize}

\end{infobox_resume}

\hypertarget{cap-sparse}{%
\chapter{Modelos sparse y métodos penalizados de regresión}\label{cap-sparse}}

\emph{María Durbán}

\hypertarget{introducciuxf3n-7}{%
\section{Introducción}\label{introducciuxf3n-7}}

El modelo de regresión lineal múltiple: \(y=\beta_0+\beta_1 X_1+\ldots + \beta_pX_p+\varepsilon,\) visto en el Capítulo 16, a pesar de su simplicidad, tiene importantes ventajas como la \textbf{interpretabilidad} y su buen poder \textbf{predictivo} en muchas situaciones.

En este Capítulo se va a ver cómo se puede hacer el modelo más interpretable y mejor predictor, y para conseguirlo se reemplazará el método de mínimos cuadrados (utilizado hasta ahora para la estimación de los parámetros) por un método alternativo.

Por lo tanto el objetivo de este Capítulo aprender técnicas para mejorar:

\begin{itemize}
\tightlist
\item
  \textbf{Precisión de la predicción:} en particular cuando el número de variables es mayor que el número de observaciones: \(p>n\) (algo que ocurre con mucha frecuencia hoy en día). En este caso no se pueden utilizar mínimos cuadrados ya que la matriz de diseño no es de rango completo, y por lo tanto, no se puede encontrar una solución al problema de minimización. Por lo tanto, se necesita reducir el número de variables, que además, evitará que se sobreajusten los datos.
\item
  \textbf{Interpretabilidad del modelo:} Al eliminar las variables irrelevantes (es decir, haciendo cero los correspondientes coeficientes) se obtendrá un modelo que es más fácil de interpretar. Por lo tanto, se presentarán varios métodos para llevar a cabo de forma automática la \emph{selección de variables}\index{selección de variables}.
\end{itemize}

Los métodos para reducir el número de variables en el modelo serían:

\begin{itemize}
\tightlist
\item
  \textbf{Selección del mejor subconjunto:} Su objetivo es identificar un subconjunto de entre los \(p\) predictores que se considera que son los que están relacionados con la variable respuesta.
\item
  \textbf{Shrinkage:}\index{Shrinkage} En este caso no se quieren seleccionar variables explícitamente, sino que se añade una penalización que penaliza el número de coeficientes o su tamaño.
\item
  \textbf{Reducción de la dimensión:} El objetivo es proyectar los \(p\)-predictores en un subespacio de dimensión más pequeña (mediante el uso de combinaciones lineales de variables, las cuales se usarán como predictores. Esto es lo que se llama \emph{Componentes principales}\index{Componentes principales} que se desarrolla en el Capítulo 11.
\end{itemize}

Por lo tanto, en este Capítulo se ven los dos primeros métodos.

\hypertarget{selecciuxf3n-del-mejor-subconjunto}{%
\section{Selección del mejor subconjunto}\label{selecciuxf3n-del-mejor-subconjunto}}

Supóngase que se tiene acceso a \(p\) variables predictoras, pero se quiere un modelo más simple que involucre solo a un subconjunto de esos \(p\) predictores. La forma lógica de conseguirlo es considerar todos los posibles subconjuntos de los \(p\) predictores y elegir el mejor modelo de entre todos los modelos calculados con cada uno de los subconjuntos de variables. Los pasos a seguir serían:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Se crea el modelo \textbf{nulo}, \(M_0\), que es aquel que solo contiene la ordenada en el origen y ningún predictor. Este modelo simplemente predice la media muestral para cada observación
\item
  Para cada valor de \(k=1,2,\ldots , p\) se calcula los \(\binom{p}{k}\) modelos que contienen \(k\) predictores. Es decir los \(p\) modelos que contienen 1 predictor, los \(p\times (p-1)/2\) modelos que contienen 2 predictores, etc.
\item
  Para cada valor de \(k\), se elige el mejor entre los \(\binom{p}{k}=\frac{p!}{(p-k)!k!}\) posibles modelos y se denota por \(M_k\). Es decir \(M_1\) sería el mejor modelo entre los \(p\) modelos con una sola variable, \(M_2\) sería el mejor modelo entre los modelos con dos variables, etc. En este caso el \textbf{mejor} modelo sería aquel cuyo \(RSS\) (suma de residuos al cuadrado) sea menor, o equivalentemente, aquel cuyo \(R^2\) es mayor.
\item
  Elegir entre los modelos: \(M_1,\ldots ,M_p\) aquel que es mejor utilizando una criterio como AIC, BIC o \(R^2\) ajustado.
\end{enumerate}

Este método se puede usar también en el caso de GLMs, en cuyo caso se usará el \emph{deviance}\index{Deviance} en vez de \(RSS\).

\hypertarget{ejemplo-sueldo-de-jugadores-de-buxe9isbol}{%
\subsection{Ejemplo: Sueldo de jugadores de béisbol}\label{ejemplo-sueldo-de-jugadores-de-buxe9isbol}}

Se va a aplicar el método descrito al conjunto de datos \texttt{Hitters} del paquete \texttt{ISRL2}. El objetivo es predecir el sueldo, \texttt{Salary}, de jugadores de béisbol a partir de varias variables asociadas con su rendimiento el año anterior.

La variable \texttt{Salary} no está disponible para alguno de los jugadores, se pueden identificar utilizando la función \texttt{is.na()}. Y la función \texttt{sum()} permite ver cuántas hay. Se utilizará \texttt{na.omit()} para eliminarlas

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ISLR2)}
\NormalTok{Hitters }\OtherTok{\textless{}{-}} \FunctionTok{na.omit}\NormalTok{(Hitters)}
\end{Highlighting}
\end{Shaded}

La función \texttt{regsubsets()} del paquete \texttt{leaps()} lleva a cabo la selección del mejor subconjunto de variables identificando el mejor modelo que contiene un número dado de variables (1,2,3, etc.) atendiendo a \(RSS\). La sintaxis usada es similar a la de la función \texttt{lm()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(leaps)}
\NormalTok{regfit.full }\OtherTok{\textless{}{-}} \FunctionTok{regsubsets}\NormalTok{(Salary }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., Hitters)}
\end{Highlighting}
\end{Shaded}

Los resultados se pueden ver usando \texttt{summary()} donde se muestra el mejor modelo para cada subconjunto de variables. Con un asterisco indica las variables incluidas en cada modelo. Por ejemplo, el mejor modelo con dos variables incluye \texttt{Hits} y \texttt{CRBI}.
Por defecto, \texttt{regsubsets()} solo muestra los resultados de los modelos que contienen hasta ocho variables. La opción \texttt{nvmax} se puede usar para incrementar esta cantidad, por ejemplo hasta 19 variables (que es el número de variables predictoras en el conjunto de datos):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{regfit.full }\OtherTok{\textless{}{-}} \FunctionTok{regsubsets}\NormalTok{(Salary }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
  \AttributeTok{data =}\NormalTok{ Hitters,}
  \AttributeTok{nvmax =} \DecValTok{19}
\NormalTok{)}
\NormalTok{reg.summary }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(regfit.full)}
\end{Highlighting}
\end{Shaded}

La función \texttt{summary()} devuelve diferentes medias de bondad de ajuste \(R^2\), \(RSS\), \(R^2\) ajustado, \(C_p\) y \(BIC\). Se utiliza esta información para elegir el \emph{mejor} de entre todos los modelos.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(reg.summary)}
\CommentTok{\#\textgreater{} [1] "which"  "rsq"    "rss"    "adjr2"  "cp"     "bic"    "outmat" "obj"}
\NormalTok{reg.summary}\SpecialCharTok{$}\NormalTok{adjr2}
\CommentTok{\#\textgreater{}  [1] 0.3188503 0.4208024 0.4450753 0.4672734 0.4808971 0.4972001 0.5007849}
\CommentTok{\#\textgreater{}  [8] 0.5137083 0.5180572 0.5222606 0.5225706 0.5217245 0.5206736 0.5195431}
\CommentTok{\#\textgreater{} [15] 0.5178661 0.5162219 0.5144464 0.5126097 0.5106270}
\end{Highlighting}
\end{Shaded}

Por ejemplo el \(R^2\) ajustado mayor corresponde al modelo con 11 variables. Se pueden también visualizar los resultados y dibujar simultáneamente por ejemplo, los valores de \(RSS\), y \(R^2\) ajustado de todos los modelos.

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/chunk7-1} \end{center}

Otra manera de visualizar los resultados es:

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{plot}\NormalTok{(regfit.full, }\AttributeTok{scale =} \StringTok{"adjr2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/chunk10-1} \end{center}

La primera fila tiene un cuadrado negro en cada una de las variables elegidas de acuerdo al modelo con mayor \(R^2\) ajustado (en este caso, sería similar para los otros criterios).

En este caso, varios modelos tienen un valor de \(R^2\) ajustado próximo a \(0.52\), pero es el modelo con 11 variables, el que alcanza el mayor valor. La función \texttt{coef()} permite ver los coeficientes estimados de este modelo.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{coef}\NormalTok{(regfit.full, }\DecValTok{11}\NormalTok{)}
\CommentTok{\#\textgreater{}  (Intercept)        AtBat         Hits        Walks       CAtBat        CRuns }
\CommentTok{\#\textgreater{}  135.7512195   {-}2.1277482    6.9236994    5.6202755   {-}0.1389914    1.4553310 }
\CommentTok{\#\textgreater{}         CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists }
\CommentTok{\#\textgreater{}    0.7852528   {-}0.8228559   43.1116152 {-}111.1460252    0.2894087    0.2688277}
\end{Highlighting}
\end{Shaded}

\hypertarget{selecciuxf3n-stepwise}{%
\section{\texorpdfstring{Selección \emph{Stepwise}}{Selección Stepwise}}\label{selecciuxf3n-stepwise}}

Cuando el número de variables predictoras, \(p\), es grande, el método anterior es computacionalmente muy costoso ya que el número de posibles combinaciones de variables crece de una manera alarmante. En general, la función \texttt{regsubset} puede lidiar con hasta 30-40 variables predictoras. Además otro problema es el sobre-ajuste. Si se tienen 40 variables, se estarían ajustando millones de modelos, y puede que el modelo elegido funcione muy bien en los datos utilizados para su construcción, pero no tan bien en un nuevo conjunto de datos. Una alternativa es el método \textbf{stepwise}\index{selección stepwise}.
La idea detrás de este método es similar a la anterior, pero solo se mira un conjunto mucho más pequeño de modelos.

Hay dos posibilidades de hacer stepwise: \textbf{forward}\index{selección stepwise forward} y \textbf{backward}\index{selección stepwise backward}. Ambas son bastante similares, la principal diferencia es el modelo del que se parte: del modelo sin ninguna variable predictora o del modelo con todas ellas

\hypertarget{forward-stepwise}{%
\subsection{Forward stepwise}\label{forward-stepwise}}

En este caso se comienza con el modelo \emph{nulo}, \(M_0\) y se van añadiendo variables secuencialmente. En particular, en cada paso (\emph{step}) la variable que proporciona la mayor mejora al ajuste es la que se añade al modelo. Los pasos a seguir serían:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Se crea el modelo \textbf{nulo}, \(M_0\).
\item
  Para cada valor de \(k=0,1,2,\ldots , p\):

  \begin{enumerate}
  \def\labelenumii{\roman{enumii}.}
  \tightlist
  \item
    Se consideran todos los \(p-k\) modelos que surgen de aumentar el modelo \(M_k\) con un predictor.
  \item
    Se elige el \textbf{mejor} de esos \(p-k\) modelos, que se denotará \(M_{k+1}\). Donde \textbf{mejor} significa tener el \(RSS\) más bajo o el \(R^2\) más alto
  \end{enumerate}
\item
  Se elige entre los modelos: \(M_0,\ldots ,M_p\) aquel que es mejor utilizando una criterio como AIC, BIC o \(R^2\) ajustado.
\end{enumerate}

Este enfoque tiene ventajas computacionales claras, ya que el número de modelos ajustados es mucho menor, pero no garantiza que el modelo elegido sea el mejor modelo posible, especialmente si existe correlación entre las variables predictoras.

\hypertarget{backward-stepwise}{%
\subsection{Backward stepwise}\label{backward-stepwise}}

En este caso se comienza con el modelo que incluye todas (\(p\)) las variables predictoras y se van eliminando de forma iterativa hasta llegar al modelo nulo (\(M_0\)). Los pasos serían:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Se ajusta el modelo, \(M_p\), que contiene todas (\(p\)) las variable predictoras
\item
  Para cada valor de \(k=p,p-1,\ldots , 1\):

  \begin{enumerate}
  \def\labelenumii{\roman{enumii}.}
  \tightlist
  \item
    Se consideran todos los \(k\) modelos que surgen de reducir en el modelo \(M_k\) un predictor, es decir, modelos con \(k-1\) variables predictoras.
  \item
    Se elige el \textbf{mejor} de esos \(k\) modelos, que se denotará \(M_{k-1}\). Donde \textbf{mejor} significa tener el \(RSS\) más bajo o el \(R^2\) más alto
  \end{enumerate}
\item
  Se elige entre los modelos: \(M_0,\ldots ,M_p\) aquel que es mejor utilizando una criterio como AIC, BIC o \(R^2\) ajustado.
\end{enumerate}

Tanto en el caso de forward como backward stepwise, se busca el mejor modelo sólo entre \(1+p(p+1)/2\) modelos, lo que permite su uso cuando \(p\) es demasiado grande para seleccionarlos mediante la búsqueda del mejor subconjunto.

El método backward necesita que el número de observaciones \(n\) sea mayor que el de variables predictoras \(p\) (ya que se necesita ajustar el modelo con todas las variables). Por el contrario, el método forward se puede usar incluso cuando \(n<p\)

\hypertarget{ejemplo-sueldo-de-jugadores-de-buxe9isbol-1}{%
\subsection{Ejemplo: Sueldo de jugadores de béisbol}\label{ejemplo-sueldo-de-jugadores-de-buxe9isbol-1}}

La función \texttt{regsubset} permite utilizar el método backward y forward, usando el argumento \texttt{method\ =\ "forward"}
o \texttt{method\ =\ "backward"}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{regfit.fwd }\OtherTok{\textless{}{-}} \FunctionTok{regsubsets}\NormalTok{(Salary }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
  \AttributeTok{data =}\NormalTok{ Hitters,}
  \AttributeTok{nvmax =} \DecValTok{19}\NormalTok{, }\AttributeTok{method =} \StringTok{"forward"}
\NormalTok{)}
\NormalTok{regfit.bwd }\OtherTok{\textless{}{-}} \FunctionTok{regsubsets}\NormalTok{(Salary }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
  \AttributeTok{data =}\NormalTok{ Hitters,}
  \AttributeTok{nvmax =} \DecValTok{19}\NormalTok{, }\AttributeTok{method =} \StringTok{"backward"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A continuación se ve como, por ejemplo, para el caso del mejor modelo con 2 variables los tres métodos: mejor subconjunto, forward y backward dan lugar conjuntos de variables diferentes

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{coef}\NormalTok{(regfit.full, }\DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} (Intercept)        Hits        CRBI }
\CommentTok{\#\textgreater{} {-}47.9559022   3.3008446   0.6898994}
\FunctionTok{coef}\NormalTok{(regfit.fwd, }\DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} (Intercept)        Hits        CRBI }
\CommentTok{\#\textgreater{} {-}47.9559022   3.3008446   0.6898994}
\FunctionTok{coef}\NormalTok{(regfit.bwd, }\DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} (Intercept)        Hits       CRuns }
\CommentTok{\#\textgreater{} {-}50.8174029   3.2257212   0.6614168}
\end{Highlighting}
\end{Shaded}

Hay que decidir un criterio para elegir el mejor modelo: \(R^2\) ajustado, \(BIC\), etc. Si se usa \(R^2\) ajustado, en ambos casos el mejor modelo es el que tiene 11 variables:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{which.max}\NormalTok{(}\FunctionTok{summary}\NormalTok{(regfit.fwd)}\SpecialCharTok{$}\NormalTok{adjr2)}
\CommentTok{\#\textgreater{} [1] 11}
\FunctionTok{which.max}\NormalTok{(}\FunctionTok{summary}\NormalTok{(regfit.bwd)}\SpecialCharTok{$}\NormalTok{adjr2)}
\CommentTok{\#\textgreater{} [1] 11}
\end{Highlighting}
\end{Shaded}

Con \(BIC\), el modelo elegido no tiene el mismo número de variables:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{which.min}\NormalTok{(}\FunctionTok{summary}\NormalTok{(regfit.fwd)}\SpecialCharTok{$}\NormalTok{bic)}
\CommentTok{\#\textgreater{} [1] 6}
\FunctionTok{which.min}\NormalTok{(}\FunctionTok{summary}\NormalTok{(regfit.bwd)}\SpecialCharTok{$}\NormalTok{bic)}
\CommentTok{\#\textgreater{} [1] 8}
\end{Highlighting}
\end{Shaded}

Otra posibilidad es utilizar como criterio el error de predicción y para ello se puede utilizar algún esquema de validación cruzada. A continuación se ilustra el caso en el que se divide la muestra en dos subconjuntos: \emph{training}\index{subconjunto training} y \emph{testing}\index{subconjunto testing}, pero se puede utilizar cualquier otro método (k-fold, etc.).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{entreno }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\ConstantTok{TRUE}\NormalTok{, }\ConstantTok{FALSE}\NormalTok{), }\FunctionTok{nrow}\NormalTok{(Hitters), }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{test }\OtherTok{\textless{}{-}}\NormalTok{ (}\SpecialCharTok{!}\NormalTok{entreno)}
\end{Highlighting}
\end{Shaded}

Usando \texttt{regsubsets()} en la muestra de entrenamiento:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{regfit.best }\OtherTok{\textless{}{-}} \FunctionTok{regsubsets}\NormalTok{(Salary }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ Hitters[entreno, ], }\AttributeTok{nvmax =} \DecValTok{19}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Para calcular el error de predicción, dado que la función \texttt{regsubset} no tiene asociada una función \texttt{predict}, se han de calcular ``manualmente'' los valores predichos para la muestra de testeo. Para eso se necesita la matriz de diseño del modelo.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test.mat }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{(Salary }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ Hitters[test, ])}
\end{Highlighting}
\end{Shaded}

Ahora para cada modelo de tamaño \(k\), se extraen los coeficientes de
\texttt{regfit.best} para el mejor modelo de ese tamaño, se multiplica el vector de coeficientes por la matriz de diseño y se obtienen las predicciones, a continuación se calcula el error cuadrático medio (\(MSE\)).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val.errors }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{19}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{19}\NormalTok{) \{}
\NormalTok{  coefi }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(regfit.best, }\AttributeTok{id =}\NormalTok{ i)}
\NormalTok{  pred }\OtherTok{\textless{}{-}}\NormalTok{ test.mat[, }\FunctionTok{names}\NormalTok{(coefi)] }\SpecialCharTok{\%*\%}\NormalTok{ coefi}
\NormalTok{  val.errors[i] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{((Hitters}\SpecialCharTok{$}\NormalTok{Salary[test] }\SpecialCharTok{{-}}\NormalTok{ pred)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

El mejor modelo es el que contiene 7 variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{val.errors}
\CommentTok{\#\textgreater{}  [1] 164377.3 144405.5 152175.7 145198.4 137902.1 139175.7 126849.0 136191.4}
\CommentTok{\#\textgreater{}  [9] 132889.6 135434.9 136963.3 140694.9 140690.9 141951.2 141508.2 142164.4}
\CommentTok{\#\textgreater{} [17] 141767.4 142339.6 142238.2}
\end{Highlighting}
\end{Shaded}

\hypertarget{muxe9todos-shrinkage}{%
\section{Métodos Shrinkage}\label{muxe9todos-shrinkage}}

Los métodos anteriores se basan en el ajuste de modelos mediante mínimos cuadrados. Ahora se trabajará con un método diferente: \textbf{shrinkage}\index{shrinkage}. Este método se basa en una modificación de mínimos cuadrados añadiendo una penalización que \emph{encoje} los coeficientes del modelo típicamente hacia \(0\). Una de las ventajas de este método es que reduce la varianza de los coeficientes estimados.

\hypertarget{regresiuxf3n-ridge}{%
\subsection{Regresión ridge}\label{regresiuxf3n-ridge}}

Se recuerda que el ajuste por mínimos cuadrados estima \(\beta_0, \beta_1, \ldots , \beta_p\) mediante los valores que minimizan:
\[RSS=\sum_{i=1}^n \left ( y_i-\beta_0-\sum_{j=1}^p \beta_jx_{ij}\right )^2.\]
La \textbf{regresión ridge}\index{regresión ridge} añade un término de penalización controlado por un parámetro (que habrá que elegir) que penalizará los coeficientes que se hacen demasiado grandes. Cuanto más grande es el coeficiente, mayor es la penalización:
\[\sum_{i=1}^n \left ( y_i-\beta_0-\sum_{j=1}^p \beta_jx_{ij}\right )^2+\lambda \sum_{j=1}^p \beta_j^2=RSS+\lambda \sum_{j=1}^p \beta_j^2.\]
En realidad lo que se está haciendo es hacer pagar al modelo un precio por el hecho de que los coeficientes no sean cero, y el precio será mayor cuanto mayor sea la magnitud del coeficiente. A esta penalización se le llama \textbf{penalización shrinkage}\index{penalización shrinkage} porque anima a los coeficientes a que se \emph{contraigan} hacia \(0\), y la cantidad que fuerza a esos coeficientes a contraerse hacia cero está determinada por \(\lambda\), el \emph{parámetro de tuneado}. Si \(\lambda=0\) se está en el caso de mínimos cuadrados, y cuanto mayor sea \(\lambda\), mayor será el precio a pagar para que esos coeficientes sean distintos de \(0\). Si \(\lambda\) es extremadamente grande los coeficientes estarán muy próximos a \(0\) para poder hacer este término pequeño (recuérdese que se quiere minimizar \(RSS\) más la penalización). Aunque valores más grandes de los coeficientes den un mejor ajuste (y por lo tanto un menor \(RSS\)), el término de penalización se hará grande y no se alcanzará el mínimo. Por lo tanto \(\lambda\) sirve como equilibrio entre un buen ajuste del modelo y el tamaño de los coeficientes (y por lo tanto el número de coeficientes distintos de cero).

Elegir un buen valor de \(\lambda\) es crítico. Se utilizará la validación cruzada para elegirlo.

\hypertarget{escalado-de-variables-predictoras}{%
\subsubsection{Escalado de variables predictoras}\label{escalado-de-variables-predictoras}}

Un punto importante en regresión ridge es si las variables predictoras están escaladas o no.

En el caso de mínimos cuadrados, el método es \emph{invariante a la escala} (scale-invariant), esto quiere decir que si se multiplica una variable predictora \(X_j\) por una constante \(c\), esto solo implica que el coeficiente estimado se ve multiplicado por \(1/c\), pero \(X_j\hat \beta_j\) no cambia. Sin embargo, en el caso de la regresión ridge los coeficientes estimados pueden cambiar sustancialmente si se multiplica una variable predictora por una constante ya que aparecen todos juntos en el término de penalización. Por lo tanto, antes de utilizar la regresión ridge (o cualquier método de regularización) es importante \textbf{estandarizar las variables predictoras}\index{estandarización}, dividiendo cada variable por su desviación estándar, de forma que todas tengan desviación estándar igual a \(1\).
\[\tilde x_{ij}= \frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^n (x_{ij}-\overline{x}_{ij})^2}}\]
Con esto se consigue que los coeficientes sean comparables.

Regresión ridge en muchas ocasiones da lugar a un menor \(MSE\) que el obtenido con mínimos cuadrados ordinarios. Sin embargo, por muy grande que sea \(\lambda\) los coeficientes no son \(0\), estarán próximos a cero, por lo que este método no es realmente un método de selección de variables.

La regresión ridge puede ser muy útil cuando hay variables predictoras altamente correlacionadas, pero es de interés mantener todas en el modelo. En estos casos, la regresión ridge soluciona los problemas de multicolinealidad.

\hypertarget{ajuste-de-regresiuxf3n-ridge-en-r}{%
\subsubsection{Ajuste de regresión ridge en R}\label{ajuste-de-regresiuxf3n-ridge-en-r}}

El paquete que se va a usar para regresión ridge (y para otros métodos regresión shrinkage) es \texttt{glmnet} .
La función principal en este paquete se llama también \texttt{glmnet()}. Esta función tiene una sintaxis un poco diferente a las funciones usuales para el ajuste de distintos modelos en \texttt{R}. Es necesario pasarle la matriz \(X\) de variables predictoras (sin la columna correspondiente a la ordenada en el origen), y el vector \(y\) con la variable respuesta. Para ilustrar su uso se utilizarán los datos anteriores sobre béisbol.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{(Salary }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., Hitters)[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ Hitters}\SpecialCharTok{$}\NormalTok{Salary}
\end{Highlighting}
\end{Shaded}

La función \texttt{glmnet()} tiene un argumento, \texttt{alpha}, que determina el tipo de penalización que se añade en el modelo. En el caso de regresión ridge \texttt{alpha=0}.

Por defecto, la función \texttt{glmnet()} elige de forma automática el rango de valores de \(\lambda\). Sin embargo, a modo ilustrativo, se va a elegir la rejilla de valores, desde \(\lambda=10^{10}\) hasta \(\lambda=10^{-2}\), cubriendo de esta forma una gran gama de escenarios, desde el modelo nulo (solo la ordenada en el origen), hasta el caso de mínimos cuadrados. Se verá más adelante que se puede calcular el ajuste para un valor determinado de \(\lambda\) que no esté entre los de la rejilla inicial.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(glmnet)}
\NormalTok{grid }\OtherTok{\textless{}{-}} \DecValTok{10}\SpecialCharTok{\^{}}\FunctionTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\AttributeTok{length =} \DecValTok{100}\NormalTok{)}
\NormalTok{ridge.mod }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{(x, y, }\AttributeTok{alpha =} \DecValTok{0}\NormalTok{, }\AttributeTok{lambda =}\NormalTok{ grid)}
\end{Highlighting}
\end{Shaded}

Por defecto, la función \texttt{glmnet()} estandariza las variables predictoras para que estén en la misma escala. Si por alguna razón no se quisiera hacer, se usaría \texttt{standardize\ =\ FALSE}.

Asociado con cada valor de \(\lambda\) hay un vector de coeficientes estimados mediante regresión ridge almacenados en un matriz accesible utilizando \texttt{coef()}. En este caso, el tamaño de la matriz es \(20 \times 100\), las \(20\) filas corresponden a cada uno de los predictores más la ordenada en el origen, y 100 columnas (una para cada valor de \(\lambda\)). Lo esperable es que los coeficientes estimados sean más pequeños cuanto mayor sea el valor de \(\lambda\). A continuación se muestra el valor de los coeficientes cuando \(\lambda=11.498\), y su suma al cuadrado, \(\sum_{j=1}^p\beta_j^2\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge.mod}\SpecialCharTok{$}\NormalTok{lambda[}\DecValTok{50}\NormalTok{]}
\CommentTok{\#\textgreater{} [1] 11497.57}
\FunctionTok{sum}\NormalTok{(}\FunctionTok{coef}\NormalTok{(ridge.mod)[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{50}\NormalTok{]}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 40.45739}
\end{Highlighting}
\end{Shaded}

Por el contrario, si \(\lambda\) es más pequeño, \(705\), el valor es mucho mayor.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge.mod}\SpecialCharTok{$}\NormalTok{lambda[}\DecValTok{60}\NormalTok{]}
\CommentTok{\#\textgreater{} [1] 705.4802}
\FunctionTok{sum}\NormalTok{(}\FunctionTok{coef}\NormalTok{(ridge.mod)[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{60}\NormalTok{]}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 3261.554}
\end{Highlighting}
\end{Shaded}

A continuación se dibuja el efecto de \(\lambda\) en los coeficientes:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(ridge.mod, }\AttributeTok{xvar =} \StringTok{"lambda"}\NormalTok{, }\AttributeTok{label =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/chunk29.2-1} \end{center}

El lado izquierdo del gráfico corresponde a un valor de \(\lambda\) muy pequeño, y por lo tanto no existen restricciones sobre los coeficientes. Conforme aumenta el valor de \(\lambda\) los coeficientes se aproximan a cero, ya que el precio a pagar por ser distinto de cero es cada vez mayor. Pero no todos se aproximan a cero de la misma manera: hay un conjunto de variables cuyo coeficiente es prácticamente cero para cualquier valor de \(\lambda\) y para un valor de \(log(\lambda)=3\) parece que hay solo \(4\) coeficientes distintos de \(0\).

La función \texttt{predict()} se puede utilizar con diferentes propósitos. Por ejemplo, se pueden obtener los coeficientes de la regresión ridge para un valor específico de \(\lambda\), por ejemplo, \(\lambda=50\):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{predict}\NormalTok{(ridge.mod, }\AttributeTok{s =} \DecValTok{50}\NormalTok{, }\AttributeTok{type =} \StringTok{"coefficients"}\NormalTok{)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

Ahora se va a dividir los datos en una muestra de entrenamiento y otra de testeo para estimar el error de predicción de la regresión ridge.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{entreno }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(x), }\FunctionTok{nrow}\NormalTok{(x) }\SpecialCharTok{/} \DecValTok{2}\NormalTok{)}
\NormalTok{test }\OtherTok{\textless{}{-}}\NormalTok{ (}\SpecialCharTok{{-}}\NormalTok{entreno)}
\NormalTok{y.test }\OtherTok{\textless{}{-}}\NormalTok{ y[test]}
\end{Highlighting}
\end{Shaded}

Se ajusta la regresión ridge a la muestra de entrenamiento usando un valor de lambda (por ejemplo \(\lambda=4\)), y se evalúa su \(MSE\) en la muestra de testeo. Para eso se usará la función \texttt{predict()}. En este caso, para obtener las predicciones para la muestra de testeo, se reemplaza \texttt{type\ =} \texttt{"coefficients"} por el argumento \texttt{newx}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge.mod }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{(x[entreno, ], y[entreno], }\AttributeTok{alpha =} \DecValTok{0}\NormalTok{, }\AttributeTok{lambda =}\NormalTok{ grid)}
\NormalTok{ridge.pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(ridge.mod, }\AttributeTok{s =} \DecValTok{4}\NormalTok{, }\AttributeTok{newx =}\NormalTok{ x[test, ])}
\FunctionTok{mean}\NormalTok{((ridge.pred }\SpecialCharTok{{-}}\NormalTok{ y.test)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 142226.5}
\end{Highlighting}
\end{Shaded}

El \(MSE\) es \(142{,}199\). Si se usa un valor muy alto de \(\lambda\), por ejemplo, \(10^{10}\) (esto sería equivalente a ajustar un modelo solo con la ordenada en el origen), el resultado es muy distinto:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge.pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(ridge.mod, }\AttributeTok{s =} \FloatTok{1e10}\NormalTok{, }\AttributeTok{newx =}\NormalTok{ x[test, ])}
\FunctionTok{mean}\NormalTok{((ridge.pred }\SpecialCharTok{{-}}\NormalTok{ y.test)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 224669.8}
\end{Highlighting}
\end{Shaded}

Por lo tanto, en este caso, ajustar un modelo de regresión ridge con \(\lambda=4\) da un \(MSE\) mucho menor que el obtenido cuando el modelo sólo contiene la ordenada en el origen.

A continuación se compara el resultado para \(\lambda=4\) con el obtenido utilizando mínimos cuadrados (\(\lambda=0\)).\footnote{además se ha de añadir `exact = T` en la función `predict()`}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge.pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(ridge.mod, }\AttributeTok{s =} \DecValTok{0}\NormalTok{, }\AttributeTok{newx =}\NormalTok{ x[test, ], }\AttributeTok{exact =}\NormalTok{ T, }\AttributeTok{x =}\NormalTok{ x[entreno, ], }\AttributeTok{y =}\NormalTok{ y[entreno])}
\FunctionTok{mean}\NormalTok{((ridge.pred }\SpecialCharTok{{-}}\NormalTok{ y.test)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 167018.2}
\end{Highlighting}
\end{Shaded}

Se ve que el error es menor cuando se usa regresión ridge (con \(\lambda=4\)) que cuando se usan mínimos cuadrados.

Hasta ahora se ha elegido el valor \(\lambda=4\) de forma arbitraria, en la siguiente Sección se ve cómo seleccionar dicho parámetro de una forma automática.

\hypertarget{selecciuxf3n-del-paruxe1metro-de-tuneado}{%
\subsection{Selección del parámetro de tuneado}\label{selecciuxf3n-del-paruxe1metro-de-tuneado}}

Se ha visto que el valor de \(\lambda\) tienen un gran impacto en los resultados obtenidos cuando se utiliza un modelo con penalización.

Una buena manera de elegir \(\lambda\) es usar validación cruzada, por ejemplo, se puede usar \emph{k-fold cross-validation}\index{k-fold cross-validation}:

\begin{itemize}
\tightlist
\item
  Se dividen los datos en \(k\) grupos, se ajusta el modelo ridge a \(k-1\) de esos grupos (para una rejilla de valores de \(\lambda\)) y se calcula el error de predicción para el otro grupo.
\item
  Se repite tomando como muestra de testeo cada uno de los \(k\) grupos y se suman los errores de predicción.
\item
  Al final se dispondrá de una curva con los errores para cada valor de \(\lambda\) y se elegirá el que dé el mínimo error.
\end{itemize}

En la práctica, se puede hacer con la función \texttt{cv.glmnet()}. Por defecto, esta función usa un \(10\)-fold cross-validation, pero se puede cambiar usando el argumento \texttt{nfolds}.

En el ejemplo del béisbol:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{cv.out }\OtherTok{\textless{}{-}} \FunctionTok{cv.glmnet}\NormalTok{(x[entreno, ], y[entreno], }\AttributeTok{alpha =} \DecValTok{0}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(cv.out)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/chunk36-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mejorlam }\OtherTok{\textless{}{-}}\NormalTok{ cv.out}\SpecialCharTok{$}\NormalTok{lambda.min}
\NormalTok{mejorlam}
\CommentTok{\#\textgreater{} [1] 326.0828}
\end{Highlighting}
\end{Shaded}

En este gráfico los puntos rojos corresponden a la media del \(MSE\) para los \(k\)-folds y las barras superior e inferior corresponden a esa cantidad más/menos una desviación estándar (el ancho será menor cuanto mayor se \(k\) en el \(k\)-fold). La primera línea vertical corresponde al valor de \(\lambda\) que hace mínimo el \(MSE\) y la segunda es el valor que corresponde al \(MSE\) más una desviación típica.

Se calcula el valor mínimo del \(MSE\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge.pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(ridge.mod, }\AttributeTok{s =}\NormalTok{ mejorlam, }\AttributeTok{newx =}\NormalTok{ x[test, ])}
\FunctionTok{mean}\NormalTok{((ridge.pred }\SpecialCharTok{{-}}\NormalTok{ y.test)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 139833.6}
\end{Highlighting}
\end{Shaded}

Esto representa una mejora sobre el error de predicción que se había obtenido cuando \(\lambda=4\).

\hypertarget{regresiuxf3n-lasso}{%
\subsection{Regresión Lasso}\label{regresiuxf3n-lasso}}

Uno de los puntos débiles de la regresión ridge es que no hace selección de variables (los coeficientes pueden ser próximos a cero pero no exactamente cero). En el modelo final se incluyen todos los coeficientes, por lo tanto \textbf{la regresión ridge es útil cuando la mayoría de las variables predictoras son útiles}.

La regresión \emph{lasso}\index{regresión lasso} es una alternativa a la regresión ridge cuyo objetivo es precisamente eliminar esa desventaja de la regresión ridge, fue introducida por \citet{Tibshirani96}. \textbf{La regresión lasso es útil cuando las mayoría de las variables predictoras no son útiles}. Los coeficientes lasso, \(\hat \beta^L\) minimizan la siguiente cantidad:
\[\sum_{i=1}^n \left ( y_i-\beta_0-\sum_{j=1}^p \beta_jx_{ij}\right )^2+\lambda \sum_{j=1}^p |\beta_j|=RSS+\lambda \sum_{j=1}^p |\beta_j|\]
Ahora los coeficientes se \emph{contraen} hacia cero utilizando el valor absoluto en vez de la suma de cuadrados. A esta norma se le llama \(l_1\), \(\|\beta\|_1=\sum_{j=1}^p|\beta_j|\). El cambio que supone es sutil pero importante. En ambos casos los coeficientes se contraen hacia \(0\) pero cuando \(\lambda\) es suficientemente grande los coeficientes serán \(0\), de modo que se estará haciendo selección de variables. Es decir, hará los coeficientes exactamente igual a \(0\) si esas variables no son importantes y \(\lambda\) es suficientemente grande. En este sentido \(lasso\) es lo que se llama un \textbf{modelo sparse}\index{modelo sparse}.

\textbf{¿Por qué lasso hace que los coeficiente se contraigan exactamente hacia cero?}
Para entenderlo se va a ver una formulación equivalente de los mínimos cuadrados penalizados en el caso de la regresión lasso:
\[\sum_{i=1}^n \left ( y_i-\beta_0-\sum_{j=1}^p \beta_jx_{ij}\right )^2\quad \text{sujeto a} \quad \sum_{j=1}^p |\beta_j|<s\]
Se esta utilizando mínimos cuadrados con una restricción, o lo que es lo mismo con un \emph{presupuesto}, en la norma \(l_1\) sobre los coeficientes. Las dos formulaciones son equivalentes en el sentido de que si tengo un \emph{presupuesto} \(s\), habrá un \(\lambda\) que corresponda en la formulación previa que corresponda al mismo problema y viceversa.
Supóngase que se hacen mínimos cuadrados y se obtienen unos ciertos parámetros estimados, y supóngase que la suma de los valores absolutos de los coeficientes es \(10\), pero alguien dice que nuestro \emph{presupuesto} es \(5\) (la suma de los valores absolutos de los coeficientes no puede ser mayor que esa cantidad). Ahora hay que resolver el problema de mínimos cuadrados pero los coeficientes no pueden tomar cualquier valor, ya que se tiene una restricción sobre los mismos. Cuanto más pequeño sea el \emph{presupuesto}, más próximos a cero serán los coeficientes. Si mi \emph{presupuesto} es \(0\), todos los coeficientes serán también \(0\). Si el presupuesto es muy alto, hay libertad para que los coeficientes tomen el valor que quieran, y se estaría en el caso de mínimos cuadrados. El \emph{presupuesto} impone que haya un equilibrio entre el ajuste a los datos y el tamaño de los coeficientes.

La Figura \ref{fig:lassoridge} (tomada de \citet{james2013introduction}) muestra por qué lasso es \emph{sparse}:

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{./img/lasso_ridge} 

}

\caption{Contornos (rojo) de $RSS$ y regiones de restricción (en azul) para lasso (izquierda) y ridge (derecha).}\label{fig:lassoridge}
\end{figure}

El gráfico corresponde a un modelo de regresión con dos variables predictoras. El punto donde está el vector de coeficientes, \(\boldsymbol{\beta}\), es donde se alcanzaría el valor mínimo de los mínimos cuadrados (\(RSS\)), los contornos serían combinaciones de valores de \(\beta_1\) y \(\beta_2\) que dan lugar al mismo valor de \(RSS\) pero que ya no sería el mínimo. Las regiones de restricción son \(|\beta_1|+|\beta_2|<s\) (lasso) y \(\beta_1^2 +\beta_2^2<s\) (ridge). En el caso de ridge, el \emph{presupuesto} sería el radio del círculo, y la regresión ridge busca el primer lugar en el que el contorno toca a la región de restricción, pero al ser un círculo, difícilmente uno u otro parámetro va a ser \(0\). En el caso de lasso la región es un diamante, y por lo tanto tiene vértices, en la Figura el contorno toca a la región de restricción en el caso en que \(\beta_1=0\).

Se vuelve al ejemplo del béisbol para mostrar la regresión lasso, en este caso el argumento \(\alpha\) toma valor \(1\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso.mod }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{(x[entreno, ], y[entreno], }\AttributeTok{alpha =} \DecValTok{1}\NormalTok{, }\AttributeTok{lambda =}\NormalTok{ grid)}
\FunctionTok{plot}\NormalTok{(lasso.mod)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/chunk39-1} \end{center}

Se puede ver que dependiendo del valor del parámetro de tuneado, algunos de los coeficientes se hacen exactamente \(0\). Ahora se va a elegir mediante validación cruzada y se calcula el error de predicción:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{cv.out }\OtherTok{\textless{}{-}} \FunctionTok{cv.glmnet}\NormalTok{(x[entreno, ], y[entreno], }\AttributeTok{alpha =} \DecValTok{1}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(cv.out)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/chunk40-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mejorlab }\OtherTok{\textless{}{-}}\NormalTok{ cv.out}\SpecialCharTok{$}\NormalTok{lambda.min}
\NormalTok{lasso.pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(lasso.mod, }\AttributeTok{s =}\NormalTok{ mejorlab, }\AttributeTok{newx =}\NormalTok{ x[test, ])}
\FunctionTok{mean}\NormalTok{((lasso.pred }\SpecialCharTok{{-}}\NormalTok{ y.test)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 143673.6}
\end{Highlighting}
\end{Shaded}

Este valor es bastante más bajo que \(MSE\) en la muestra de testeo en el caso de mínimos cuadrados (\(224669.8\)), y bastante parecido al obtenido con regresión ridge (cuando el parámetro de tuneado se elige mediante validación cruzada, \(139856.6\)).

Sin embargo, lasso tiene una ventaja importante con respecto a la regresión ridge ya que los coeficientes estimados son \emph{sparse}. Aquí se ve que 10 de los 20 coeficientes estimados son \(0\). Por lo tanto el modelo lasso con \(\lambda\) elegido mediante validación cruzada contiene solo nueve variables predictoras.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{(x, y, }\AttributeTok{alpha =} \DecValTok{1}\NormalTok{)}
\NormalTok{lasso.coef }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(out, }\AttributeTok{type =} \StringTok{"coefficients"}\NormalTok{, }\AttributeTok{s =}\NormalTok{ mejorlab)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{, ]}
\NormalTok{lasso.coef[lasso.coef }\SpecialCharTok{!=} \DecValTok{0}\NormalTok{]}
\CommentTok{\#\textgreater{}   (Intercept)          Hits         Walks        CHmRun         CRuns }
\CommentTok{\#\textgreater{}   {-}3.04787656    2.02551572    2.26853781    0.01647106    0.21177390 }
\CommentTok{\#\textgreater{}          CRBI       LeagueN     DivisionW       PutOuts        Errors }
\CommentTok{\#\textgreater{}    0.41944632   20.48456551 {-}116.59062083    0.23718459   {-}0.94739923}
\end{Highlighting}
\end{Shaded}

\hypertarget{elastic-net}{%
\subsection{\texorpdfstring{Elastic net \index{elastic net}}{Elastic net }}\label{elastic-net}}

Uno de los problemas de la regresión lasso es cuando hay variables predictoras correladas entre sí, pues elegirá una de ellas (y los coeficientes de las demás los hará cero) sin un criterio objetivo. Además, supóngase que se está en una situación en la que el número de variables \(p\) es mayor que el número de observaciones \(n\), en este caso lasso elegiría como mucho \(p\) variables, aunque haya más de \(p\) relevantes; mientras que la regresión ridge las utilizaría todas, aunque disminuye la complejidad del modelo (esto en algunos casos puede ser lo deseable o no). Elastic net \citep{Zou2005} es una generalización de los métodos anteriores que combinan la penalización ridge y la lasso:
\[\sum_{i=1}^n \left ( y_i-\beta_0-\sum_{j=1}^p \beta_jx_{ij}\right )^2+\lambda_1 \sum_{j=1}^p \beta_j^2+\lambda_2 \sum_{j=1}^p |\beta_j|\]
También aparece en muchas ocasiones de esta otra forma:
\[\sum_{i=1}^n \left ( y_i-\beta_0-\sum_{j=1}^p \beta_jx_{ij}\right )^2+\lambda \left [ \frac{1}{2} (1-\alpha)\sum_{j=1}^p \beta_j^2+\alpha \sum_{j=1}^p |\beta_j|\right ]\]
donde \(\alpha\in [0,1]\). Se puede ver \(\alpha\) como el parámetro que controla la mezcla entre las dos penalizaciones y \(\lambda\) como el que controla la cantidad de penalización. Si \(\alpha=0\) se está en el caso de regresión ridge, y si \(\alpha=1\) en el caso de regresión lasso.

La función \texttt{glmnet} también sirve para ajustar elastic net, pero el parámetro \(\alpha\) hay que elegirlo a priori (eso es lo que se ha hecho para usar regresión ridge o lasso). Otra opción es utilizar el paquete \texttt{caret} para hacer validación cruzada sobre \(\alpha\) y \(\lambda\) simultáneamente:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\FunctionTok{library}\NormalTok{(caret)}
\NormalTok{cv\_glmnet }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ x[entreno, ],}
  \AttributeTok{y =}\NormalTok{ y[entreno],}
  \AttributeTok{method =} \StringTok{"glmnet"}\NormalTok{,}
  \AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"cv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{10}\NormalTok{),}
  \AttributeTok{tuneLength =} \DecValTok{10}
\NormalTok{)}
\CommentTok{\# modelo con el MSE más pequeño}
\NormalTok{cv\_glmnet}\SpecialCharTok{$}\NormalTok{bestTune}
\CommentTok{\#\textgreater{}   alpha   lambda}
\CommentTok{\#\textgreater{} 9   0.1 99.12337}
\FunctionTok{ggplot}\NormalTok{(cv\_glmnet)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/chunk46-1} \end{center}

En el gráfico se ve cómo la combinación de \(\alpha\) y \(\lambda\) da lugar a diferentes \(MSE\) (aquí aparece el \(RMSE\), o sea, su raíz cuadrada). Cada línea corresponde a un valor de \(\lambda\) distinto, y en el eje \(x\) se representa los valores de \(\alpha\).

Se calcula el error de predicción para estos dos valores de los parámetros de tuneado:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elastic.mod }\OtherTok{\textless{}{-}} \FunctionTok{glmnet}\NormalTok{(x[entreno, ], y[entreno], }\AttributeTok{alpha =}\NormalTok{ cv\_glmnet}\SpecialCharTok{$}\NormalTok{bestTune}\SpecialCharTok{$}\NormalTok{alpha)}
\NormalTok{elastic.pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(elastic.mod, }\AttributeTok{newx =}\NormalTok{ x[test, ], }\AttributeTok{s =}\NormalTok{ cv\_glmnet}\SpecialCharTok{$}\NormalTok{bestTune}\SpecialCharTok{$}\NormalTok{lambda)}
\FunctionTok{mean}\NormalTok{((elastic.pred }\SpecialCharTok{{-}}\NormalTok{ y.test)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 141626.1}
\end{Highlighting}
\end{Shaded}

Se ve que es peor que el de la regresión ridge, pero mejor que el de lasso. Si no se quiere hacer ningún tipo de selección, en este caso se elegiría ridge, pero si se quiere reducir al máximo las variables se usaría lasso (a costa de que el error de predicción aumente) y el equilibrio vendría con el uso de elastic-net que hace selección de variables pero no aumenta el error de predicción.

Existen otros métodos de regularización que se derivan de estos, como el \emph{group lasso}, \emph{sparse group-lasso}, etc. Se puede encontrar información de estos métodos en \citep{hastiebook2015}.

\begin{infobox_resume}

\hypertarget{resumen-4}{%
\subsection*{Resumen}\label{resumen-4}}
\addcontentsline{toc}{subsection}{Resumen}

En este capítulo se han introducido técnicas para mejorar la predicción y la interpretabilidad de los modelos de regresión, en particular:

\begin{itemize}
\tightlist
\item
  Se ha mostrado el uso de la técnica de selección del mejor subconjunto de variables en el modelo, así como los métodos \emph{stepwise}.
\item
  Se han presentado 3 métodos tipo \emph{shrinkage}: regresión ridge, lasso y elastic net, bien para la selección de variables, o para solventar problemas de multicolinealidad en el modelo.
\item
  Se ha mostrado como seleccionar los parámetros de tuneado que controlan la regresión penalizada.
\item
  Se ha ilustrado el uso de todas las metodologías propuesta en el capítulo mediante el análisis de un caso práctico.
\end{itemize}

\end{infobox_resume}

\hypertarget{cap-series-temp}{%
\chapter{Modelización de series temporales}\label{cap-series-temp}}

\emph{Mª Carmen García Centeno}

\hypertarget{conceptos-buxe1sicos-1}{%
\section{Conceptos básicos}\label{conceptos-buxe1sicos-1}}

El análisis de series temporales es muy útil para analizar tendencias y comportamiento de datos. Con dicho análisis, se tratará de obtener modelos que expliquen su dinámica y puedan utilizarse para predecir valores futuros y tomar decisiones de la forma más efectiva posible.

Una serie temporal se puede definir como el conjunto de valores observados, en periodos consecutivos de tiempo de la misma amplitud, correspondientes a distintas variables aleatorias. Algunos conceptos claves sobre los que se fundamenta el análisis de series temporales son los siguientes:

\begin{enumerate}

\item \textbf{Proceso estocástico}. Es una sucesión de variables aleatorias, de tal forma que, en cada instante del tiempo, un proceso estocástico es una única variable aleatoria con una distribución de probabilidad. Desde el punto de vista práctico, es muy complicado y a veces imposible, caracterizar a un proceso estocástico según su distribución de probabilidad conjunta. Por esta razón, se recurrirá a una forma menos completa pero más sencilla y práctica basada en los dos primeros momentos de una distribución. En concreto, la media ($\mu_t=E(Y_t)$), la varianza ($Var(Y_t)=\sigma_t^2=E(Y_t-\mu_t)^2$) y las covarianzas ($\gamma(k)=Cov(Y_t,Y_{t-k})=E(Y_t-\mu_t)(Y_{t-k}-\mu_{t-k})$).

\item \textbf{Estacionariedad}. Un proceso estocástico es estacionario en sentido simple o amplio si se cumple que la media y la varianza se mantienen invariantes a lo largo del tiempo y las covarianzas solo dependen del lapso de tiempo que transcurre entre las variables. Es decir, $E(Y_t)=\mu$,  $Var(Y_t)=E(Y_t-\mu)^2=\sigma^2$ y  $Cov(Y_t,Y_{t-k})=E(Y_t-\mu)(Y_{t-k}-\mu)=\gamma(k)$.

\item  \textbf{Función de autocovarianzas}. Está formada por el conjunto de autocovarianzas calculadas para distintos órdenes (k). Tiene como finalidad determinar las relaciones de dependencia existentes entre las variables del proceso con el fin de identificar el modelo que mejor explica su dinámica a lo largo del tiempo.

\item  \textbf{Función de autocorrelación de un proceso estocástico (ACF)}. Está formada por el conjunto de correlaciones calculadas para distintos órdenes (k). De forma genérica, para medir la correlación existente entre $Y_t$ e $Y_{t-k}$, el coefiente de correlación de orden  $\rho(k)$, se calcula como el cociente entre la covarianza de orden k y la varianza (ya que, al ser estacionario las desviaciones típicas de $Y_t$ e $Y_{t-k}$ son iguales) de la siguiente forma:

$$\rho(k) = \frac{Cov(Y_t,Y_{t-k})}{(\sqrt{ Var(Y_t)})(\sqrt {Var(Y_{t-k})})}=\frac{\gamma(k)}{\gamma(0)}$$
  
  La representación gráfica de $\rho(k)$  para k=0,1,2,… se llama \textit{correlograma}.
  
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\item
  \textbf{Función de autocorrelación parcial de un proceso estocástico (PACF)}. Está formada por el conjunto de las correlaciones parciales obtenidas para los distintos valores de k. Así, para dos valores de una serie temporal \(Y_t\) e \(Y_{t-k}\), mide la correlación existente entre ellos ajustada del efecto de los periodos intermedios (\(Y_{t-1},Y_{t-2},..., Y_{t-(k-1)}\)).
\item
  \textbf{Ergodicidad}. Un proceso estocástico es ergódico cuando a partir de un determinado desfase temporal entre las variables, la relación existente entre ellas tiende a desaparecer. Es decir, las covarianzas y el coeficiente de correlación tienden a cero, (\(\lim\limits_{k \to \infty}\gamma(k)=0\) y \(\lim\limits_{k \to \infty}\rho(k)=0\)).
\item
  \textbf{Ruido blanco}. Es un proceso puramente aleatorio que se puede expresar de la siguiente forma:
  \[Y_t=a_t\]
  Se caracteriza por tener esperanza nula, varianza constante y covarianzas nulas. Es decir,
  \[E(a_t)=0 \quad\forall t;  \quad E(a_t^2)=\sigma^2 \quad\forall t;\quad  E(a_ta_s)=0 \quad\forall t\neq s\]
\end{enumerate}

\hypertarget{modelos-arima}{%
\section{Modelos ARIMA}\label{modelos-arima}}

Una de las metodologías utilizadas para modelizar el comportamiento de las series temporales son los modelos ARIMA (AutoRegresive Integrated Moving Average) desarrollados por Box y Jenkins. Estos modelos tratan de captar la dinámica y la dependencia que existe entre los datos de una serie temporal, \citet{Hamilton1994}, \citet{Uriel2000}, \citet{Cryer2008}, \citet{Pemberton2011}, \citet{Minguez2011}, \citet{Brockwell2016}, \citet{Shumway2017}.

En la modelización ARIMA se pueden destacar varias fases. La primera se centra en la \textbf{identificación} del modelo ARIMA más adecuado que ha podido generar los datos de la serie temporal. En ella, hay que decidir los órdenes del proceso tanto de la parte regular como de la estacional. Las funciones de autocorrelación simple (ACF) y parcial (PACF) desempeñan un papel clave en la identificación de estos órdenes. También en esta fase, es necesario comprobar si las series son estacionarias. En el caso de que no lo sean, se realizarán las transformaciones necesarias para convertirla en estacionaria. Las dos razones fundamentales por las cuales puede no ser estacionaria se deben a que la media o la varianza no se mantengan constantes. Si la serie no sea estacionaria en varianza, de las transformaciones posibles de Box-Cox se utilizará la transformación logarítmica. Si la serie no es estacionaria en media, puede ser debido a la existencia de tendencia o de estacionalidad. En el caso de tendencia se calcularán diferencias regulares (una o dos como máximo, según el tipo de tendencia lineal o no lineal, respectivamente) y en el caso de estacionalidad, se calculará una diferencia estacional como máximo para convertirla en estacionaria.

Después de proponer el modelo en la segunda fase se procede a su \textbf{estimación}. En esta fase, se obtienen los valores de los parámetros correspondientes al modelo propuesto, así como, sus desviaciones típicas y los residuos del modelo. Posteriormente, en la fase de \textbf{validación o diagnóstico} se realizarñan, por un lado, los contrastes necesarios que permitan determinar si el modelo propuesto es adecuado o no. Y, por otro, comprobar si los residuos del modelo siguen un proceso ruido blanco o no. En el caso de que no lo sean será necesario corregir el modelo con la información proporcionada por los residuos hasta obtener un modelo cuyos residuos sean ruido blanco. Finalmente, se procederá a la utilización del modelo para \textbf{predecir}.

\hypertarget{anuxe1lisis-de-series-temporales-con-r}{%
\section{\texorpdfstring{Análisis de series temporales con \textbf{R}}{Análisis de series temporales con R}}\label{anuxe1lisis-de-series-temporales-con-r}}

Las librerías necesarios en la modelización de series temporales con R son:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tseries)}
\FunctionTok{library}\NormalTok{(astsa)}
\FunctionTok{library}\NormalTok{(forecast)}
\FunctionTok{library}\NormalTok{(lubridate)}
\FunctionTok{library}\NormalTok{(foreign)}
\FunctionTok{library}\NormalTok{(quantmod)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\end{Highlighting}
\end{Shaded}

Posteriormente es necesario cargar los datos. Como se ha comentado en capítulos anteriores los datos pueden tener diferente formato (csv, Excel, etc.). En este caso, se utilizarán datos mensuales del INE (www.ine.es) correspondientes al IPC general en el periodo muestral comprendido entre enero de 2002 y marzo de 2022, en formato Excel. La fecha en la que se observan los datos está incluida en la primera columna de este fichero. Los datos se leerán de la siguiente forma:

\vspace*{8pt}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(CDR)}
\FunctionTok{data}\NormalTok{(IPC)}
\end{Highlighting}
\end{Shaded}

Si los datos se cargan sin fecha, para trabajar con series temporales es necesario ponerle la fecha. Por ejemplo, para series mensuales, el código sería:

\vspace*{8pt}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ipc\_ts }\OtherTok{\textless{}{-}} \FunctionTok{ts}\NormalTok{(IPC}\SpecialCharTok{$}\NormalTok{ipc, }\AttributeTok{start =} \FunctionTok{c}\NormalTok{(}\DecValTok{2002}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{end =} \FunctionTok{c}\NormalTok{(}\DecValTok{2022}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{frequency =} \DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

La representación gráfica de la serie original, puede ayudar a saber si la serie ha sido generada por un proceso estacionario o no. Para obtener este gráfico, se ejecutará el siguiente código:

\vspace*{8pt}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{IPC}\SpecialCharTok{$}\NormalTok{Time }\OtherTok{\textless{}{-}} \FunctionTok{as.Date}\NormalTok{(IPC}\SpecialCharTok{$}\NormalTok{Time)}
\FunctionTok{ggplot}\NormalTok{(}
  \AttributeTok{data =}\NormalTok{ IPC,}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Time, }\AttributeTok{y =}\NormalTok{ ipc)}
\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{colour =} \StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/Serie-ipc-1} 

}

\caption{Evolución del IPC. Perido muestral. Enero 2002 Marzo 2022}\label{fig:Serie-ipc}
\end{figure}

También puede ayudar a determinar si la serie es estacionario o no, la descomposición en tendencia, componente estacional estacional, componente cíclico e irregular de la serie del IPC y su representación gráfica.
\vspace*{8pt}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{componentes\_ts }\OtherTok{\textless{}{-}} \FunctionTok{decompose}\NormalTok{(ipc\_ts)}
\FunctionTok{autoplot}\NormalTok{(componentes\_ts)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/decompose} 

}

\caption{Descomposición aditiva del IPC}\label{fig:plot-decompose}
\end{figure}

\vspace*{8pt}

Tanto en el gráfico de la serie original del IPC como en su descomposición se aprecia que la serie tiene tendencia y componente estacional, por lo tanto, no es estacionaria en media. Tampoco lo es en varianza.

Si una serie no es estacionaria ni en media ni en varianza, para evitar problemas de cálculo matemático, es necesario empezar corriguiendo la no estacionariedad en varianza y, posteriormente, la no estacionariedad en media.

Las transformaciones de Box-Cox son las más utilizadas para corregir el problema de no estacionariedad en varianza. De todas estas transformaciones, la más habitual es el logaritmo. Así, para obtener la representación gráfica del logaritmo de \(Y_t\), se puede ejecutar el siguiente código:
\vspace*{8pt}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logipc }\OtherTok{\textless{}{-}} \FunctionTok{log10}\NormalTok{(ipc\_ts)}
\NormalTok{ts\_logipc }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{value =}\NormalTok{ logipc, }\AttributeTok{Time =} \FunctionTok{time}\NormalTok{(logipc))}
\FunctionTok{ggplot}\NormalTok{(}
  \AttributeTok{data =}\NormalTok{ ts\_logipc,}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Time, }\AttributeTok{y =}\NormalTok{ logipc)}
\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{colour =} \StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/lipc-1} 

}

\caption{Logaritmo del IPC}\label{fig:lipc}
\end{figure}

Como se observa en el gráfico del logaritmo del IPC, la serie sigue sin ser estacionaria en media, ya que, tiene estacionalidad y una tendencia creciente. Para correguir la tendencia, se calculará una diferencia regular del logaritmo del IPC \((dlogipc_{t} = log(ipc_t) - log(ipc_{t-1})\). El código para calcular esta diferencia regulgar y su represenación gráfica es:

\vspace*{8pt}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dlogipc }\OtherTok{\textless{}{-}} \FunctionTok{diff}\NormalTok{(logipc, }\AttributeTok{differences =} \DecValTok{1}\NormalTok{)}
\NormalTok{ts\_dlogipc }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{value =}\NormalTok{ dlogipc, }\AttributeTok{Time =} \FunctionTok{time}\NormalTok{(dlogipc))}
\FunctionTok{ggplot}\NormalTok{(}
  \AttributeTok{data =}\NormalTok{ ts\_dlogipc,}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Time, }\AttributeTok{y =}\NormalTok{ dlogipc)}
\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{colour =} \StringTok{"green"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/g-dlipc-1} 

}

\caption{Diferencia regular del Logaritmo del IPC}\label{fig:g-dlipc}
\end{figure}

Para correguir la estacionalidad, se calculará una diferencia estacional \((d12logipc_{t} = log(ipc_t) - log(ipc_{t-12})\). El código para calcular esta diferencia estacional y su represenación gráfica es:

\vspace*{8pt}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d12dlogipc }\OtherTok{\textless{}{-}} \FunctionTok{diff}\NormalTok{(dlogipc, }\DecValTok{12}\NormalTok{)}
\NormalTok{ts\_d12dlogipc }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{value =}\NormalTok{ d12dlogipc, }\AttributeTok{Time =} \FunctionTok{time}\NormalTok{(d12dlogipc))}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ ts\_d12dlogipc, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Time, }\AttributeTok{y =}\NormalTok{ d12dlogipc)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{colour =} \StringTok{"purple"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/g-d12dlipc-1} 

}

\caption{Diferencia estacional de la diferencia regular del Logaritmo del IPC}\label{fig:g-d12dlipc}
\end{figure}

También, para comprobar si la serie es estacionaria en media o no, se puede utilizar el test de raíces unitarias de Dickey-Fuller. La hipótesis nula de este contraste implica la existencia de raíces unitarias y, por lo tanto, que la serie no es estacionaria, mientras que la hipótesis alternativa implica que sí es estacionaria. Si se realiza el test el test sobre la serie original (ipc\_ts) y sobre la serie transformada del IPC (d12dLogIPC), se puede comprobar que esta última es la transformación estacionaria. El código utilizado para realizar este contraste es:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{adf.test}\NormalTok{(ipc\_ts)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Augmented Dickey{-}Fuller Test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  ipc\_ts}
\CommentTok{\#\textgreater{} Dickey{-}Fuller = {-}1.8396, Lag order = 6, p{-}value = 0.6433}
\CommentTok{\#\textgreater{} alternative hypothesis: stationary}
\FunctionTok{adf.test}\NormalTok{(d12dlogipc)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Augmented Dickey{-}Fuller Test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  d12dlogipc}
\CommentTok{\#\textgreater{} Dickey{-}Fuller = {-}3.3727, Lag order = 6, p{-}value = 0.06002}
\CommentTok{\#\textgreater{} alternative hypothesis: stationary}
\end{Highlighting}
\end{Shaded}

Teniendo en cuenta el p-valor para los valores originales del IPC, para un nivel de significación del 5\%, se acepta la hipótesis nula (y, por lo tanto, la serie no es estacionaria). Sin embargo, para la transformación estacionaria (d12dLogipc) se rechaza la \(H_0\). Por lo tanto, se puede concluir que dicha transformación la ha convertido en estacionaria.

\hypertarget{identificaciuxf3n-o-especificaciuxf3n-del-modelo}{%
\subsection{Identificación o especificación del modelo}\label{identificaciuxf3n-o-especificaciuxf3n-del-modelo}}

Obtenida la transformación estacionaria, es necesario, calcular las funciones de autocorrelación simple (ACF) y parcial (PACF) estimadas. A partir de ellas se especificará el modelo más adecuado para la serie del IPC en el periodo muestral analizado.

Los códigos R para ejecutar los comandos que permiten calcular la ACF y PACF de la transformación estacionaria del IPC, con 40 retardos, son respectivamente:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{acf}\NormalTok{(}\FunctionTok{ts}\NormalTok{(d12dlogipc, }\AttributeTok{frequency =} \DecValTok{1}\NormalTok{), }\AttributeTok{lag.max =} \DecValTok{40}\NormalTok{, }\AttributeTok{main =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/gacf-d12dlipc-1} 

}

\caption{ACF. Función de autocorrelación simple de d12dLogIPC}\label{fig:gacf-d12dlipc}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pacf}\NormalTok{(}\FunctionTok{ts}\NormalTok{(d12dlogipc, }\AttributeTok{frequency =} \DecValTok{1}\NormalTok{), }\AttributeTok{lag.max =} \DecValTok{40}\NormalTok{, }\AttributeTok{main =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/gpacf-d12dlipc-1} 

}

\caption{PACF. Función de autocorrelación parcial de d12dLogIPC}\label{fig:gpacf-d12dlipc}
\end{figure}

Conseguida la transformación estacionaria, estas funciones son muy útiles para identificar los ordenes p y q de los procesos puros autorregresivo (AR(p)), de móviles (MA(q)) o mixto ARMA(p,q) de la parte regular. Del mismo modo, se procederá con órdenes P y Q correspondientes a los modelos SAR(P), SMA(Q) o SARMA(P,Q) de la parte estacional (por ejemplo, en el caso de series mensuales, habría que fijarse en los coeficientes correspondientes a los retardos 12, 24, 36, etc. para identificar el modelo de la parte estacional). El comportamiento de estas funciones según el tipo de modelo es el siguiente:

\begin{itemize}
\item
  En un MA(q) o SMA(Q):

  \begin{itemize}
  \item
    La función de autocorrelación simple muestral (ACF) se anula para órdenes superiores a q (o de Q en el caso estacional). Es decir, solo los q primeros coeficientes son significativos (o Q en el caso estacional). El resto de los coeficientes son nulos (o estadísticamente nulos en el caso de la ACF muestral).
  \item
    La función de autocorrelación parcial muestral (PACF) decrece de forma exponencial o de forma sinusoidal hacía cero.
  \end{itemize}
\end{itemize}

\vspace*{6pt}

\textbf{Ejemplo de la ACF y PAC teóricas, para un proceso estacionario (Yt), generado por un medias móviles de primer orden o MA(1)}.

La ecuación que describe la dinámica de un modelo MA(1) o ARMA(0,1) es:

\[Y_t=a_t-\theta a_{t-1}=a_t(1-\theta L)\]

Donde, \(L\) es el operador de retardos y \(a_t\) es un ruido blanco, es decir: \(E(a_t)=0 \;\forall t\); \(E(a_t^2)=\sigma^2 \; \forall t\); \(E(a_ta_s)=0 \quad\forall t\neq s\).

Este modelo siempre es estacionario y para que sea invertible es necesario que las raíces del polinomio estén fuera del circulo unidad o lo que es lo mismo que \(|\theta|<1\).

La función de autocorrelación (ACF) es:

\[
\rho(k)=\left\{
\begin{array}{ll}
\frac{-\theta}{(1+\theta^2)} & \text{si } k=1 \\
0   &  \text{}\forall k>1
\end{array}
\right.
\]

La función de autocorrelación parcial (PACF) se obtiene de la siguiente forma:
\[
\phi_{kk}=\frac{-\theta^k(1-\theta^2)}{1-\theta^{2(k+1)}}\quad \text {para} \quad k\geq 1
\]

\begin{itemize}
\item
  En un AR(p) o SAR(P):

  \begin{itemize}
  \item
    La ACF decrece rápidamente de forma exponencial o sinusoidal hacia cero.
  \item
    La PACF se anula para órdenes superiores a p (o de P, en el caso estacional). Es decir, solo los p (o P en el caso estacional) primeros coeficientes son significativos. El resto de los coeficientes son nulos (o estadísticamente nulos si se analiza la PACF muestral).
  \end{itemize}
\end{itemize}

\textbf{Ejemplo de la ACF y PAC teóricas, para un proceso estacionario (Yt), generado por un autorregresivo de primer orden o AR(1)}.

La ecuación que describe la dinámica de un modelo AR(1) o ARMA(1,0) es:

\[Y_t=\phi Y_{t-1}+ a_t\]

O bien en forma polinómica: \(Y_t(1-\phi L)=a_t\)

Donde, \(L\) es el operador de retardos y \(a_t\) es un ruido blanco.

Este modelo siempre es invertible y para que sea estacionario es necesario que las raíces del polinomio de retardos estén fuera del circulo unidad o lo que es lo mismo que \(|\phi|<1\).

La función de autocorrelación (ACF) es:

\[
\rho(k)=\phi\rho(k-1)=\phi^k
\]

La PACF se obtiene de la siguiente forma:

\[
\phi_{kk}=\left\{
\begin{array}{ll}
\rho_1=\phi & \text{si } k=1 \\
0   &  \text{}\forall k>1
\end{array}
\right.
\]

\begin{itemize}
\item
  En un ARMA(p,q) o SARMA(P,Q):

  \begin{itemize}
  \item
    La ACF tiene un comportamiento irregular en los q (o de Q en el caso estacional) coeficientes. A partir del orden q (o de Q en el caso estacional), se comporta como la de un AR(p) (o un SAR(P) en el caso estacional).
  \item
    La PACF tiene un comportamiento irregular en los p coeficientes (o de P en el caso estacional). A partir del orden p (o de orden P en el caso estacional), se comporta como la de un MA(q) (o de un SMA(Q) en el caso estacional).
  \end{itemize}
\end{itemize}

\textbf{Ejemplo de la ACF y PAC teóricas, para un proceso estacionario (Yt), que sigue un ARMA(1,1)}.

La ecuación que describe la dinámica de un modelo ARMA(1,1) es:

\(Y_t=\phi Y_{t-1}+ a_t-\theta a_{t-1}\) o bien en forma polinómica, \(Y_t(1-\phi L)=(1-\theta L)a_t\)

Donde, \(L\) es el operador de retardos y \(a_t\) es un ruido blanco.

Para que sea estacionario es necesario que las raíces del polinomio de la parte autorregresiva estén fuera del círculo unidad (o que \(|\phi|<1\)).

Para que sea invertible es necesario que las raíces del polinomio de las medias móviles estén fuera del círculo unidad (o que \(|\theta |<1\)).

Además, es necesario que no existan raíces comunes, es decir, \(\phi \neq \theta\).

La ACF es igual a:

\[
\rho(k)=\left\{
\begin{array}{ll}
\frac{(1-\phi\theta)(\phi-\theta)}{1+\theta^2-2\phi\theta} & \text{si } k=1 \\
\phi\rho(k-1)   &  \text{}\forall k>1
\end{array}
\right.
\]

La función de autocorrelación parcial (FACP) se obtiene:

\[
\begin{array}{l}
\phi_{11}=\rho_1 \\
\\
\phi_{22}=\frac{\begin{vmatrix} 1&\rho_1 \\ \rho_1 &\rho_2\end{vmatrix}}{{\begin{vmatrix} 1&\rho_1 \\ \rho_1 &1\end{vmatrix}}}=\frac{\rho_2-\rho_1^2}{1-\rho_1^2}\\
\\
\phi_{33}=\frac{\begin{vmatrix} 1&\rho_1&\rho_1 \\ \rho_1 &1&\rho_2\\\rho_2 &\rho_1&\rho_3\end{vmatrix}}{{\begin{vmatrix} 1&\rho_1&\rho_2 \\ \rho_1 &1&\rho_1\\\rho_2 &\rho_1&1 \end{vmatrix}}}=\frac{\rho_1^3-\rho_1\rho_2(2-\rho_2)+\rho_3(1-\rho_1^2)}{1-\rho_2^2-2\rho_1^2(1-\rho_2)}\\
\\
\vdots
\end{array}
\]

Tendiendo en cuenta lo anterior, en el caso concreto del IPC, si se analiza el comportamiento de las ACF y PACF muestrales obtenidas para la transformación estacionaria, el modelo ARIMA sería un ARIMA(1,1,0)(0,1,1)12 o SARIMA(1,1,0)(0,1,1).

\hypertarget{estimaciuxf3n-del-modelo}{%
\subsection{Estimación del modelo}\label{estimaciuxf3n-del-modelo}}

Para estimar los parámetros del modelo, ya que, el modelo es no lineal, se utiliza un procedimiento iterativo de estimación no lineal. El código en R que permite obtener las estimaciones de los parámetros y sus desviaciones típicas correspondiente al modelo propuesto es:

\vspace*{6pt}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modelo }\OtherTok{\textless{}{-}} \FunctionTok{arima}\NormalTok{(ipc\_ts, }\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{modelo}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} arima(x = ipc\_ts, order = c(1, 1, 0), seasonal = c(0, 1, 1))}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients:}
\CommentTok{\#\textgreater{}          ar1     sma1}
\CommentTok{\#\textgreater{}       0.4665  {-}0.8302}
\CommentTok{\#\textgreater{} s.e.  0.0701   0.0860}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} sigma\^{}2 estimated as 0.1082:  log likelihood = {-}77.76,  aic = 161.51}
\end{Highlighting}
\end{Shaded}

\hypertarget{diagnosis-validaciuxf3n-y-contrastaciuxf3n}{%
\subsection{Diagnosis, validación y contrastación}\label{diagnosis-validaciuxf3n-y-contrastaciuxf3n}}

En la metodología ARIMA desarrollada por Box y Jenkins es necesario determinar si el modelo propuesto es correcto o no y si se ajusta o no, correctamente, a los datos de la muestra. Para ello, fundamentalmente, es necesario comprobar que:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Los parámetros del modelo cumplen con las condiciones de estacionariedad e invertibilidad.
\end{enumerate}

El modelo estimado para el IPC, es estacionario e invertible, ya que, el AR(1) estimado para la parte regular es invertible y como el parámetro estimado es \(|0.4665| <1\), también es estacionario. La parte estacional sigue un SMA(1) que es estacionario y como el parámetro estimado \(|-0.8302|<1\), también es invertible.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Los parámetros estimados son estadísticamente significativos.
\end{enumerate}

Para comprobar si los parámetros son estadísticamente significativos o no. O también, para comprobar si se ha sobreparametrizado o infraparametrizado, se realiza un contraste de significatividad individual para cada uno de ellos. Por ejemplo, para el parámetro del AR(1) sería:
\[
\begin{array}{l}
H_0:\phi=0 \\
H_1:\phi\neq 0
\end{array}
\]

Para realizar el contraste se utiliza el estadístico \(t\) que sigue una distribución t-Student con \((n-k)\) grados de libertad. Este estadístico t se calcula dividiendo el valor estimado del parámetro entre su correspondiente error estándar. En concreto para los parámetros estimados de este modelo sería:

\[
t=\frac{0,4675}{0,0701} = 6,669 \quad y\quad |t|=|\frac{-0,8302}{0,0860}|=9,653
\]

Como, para un nivel de significación del 5\% y los grados de libertad, el valor de la t-Student es aproximadamente 1,96, se rechaza la \(H_0\) y, por lo tanto, son estadísticamente distintos de cero.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Los residuos del modelo son ruido blanco.
\end{enumerate}

En la fase de validación es necesario comprobar que los residuos del modelo estimado son consistentes con el supuesto de ruido blanco. El siguiente código R, permite obtener los residuos,
\vspace*{6pt}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{residuos }\OtherTok{\textless{}{-}} \FunctionTok{residuals}\NormalTok{(modelo)}
\NormalTok{ts\_residuos }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{value =}\NormalTok{ residuos, }\AttributeTok{Time =} \FunctionTok{time}\NormalTok{(residuos))}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ ts\_residuos, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Time, }\AttributeTok{y =}\NormalTok{ residuos)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{colour =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/gresiduos-1} 

}

\caption{Gráfico de los residuos}\label{fig:gresiduos}
\end{figure}

Para contrastar que los residuos están incorrelacionados y que se comportan como un ruido blanco, se puede utilizar el estadístico de Box-Ljung. En este caso concreto, su hipótesis nula, es que los primero 40 coeficientes de correlación son cero frente a la hipótesis alternativa de que no lo sean. Para un nivel de significación del 5\%, se acepta \(H_0\), ya que, el p-valor de la X-squared es igual a 0.5448. El código R para obtener el test de Box-Ljung es,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{Box.test}\NormalTok{(residuos, }\AttributeTok{type =} \StringTok{"Ljung{-}Box"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Box{-}Ljung test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  residuos}
\CommentTok{\#\textgreater{} X{-}squared = 0.36682, df = 1, p{-}value = 0.5447}
\end{Highlighting}
\end{Shaded}

Además, se puede calcular ACF y PACF de los residuos para comprobar que están incorrelacionados. Los correlogramas obtenidos, con el código siguiente, muestran que los residuos son ruido blanco.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.1}\NormalTok{)}
\FunctionTok{acf}\NormalTok{(}\FunctionTok{ts}\NormalTok{(residuos, }\AttributeTok{frequency =} \DecValTok{1}\NormalTok{), }\AttributeTok{lag.max =} \DecValTok{40}\NormalTok{)}
\FunctionTok{pacf}\NormalTok{(}\FunctionTok{ts}\NormalTok{(residuos, }\AttributeTok{frequency =} \DecValTok{1}\NormalTok{), }\AttributeTok{lag.max =} \DecValTok{40}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/g-acf-pacf-residuos-1} 

}

\caption{ACF y PACF estimadas de los residuos}\label{fig:g-acf-pacf-residuos}
\end{figure}

También se debe comprobar la capacidad de ajuste del modelo comparando los valores observados y estimados, utilizando el código R siguiente:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(ipc\_ts, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(ipc\_ts }\SpecialCharTok{{-}}\NormalTok{ modelo}\SpecialCharTok{$}\NormalTok{residuals, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/g-ajuste-1} 

}

\caption{Ajuste con el modelo estimado}\label{fig:g-ajuste}
\end{figure}
\vspace*{6pt}

Los criterios de información como el AIC se pueden utilizar para comparar entre diferentes modelos cuál se ajusta mejor a los datos. Así, será mejor el modelo que tenga un menor valor del criterio de información utilizado.

\hypertarget{predicciuxf3n-2}{%
\subsection{Predicción}\label{predicciuxf3n-2}}

Después de comprobar que el modelo es adecuado se puede utilizar para predecir. Para obtener 12 predicciones, sus correspondientes intervalos de predicción al 80\% y 95\% de confianza y su representación gráfica, con el modelo ARIMA estimado, el código R que se puede utilizar es:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{forecast}\SpecialCharTok{::}\FunctionTok{forecast}\NormalTok{(modelo, }\AttributeTok{h =} \DecValTok{12}\NormalTok{)}
\CommentTok{\#\textgreater{}          Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95}
\CommentTok{\#\textgreater{} Apr 2022       109.7116 109.2900 110.1332 109.0668 110.3564}
\CommentTok{\#\textgreater{} May 2022       110.5926 109.8443 111.3410 109.4481 111.7371}
\CommentTok{\#\textgreater{} Jun 2022       111.1030 110.0714 112.1346 109.5253 112.6806}
\CommentTok{\#\textgreater{} Jul 2022       110.5518 109.2748 111.8289 108.5987 112.5050}
\CommentTok{\#\textgreater{} Aug 2022       110.7714 109.2787 112.2641 108.4885 113.0543}
\CommentTok{\#\textgreater{} Sep 2022       111.0288 109.3435 112.7140 108.4515 113.6060}
\CommentTok{\#\textgreater{} Oct 2022       111.9631 110.1034 113.8228 109.1189 114.8073}
\CommentTok{\#\textgreater{} Nov 2022       112.1740 110.1540 114.1939 109.0847 115.2632}
\CommentTok{\#\textgreater{} Dec 2022       112.3896 110.2209 114.5583 109.0728 115.7064}
\CommentTok{\#\textgreater{} Jan 2023       111.6049 109.2968 113.9130 108.0750 115.1349}
\CommentTok{\#\textgreater{} Feb 2023       111.6666 109.2270 114.1061 107.9355 115.3976}
\CommentTok{\#\textgreater{} Mar 2023       112.5012 109.9369 115.0656 108.5794 116.4231}
\NormalTok{predicciones }\OtherTok{\textless{}{-}}\NormalTok{ forecast}\SpecialCharTok{::}\FunctionTok{forecast}\NormalTok{(modelo, }\AttributeTok{h =} \DecValTok{12}\NormalTok{)}
\FunctionTok{autoplot}\NormalTok{(predicciones)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/prediccion-1} 

}

\caption{Predicciones con el modelo ARIMA(1,1,0)(0,1,1)12 estimado}\label{fig:prediccion}
\end{figure}

\begin{infobox_resume}
\textbf{RESUMEN}

Los modelos univariantes de series temporales ARIMA son muy útiles para proponer el modelo que mejor capte la dinámica de una serie temporal. Fundamentalmente, se pueden utilizar para predicir y la toma de decisiones.

\end{infobox_resume}

\hypertarget{cap-discriminante}{%
\chapter{Análisis discriminante}\label{cap-discriminante}}

\emph{Mª Leticia Meseguer Santamaría}

\hypertarget{introducciuxf3n-8}{%
\section{Introducción}\label{introducciuxf3n-8}}

El \textbf{análisis discriminante} es una técnica de dependencia, donde se establece la relación entre una única variable dependiente cualitativa o categórica, cuyas clases identifican a los grupos preexistentes, y un conjunto de variables independientes cuantitativas, denominadas clasificadoras o predictoras. Su \textbf{objetivo} es clasificar individuos en distintos grupos preexistentes, a partir de la información de un conjunto de variables clasificadoras, la información se sintetiza en lo que se denominan \textbf{funciones discriminantes}. Por tanto, se identifican dos finalidades: la descriptiva, caracterizar la separación entre grupos, determinando la contribución de cada variable clasificadora a la separación de los individuos; y la predictiva, dónde se clasificará un individuo nuevo.

Dependiendo de sus características se diferencian varios casos:

\textbf{Dos grupos y una variable clasificadora}:

Es el supuesto más simple, son dos grupos (I y II) y un conjunto de individuos para clasificar a partir de la información de una sola variable. En este caso las distribuciones en los grupos I y II solo difieren en la media, Como se muestra en la Figura \ref{fig:150025-img1}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/150025-img1} 

}

\caption{Dos grupos y una variable clasificadora}\label{fig:150025-img1}
\end{figure}

El \textbf{punto de corte (C)} es:
\begin{equation}
  C=\frac{\overline{x_I}+\overline{x_{II}}}{2}
\end{equation}

Y el \textbf{criterio de asignación} para cada individuo:
\begin{equation}
Si\ \ X_i<C \in grupo\ \ I\ \ y\ \ si\ \ X_i>C \in grupo\ \ II
\end{equation}

El error que podamos cometer depende del nivel de solapamiento de las dos distribuciones.

\textbf{Dos grupos y p variables clasificadoras}:

El objetivo es encontrar un criterio que permita separar los dos grupos. Para ello se necesita obtener la \textbf{función discriminante de Fisher}, que se plantea como una combinación lineal de las p variables clasificadoras:
\begin{equation}
  D=u_1X_1+u_2X_2+...+u_pX_p
\end{equation}
La \textbf{punturación discriminante del individuo i-ésimo} es:
\begin{equation}
  D_i=u_1X_{1i}1+u_2X_{2i}+...+u_pX_{pi}
\end{equation}
Se trata de obtener el valor teórico como combinación lineal de las variables clasificadoras que mejor discriminen entre los grupos definidos a priori, por lo tanto, la función discriminante que maximiza las variabilidad entre-grupos respecto de la intra-grupos.

El \textbf{punto de corte (C)} se obtiene evaluando la función discriminante en cada grupo y promediando:
\begin{equation}
  D_I=u_1X_{1I}1+u_2X_{2I}+...+u_pX_{pI}
\end{equation}
\begin{equation}
 D_{II}=u_1X_{1II}1+u_2X_{2II}+...+u_pX_{pII}
\end{equation}
\begin{equation}
 C=\frac{\overline{D_I}+\overline{D_{II}}}{2}
\end{equation}

Y el \textbf{criterio de asignación para cada individuo} será:
\begin{equation}
 Si \ \ D_i<C \in grupo\ \ I\ \ y \ \ si \ \ D_i>C \in grupo \ \ II
\end{equation}

\textbf{G grupos y p variables}:

Se obtinen más de una función discriminante, concretamente, el mínimo de g-1 y p.

Esta matriz no es simétrica, por lo que las funciones no son ortogonales.

El resultado proporcionará las funciones discriminantes que maximizan la variabilidad entre-grupos respecto de la intra-grupos, indicando la importancia relativa de cada una, de tal forma que la primera función será la que más discrimine, la segunda, discriminará menos y ,así, hasta la última.

\hypertarget{tipos-de-anuxe1lisis-discriminantes}{%
\section{Tipos de análisis discriminantes:}\label{tipos-de-anuxe1lisis-discriminantes}}

\textbf{LDA, Análisis discriminante lineal}:

Es el que se ha visto hasta ahora, las variables predictoras (p) están distribuidas normalmente y las clases tienen varianzas idénticas (si p=1) o matrices de covarianza idénticas (si p\textgreater1).

\textbf{QDA, Análisis discriminante cuadrático}:

Elimina el supuesto de que la matriz de covarianzas sea la misma para cada clase, por lo que aumenta su fiabilidad; ahora, se calculará para cada función discriminante k (k=1,2, 3\ldots.) y no se descartan los términos cuadráticos, por lo tanto, la función discriminante tendrá términos de segundo orden.

La función discriminate viene dada por:
\begin{equation}
 \delta_k(x)=\frac{1}{2}log|\Sigma_k|-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)+\log\Pi_k
\end{equation}

Donde \(\Sigma_k\) es la matriz de covarianzas.

La regla de clasificación es:
\begin{equation}
 G=\underset{x}{\operatorname{argmax}}\delta_k(x)
\end{equation}

Los límites de decisión son ecuaciones cuadráticas de x.

Se recomienda QDA si el conjunto de entrenamiento es grande o en el supuesto de que una matriz de covarianzas igual para todas las variables no sea sostenible.

\textbf{MDA, Análisis discriminante mixto}:

Cada clase es una mezcla normal de subclases, en la que cada grupo de datos tiene una probabilidad de pertenecer a cada clase.
Se asume la igualdad de la matriz de covarianza entre clases.

\hypertarget{procedimiento-con-r-las-funciones-lda-qda-y-mda}{%
\section{\texorpdfstring{Procedimiento con \textbf{R}: las funciones \texttt{lda()}, \texttt{qda()} y \texttt{mda()}}{Procedimiento con R: las funciones lda(), qda() y mda()}}\label{procedimiento-con-r-las-funciones-lda-qda-y-mda}}

\textbf{ESTO HAY QUE REDACTARLO ENTERO}

\textbf{IDEA: CUANDO SE EXPLIQUE EL LINEAL, HACER EJEMPLO DEL LINEAL, Y ASÍ CON EL RESTO}

A continuación, se indican los pasos a seguir en la realización del análisis y un ejemplo de cada tipo (lda, qda y mda) realizados con R

\textbf{Etapas}:

\begin{itemize}
\item
  \textbf{Carga de paquetes y librerías}:

  \begin{itemize}
  \item
    library(tidyverse)
  \item
    library(caret)
  \item
    library(MASS)---\textgreater{} Específica para los modelos LDA y QDA
  \item
    library(klaR)---\textgreater{} Específicas para los modelos QDA
  \item
    library (mda)---\textgreater{} Específica para el modelo MDA
  \end{itemize}
\item
  \textbf{Preparación de datos}:

  Se utilizará y cargará la base de datos \textbf{iris}, disponible para R, que consta de 150 observaciones y 5 variales, 4 numéricas, que serán las predictoras, y una categórica, sobre la que se realiza el análisis, con tres categorías: setosa, versicolor y virginica.

  Se clasificarán las flores iris, identificadas con la variable \emph{Species} (especies de iris), utilizando como variables predictoras: \emph{Sepal.Length} (Longitud del sépalo), \emph{Sepal.Width} (anchura del sépalo), \emph{Petal.Lenght} (longitud del pétalo) y \emph{Petal.Width} (anchura del pétalo).

  Se dividen los datos entre la muestra, 80\% para el entrenamiento y 20\% para el test.

  Para evitar distorsiones, es recomendable estandarizar las predictoras continuas, en R las variables categóricas se ignoran automáticamente.
\item
  \textbf{Análisis: LDA, QDA y MDA}

  Las distribuciones univariadas deben estar distribuidas normalmente, si no fuera así, se deberán transformar utilizando rog y root (distribuciones exponenciales) y Box-Cox (distribuciones sesgadas).

  Funciones lda, qda (del paquete MASS) y mda (del paquete mda), para calcular los modelos.
\item
  \textbf{Resultados} de lda, qda y mda:

  \begin{itemize}
  \item
    \textbf{Probabilidades previas} (\emph{Prior probabilities of groups}): Proporciones de cada grupo en el total de la muestra, en este caso, cada grupo un 33.33\%.
  \item
    \textbf{Media de cada grupo} (\emph{Group means}): Proporciona las medias de cada grupo y establece para cada individuo la probabilidad de pertenecer a los grupos.
  \item
    \textbf{Coeficientes de discriminantes lineales} (\emph{Coefficients of linear discriminants}): Son los coeficientes de cada variable predictora con los que se configuran las funciones lineales, sirven para para establecer la regla de decisión.
  \item
    \textbf{Proporción de cada discriminante} (\emph{Proportion of trace}): El peso de cada función para discriminar.
  \end{itemize}
\item
  \textbf{Gráfica}
\item
  \textbf{Predicciones} se uilizarán las funciones \textbf{predict}, para realizar predicciones, y \textbf{table}, para la tabla que indica la clasificación del modelo.
\item
  \textbf{Precisión} o ajuste del modelo, mediante la función \textbf{mean} se calculará la precisión clasificadora del modelo.
\end{itemize}

\textbf{Resolución con R}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Carga de librerías y datos}
\FunctionTok{library}\NormalTok{(caret)}
\FunctionTok{library}\NormalTok{(MASS)}
\FunctionTok{library}\NormalTok{(klaR)}
\FunctionTok{library}\NormalTok{(mda)}
\FunctionTok{data}\NormalTok{(}\StringTok{"iris"}\NormalTok{)}

\CommentTok{\# División de los datos: 80\% para entrenamiento y 20\% para test}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{muestra }\OtherTok{\textless{}{-}}\NormalTok{ iris}\SpecialCharTok{$}\NormalTok{Species }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{createDataPartition}\NormalTok{(}\AttributeTok{p =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{list =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{entrenamiento\_d }\OtherTok{\textless{}{-}}\NormalTok{ iris[muestra, ]}
\NormalTok{test\_d }\OtherTok{\textless{}{-}}\NormalTok{ iris[}\SpecialCharTok{{-}}\NormalTok{muestra, ]}

\CommentTok{\# Estimar parámetros de preprocesamiento (estandarizar)}
\NormalTok{preproc\_param }\OtherTok{\textless{}{-}}\NormalTok{ entrenamiento\_d }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{preProcess}\NormalTok{(}\AttributeTok{method =} \FunctionTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{))}

\CommentTok{\# Transformar los datos usando los parámetros estimados}
\NormalTok{entrenamiento\_t }\OtherTok{\textless{}{-}}\NormalTok{ preproc\_param }\SpecialCharTok{|\textgreater{}} \FunctionTok{predict}\NormalTok{(entrenamiento\_d)}
\NormalTok{test\_t }\OtherTok{\textless{}{-}}\NormalTok{ preproc\_param }\SpecialCharTok{|\textgreater{}} \FunctionTok{predict}\NormalTok{(test\_d)}
\end{Highlighting}
\end{Shaded}

\textbf{MODELO LDA}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Estimación del modelo LDA}
\NormalTok{modelo\_lda }\OtherTok{\textless{}{-}} \FunctionTok{lda}\NormalTok{(Species }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ entrenamiento\_t)}
\NormalTok{modelo\_lda}
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} lda(Species \textasciitilde{} ., data = entrenamiento\_t)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Prior probabilities of groups:}
\CommentTok{\#\textgreater{}     setosa versicolor  virginica }
\CommentTok{\#\textgreater{}  0.3333333  0.3333333  0.3333333 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Group means:}
\CommentTok{\#\textgreater{}            Sepal.Length Sepal.Width Petal.Length Petal.Width}
\CommentTok{\#\textgreater{} setosa       {-}1.0112835  0.78048647   {-}1.2900001  {-}1.2453195}
\CommentTok{\#\textgreater{} versicolor    0.1014181 {-}0.68674658    0.2566029   0.1472614}
\CommentTok{\#\textgreater{} virginica     0.9098654 {-}0.09373989    1.0333972   1.0980581}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Coefficients of linear discriminants:}
\CommentTok{\#\textgreater{}                     LD1         LD2}
\CommentTok{\#\textgreater{} Sepal.Length  0.6794973  0.04463786}
\CommentTok{\#\textgreater{} Sepal.Width   0.6565085 {-}1.00330120}
\CommentTok{\#\textgreater{} Petal.Length {-}3.8365047  1.44176147}
\CommentTok{\#\textgreater{} Petal.Width  {-}2.2722313 {-}1.96516251}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Proportion of trace:}
\CommentTok{\#\textgreater{}    LD1    LD2 }
\CommentTok{\#\textgreater{} 0.9902 0.0098}
\CommentTok{\# Probabilidades a priori: 0,3333 para cada grupo}
\CommentTok{\# Coeficientes y funciones discriminantes:}
\CommentTok{\#  D1=0,6795*SL+0,6565*SW{-}3,8365*PL{-}2,2722*PW}
\CommentTok{\#  D2=0,0446*SL{-}1,0033*SW+1,4418*PL{-}1,9651*PW}
\CommentTok{\# Proporción discriminar de D1 y D2, 0,9902 y 0,0098, respectivamente}

\CommentTok{\# Gráfico Modelo LDA}
\NormalTok{datos\_lda }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(entrenamiento\_t, }\FunctionTok{predict}\NormalTok{(modelo\_lda)}\SpecialCharTok{$}\NormalTok{x)}
\FunctionTok{ggplot}\NormalTok{(datos\_lda, }\FunctionTok{aes}\NormalTok{(LD1, LD2)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ Species)) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Gráfico LDA"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/150025-lda-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Predicciones del modelo LDA}
\NormalTok{predicciones\_lda }\OtherTok{\textless{}{-}}\NormalTok{ modelo\_lda }\SpecialCharTok{|\textgreater{}} \FunctionTok{predict}\NormalTok{(test\_t)}
\FunctionTok{table}\NormalTok{(test\_t}\SpecialCharTok{$}\NormalTok{Species, predicciones\_lda}\SpecialCharTok{$}\NormalTok{class, }\AttributeTok{dnn =} \FunctionTok{c}\NormalTok{(}\StringTok{"Clase real"}\NormalTok{, }\StringTok{"Clase predicha"}\NormalTok{))}
\CommentTok{\#\textgreater{}             Clase predicha}
\CommentTok{\#\textgreater{} Clase real   setosa versicolor virginica}
\CommentTok{\#\textgreater{}   setosa         10          0         0}
\CommentTok{\#\textgreater{}   versicolor      0         10         0}
\CommentTok{\#\textgreater{}   virginica       0          1         9}

\CommentTok{\# Precisión del modelo LDA}
\FunctionTok{mean}\NormalTok{(predicciones\_lda}\SpecialCharTok{$}\NormalTok{class }\SpecialCharTok{==}\NormalTok{ test\_t}\SpecialCharTok{$}\NormalTok{Species)}
\CommentTok{\#\textgreater{} [1] 0.9666667}

\CommentTok{\# Ajusta al 96,9667\%.}
\end{Highlighting}
\end{Shaded}

\textbf{MODEL QDA}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Estimación del modelo QDA}
\NormalTok{modelo\_qda }\OtherTok{\textless{}{-}} \FunctionTok{qda}\NormalTok{(Species }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ entrenamiento\_t)}
\NormalTok{modelo\_qda}
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} qda(Species \textasciitilde{} ., data = entrenamiento\_t)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Prior probabilities of groups:}
\CommentTok{\#\textgreater{}     setosa versicolor  virginica }
\CommentTok{\#\textgreater{}  0.3333333  0.3333333  0.3333333 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Group means:}
\CommentTok{\#\textgreater{}            Sepal.Length Sepal.Width Petal.Length Petal.Width}
\CommentTok{\#\textgreater{} setosa       {-}1.0112835  0.78048647   {-}1.2900001  {-}1.2453195}
\CommentTok{\#\textgreater{} versicolor    0.1014181 {-}0.68674658    0.2566029   0.1472614}
\CommentTok{\#\textgreater{} virginica     0.9098654 {-}0.09373989    1.0333972   1.0980581}
\CommentTok{\# Probabilidades a priori: 0,3333 para cada grupo}
\CommentTok{\# El matriz de las medias de los grupos para cada una de nuestras varialbles según la variable predictora.}

\CommentTok{\# Gráfico Modelo QDA, se utiliza la función partimat que muestra el comportamiento de cada variable y su error.}
\FunctionTok{partimat}\NormalTok{(Species }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Sepal.Length }\SpecialCharTok{+}\NormalTok{ Sepal.Width }\SpecialCharTok{+}\NormalTok{ Petal.Length }\SpecialCharTok{+}\NormalTok{ Petal.Width, }\AttributeTok{data =}\NormalTok{ entrenamiento\_t, }\AttributeTok{method =} \StringTok{"qda"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/150025-qda-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Predicciones del modelo QDA}
\NormalTok{predicciones\_qda }\OtherTok{\textless{}{-}}\NormalTok{ modelo\_qda }\SpecialCharTok{|\textgreater{}} \FunctionTok{predict}\NormalTok{(test\_t)}
\FunctionTok{table}\NormalTok{(test\_t}\SpecialCharTok{$}\NormalTok{Species, predicciones\_qda}\SpecialCharTok{$}\NormalTok{class, }\AttributeTok{dnn =} \FunctionTok{c}\NormalTok{(}\StringTok{"Clase real"}\NormalTok{, }\StringTok{"Clase predicha"}\NormalTok{))}
\CommentTok{\#\textgreater{}             Clase predicha}
\CommentTok{\#\textgreater{} Clase real   setosa versicolor virginica}
\CommentTok{\#\textgreater{}   setosa         10          0         0}
\CommentTok{\#\textgreater{}   versicolor      0         10         0}
\CommentTok{\#\textgreater{}   virginica       0          1         9}

\CommentTok{\# Precisión del modelo QDA}
\FunctionTok{mean}\NormalTok{(predicciones\_qda}\SpecialCharTok{$}\NormalTok{class }\SpecialCharTok{==}\NormalTok{ test\_t}\SpecialCharTok{$}\NormalTok{Species)}
\CommentTok{\#\textgreater{} [1] 0.9666667}
\CommentTok{\# Ajusta al 96,9667\%.}
\end{Highlighting}
\end{Shaded}

\textbf{MODEL MDA}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Estimación del modelo MDA}
\NormalTok{modelo\_mda }\OtherTok{\textless{}{-}} \FunctionTok{mda}\NormalTok{(Species }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ entrenamiento\_t)}
\NormalTok{modelo\_mda}
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} mda(formula = Species \textasciitilde{} ., data = entrenamiento\_t)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Dimension: 4 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Percent Between{-}Group Variance Explained:}
\CommentTok{\#\textgreater{}     v1     v2     v3     v4 }
\CommentTok{\#\textgreater{}  93.36  97.61  99.64 100.00 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Degrees of Freedom (per dimension): 5 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Training Misclassification Error: 0.01667 ( N = 120 )}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Deviance: 9.704}
\CommentTok{\# La salidas muestran los porcentajes, los grados de libertad y el error de clasificación.}

\CommentTok{\# Predicciones del modelo MDA, su cálculo requiere del uso de data.frame y no se obtiene la categoría class, sino que se calcula directamente.}
\NormalTok{predicciones\_mda }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(modelo\_mda }\SpecialCharTok{|\textgreater{}} \FunctionTok{predict}\NormalTok{(test\_t))}

\DocumentationTok{\#\# TABLE NO DA ERRROR}

\CommentTok{\# table(test\_t$Species, predicciones\_mda$modelo\_mda, dnn=c("Clase real", "Clase predicha"))}

\CommentTok{\# Precisión del modelo MDA}
\FunctionTok{mean}\NormalTok{(predicciones\_mda}\SpecialCharTok{$}\NormalTok{modelo\_mda }\SpecialCharTok{==}\NormalTok{ test\_t}\SpecialCharTok{$}\NormalTok{Species)}
\CommentTok{\#\textgreater{} [1] NaN}
\CommentTok{\# Ajusta al 96,9667\%.}
\end{Highlighting}
\end{Shaded}

\begin{infobox_resume}
\textbf{Resumen}

El \emph{análisis discriminante} permite clasificar individuos en distintos grupos preexistentes en relación a una variable cualitativa, a partir de las variables clasificadoras. La información se sintetiza en las funciones discriminantes. Su uso puede tener una finalidad descriptiva, identificar la separación entre grupos y la contribución de cada variable clasificadora; y una finalidad predictiva, para clasificar un individuo nuevo.
Los principales tipos son el LDA (lineal), QDA (cuadrático) y MDA (mixto), que se desarrollan en R con las funciones lda, qda y mda, respectivamente.

\end{infobox_resume}

\hypertarget{cap-conjunto}{%
\chapter{Análisis conjunto}\label{cap-conjunto}}

\emph{Mª Leticia Meseguer Santamaría}

\hypertarget{introducciuxf3n-conceptos-clave-y-tipos-de-anuxe1lisis}{%
\section{Introducción, conceptos clave y tipos de análisis}\label{introducciuxf3n-conceptos-clave-y-tipos-de-anuxe1lisis}}

El \textbf{análisis conjunto} o \emph{Conjoint analysis} estudia situaciones de elección múltiple. Funciona dividiendo un producto o servicio en sus componentes (atributos y niveles) y analizando las utilidades parciales de cada uno; después se realizan diferentes combinaciones de éstos para identificar las preferencias del consumidor. Permite conocer los preferencias del público ante el lanzamiento de nuevos productos o servicios, para adaptarlo a ellas y maximizar el éxito, evalua la sensibilidad al precio u otras características del producto y predice el comportamiento en el mercado. Mediante este análisis se pueden establecer qué atributo y nivel son los más valorados y cuantificarlos de forma relativa.

Los principales conceptos clave son:

\begin{itemize}
\item
  \textbf{Atributos}: Características de un producto o servicio sobre las que se basará la elección.
\item
  \textbf{Niveles}: Valores que puede tomar cada atributo. El número de niveles no tiene por qué ser igual en todos los atributos, pero es conveniente que sea similar para facilitar la elección del entrevistado.
\item
  \textbf{Diseño experimental}: Proceso estadístico por el que se confeccionan las opciones de las preguntas de la encuesta, y que realiza el investigador.
\item
  \textbf{Utilidades parciales}: Valoración numérica que representa el grado de preferencia por cada nivel de atributo. Se hace en referencia a las otras opciones. También denominadas ``Partworth Utility'', ``Nivel de preferencia'', ``niveles relativos de preferencias'' o ``valoraciones relativas''.
\item
  \textbf{Importancia}: Valores numéricos que señalan la preferencia por cada atributo.
\item
  \textbf{Perfil}: Combinación concreta de niveles de atributo de un producto o servicio. También se denomina alternativa.
\item
  \textbf{Escenarios}: Conjunto de perfiles entre los que el entrevistado tiene que señalar sus preferencias.
\item
  \textbf{Pregunta}: Conjunto de opciones, normalmente habrá unas 8 o 12.
\item
  \textbf{Probabilidad de elección}: Probabilidad de tomar una decisión determinada considerando las utilidades parciales.
\item
  \textbf{Modelo de comportamiento}: Modelo de decisión que se infiere desde las probabilidades de elección, después del análisis de las preferencias (utilidades parciales) de los entrevistados.
\item
  \textbf{Análisis}: Estimación de las utilidades parciales.
\item
  \textbf{Valoración}: Impacto del nuevo producto o servicio, basado en el modelo de comportamiento que se haya establecido.

  \textbf{Ejemplo Elección de gimnasio}: Sea una muesta de gimnasios en la que se identifican 4 atributos (características) con distintos niveles. Se realiza una encuesta sobre qué gimnasio se prefiere. Los paricipantes eligen una opción entre distintas combinaciones ofrecidas (\textbf{preguntas}). Mediante el análisis de los resultados de la encuesta se extraerá el peso de cada atributo y nivel en las respuestas (\textbf{utilidades parciales}), que describen las preferencias medias de los encuestados y se identifican los atributos y niveles más valorados y su importancia relativa.

  Por ejemplo, los resultados pueden ofrecen que el horario más demandado sea el de 09:00 a 23:00 durante 7 días a la semana.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{img/150024-gim} 

}

\caption{Gimnasio. Atributos: Horario, Clases programadas, Limpieza y Precio}\label{fig:150024-gim}
\end{figure}

En la literatura científica sobre este análisis se diferencian tres formas de abordar la investigación:

\textbf{El análisis tradicional, de perfil completo (Full Profile) o Conjoint Value Analysis (CVA)}

Se muestra al entrevistado una selección de productos ya combinados de atributos y niveles (perfiles) y se le pide que los valoren según su preferencia con una escala numérica, que será la utilidad. Las preferencias globales de cada individuo se descomponen en valores de utilidades independientes y compatibles para cada atributo y nivel mediante métodos basados en regresión lineal múltiple, son las funciones de utilidad por los niveles, partworth.
En el diseño original se detallan todos los perfiles hipotéticos (perfiles completos). Sin embargo, su número suele ser alto y se hace uso de técnicas de investigación que señalen los más significativos. Está limitado a un análisis con muy pocos atributos y niveles.

\textbf{El análisis conjunto adaptativo o Adaptative Conjoint Analysis (ACA)}

La recolección de datos se hace en dos fases, en la primera, el encuestado señala la importancia para él de cada atributo, y en la segunda, asigna utilidades a un número limitado de perfiles. Es decir, se le conduce a través de una investigación sistemática por diferentes secciones, en las que se le presentan uno o pocos atributos, y que se va adaptando a sus respuestas. Se obtienen las valoraciones de los niveles de interés (utilidades parciales).
Está limitado por su complejidad y por el uso de software especializado, aunque permite un gran número de atributos y niveles.

\textbf{El análisis conjunto basado en elecciones o Choise-Based Conjoint (CBC)}

Pretende un mayor realismo, mostrando a cada entrevistado un grupo de productos y se le pide que elija cuál de ellos prefiere. Además, contempla la posibilidad de no elegir ninguna alternativa. Mediante un modelo de regresión no lineal se calculan las utilidades de los atributos-niveles, que son conocidos como modelo de elección discreta. Como inconvenientes se pueden señalar que tiene una mayor complejidad en su diseño y análisis, que requiere de muestras muy grandes para tener validez estadística y que el número máximo de atributos es 10, y como ventaja, que presenta un mejor sistema de elección.

\hypertarget{aplicaciuxf3n-del-anuxe1lisis-conjunto-etapas}{%
\section{Aplicación del Análisis Conjunto (etapas):}\label{aplicaciuxf3n-del-anuxe1lisis-conjunto-etapas}}

\textbf{Planteamiento del problema}: Se analiza la oportunidad de aplicar esta técnica dados el objetivo y los datos de la investigación.

\textbf{Elección de la metodología conjoint a aplicar} (dependerá del producto o servicio y de los atributos elegidos):

\begin{itemize}
\item
  CBC: hasta 6 atributos y productos habituales (de elección rápida, sin reflexión).
\item
  ACA: Productos de elección rápida y reflexiva. Más de 10 atributos.
\item
  CVA: Productos de elección rápida y reflexiva. Desde 6 hasta 10 atributos.
\end{itemize}

\textbf{La selección de elementos}: \textbf{Atributos}, se escogerán los que condicionan la elección, es decir, los que explican sus preferencias y permiten diferenciar bien entre los productos o servicios, deben ser lo más independientes entre sí; \textbf{los niveles}, determinados dentro de cada atributo, deben ser mutuamente excluyentes y cubrir todo el rango de posibilidades; y, en su caso, \textbf{perfiles y escenarios}, dependerán de la metodología a usar.

\textbf{Creación de estímulos}: Se utilizarán diseños factoriales fraccionados ortogonales que reducen el número de perfiles que se le muestran al entrevistado.

\textbf{Forma de presentación}, dependerá de la metodología elegida, se exponen ejemplos en la Fig. \ref{fig:150024-pres}:

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/150024-pres} 

}

\caption{Formas de presentación}\label{fig:150024-pres}
\end{figure}

\begin{itemize}
\item
  CVA: \emph{Matriz ``Trade-off'' o de comparaciones}: El entrevistado valora la combinación de atributos y niveles. Sólo es válido para dos atributos; \emph{Perfiles completos para ordenar}: Se elaboran perfiles de cada producto o servicio utilizando sólo un nivel de cada atributo y el encuestado los valora (ordena) según sus preferencias; y \emph{Perfil determinado para valorar}: el encuestado la valore según sus preferencias.
\item
  ACA: \emph{Comparaciones Pareadas}: Se comparan dos perfiles incompletos.
\item
  CBC: \emph{Elección de un perfil}: los encuestados señalan el perfil preferido entre el subconjunto que se les muestra, sin valorarlos ni ordenarlos.
\end{itemize}

\textbf{Trabajo de campo y tratamiento de los datos}

\begin{itemize}
\item
  CVA: La recogida de datos es en papel u ordenador, y el análisis en ordenador con software especializado.
\item
  ACA y CBC: La recogida y el análisis son con ordenador.
\end{itemize}

\textbf{Estimación de las utilidades}

\begin{itemize}
\item
  CVA: Se utiliza, por lo general, un modelo de Mínimos Cuadrados Ordinales (OLS), mediante el cual se determinan las utilidades parciales o parth-worth, que indican las preferencias del encuestado mediante un modelo aditivo lineal en relación a los niveles de los atributos considerados. La Utilidad total de un perfil es la suma de las parth-worth, que se podrá estimarse para cada individuo o para el total de la muestra.

  \begin{itemize}
  \tightlist
  \item
    Modelo de preferencias totales de cada perfil (i) para el total de la muestra:
  \end{itemize}
\end{itemize}

\begin{equation}
  \widehat{U_i}=\beta_0 + \sum_{j=1}^J\widehat{u_{jk}}
\end{equation}
donde: \(\widehat{U_i}\) es la utilidad global para el perfil i; \(\widehat{u_{jk}}\) es la utilidad parcial asociada con el nivel k del atributo j; y \(\beta_0\) esla constante de regresión.

\begin{itemize}
\tightlist
\item
  Modelo de preferencias totales de cada perfil (i) para cadad individuo (z):
\end{itemize}

\begin{equation}
  \widehat{U_{iz}}= \beta_{0z}+\sum_{j=1}^J \widehat{u_{jkz}}
\end{equation}
donde: \(\widehat{U_{iz}}\) es la utilidad total del perfil i para el perfil z; \(\widehat{u_{jkz}}\) es la utilidad parcial asociada con el nivel k del atributo j para cada individuo z; y \(\beta_{0z}\) esla constante de regresión para cada individuo z.

\textbf{Interpretación de resultados}, se estudian:

\begin{itemize}
\item
  La importancia de los atributos y niveles.
\item
  Las utilidades globales, tanto de los productos de forma individual como en comparación con otros.
\item
  La elasticidad de alguna determinada variable, normalmente el precio, en relación a la elección que hace el sujeto.
\end{itemize}

\hypertarget{procedimiento-con-r-la-funciuxf3n-conjoint}{%
\section{\texorpdfstring{Procedimiento con \textbf{R}: la función \texttt{Conjoint()}}{Procedimiento con R: la función Conjoint()}}\label{procedimiento-con-r-la-funciuxf3n-conjoint}}

Se hará uso de la librería \textbf{conjoint} y de su base de datos \textbf{tea}, compuesta por 5 listados: niveles (tlevn), perfiles (tprof), preferencias (tpref), preferencias en forma matricial (tprefm), y la simulación de perfiles (tsimp).

\textbf{Ejemplo}: Se desea conocer el porqué unos tés son preferidos a otros, para ello se seleccionan 4 atributos, con sus correspondiente niveles:

\begin{itemize}
\item
  Precio, distinguiendo entre bajo, medio y alto.
\item
  Variedad, distinguiendo entre negro, verde y rojo.
\item
  Forma de presentación, distinguiendo entre bolsita, granulado y hojas.
\item
  Aroma, distinguiendo entre aromático y no aromático.
\end{itemize}

Del total de 54 posibles combinaciones se seleccionan 13 perfiles, valorados de 0 a 10 según las preferencias de cada uno de los 100 encuestados.

Se trata de un análisis tradicional con perfiles incompletos.

Se cargan la librería y los datos:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(conjoint)}
\FunctionTok{data}\NormalTok{(}\StringTok{"tea"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Se muestran los datos, se puede utilizar el comando \emph{str()} para que solo muestre unos pocos.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tlevn}
\NormalTok{tprof}
\FunctionTok{str}\NormalTok{(tpref)}
\FunctionTok{str}\NormalTok{(tprefm)}
\NormalTok{tsimp}
\end{Highlighting}
\end{Shaded}

Con la función \emph{conjoint} para el \emph{total de la muestra}, devolverá las utilidades de los niveles (part-worths), el vector de porcentajes (la suma debe ser 100) de la importancia de los atributos y las gráficas correspondientes.

\textbf{OJO: ES INTERACTIVA Y ME DA ERROR, SACAR LOS RESULTADOS MÁS IMPORTNATES Y LOS PLOTS A COLOR Y PONERLOS COMO IMÁGEN}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(conjoint)}
\FunctionTok{Conjoint}\NormalTok{(}\AttributeTok{y =}\NormalTok{ tpref, }\AttributeTok{x =}\NormalTok{ tprof, }\AttributeTok{z =}\NormalTok{ tlevn)}
\end{Highlighting}
\end{Shaded}

\textbf{Interpretación del resultado:}

\begin{itemize}
\item
  Los coeficientes del modelo de regresión referidos a todos los niveles, en el que se señala con asteriscos los que resultan significativos, así como su grado de ajuste.
\item
  Ajuste del modelo de regresión.
\item
  Las utilidades por niveles o part-worth.
\item
  La importancia de los atributos o factores, en este ejemplo el atributo con más peso es la variedad (32.22); el siguiente es la presentación (27.15); después; el siguiente, el precio (24.76); y por último, el aroma (15.88). Esta información también se nos muestra de forma gráfica y agrupada.
\end{itemize}

Además, analizando los datos por niveles (parth-worth) se puede señalar que:

\begin{itemize}
\item
  El atributo más importante: Variedad, con una utilidad parcial de 32.22, y la variedad de té negro (black) es el nivel más importante, con 0.6149.
\item
  El segundo atributo más importante: Presentación, con una utilidad parcial de 27.15, y en hojas (leafy) es el nivel más importante, con 0.7529.
\item
  El tercer atributo más importante: Precio, con una utilidad parcial de 24.76, y el precio bajo (low) es el nivel más importante, con 0.2402.
\item
  El cuarto atributo más importante: Aroma, con una utilidad parcial de 15.88 y que sea aromático (yes) es el nivel más importante, con 0.4108.
\end{itemize}

En la siguiente tabla se exponen otras funciones en R para determinados cálculos:

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5217}}
  >{\centering\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4783}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\centering
Función en R
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Utilidad-resultado
\end{minipage} \\
\midrule()
\endhead
caUtilities (y=tprefm , x=tprof, z=tlevn) & Utilidades para cada nivel sobre el total de los individuos \\
caPartUtilities (y=tprefm , x=tprof, z=tlevn) & Matriz de utilidades de los niveles para cada individuo \\
caTotalUtilities (y=tprefm , x=tprof) & Utilidades totales de los perfiles para los encuestados individualmente \\
colMeans(caTotalUtilities(y=tprefm , x=tprof)) & Utilidades totales por perfil \\
ShowAllUtilities (y=tprefm , x=tprof, z=tlevn) & Todas las utilidades \\
caImportance (y=tprefm , x=tprof) & Importancia relativa de cada atributo considerando el total de individuos \\
print(caImportance (y=tprefm , x=tprof) & Para obtener la gráfica \\
caModel (y=tprefm{[}20,{]}, x=tprof) & Preferencias para un solo individuo, por ejemplo el 20. \\
caUtilities (y=tprefm{[}20,{]} , x=tprof, z=tlevn) & Utilidades de los niveles para un individuo (p.e. 20) \\
\bottomrule()
\end{longtable}

\textbf{Aplicación: Segmentación}

\begin{itemize}
\item
  Objetivo: identificar grupos de encuestados con preferencias similares.
\item
  Función a utilizar: caSegmentatión, devuelve un análisis de conglomerados dividiendo a los individuos en n clusters usando el método de k-means. Se tomarán 3 clusters.
\end{itemize}

El vector de agrupación que se mostrará contiene los valores del cluster.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{caSegmentation}\NormalTok{(}\AttributeTok{y =}\NormalTok{ tprefm, }\AttributeTok{x =}\NormalTok{ tprof, }\AttributeTok{c =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Para la gráfica se utilizará la función plotcluster

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{segments }\OtherTok{\textless{}{-}} \FunctionTok{caSegmentation}\NormalTok{(}\AttributeTok{y =}\NormalTok{ tprefm, }\AttributeTok{x =}\NormalTok{ tprof, }\AttributeTok{c =} \DecValTok{3}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(segments)}
\FunctionTok{require}\NormalTok{(fpc)}
\FunctionTok{plotcluster}\NormalTok{(segments}\SpecialCharTok{$}\NormalTok{util, segments}\SpecialCharTok{$}\NormalTok{sclu)}
\end{Highlighting}
\end{Shaded}

\begin{infobox_resume}
\textbf{RESUMEN}

El \textbf{análisis conjunto} estudia situaciones de elección múltiple, divide un producto o servicio en atributos y niveles y analiza las utilidades parciales de cada uno, después, se realizan diferentes combinaciones de éstos para identificar las preferencias del consumidor, estableciendo qué atributos y niveles son los más valorados y cuantificarlos de forma relativa. Permite evaluar las preferencias del público ante el lanzamiento de nuevos productos o la sensibilidad de alguna característica, por ejemplo el precio, a los cambios.
Función principal de uso en R: \textbf{conjoint}.

\end{infobox_resume}

\hypertarget{tablas-contingencia}{%
\chapter{Análisis de tablas de contingencia}\label{tablas-contingencia}}

\emph{José-María Montero}

\hypertarget{introducciuxf3n-9}{%
\section{Introducción}\label{introducciuxf3n-9}}

\hypertarget{motiv}{%
\subsection{Motivación}\label{motiv}}

Las tablas de contingencia analizan la relación existente entre variables categóricas, o susceptibles de categorizar, con un número de categorías finito. Dada su naturaleza, no permiten el uso de las tradicionales operaciones aritméticas, con lo cual, en el ámbito de la Estadística Descriptiva su análisis suele basarse en diagramas de barras y porcentajes (poco más), y en la esfera de la Inferencia Estadística se centra en los contrastes de hipótesis no paramétricos y, básicamente, en el contraste de independencia entre dos o más de estas variables. Una pregunta que suele hacerse toda aquella persona que se acerca por primera vez al análisis de tablas de contingencia es el significado del término ``contingencia''. Pues bien, este término fue acuñado por Pearson \citep{Pearson1904} al apuntar: ``\emph{\ldots{} Este resultado nos permite partir de la teoría matemática de la independencia probabilística, tal como se desarrolla en los libros de texto elementales, y construir a partir de ella una teoría generalizada de la \textbf{asociación} o, como yo la llamo, \textbf{contingencia}.}''

El análisis de tablas de contingencia (o de asociación) permite dar respuesta, entre otras, a preguntas como: Los factores involucrados en una tabla de contingencia, ¿son independientes o están asociados? Si están asociados, ¿qué niveles de dichos factores son los que están asociados?, ¿cuál es la intensidad de dicha asociación?

\hypertarget{notac}{%
\subsection{Notación}\label{notac}}

Sea una población (o una muestra) de \emph{N} elementos sobre la que se pretende analizar, simultáneamente, dos (por simplicidad) \textbf{atributos o factores} (\emph{A} y \emph{B}) con \emph{R} y \emph{C} \textbf{\emph{niveles, modalidades o categorías}}, respectivamente. Sean \{\emph{A}\textsubscript{1}, \emph{A}\textsubscript{2}, \ldots, \emph{A\textsubscript{R}}\} y \{\emph{B}\textsubscript{1}, \emph{B}\textsubscript{2}, \ldots, \emph{B\textsubscript{C}}\} los niveles anteriormente aludidos. Sea \emph{n\textsubscript{ij}} el número de elementos que presentan a la vez las modalidades \emph{i} y \emph{j} de los factores \emph{A} y \emph{B}, respectivamente. La tabla estadística que describe, conjuntamente, estos \emph{N} elementos (en otros términos, que muestra las frecuencias conjuntas de los niveles de ambos factores) se denomina ``tabla de contingencia''.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\centering\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Factor B
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\end{minipage} \\
\midrule()
\endhead
& & Nivel \emph{B}\textsubscript{1} & Nivel \emph{B}\textsubscript{2} & . . . & Nivel \emph{B}\textsubscript{C} & \textbf{Total} \\
& Nivel \emph{A}\textsubscript{1} & \emph{n}\textsubscript{11} & \emph{n}\textsubscript{12} & . . . & \emph{n}\textsubscript{1\emph{C}} & \emph{n}\textsubscript{1.} \\
& Nivel \emph{A}\textsubscript{2} & \emph{n}\textsubscript{21} & \emph{n}\textsubscript{22} & . . . & \emph{n}\textsubscript{1\emph{C}} & \emph{n}\textsubscript{2.} \\
& . & . & . & . & . & . \\
Factor \emph{A} & . & . & . & . & . & . \\
& . & . & . & . & . & . \\
& Nivel \emph{A\textsubscript{R}} & ~\emph{n\textsubscript{R}}\textsubscript{1} & \emph{n\textsubscript{R}}\textsubscript{2} & . . . & \emph{n\textsubscript{RC}} & \emph{n\textsubscript{R}}\textsubscript{.} \\
& \textbf{Total} & \emph{n.}\textsubscript{1} & \emph{n}\textsubscript{.2} & . . . & \emph{n}\textsubscript{.\emph{C}} & \emph{N} \\
\bottomrule()
\end{longtable}

\begin{example}[Tabla de contingencia con dos factores y dos niveles cada factor]
\protect\hypertarget{exm:ex-tablas}{}\label{exm:ex-tablas}Se toma una muestra de 80 ayuntamientos de una CC.AA. y se anota el signo político del equipo gubernamental y si prestan o no públicamente el servicio \emph{X}.
\end{example}

\begin{longtable}[]{@{}llccc@{}}
\toprule()
\endhead
& & Prestación & pública & \\
& & NO & SI & \textbf{Total} \\
Signo & Avanzados & 14 & 28 & \textbf{42} \\
político & Ilustrados & 6 & 32 & \textbf{38} \\
& \textbf{Total} & \textbf{20} & \textbf{60} & \textbf{80} \\
\bottomrule()
\end{longtable}

Del estudio de las distribuciones marginales de ambos factores se deduce que el 52,5\% de los ayuntamientos de la CC.AA. están regidos por los Avanzados y el 47,5\% por los Ilustrados. Y, más interesante, que en el 75\% de los ayuntamientos prestan el servicio \emph{X}.

El análisis de la tabla de contingencia daría respuesta a las siguientes preguntas: ¿La prestación pública del servicio \emph{X} es independiente del signo político del Ayuntamiento o depende de dicho signo? En este último caso: ¿Qué signo político está asociado con la prestación pública y cuál no?, ¿la asociación entre los factores ``Signo político del equipo gubernamental'' y ``Prestación pública del servicio \emph{X}'' es muy intensa? Del análisis de la tabla de contingencia anterior nos ocuparemos posteriormente. Por el momento, nos centramos en su construcción a partir de la base de datos de este ejemplo y en el cálculo de porcentajes (en este caso, por fila). Se procede como sigue:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(CDR)}
\FunctionTok{data}\NormalTok{(}\StringTok{"ayuntam"}\NormalTok{)}

\NormalTok{ayuntam }\SpecialCharTok{\%$\%}
\NormalTok{  summarytools}\SpecialCharTok{::}\FunctionTok{ctable}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{signo\_gob}\StringTok{\textasciigrave{}}\NormalTok{, }\StringTok{\textasciigrave{}}\AttributeTok{serv}\StringTok{\textasciigrave{}}\NormalTok{, }\AttributeTok{headings =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{print}\NormalTok{()}
\CommentTok{\#\textgreater{} Cross{-}Tabulation, Row Proportions  }
\CommentTok{\#\textgreater{} signo\_gob * serv  }
\CommentTok{\#\textgreater{} Data Frame: ayuntam  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\#\textgreater{}                serv           No           Sí         Total}
\CommentTok{\#\textgreater{}    signo\_gob                                               }
\CommentTok{\#\textgreater{}    Avanzados          14 (33.3\%)   28 (66.7\%)   42 (100.0\%)}
\CommentTok{\#\textgreater{}   Ilustrados           6 (15.8\%)   32 (84.2\%)   38 (100.0\%)}
\CommentTok{\#\textgreater{}        Total          20 (25.0\%)   60 (75.0\%)   80 (100.0\%)}
\CommentTok{\#\textgreater{} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-} {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

En función del número de factores involucrados en la tabla y del número de niveles de cada uno de ellos se tiene la siguiente tipología de tablas de contingencia:

\begin{itemize}
\item
  Tablas \(R\times C\): 2 factores, el primero con \emph{R} niveles y el segundo con \emph{C} niveles.
\item
  Tablas \(R\times C \times M\): 3 factores, con \emph{R}, \emph{C} y \emph{M} niveles, respectivamente.
\item
  Y así sucesivamente.
\end{itemize}

Dentro de las tablas \(R\times C\) se distinguen las tablas \(2\times 2\) de las demás, por su especial interés en la realidad y por criterios pedagógicos, al ser las más sencillas.

\hypertarget{diseuxf1os-experimentales-o-procedimientos-de-muestreo-que-dan-lugar-a-una-tabla-de-contingencia}{%
\subsection{Diseños experimentales o procedimientos de muestreo que dan lugar a una tabla de contingencia}\label{diseuxf1os-experimentales-o-procedimientos-de-muestreo-que-dan-lugar-a-una-tabla-de-contingencia}}

Una cuestión a la que no se le da la suficiente importancia es la forma en la que se toma la información contenida en la tabla (el diseño del experimento o procedimiento de muestreo). Dada una determinada tabla de contingencia, ésta puede haber sido obtenida mediante uno u otro diseño de experimento o procedimiento de muestreo, y esta circunstancia no es baladí, puesto que condiciona su análisis, sobre todo cuando el tamaño muestral es pequeño.

Sin ánimo de exhaustividad, los diseños experimentales o procedimientos de muestreo más habituales que dan lugar a una tabla de contingencia son los siguientes\footnote{Otros procedimientos de muestreo o diseños experimentales no habituales pueden verse en \citet{Ruiz_Maya_et_al1995}.}:

\begin{itemize}
\tightlist
\item
  \textbf{Tipo 1: Se fijan los totales marginales de ambos factores}
\end{itemize}

\begin{example}
\protect\hypertarget{exm:ex-tablas3}{}\label{exm:ex-tablas3}Se desea investigar si la preferencia de la larva de gorgojo por el tipo de judía es independiente de la cubierta de la semilla o depende de ésta. Para ello se toman 22 judías de tipo \emph{A} y 18 de tipo \emph{B}, que se introducen en un recipiente con 33 larvas. Dadas las condiciones de densidad, no entrará más de una larva por judía. Pasado un tiempo prudencial, para que las larvas entren en las judías se hace el recuento de las que han sido atacadas de cada tipo y las que no.
\end{example}

\begin{longtable}[]{@{}llccc@{}}
\toprule()
\endhead
& & Presencia de larva & atacante & \\
& & NO & SI & \textbf{Total} \\
Tipo de & A & \emph{N}\textsubscript{11} & \emph{N}\textsubscript{12} & \textbf{22} \\
judía & B & \emph{N}\textsubscript{21} & \emph{N}\textsubscript{22} & \textbf{18} \\
& \textbf{Total} & \textbf{7} & \textbf{33} & \textbf{40} \\
\bottomrule()
\end{longtable}

Como puede apreciarse, los totales marginales de ambos factores han sido fijados en el diseño del experimento.

\begin{itemize}
\tightlist
\item
  \textbf{Tipo 2: Solo se fijan los totales marginales de uno de los factores}
\end{itemize}

\begin{example}
\protect\hypertarget{exm:ex-tablas5}{}\label{exm:ex-tablas5}En un municipio se desea investigar si el desempleo es o no independiente del sexo del desempleado. Se seleccionan aleatoriamente 100 varones y 100 mujeres y se les pregunta por su situación laboral (trabajando; en paro).
\end{example}

\begin{longtable}[]{@{}llccc@{}}
\toprule()
& & Situación & laboral & \\
\midrule()
\endhead
& & Trabajando & En paro & \textbf{Total} \\
Sexo & Varón & \emph{N}\textsubscript{11} & \emph{N}\textsubscript{12} & \textbf{100} \\
& Mujer & \emph{N}\textsubscript{21} & \emph{N}\textsubscript{22} & \textbf{100} \\
& \textbf{Total} & \emph{N}\textsubscript{.1} & \emph{N}\textsubscript{.2} & \textbf{200} \\
\bottomrule()
\end{longtable}

\begin{itemize}
\tightlist
\item
  \textbf{Tipo 3: Únicamente se fija el tamaño muestral}
\end{itemize}

\begin{example}
\protect\hypertarget{exm:ex-tablas6}{}\label{exm:ex-tablas6}Un estudio transversal sobre la prevalencia de osteoporosis y su relación con dietas pobres en calcio incluyó a 400 mujeres entre 50 y 54 años. Cada una de ellas realizó una densiometría de columna y rellenó un cuestionario sobre sus antecedentes dietéticos para determinar si su dieta era o no pobre en calcio.
\end{example}

\begin{longtable}[]{@{}llccc@{}}
\toprule()
& & Dieta pobre & en calcio & \\
\midrule()
\endhead
& & SI & NO & \textbf{Total} \\
Osteoporosis & SI & \emph{N}\textsubscript{11} & \emph{N}\textsubscript{12} & \emph{N}\textsubscript{1.} \\
& NO & \emph{N}\textsubscript{21} & \emph{N}\textsubscript{22} & \emph{N}\textsubscript{2.} \\
& \textbf{Total} & \emph{N}\textsubscript{.1} & \emph{N}\textsubscript{.2} & \textbf{400} \\
\bottomrule()
\end{longtable}

\hypertarget{contraste-de-independencia-en-tablas-2times-2}{%
\section{\texorpdfstring{Contraste de independencia en tablas \(2\times 2\)}{Contraste de independencia en tablas 2\textbackslash times 2}}\label{contraste-de-independencia-en-tablas-2times-2}}

\hypertarget{introducciuxf3n-10}{%
\subsection{Introducción}\label{introducciuxf3n-10}}

Como se avanzó en la sección \ref{motiv}, la primera pregunta a la que debe dar respuesta el análisis de tablas de contingencia es si los factores involucrados en la tabla son independientes o, por el contrario, están asociados. La respuesta a esta pregunta exige llevar a cabo un contraste de independencia y, para ilustrarlo, nos centraremos inicialmente en el caso de las tablas \(2\times 2\). Dicho contraste se llevará a cabo de tres formas: (i) exacta, (ii) aproximada, y (iii) aproximada con corrección de continuidad.

\hypertarget{plantgen}{%
\subsection{Planteamiento general del contraste exacto de independencia}\label{plantgen}}

\begin{itemize}
\tightlist
\item
  Hipótesis: H\textsubscript{0}: Los factores son independientes. H\textsubscript{1}: Están asociados.\footnote{También puede establecerse como hipótesis alternativa la asociación en un determinado sentido: ``El nivel 1 del factor \emph{A} está asociado con el 1 del \emph{B} y el 2 del \emph{A} con el 2 del \emph{B}'' (asociación positiva); o ``El nivel 1 del factor \emph{A} está asociado con el 2 del \emph{B} y el 2 del \emph{A} con el 1 del \emph{B}'' (asociación negativa). En estos dos casos el contraste no sería bilateral sino unilateral.}
\end{itemize}

\begin{itemize}
\tightlist
\item
  Filosofía del contraste: Se trata de un contraste de significación. Por tanto, la tabla observada será ``rara'' (bajo H\textsubscript{0}) si su probabilidad, más la probabilidad de obtener tablas más alejadas de H\textsubscript{0} que ella, es superior al nivel de significación, \(\alpha\), prefijado para el contraste. En ese caso, se rechaza la hipótesis de independencia entre los factores involucrados en la tabla.
\end{itemize}

\hypertarget{algoritmo}{%
\subsection{Algoritmo para la realización del contraste exacto de independencia}\label{algoritmo}}

De acuerdo con la filosofía de los contrastes de significación, \textbf{(VEASE SECCIÓN)} el algoritmo para la realización del contraste de independencia en tablas de contingencia es como sigue:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Selección de la tablas del espacio muestral que se alejen de la hipótesis de independencia, en la dirección marcada por la hipótesis alternativa, tanto o más que la tabla observada, incluida esta última.
\item
  Cálculo, bajo la hipótesis de independencia, de la probabilidad de ocurrencia de cada una de las tablas seleccionadas en el punto 1.
\item
  Suma de dichas probabilidades y comparación con el \(\alpha\) prefijado.
\item
  Toma de la decisión relativa al rechazo o no de la hipótesis de independencia.
\end{enumerate}

Nótese que \((i)\) los pasos 1 y 2 dependen del diseño del experimento o procedimiento de muestreo llevado a cabo; \((ii)\) en ausencia del software adecuado, la realización de un test exacto es un procedimiento laborioso (a veces un reto), con lo cual, si ese fuera el caso, los test aproximados de independencia son bienvenidos.

A continuación, se expone el contraste de independencia, en sus versiones exacta, aproximada y aproximada con corrección de continuidad, cuando el procedimiento de muestreo o diseño experimental es el de tipo 1. En la sección \ref{dise} se comentan algunas cuestiones de interés cuando el diseño de muestreo es de tipo 2 o tipo 3.

\hypertarget{contraste-de-independencia-diseuxf1o-tipo-1}{%
\subsection{Contraste de independencia: Diseño Tipo 1}\label{contraste-de-independencia-diseuxf1o-tipo-1}}

\hypertarget{contraste-exacto-test-exacto-de-fisher}{%
\subsubsection{Contraste exacto (test exacto de Fisher)}\label{contraste-exacto-test-exacto-de-fisher}}

Considérese el ejemplo \ref{exm:ex-tablas3}\footnote{Se trata de un ejemplo clásico de \citet{Sokal2012}}. Supóngase que el resultado obtenido fue el siguiente:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datos\_jud }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{12}\NormalTok{)}
\NormalTok{tabla }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}
  \FunctionTok{expand.grid}\NormalTok{(}\FunctionTok{list}\NormalTok{(}
\NormalTok{    Tipo\_de\_judía }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{),}
    \AttributeTok{Presencia\_larva =} \FunctionTok{c}\NormalTok{(}\StringTok{"No"}\NormalTok{, }\StringTok{"Sí"}\NormalTok{)}
\NormalTok{  )),}
  \AttributeTok{count =}\NormalTok{ datos\_jud}
\NormalTok{)}
\NormalTok{tabla\_jud }\OtherTok{\textless{}{-}} \FunctionTok{ftable}\NormalTok{(}\FunctionTok{xtabs}\NormalTok{(count }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Tipo\_de\_judía }\SpecialCharTok{+}\NormalTok{ Presencia\_larva, tabla))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llccc@{}}
\toprule()
\endhead
& & Presencia de larva & atacante & \\
& & NO & SI & \textbf{Total} \\
Tipo de & A & 1 & 21 & \textbf{22} \\
judía & B & 6 & 12 & \textbf{18} \\
& \textbf{Total} & \textbf{7} & \textbf{33} & \textbf{40} \\
\bottomrule()
\end{longtable}

Según el algoritmo expuesto en la sección \ref{algoritmo}, el contraste es como sigue:

\textbf{1}. \emph{Selección de las tablas que se alejan de \(H_0\) tanto o más que la observada}.\footnote{El contraste se ha llevado a cabo de forma bilateral. En caso de una alternativa unilateral (asociación positiva o en el sentido de la diagonal principal; o negativa, en el sentido de la diagonal no principal), el procedimiento sería el mismo, las tablas seleccionadas serían, en el primer caso, todas menos la \(T_7\), y en el segundo \(T_0\) y \(T_1\).} Como se señalaba en \citet{Pearson1904}, la teoría de la independencia probabilística indica que, bajo la hipótesis de independencia, el porcentaje de judías de tipo \emph{A} y de tipo \emph{B} no atacadas (o atacadas) por una larva de gorgojo tiene que ser el mismo. En otros términos, bajo la hipótesis de independencia, en cada una de las cuatro celdas se tiene que verificar que: \(n_{ij} = \frac{n_{i.} n_{.j}}{N}, \forall i,j\), donde \(\frac{n_{i.} n_{.j}}{N}= \hat{E}_{ij}\) se denomina estimación de la frecuencia esperada bajo la hipótesis de independencia. Se puede que comprobar que en una tabla 2x2, \(\hat{D}_{11}= \hat{D} _{22}= - \hat{ D}_{12}= - \hat{D}_{21}\), con lo cual las tablas que se alejan tanto o más que la observada de la hipótesis de independencia son aquellas que verifican, en valor absoluto, y tomando de referencia, por ejemplo, la celda \{1,1\}: \(\hat{D}_{11}=n_{11}-\hat {E}_{11}=n_{11}-\frac {n_{1.} n_{\cdot1}}{N} \geq \hat{D}_{11obs}\).

En nuestro caso, las \(\hat{D_{11}}\) son las siguientes (en negrita las de la tabla observada y aquellas otras que se alejan tanto o más que ella de H\textsubscript{0}):

\textbf{\emph{T}}\textsubscript{0}: -3,85; \textbf{\emph{T}}\textsubscript{1}: -2,85; \emph{T}\textsubscript{2}: -1,85; \emph{T}\textsubscript{3}: -0,85; \emph{T}\textsubscript{4}: 0,15; \emph{T}\textsubscript{5}: 1,15; \emph{T}\textsubscript{6}: 2,15; \textbf{\emph{T\textsubscript{7}}}: 3,15

donde el subíndice de \emph{T} indica el valor de \(n_{11}\) en dicha tabla.

Nótese que el criterio anterior no es otro que el criterio general de seleccionar las tablas en las que la diferencia de porcentajes, por ejemplo, por fila, en valor absoluto, sea superior a la de la tabla observada, puesto que \(\left|\frac {n_{11}}{n_{1.}} -\frac {n_{21}}{n_{2.}} \right|=|\hat{D}_{11} | \frac {N}{n_{1.} n_{2.} }\).

\textbf{2}. \emph{Cálculo, bajo la hipótesis de independencia, de la probabilidad de ocurrencia de cada una de las tablas seleccionadas en 1}. La probabilidad de ocurrencia de una tabla de contingencia con los totales marginales fijos se puede obtener como el cociente entre el número de disposiciones de las frecuencias observadas favorables a dicha tabla y el número de disposiciones posibles. El número de disposiciones favorables coincide con el coeficiente multinomial (maneras de que de \(n\) frecuencias, \(n_{11}\) caigan en la celda \{1,1\}, \(n_{12}\) lo hagan en la celda \{1,2\}, \(n_{21}\) lo hagan en la celda \{2,1\} y \(n_{22}\) lo hagan en la celda \{2,2\}:\(\frac {n!}{n_{11}! n_{12}! n_{21}! n_{22}!}\).

El número de disposiciones posibles, supuesta \(H_0\), es: \(\binom{n}{n_{1.}} \binom{n}{n_{\cdot1}} = \frac{n!}{(n_{1.}! n_{2.}!)} \frac {n!}{(n_{\cdot1} ! n_{\cdot2}}\).

Por tanto, el cociente entre ambas es: \(P=\frac {n_{1.} !n_{2.} !n_{\cdot1} !n_{\cdot2} !} {n!n_{11} !n_{12} !n_{21} !n_{22} !}\).

En consecuencia, las probabilidades de las tablas seleccionadas en 3 son: \(T_0: 0,0017; T_1: 0,0219; T_7: 0,0091\).

\textbf{3}. \emph{Suma de dichas probabilidades}: 0,0327.

\textbf{4}. \emph{Comparación con \(\alpha\) y decisión sobre el rechazo o no de la hipótesis de independencia}: La decisión depende del valor de \(\alpha\). Si fuera, por ejemplo, 0,05, se rechazaría la independencia entre el tipo de judía y si es o no atacada por la larva de gorgojo.

El código R necesario para tomar llevar a cabo el test exacto de Fisher anterior es:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Ho: Los factores son independientes.}
\CommentTok{\# H1: Los factores están asociados}
\NormalTok{fisher }\OtherTok{\textless{}{-}} \FunctionTok{fisher.test}\NormalTok{(tabla\_jud, }\AttributeTok{alternative =} \StringTok{"two.sided"}\NormalTok{)}
\NormalTok{fisher}\SpecialCharTok{$}\NormalTok{p.value}
\CommentTok{\#\textgreater{} [1] 0.0327607}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Ho: Los factores son independientes.}
\CommentTok{\# H1: Existe asociación negativa.}
\NormalTok{fisher\_less }\OtherTok{\textless{}{-}} \FunctionTok{fisher.test}\NormalTok{(tabla\_jud, }\AttributeTok{alternative =} \StringTok{"less"}\NormalTok{)}
\NormalTok{fisher\_less}\SpecialCharTok{$}\NormalTok{p.value}
\CommentTok{\#\textgreater{} [1] 0.02361309}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Ho: Los factores son independientes.}
\CommentTok{\# H1: Existe asociación positiva.}
\NormalTok{fisher\_greater }\OtherTok{\textless{}{-}} \FunctionTok{fisher.test}\NormalTok{(tabla\_jud, }\AttributeTok{alternative =} \StringTok{"greater"}\NormalTok{)}
\NormalTok{fisher\_greater}\SpecialCharTok{$}\NormalTok{p.value}
\CommentTok{\#\textgreater{} [1] 0.998293}
\end{Highlighting}
\end{Shaded}

Como puede apreciarse, se rechaza la hipótesis de independencia frente a la de asociación (test bilateral). Esto no significa que las larvas ataquen a un tipo de judía y no al otro. Atacan a ambos tipos, ¡y bastante! Este es el primer hecho que se constata. Sin embargo, atacan más a las judías de tipo \emph{A} (un 95\% son atacadas) que a las de tipo B (dos terceras partes son atacadas). Esa diferencia porcentual de judías atacadas se considera significativa bajo el supuesto de independencia y, en ese sentido, se dice que existe asociación entre el tipo de judía y la presencia o no de larva atacante. La asociación sería \emph{A}-SI y \emph{B}-NO. Sin embargo, ¡cuidado!, las larvas atacan siempre. La asociación anterior debe entenderse como ``el porcentaje de ataque es muy grande en ambos casos, pero en \emph{A} (mucho) más que en \emph{B}. Este es el segundo hecho importante que se constata: Las larvas muestran una preferencia significativa por las judías tipo \emph{A}. Aunque ya se ha visto la dirección de la asociación (en el sentido de la diagonal ascendente), en la sección \ref{medidas}, dedicada a las medidas de asociación en tablas 2x2, se cuantificará su intensidad.

\hypertarget{contraste-aproximado}{%
\subsubsection{Contraste aproximado}\label{contraste-aproximado}}

El contraste exacto de independencia se basa en el cálculo de la probabilidad de ocurrencia de cada una de las tablas que se alejen de la hipótesis de independencia tanto o más que la observada y su comparación con \(\alpha\). Cuando el procedimiento de muestreo es de tipo 1, si se toma como celda de referencia, por ejemplo, la \{1,1\} la probabilidad a comparar con \(\alpha\) es: \(P \left( N_{11}-\frac{n_{1.} n_{\cdot1}} {n} \right) ^2 \geq \left(n_{11} - \frac{n_{1.} n_{\cdot1}}{n}\right)^2\).

En este caso (tipo 1), bajo \(H_0\): independencia, la frecuencia conjunta de una celda, \(N_{ij}\), cualquiera que sea, se distribuye según una ley hipergeométrica con \(E(N_{ij})=\frac{N_{ij}} {n_{i.}n_{\cdot j}}\) y \(V(N_{ij})=\frac{n_{i.}n_{\cdot j}(n-n_{i.}) (n-n_{\cdot j})}{n^2 (n-1)}\). Por consiguiente,
\[P\left(\left( N_{11}-\frac{n_{1.} n_{\cdot1}} {n} \right) ^2 \geq \left(n_{11} - \frac{n_{1.} n_{\cdot1}}{n}\right)^2 \right)= P\left(\frac{(N_{11}-\frac{n_{1.} n_{\cdot1}}
{n})^2}{\frac{n_{1.}n_{\cdot1}n_{2.} n_{\cdot2}}{n^2 (n-1)}}
\geq \frac{(n_{11}-\frac{n_{1.} n_{\cdot1}}{n})^2}{\frac{n_{1.}n_{\cdot1}n_{2.} n_{\cdot2})}{n^2 (n-1)}}\right)\]

Y si ninguna \(\hat{E}_{ij}=\frac{n_{ij}} {n_{i.}n_{\cdot j}}\) es inferior a 5, la probabilidad anterior puede aproximarse (Teorema Central del Límite) por

\[P\left(\chi^2_1 \geq {\frac{\left( n_{11}-\frac{n_{1.} n_{\cdot1}}
{n}\right)^2} {\frac{n_{1.}n_{\cdot1}n_{2.} n_{\cdot2}}{n^2 (n-1)}}}\right)=P\left( \chi^2_1 \geq {\frac{ (n-1)(n_{11}n_{22}-n_{21}n_{12}
)^2} {n_{1.}n_{\cdot1}n_{2.} n_{\cdot2}}}\right),\]
donde el estadístico \(\frac{ (n-1)(n_{11}n_{22}-n_{21}n_{12} )^2} {n_{1.}n_{\cdot1}n_{2.} n_{.2}}\)
se denomina chi-cuadrado ajustado \((\chi^2_{ajd})\) y es tal que
\(\chi^2_{ajd}= \frac {n-1} {n}\frac{ n (n_{11}n_{22}-n_{21}n_{12} )^2} {n_{1.}n_{\cdot1}n_{2.} n_{\cdot2}}\),
donde \(\frac{ n (n_{11}n_{22}-n_{21}n_{12} )^2} {n_{1.}n_{\cdot1}n_{2.} n_{\cdot2}}\)
es el estadístico chi-cuadrado (\(\chi^2\)) que proporcionan todos los softwares de contraste de independencia en tablas de contingencia.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(tabla\_jud)}\SpecialCharTok{$}\NormalTok{expected}
\CommentTok{\#\textgreater{}      [,1]  [,2]}
\CommentTok{\#\textgreater{} [1,] 3.85 18.15}
\CommentTok{\#\textgreater{} [2,] 3.15 14.85}
\FunctionTok{chisq.test}\NormalTok{(tabla\_jud, }\AttributeTok{correct =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Pearson\textquotesingle{}s Chi{-}squared test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  tabla\_jud}
\CommentTok{\#\textgreater{} X{-}squared = 5.6828, df = 1, p{-}value = 0.01713}
\end{Highlighting}
\end{Shaded}

Como \(\chi^2= 5,6828\), entonces \(\chi^2_{ajd}=5,54073\) y \(P\left(\chi^2_{ajd} \geq 5,54073\right)=0,0186\).\footnote{Este ``cálculo extra'' es un pequeño peaje que hay que pagar en aras de la exactitud. Y es importante, porque pudiera hacer cambiar la decisión resultante del contraste.}

Nótese que la probabilidad exacta de obtener una tabla tan alejada o más de la hipótesis de independencia que la observada (incluida esta) es 0,0327 mientras que la probabilidad aproximada es 0,0186. La aproximación no es muy buena, y ello se debe a la existencia de frecuencias esperadas menores que 5.

\hypertarget{contraste-aproximado-con-correcciuxf3n-de-continuidad}{%
\subsubsection{Contraste aproximado con corrección de continuidad}\label{contraste-aproximado-con-correcciuxf3n-de-continuidad}}

Como se vio en la sección anterior, al aproximar la probabilidad de obtención de tablas tanto o más alejadas de \(H_0\) que la observada (que se calcula con una distribución hipergeométrica, que es discreta) mediante una distribución \(\chi^2_1\) (que es continua), se comete un ``error de continuización''. Dicho error se intenta corregir incluyendo en el contraste una ``corrección de continuidad''. Hay varias correcciones que han tenido cierto éxito en la literatura. La más popular es la corrección de Yates, si bien solo se recomienda cuando las \(\hat{E}_{ij}\) sean múltiplos de 0,5.

En el contraste aproximado con corrección de Yates, se rechaza \(H_0\) si:
\[P\left( \chi^2_1 \geq {\frac{ (n-1)(|n_{11}n_{22}-n_{21}n_{12}
|-0,5n)^2} {n_{1.}n_{\cdot1}n_{2.} n_{\cdot2}}}  \right)\leq \alpha ,\]
donde el estadístico \({\frac{ (n-1)(|n_{11}n_{22}-n_{21}n_{12}|-0,5n)^2} {n_{1.}n_{\cdot1}n_{2.} n_{\cdot2}}}\) se denomina estadístico chi-cuadrado ajustado corregido de continuidad de Yates (\(\chi^2_{ajd,CCY}\)).

En el ejemplo propuesto,

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(tabla\_jud)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Pearson\textquotesingle{}s Chi{-}squared test with Yates\textquotesingle{} continuity correction}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  tabla\_jud}
\CommentTok{\#\textgreater{} X{-}squared = 3.8637, df = 1, p{-}value = 0.04934}
\end{Highlighting}
\end{Shaded}

\(\chi^2_{CCY}= 3,8337\) y entonces \(\chi^2_{ajd,CCY}=\frac{39}{40}\times 3,8337=3,7636\); y como \(P\left( \chi^2_1 \geq 3,7636\right)=0,0524\),
\(H_0\) se rechazaría cuando \(\alpha > 0,0524\). Nótese que si, por ejemplo, \(\alpha = 0,05\), la decisión sobre el rechazo o no de \(H_0\) es distinta con \(\chi^2_{CCY}\) y \(\chi^2_{ajd,CCY}\); de ahí la importancia de utilizar el estadístico ajustado.

Por tanto, la corrección de Yates ha transformado la infraestimación de la probabilidad exacta en una sobreestimación de más o menos el mismo tamaño. Ello se debe a la existencia de \(\hat{E}_{ij}\) que distan mucho de ser multiplos de 0,5. La corrección de Yates es la que incluye la librería utilizada (`stats'). Otras correcciones pueden verse en \citet{Ruiz_Maya_et_al1995} y \citet{Montero2002}.

\hypertarget{dise}{%
\subsection{Contraste de independencia: Diseños Tipo 2 y Tipo 3}\label{dise}}

En el caso tipo 2, para la realización del test exacto, las tablas que se alejan tanto o más que la observada de la hipótesis de independencia son las que verifican: \[\left| \frac {N_{11}}{n_{1.}}-\frac{N_{21}}{n_{2.}}\right|
\geq \left|\frac{n_{11}}{n_{1.}}-\frac{n_{21}}{n_{2.}}\right|\]
y la probabilidad de ocurrencia de una tabla de contingencia viene dada por

\begin{equation}
P\left({N_{11}}={n_{11}} ;{N_{12}}={n_{12}};{N_{21}}={n_{21}};{N_{22}}={n_{22}} |N=n\right) 
= \\
\binom{n_{1.}}{N_{11}} \binom {n_{2.}}{N_{21}}
 \left( \frac{N_{\cdot1}}{n} \right)^{N_{\cdot1}} \left( \frac {N_{\cdot2}}{n}\right)^{N_{\cdot2}}
 \end{equation}

El estadístico de contraste en el test aproximado viene dado por \(\chi^2=\frac{ n (n_{11}n_{22}-n_{21}n_{12})^2} {n_{1.}n_{\cdot1}n_{2.} n_{\cdot2}}\), y por \(\chi^2_{CC}=\frac { n \left( \left|n_{11}n_{22}-n_{21}n_{12}\right|-\frac {f} {2}\right)^{2}} {n_{1.}n_{\cdot1}n_{2.} n_{\cdot2}}\) en el caso de estar corregido de continuidad, siendo \emph{f} el mayor factor común de los tamaños muestrales fijados.

En el caso tipo 3, las tablas que se alejan tanto o más que la observada de la hipótesis de independencia son las que verifican la condición expuesta en el tipo 2 (y tipo 1), siendo su probabilidad de ocurrencia:

\begin{equation}
\begin{split}
 P\left({N_{11}}  ={n_{11}} ;{N_{12}}={n_{12}};{N_{21}}={n_{21}};{N_{22}}={n_{22}} |N=n \right) = \\
 \frac {n!}{{n_{1.}! n_{2.}! n_{\cdot 1}! n_{\cdot2}!}}
\left(\frac {n_{1.}}{n} \frac{n_{\cdot1}}{n} \right)^{n_{11}}
\left(\frac {n_{1.}}{n} \frac{n_{\cdot2}}{n} \right)^{n_{12}}
\left(\frac {n_{2.}}{n} \frac{n_{\cdot1}}{n} \right)^{n_{21}}
\left(\frac {n_{1.}}{n} \frac{n_{\cdot2}}{n} \right)^{n_{22}}
\end{split}
\end{equation}

El test aproximado, en este caso, es un test razón de verosimilitudes donde el estadístico de contraste, \(G=-2ln\frac {n_{1.}^{n_{1.}} n_{2.}^{n_{2.}} n_{\cdot1}^{n_{\cdot1}} n_{\cdot2}^{n_{\cdot2}}}{n_{11}^{n_{11}} n_{21}^{n_{21}} n_{12}^{n_{12}} n_{22}^{n_{22}} n^n}\), también se distribuye como una \(\chi^2_1\) en caso de independencia. Apenas hay literatura sobre correcciones de continuidad en este modelo y la poca que hay sugiere la aplicación de la corrección de Yates. Mi recomendación es no usar ninguna, salvo que el tamaño muestral sea muy pequeño y sea imprescindible la realización del test.

El código R de estos dos contrastes aproximados puede verse en la sección \ref{contaprox}.

\hypertarget{contraste-de-independencia-en-tablas-rtimes-c}{%
\section{\texorpdfstring{Contraste de independencia en tablas \(R\times C\)}{Contraste de independencia en tablas R\textbackslash times C}}\label{contraste-de-independencia-en-tablas-rtimes-c}}

\hypertarget{IntroRxC}{%
\subsection{Introducción}\label{IntroRxC}}

El análisis de tablas \(R\times C\) puede considerarse, en principio, una generalización del caso de tablas \(2\times 2\). Ahora bien, en el caso \(R\times C\) los test exactos, recomendados en el caso de que \(E_{ij}\leq5\) en más del 20\% de las celdas {[}\citet{Reynolds1984}{]}\footnote{La razón es que, sea cual sea el diseño de muestreo, si la probabilidad de que un elemento de la tabla caiga en una determinada celda \{\emph{i},\emph{j}\} es muy pequeña (se suele utilizar el límite del 5\%), entonces la distribución de probabilidad de \(N_{ij}\) es muy asimétrica, y para que se simetrice lo suficiente y se pueda aproximar por una Normal (que después, al cuadrado, participará en una chi-cuadrado) el tamaño muestral, en el tipo 3, y los totales marginales fijados, en los tipos 1 y 2, tienen que ser grandes (se suele fijar el valor de 100), de ahí el límite de \(E_{ij}\leq5\)); por eso es recomendable que el tamaño muestral sea grande y los totales marginales no estén desequilibrados. En todo caso, lo ideal es que requisito \(E_{ij}\leq5\) se cumpla en todas las celdas de la tabla.}, son un auténtico reto y aún no están disponibles en el software convencional.

Si no se cumple el requisito anterior, una solución es agrupar categorías, con sentido común y coherencia\footnote{En tablas \(2\times 2\) no se pueden agrupar filas, y la única solución son los tests exactos.}. Si la agrupación de categorías no pudiese hacerse, por carecer de sentido o cualquier otro motivo, lo más honesto sería no realizar el contraste hasta disponer de una base de datos mejor.

A la luz de lo anteriormente expuesto, en el caso de tablas \(R\times C\) la atención se centrará en los tests aproximados.

\hypertarget{contaprox}{%
\subsection{Contrastes aproximados}\label{contaprox}}

Cuando el procedimiento de muestreo o el diseño experimental es de tipo 1 o 2 el contraste aproximado de independencia es el denominado contraste Chi-cuadrado. La filosofía de dicho contraste es la siguiente: Parece lógico que el contraste se base en las diferencias (cuadráticas, para que no se compensen las negativas con las positivas) entre las frecuencias observadas y las esperadas bajo la hipótesis de independencia. Si los factores son independientes, dichas diferencias serán pequeñas y atribuibles a fluctuaciones aleatorias. Si están asociados, serán grandes y atribuibles a la asociación existente entre sus niveles. Pearson propuso el siguiente estadístico de contraste: \[\chi^2 =\sum_{i=1}^{R} \sum_{j=1}^{C}\frac {\left(N_{ij}-\hat{E}_{ij}\right)^2} {\hat{E}_{ij}},\] que, si no se incumple el requisito expuesto en \ref{IntroRxC}, y bajo la hipótesis de independencia, se distribuye como una \(\chi_{(R-1)(C-1) }^2\). En caso de que la probabilidad de que una \(\chi_{(R-1)(C-1)}^2\) sea inferior al nivel de significación prefijado, se rechazará la hipótesis de independencia. Téngase en cuenta que en el caso \(R\times C\) la hipótesis alternativa es ``al menos un nivel de un factor está asociado con un nivel del otro factor''.

La razón de que las diferencias
\(\left(N_{ij}-\hat{E}_{ij}\right)^2\) se dividan por \(\hat{E}_{ij}\) en el estadístico chi-cuadrado de Pearson es la siguiente: La misma diferencia \(N_{ij}-\hat{E}_{ij}\) puede significar cosas bien diferentes. Una diferencia de 5 no es nada si \(\hat{E}_{ij}=1.000\); pero es muchísimo si \(\hat{E}_{ij}=2\). Por eso la diferencia (cuadrática) se pone en relación con la frecuencia esperada.

En el caso de que el procedimiento de muestreo sea del tipo 3, aunque puede aplicarse el contraste chi-cuadrado, es recomendable proceder con el contraste de independencia de razón de verosimilitudes, que compara las frecuencias esperadas bajo la hipótesis de independencia y las observadas por cociente. Se basa en la razón de la verosimilitud de la hipótesis de independencia a la luz de la muestra obtenida y del máximo de la función de verosimilitud, \(\Lambda\). Bajo el supuesto de independencia la razón será cercana a la unidad, atribuyéndose la diferencia al fluctuaciones aleatorias; el logaritmo neperiano de dicha razón (negativo) estará cercano a cero. En caso contrario, el cociente de verosimilitudes (negativo) disminuye, tanto más cuanto más diferencia hay entre la verosimilitud de la hipótesis de independencia y el máximo de la función de verosimilitud. En \citet{Wilks1935} se demostró que, cuando la hipótesis de independencia es cierta, \(G=-2ln \Lambda\), con \(\Lambda= \prod_{i=1}^R \prod_{j=1}^C \left (\frac {\hat{E}_{ij}} {n_{ij}}\right)^{n_{ij}}\) se distribuye como una \(\chi_{(R-1)(C-1)}^2\). Ambos estadísticos de contraste, \(\chi^2\) y \(G\), son asintóticamente equivalentes.

A modo de ejemplo, se quiere contrastar si en la Comunidad de Madrid la opinión
sobre la presidente Dña. Isabel Díaz Ayuso depende de la zona geográfica o si,
por el contrario, es independiente de ella. Para ello se encuestan,
por algún procedimiento aleatorio 2795 personas con derecho a voto en la comunidad
y se eliminan las respuestas ``NS/NC/me es indiferente''. Los resultados obtenidos fueron los siguientes:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"ayuso"}\NormalTok{)}
\NormalTok{tabla\_ayuso }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(ayuso)}
\NormalTok{tabla\_ayuso}
\CommentTok{\#\textgreater{}                opinion}
\CommentTok{\#\textgreater{} zona            n1\_nefasta n2\_mala n3\_buena n4\_excelente}
\CommentTok{\#\textgreater{}   n1\_mad\_muni           25     500       50         1000}
\CommentTok{\#\textgreater{}   n2\_metropol           10     280       50          460}
\CommentTok{\#\textgreater{}   n3\_extraradio          5     130       25          260}
\FunctionTok{chisq.test}\NormalTok{(tabla\_ayuso)}\SpecialCharTok{$}\NormalTok{expected}
\CommentTok{\#\textgreater{}                opinion}
\CommentTok{\#\textgreater{} zona            n1\_nefasta  n2\_mala n3\_buena n4\_excelente}
\CommentTok{\#\textgreater{}   n1\_mad\_muni    22.540250 512.7907 70.43828     969.2308}
\CommentTok{\#\textgreater{}   n2\_metropol    11.449016 260.4651 35.77818     492.3077}
\CommentTok{\#\textgreater{}   n3\_extraradio   6.010733 136.7442 18.78354     258.4615}
\FunctionTok{chisq.test}\NormalTok{(tabla\_ayuso, }\AttributeTok{correct =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Pearson\textquotesingle{}s Chi{-}squared test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  tabla\_ayuso}
\CommentTok{\#\textgreater{} X{-}squared = 19.486, df = 6, p{-}value = 0.003418}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DescTools)}
\FunctionTok{GTest}\NormalTok{(tabla\_ayuso, }\AttributeTok{correct =} \StringTok{"none"}\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Log likelihood ratio (G{-}test) test of independence without correction}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  tabla\_ayuso}
\CommentTok{\#\textgreater{} G = 19.357, X{-}squared df = 6, p{-}value = 0.003602}
\end{Highlighting}
\end{Shaded}

Como puede verse, sea cual sea el estadístico de contraste, la hipótesis de independencia se rechaza para valor de \(\alpha\) de los utilizados en la práctica (1\%, 2,5\%, 5\%, 10\%).

\hypertarget{contraste-aproximado-con-correcciuxf3n-de-continuidad-1}{%
\subsection{Contraste aproximado con corrección de continuidad}\label{contraste-aproximado-con-correcciuxf3n-de-continuidad-1}}

Afortunadamente, en la mayoría de las ocasiones, el tamaño muestral es grande y los totales marginales no están muy desequilibrados, con lo que los estadísticos chi-cuadrado y chi-cuadrado corregido de continuidad son prácticamente iguales, sobre todo si el número de niveles de ambos factores es elevado. En caso de utilizar una corrección de continuidad, hay unanimidad en utilizar la de Yates, sea cual sea el procedimiento de muestreo y el test (chi-cuadrado o G), si bien dicha unanimidad tiene mucho que ver con que es la única que está programada en el software convencional sobre tablas de contingencia.

\hypertarget{medidas}{%
\section{\texorpdfstring{Medidas de asociación en tablas \(2\times 2\)}{Medidas de asociación en tablas 2\textbackslash times 2}}\label{medidas}}

\hypertarget{introducciuxf3n-11}{%
\subsection{Introducción}\label{introducciuxf3n-11}}

Si no se rechaza la hipótesis de independencia, el análisis de la tabla se puede dar por finalizado. En caso contrario, el nuevo objetivo es determinar la dirección de la asociación detectada (o las fuentes de asociación en el caso \(R\times C\)) y su intensidad, y para ello se utilizan las denominadas medidas de asociación. Igual que en el contraste de independencia, se distinguirán los casos 2x2 y \(R\times C\), en esta ocasión no tanto por motivos académicos sino porque las situaciones son bien diferentes.

En el caso \(2\times 2\), los tipos de asociación derivados de la dirección de la misma (positiva y negativa) ya se se definieron en \ref{plantgen}. Por lo que se refiere a los límites de la intensidad de la misma, se dice que asociación es perfecta cuando al menos uno de los niveles de uno de los factores queda determinado por un nivel del otro factor. Puede ser estricta o implícita de tipo 2 \footnote{La asociación perfecta implícita de tipo 1, que no puede darse en tablas \(2\times 2\), consiste en que, dado un nivel de un factor, el nivel del otro queda inmediatamente determinado; pero dado el nivel de este último no se puede determinar el nivel del primero.}: - Estricta: Dado el nivel de un factor, el nivel del otro queda inmediatamente determinado. - Implícita de tipo 2: Dado un nivel de un factor, el nivel del otro queda inmediatamente determinado; dado el otro nivel, no queda determinado el nivel del otro factor.

\hypertarget{la-hatq-de-yule}{%
\subsection{\texorpdfstring{La \(\hat{Q}\) de Yule}{La \textbackslash hat\{Q\} de Yule}}\label{la-hatq-de-yule}}

En caso de independencia, las frecuencias observadas coinciden con las estimaciones de la esperadas. A medida que las primeras se separan de las segundas se produce un alejamiento de dicha hipótesis y los niveles de los factores aumentan la intensidad de su asociación. Por consiguiente, las diferencias \(\hat{D}_{ij}\) entre las frecuencias observadas y las estimaciones de las esperadas bajo el supuesto de independencia pueden ser la base de una magnífica medida de asociación. A mayores diferencias, mayor asociación. Mas sencillo todavía: Una única diferencia, por ejemplo la \(\hat{D}_{11}\), podría servir como medida de asociación porque, como bajo la hipótesis de independencia, \(\hat{D}_{ij}=0\) y \(\hat{D}_{11}=\hat{D}_{22}= -\hat{D}_{12}=-\hat{D}_{21}\), entonces se tiene que:

\begin{itemize}
\item
  En caso de independencia: \(\hat{D}_{11}=\hat{D}_{22}= 0\) y \(\hat{D}_{12}=\hat{D}_{21}=0\), o simplemente, \(\hat{D}_{11}=0\)
\item
  En caso de asociación positiva: \(\hat{D}_{11}=\hat{D}_{22} \geq 0\) y \(\hat{D}_{12}=\hat{D}_{21}\leq 0\), o simplemente, \(\hat{D}_{11}\geq 0\)
\item
  En caso de asociación negativa: \(\hat{D}_{11}=\hat{D}_{22} \leq 0\) y \(\hat{D}_{12}=\hat{D}_{21}\geq 0\), o simplemente, \(\hat{D}_{11}\leq 0\)
\end{itemize}

Por tanto, \(\hat{D}_{11}\) determina muy fácilmente la dirección de la asociación. Sin embargo, en cuanto a la intensidad de la misma, el campo de variación de \(\hat{D}_{11}\), \(\left[-\frac {n_{12} n_{21}} {n};\frac {n_{12} n_{21}} {n}\right]\) depende de los valores de las frecuencias observadas (esto es un problema a la hora de la interpretación) y la máxima intensidad asociativa se da cuando la diagonal descendente o la diagonal ascendente contienen ceros, es decir en caso de asociación perfecta estricta (negativa o positiva).

Para solucionar el problema anterior, se define la \(\hat{Q}\) de Yule como:
\[\hat{Q}=\frac {n \hat{D}_{11}} {n_{11}n_{22} - n_{12}n_{21}}=\frac{n_{11}n_{22} - n_{12}n_{21}} {n_{11}n_{22}+ n_{12}n_{21}}\] El campo de variación de \(\hat{Q}\) es \([-1;1]\) y:

\begin{itemize}
\tightlist
\item
  En caso de independencia, \(\hat{Q}=0\).
\item
  En caso de asociación positiva \(\hat{Q}< 0\).
\item
  En caso de asociación negativa \(\hat{Q}> 0\).
\end{itemize}

Lógicamente, a mayor valor absoluto de \(\hat{Q}\) mayor intensidad de la asociación.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DescTools)}
\FunctionTok{YuleQ}\NormalTok{(tabla\_jud)}
\CommentTok{\#\textgreater{} [1] {-}0.826087}
\end{Highlighting}
\end{Shaded}

A la luz del valor de \(\hat{Q}\) se concluye la existencia de una fuerte asociación negativa.

\hypertarget{otras-medidas-de-asociaciuxf3n-para-tablas-2times-2}{%
\subsection{\texorpdfstring{Otras medidas de asociación para tablas \(2\times 2\)}{Otras medidas de asociación para tablas 2\textbackslash times 2}}\label{otras-medidas-de-asociaciuxf3n-para-tablas-2times-2}}

\hypertarget{cuadrado-medio-de-la-contingencia-de-pearson}{%
\subsubsection{Cuadrado medio de la contingencia de Pearson}\label{cuadrado-medio-de-la-contingencia-de-pearson}}

La primera medida de asociación que se nos viene a todos a la cabeza es el propio estadístico de contraste \(\chi^2\). Sin embargo, no puede utilizarse como medida de asociación porque es siempre positivo y, sobre todo, porque su valor máximo, \(N(K-1)\), depende del tamaño muestral \(N\) y de \(K\), el número más pequeño de filas o columnas. En el caso \(2\times 2\) el depende únicamente de \(N\) porque \(K-1=1\). Para eliminar el efecto tamaño muestral, se define el cuadrado medio de la contingencia de Pearson como:
\[\hat{\phi^2}= \frac{\chi^2}{N}=\frac{\left(n_{11}n_{22} - n_{12}n_{21}\right)^2} {n_{1.} n_{2.} n_{\cdot1}n_{\cdot2}}\]
Su campo de variación es \([0;1]\), tomando el valor 0 en caso de independencia y 1 cuando hay asociación perfecta y estricta. Cuanto mayor sea el valor del coeficiente, mayor es intensidad de la asociación.

No proporciona la dirección de la asociación, si bien se puede saber por el signo de \(n_{11}n_{22}-n_{12}n_{21}\). Otra consideración importante es que, si se codifican los niveles de los factores como (0;1), \(\hat{\phi}^2\) coincide con el coeficiente de determinación lineal entre los factores. Por tanto,la asociación que mide es ``lineal'' (de ahí que su valor suela ser más bajo que el de \(\hat{Q}\)). Su raíz cuadrada es conocida como ``la V de Cramer''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\FunctionTok{Phi}\NormalTok{(tabla\_jud))}\SpecialCharTok{\^{}}\DecValTok{2}
\CommentTok{\#\textgreater{} [1] 0.1420701}
\end{Highlighting}
\end{Shaded}

\hypertarget{odds-ratio-o-cociente-de-posibilidadestablas_conting-6}{%
\subsubsection[Odds ratio o cociente de posibilidades]{\texorpdfstring{Odds ratio o cociente de posibilidades\footnote{Para no confundirlo con el riesgo relativo.}}{Odds ratio o cociente de posibilidades}}\label{odds-ratio-o-cociente-de-posibilidadestablas_conting-6}}

Se define como \(\alpha=\frac {P_{11}/P_{12}} {P_{21}/P_{22}}\) y se estima como \(\hat{\alpha}=\frac {n_{11}/n_{12}} {n_{21}/n_{22}}=\frac {n_{11}n_{22}} {n_{12}n_{21}}\)

Su campo de variación es \([0;\infty)\), asimétrico, y por consiguiente difícil de interpretar. En todo caso:

\begin{itemize}
\tightlist
\item
  Si \(\alpha < 1\), la probabilidad (aquí denominada posibilidad) de pertenecer al nivel 1 del factor B es mayor en el nivel 1 del factor A que en 2.
\item
  Si \(\alpha = 1\), la probabilidad de pertenecer al nivel 1 del factor B es la misma en ambos niveles del factor A.
\item
  Si \(\alpha > 1\), la probabilidad pertenecer de al nivel 1 del factor B es menor en el nivel 1 del factor A que en el 2.
\end{itemize}

Una posible solución a la dificultad de interpretación es definir \(ln\hat {\alpha}\), que es una medida simétrica en \((-\infty;+\infty)\). Sin embargo su interpretación, dada la gran amplitud del campo de variación, continúa siendo muy difusa.

Una ventaja que tiene respecto a \(\hat{\alpha}\) es que no cambia si las filas se convierten en columnas y las columnas en filas.\footnote{Es decir, no es necesario identificar la variable respuesta para utilizar esta medida.} Por ello, la razón de posibilidades se puede utilizar no solo en estudios retrospectivos, sino también en aquéllos prospectivos y transversales. Finalmente, nótese que (i)la razón de posibilidades y el riesgo relativo (\(P_1/P_2\)) se relacionan como sigue: \(\alpha= \frac {P_1 (1-P_2)} {P_2 (1-P_1)}\) y que (ii) ambos son similares cuando la probabilidad de éxito \(P_i\) está cerca de cero en ambos grupos.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(epiR)}
\FunctionTok{epi.2by2}\NormalTok{(tabla\_jud, }\AttributeTok{method =} \StringTok{"cohort.count"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{massoc.summary[}\DecValTok{2}\NormalTok{, ]}
\CommentTok{\#\textgreater{}          var       est      lower     upper}
\CommentTok{\#\textgreater{} 2 Odds ratio 0.0952381 0.01021365 0.8880565}
\end{Highlighting}
\end{Shaded}

\hypertarget{medidas-de-asociaciuxf3n-en-tablas-rtimes-c}{%
\section{\texorpdfstring{Medidas de asociación en tablas \(R\times C\)}{Medidas de asociación en tablas R\textbackslash times C}}\label{medidas-de-asociaciuxf3n-en-tablas-rtimes-c}}

\hypertarget{introducciuxf3n-12}{%
\subsection{Introducción}\label{introducciuxf3n-12}}

En caso de rechazo de la hipótesis de independencia en una tabla \(R\times C\), se concluye que al menos un nivel de uno de los factores está asociado con uno del otro factor. En ese caso se utilizarán las medidas de asociación para determinar la intensidad de la misma. Las asociaciones de determinados niveles del factor A con determinados niveles del factor B que llevan al rechazo de la independencia de ambos se denominan ``fuentes de asociación'', y se determinan mediante los residuos estandarizados ajustados.

\hypertarget{medidas-derivadas-del-estaduxedstico-chi-cuadrado}{%
\subsection{Medidas derivadas del estadístico Chi-cuadrado}\label{medidas-derivadas-del-estaduxedstico-chi-cuadrado}}

El estadístico \(\chi^2\) no puede utilizarse como medida de asociación porque su valor máximo, \(N(K-1)\), siendo \(N\) el tamaño muestral y \(K\) el número más pequeño de filas o columnas, depende tanto de \(N\) como del número de niveles de los factores.

El cuadrado medio de la contingencia, \(\phi^2\), elimina el efecto ``tamaño muestral'', pero no el efecto ``número de niveles de los factores''. Igual le ocurre al coeficiente de contingencia; y a la T de Tschuprow, salvo en las tablas cuadradas. La única medida derivada del estadístico \(\chi^2\) que corrige ambos efectos es la V de Cramer: \(V=\sqrt\frac{\chi^2}{KN}\), con \(K=min\left(R-1; C-1\right)\). Su campo de variación es \([0,1]\) y alcanza su máximo en caso de asociación perfecta. En tablas cuadradas \(V=T\).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{CramerV}\NormalTok{(tabla\_ayuso)}
\CommentTok{\#\textgreater{} [1] 0.0590406}
\end{Highlighting}
\end{Shaded}

Aunque se rechaza la hipótesis de independencia, la asociación existente entre la opinión sobre la presidente y la zona geográfica es muy pequeña. Y ello, porque, sea cual sea la zona geográfica, aunque hay ligerísimas variaciones la opinión es muy buena: para alrededor del 60\% es excelente, para la tercera parte muy buena y tan solo para el 5\% es mala (alrededor del 4\%) o muy mala (apenas el 1\%).

\hypertarget{medidas-basadas-en-la-reducciuxf3n-proporcional-del-error-lambda-de-goodman-y-kruskal}{%
\subsection{\texorpdfstring{Medidas basadas en la reducción proporcional del error: \(\lambda\) de Goodman y Kruskal}{Medidas basadas en la reducción proporcional del error: \textbackslash lambda de Goodman y Kruskal}}\label{medidas-basadas-en-la-reducciuxf3n-proporcional-del-error-lambda-de-goodman-y-kruskal}}

Al contrario que las medidas basadas en el estadístico Chi-cuadrado, exigen determinar cuál es el factor explicativo y cuál el factor a explicar. Sea \emph{A} el factor explicativo y \emph{B} el factor a explicar: Supóngase que se selecciona aleatoriamente uno de los elementos de la tabla. Este elemento pertenecerá a un nivel ``\emph{i}'' de \emph{A} y a un nivel ``\emph{j}'' de \emph{B}. Supóngase que se quiere predecir el nivel de \emph{B} al que pertenece, \((i)\) sin utilizar el hecho de saber a que nivel de \emph{A} pertenece, y \((ii)\) utilizando dicho hecho. Lógicamente, tanto en el caso \((i)\) como en el \((ii)\) se comete un error \((P(i)\) y \(P(ii)\), respectivamente). La probabilidad de error será la misma si los factores son independientes. Sin embargo, si están asociados, el conocimiento del nivel de \emph{A} al que pertenece el elemento seleccionado ayudará en la predicción del nivel de \emph{B} al que pertenece (tanto más cuanto más asociados estén los factores), y la probabilidad de error disminuirá respecto al caso \((i)\). La reducción proporcional que se opera es: \(\lambda=\frac{P(i)-P(ii)} {P(i)}\), con \(P(i)= N- \max_{j} n_{\cdot j}\) y \(P(ii)= N-\sum_{j=1}^C \max_{j} n_{\cdot j}\).
En el caso en que A sea el factor a explicar: \(\lambda=\frac{P(i)-P(ii)} {P(i)}\), con \(P(i)= N- \max_{i} n_{i.}\) y \(P(ii)= N-\sum_{i=1}^R \max_{i} n_{i.}\)
En caso de no tener claro cual es el factor a explicar, se utiliza la media agregativa de las dos medidas anteriores \(\lambda=\frac{\sum_{j=1}^C \max_i n_{i.}-max_i n_{i.}+\sum_{i=1}^R \max_j n_{\cdot j}-\max_j n_{\cdot j} }{2N-\max_i n_{i.}\max_j n_{\cdot j}}\).
Su campo de variación es \([0,1]\).En caso de independencia \(\lambda=0\). Ahora bien, que \(\lambda=0\) no implica necesariamente que \emph{A} y \emph{B} tengan que ser independientes, puesto que también \(\lambda\) vale 0 cuando en uno de los niveles del factor a explicar las frecuencias son superiores a las de los demás niveles, y ello para todos los niveles del factor explicativo, aunque los factores no sean independientes. En caso de asociación, \(0< \lambda \leq 1\), alcanzándose la unidad en caso de asociación perfecta.

Una limitación de \(\lambda\) (además de la anterior) es que exige determinar el factor explicativo y el factor a explicar. Otra es su sensibilidad a totales marginales desequilibrados; en este caso toma valores anormalmente bajos.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{Lambda}\NormalTok{(tabla\_ayuso, }\AttributeTok{direction =} \StringTok{"row"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 0}
\end{Highlighting}
\end{Shaded}

Nuestro ejemplo práctico es un caso donde \(\lambda=0\), con factor a explicar la opinión, y los factores no son independientes. Y es que, sea cual sea la zona, las frecuencias de la categoría de opinión ``excelente'' son siempre las más elevadas.

\hypertarget{determinaciuxf3n-de-las-fuentes-de-asociaciuxf3n}{%
\subsection{Determinación de las fuentes de asociación}\label{determinaciuxf3n-de-las-fuentes-de-asociaciuxf3n}}

En el caso de Tablas RxC el estadístico, el rechazo la hipótesis de independencia, no indica que cada nivel de uno de los factores esté asociado con uno de los niveles del otro factor, como en tablas \(2\times 2\). Lo que indica es que al menos uno de los niveles de uno de los factores está asociado con un nivel del otro. Por tanto, puede ser, y así es normalmente, que dicho rechazo se deba a que algunos niveles de uno de los factores (incluso solo uno) están asociados con alguno de los del otro factor. Ya no hay dirección de la asociación. Hay fuentes de asociación.

Para identificar las fuentes de asociación lo lógico es fijarse en cada celda en las diferencias entre la frecuencia observada y la esperada bajo el supuesto de independencia (los también denominados residuos). Pero su interpretación depende del tamaño de la frecuencia esperada, y por ello se estandarizan, es decir, se ponen en relación a la raíz cuadrada de las correspondientes frecuencias esperadas.

Como se necesita una distribución de probabilidad para dichos residuos estandarizados, bajo la hipótesis de independencia, para decidir si son significativamente grandes (asociación) o no (independencia),se dividen por su desviación típica, y así tienen aproximadamente una distribución N(0;1). Estos nuevos residuos se denominan residuos estandarizados ajustados (o de Haberman), y son los que se utilizan para identificar las fuentes de asociación. Por tanto: \(\hat{R}_{ij}=n_{ij}-\hat{E}_{ij}\); \(\hat{R}_{ij}(est)=\frac {n_{ij}-\hat{E}_{ij}}{\sqrt{\hat{E}_{ij}}}\); \(\hat{R}_{ij}(est;ajd)=\frac{\hat{R}_{ij}(est)}{\sqrt{\left(1-\frac {n_{i.}} {N}\right)\left(1-\frac {n_{\cdot j}} {N}\right)}}\)

Habrá una fuente de asociación en cada celda \{\emph{i};\emph{j}\} que verifique: \(|\hat{R}_{ij}(est;ajd)|\geq{k}\), con \(k= 2,33; 1,96; 1,64\) para \(\alpha=0,01; 0,05; 0,10\), respectivamente.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(questionr)}
\FunctionTok{chisq.residuals}\NormalTok{(tabla\_ayuso, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{, }\AttributeTok{std =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{}                opinion}
\CommentTok{\#\textgreater{} zona            n1\_nefasta n2\_mala n3\_buena n4\_excelente}
\CommentTok{\#\textgreater{}   n1\_mad\_muni         0.79   {-}1.04    {-}3.77         2.41}
\CommentTok{\#\textgreater{}   n2\_metropol        {-}0.51    1.74     2.88        {-}2.78}
\CommentTok{\#\textgreater{}   n3\_extraradio      {-}0.45   {-}0.76     1.59         0.17}
\end{Highlighting}
\end{Shaded}

En el ejemplo que nos ocupa, asumiendo \(\alpha=0,05\), las fuentes de asociación son ``Madrid municipio-excelente'' a costa de ``buena'', y ``Madrid metropolitano-buena'' a costa de ``excelente''.

\hypertarget{contrastes-de-independencia-en-tablas-multidimensionales}{%
\section{Contrastes de independencia en tablas multidimensionales}\label{contrastes-de-independencia-en-tablas-multidimensionales}}

En tablas con más de dos factores (nos centramos en el caso \(R \times C \times M\), por simplicidad), no solo se puede contrastar la hipótesis de independencia global sino que, en caso de ser rechazada, también se pueden contrastar las hipótesis de \((i)\) independencia parcial: Dos factores están asociados y el tercero es independiente de ellos, y \((ii)\) condicional: Dos de los factores son independientes para cada nivel del tercero pero están asociados con él.

En los tres casos, el estadístico de contraste (contraste aproximado) es \(\chi^2=\sum_{i=1}^R \sum_{j=1}^C \sum_{m=1}^M \frac{\left (n_{ijm}-\hat{E}_{ijm}\right)^2}{\hat{E}_{ijm})}\), con los siguientes grados de libertad (g.l.) y \(\hat{E}_{ijm}\) bajo la correspondiente \(H_0\):

\textbf{Independencia global}:

\(dl=(R\times C\times M)-(R-1)-(C-1)-(M-1)-1\) y
\(\hat{E}_{ijm}=\frac{n_{i..}n_{.j.}n_{..m}}{N^2}\)

\textbf{Independencia parcial}:

A y B asociados entre sí pero independientes de C

\begin{itemize}
\tightlist
\item
  \(g.l.=(R\times C\times M)-(R\times C-1)-(M-1)-1\) y \(\hat{E}_{ijm}=\frac{n_{ij.}n_{..m}}{N}\)
\end{itemize}

A y C asociados entre sí pero independientes de B

\begin{itemize}
\tightlist
\item
  \(g.l.=(R\times C\times M)-(R\times M-1)-(C-1)-1\) y \(\hat{E}_{ijm}=\frac{n_{i.m}n_{.j.}}{N}\)
\end{itemize}

B y C asociados entre sí pero independientes de A

\begin{itemize}
\tightlist
\item
  \(g.l.=(R\times C\times M)-(C\times M-1)-(R-1)-1\) y \(\hat{E}_{ijm}=\frac{n_{.jm}n_{i..}}{N}\)
\end{itemize}

\textbf{Independencia condicional}

A y B son independientes entre sí, pero están asociados con C

\begin{itemize}
\tightlist
\item
  \(g.l.=(R\times C\times M)-(R\times M-1)-(C\times M-1)-1\) y \(\hat{E}_{ijm}=\frac{n_{i.m}n_{.jm}}{N}\)
\end{itemize}

A y C son independientes entre sí, pero están asociados con B

\begin{itemize}
\tightlist
\item
  \(g.l.=(R\times C\times M)-(R\times C-1)-(M\times C-1)-1\) y
  \(\hat{E}_{ijm}=\frac{n_{ij.}n_{.jm}}{N}\)
\end{itemize}

B y C son independientes entre sí, pero están asociados con A

\begin{itemize}
\tightlist
\item
  \(g.l.=(R\times C\times M)-(C\times R-1)-(M\times R-1)-1\) y
  \(\hat{E}_{ij.}=\frac{n_{i.m}n_{.jm}}{N}\)
\end{itemize}

También son interesantes las relaciones de segundo orden o superior (por ejemplo, si la asociación entre dos de los factores difiere en dirección y/o intensidad para distintos niveles del tercero), pero se estudian mediante modelos logarítmico-lineales.

\begin{infobox_resume}
\textbf{Resumen}

Las tablas de contingencia analizan la relación entre variables categóricas. Su análisis responde preguntas como: Los factores involucrados en la tabla, ¿son independientes o están asociados? Si están asociados, ¿qué niveles de dichos factores son los que están asociados?, ¿cuál es la intensidad de dicha asociación? Se aborda ampliamente el caso de tablas bifactoriales y se proponen test exactos y aproximados para el contraste de la hipótesis de independencia (para tres procedimientos de muestreo diferentes), y se proporciona una selección de medidas de asociación. Finalmente, se hace una breve incursión en el ámbito de las tablas multidimensionales.

\end{infobox_resume}

\hypertarget{part-machine-learning-supervisado}{%
\part{Machine learning supervisado}\label{part-machine-learning-supervisado}}

\hypertarget{uxe1rboles-de-clasificaciuxf3n-y-regresiuxf3n}{%
\chapter{\texorpdfstring{Árboles de clasificación y regresión \label{cap_arboles}}{Árboles de clasificación y regresión }}\label{uxe1rboles-de-clasificaciuxf3n-y-regresiuxf3n}}

\emph{Ramón A. Carrasco e Itzcóatl Bueno\footnote{Universidad Complutense de Madrid}}

\nocite{theobald2017machine,burkov2019hundred,boehmke2019hands}

\hypertarget{introducciuxf3n-13}{%
\section{\texorpdfstring{Introducción \label{intro_dectree}}{Introducción }}\label{introducciuxf3n-13}}

Los árboles de decisión, donde decisión es el término genérico para clasificación y regresión, son modelos que se utilizan principalmente para la resolución de problemas de clasficación, aunque también son aplicables a la predicción de valores numéricos, esto es, como modelos de regresión. De ahi, que sean conocidos como árboles de clasificación y regresión (CART). Los árboles de decisión predicen categorías y variables cuantitativas utilizando variables de entrada númericas y categóricas. Algunos ejemplos de arboles de decisión para clasificación y regresión son: \index{árboles de decisión}

\begin{itemize}
\tightlist
\item
  \textbf{Clasificación}:\index{árboles de clásificación} Tomar la decisión de que empleados promocionar en base a sus méritos, capacidades, edad, etc. O por ejemplo, organizar un partido de ténis en base a la climatología prevista. Este ejemplo se muestra gráficamente en la figura \ref{fig:dectree-plot}. En este ejemplo, en base a los registros climatológicos de los partidos que ya se hayan jugado el algoritmo podrá tomar decisiones. Así, si un nuevo día se quiere jugar al tenis se deberán aportar los datos de previsión, fuerza del viento y fuerza de la humedad. En caso de ser un día nublado, el algoritmo nos sugerirá que juguemos. En caso de ser soleado, comprobará el nivel de humedad y en caso de ser debíl recomendará que se juegue el partido. Lo mismo pasará si la previsión es de lluvia pero comprobando la fuerza del viento.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/dectree_example} 

}

\caption{Ejemplo de árbol de decisión.}\label{fig:dectree-plot}
\end{figure}

\begin{itemize}
\tightlist
\item
  \textbf{Regresión}:\index{árboles de regresión} Siguiendo con el ejemplo del partido de tenis, también se puede aplicar un arbol de decisión para determinar cuantas horas jugar de acuerdo a la climatología. En este ejemplo, en la figura \ref{fig:dectree-plot} se sustituirían la predicción dicotómica SI/NO por un valor númerico. Por ejemplo, el algoritmo puede sugerir jugar 5 horas al tenis si el día esta soleado pero tiene una humedad débil, y 3.5 horas si está soleado pero la humedad es fuerte. También puede decidir que si la previsión es nublada se jueguen 4 horas.O en caso de lluvia podría decidir que el partido con viento fuerte no dure más de 0.75 horas y en caso de viento débil poder jugar 1.15 horas.
\end{itemize}

\index{algoritmos de árbol}
CART es un término genérico para describir algoritmos de árbol y también un nombre específico para el algoritmo original de \cite{breiman1984classification} para construir árboles de clasificación y regresión. Sin embargo, existen Sin embargo, existen otros como el ID3, que se presenta en secciones posteriores, o el C4.5, que esta basado en ID3. En la tabla \ref{comparativa-treealg} se muestra una pequeña comparativa de estos tres algoritmos:

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2162}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1892}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2973}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2973}}@{}}
\caption{Características de los principales algoritmos de árboles de decisión.}\tabularnewline
\toprule()
\begin{minipage}[b]{\linewidth}\raggedleft
Algoritmo
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Criterio de división
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Tipo de variables
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Estrategia de poda
\end{minipage} \\
\midrule()
\endfirsthead
\toprule()
\begin{minipage}[b]{\linewidth}\raggedleft
Algoritmo
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Criterio de división
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Tipo de variables
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Estrategia de poda
\end{minipage} \\
\midrule()
\endhead
ID3 & Ganancia de información & Solo categóricas & No poda \\
CART & Indice de Gini & Categóricas y numéricas & Poda basada en el coste de complejidad \\
C4.5 & Ratio de ganancia & Categóricas y numéricas & Poda basada en el error \\
\bottomrule()
\end{longtable}

En resumen, los arboles de decisión tienen multiples ventajas. Entre las que destacan:

\begin{itemize}
  \item Es facil de entender e interpretar. Su visualización clara permite interpretar la salida del modelo, y entender su proceso como un conjunto de condicionantes.
  \item El mismo algoritmo es valido tanto para problemas de clasificación como de regresión, y se pueden incluir tanto variables continuas como categóricas. Además, el algoritmo de árboles de decisión no requiere la estandarización de variables puesto que se basa en reglas y no en el cálculo de distancias entre observaciones. 
  \item Trata de forma más adecuada la falta de linealidad entre las variables independientes que otros algoritmos. 
  \item Respecto a los datos, el algoritmo de árbol de decisión hace un tratamiento automático de valores ausentes y no se resiente con las observaciones atípicas. 
\end{itemize}

Sin embargo, el algoritmo tambien tiene ciertas desventajas:

\begin{itemize}
  \item El árbol de decisión es inestable, esto es, que incluir una nueva observación de entrenamiento obliga a reconstruirlo pudiendo modificar todos los nodos en el proceso de cálculo. 
  \item El árbol de decisión no es adecuado cuando el conjunto de datos es muy grande puesto que el modelo entrenado puede ser excesivamente complejo y provocar sobreajuste. El sobreajuste es el principal problema de los árboles de decisión, profundiza mucho en el modelo hasta convertirlo en demasiado complejo como para generalizar; lo que lleva a ajustar muy bien los datos observados y cometer muchos errores en la predicción de nuevas observaciones. 
  \item El sobreajuste produce una gran varianza en las predicciones llevando de nuevo a muchos fallos en las predicciones y reduciendo su rendimiento.
\end{itemize}

\hypertarget{aprendizaje-con-uxe1rboles-de-decisiuxf3n}{%
\section{Aprendizaje con árboles de decisión}\label{aprendizaje-con-uxe1rboles-de-decisiuxf3n}}

\index{árboles de decisión}
Formalmente, un árbol de decisión es un gráfico acíclico que se inicia en un \textbf{nodo raiz}\index{nodo raiz}, el cual se divide en \textbf{ramas}\index{rama}, también conocidas como \textbf{aristas}\index{arista}. Después, estas ramas se unen a las \textbf{hojas}\index{hoja}, también denominadas \textbf{nodos}\index{nodo}, las cuales determinan \textbf{puntos de decisión}\index{punto de decisión}. En el ejemplo de la figura \ref{fig:dectree-plot} el nodo raiz es la caja \emph{Previsión}. Las ramas o aristas, son los tres posibles valores que puede tomar la previsión: \emph{Soleado}, \emph{Nublado} o \emph{Lluvia}. Cada una de estas ramas conecta con una nueva hoja o nodo, a \emph{Humedad} o \emph{Viento} en los casos de soleado o lluvia. Sin embargo, en ese ejemplo, el \emph{Nublado} representa un nodo terminal, puesto que llegado a ese punto la salida que proporcionaría el árbol es \emph{``No jugar al tenis''}. Este proceso se repite utilizando el conjunto de datos disponible en cada hoja. Así, se genera una clasificación final cuando una hoja ya no produce ramas nuevas, y es lo que se conoce como un nodo terminal. El objetivo al construir un árbol, es mantenerlo lo más pequeño posible. Esto se consigue eligiendo una variable que divida de forma óptima los datos en conjuntos homogeneos, de tal forma que se prediga mejor la clase objetivo.

\hypertarget{cuxf3mo-se-va-dividiendo-el-uxe1rbol}{%
\section{\texorpdfstring{¿Cómo se va dividiendo el árbol? \index{partición}}{¿Cómo se va dividiendo el árbol? }}\label{cuxf3mo-se-va-dividiendo-el-uxe1rbol}}

Como ya se ha mencionado, un árbol de decisión se va diviendo en dos nuevas ramas de forma recursiva, es decir, cada división esta condicionada a las anteriores. El objetivo en cada hoja es encontrar la variable más adecuada para dividir los datos de ese nodo en dos nuevas hojas de tal forma que el error global entre la clase observada y la predicha por el arbol se minimice. En problemas de regresión, la función a minimizar es la suma residual de cuadrados (SSR, por las siglas en inglés de: \emph{sum of squares due to regression}) total. Para problemas de clasificación, la partición generalmente se realiza para maximizar la reducción en la entropía.

El algoritmo CART utiliza la impureza de Gini para generar las particiones, mientras que los algoritmo ID3 y C4.5 utilizan la entropía o la ganancia de información, la cual esta relacionada con la entropía. Y como ya se ha mencionado, en regresión la función a minimizar es la suma residual de cuadrados.

\hypertarget{impureza-de-gini}{%
\subsection{\texorpdfstring{Impureza de Gini\index{impureza de Gini}}{Impureza de Gini}}\label{impureza-de-gini}}

La Impureza de Gini, utilizada por el algoritmo CART, es una medida de la frecuencia con la que una observación elegida aleatoriamente de los datos se asignaría a la clase erronea si se etiquetase al azar de acuerdo con la distribución de las clases en el conjunto de datos. Entonces, sea \(X\) un conjunto de datos con \(\kappa\) clases, y sea \(p_i\) la probabilidad de que una observación pertenezca a la clase \(i\), la Impureza de Gini para \(X\) se puede definir como:

\begin{equation}
Gini(X) = 1 - \sum^{\kappa}_{i=1}{p^{2}_{i}}
\end{equation}

En el ejemplo de la figura \ref{fig:dectree-plot} podría pasar que sucediese la siguiente situación:

\begin{longtable}[]{@{}lccc@{}}
\caption{Ejemplo impureza de Gini}\tabularnewline
\toprule()
& \(p_{SÍ}\) & \(p_{NO}\) & Impureza de Gini \\
\midrule()
\endfirsthead
\toprule()
& \(p_{SÍ}\) & \(p_{NO}\) & Impureza de Gini \\
\midrule()
\endhead
Soleado & 0,75 & 0,25 & 0,3750 \\
Nublado & 1,00 & 0,00 & 0,0000 \\
Lluvia & 0,50 & 0,50 & 0,5000 \\
\bottomrule()
\end{longtable}

Se puede ver que la impureza es mayor en el nodo en el que la distribución de las observaciones es uniforme entre clases, mientras que el menor valor se obtiene en el nodo en el que todas las observaciones pertenecen a una clase. Se selecciona un atributo con la menor impureza de Gini para dividir el nodo. Si un conjunto de datos \(X\) se divide en un atributo \(\varphi\) en dos subconjuntos \(X_1\) y \(X_2\) con tamaños \(n_1\) y \(n_2\), respectivamente, la impureza de Gini se puede definir como:

\begin{equation}
Gini_{\varphi}(X) = \frac{n_1}{n}{Gini(X_{1})} + \frac{n_2}{n}{Gini(X_{2})}
\end{equation}

Al entrenar un árbol de decisión, se elige para dividir el nodo la variable que proporciona el menor \(Gini_{\varphi}(X)\). Para conocer la ganancia de información para una variable, las impurezas ponderadas de las ramas se restan de la impureza original. La ganancia de Gini se calcula tal que:

\begin{equation}
\Delta Gini(\varphi) =  Gini(X) - Gini_{\varphi}(X)
\end{equation}

Siguiendo el ejemplo del árbol de decisión para saber si se puede jugar al tenis o no se tendría que obtener la impureza de gini para el nodo \emph{niebla} o el nodo \emph{viento}. En caso, de que para el nodo \emph{niebla} se tuviese \(P(SÍ)=0,75\) y \(P(NO) = 0,25\), siendo su impureza igual a 0,375; y para el nodo \emph{viento} \(P(SÍ)= 0.9\) y \(P(NO)=0,1\) teniendo un valor de 0,18 para su impureza de Gini. Entonces, la ganancia de Gini para cada nodo será:

\begin{center}
$\Delta Gini(Niebla) = 0.375-0.375=0$\\
$\Delta Gini(Viento) =0.5-0.18=0.32$
\end{center}

Y por tanto, el árbol tomaría como nodo a dividir el que indica que decisión tomar en caso de que la humedad sea débil o fuerte.

\hypertarget{entropuxeda}{%
\subsection{\texorpdfstring{Entropía \index{entropía}}{Entropía }}\label{entropuxeda}}

La entropia es un concepto matemático que explica la varianza en los datos entre diferentes clases. Busca que los datos sean más homogeneos en cada capa respecto de la anterior partición. A partir de la ecuación \ref{eq:entropy} se calcula, expresada en bits entre 0 y 1, la entropía para cada división de variable potencial:

\begin{equation}\label{eq:entropy}
E = -p_1\log_2 (p_1) - p_2\log_2 (p_2)
\end{equation}

donde \(p_1\) y \(p_2\) representan la probabilidad de pertenecer a cada una de las clases en ese nodo. En teoría de la información, la base logarítmica varía dependiendo de la aplicación y con ella varía la unidad de medida. En el caso de la entropía, se utiliza el logaritmo base 2 que da la unidad de bits o shannons. En cambio, el logaritmo neperiano (con base \(e\)) medirá en unidades naturales, y el logaritmo en base 10 utiliza como unidades los dits, bans o hartleys.

\hypertarget{ganancia-de-informaciuxf3n}{%
\subsection{Ganancia de información}\label{ganancia-de-informaciuxf3n}}

Se puede definir la ganancia de información como cuanta información proporciona una variable sobre una clase. Obtener esta información ayuda a ordenar los atributos en los nodos del árbol de decisión. La ganancia de información se obtiene tal que:

\begin{equation}
IG = E_{\varkappa} - E_{\varkappa + 1}
\end{equation}

donde \(E_\varkappa\) representa la entropía en el nodo padre, mientras que \(E_{\varkappa+1}\) es la entropia en el nodo que resulta de dividir el nodo padre. Cuanto más entropía se elimine, mayor será la ganancia de información, y por tanto, cuanto mayor sea la ganancia de información, mejor será la división.

\hypertarget{suma-residual-de-cuadrados-muxednima}{%
\subsection{Suma residual de cuadrados mínima}\label{suma-residual-de-cuadrados-muxednima}}

En el caso de un problema de regresión, el árbol elige la variable con la que seguir diviendo nodos aquella que reduce la suma de residuos al cuadrado. Esto es, que al dividir los datos utilizando la variable \emph{X} en dos regiones \(R_1\) y \(R_2\) el error global entre la variable respuesta observada \emph{Y} y el valor constante predicho por el árbol \(\kappa_i\) se minimice. Entonces, la función a minimizar es:

\begin{equation}
SSR = \sum_{i\in R_1}{(y_i-\kappa_1)^2} + \sum_{i\in R_2}{(y_i-\kappa_2)^2}
\end{equation}

\hypertarget{sobreajuste}{%
\section{\texorpdfstring{Sobreajuste \index{sobreajuste}}{Sobreajuste }}\label{sobreajuste}}

Ya se ha comentado en la sección \ref{intro_dectree} que una de las principales desventajas de los árboles de decisión es su propensión a sobreajustar el modelo al conjunto de datos de entrenamiento, y por tanto, hay que prestar especial atención a la complejidad del modelo. Basandose en los datos de entrenamiento, un arbol de decisión puede extraer los patrones presentes en éste y ser muy preciso en el ajuste de estos datos. Sin embargo, puede ocurrir que el árbol resultante no sea capaz de clasificar correctamente ni el conjunto de validación ni nuevas observaciones. Esto puede darse porque haya patrones no observados en los datos de entrenamiento que el modelo no es capaz de detectar o porque la división de los datos entre entrenamiento y validación no se realizo correctamente y no era representativa del conjunto completo. La forma de evitar el sobreajuste es controlar el crecimiento del árbol para evitar que se vuelva muy complejo.

\hypertarget{cuuxe1nto-debe-crecer-un-uxe1rbol}{%
\section{\texorpdfstring{¿Cuánto debe crecer un árbol? \index{profundidad del árbol}}{¿Cuánto debe crecer un árbol? }}\label{cuuxe1nto-debe-crecer-un-uxe1rbol}}

Una vez se determina la variable optima para realizar la división de los datos, se generan dos nuevas hojas y como se ha mencionado el proceso se repite recursivamente. Entonces, ¿cuándo se detiene? se debe determinar un criterio de parada adecuado. Por ejemplo, se puede determinar una profundidad máxima de árbol para que no se vuelva excesivamente complejo. Sin embargo, esto genera una nueva pregunta ¿qué profundidad máxima determinar? Si el árbol tiene mucha profundidad se hará muy complejo \index{complejidad} y sobreajustará \index{sobreajuste} los datos de entrenamiento resultando en un ajuste deficiente a nuevas observaciones.

En consecuencia, se debe llegar a un equilibrio entre la profundidad y complejidad del árbol para optimizar la predicción de futuras observaciones. Este equilibrio se puede lograr siguiendo alguno de los siguientes enfoques: la parada temprana o la poda.

\hypertarget{la-parada-temprana}{%
\subsection{\texorpdfstring{La parada temprana \index{parada temprana}}{La parada temprana }}\label{la-parada-temprana}}

La parada temprana restringe el crecimiento del árbol de forma explicita. Existen distintas maneras de establecer esta restricción al árbol, pero dos de las técnicas más populares son las de restringir la profundidad a un cierto nivel o la de establecer un número mínimo de observaciones permitidas en un nodo terminal. En el primer caso, el árbol deja de dividirse al llegar a cierta profundidad. Así, cuanto menos profundo sea el árbol, menos variación habrá en las predicciones que proporcione. Sin embargo, existe el riesgo de introducir mucho sesgo al modelo al no ser capaz de captar interacciones y patrones complejos en los datos. El segundo enfoque lo que provoca es que no se dividan nodos intermedios con pocas observaciones. En el un extremo, si se permite que un nodo terminal solo contuviese una observación esta actuaría como predicción. De este modo, los resultados no serían generalizables y tendrían mucha variabilidad. En el otro extremo, si se exigen un gran número de observaciones en el nodo terminal se reduce el número de divisiones y en consecuencia se reduce la varianza.

\hypertarget{la-poda}{%
\subsection{\texorpdfstring{La poda \index{poda}}{La poda }}\label{la-poda}}

El otro enfoque es el de la poda. Éste consiste en dejar crecer un árbol muy profundo y complejo y despues podarlo para encontrar un sub-árbol óptimo. El sub-árbol optimo se encuentra utilizando un parametro de complejidad del coste \((\zeta)\) que penaliza la función objetivo de la partición por el número de nodos terminales del árbol \((\tau)\) tal que:

\begin{equation}
\min\{SSR + \zeta |\tau |\}
\end{equation}

Para un valor dado de \(\zeta\) se encuentra el árbol podado más pequeño que tenga el error penalizado más bajo. Las penalizaciones más bajas tienden a producir modelos más complejos, y en consecuencia árboles más grandes. Por otro lado, las mayores penalizaciones dan como resultado árboles pequeños. En conclusión, a medida que un árbol crece, el SSR debe tener una reducción mayor que la penalización por la complejidad en los costes.

\hypertarget{el-algoritmo-id3-para-la-construcciuxf3n-de-un-uxe1rbol-de-decisiuxf3n}{%
\section{El algoritmo ID3 para la construcción de un árbol de decisión}\label{el-algoritmo-id3-para-la-construcciuxf3n-de-un-uxe1rbol-de-decisiuxf3n}}

\index{algoritmos de árbol}
\index{árboles de decisión}

Dado un conjunto de observaciones pertenecientes a dos clases \{0,1\}, se quiere construir un árbol de decisión que permita predecir a que clase pertenece una observación dado un vector de características. Existen múltiples formulaciones del algoritmo de árboles de decisión. En este libro se presenta el \emph{ID3}. En este caso, el criterio de optimización es la verosimilitud logarítmica media:

\begin{equation}
\frac{1}{N}\sum_{i=1}^{N} y_{i}\ln f_{ID3}(x_i) + (1-y_{i})\ln(1-f_{ID3}(x_i))
\end{equation}

donde \(f_{ID3}\) es un árbol de decisión. El algoritmo ID3 obtiene la solución óptima construyendo un modelo no parametrico \(f_{ID3}(x)=P(y=1|x)\). El funcionamiento del algorítmo (\cite{quinlan1983learning}) es el siguiente.

Sea \(\mathbb{S}\) el conjunto de observaciones etiquetadas. En el nodo raiz del árbol se incluyen todas las observaciones \(\mathbb{S}=\{(x_i,y_i)\}^{N}_{i=1}\), empezando así con un modelo constante:

\begin{equation}
f^{\mathbb{S}}_{ID3} = \frac{1}{|\mathbb{S}|}\sum_{(x,y)\in\mathbb{S}}y.
\end{equation}

Este modelo produce la misma predicción independientemente de los valores de entrada. Teniendo en cuenta las variables \(j=1,\dots,p\) y los todos los umbrales \(t\) se divide el conjunto \(\mathbb{S}\) en dos subconjuntos \(\mathbb{S}\_ = \{(x,y)|(x,y)\in\mathbb{S},x^{(j)}<t\}\) y \(\mathbb{S}_{+} = \{(x,y)|(x,y)\in\mathbb{S},x^{(j)}\geq t\}\). Estos nuevos conjuntos llevan al árbol a producir hojas nuevas, por lo que se evalua que pareja \((j,t)\) es la mejor utilizando la ya mencionada entropía. Este proceso continua recursivamente hasta llegar a los nodos terminales.

\hypertarget{procedimiento-con-r-la-funcion-rpart}{%
\section{\texorpdfstring{Procedimiento con R: la funcion \texttt{rpart}}{Procedimiento con R: la funcion rpart}}\label{procedimiento-con-r-la-funcion-rpart}}

En el paquete \texttt{rpart} de \textbf{R} se encuentra la función \texttt{rpart} que se utiliza para entrenar un arbol de decisión:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rpart}\NormalTok{(formula, data, ...)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
  \item `formula`: Que refleja el predictor lineal $Y \sim X_1 + ... + X_p$.
  \item `data`: Un conjunto de datos con el que interpretar el nombre de las variables incluidas en la formula. 
\end{itemize}

\hypertarget{aplicaciones-de-los-uxe1rboles-de-decisiuxf3n}{%
\section{Aplicaciones de los árboles de decisión}\label{aplicaciones-de-los-uxe1rboles-de-decisiuxf3n}}

En esta sección se incluyen ejemplos de distintos modelos de arboles de decisión que se pueden obtener con \textbf{R}. Usando los datos \texttt{dp\_entr} incluidos en el paquete \texttt{CDR} consistentes en la información de compras de distintos productos por parte de los clientes, se pretende predecir si estos compraran o no un nuevo producto (tensiómetro digital) no incluido en el modelo. A continuación se expone el caso a resolver en este y en futuros capítulos.

\hypertarget{el-caso-de-negocio}{%
\subsection{El caso de negocio}\label{el-caso-de-negocio}}

A continuación se describe el caso que se va a resolver mediante modelos de clasificación tanto en este como en los siguientes capítulos. Existen diversas aserciones para definir Comercio Electrónico (CE).
Entre ellas, la Organización para la Cooperación y el Desarrollo Económico lo define como el proceso de compra, venta o intercambio de bienes, servicios e información a través de redes de comunicación, comunmente Internet.
La clasificación más básica del CE se hace en base al tipo de entes que se relacionan: empresas (businesses, B), consumidores (consummers, C) y entes públicos (governments, G).
De esta forma, una empresa de CE convencional suele ser B2B si vende a otras empresas, B2G si su relación comercial es con administraciones o B2C si vende consumidores finales.

En nuestro caso, la empresa ``Beauty eSheep'' se puede considerar un CE de tipo B2C.
Su producto estrella es una crema hidratante unisex, denominada internamente como ``Crema Luxury'', con mucho éxito entre su clientela.
A partir de este producto inicial, la empresa ha ido ofreciendo un catálogo de productos tanto de tanto de belleza como de bienestar y salud.

Hace tiempo la empresa instauró una estrategia relacional, centrada en el cliente, de tal manera que ha ido recabando diversos datos sobre los mismos incluidas las distintas compras que han realizado.

Basándose en los datos recopilados para cada cliente, la empresa quiere realizar una campaña para impulsar la venta de tensiómetros digitales.
La empresa tiene acceso a un stock muy flexible en fechas de envío de estos productos y a muy buen precio, por lo que se espera una buena rentabilidad en su venta.

Por tanto, en este proyecto hay que identificar el público objetivo susceptible de comprar dicho producto para ofrecerselo a través de la plataforma de CE de la compañía, SMS y/o webmail durante el periodo que dura la campaña.

La tabla con los datos integrados a nivel de cliente, incluyendo el consumo de los distintos productos de la empresa, es la siguiente:

\begin{itemize}
\tightlist
\item
  \textbf{dp\_ENTR}: incluida en el paquete \texttt{CDR}
\end{itemize}

A continuación se describe los campos de dicha tabla:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4930}}@{}}
\caption{Descripción de las variables del conjunto de datos utilizado en los ejemplos.}\tabularnewline
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\emph{COLUMNA}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{TIPO}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{DESCRIPCIÓN}
\end{minipage} \\
\midrule()
\endfirsthead
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\emph{COLUMNA}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{TIPO}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{DESCRIPCIÓN}
\end{minipage} \\
\midrule()
\endhead
ind\_pro11 & Factor & Indicador si el cliente es consumidor del producto ``Fragancia Luxury'' (`S') o no (`N') \\
ind\_pro12 & Factor & Indicador si el cliente es consumidor del producto ``Depiladora Eléctrica'' (`S') o no (`N') \\
ind\_pro14 & Factor & Indicador si el cliente es consumidor del producto ``Crema Luxury'' (`S') o no (`N') \\
ind\_pro15 & Factor & Indicador si el cliente es consumidor del producto ``Smartwatch Fitness'' (`S') o no (`N') \\
ind\_pro16 & Factor & Indicador si el cliente es consumidor del producto ``Kit Pesas Inteligentes'' (`S') o no (`N') \\
ind\_pro17 & Factor & Indicador si el cliente es consumidor del producto ``Estimulador Muscular'' (`S') o no (`N') \\
importe\_pro11 & Doble & Importe neto global consumido por el cliente en ese producto en euros \\
importe\_pro12 & Doble & Importe neto global consumido por el cliente en ese producto en euros \\
importe\_pro14 & Doble & Importe neto global consumido por el cliente en ese producto en euros \\
importe\_pro15 & Doble & Importe neto global consumido por el cliente en ese producto en euros \\
importe\_pro16 & Doble & Importe neto global consumido por el cliente en ese producto en euros \\
importe\_pro17 & Doble & Importe neto global consumido por el cliente en ese producto en euros \\
edad & Entero & Edad del cliente \\
tamano\_fam & Entero & Número de miembros de la unidad familiar a la que pertenece el cliente incluyéndolo a él mismo \\
anos\_exp & Entero & Años de trabajo del cliente \\
ingresos\_ano & Doble & Ingresos anuales del cliente en euros \\
des\_nivel\_edu & Factor & Descripción del nivel de educación del cliente \\
CLS\_PRO\_pro13 & Factor & Clase objetivo, es un indicador si el cliente es consumidor de ese producto ``Tensiómetro Digital'' (`S') o no (`N') \\
\bottomrule()
\end{longtable}

\hypertarget{uxe1rbol-de-clasificaciuxf3n-para-determinar-la-intenciuxf3n-de-compra}{%
\subsection{Árbol de clasificación para determinar la intención de compra}\label{uxe1rbol-de-clasificaciuxf3n-para-determinar-la-intenciuxf3n-de-compra}}

Se contruye un árbol de clasificación \index{árboles de clásificación} usando todo el conjunto de entrenamiento completo sin transformar (en su escala orginal) mediante el algoritmo de Partición Recursiva y Àrboles de Regresión (Recursive Partitioning and Regression Trees, RPART) que se puede usar tanto para regresión como para clasificación.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(CDR)}

\FunctionTok{data}\NormalTok{(dp\_entr)}

\NormalTok{trControl }\OtherTok{\textless{}{-}} \FunctionTok{trainControl}\NormalTok{(}
  \AttributeTok{method =} \StringTok{"cv"}\NormalTok{,}
  \AttributeTok{number =} \DecValTok{10}\NormalTok{,}
  \AttributeTok{classProbs =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{summaryFunction =}\NormalTok{ twoClassSummary}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

En primer lugar, se ha cargado la librería necesaria para entrenar el modelo y los datos de compras de los clientes para predecir si compraran o no el nuevo producto. Además, se define el método de remuestreo para el entrenamiento del modelo. El método definido es validación cruzada con 10 folds, visto en el capítulo \ref{#featureengineering}. A continuación, se determina la semilla aleatoria para hacer los resultados comparables y a su vez se entrena el modelo.

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# se fija una semilla común a todos los modelos}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}

\CommentTok{\# se entrena el modelo}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(CLS\_PRO\_pro13 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
  \AttributeTok{data =}\NormalTok{ dp\_entr,}
  \AttributeTok{method =} \StringTok{"rpart"}\NormalTok{,}
  \AttributeTok{metric =} \StringTok{"ROC"}\NormalTok{,}
  \AttributeTok{trControl =}\NormalTok{ trControl}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{melt}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{resample[, }\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{]), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ variable, }\AttributeTok{y =}\NormalTok{ value, }\AttributeTok{fill =}\NormalTok{ variable)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/006-002-001RPARTRESULTS-1} 

}

\caption{Resultados del modelo durante la validación cruzada.}\label{fig:006-002-001RPARTRESULTS}
\end{figure}

Los resultados de validación cruzada quedan recogidos en los boxplot, por lo que podemos ver en que rangos oscilan las principales medidas, definidas \ref{#featureengineering}, de precisión del modelo (ROC, sensibilidad y especificidad). A continuación se muestra el árbol generado. Se puede observar que este árbol es muy sencillo, y por tanto es facil seguir su interpretación. En primer lugar, decide que si un cliente compra el \emph{smartchwatch fitness} comprará el nuevo producto. En caso de no comprar el \emph{smartchwatch fitness}, pero si comprar la \emph{depiladora eléctrica} si comprará el \emph{tensiómetro digital}. Mientras que si no compra ninguno de esos dos productos no comprará el nuevo producto.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# se pinta el árbol obtenido}
\FunctionTok{rpart.plot}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{finalModel)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/006-002-001RPARTRESULTS2-1} 

}

\caption{Árbol de clasificación sin ajuste automático.}\label{fig:006-002-001RPARTRESULTS2}
\end{figure}

Este modelo se puede mejorar ajustando automaticamente \index{ajuste automático} los hiperparametros\index{hiperparámetro} correspondientes a un modelo de árboles de decisión. De tal manera, primero es necesario saber los hiperparámetros a optimizar en el algoritmo implementado \textbf{R}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{modelLookup}\NormalTok{(}\StringTok{"rpart"}\NormalTok{)}
\CommentTok{\#\textgreater{}   model parameter                label forReg forClass probModel}
\CommentTok{\#\textgreater{} 1 rpart        cp Complexity Parameter   TRUE     TRUE      TRUE}
\end{Highlighting}
\end{Shaded}

El hiperparámetro\index{hiperparámetro} a optimizar es la complejidad del árbol. El valor de \texttt{cp} es un parámetro de parada. Ayuda a acelerar la búsqueda de divisiones porque puede identificar las divisiones que no cumplen con este criterio y podarlas antes de ir avanzar por ella. También se puede entender \texttt{cp} como la mejora mínima necesaria en cada nodo del modelo. En busca del mejor valor de \texttt{cp} se define una red de posibles valores con los que evaluar el modelo.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Se especifica un rango de valores típicos para los hiperparámetros}
\NormalTok{tuneGrid }\OtherTok{\textless{}{-}} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{cp =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.01}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# se entrena el modelo}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}

\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(CLS\_PRO\_pro13 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
  \AttributeTok{data =}\NormalTok{ dp\_entr,}
  \AttributeTok{method =} \StringTok{"rpart"}\NormalTok{,}
  \AttributeTok{metric =} \StringTok{"ROC"}\NormalTok{,}
  \AttributeTok{trControl =}\NormalTok{ trControl,}
  \AttributeTok{tuneGrid =}\NormalTok{ tuneGrid}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model}
\CommentTok{\#\textgreater{} CART }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} 558 samples}
\CommentTok{\#\textgreater{}  17 predictor}
\CommentTok{\#\textgreater{}   2 classes: \textquotesingle{}S\textquotesingle{}, \textquotesingle{}N\textquotesingle{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} No pre{-}processing}
\CommentTok{\#\textgreater{} Resampling: Cross{-}Validated (10 fold) }
\CommentTok{\#\textgreater{} Summary of sample sizes: 502, 502, 502, 503, 503, 502, ... }
\CommentTok{\#\textgreater{} Resampling results across tuning parameters:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   cp    ROC        Sens       Spec     }
\CommentTok{\#\textgreater{}   0.00  0.9390141  0.8531746  0.8854497}
\CommentTok{\#\textgreater{}   0.01  0.8962254  0.8678571  0.8167989}
\CommentTok{\#\textgreater{}   0.02  0.8663454  0.9000000  0.7667989}
\CommentTok{\#\textgreater{}   0.03  0.8458097  0.9392857  0.7310847}
\CommentTok{\#\textgreater{}   0.04  0.8449381  0.9214286  0.7383598}
\CommentTok{\#\textgreater{}   0.05  0.8172123  0.9214286  0.7026455}
\CommentTok{\#\textgreater{}   0.06  0.7978269  0.9607143  0.6628307}
\CommentTok{\#\textgreater{}   0.07  0.7978269  0.9607143  0.6628307}
\CommentTok{\#\textgreater{}   0.08  0.7978269  0.9607143  0.6628307}
\CommentTok{\#\textgreater{}   0.09  0.7866662  0.9321429  0.6664021}
\CommentTok{\#\textgreater{}   0.10  0.7677249  0.8821429  0.6699735}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} ROC was used to select the optimal model using the largest value.}
\CommentTok{\#\textgreater{} The final value used for the model was cp = 0.}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{melt}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{resample[, }\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{]), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ variable, }\AttributeTok{y =}\NormalTok{ value, }\AttributeTok{fill =}\NormalTok{ variable)) }\SpecialCharTok{+}
  \FunctionTok{geom\_violin}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/006-002-003RPARTRESULTS1-1} 

}

\caption{Resultados modelo con ajuste automático durante la validación cruzada}\label{fig:006-002-003RPARTRESULTS1}
\end{figure}

De nuevo se puede ver el rendimiento de cada una de las métricas de los árboles entrenados utilizando validación cruzada. A continuación se muestra el árbol que genera este nuevo modelo.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# se pinta el árbol obtenido}
\FunctionTok{rpart.plot}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{finalModel)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/006-002-003RPARTPLOT2-1} 

}

\caption{Árbol de clasificación con ajuste automático}\label{fig:006-002-003RPARTPLOT2}
\end{figure}

\hypertarget{uxe1rbol-de-regresiuxf3n-para-estimar-el-nuxfamero-de-duxedas-hospitalizado}{%
\subsection{Árbol de regresión para estimar el número de días hospitalizado}\label{uxe1rbol-de-regresiuxf3n-para-estimar-el-nuxfamero-de-duxedas-hospitalizado}}

En este ejemplo se utilizan los datos \texttt{cleveland}, los cuales estan contenidos en el paquete \texttt{CDR}, y que han sido utilizados en el capítulo \ref{###} para estimar \textbf{\textcolor{red}{¿dhosp?}}. El conjunto de datos contiene información sobre pacientes que llegan a un hospital con dolor de pecho, de los que se han registrado distintas características. Se pretende predecir el número de días que un paciente necesitará de hospitalización en base al resto de caracteristicas observadas.\index{árboles de regresión} Estas variables registran si el paciente esta diagnosticado de accidente coronario, su edad, su género, el tipo de dolor que padece y la depresión en el segmento ST inducida por ejercicio en relación al reposo.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# se cargan los datos}
\FunctionTok{data}\NormalTok{(}\StringTok{"cleveland"}\NormalTok{)}

\CommentTok{\# se entrena el modelo}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(}
\NormalTok{  dhosp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ diag }\SpecialCharTok{+}\NormalTok{ edad }\SpecialCharTok{+}\NormalTok{ sexo }\SpecialCharTok{+}
\NormalTok{    tdolor }\SpecialCharTok{+}\NormalTok{ dep,}
  \AttributeTok{data =}\NormalTok{ cleveland,}
  \AttributeTok{method =} \StringTok{"rpart"}\NormalTok{,}
  \AttributeTok{metric =} \StringTok{"RMSE"}\NormalTok{,}
  \AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"cv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{10}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model}
\CommentTok{\#\textgreater{} CART }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} 303 samples}
\CommentTok{\#\textgreater{}   5 predictor}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} No pre{-}processing}
\CommentTok{\#\textgreater{} Resampling: Cross{-}Validated (10 fold) }
\CommentTok{\#\textgreater{} Summary of sample sizes: 272, 273, 273, 272, 274, 272, ... }
\CommentTok{\#\textgreater{} Resampling results across tuning parameters:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   cp          RMSE      Rsquared   MAE     }
\CommentTok{\#\textgreater{}   0.01132433  1.508393  0.3168135  1.135381}
\CommentTok{\#\textgreater{}   0.01674747  1.472518  0.3435459  1.103713}
\CommentTok{\#\textgreater{}   0.37275022  1.576962  0.3526015  1.210831}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} RMSE was used to select the optimal model using the smallest value.}
\CommentTok{\#\textgreater{} The final value used for the model was cp = 0.01674747.}
\end{Highlighting}
\end{Shaded}

Los resultados durante la validación cruzada se observan en la figura \ref{006-002-003RPARTRESULTSREG}. Se puede comprobar que los resultados son bastante pobres, obteniendo un valor de \(R^2\) que oscila entre un 20\% y un 55\%. Estos valores bajos de \(R^2\) se corresponden con los altos valores obtenidos en RMSE.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{melt}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{resample[, }\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{]), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ variable, }\AttributeTok{y =}\NormalTok{ value, }\AttributeTok{fill =}\NormalTok{ variable)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/006-002-003RPARTRESULTSREG-1} 

}

\caption{Resultados árbol de regresión durante la validación cruzada}\label{fig:006-002-003RPARTRESULTSREG}
\end{figure}

El resultado del modelo se muestra en el árbol de la figura \ref{RPARTREGPLOT}. La interpretación de este árbol sería:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Si el paciente no tiene diagnóstico de accidente coronario, solo necesita un día de hospitalización.
\item
  En el caso de tener este diagnostico, y una depresión mayor igual a dos en el segmento ST inducida por ejercicio en relación al reposo, necesitará 1,9 días de hospitalización.
\item
  En un último ejemplo, si la depresión en el segmento ST inducida por ejercicio en relación al reposo es mayor a 0,35 y menor 2 entonces se distingue por genero para saber el número de días hospitalizados. En el caso de los hombres, permaneceran 3,2 días hospitalizados. Mientras que las mujeres solo 1,9 días.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Se entrena de nuevo para obtener un objeto rpart}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{rpart}\NormalTok{(dhosp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ cleveland, }\AttributeTok{cp =} \FunctionTok{unlist}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{bestTune, }\AttributeTok{use.names =} \ConstantTok{FALSE}\NormalTok{))}
\CommentTok{\# se pinta el árbol obtenido}
\FunctionTok{rpart.plot}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/RPARTREGPLOT-1} 

}

\caption{Árbol de regresión para predecir el número de días de hospitalización}\label{fig:RPARTREGPLOT}
\end{figure}

\begin{infobox_resume}

\hypertarget{resumen-5}{%
\subsection*{Resumen}\label{resumen-5}}
\addcontentsline{toc}{subsection}{Resumen}

En este capítulo se ha introducido al lector en los árboles de decisión para clasificación y regresión, en particular:

\begin{itemize}
\tightlist
\item
  Se ha presentado la lógica para la construcción de árboles de decisión, ya sean de regresión o clasificación.
\item
  Se han contemplado diferentes medidas con las que el árbol decide avanzar hacia un nuevo punto de decisión.
\item
  Se presentan los conceptos de sobreajuste y complejidad del árbol, así como la forma de controlarlos.
\item
  Se ha mostrado el uso de \texttt{R} para la clasificación de clases binarias, y para la predicción de variables respuesta numéricas a través de casos aplicados.
\end{itemize}

\end{infobox_resume}

\hypertarget{cap-svm}{%
\chapter{Máquinas de vector soporte}\label{cap-svm}}

\emph{Ramón A. Carrasco e Itzcóatl Bueno\footnote{Universidad Complutense de Madrid}}

\hypertarget{introducciuxf3n-14}{%
\section{Introducción}\label{introducciuxf3n-14}}

Aunque las maquinas de vector soporte se desarrollaron en los años 90 dentro de la comunidad informática(\cite{boser1992training, cortes1995support}) como un método de clasificación binaria, su aplicación se ha extendido a problemas de clasificación múltiple y regresión. Como técnica de clasificación, las maquinas de vector soporte, SVM por sus siglas ingles \emph{Support Vector Machines}, son similares a la regresión logistica pero la SVM enfatiza en el margen entorno a la frontera de decisión. \index{frontera de decisión}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/svm_vs_log} 

}

\caption{SVM vs Regresión logística.}\label{fig:svm-log}
\end{figure}

En la Figura \ref{fig:svm-log} se muestra que la regresión logística divide las observaciones en dos clases de tal forma que se minimice la distancia entre los puntos y la \textbf{frontera de decisión} (A). Por otro lado, la frontera de decisión (B) de la SVM separa los datos en dos clases pero maximizando la distancia entre esta y los puntos de ambas clases. El \textbf{margen} \index{margen} es la distancia entre la frontera de decisión y los puntos más cercanos. El margen es una parte clave del SVM, puesto que evita clasificaciones erroneas como podría pasar en el caso de la regresión logística y como se ilustra en la siguiente figura.

\begin{figure}
\centering
\includegraphics{img/new_svm_obs.png}
\caption{Nueva observación clasificada en SVM vs Regresión logística}
\end{figure}

En resumen, los nuevos datos pueden ser clasificados dentro del margen, y cuanto mayor sea este, más margen de maniobra se tendrá para clasificar estos puntos. Por tanto, para obtener una clasificación erronea en la SVM es necesario que una observación se clasifique aún más al marge que en cualquier otro discriminante lineal. Otra ventaja de las SVM viene de la dificultad de clasificar perfectamente problemas del mundo real, pues es dificil que los discriminantes lineales, vistos en el capítulo \ref{###}, logren una linea que divida perfectamente las categorias a clasificar. Sin embargo, en la SVM, se incluye en la funcion objetivo que mide la calidad del ajuste de los datos de entrenamiento una penalización a los puntos que queden del lado equivocado del límite de decisión. En caso de que los datos puedan ser ser divididos linealmente, no se cometerá ninguna penalización y se maximizará el margen. Mientras que si los datos no son linealmente separables, el mejor ajuste vendrá dado por el equilibrio entre una penalización del error total baja y un margen grande. La penalización a un punto mal clasificado es proporcional a la distancia desde el limite de decisión.

Sin embargo, la SVM también tiene desventajas reseñables. En primer lugar, la SVM no es adecuada en conjuntos de datos grandes porque la complejidad\index{complejidad} de entrenamiento es alta. Además, la SVM no funciona bien cuando los datos tienen mucho ruido, es decir, cuando las clases se superponen. Finalmente, si el conjunto de datos de entrenamiento tiene más variables que observaciones el rendimiento del modelo se verá disminuido.

\hypertarget{algoritmo-svm-para-clasificaciuxf3n-binaria}{%
\section{Algoritmo SVM para clasificación binaria}\label{algoritmo-svm-para-clasificaciuxf3n-binaria}}

\index{SVM}
\index{clasificación binaria}
El algoritmo por el que se obtiene un modelo SVM(\cite{vapnik1997support}) se basa en la ecuación del hiperplano compuesta por dos parametros: un vector de números reales \(\omega\) de la misma dimensión que el vector de variables de entrada \(x\), y un número real \(b\) tal que:

\begin{equation}
  \omega x- b=0
\end{equation}

donde \(\omega x\) es \(\omega^{(1)}x^{(1)} + \omega^{(1)}x^{(1)} + \dots + \omega^{(p)}x^{(p)}\) siendo \(p\) el número de variables incluidas en \(x\). De este modo, la predicción para una instancia de \(x\) viene dada por:

\begin{equation}
  y=sign(\omega x-b)
\end{equation}

donde sign es el operador que devuelve +1 para cualquier valor positivo y -1 para los valores negativos. Entonces, el objetivo del algoritmo es obtener los valores óptimos \(\omega^*\) y \(b^*\).

Estos parametros se obtienen resolviendo un problema de optimización sujeto a las siguientes restricciones:

\begin{align} 
\omega x_i - b\geq 1 \textrm{ si } y_i=+1 \textrm{ y } \\ 
\omega x_i - b\leq 1 \textrm{ si } y_i=-1 \nonumber
\end{align}

Por otro lado, se busca maximizar el margen \index{margen} en torno a la frontera de decisión. Para conseguir esto es necesario minimizar la norma euclidea, y por tanto, el problema a resolver es:

\begin{center}
$\min||\omega||$ sujeto a 
\end{center}
\begin{center}
$y_i(\omega x_i -b)\geq1$ para $i=1,\dots,N$
\end{center}

\hypertarget{y-si-tengo-muxe1s-de-dos-clases}{%
\section{¿Y si tengo más de dos clases?}\label{y-si-tengo-muxe1s-de-dos-clases}}

\index{clasificación multiclase}

Hasta ahora se ha presentado la SVM como un algoritmo solo aplicable a la clasificación de dos clases pero, ¿y si tengo más de dos clases? En general, hay dos enfoques para resolver esto: \textbf{uno contra todos} (OVA, por One Vs All) y \textbf{uno contra uno} (OVO, por One Vs One). En el enfoque OVA, se ajusta una SVM para cada clase, es decir, una clase contra las demás; y se clasifica a la clase para la cual el margen es mayor. En cambio, en el enfoque OVO, se ajustan todas las SVM por pares y se clasifica a la clase que gana la mayor de competiciones por pares.

\hypertarget{truco-del-kernel-tratando-con-la-no-linearidad}{%
\section{Truco del Kernel: Tratando con la no linearidad}\label{truco-del-kernel-tratando-con-la-no-linearidad}}

Cuando los datos no son linearmente separables, se extiende el modelo SVM incluyendo la función de perdida \index{función de perdida}
``hinge''(\cite{gentile1998linear,lee2013study}), tal que \(\max(0,1-y_i(\omega x_i-b))\). En machine learning, esta función de perdida se utiliza para entrenar clasificadores, más concretamente para la clasificación por el margen máximo, sobre todo para las SVM. La función de perdida es cero cuando se cumplen las restricciones, es decir, si \(\omega x_i\) es clasificado en el lado correcto de la frontera de decisión. Por otro lado, si un dato es mal clasificados, el valor obtenido con la función de perdida es proporcional a la distancia hasta la frontera de decisión. Entonces, se quiere minimizar la siguiente función de coste:

\begin{equation}\label{svm-loss}
C||\omega||^{2}+\frac{1}{N}\sum_{i=1}^{N}{\max(0,1-y_i(\omega x_i-b))}
\end{equation}

donde \(C\) es un hiperparametro que determina la compensación entre incrementar el tamaño de la frontera de decisión y asegurar que cada \(x_i\) sea clasificado en el lado correcto de la frontera de decisión.

Un modelo SVM que opitimiza la función de perdida es llamado una SVM \emph{soft-margin}\index{soft margin} mientras que el modelo original es denominado SVM \emph{hard-margin}\index{hard margin}. Si se observa \citet{ref}(svm-loss) se puede ver que para valores grandes de \(C\) el segundo término es despreciable por lo que el algoritmo ignorará por completo la clasificación erronea y tratará de obtener el mayor margen \index{margen} posible. Si se reduce el valor de \(C\), se penaliza más cada error de clasificación, por lo que se cometeran menos errores sacrificando amplitud del margen.

A veces, no es posible separar los datos por un hiperplano\index{hiperplano} en su espacio original. Sin embargo, el \textbf{truco del kernel}\index{truco del kernel} utiliza una función que implícitamente transforme el espacio original a un espacio de mayor dimensión durante la optimización de la función de coste. Así, es posible transformar un espacio de datos bidimensional no separable linealmente en un espacio de datos tridimensional linealmente separable usando un mapeo especifico definido por \(\phi:x\rightarrow\phi(x)\) donde \(\phi(x)\) es un vector de mayor dimensionalidad que \(x\). Sin embargo, no se conoce la función de mapeo que funcionará en los datos. Si se prueban todas las transformaciones posibles podría ser ineficiente y no llegar a la resolución del problema de clasificación planteado.

Se puede trabajar eficientemente en espacios de mayor dimensión sin necesidad de hacer las trasformaciones explicitamente. El problema de optimización de una SVM puede ser reformulado como:

\begin{align} 
\max_{{\alpha_1,\dots,\alpha_N}}\sum_{{i=1}}^{N}{\alpha_i}-\frac{1}{2}\sum_{i=1}^{N}\sum_{{k=1}}^{N}{y_i\alpha_i(x_ix_k)y_k\alpha_k} \textrm{ sujeto a } \\ 
\sum_{i=1}^{N}{\alpha_iy_i}=0 \textrm{ y } \alpha_i\geq0\textrm{, } i=1,\dots,N \nonumber
\end{align}

donde \(\alpha_i\) representa los multiplicadores de Lagrange. En esta formulación solo se aplica los datos de las variables en el término \(x_ix_k\) asi que si se quiere transformar nuestros datos a una espacio de mayor dimension se necesita transformar \(x_i\) en \(\phi(x_i)\) y \(x_k\) en \(\phi(x_k)\) y después multiplicarlas. Sin embargo, esto es muy costoso.

Utilizando el truco del kernel se puede evitar este proceso costoso de transformación y evitarnos calcular el producto escalar reemplazandolo por una operación más simple con las variables originales que proporcione el mismo resultado.

\hypertarget{algunos-kernels-populares}{%
\subsection{Algunos kernels populares}\label{algunos-kernels-populares}}

Los kernels (\cite{scholkopf1997prior, scholkopf1997comparing})\index{kernel} más populares en el entrenamiento de SVM estan incluidos dentro de la función \texttt{svm} del paquete \texttt{e1071} donde se puede determinar a través del parametro \texttt{kernel}. Estos kernel son:

\begin{itemize}
\item lineal: $K(u,v) = \langle u,v\rangle$
\item polinomial de grado $\delta$: $K(u,v) = \gamma(k_1+\langle u,v\rangle)^\delta$
\item base radial: $K(u,v) = e^{\gamma||u-v||^2}$ 
\item sigmoidal: $K(u,v) = \tanh(\gamma\langle u,v\rangle+k_1)$
\end{itemize}

donde \(\langle u,v\rangle = \sum_{i=1}^{n}{u_iv_i}\) es el producto escalar. Cada uno de estos kernels tiene sus propios hiperparametros\index{hiperparámetro}, como \(\delta\) o \(\gamma\), que es necesario tunear\index{ajuste automático}. En machine learning se utiliza mucho el término \textbf{tunear}, el cual se refiere al hecho de ajustar autómaticamente tratando de optimizar los parametros del algoritmo. A la hora de ajustar en \textbf{R} un modelo SVM se puede conocer los parametros a ajustar utilizando la funcion \texttt{modelLookup}. Por ejemplo, para una SVM con kernel de base radial\index{kernel} se observa:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{caret}\SpecialCharTok{::}\FunctionTok{modelLookup}\NormalTok{(}\StringTok{"svmRadial"}\NormalTok{)}
\CommentTok{\#\textgreater{}       model parameter label forReg forClass probModel}
\CommentTok{\#\textgreater{} 1 svmRadial     sigma Sigma   TRUE     TRUE      TRUE}
\CommentTok{\#\textgreater{} 2 svmRadial         C  Cost   TRUE     TRUE      TRUE}
\end{Highlighting}
\end{Shaded}

Los dos parametros que se pueden ajustar en un modelo SVM con kernel de base radial en \textbf{R} son el coste (C) que es la constante \(C\) en la ecuación \ref{svm-loss} y \emph{sigma} que, en el caso de un modelo de regresión probabilística, es el parámetro de escala de la distribución de Laplace hipotética, con media cero, estimada por máxima verosimilitud.

\hypertarget{procedimiento-con-r-la-funcion-svm}{%
\section{\texorpdfstring{Procedimiento con R: la funcion \texttt{svm}}{Procedimiento con R: la funcion svm}}\label{procedimiento-con-r-la-funcion-svm}}

En el paquete \texttt{e1071} de R se encuentra la función \texttt{svm} que se utiliza para entrenar un modelo maquinas vector soporte:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{svm}\NormalTok{(x, y, }\AttributeTok{scale =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{type =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{kernel =}\NormalTok{ ..., ...)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
  \item `x`: Conjunto de datos de entrenamiento que contiene los predictores
  \item `y`: Un vector respuesta con las clases o valores de la variable respuesta.
  \item `scale`: Un valor booleano indicando si es necesario escalar las variables.
  \item `type`: Indicando si se pretende resolver un problema de clasificación o de regresión.
  \item `kernel`: El utilizado durante el entrenamiento y la predicción.
\end{itemize}

\hypertarget{aplicaciuxf3n-de-un-modelo-svm-radial-con-ajuste-automuxe1tico-en-r}{%
\section{Aplicación de un modelo SVM Radial con ajuste automático en R}\label{aplicaciuxf3n-de-un-modelo-svm-radial-con-ajuste-automuxe1tico-en-r}}

Los datos utilizados para entrenar el modelo SVM en este capitulo se cargan desde la libreria \texttt{CDR}. Además, para su entrenamiento se requieren las librerías \texttt{caret} y \texttt{e1071} que ya vienen incluidas en el paquete de este libro \texttt{CDR}.

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{library}\NormalTok{(CDR)}

\FunctionTok{data}\NormalTok{(dp\_entr\_NUM)}
\end{Highlighting}
\end{Shaded}

Se contruye un modelo SVM con Kernel Radial usando el conjunto de entrenamiento con todas las variables transformadas a numéricas y aplicándole previamente a este conjunto de entrenamiento una normalización z-score, presentadas en el capítulo \ref{###}, para que las distintas escalas numéricas sean equiparables. Además se va a proceder a ajustar de forma automática \index{ajuste automático} los parámetros más transcendentes de dicho algoritmo.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trControl }\OtherTok{\textless{}{-}} \FunctionTok{trainControl}\NormalTok{(}
  \AttributeTok{method =} \StringTok{"cv"}\NormalTok{,}
  \AttributeTok{number =} \DecValTok{10}\NormalTok{,}
  \AttributeTok{classProbs =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{preProcOptions =} \FunctionTok{list}\NormalTok{(}\StringTok{"center"}\NormalTok{),}
  \AttributeTok{summaryFunction =}\NormalTok{ twoClassSummary}
\NormalTok{)}

\CommentTok{\# Se especifica un rango de valores típicos para los hiperparámetros}
\NormalTok{tuneGrid }\OtherTok{\textless{}{-}} \FunctionTok{expand.grid}\NormalTok{(}
  \AttributeTok{sigma =} \FunctionTok{seq}\NormalTok{(}\AttributeTok{from =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{to =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.05}\NormalTok{),}
  \AttributeTok{C =} \DecValTok{10}\SpecialCharTok{**}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Se define como procedimiento de muestreo una validación cruzada, como la presentada en el capítulo \ref{###}, de 10 folds. Además, se le indica al modelo que debe calcular las probabilidad de clase en cada remuestreo en caso de estar entrenando un modelo de clasificación. Con el argumento \texttt{summaryFunction\ =\ twoClassSummary} se le indica al modelo que para resumir los resultados se calculen la sensitividad, especificidad y el area bajo la curva ROC. Finalmente, ante la necesidad de estandarizar los datos se le indica al modelo a través del argumento \texttt{preProcOptions}. Una vez tenemos definida una red de parametros\index{hiperparámetro} a optimizar así como un procedimiento de muestreo para el entrenamiento se puede ajustar el modelo automáticamente\index{ajuste automático} con los parametros óptimos encontrados.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Se fija una semilla común a todos los modelos}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}

\CommentTok{\# se entrena el modelo}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(CLS\_PRO\_pro13 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
  \AttributeTok{data =}\NormalTok{ dp\_entr\_NUM,}
  \AttributeTok{method =} \StringTok{"svmRadial"}\NormalTok{,}
  \AttributeTok{metric =} \StringTok{"ROC"}\NormalTok{,}
  \AttributeTok{trControl =}\NormalTok{ trControl,}
  \AttributeTok{tuneGrid =}\NormalTok{ tuneGrid}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model}
\CommentTok{\#\textgreater{} Support Vector Machines with Radial Basis Function Kernel }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} 558 samples}
\CommentTok{\#\textgreater{}  19 predictor}
\CommentTok{\#\textgreater{}   2 classes: \textquotesingle{}S\textquotesingle{}, \textquotesingle{}N\textquotesingle{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} No pre{-}processing}
\CommentTok{\#\textgreater{} Resampling: Cross{-}Validated (10 fold) }
\CommentTok{\#\textgreater{} Summary of sample sizes: 502, 502, 502, 503, 503, 502, ... }
\CommentTok{\#\textgreater{} Resampling results across tuning parameters:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   sigma  C      ROC        Sens       Spec     }
\CommentTok{\#\textgreater{}   0.10   1e{-}02  0.9553241  0.8785714  0.7071429}
\CommentTok{\#\textgreater{}   0.10   1e{-}01  0.9566327  0.8924603  0.8247354}
\CommentTok{\#\textgreater{}   0.10   1e+00  0.9434902  0.8604497  0.8496032}
\CommentTok{\#\textgreater{}   0.10   1e+01  0.9227230  0.8460317  0.8423280}
\CommentTok{\#\textgreater{}   0.10   1e+02  0.8804894  0.8567460  0.8279101}
\CommentTok{\#\textgreater{}   0.10   1e+03  0.8645692  0.8674603  0.8206349}
\CommentTok{\#\textgreater{}   0.10   1e+04  0.8548469  0.8423280  0.8242063}
\CommentTok{\#\textgreater{}   0.15   1e{-}02  0.9527636  0.8535714  0.6642857}
\CommentTok{\#\textgreater{}   0.15   1e{-}01  0.9513653  0.9105820  0.8105820}
\CommentTok{\#\textgreater{}   0.15   1e+00  0.9310091  0.8783069  0.8494709}
\CommentTok{\#\textgreater{}   0.15   1e+01  0.8941421  0.8531746  0.8387566}
\CommentTok{\#\textgreater{}   0.15   1e+02  0.8602088  0.8781746  0.8242063}
\CommentTok{\#\textgreater{}   0.15   1e+03  0.8369331  0.8458995  0.8134921}
\CommentTok{\#\textgreater{}   0.15   1e+04  0.8369284  0.8637566  0.8064815}
\CommentTok{\#\textgreater{}   0.20   1e{-}02  0.9443925  0.8535714  0.6321429}
\CommentTok{\#\textgreater{}   0.20   1e{-}01  0.9440098  0.9250000  0.7384921}
\CommentTok{\#\textgreater{}   0.20   1e+00  0.9199310  0.8818783  0.8387566}
\CommentTok{\#\textgreater{}   0.20   1e+01  0.8752031  0.8674603  0.8207672}
\CommentTok{\#\textgreater{}   0.20   1e+02  0.8477324  0.8674603  0.8063492}
\CommentTok{\#\textgreater{}   0.20   1e+03  0.8308296  0.8638889  0.8134921}
\CommentTok{\#\textgreater{}   0.20   1e+04  0.8308296  0.8638889  0.8099206}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} ROC was used to select the optimal model using the largest value.}
\CommentTok{\#\textgreater{} The final values used for the model were sigma = 0.1 and C = 0.1.}
\end{Highlighting}
\end{Shaded}

Los argumentos que requiere la función son la \texttt{formula}, es decir, indicar la variable respuesta y que predictores intervienen en el modelo. Los datos que se van a utilizar así como el algoritmo a entrenar, en este caso la SVM con kernel de base radial. Además, se le puede indicar una métrica para el rendimiento del modelo, en caso de no indicarlo \textbf{R} asigna la más acorde al tipo de modelo. Finalmente, se incluyen las opciones de entrenamiento y la red de parámetros entre los que buscar la combinación óptima. Los parametros elegidos para el modelo entrenado son \(\sigma=0.1\) y \(C=0.1\) como puede observarse a continuación. Además de ver este resultado en la salida del modelo, se puede observar en la figura \ref{006-002-202SVMTUNEPLOT}. En este gráfico el eje y mide el rendimiento del modelo para ciertos valores de los parametros. Cada linea representa un valor para el hiperparámetro sigma, y se mide su rendimiento variando distintos nivels del parámetro coste (C), que queda representado en el eje x. Así, se observa que la linea roja (sigma=0,1) alcanza el mayor nivel de precisión en el valor C=0,1.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/006-002-202SVMTUNEPLOT-1} 

}

\caption{Optimización de los parámetros C y sigma de una SVM.}\label{fig:006-002-202SVMTUNEPLOT}
\end{figure}

Los siguientes boxplot muestran el rendimiento del modelo en las distintas repeticiones del proceso de validación cruzada. De esta manera, vemos como la sensibilidad oscila en valores superiores al 75\% y la especificidad en valores superiores al 70\% lo cual indica que el modelo entrenado es capaz de predecir bastante bien tanto a los clientes que van a comprar el nuevo producto como los que no lo van a hacer.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{melt}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{resample[, }\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{]), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ variable, }\AttributeTok{y =}\NormalTok{ value, }\AttributeTok{fill =}\NormalTok{ variable)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/006-002-202SVMRESULTS3-1} 

}

\caption{Resultados del modelo obtenidos durante la validación cruzada.}\label{fig:006-002-202SVMRESULTS3}
\end{figure}

\hypertarget{importancia-de-las-variables}{%
\subsection{Importancia de las variables}\label{importancia-de-las-variables}}

Muchos algoritmos de machine learning\index{machine learning} de caja negra\index{caja negra}, no proporcionan información sobre que importancia\index{importancia} tiene cada variable en el modelo. Este es el caso de la SVM. Sin embargo, utilizando el paquete \texttt{vip} podemos cuantificar la importancia de cada variable. Otros paquetes de R como son \texttt{DALEX} o \texttt{iml} pueden hacer esto mismo.

Para medir la importancia\index{importancia}, la función debe reflejar que la metrica para entrenar el modelo fue el area bajo la curva (AUC). La función de medida que se aporte a \texttt{vip()} en el parametro \texttt{pred\_wrapper} deber contener los argumentos de valores reales observados y valores predichos. La SVM entrenada utiliza AUC para el ajuste del modelo, por lo que función aportada a \texttt{vip} deberá devolver la probabilidad de que el modelo asigne una observación a la clase de referencia. En el ejemplo mostrado, la clase de referencia es ``SI'', puesto que interesa saber si un cliente comprará el nuevo producto. Entonces, la función de predicción viene dada por:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prob\_si }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(object, newdata) \{}
  \FunctionTok{predict}\NormalTok{(object, }\AttributeTok{newdata =}\NormalTok{ newdata, }\AttributeTok{type =} \StringTok{"prob"}\NormalTok{)[, }\StringTok{"S"}\NormalTok{]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Finalmente, si se ejecuta la función \texttt{vip()} aportandole los argumentos necesarios generará la figura \ref{vip-svm}. Este gráfico indica la importancia\index{importancia} de cada variable en el modelo de más a menos importante. En el ejemplo mostrado, la variable más importante es el importe gastado en el \emph{smartchwatch fitness}, seguida muy de cerca por la variable que indica si el cliente compra o no el \emph{smartchwatch fitness}. En el otro extremo, se observa que de las variables mostradas tener un nivel de educación básico no es muy relevante en la SVM entrenada.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(vip)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}
\FunctionTok{vip}\NormalTok{(model,}
  \AttributeTok{train =}\NormalTok{ dp\_entr\_NUM, }\AttributeTok{target =} \StringTok{"CLS\_PRO\_pro13"}\NormalTok{, }\AttributeTok{metric =} \StringTok{"auc"}\NormalTok{,}
  \AttributeTok{reference\_class =} \StringTok{"S"}\NormalTok{, }\AttributeTok{pred\_wrapper =}\NormalTok{ prob\_si, }\AttributeTok{method =} \StringTok{"permute"}\NormalTok{,}
  \AttributeTok{aesthetics =} \FunctionTok{list}\NormalTok{(}\AttributeTok{color =} \StringTok{"steelblue2"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"steelblue2"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/vip-svm-1} 

}

\caption{Importancia de las variables incluidas en la SVM}\label{fig:vip-svm}
\end{figure}

Así, de la figura \ref{vip-svm} se concluye que para predecir si un cliente comprará o no el \emph{tensiómetro digital} las variables que más importancia tienen son el importe que gastó en el \emph{smartchwatch fitness}, si compró o no el \emph{smartchwatch fitness}, el importe que gastó en la \emph{depiladora eléctrica} y si compró o no la \emph{depiladora eléctrica}. De forma similar se podrían probar el resto de kernels\index{kernel} disponibles para el algoritmo SVM\index{SVM}.

\begin{infobox_resume}

\hypertarget{resumen-6}{%
\subsection*{Resumen}\label{resumen-6}}
\addcontentsline{toc}{subsection}{Resumen}

En este capítulo se ha introducido al lector en el algoritmo de maquinas vector soporte, en particular:

\begin{itemize}
\tightlist
\item
  Se presenta el concepto de margen de decisión, y las ventajas de la SVM respecto a otras técnicas de clasificación por incorporarlo.
\item
  Se explica el truco del kernel cuando los datos no son separables por un hiperplano en su espacio original. Además, son mencionados algunos de los kernels más utilizados.
\item
  Se presentan casos aplicados de una SVM con kernel radial en \texttt{R} para la clasificación de datos con respuesta binaria, y se presenta el caso extendido en el que se ajustan automáticamente los hiperparametros.
\item
  Se contempla la importancia de las variables en algoritmos de caja negra.
\end{itemize}

\end{infobox_resume}

\hypertarget{cap-knn}{%
\chapter{Clasificador k-vecinos más próximos}\label{cap-knn}}

\emph{Ramón A. Carrasco e Itzcóatl Bueno\footnote{Universidad Complutense de Madrid}}

\hypertarget{introducciuxf3n-15}{%
\section{Introducción}\label{introducciuxf3n-15}}

El k-vecinos más próximos (KNN, por sus siglas inglesas de k-nearest neighbors) es un algoritmo de aprendizaje no paramétrico. Al contrario que otros algoritmos de aprendizaje que permiten deshacerse de los datos de entrenamiento una vez el modelo es construido, KNN guarda las observaciones de entrenamiento en memoria. Cuando se cuenta con una nueva observación \emph{\(x\)} el algoritmo KNN encuentra las \(k\) observaciones de entrenamiento más cercanas a la nueva y devuelve la clase mayoritaria (en el caso de clasificación) o el valor medio (en el caso de regresión).

Determinar el número de puntos, \(k\),\index{k-vecinos} que se van a utilizar para clasificar los nuevos puntos es crucial (\cite{james2013introduction}). Si por ejemplo, se determina que \(k=3\), el modelo KNN utilizará los tres puntos más cercanos (vecinos) a la nueva observación para clasificarlo. El número de vecinos elegidos es decisivo para la determinación del resultado. Por ello, se recomienda probar multiples combinaciones de \(k\) para dar con el mejor ajuste y evitar valores muy bajos o muy altos de \(k\). Si se establece un valor muy bajo de \(k\) aumentara el sesgo y llevará a clasificaciones erroreas. Mientras que valores muy elevados de \(k\) harán que el algoritmo sea computacionalmente costoso. Además, es recomendable establecer valores impares de \(k\) para evitar puntos muertos estadisticos y un resultado no valido.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/knn} 

}

\caption{Ejemplo de k-vecinos más cercanos}\label{fig:knn-ejemplo}
\end{figure}

Dado que la escala\index{escala} de las variables puede impactar en el resultado del modelo KNN, el conjunto de datos debe escalarse para estandarizar la varianza. Aunque el modelo KNN es facil de entender y generalmente preciso, almacenar todo un conjunto de datos y clacular la distancia entre cada nuevo punto a clasificar y todos los puntos del conjunto de datos supone una necesidad de recursos informáticos muy elevados. Esto implica, que cuanto mayor es la cantidad de observaciones en el conjunto de datos mayor es el tiempo para la ejecución de una sola predicción, y por tanto esto puede dar lugar a tiempos de procesamiento muy lentos. Por este motivo, no se recomienda el uso del algoritmo knn cuando se dispone de conjuntos de datos muy grandes. Otro inconveniente a tener en cuenta es la dificultad de aplicar knn a conjuntos de datos con un gran número de variables, puesto que calcular las distancias entre puntos con multiples dimesiones también incrementaría la necesidad de recursos informáticos y podría dificultar conseguir una clasificación precisa.

\hypertarget{decisiones-a-tener-en-cuenta}{%
\section{Decisiones a tener en cuenta}\label{decisiones-a-tener-en-cuenta}}

La elección de la función de distancia así como el número de vecinos \(k\) son elegidos por el investigador, como ya se ha mencionado, se hace antes de ejecutar el algoritmo. Siendo este último el hiperparametro\index{hiperparametro} del modelo.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{caret}\SpecialCharTok{::}\FunctionTok{modelLookup}\NormalTok{(}\StringTok{"knn"}\NormalTok{)}
\CommentTok{\#\textgreater{}   model parameter      label forReg forClass probModel}
\CommentTok{\#\textgreater{} 1   knn         k \#Neighbors   TRUE     TRUE      TRUE}
\end{Highlighting}
\end{Shaded}

\hypertarget{funciuxf3n-de-distancia-a-utilizar}{%
\subsection{Función de distancia a utilizar}\label{funciuxf3n-de-distancia-a-utilizar}}

La forma en que el modelo KNN determina la cercanía entre dos observaciones es con una función de distancia\index{distancia}. Generalmente se utilizan la distancia\index{distancia} euclidea\index{distancia euclidea}, mostrada en la ecuación \ref{eq:dist-eu}, y la distancia de Manhattan\index{distancia manhattan}, cuya formula es la mostrada en la ecuación \ref{eq:dist-man}. Otras distancias como las presentadas en el capítulo \ref{###} también pueden ser utilizadas para el entrenamiento de este algoritmo.

\begin{equation}\label{eq:dist-eu}
d(x_i,x_k)=\sqrt{\sum_{j=1}^{p}{(x_i^{(j)}-x_k^{(j)})^2}}
\end{equation}

\begin{equation}\label{eq:dist-man}
d(x_i,x_k)=\sum_{j=1}^{p}{|x_i^{(j)}-x_k^{(j)}|}
\end{equation}

Otra función de distancia utilizada es la similitud de coseno negativa, definida por:

\begin{equation}
s(x_i,x_k)=\cos(\angle(x_i,x_k))=\frac{\sum_{j=1}^{p}{x^{(j)}_{i}x^{(j)}_{k}}}{\sqrt{\sum_{j=1}^{p}{(x^{(j)}_{i})^2}}\sqrt{\sum_{j=1}^{p}{(x^{(j)}_{k})^2}}}
\end{equation}

Esta función mide la similitud en las direcciones de dos vectores. Si el ángulo entre dos vectores es cero, entonces esos dos vectores tienen la misma dirección y la similitud del coseno es igual a 1. En caso de que dos vectores vayan en direcciones opuestas, la similitud del coseno será -1. Si se quiere utilizar esta similitud como métrica de distancia es necesario multiplicarla por -1.

Finalmente, en el caso de querer incluir variables de distintos tipos en el cálculo de la distancia la opción más a adecuada es el coeficiente de disimilitud de Gower\index{distancia de Gower}. Esta distancia se define como:

\begin{equation}
d(i,j) = \frac{\sum^{p}_{k=1}{\omega_k \delta^{(k)}_{ij}d^(k)_{ij}}}{\omega_k \delta^{(k)}_{ij}}
\end{equation}

En resumen, es una media ponderada de las distancias \(d^{(k)}_{ij}\) con ponderaciones \(\omega_{k}\delta_{ij}^{(k)}\).

\hypertarget{nuxfamero-de-vecinos-k-seleccionados}{%
\subsection{Número de vecinos (k) seleccionados}\label{nuxfamero-de-vecinos-k-seleccionados}}

La elección de cuantos vecinos (k)\index{k-vecinos} intervienen en el ajuste del algoritmo es determinante para su rendimiento. Si se escogen demasiados pocos vecinos el modelo sobreajusta. El caso más extremo, en el que elegimos \(k=1\) se basa la predicción en la observación con la distancia más cercana. Por otro lado, eligiendo un número alto de vecinos hace que el modelo no ajuste bien. En este sentido, en el caso extremo de elegir todas las observaciones como vecinos más cercanos, es decir, \(k=n\), se obtendrá el valor medio (en el caso de la regresión) o la clase mayoritaria (en el caso de la clasificación) como valor predicho para todas las observaciones del conjunto de entrenamiento.

No existe una regla general para la elección optima de k, puesto que en gran medida depende del conjunto de datos utilizado. Cuando se trabaja con conjuntos de datos que tienen pocas variables que no aporten información, valores pequeños de \(k\) tienden a funcionar mejor. Cuantas más variables sin importancia se incorporan en el conjunto de datos, mayores deberan ser los valores de \(k\) para suavizar su efecto.

\hypertarget{procedimiento-con-r-la-funcion-knn}{%
\section{Procedimiento con R: la funcion knn}\label{procedimiento-con-r-la-funcion-knn}}

En el paquete class de R se encuentra la función knn que se utiliza para obtener el modelo k-vecinos más próximos:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{knn}\NormalTok{(train, test, cl, }\AttributeTok{k =} \DecValTok{1}\NormalTok{, ...)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
  \item `train`: Conjunto de datos con las observaciones de entrenamiento.
  \item `test`: Conjunto de datos con las observaciones de validación. Un vector se interpreta como una única observación a validar.
  \item `cl`: Factor con las autenticas clases de las observaciones de entrenamiento.
  \item `k`: Número de vecinos a considerar 
\end{itemize}

\hypertarget{aplicaciuxf3n-del-modelo-knn-en-r}{%
\section{Aplicación del modelo KNN en R}\label{aplicaciuxf3n-del-modelo-knn-en-r}}

Se va a implementar un modelo KNN para la clasificación de clientes que compren o no el \emph{tensiómetro digital} teniendo en cuenta sus caracteristicas y el resto de sus compras. Este conjunto de datos viene incluido el paquete \texttt{CDR}, y es la versión numérica del conjunto de datos \texttt{dp\_entr}. Estos datos tienen sus variables en escala numérica, posibilitando el cálculo de las distancias para el modelo KNN. Sin embargo, las variables tienen distintas escalas de medida (euros, años, unidades, etc.) por lo que es necesario indicar en la función \texttt{trainControl} que se haga un preprocesamiento para estandarizar las variables. Además, se define que el método de muestreo sea una validación cruzada, como la presentada en el capítulo \ref{###}, con 10 folds.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(CDR)}
\FunctionTok{data}\NormalTok{(dp\_entr\_NUM)}

\CommentTok{\# Definimos un método de remuestreo}
\NormalTok{cv }\OtherTok{\textless{}{-}} \FunctionTok{trainControl}\NormalTok{(}
  \AttributeTok{method =} \StringTok{"repeatedcv"}\NormalTok{,}
  \AttributeTok{number =} \DecValTok{10}\NormalTok{,}
  \AttributeTok{repeats =} \DecValTok{5}\NormalTok{,}
  \AttributeTok{classProbs =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{preProcOptions =} \FunctionTok{list}\NormalTok{(}\StringTok{"center"}\NormalTok{),}
  \AttributeTok{summaryFunction =}\NormalTok{ twoClassSummary}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Antes de entrenar el modelo, es necesario la selección del número óptimo de vecinos \(k\). Evaluar uno a uno los posibles valores sería un trabajo arduo, por ello en \textbf{R} se puede definir una red de posibles valores sobre los que evaluar el modelo KNN, y que seleccione el que mejor rendimiento proporcione. A continuación se determinan los posibles valores de \(k\) que se quieren evaluar, en este caso todas las opciones entre uno y quince vecinos.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Definimos la red de busqueda del parámetro}
\NormalTok{hyper\_grid }\OtherTok{\textless{}{-}} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{k =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{15}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Una vez se definen tanto el método de muestreo como la red de posibles valores del hiperparámetro se puede entrenar el modelo. Se procede con el ajuste automático\index{ajuste automático} del modelo de k vecinos más próximos. A la vez que se determina el número óptimo de vecinos\index{k-vecinos}, se entrena el modelo final para que puede ser utilizado con futuras observaciones.

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}
\CommentTok{\# Entrenamos el modelo buscando el parámetro óptimo}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(}
\NormalTok{  CLS\_PRO\_pro13 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
  \AttributeTok{data =}\NormalTok{ dp\_entr\_NUM,}
  \AttributeTok{method =} \StringTok{"knn"}\NormalTok{,}
  \AttributeTok{trControl =}\NormalTok{ cv,}
  \AttributeTok{tuneGrid =}\NormalTok{ hyper\_grid,}
  \AttributeTok{metric =} \StringTok{"ROC"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model}
\CommentTok{\#\textgreater{} k{-}Nearest Neighbors }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} 558 samples}
\CommentTok{\#\textgreater{}  19 predictor}
\CommentTok{\#\textgreater{}   2 classes: \textquotesingle{}S\textquotesingle{}, \textquotesingle{}N\textquotesingle{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} No pre{-}processing}
\CommentTok{\#\textgreater{} Resampling: Cross{-}Validated (10 fold, repeated 5 times) }
\CommentTok{\#\textgreater{} Summary of sample sizes: 502, 502, 502, 503, 503, 502, ... }
\CommentTok{\#\textgreater{} Resampling results across tuning parameters:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   k   ROC        Sens       Spec     }
\CommentTok{\#\textgreater{}    1  0.6584524  0.6466402  0.6702646}
\CommentTok{\#\textgreater{}    2  0.6769109  0.6280952  0.6309259}
\CommentTok{\#\textgreater{}    3  0.6828893  0.6216931  0.6489153}
\CommentTok{\#\textgreater{}    4  0.6851087  0.6223545  0.6401852}
\CommentTok{\#\textgreater{}    5  0.6951129  0.6540212  0.6651852}
\CommentTok{\#\textgreater{}    6  0.6914664  0.6173810  0.6616138}
\CommentTok{\#\textgreater{}    7  0.6982592  0.6252381  0.6953439}
\CommentTok{\#\textgreater{}    8  0.6974556  0.6223545  0.6982275}
\CommentTok{\#\textgreater{}    9  0.6992229  0.6145238  0.7110582}
\CommentTok{\#\textgreater{}   10  0.6994133  0.6123545  0.7124868}
\CommentTok{\#\textgreater{}   11  0.6980371  0.5843651  0.7232011}
\CommentTok{\#\textgreater{}   12  0.6957729  0.5851323  0.7188889}
\CommentTok{\#\textgreater{}   13  0.6965353  0.5794180  0.7246825}
\CommentTok{\#\textgreater{}   14  0.6910927  0.5664815  0.7189418}
\CommentTok{\#\textgreater{}   15  0.6875879  0.5763492  0.7232011}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} ROC was used to select the optimal model using the largest value.}
\CommentTok{\#\textgreater{} The final value used for the model was k = 10.}
\end{Highlighting}
\end{Shaded}

Además, este mismo resultado se puede mostrar graficamente para facilitar su presentación a terceros o facilitar la comprensión de por qué se lleva a cabo esa decisión. Por ejemplo, en la figura \ref{006-002-102KNNKCHOOSING3} se observa que el número óptimo de vecinos es \(k=10\), donde se alcanza el mejor rendimiento del modelo.

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{ggplot}\NormalTok{(model) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \FunctionTok{unlist}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{bestTune), }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_light}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/006-002-102KNNKCHOOSING3-1} 

}

\caption{Número óptimo de vecinos (k)}\label{fig:006-002-102KNNKCHOOSING3}
\end{figure}

El boxplot de los resultados obtenidos durante el proceso de validación cruzada muestra que el ROC del modelo oscila entre un 55\% y un 85\% aproximadamente.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/006-002-102KNNRESULTS3-1} 

}

\caption{Resultados obtenidos durante el proceso de validación cruzada}\label{fig:006-002-102KNNRESULTS3}
\end{figure}

El modelo se resiente en su rendimiento pues parece mostrar dificultades para ajustar la clase positiva, al tener valores de sensibilidad de entre el 40\% y el 85\%, como puede observarse en la figura \ref{006-002-102KNN_RESULTS3}; resultados ligeramente peores que los que obtiene al ajustar observaciones de la clase negativa, los cuales oscilan entren el 54\% y el 89\%.

\begin{infobox_resume}

\hypertarget{resumen-7}{%
\subsection*{Resumen}\label{resumen-7}}
\addcontentsline{toc}{subsection}{Resumen}

En este capítulo se ha introducido al lector en el algoritmo de aprendizaje supervisado conocido como k-vecinos más próximos, destacando:

\begin{itemize}
\tightlist
\item
  Las decisiones a tener en cuenta antes de proceder a entrenar un modelo de k-vecinos más próximos.
\item
  Se han planteado algunas de las distancias más utilizadas para el entrenamiento de este tipo de modelos.
\item
  Se han definido los pros y contras de la cantidad de vecinos a tener en cuenta, así como su ajuste automático.
\end{itemize}

\end{infobox_resume}

\hypertarget{cap-naive-bayes}{%
\chapter{Naive Bayes}\label{cap-naive-bayes}}

\emph{Ramón A. Carrasco e Itzcóatl Bueno\footnote{Universidad Complutense de Madrid}}

\hypertarget{introducciuxf3n-16}{%
\section{Introducción}\label{introducciuxf3n-16}}

Mientras que los algoritmos hasta ahora eran un conjunto de reglas para llevar a cabo un cálculo, un algoritmo bayesiano\index{algoritmo bayesiano} es un conjunto de reglas para usar la evidencia de tus datos para cambiar tus creencias. Los metodos bayesianos pueden formularse como un algoritmo (\cite{lancaster2004introduction}) con los siguientes pasos:

\begin{enumerate}
\item Formular un modelo como una colección de distribuciones de probabilidad condicional a diferentes valores para un parámetro del modelo $\theta\in\Theta$.
\item Organizar las creencias sobre $\theta$ en una distribución de probabilidad a priori de $\theta$.
\item  Introducir los datos en la familia de distribuciones proporcionada en el paso 1.
\item Usa el teorema de Bayes para calcular tus nuevas creencias sobre $\theta$.
\item Critica tu modelo.
\end{enumerate}

Como puede observarse, en esta formulación de un algoritmo bayesiano es necesario el uso del \textbf{Teorema de Bayes}\index{teorema de Bayes} y en el caso del modelo Naive Bayes también es así.

\hypertarget{teorema-de-bayes}{%
\section{Teorema de Bayes}\label{teorema-de-bayes}}

Sean dos eventos A y B definidos en un espacio muestral, se puede definir la probabilidad condicional de que ocurra el evento A dado que previamente se haya observado B como:

\begin{equation}
P(A|B) = \frac{P(A\cap B)}{P(B)}
\end{equation}

siempre que \(P(B) \neq 0\) y donde \(P(A\cap B)\) es la probabilidad de que ocurran ambos eventos a la vez. Los eventos son intercambiables de tal forma que \(P(A\cap B) = P(B|A)P(A)\) y si se reemplaza en la primera ecuación tenemos:

\begin{equation}
P(A|B) = \frac{P(B|A)\cdot P(A)}{P(B)}
\end{equation}

Esta formula es la definición del teorema de Bayes. El algoritmo de clasificación Naive Bayes (NB)\index{Naive Bayes} esta basado en este teorema. Los conceptos de estadística aquí presentados pueden consultarse en más detalle en el capítulo \ref{}.

\hypertarget{el-algoritmo-naive-bayes}{%
\section{El algoritmo Naive Bayes}\label{el-algoritmo-naive-bayes}}

Si se adapta el teorema de Bayes\index{teorema de Bayes} a un problema de clasificación, tendríamos:

\begin{equation}
P(C=c|\ell)=\frac{P(\ell|C=c)\cdot P(C=c)}{P(\ell)}
\end{equation}

En este caso, el elemento a la derecha del igual representa el objetivo de estimación en un problema de clasificación, es decir, la probabilidad de que un individuo pertenezca a la clase \(c\) despues de haber observado la evidencia \(\ell\) (las variables incluidas en el modelo). Esta es la denominada probabilidad a posteriori. El resto de elementos de la formula, los que se ven a la izquierda del igual se definen a continuación:

\begin{itemize}
\item
  \(P(C=c)\) es la probabilidad a priori de pertenecer a la clase \(c\), es decir, la probabilidad que un individuo tiene de ser asignado a esa clase sin observar sus características previamente.
\item
  \(P(\ell|C=c)\) es la verosimilitud de observar una instancia particular de las variables incluidas en el modelo cuando el individuo pertenece a la clase \(c\).
\item
  \(P(\ell)\) es la verosimilitud de observar una instancia particular de las variables incluidas en el modelo independientemente de a que clase pertnezca el individuo.
\end{itemize}

Sin embargo, una gran dificultad para aplicar esta ecuación es la necesidad de conocer que \(P(\ell|c)\) es igual a \(P(\ell_1\cap\ell_2\cap\dots\cap\ell_\kappa|c)\). Esto es dificil de medir y muy especifico. Es complicado que encontremos un ejemplo concreto en nuestros datos de entrenamiento que coincida a la perfección con \(\ell\), y en el caso de tenerlo no tendríamos suficientes para poder estimar una probabilidad de forma fiable. La forma de solucionar este problema es incluir una suposición de independencia particularmente fuerte.

El concepto de indenpendencia condicional\index{indepencia condicional} implica que conocer un evento no aporta información sobre otro evento. Esto puede expresarse de la siguiente forma:

\begin{equation}
P(AB|C) = P(A|C)\cdot P(B|C)
\end{equation}

De este modo, volviendo a nuestro problema de clasificación en el que teniamos la dificultad de estimar \(P(\ell_1\cap\ell_2\cap\dots\cap\ell_\kappa|c)\), ahora tendriamos que:

\begin{equation}
P(\ell|c)=P(\ell_1|c)\cdot P(\ell_2|c)\cdots P(\ell_\kappa|c)
\end{equation}

Y cada uno de los elementos \(P(\ell_i|c)\) puede obtenerse directamente de los datos. Combinando este resultado con la regla de Bayes\index{teorema de Bayes} aplicada a un problema de decisión obtenemos la ecuación dada por el modelo Naive Bayes\index{Naive Bayes}:

\begin{equation}
P(c|\ell)=\frac{P(\ell_1|c)\cdot P(\ell_2|c)\cdots P(\ell_\kappa|c)P(c)}{P(\ell)}
\end{equation}

El clasificador Naive Bayes\index{Naive Bayes} clasifica una nueva observación estimando la probabilidad de que pertenezca a cada clase y asignandolo a aquella que tenga la mayor probabilidad.

En definitiva, el clasificador Naive Bayes\index{Naive Bayes} es muy eficiente en terminos de espacio de almacenamiento necesario así como tiempos de procesamiento. Además, a pesar de ser muy simple tiene en cuenta las caracteristicas observadas. Otra de las ventajas de este clasificador es que es un modelo de aprendizaje incremental. Esto quiere decir que es una técnica de inducción que se actualiza con cada nueva observación de entrenamiento. Es decir, no es necesario volver a procesar todo el conjunto de entrenamiento cuando se disponde de nuevas observaciones.

El ejemplo presentado en el capítulo \ref{cap_arboles} en el que se buscaba predecir si se podría jugar o no al tenís bajo unas condiciones meteorológicas concretas, puede desarrollarse utilizando el modelo Naive Bayes. En este caso, el procedimiento puede resumirse en tres pasos:

\begin{enumerate}
  \item Resumir los datos en una tabla de frecuencias. 
  \item Generar una tabal de verosimilitud obteniendo las probabilidades de las variables.
  \item Aplicar el teorema de Bayes para calcular la probabilidad a posteriori.
\end{enumerate}

De este modo, en caso de tener 30 observaciones que registrasen la previsión del día (soleado, nublado, lluvioso) y si ese día se jugó o no, y no se contase con información sobre niebla ni viento, el primer paso sería resumir estos datos en una tabla de frecuencias como la tabla \ref{tab:tenis-freq}.

\begin{table}[h]
\centering
\begin{tabular}{c|cc|c}
\cline{2-3}
 & \multicolumn{2}{c|}{\textbf{¿Se jugó?}} &  \\ \hline
\multicolumn{1}{|c|}{\textbf{Clima}} & \multicolumn{1}{c|}{SÍ} & NO & \multicolumn{1}{c|}{Total} \\ \hline
\multicolumn{1}{|c|}{Soleado} & \multicolumn{1}{c|}{10} & 1 & \multicolumn{1}{c|}{11} \\ \hline
\multicolumn{1}{|c|}{Nublado} & \multicolumn{1}{c|}{5} & 5 & \multicolumn{1}{c|}{10} \\ \hline
\multicolumn{1}{|c|}{Lluvioso} & \multicolumn{1}{c|}{2} & 7 & \multicolumn{1}{c|}{9} \\ \hline
\multicolumn{1}{|c|}{Total} & \multicolumn{1}{c|}{17} & 13 & \multicolumn{1}{c|}{30} \\ \hline
\end{tabular}
\caption{Tabla de frecuencias - Clima vs Partidos jugados}
\label{tab:tenis-freq}
\end{table}

En un segundo paso, se obtienen las probabilidades de cada posibilidad a partir de la Tabla \ref{tab:tenis-freq} y resultando en la tabla \ref{tab:tenis-likelihood}.

\begin{table}[h]
\centering
\begin{tabular}{c|cc|c}
\cline{2-3}
 & \multicolumn{2}{c|}{\textbf{¿Se jugó?}} &  \\ \hline
\multicolumn{1}{|c|}{\textbf{Clima}} & \multicolumn{1}{c|}{SÍ} & NO & \multicolumn{1}{c|}{$P(Clima_i)$} \\ \hline
\multicolumn{1}{|c|}{Soleado} & \multicolumn{1}{c|}{10} & 1 & \multicolumn{1}{c|}{$\frac{11}{30}=0,37$} \\ \hline
\multicolumn{1}{|c|}{Nublado} & \multicolumn{1}{c|}{5} & 5 & \multicolumn{1}{c|}{$\frac{10}{30}=0,33$} \\ \hline
\multicolumn{1}{|c|}{Lluvioso} & \multicolumn{1}{c|}{2} & 7 & \multicolumn{1}{c|}{$\frac{9}{30}=0,30$} \\ \hline
\multicolumn{1}{|c|}{$P(Jugar)$} & \multicolumn{1}{c|}{$\frac{17}{30}=0,57$} & $\frac{13}{30}=0,43$ &  \\ \cline{1-3}
\end{tabular}
\caption{Tabla de verosimilitud - Clima vs Partidos jugados}
\label{tab:tenis-likelihood}
\end{table}

Finalmente, aplicando el teorema de Bayes se podría predecir si se juega o no el partido ante la previsión de un nuevo día. Por ejemplo, ¿cuál es la probabilidad de no jugar al tenis si el día se prevee soleado? De acuerdo al teorema de Bayes, esta pregunta se respondería a través de:

\begin{equation*}
P(No|Soleado) = \frac{P(Soleado|No)\cdot P(No)}{P(Soleado)}
\end{equation*}

A partir de la tabla \ref{tab:tenis-likelihood} se pueden obtener todos los elementos de este ejemplo. Así, se tendría:

\begin{align*}
P(Soleado|No) = \frac{1}{13} = 0,08\\
P(Soleado) = 0,37\\
P(No) = 0,43\\
\end{align*}

Entonces, la probabilidad de no jugar, es decir, de asignar el día como no jugable dado que la previsión del clima es soleado es igual a 0,09 de acuerdo a la ecuación \ref{end-example}.

\begin{equation}\label{end-example}
P(No|Soleado) = \frac{0,08\cdot 0,43}{0,37} = 0,09
\end{equation}

\hypertarget{procedimiento-con-r-la-funcion-naive_bayes}{%
\section{\texorpdfstring{Procedimiento con R: la funcion \texttt{naive\_bayes}}{Procedimiento con R: la funcion naive\_bayes}}\label{procedimiento-con-r-la-funcion-naive_bayes}}

En el paquete \texttt{naivebayes} de R se encuentra la función \texttt{naive\_bayes} que se utiliza para entrenar un modelo Naive Bayes:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{naive\_bayes}\NormalTok{(formula, data, }\AttributeTok{prior =}\NormalTok{ ..., ...)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
  \item `formula`: Que refleja el predictor lineal $Y \sim X_1 + ... + X_p$.
  \item `data`: Un conjunto de datos con el que interpretar el nombre de las variables incluidas en la formula. 
  \item `prior`: Vector con las probabilidades a priori de las clases.
\end{itemize}

\hypertarget{aplicaciuxf3n-del-modelo-naive-bayes}{%
\section{Aplicación del modelo Naive Bayes}\label{aplicaciuxf3n-del-modelo-naive-bayes}}

Se contruye el modelo Naive Bayes usando el conjunto de entrenamiento de compras realizadas por clientes incluido en el paquete \texttt{CDR}. Este conjunto de datos tiene unos predictores que indican que productos han comprando los clientes, el importe que han gastado y otras características como su edad o su nivel educativo. Se utiliza el conjunto de datos sin transformar (\texttt{dp\_entr}), es decir, en su escala original y con las variables categóricas sin codificar. La variable objetivo indica si un cliente comprará o no el nuevo producto (\emph{tensiómetro digital}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(CDR)}
\FunctionTok{data}\NormalTok{(}\StringTok{"dp\_entr"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# se fija una semilla común a todos los modelos}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}

\CommentTok{\# se entrena el modelo}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(CLS\_PRO\_pro13 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
  \AttributeTok{data =}\NormalTok{ dp\_entr,}
  \AttributeTok{method =} \StringTok{"nb"}\NormalTok{,}
  \AttributeTok{metric =} \StringTok{"Accuracy"}\NormalTok{,}
  \AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}
    \AttributeTok{classProbs =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{method =} \StringTok{"cv"}\NormalTok{,}
    \AttributeTok{number =} \DecValTok{10}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model}
\CommentTok{\#\textgreater{} Naive Bayes }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} 558 samples}
\CommentTok{\#\textgreater{}  17 predictor}
\CommentTok{\#\textgreater{}   2 classes: \textquotesingle{}S\textquotesingle{}, \textquotesingle{}N\textquotesingle{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} No pre{-}processing}
\CommentTok{\#\textgreater{} Resampling: Cross{-}Validated (10 fold) }
\CommentTok{\#\textgreater{} Summary of sample sizes: 502, 502, 502, 503, 503, 502, ... }
\CommentTok{\#\textgreater{} Resampling results across tuning parameters:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   usekernel  Accuracy   Kappa    }
\CommentTok{\#\textgreater{}   FALSE      0.8512662  0.7026716}
\CommentTok{\#\textgreater{}    TRUE      0.8512338  0.7025165}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Tuning parameter \textquotesingle{}fL\textquotesingle{} was held constant at a value of 0}
\CommentTok{\#\textgreater{} Tuning}
\CommentTok{\#\textgreater{}  parameter \textquotesingle{}adjust\textquotesingle{} was held constant at a value of 1}
\CommentTok{\#\textgreater{} Accuracy was used to select the optimal model using the largest value.}
\CommentTok{\#\textgreater{} The final values used for the model were fL = 0, usekernel = FALSE and adjust}
\CommentTok{\#\textgreater{}  = 1.}
\end{Highlighting}
\end{Shaded}

Los resultados del proceso de entrenamiento muestran que para el argumento \texttt{usekernel} es indiferente indicarlo en FALSE o TRUE, los resultados de precisión son similares. En caso de indicar que se quiere usar, se utiliza la densidad para estimar las densidades condicionales de clase de los predictores métricos. El resto de hiperparámetros no se han ajustado al determinar un valor constante para este ejemplo. Finalmente, la salida muestra que la precisión media obtenida durante la validación cruzada alcanza un 85,1\% indicando que el modelo ajusta bastante bien la intención de compra de nuevos clientes.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confusionMatrix}\NormalTok{(model)}
\CommentTok{\#\textgreater{} Cross{-}Validated (10 fold) Confusion Matrix }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} (entries are percentual average cell counts across resamples)}
\CommentTok{\#\textgreater{}  }
\CommentTok{\#\textgreater{}           Reference}
\CommentTok{\#\textgreater{} Prediction    S    N}
\CommentTok{\#\textgreater{}          S 41.8  6.6}
\CommentTok{\#\textgreater{}          N  8.2 43.4}
\CommentTok{\#\textgreater{}                             }
\CommentTok{\#\textgreater{}  Accuracy (average) : 0.8513}
\end{Highlighting}
\end{Shaded}

En la matriz de confusión del modelo se observa para cada celda el promedio porcentual entre remuestreos. Así, se observaque en media el modelo predice mejor cuando un cliente no va a comprar que cuando si lo hace, aunque no con mucha diferencia (menos de un 2\%).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# resultados del modelo por cada iteración del cv}
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{melt}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{resample[, }\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{]), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ variable, }\AttributeTok{y =}\NormalTok{ value, }\AttributeTok{fill =}\NormalTok{ variable)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/NBRESULTS-1} 

}

\caption{Resultados del modelo Naive Bayes obtenidos durante el proceso de validación cruzada.}\label{fig:NBRESULTS}
\end{figure}

Los resultados obtenido durante la validación cruzada se han guardado y se puede observar como la precisión oscila entre el 76\% y el 91\%, aunque en uno de los resultados se obtuvo un 96\% de precisión el cual se marca como un resultado atípico.

\begin{infobox_resume}

\hypertarget{resumen-8}{%
\subsection*{Resumen}\label{resumen-8}}
\addcontentsline{toc}{subsection}{Resumen}

En este capítulo se ha introducido al lector en el algoritmo de Naive Bayes, en concreto:

\begin{itemize}
\tightlist
\item
  Se han presentado los fundamentos del algoritmo bayesiano, particularmente el Teorema de Bayes.
\item
  Se ha explicado el funcionamiento del algoritmo Naive Bayes y su relación con el teorema de Bayes.
\item
  Se ha demostrado su aplicabilidad a casos reales de clasificación a traves de \texttt{R}.
\end{itemize}

\end{infobox_resume}

\hypertarget{bagging.-random-forest}{%
\chapter{\texorpdfstring{Bagging. Random Forest \label{cap-bagg-rf}}{Bagging. Random Forest }}\label{bagging.-random-forest}}

\emph{Ramón A. Carrasco e Itzcóatl Bueno\footnote{Universidad Complutense de Madrid}}

\hypertarget{introducciuxf3n-metodos-de-ensamble}{%
\section{Introducción: Metodos de Ensamble}\label{introducciuxf3n-metodos-de-ensamble}}

A veces puede darse el caso en que ninguno de los modelos hasta ahora presentados proporciona resultados convincentes para nuestro problema. El \emph{aprendizaje ensamblado}\index{aprendizaje ensamblado} \cite{zhou2012ensemble} es un paradigma que en lugar de entrenar un modelo muy preciso se centra en entrenar un gran número de modelos con baja precisión y después combinar sus predicciones para obtener un metamodelo una precisión más alta.

Los modelos de baja precisión son entrenados por algoritmos débiles, es decir, algoritmos incapaces de aprender modelos complejos; y por tanto, generalmente son rápidos tanto en tiempos de entrenamiento como de procesamiento. Existen dos paradigmas de aprendizaje ensamblado\index{aprendizaje ensamblado}: el \emph{bagging}\index{bagging} y el boosting\index{boosting}.

\hypertarget{bagging}{%
\section{Bagging}\label{bagging}}

En vez de buscar la división más eficiente en cada capa como ocurre en el árbol de decisión\index{árbol de decisión}, una alternativa sería construir multiples árboles de decisión y combinar sus resultados. Esta técnica se conoce como bagging\index{bagging} y consiste en hacer crecer varios árboles\index{árbol de decisión} utilizando una selección aleatoria de los datos que se usan para cada árbol y combinando la predicción de cada uno de ellos a través de la media, en el caso de regresión\index{árbol de regresión}, o mediante un sistema de votación, en el caso de un problema de clasificación\index{árbol de clasificación}.

La principal característica del bagging\index{bagging} es el llamado muestreo bootstrap\index{bootstrap}. La intuición tras esto es que para que los árboles\index{árbol de decisión} generen una respuesta única, debe existir aleatoriedad y variación en cada árbol\index{árbol de decisión} que conforme el modelo final; puesto que no tendría sentido construir varios árboles identicos. Este problema queda resuelto por el muestreo bootstrap\index{bootstrap}, el cual extrae una variación aleatoria de los datos en cada ronda. En el caso del bagging\index{bagging}, se ejecutan distintas muestras de datos para el entrenamiento de cada árbol\index{árbol de decisión}. A pesar de que esto no elimina la problematica del sobreajuste\index{sobreajuste}, los patrones presentes en el conjunto de datos apareceran en la mayoría de los árboles\index{árbol de decisión} entrenados y aparecerán en la predicción final. Por tanto, el bagging\index{bagging} es una técnica de gran eficacia para el tratamiento de los valores atipicos y para la reducción de la varianza que generalmente afecta a un único árbol de decisión\index{árbol de decisión}.

\hypertarget{procedimiento-con-r-la-funciuxf3n-bagging}{%
\section{\texorpdfstring{Procedimiento con R: la función \texttt{bagging} \label{rbagging}}{Procedimiento con R: la función bagging }}\label{procedimiento-con-r-la-funciuxf3n-bagging}}

En el paquete \texttt{ipred} de \textbf{R} se encuentra la función \texttt{bagging} que se utiliza para entrenar un modelo bagging:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{bagging}\NormalTok{(formula, data, ...)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
  \item `formula`: Que refleja el predictor lineal $Y \sim X_1 + ... + X_p$.
  \item `data`: Un conjunto de datos con el que interpretar el nombre de las variables incluidas en la formula. 
\end{itemize}

\hypertarget{implementando-bagging-en-r}{%
\section{Implementando bagging en R}\label{implementando-bagging-en-r}}

Es posible la implementación de un modelo de predicción de agregación bootstrap en R. Para ello, se pueden utilizar multiples funciones como la ya mencionada en la sección \ref{rbagging} \texttt{bagging}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(CDR)}
\FunctionTok{data}\NormalTok{(}\StringTok{"dp\_entr"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# se fija una semilla para que el modelo sea reproducible}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}

\CommentTok{\# Se entrena el modelo}
\NormalTok{bag\_model }\OtherTok{\textless{}{-}} \FunctionTok{bagging}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ CLS\_PRO\_pro13 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
  \AttributeTok{data =}\NormalTok{ dp\_entr,}
  \AttributeTok{nbagg =} \DecValTok{100}\NormalTok{,}
  \AttributeTok{coob =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{control =} \FunctionTok{rpart.control}\NormalTok{(}\AttributeTok{minsplit =} \DecValTok{2}\NormalTok{, }\AttributeTok{cp =} \DecValTok{0}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bag\_model}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Bagging classification trees with 100 bootstrap replications }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call: bagging.data.frame(formula = CLS\_PRO\_pro13 \textasciitilde{} ., data = dp\_entr, }
\CommentTok{\#\textgreater{}     nbagg = 100, coob = TRUE, control = rpart.control(minsplit = 2, }
\CommentTok{\#\textgreater{}         cp = 0))}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Out{-}of{-}bag estimate of misclassification error:  0.1416}
\end{Highlighting}
\end{Shaded}

Desafortunadamente, \texttt{bagging()} no proporciona el error de clasificación incorrecta por árbol, por lo que para obtener una curva de error mostrada en la figura \ref{fig:bagg-plot} se itera el modelo variando los valores del parametro \texttt{nbagg} entre 10 y 150, incrementandolo de cinco en cinco. Se observa que el error mínimo se obtiene al utilizar 60 árboles.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/bagging_missclass} 

}

\caption{Número de árboles óptimo}\label{fig:bagg-plot}
\end{figure}

El modelo se puede optimizar ajustando a sus hiperparametros\index{hiperparámetro} óptimos. Esto se debe a que el bagging\index{bagging} tambien está incluido en la función \texttt{caret} y por tanto podemos saber qué parametros son los que hay que ajustar\index{ajuste automático}. Además, se puede entrenar utilizando validación cruzada para comprobar si el modelo se puede generalizar. Se observa que si se entrena un modelo bagging con 60 árboles, la precisión del modelo es del 87\%. Esto es equivalente al resultado obtenido anteriormente en el que para 60 árboles el modelo tenía un error de clasificación del 13\%.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}
\NormalTok{model\_bag }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(}
\NormalTok{  CLS\_PRO\_pro13 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
  \AttributeTok{data =}\NormalTok{ dp\_entr,}
  \AttributeTok{method =} \StringTok{"treebag"}\NormalTok{,}
  \AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"cv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{10}\NormalTok{),}
  \AttributeTok{nbagg =} \DecValTok{60}\NormalTok{,}
  \AttributeTok{control =} \FunctionTok{rpart.control}\NormalTok{(}\AttributeTok{minsplit =} \DecValTok{2}\NormalTok{, }\AttributeTok{cp =} \DecValTok{0}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_bag}
\CommentTok{\#\textgreater{} Bagged CART }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} 558 samples}
\CommentTok{\#\textgreater{}  17 predictor}
\CommentTok{\#\textgreater{}   2 classes: \textquotesingle{}S\textquotesingle{}, \textquotesingle{}N\textquotesingle{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} No pre{-}processing}
\CommentTok{\#\textgreater{} Resampling: Cross{-}Validated (10 fold) }
\CommentTok{\#\textgreater{} Summary of sample sizes: 502, 502, 502, 503, 503, 502, ... }
\CommentTok{\#\textgreater{} Resampling results:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   Accuracy   Kappa    }
\CommentTok{\#\textgreater{}   0.8692532  0.7385449}
\end{Highlighting}
\end{Shaded}

\hypertarget{interpretaciuxf3n-de-variables-en-el-bagging}{%
\subsection{Interpretación de variables en el bagging}\label{interpretaciuxf3n-de-variables-en-el-bagging}}

Una de las principales desventajas del bagging\index{bagging} es que los modelos que son intepretables, tras el uso del bagging\index{bagging} ya no lo son. A pesar de esto, todavía se puede hacer inferencia de como cada una de las variables influye en el modelo generado. La forma de medir la importancia\index{importancia} de las variables incluidas en un árbol es tabular la reducción de la función de perdida atribuida en cada particióna cada variable. Una variable puede utilizarse varias veces para dividir el árbol\index{árbol de decisión} y por tanto, se suma la reducción total en la función de perdida en todas las divisiones de esa variable, utilizandose como su importancia total. Este proceso es similar para el bagging. En este caso, para cada árbol se cácula la reducción de la función de pérdida en todas lasdivisiones. Tras esto, se agrega esta medida en todos los árboles para cada función. El paquete \texttt{ipred} en el que se encuentra la función \texttt{bagging} no captura la información requerida para calcular la importancia de las variables. Sin embargo, el paquete \texttt{caret} si lo hace y se puede construir un gráfico de importancia usando la función \texttt{vip}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(vip)}
\FunctionTok{vip}\NormalTok{(model\_bag,}
  \AttributeTok{num\_features =} \DecValTok{15}\NormalTok{,}
  \AttributeTok{aesthetics =} \FunctionTok{list}\NormalTok{(}\AttributeTok{color =} \StringTok{"skyblue"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"skyblue"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/BAGGINGVIP-1} 

}

\caption{Importancia de las variables incluidas en el modelo bagging.}\label{fig:BAGGINGVIP}
\end{figure}

La figura \ref{BAGGINGVIP} muestra que las variables más importantes en el modelo bagging entrenado para predecir si un cliente comprará o no el \emph{tensiómetro digital} son si comprará la \emph{depiladora eléctrica} y cuánto importe gastará en ese producto, seguido de si comprará el \emph{estimulador muscular} y el \emph{smartchwatch fitness}.

\hypertarget{random-forest}{%
\section{Random Forest}\label{random-forest}}

El bagging\index{bagging} es el paradigma tras el algoritmo de random forest\index{random forest}. Este algoritmo fue desarrollado por primera vez por Tin Kam Ho en 1995 \cite{ho1995random}. Sin embargo, fueron Cutler \cite{cutler1999fast} y Breiman \cite{breiman2001random} quienes desarrollaron una versión extendida del modelo y registraron \textbf{random forest} como marca comercial. El algoritmo básico de bagging funciona del siguiente modo: a partir del conjunto de entrenamiento, se generan \(K\) muestras aleatorias \(\mathbb{S}_{k}\) y se entrena un modelo de árbol de decisión \(f_k\) utilizando cada muestra \(\mathbb{S}_{k}\) como el conjunto de entrenamiento. Tras el entrenamiento, se dispone de \(K\) árboles de decisión. La predicción de una nueva observación \(x\) se obtiene como la media de las \(K\) predicciones:

\begin{equation}
y\leftarrow\hat{f}(x)=\frac{1}{K}\sum^{K}_{k=1}f_{k}(x)
\end{equation}

en el caso de regresión, o por la mayoría de votación en el caso de clasificación.

Tanto el bagging\index{bagging} como el random forest\index{random forest} desarrollan multiples árboles y utilizan el muestreo bootstrap\index{bootstrap} para la aleatorización de los datos. Sin embargo, el random forest establece una limitación artificial a la selección de variables al no considerar todas en cada partición\index{partición}.

\begin{figure}
\centering
\includegraphics{img/randomforest.png}
\caption{Ejemplo de Random Forest}
\end{figure}

El bagging\index{bagging} considera las mismas variables para construir cada árbol\index{árbol de decisión} con el objetivo de minimizar su entropía\index{entropía}, y por tanto todos los árboles suelen tener un aspecto similar. Esto lleva a que las predicciones dadas por los árboles esten altamente correlacionadas. El modelo random forest\index{random forest} evita este problema al establecer la obligación en cada división de utilizar un subconjunto de las variables, lo que proporciona a algunas variables mayor probabilidad de ser seleccionadas, y al generar árboles únicos y no correlacionados se consigue una estructura de decisión final más fiable.

En general, es mejor que el random forest esté formado por una gran cantidad de árboles (por lo menos 100) para suavizar el impacto de valores atípicos. Sin embargo, la tasa de efectividad disminuye a medida que se incorporan más árboles. Llegado a cierto punto, los nuevos árboles no aportan una mejora significativa al modelo pero si incrementan los tiempos de procesamiento.

El modelo random forest es rápido de entrenar y es una buena técnica para obtener un modelo de referencia. Finalmente, aunque estos modelos funcionan bien en la interpretación de patrones complejos y son versatiles, otras técnicas, como por ejemplo el gradient boosting\index{gradient boosting}, proporcionan una mayor precisión en las predicciones.

Estos modelos se han vuelto populares porque tienden a proporcionar un muy buen rendimiento con los modelos predeterminados. A pesar de tener muchos hiperparametros\index{hiperparámetro} que pueden ser ajustados, los valores por defecto de estos tienden a ofrecer buenos resultados en la predicción. Los hiperparametros más importantes que hay que ajustar al entrenar un modelo random forest son: el número de árboles (\(K\)), el numero de variables incluidos en el subconjunto aleatorio en cada división (\(mtry\)), la complejidad\index{complejidad} de cada árbol, el esquema de muestreo y la regla de división a utilizar durante la construcción del árbol.

\hypertarget{nuxfamero-de-uxe1rboles-k}{%
\subsection{\texorpdfstring{Número de árboles (\(K\))\index{número de árboles}}{Número de árboles (K)}}\label{nuxfamero-de-uxe1rboles-k}}

El primer parametro que la lógica nos lleva a ajustar es el número de árboles que componen el modelo de random forest\index{random forest}. Aunque no se considera un hiperparametro como tal, es necerio ajustar el número de árboles a utilizar, pues debe ser lo suficientemente grande como para que la tasa de error se estabilice. La regla general es que el valor mínimo de árboles sea igual a 10 veces el número de variables incluidas en el modelo. Sin emabrgo, cuando se tienen en cuenta otros hiperparámetros para optimizar, es posible que el número de árboles se vea afectado. El tiempo de procesamiento aumenta linealmente con la cantidad de árboles incluidos, pero cuantos más se incluyan, se obtendran estimaciones de error más estables.

\hypertarget{nuxfamero-de-variables-a-considerar-mtry}{%
\subsection{\texorpdfstring{Número de variables a considerar (\(mtry\))}{Número de variables a considerar (mtry)}}\label{nuxfamero-de-variables-a-considerar-mtry}}

\(mtry\) se refiere al hiperparametro\index{hiperparámetro} encargado de controlar la aleatorización de variables utilizadas para las particiones\index{hiperparámetro} de los árboles. Este hiperparametro ayuda a equilibrar la baja correlación del árbol con una razonable fuerza predictiva. Existe un valor predeterminado para este hiperparametro el cual se puede utilizar en caso de no querer o no poder ajustarlo. En el caso de la regresión\index{árbol de regresión}, se determina que \(mtry=\frac{p}{3}\) siendo \(p\) el número de variables incluidas en el modelo. Y en los problemas de clasificación\index{árbol de clasificación}, el valor predeterminado es \(mtry=\sqrt p\). Cuando hay pocas variables relevantes, es decir, los datos son muy ruidosos, tiende a funcionar mejor que el valor de \(mtry\) sea alto pues hace que sea más probable seleccionar esas variables. En cambio, cuando muchas variables son importantes, funciona mejor un valor bajo de \(mtry\).

\hypertarget{complejidad-de-los-uxe1rboles}{%
\subsection{Complejidad de los árboles}\label{complejidad-de-los-uxe1rboles}}

La base de un random forest\index{random forest} es que se construye con árboles de decisión\index{árbol de decisión}, de los cuales se puede controlar su profundidad\index{profundidad del árbol} y complejidad\index{complejidad} como se vió en el capítulo \ref{cap_arboles}. Esto se puede hacer ajustando los hiperparametros\index{hiperparámetro} de profundidad máxima permitida, tamaño del nodo o la cantidad máxima de nodos terminales, por ejemplo.

El tamaño del nodo es probablemente el hiperparámetro más común para controlar la complejidad\index{complejidad} del árbol y la mayoría de las implementaciones usan los valores predeterminados de uno para la clasificación\index{árbol de clasificación} y cinco para la regresión\index{árbol de regresión}, ya que estos valores tienden a producir buenos resultados. Si se quiere controlar el tiempo de procesamiento, se puede hacer reducciones significativas del tiempo aumentando el tamaño del nodo y esto solo impactará marginalmente en la estimación del error.

\hypertarget{esquema-de-muestreo}{%
\subsection{Esquema de muestreo}\label{esquema-de-muestreo}}

Por defecto, el random forest\index{random forest} tiene como esquema de muestreo el bootstrapping\index{bootstrap}, en el cual todas las observaciones se muestrean con reemplazo. Esto es que todas las copias de bootstrap tienen el mismo tamaño que el conjunto de datos de entrenamiento. Sin embargo, el esquema de muestreo se puede ajustar tanto en el tamaño de la muestra como en si se muestrea con o sin reposición. El parámetro de tamaño de muestra determina cuántas observaciones se extraen para el entrenamiento de cada árbol. Cuanto menor sea el tamaño muestral, menor será la correlación entre los árboles, lo cual puede llevar a mejores resultados de precisión en la predicción. La forma de determinar el tamaño muestral optimo puede hayarse evaluando tres o cuatro valores que oscilen entre el 25\% y el 100\% y en el caso de que haya variables no balanceadas categóricas se puede intentar muestrear sin reposición.

\hypertarget{regla-de-divisiuxf3n}{%
\subsection{Regla de división}\label{regla-de-divisiuxf3n}}

Por defecto, la regla de división\index{partición} que utilizan los árboles de decisión\index{árbol de decisión} que conforman un random forest\index{random forest} es la que se presentó en el capítulo \ref{cap_arboles}. Esto es, en el caso de regresión\index{árbol de regresión} seleccionar la división que minimiza la suma residual de cuadrados (SRC); y en el caso de clasificación\index{árbol de clasificación} la división que minimiza la impureza de Gini\index{impureza de Gini} o la entropía\index{entropía}.

\hypertarget{procedimiento-con-r-la-funciuxf3n-randomforest}{%
\section{\texorpdfstring{Procedimiento con R: la función \texttt{randomForest}}{Procedimiento con R: la función randomForest}}\label{procedimiento-con-r-la-funciuxf3n-randomforest}}

En el paquete \texttt{randomForest} de \textbf{R} se encuentra la función \texttt{randomForest} que se utiliza para entrenar un modelo random forest:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{randomForest}\NormalTok{(formula, }\AttributeTok{data =}\NormalTok{ ..., ...)}
\FunctionTok{randomForest}\NormalTok{(x, y, xtest, ytest, }\AttributeTok{ntree =} \DecValTok{500}\NormalTok{, mtry, ...)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
  \item `formula`: Que refleja el predictor lineal $Y \sim X_1 + ... + X_p$.
  \item `data`: Un conjunto de datos con el que interpretar el nombre de las variables incluidas en la formula. 
  \item `x`: Conjunto de datos de entrenamiento que contiene los predictores
  \item `y`: Un vector respuesta con las clases o valores de la variable respuesta.
  \item `xtest`: Conjunto de datos que contiene los predictores del dataset de validación.
  \item `ytest`: Variable respuesta del conjunto de validación.
  \item `ntree`: Número de arboles a construir en el modelo.
  \item `mtry`: Numero de variables muestreadas aleatoriamente como candidatas en cada partición.
\end{itemize}

\hypertarget{aplicaciuxf3n-del-modelo-random-forest-en-r}{%
\section{Aplicación del modelo Random Forest en R}\label{aplicaciuxf3n-del-modelo-random-forest-en-r}}

En esta sección se aplica el modelo random forest al ejemplo de datos de retail incluido en el paquete \texttt{CDR}. Se carga el paquete y con ello, los datos \texttt{dp\_entr\_NUM}. Estos datos tienen las variables categóricas de \texttt{dp\_entr} ya convertidas en variables one-hot-enconding. Se busca predecir si un cliente va a comprar o no el nuevo producto de acuerdo a que productos compra, el importe que gasta en ellos y otras características como, por ejemplo, su nivel educativo.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(CDR)}
\FunctionTok{data}\NormalTok{(dp\_entr\_NUM)}
\end{Highlighting}
\end{Shaded}

\hypertarget{aplicaciuxf3n-del-random-forest}{%
\subsection{Aplicación del Random Forest}\label{aplicaciuxf3n-del-random-forest}}

Este algoritmo al estar basado en árboles de clasificación tiene los mismos requisitos para el entrenamiento que tenían dicho árboles, así se contruye el modelo usando el conjunto de entrenamiento.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# se fija una semilla común a todos los modelos}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}

\CommentTok{\# se entrena el modelo}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(CLS\_PRO\_pro13 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
  \AttributeTok{data =}\NormalTok{ dp\_entr\_NUM,}
  \AttributeTok{method =} \StringTok{"rf"}\NormalTok{, }\AttributeTok{metric =} \StringTok{"Accuracy"}\NormalTok{, }\AttributeTok{ntree =} \DecValTok{500}\NormalTok{,}
  \AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}
    \AttributeTok{method =} \StringTok{"cv"}\NormalTok{,}
    \AttributeTok{number =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{classProbs =} \ConstantTok{TRUE}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model}
\CommentTok{\#\textgreater{} Random Forest }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} 558 samples}
\CommentTok{\#\textgreater{}  19 predictor}
\CommentTok{\#\textgreater{}   2 classes: \textquotesingle{}S\textquotesingle{}, \textquotesingle{}N\textquotesingle{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} No pre{-}processing}
\CommentTok{\#\textgreater{} Resampling: Cross{-}Validated (10 fold) }
\CommentTok{\#\textgreater{} Summary of sample sizes: 502, 502, 502, 503, 503, 502, ... }
\CommentTok{\#\textgreater{} Resampling results across tuning parameters:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   mtry  Accuracy   Kappa    }
\CommentTok{\#\textgreater{}    2    0.8602922  0.7206238}
\CommentTok{\#\textgreater{}   10    0.8620455  0.7241029}
\CommentTok{\#\textgreater{}   19    0.8620130  0.7240248}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Accuracy was used to select the optimal model using the largest value.}
\CommentTok{\#\textgreater{} The final value used for the model was mtry = 10.}
\end{Highlighting}
\end{Shaded}

Los resultados de la validación cruzada se pueden ver en el siguiente boxplot. Se observa como la precisión oscila entre el 80\% y el 95\%. Además, se puede ver en el resultado del modelo que el hiperparametro \(mtry\) se ha ajustado a 10 variables.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/RFRESULTS-1} 

}

\caption{Resultados del modelo random forest durante el proceso de validación cruzada.}\label{fig:RFRESULTS}
\end{figure}

Finalmente, aunque el random forest generado esta compuesto por 500 árboles, se puede acceder a cualquiera de ellos para estudiarlos en profundidad. Para ello, es necesario instalar el paquete \texttt{reprtree} el cual se encuentra en un repositorio de github.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(devtools)}
\ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\NormalTok{(}\StringTok{"reprtree"} \SpecialCharTok{\%in\%} \FunctionTok{installed.packages}\NormalTok{())) \{}
\NormalTok{  devtools}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"araastat/reprtree"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Se pueden observar las decisiones que se toman en el árbol de forma tabulada, indicando que variable se utiliza para la partición, cuál es el valor que decide la división, indicando si es un nodo terminal o no y la predicción del nodo, el cual es \texttt{NA} si no es un nodo terminal.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}
\NormalTok{rf }\OtherTok{\textless{}{-}} \FunctionTok{randomForest}\NormalTok{(CLS\_PRO\_pro13 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
  \AttributeTok{data =}\NormalTok{ dp\_entr\_NUM, }\AttributeTok{ntree =} \DecValTok{500}\NormalTok{,}
  \AttributeTok{mtry =} \FunctionTok{unlist}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{bestTune)}
\NormalTok{)}

\CommentTok{\# Observamos el árbol número 205}
\NormalTok{tree205 }\OtherTok{\textless{}{-}} \FunctionTok{getTree}\NormalTok{(rf, }\DecValTok{205}\NormalTok{, }\AttributeTok{labelVar =} \ConstantTok{TRUE}\NormalTok{)}

\FunctionTok{head}\NormalTok{(tree205[, }\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)])}
\CommentTok{\#\textgreater{}       split var split point status prediction}
\CommentTok{\#\textgreater{} 1 importe\_pro15         100      1       \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} 2 importe\_pro12          60      1       \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} 3 importe\_pro16          90      1       \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} 4  ingresos\_ano      156500      1       \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} 5 importe\_pro17         150      1       \textless{}NA\textgreater{}}
\CommentTok{\#\textgreater{} 6      anos\_exp          33      1       \textless{}NA\textgreater{}}
\end{Highlighting}
\end{Shaded}

Este árbol se puede mostrar de la siguiente manera. Sin embargo, el método por el que se representa gráficamente no es muy claro y puede llevar a confusión o a dificultar la interpretación del árbol. Si se desea estudiar hasta cierto nivel del árbol, se puede incluir el parametro \texttt{depth} como en el ejemplo abajo mostrado.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(reprtree)}
\FunctionTok{plot.getTree}\NormalTok{(rf, }\AttributeTok{k =} \DecValTok{205}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/tree-plot-1} 

}

\caption{Árbol número 205 del random forest entrenado.}\label{fig:tree-plot}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot.getTree}\NormalTok{(rf, }\AttributeTok{k =} \DecValTok{205}\NormalTok{, }\AttributeTok{depth =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/tree-plot2-1} 

}

\caption{Árbol número 205 del random forest entrenado hasta la capa 5.}\label{fig:tree-plot2}
\end{figure}

\hypertarget{aplicaciuxf3n-del-modelo-random-forest-con-ajuste-automuxe1tico}{%
\subsubsection{Aplicación del modelo Random Forest con ajuste automático}\label{aplicaciuxf3n-del-modelo-random-forest-con-ajuste-automuxe1tico}}

En este segundo ejemplo, se pretenden mejorar los resultados del modelo anterior. Para ello, se va a proceder a ajustar de forma automática\index{ajuste automático} los parámetros más transcendentes de dicho algoritmo. De los mencionados anteriormente, solo vamos a intentar ajustar el \(mtry\), que es el que incluye el método \texttt{rf} como hiperparametro.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{modelLookup}\NormalTok{(}\StringTok{"rf"}\NormalTok{)[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]}
\CommentTok{\#\textgreater{}   model parameter                         label}
\CommentTok{\#\textgreater{} 1    rf      mtry \#Randomly Selected Predictors}
\end{Highlighting}
\end{Shaded}

Para ajustar el número de árboles, y el resto de hiperparametros se podría hacer iterando el modelo y cambiando sus valores. Los valores que se quieren probar para el parametro \(mtry\) se incluyen en una red de opciones que se determina a continuación.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Se especifica un rango de valores típicos para los hiperparámetros}
\NormalTok{tuneGrid }\OtherTok{\textless{}{-}} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{mtry =} \DecValTok{2}\SpecialCharTok{:}\DecValTok{19}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A continuación se entrena el modelo para que lo ajuste a los valores de los hiperparametros que maximicen el rendimiento predictivo del modelo.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# se fija una semilla común a todos los modelos}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}

\CommentTok{\# se entrena el modelo}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(CLS\_PRO\_pro13 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
  \AttributeTok{data =}\NormalTok{ dp\_entr\_NUM,}
  \AttributeTok{method =} \StringTok{"rf"}\NormalTok{, }\AttributeTok{metric =} \StringTok{"Accuracy"}\NormalTok{,}
  \AttributeTok{tuneGrid =}\NormalTok{ tuneGrid,}
  \AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{classProbs =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model}
\CommentTok{\#\textgreater{} Random Forest }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} 558 samples}
\CommentTok{\#\textgreater{}  19 predictor}
\CommentTok{\#\textgreater{}   2 classes: \textquotesingle{}S\textquotesingle{}, \textquotesingle{}N\textquotesingle{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} No pre{-}processing}
\CommentTok{\#\textgreater{} Resampling: Bootstrapped (25 reps) }
\CommentTok{\#\textgreater{} Summary of sample sizes: 558, 558, 558, 558, 558, 558, ... }
\CommentTok{\#\textgreater{} Resampling results across tuning parameters:}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}   mtry  Accuracy   Kappa    }
\CommentTok{\#\textgreater{}    2    0.8641273  0.7280186}
\CommentTok{\#\textgreater{}    3    0.8645144  0.7287607}
\CommentTok{\#\textgreater{}    4    0.8618664  0.7234885}
\CommentTok{\#\textgreater{}    5    0.8630134  0.7257459}
\CommentTok{\#\textgreater{}    6    0.8600585  0.7198120}
\CommentTok{\#\textgreater{}    7    0.8593013  0.7182988}
\CommentTok{\#\textgreater{}    8    0.8629885  0.7256405}
\CommentTok{\#\textgreater{}    9    0.8589407  0.7175065}
\CommentTok{\#\textgreater{}   10    0.8567871  0.7132165}
\CommentTok{\#\textgreater{}   11    0.8588108  0.7172628}
\CommentTok{\#\textgreater{}   12    0.8595203  0.7187110}
\CommentTok{\#\textgreater{}   13    0.8579929  0.7156347}
\CommentTok{\#\textgreater{}   14    0.8581897  0.7160362}
\CommentTok{\#\textgreater{}   15    0.8585831  0.7168220}
\CommentTok{\#\textgreater{}   16    0.8581382  0.7159461}
\CommentTok{\#\textgreater{}   17    0.8578179  0.7152923}
\CommentTok{\#\textgreater{}   18    0.8579168  0.7155132}
\CommentTok{\#\textgreater{}   19    0.8569240  0.7135054}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Accuracy was used to select the optimal model using the largest value.}
\CommentTok{\#\textgreater{} The final value used for the model was mtry = 3.}
\end{Highlighting}
\end{Shaded}

Ahora que la busqueda del valor óptimo \(mtry\) ha sido más exhaustiva que en el ejemplo se observa que los resultados con 3 variables seleccionadas en cada partición es suficiente, y que no son necesarias 10 como en el ejemplo anterior pues la precisión decae. Finalmente, se puede observar en el boxplot los resultados obtenidos durante la validación cruzada. Se observa cómo no solo la precisión es mayor que en el ejemplo anterior, sino que además los resultados tienen menos dispersión.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/rfresults2-1} 

}

\caption{Resultados obtenidos por el random forest con ajuste automático durante el proceso de validación cruzada.}\label{fig:rfresults2}
\end{figure}

\begin{infobox_resume}

\hypertarget{resumen-9}{%
\subsection*{Resumen}\label{resumen-9}}
\addcontentsline{toc}{subsection}{Resumen}

En este capítulo se ha introducido al lector en el algoritmo de aprendizaje supervisado conocido como random forest, en concreto:

\begin{itemize}
\tightlist
\item
  Se ha presentado el concepto de aprendizaje ensamblado, y profundizado en uno de sus paradigmas: el bagging.
\item
  Se ha implementado el bagging en \texttt{R} a través de un caso de aplicación para la clasificación binaria de datos.
\item
  Se ha expuesto como medir la importancia de las variables incluidas en un modelo bagging para facilitar su interpretación.
\item
  Se ha explicado el modelo random forest, fundamentado en los árboles decisión y en el bagging. Así como los hiperparámatros más importantes para ajustar un modelo con el mejor rendimiento.
\item
  Se ha aplicado un ejemplo de clasificación binaria utilizando el modelo random forest en \texttt{R}.
\end{itemize}

\end{infobox_resume}

\hypertarget{cap-boosting-xgboost}{%
\chapter{Boosting. XGBoost}\label{cap-boosting-xgboost}}

\emph{Ramón A. Carrasco e Itzcóatl Bueno\footnote{Universidad Complutense de Madrid}}

\hypertarget{introducciuxf3n.-boosting.}{%
\section{Introducción. Boosting.}\label{introducciuxf3n.-boosting.}}

El boosting\index{boosting} \cite{10.5555/2207821} es el otro de los paradigmas de aprendizaje ensamblado\index{aprendizaje ensamblado} presentado en el capítulo \ref{cap-bagg-rf}. De nuevo, el boosting agrega multiples árboles de decisión\index{árbol de decisión}. La lógica de los algoritmos boosting es combinar modelos débiles para conseguir un modelo fuerte. En este caso, el término ``débil'' hace referencia a un modelo con poco poder predictivo pero mejor que una predicción aleatoria. Mientras que ``fuerte'' es considerado un buen predictor para los datos. Para llegar a un algoritmo fuerte a partir de varios modelos debiles es necesario introducir ponderaciones a los árboles basadas en las clasificaciones erroneas del árbol anterior.

El boosting\index{boosting} reduce el problema del sobreajuste\index{sobreajuste} utilizando menos árboles que un modelo random forest\index{random forest}. Mientras que agregar más árboles al random forest ayuda a compensar el sobreajuste, también puede llevar a un aumento del mismo; y por ello hay que ser cauteloso a la hora de agregar nuevos árboles. El enfoque del boosting\index{boosting} a apreder reiteradamente de los errores anteriores puede llevarle a generar sobreajuste\index{sobreajuste}. Aunque ese enfoque produce predicciones más precisas, muchas veces mejores a la mayoría de algoritmos, puede llevar a ajustar las observaciones atípicas. Por ello, el random forest\index{random forest} es una técnica más recomendada para conjuntos de datos muy complejos con un gran número de observaciones atípicas.

Otra de las grandes desventajas del boosting es que tiene unos elevados tiempos de procesamiento dado que su entrenamiento sigue una lógica secuencial. Puesto que un árbol debe esperar al anterior para ser entrenado, se limita la escalabilidad del modelo. Sobre todo cuando se agregan más árboles. Mientras tanto, un random forest entrena los árboles en paralelo, lo que hace que sea más rápido de procesar. El inconveniente que es aplicable tanto a los algoritmos de boosting\index{boosting} como a los de bagging\index{bagging} y al random forest\index{random forest}, es la perdidad de la simplicidad visual y la dificultad de interpretación que tienen respecto a un simple árbol de decisión\index{árbol de decisión}.

\hypertarget{gradient-boosting}{%
\section{Gradient Boosting}\label{gradient-boosting}}

Uno de los algoritmos de boosting más famosos es el gradient boosting\index{gradient boosting}. Mientras que el random forest\index{random forest} seleccionaba combinaciones aleatorias de variables, el gradient boosting selecciona variables que mejoren la precisión con cada nuevo árbol. Por lo tanto, la construcción del modelo secuencial puesto que cada nuevo árbol se crea utilizando información derivada del árbol anterior, y en consecuencia no son independientes. En cada iteración se registran los errores cometidos en los datos de entrenamiento y se aplican a la siguiente ronda de datos de entrenamiento. Además, se agregan pesos a los datos basandose en los resultados de la iteración anterior. Las ponderaciones más altas se aplicarán a los datos que fueron errorneamente clasificados, y no se aplicará tanta atención a los bien clasificados. Este proceso se repite hasta que se llega a un nivel bajo de error. El resultado final se obtiene a través de la media ponderada de las predicciones de los árboles de decisión.\index{árbol de decisión}

\begin{figure}
\centering
\includegraphics{img/boosting.png}
\caption{Ejemplo de Boosting}
\end{figure}

Matemáticamente, un algoritmo gradient boosting\index{gradient boosting} para clasificación sigue los pasos que a continuación se detallan. Sea un problema de clasificación binaria, y asumiendo que se tienen \(K\) árboles de decisión de regresión. Equivalentemente a la regresión logística, la predicción del ensamblado de los árboles de decisión es modelado utilizando una función sigmoidal tal que:

\begin{equation}
P(y=1|x,f)=\frac{1}{1+e^{-f(x)}}
\end{equation}

donde \(f(x)=\sum_{\kappa=1}^{K}{f_{\kappa}(x)}\) y \(f_m\) es un árbol de regresión. De nuevo, como en la regresión logística, se aplica el principio de máxima verosimilitud tratando de hayar una \(f\) que maximice \(\mathcal{L}_f = \sum_{i=1}^{N}{\ln(P(y_i=1|x_i,f))}\)

El algorítmo en inicio es un modelo constante de la forma \(f=f_0=\frac{p}{1-p}\) donde \(p=\frac{1}{N}\sum^{N}_{i=1}\). Tras cada iteración se añade un nuevo arbol \(f_\kappa\) al modelo. Para encontrar el mejor árbol \(f_\kappa\), la primera derivada parcial \(g_i\) del modelo actual se obtiene para \(i=1,\dots,N\):

\begin{equation}
g_i = \frac{\delta\mathcal{L}_f}{\delta f}
\end{equation}

donde \(f\) es el modelo de clasificación ensamblado construido en la iteración previa. Se necesita obtener las derivadas de \(\ln(P(y_i=1|x_i,f))\) con respecto a \(f\) para todo \(i\) para poder calcular \(g_i\). Notese que:

\begin{equation}
\ln(P(y_i=1|x_i,f))=\ln(\frac{1}{1+e^{-f(x_i)}})
\end{equation}

y por tanto, la derivada respecto a \(f\) es igual a:

\begin{equation}
\frac{\delta \ln(P(y_i=1|x_i,f))}{\delta f} = \frac{1}{e^{f(x_i)}+1}
\end{equation}

Después, se reemplaza en el conjunto de entrenamiento la categoría original \(y_i\) por su correspondiente derivada parcial \(g_i\) y se construye un nuevo modelo \(f_\kappa\) utilizando el conjunto de entrenamiento transformado. Tras esto, se obtiene el paso de actualización óptimo \(\rho_\kappa\) como:

\begin{equation}
\rho_\kappa = \arg \max\limits_{\rho}{\mathcal{L}_{f+\rho f_\kappa}}
\end{equation}

Al terminar la iteración \(\kappa\), se actualiza el modelo ensamblado \(f\) añadiendo el nuevo arbol \(f_\kappa\):

\begin{equation}
f\leftarrow f+\alpha\rho_\kappa f_\kappa
\end{equation}

Se itera hasta que \(\kappa=K\), entonces el proceso se detiene y se obtiene el modelo ensamblado final \(f\).

\hypertarget{hiperparuxe1metros-del-modelo-gradient-boosting}{%
\subsection{Hiperparámetros del modelo gradient boosting}\label{hiperparuxe1metros-del-modelo-gradient-boosting}}

Un modelo de gradient boosting tiene dos tipos de hiperparámetros\index{hiperparámetro}:

\begin{enumerate}
  \item Hiperparámetros de boosting
  \item Hiperparámetros del árbol
\end{enumerate}

\hypertarget{hiperparuxe1metros-de-boosting}{%
\subsubsection{Hiperparámetros de boosting}\label{hiperparuxe1metros-de-boosting}}

Los hiperparametos de boosting son principalmente dos: el número de árboles\index{número de árboles} y la tasa de aprendizaje\index{tasa de aprendizaje}. El primero indica la cantidad de árboles que crecen de forma secuencial, y que es importante optimizar para evitar el sobreajuste del modelo. A diferencia de los modelos random forest\index{random forest} o bagging\index{bagging}, en el boosting\index{boosting} los árboles crecen en secuencia para que cada árbol corrija los errores del anterior. Además, el número de árboles necesarios por el GBM puede incrementarse debido a los valores que tomen otros hiperparámetros.

La tasa de aprendizaje es el hiperparámetro con el que se determina la contribución de cada árbol en el resultado final y controla la rapidez con la que el algoritmo avanza por el descenso del gradiente, es decir, la velocidad a la que aprende. Este hiperparámetro toma valores entre 0 y 1, aunque los valores habituales oscilan entre 0,001 y 0,3. El modelo es más robusto a las características especificas de cada árbol, permitiendo una buena generalización, cuando la tasa de aprendizaje toma valores bajos. Estos valores también facilitan la parada temprana antes del sobreajuste\index{sobreajuste} del modelo. Sin embargo, utilizar estos valores vuelve al modelo más exigente computacionalmente hablando y dificulta alcanzar el óptimo con un número fijo de árboles. En resumen, cuanto menor sea este valor, más preciso puede ser el modelo, pero también requerirá más árboles en la secuencia.

\hypertarget{hiperparuxe1metros-de-uxe1rbol}{%
\subsubsection{Hiperparámetros de árbol}\label{hiperparuxe1metros-de-uxe1rbol}}

Los principales hiperparametos\index{hiperparámetro} de árbol son: la profundidad del árbol\index{profundidad del árbol} y el número mínimo de observaciones en nodos terminales. El primer hiperparámetro controla la profundidad de los árboles individuales\index{árbol de decisión}. Los valores habituales de profundidad oscilan entre 3 y 8. Los árboles de menor profundidad son eficientes computacionalmente hablando, pero necesitan más árboles. Sin embargo, los árboles de mayor profundidad permiten que el algoritmo capture interacciones únicas, aunque a su vez aumentan el riesgo de sobreajuste. El segundo hiperparámetro, además de controlar el número mínimo de observaciones en nodos terminales, controla la complejidad de cada árbol. Los valores típicos de este hiperparámetro suelen estar entre 5 y 15. Los valores más altos ayudan a evitar que un modelo aprenda relaciones que pueden ser muy específicas de la muestra particular seleccionada para un árbol, evitando así el sobreajuste. Sin embargo, los valores más pequeños pueden ayudar con clases objetivo desbalanceadas en problemas de clasificación.

\hypertarget{estrategia-de-ajuste-de-hiperparametros}{%
\subsection{Estrategia de ajuste de hiperparametros}\label{estrategia-de-ajuste-de-hiperparametros}}

A diferencia del random forest\index{random forest}, los modelos gradient boosting\index{gradient boosting} pueden variar mucho en su precisión de acuerdo a su configuración de hiperparametros. Por ello, el ajuste puede requerir seguir una estrategia. Un buen enfoque para esto es:

\begin{itemize}
  \item Elegir una tasa de aprendizaje relativamente alta. El valor predeterminado es 0,1 y funciona. Sin embargo, para la mayoría de problemas funcionan valores entre 0,05 y 0,2.
  \item Determinar el número óptimo de árboles para la tasa de aprendizaje elegida.
  \item Ajustar los hiperparámetros del árbol y la tasa de aprendizaje y evaluar la velocidad frente al rendimiento.
  \item Ajustar los parámetros específicos del árbol para determinar la tasa de aprendizaje.
  \item Una vez que se ajustan los parámetros específicos del árbol, se reduce la tasa de aprendizaje para evaluar cualquier mejora en la precisión.
  \item Utilizar la configuración final de hiperparámetros y aumentar los procedimientos de validación cruzada para obtener estimaciones más robustas. Si se utiliza validación cruzada en los pasos anteriores, entonces este paso no es necesario.
\end{itemize}

\hypertarget{procedimiento-con-r-la-funcion-gbm}{%
\section{\texorpdfstring{Procedimiento con R: la funcion \texttt{gbm}}{Procedimiento con R: la funcion gbm}}\label{procedimiento-con-r-la-funcion-gbm}}

En el paquete \texttt{gbm} de \textbf{R} se encuentra la función \texttt{gbm} que se utiliza para entrenar un modelo gradient boosting:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gbm}\NormalTok{(formula, }\AttributeTok{data =}\NormalTok{ ..., ...)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
  \item `formula`: Que refleja el predictor lineal $Y \sim X_1 + ... + X_p$.    
  \item `data`: Un conjunto de datos con el que interpretar el nombre de las variables incluidas en la formula. 
\end{itemize}

\hypertarget{aplicaciuxf3n-del-modelo-gbm-en-r}{%
\section{Aplicación del modelo GBM en R}\label{aplicaciuxf3n-del-modelo-gbm-en-r}}

A través de los datos de compras \texttt{dp\_entr} incluidos en el paquete \texttt{CDR} se va a aplicar el modelo gradient boosting para clasificar qué clientes van a comprar un nuevo producto (\emph{tensiómetro digital}) y cuales no. Se contruye el modelo usando el conjunto de entrenamiento sin transformar (en su escala orginal). Así, en lugar de tener las variables categóricas transformadas a one-hot-encoding se guardan en su variable original, como ocurre con el caso de la variable que mide el nivel educativo.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(CDR)}
\FunctionTok{data}\NormalTok{(dp\_entr)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# se fija una semilla común a todos los modelos}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}

\CommentTok{\# se entrena el modelo}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(CLS\_PRO\_pro13 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
  \AttributeTok{data =}\NormalTok{ dp\_entr,}
  \AttributeTok{method =} \StringTok{"gbm"}\NormalTok{,}
  \AttributeTok{metric =} \StringTok{"Accuracy"}\NormalTok{,}
  \AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}
    \AttributeTok{classProbs =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{method =} \StringTok{"cv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{10}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Stochastic Gradient Boosting 
#> 
#> 558 samples
#>  17 predictor
#>   2 classes: 'S', 'N' 
#> 
#> No pre-processing
#> Resampling: Cross-Validated (10 fold) 
#> Summary of sample sizes: 502, 502, 502, 503, 503, 502, ... 
#> Resampling results across tuning parameters:
#> 
#>   interaction.depth  n.trees  Accuracy   Kappa    
#>   1                   50      0.8564610  0.7130031
#>   1                  100      0.8690909  0.7383556
#>   1                  150      0.8762338  0.7526413
#>   2                   50      0.8690909  0.7382344
#>   2                  100      0.8762338  0.7526413
#>   2                  150      0.8799026  0.7599227
#>   3                   50      0.8763636  0.7528004
#>   3                  100      0.8781494  0.7563575
#>   3                  150      0.8835390  0.7671499
#> 
#> Tuning parameter 'shrinkage' was held constant at a value of 0.1
#> 
#> Tuning parameter 'n.minobsinnode' was held constant at a value of 10
#> Accuracy was used to select the optimal model using the largest value.
#> The final values used for the model were n.trees = 150, interaction.depth =
#>  3, shrinkage = 0.1 and n.minobsinnode = 10.
\end{verbatim}

El modelo resultante del proceso de entrenamiento es un gradient boosting con 100 árboles, una profundidad de 3, ambos parametros han sido ajustados. Además, se habia dejado constantes tanto el número mínimo de observaciones en nodos igual a 10 y una tasa de aprendizaje de 0,1. Los resultados en el proceso de validación cruzada se muestra en la figura \ref{GBMBOXPLOT}, en el que se observa como la precisión oscila entre el 84\% y el 93\% en las iteraciones.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{melt}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{resample[, }\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{]), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ variable, }\AttributeTok{y =}\NormalTok{ value, }\AttributeTok{fill =}\NormalTok{ variable)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/GBMBOXPLOT-1} 

}

\caption{Resultados del modelo GB obtenidos durante el proceso de validación cruzada.}\label{fig:GBMBOXPLOT}
\end{figure}

\hypertarget{gbm-con-ajuste-automuxe1tico}{%
\subsection{GBM con ajuste automático}\label{gbm-con-ajuste-automuxe1tico}}

Se repite el procedimiento para el ejemplo anterior. Sin embargo, en este ejemplo se va a proceder a ajustar de forma automática\index{ajuste automático} los hiperparámetros más relevantes de dicho algoritmo para mejorar los resultados respecto al modelo anterior. Se observa como los parámetros a ajustar para el método \texttt{gbm} son: el número de árboles, la profundidad, la tasa de aprendizaje y el número de observaciones en un nodo.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{modelLookup}\NormalTok{(}\StringTok{"gbm"}\NormalTok{)}
\CommentTok{\#\textgreater{}   model         parameter                   label forReg forClass probModel}
\CommentTok{\#\textgreater{} 1   gbm           n.trees   \# Boosting Iterations   TRUE     TRUE      TRUE}
\CommentTok{\#\textgreater{} 2   gbm interaction.depth          Max Tree Depth   TRUE     TRUE      TRUE}
\CommentTok{\#\textgreater{} 3   gbm         shrinkage               Shrinkage   TRUE     TRUE      TRUE}
\CommentTok{\#\textgreater{} 4   gbm    n.minobsinnode Min. Terminal Node Size   TRUE     TRUE      TRUE}
\end{Highlighting}
\end{Shaded}

Siguiendo la estrategia descrita definimos rangos de valores para los principales hiperparámetros a optimizar.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Se especifica un rango de valores típicos para los hiperparámetros}
\NormalTok{tuneGrid }\OtherTok{\textless{}{-}} \FunctionTok{expand.grid}\NormalTok{(}
  \AttributeTok{interaction.depth =} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{),}
  \AttributeTok{n.trees =} \FunctionTok{c}\NormalTok{(}\DecValTok{10} \SpecialCharTok{*} \FunctionTok{ncol}\NormalTok{(dp\_entr), }\DecValTok{300}\NormalTok{, }\DecValTok{500}\NormalTok{),}
  \AttributeTok{shrinkage =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.2}\NormalTok{),}
  \AttributeTok{n.minobsinnode =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Esta red de posibles valores para los parámetros del modelo se incorporan a la función de entrenamiento. Cuanto más exhaustiva sea la busqueda de estos valores, mayor será el tiempo de ajuste del modelo. La red presentada está formada por 81 combinaciones de los posibles cuatro hiperparametros.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# se fija una semilla común a todos los modelos}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}


\CommentTok{\# se entrena el modelo}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(CLS\_PRO\_pro13 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
  \AttributeTok{data =}\NormalTok{ dp\_entr,}
  \AttributeTok{method =} \StringTok{"gbm"}\NormalTok{,}
  \AttributeTok{metric =} \StringTok{"Accuracy"}\NormalTok{,}
  \AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}
    \AttributeTok{classProbs =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{method =} \StringTok{"cv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{10}
\NormalTok{  ),}
  \AttributeTok{tuneGrid =}\NormalTok{ tuneGrid}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

El modelo que mejores resultados proporciona es aquel que ajusta los hiperparametros a los siguientes valores: está compuesto de 180 árboles, con una profundidad igual a 6, una tasa de aprendizaje de 0.05 y un tamaño mínimo de los nodos de 10 observaciones.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model}\SpecialCharTok{$}\NormalTok{bestTune}
\CommentTok{\#\textgreater{}    n.trees interaction.depth shrinkage n.minobsinnode}
\CommentTok{\#\textgreater{} 13     180                 6      0.05             10}
\end{Highlighting}
\end{Shaded}

En el la figura \ref{modelgbmBOXPLOT} se ven los resultados obtenidos durante el proceso de validación cruzada. Se puede ver que los resultados son similares a los del modelo anterior, aunque hay puntos diferenciables importantes. En primer lugar, se alcanza un valor máximo de precisión mayor al anterior, pues en este caso la precisión oscila entre el 84\% y el 95\%. En segundo lugar, vemos que el valor mediano de la precisión ha subido del 87.5\% del modelo anterior hasta el 90\% de este modelo. Por último, que el rendimiento haya variado tan poco desde el modelo por defecto a un modelo en el que se ha intentado ajustar los hiperparámetros, confirma lo ya expuesto sobre el buen rendimiento de un modelo de gradient boosting con los parámetros por defecto.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{melt}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{resample[, }\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{]), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ variable, }\AttributeTok{y =}\NormalTok{ value, }\AttributeTok{fill =}\NormalTok{ variable)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/modelgbmBOXPLOT-1} 

}

\caption{Resultados del modelo GB con ajuste autómatico obtenidos durante el proceso de validación cruzada.}\label{fig:modelgbmBOXPLOT}
\end{figure}

\hypertarget{extreme-gradient-boosting-xgb}{%
\section{eXtreme Gradient Boosting (XGB)}\label{extreme-gradient-boosting-xgb}}

El eXtreme Gradient Boosting\index{extreme gradient boosting} es una implementación eficiente y escalable del modelo gradient boosting\index{gradient boosting}. Este modelo, abreviado como XGBoost, es un paquete de código abierto en C++, Java, Python (\cite{wade2020hands}), R, Julia, Perl y Scala. En R el modelo se incluye dentro del paquete \texttt{xgboost} (\cite{chen2015xgboost}). El paquete incluye un solucionador eficiente de modelos lineales y un algoritmo de aprendizaje de árboles.

El paquete es compatible con funciones objetivo de regresión, clasificación y ranking. El paquete tiene varias características:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Velocidad: xgboost puede realizar automáticamente cálculos paralelos. Por lo general, es más de 10 veces más rápido que el modelo Gradient Boosting.
\item
  Tipo de entrada: xgboost toma varios tipos de datos de entrada:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Matriz densa (matrix)
\item
  Matriz dispersa (Matrix::dgCMatrix)
\item
  Archivo de datos locales
\item
  Un tipo de datos propio del paquete: xgb.DMatrix
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  Dispersión: xgboost acepta datos de entrada dispersos para los modelos incluidos.
\item
  Personalización: xgboost admite tanto funcion de objetivo y función de evaluación personalizadas.
\item
  Rendimiento: xgboost tiene un mejor rendimiento en varios conjuntos de datos.
\end{enumerate}

\hypertarget{hiperparametros-del-modelo-xgboost}{%
\subsection{Hiperparametros del modelo XGBoost}\label{hiperparametros-del-modelo-xgboost}}

El modelo XGBoost proporciona los hiperparámetros\index{hiperparámetro} que ya incluía el modelo gradient boosting\index{gradient boosting} referentes tanto al boosting\index{boosting} como a los árboles\index{árbol de decisión}. Sin embargo, xgboost también proporciona hiperparámetros adicionales que pueden ayudar a reducir las posibilidades de sobreajuste, lo que lleva a una menor variabilidad de predicción y, por lo tanto, a una mayor precisión. Estos parametros son: la regularización y el dropout.

Los parámetros de regularización\index{regularización} se incluyen para ayudar a evitar el sobreajuste\index{sobreajuste} y reducir la complejidad\index{complejidad} del modelo. Existen tres hiperparámetros que tienen esta funcionalidad: gamma (\(\gamma\)), alpha (\(\alpha\)) y lambda (\(\lambda\)). Gamma es un hiperparámetro de pseudo-regularización conocido como multiplicador Lagrangiano y controla la complejidad de un árbol dado. Este hiperparámetro establece que para hacer una partición adicional en un nodo es necesaria una reducción de pérdida mínima especificada por gamma. Al especificarlo, el modelo XGBoost hace crecer los árboles hasta una profundidad máxima establecida, pero en un paso de poda eliminará las divisiones que no cumplan con la regularización \(\gamma\). Este hiperparámetro toma valores entre 0 e infinito (\(\infty\)), siguiendo la regla de que a mayor valor mayor será la regularización. Los otros hiperparámetros de regularización, \(\alpha\) y \(\lambda\), son más clásicos. Mientras que \(\alpha\) proprociona una regularización \(L_1\), \(\lambda\) proprociona una regularización \(L_2\). Estos parámetros de regularización establecen un límite a cómo de extremos pueden llegar a ser los pesos de los nodos en un árbol. Sus valores se encuentran, al igual que los de \(\gamma\), entre 0 y \(\infty\).

El dropout es un enfoque alternativo para reducir el sobreajuste\index{sobreajuste}. Cuando se entrena un modelo de gradient boosting\index{gradient boosting}, los primeros árboles tienden a dominar el rendimiento del modelo mientras que los que se agregan después suelen mejorar la predicción solo para un pequeño grupo de variables. Esto puede llevar a que se incremente el riesgo de sobreajuste. Con el dropout, se descartan árboles aleatoriamente en el proceso de entrenamiento.

En su implementación en R, el modelo XGBoost incluye principalmente los siguientes parámetros para ser optimizados: número de iteraciones, profundidad máxima de los árboles, tasa de aprendizaje y la regularización \(\gamma\).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(}\FunctionTok{modelLookup}\NormalTok{(}\StringTok{"xgbTree"}\NormalTok{), }\DecValTok{4}\NormalTok{)}
\CommentTok{\#\textgreater{}     model parameter                  label forReg forClass probModel}
\CommentTok{\#\textgreater{} 1 xgbTree   nrounds  \# Boosting Iterations   TRUE     TRUE      TRUE}
\CommentTok{\#\textgreater{} 2 xgbTree max\_depth         Max Tree Depth   TRUE     TRUE      TRUE}
\CommentTok{\#\textgreater{} 3 xgbTree       eta              Shrinkage   TRUE     TRUE      TRUE}
\CommentTok{\#\textgreater{} 4 xgbTree     gamma Minimum Loss Reduction   TRUE     TRUE      TRUE}
\end{Highlighting}
\end{Shaded}

\hypertarget{procedimiento-con-r-la-funcion-xgboost}{%
\section{\texorpdfstring{Procedimiento con R: la funcion \texttt{xgboost}}{Procedimiento con R: la funcion xgboost}}\label{procedimiento-con-r-la-funcion-xgboost}}

En el paquete \texttt{xgboost} de \textbf{R} se encuentra la función \texttt{xgboost} que se utiliza para entrenar un modelo extreme gradient boosting:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{xgboost}\NormalTok{(}\AttributeTok{data =}\NormalTok{ ..., }\AttributeTok{label =}\NormalTok{ ..., ...)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
  \item `data`: Un conjunto de datos con el que entrenar el modelo.
  \item `label`: Vector con la variable respuesta.
\end{itemize}

\hypertarget{aplicaciuxf3n-del-muxf3delo-xgboost-en-r}{%
\section{Aplicación del módelo XGBoost en R}\label{aplicaciuxf3n-del-muxf3delo-xgboost-en-r}}

Este algoritmo vuelve a estar basado en árboles, así se contruye el modelo usando el conjunto de entrenamiento sin transformar (en su escala orginal). Se continua así el ejemplo expuesto durante la aplicación del modelo gradient boosting sin y con ajuste automático de sus hiperparámetros. Se repite el procedimiento de entrenar el modelo para los parámetros por defecto que proporciona R.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# se fija una semilla común a todos los modelos}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}

\CommentTok{\# se entrena el modelo}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(CLS\_PRO\_pro13 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
  \AttributeTok{data =}\NormalTok{ dp\_entr,}
  \AttributeTok{method =} \StringTok{"xgbTree"}\NormalTok{,}
  \AttributeTok{metric =} \StringTok{"Accuracy"}\NormalTok{,}
  \AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}
    \AttributeTok{classProbs =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{method =} \StringTok{"cv"}\NormalTok{,}
    \AttributeTok{number =} \DecValTok{10}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Por defecto, el entrenamiento establece valores constantes para la regularización \(\gamma\) (igual a 0) y para el tamaño mínimo del nodo (igual a 1). En cambio, ajusta los parametros del modelo dentro de los valores por defecto de la función. Así, el modelo XGBoost resultante ha sido uno con 50 iteraciónes, una profundidad máxima igual a 2 y una tasa de aprendizaje de 0,3. Los resultados de la validación cruzada muestran que la precisión obtenida oscila entre el 85\% y el 95\%, resultado similar al del gradient boosting con parametros ajustados. Sin embargo, el valor mediano de la precisión es del 88\%, ligeramente inferior a la observada en el modelo gradient boosting con ajuste automático.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# resultados del modelo por cada iteración del cv}
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{melt}\NormalTok{(model}\SpecialCharTok{$}\NormalTok{resample[, }\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{]), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ variable, }\AttributeTok{y =}\NormalTok{ value, }\AttributeTok{fill =}\NormalTok{ variable)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/XGBRESULTS-1} 

}

\caption{Resultados del modelo XGBoost obtenidos durante el proceso de validación cruzada.}\label{fig:XGBRESULTS}
\end{figure}

\hypertarget{xgboost-y-ajuste-automuxe1tico}{%
\subsection{XGBoost y ajuste automático}\label{xgboost-y-ajuste-automuxe1tico}}

Se continua el ejemplo de aplicar los datos sobre compra de un nuevo producto por parte de los clientes utilizando un modelo extreme gradient boosting en R. Sin embargo, se quieren mejorar los resultados obtenidos, y por ello se va a proceder a ajustar de forma automática\index{ajuste automático} los parámetros más relevantes de dicho algoritmo. Se genera una red de posibles valores para dichos parametros. Por motivos computacionales, esta no se hace muy exhaustiva para evitar largos tiempos de entrenamiento, pero es aconsejable tratar de estudiar más valores para los parámetros que interese optimizar.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Se especifica un rango de valores típicos para los hiperparámetros}
\NormalTok{tuneGrid }\OtherTok{\textless{}{-}} \FunctionTok{expand.grid}\NormalTok{(}
  \AttributeTok{nrounds =} \FunctionTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{500}\NormalTok{),}
  \AttributeTok{max\_depth =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{8}\NormalTok{),}
  \AttributeTok{eta =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.3}\NormalTok{),}
  \AttributeTok{gamma =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\DecValTok{5}\NormalTok{),}
  \AttributeTok{colsample\_bytree =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{),}
  \AttributeTok{min\_child\_weight =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{),}
  \AttributeTok{subsample =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# se fija una semilla común a todos los modelos}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}


\CommentTok{\# se entrena el modelo}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(CLS\_PRO\_pro13 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
  \AttributeTok{data =}\NormalTok{ dp\_entr,}
  \AttributeTok{method =} \StringTok{"xgbTree"}\NormalTok{,}
  \AttributeTok{metric =} \StringTok{"Accuracy"}\NormalTok{,}
  \AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}
    \AttributeTok{classProbs =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{method =} \StringTok{"cv"}\NormalTok{,}
    \AttributeTok{number =} \DecValTok{10}
\NormalTok{  ),}
  \AttributeTok{tuneGrid =}\NormalTok{ tuneGrid}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model}\SpecialCharTok{$}\NormalTok{bestTune[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]}
\CommentTok{\#\textgreater{}    nrounds max\_depth eta gamma}
\CommentTok{\#\textgreater{} 71     100         4 0.2     5}
\end{Highlighting}
\end{Shaded}

El modelo resultante establece que se utilicen 100 iteraciones, que los árboles tengan una profundidad máxima de 4, que la tasa de aprendizaje sea del 0,2 y que la regularización \(\gamma\) tome el valor 5.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/xgb-tuned-RESULTS-1} 

}

\caption{Resultados del modelo XGBoost con ajuste autómatico obtenidos durante el proceso de validación cruzada.}\label{fig:xgb-tuned-RESULTS}
\end{figure}

Los resultados guardados durante la validación cruzada muestran que la precisión es muy similar a la del modelo por defecto, al encontrarse entre el 85\% y el 95\%. Sin embargo, se observa en el valor mediano de la precisión una ligera mejoría, al aumentar hasta el 90\%.

\begin{infobox_resume}

\hypertarget{resumen-10}{%
\subsection*{Resumen}\label{resumen-10}}
\addcontentsline{toc}{subsection}{Resumen}

En este capítulo se ha introducido al lector en el algoritmo de aprendizaje supervisado conocido como gradient boosting, en concreto:

\begin{itemize}
\tightlist
\item
  Se ha presentado el otro paradigma principal de aprendizaje ensamblado: el boosting.
\item
  Se explica el modelo basado en este paradigma, el gradient boosting, así como sus diferencias con el random forest (basado en bagging).
\item
  Se han expuesto los hiperparámetros más relevantes a la hora de optimizar un modelo de gradient boosting.
\item
  Se ha presentado el extreme gradient boosting, una implementación eficiente y escalable del modelo gradient boosting. Así como los hiperparámetros de regularización y otros parametros importantes en esta implementación.
\item
  Se han aplicado ambos algoritmos en \texttt{R} en un caso práctico para la clasificación binaria de datos.
\end{itemize}

\end{infobox_resume}

\hypertarget{part-machine-learning-no-supervisado}{%
\part{Machine learning no supervisado}\label{part-machine-learning-no-supervisado}}

\hypertarget{anuxe1lisis-cluster-clusterizaciuxf3n-jeruxe1rquica}{%
\chapter{Análisis cluster: clusterización jerárquica}\label{anuxe1lisis-cluster-clusterizaciuxf3n-jeruxe1rquica}}

\emph{José-María Montero y Gema Fernández-Avilés}

\index{Técnicas de agrupación}

\hypertarget{origen-cluster}{%
\section{Introducción}\label{origen-cluster}}

El origen de la actividad agrupatoria, hoy en día conocida como análisis cluster o de conglomerados (AC), taxonomía numérica o reconocimiento de patrones, entre otras denominaciones, se remonta a tiempos de Aristóteles y su discípulo Teofrasto. Por tanto, tiene unas profundas raíces y hoy en día se aplica en todos los campos del saber. Se ha evitado la palabra ``clasificación'' porque existe una pequeña diferencia entre agrupación y clasificación. En la actividad clasificatoria se conoce el número de grupos y qué observaciones del conjunto de datos pertenecen a cada uno, siendo el objetivo clasificar nuevas observaciones en los grupos ya existentes. En la actividad agrupatoria, el número de grupos puede ser conocido (normalmente no lo es), pero no las observaciones que pertenecen a cada uno de ellos, siendo el objetivo la asignación de dichas observaciones a diferentes grupos. Este y los dos siguientes capítulos se centran en este último problema y nos referiremos a él por su denominación más popular: AC.

AC está orientado a la síntesis de la información contenida en un conjunto de datos, normalmente una muestra relativa a objetos, individuos o, en general, elementos, definidos por una serie de características, con vistas a establecer una agrupación de los mismos en función de su mayor o menor homogeneidad. En otros términos, AC trata de agrupar dichos elementos en grupos mutuamente excluyentes, de tal forma que los elementos de cada grupo sean lo más parecidos posible entre sí y lo más diferentes posible de los pertenecientes a otros grupos (Fig. \ref{fig:cluster-sim-ch27}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1980}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{1500}\NormalTok{, }\AttributeTok{mean =} \SpecialCharTok{{-}}\DecValTok{4}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{1500}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{))}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{1500}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{1500}\NormalTok{, }\AttributeTok{mean =} \FloatTok{1.5}\NormalTok{))}
\NormalTok{cluster }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\DecValTok{1500}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"B"}\NormalTok{, }\DecValTok{1500}\NormalTok{)))}
\NormalTok{xy }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(x, y, cluster)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ xy, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ cluster)) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"blue"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/cluster-sim-ch27-1} 

}

\caption{Datos simulados que presentan clusters}\label{fig:cluster-sim-ch27}
\end{figure}

Para llevar a cabo un AC, se deben tomar una serie de decisiones:

\begin{itemize}
\tightlist
\item
  Selección de las variables en función de las cuales se van a agrupar los elementos.
\item
  Elección del tipo de distancia o medida de similitud que se va a utilizar para medir la disimilitud entre los elementos objeto de clasificación.
\item
  Elección de la técnica para formar los grupos o conglomerados.
\item
  Determinación del número óptimo de clusters (si no se determina a priori).
\end{itemize}

En este capítulo se abordarán la primera y, sobre todo, la segunda cuestión, dejando las otras dos para los dos capítulos siguientes.

Como ilustración práctica, se utilizará la base de datos \texttt{TIC2021} del paquete \texttt{cdr}, relativa a las estadísticas de uso de las TIC en la Unión Europea en 2021.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(CDR)}
\FunctionTok{data}\NormalTok{(}\StringTok{"TIC2021"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{selecciuxf3n-de-las-variables}{%
\section{Selección de las variables}\label{selecciuxf3n-de-las-variables}}

La selección de las \(p\) variables o características, \(\{X_1, X_2, ..., X_p\}\), en función de las cuales se va a proceder a la agrupación de los \(n\) elementos disponibles es crucial, ya que determina la agrupación final, independientemente de los procedimientos técnicos utilizados. Una vez determinadas éstas, la información disponible, para los elementos objeto de agrupación será:

\begin{longtable}[]{@{}clllll@{}}
\caption{\label{tab:info-muestral} Información muestral}\tabularnewline
\toprule()
& \(X_1\) & \(X_2\) & \(X_3\) & \(\cdots\) & \(X_p\) \\
\midrule()
\endfirsthead
\toprule()
& \(X_1\) & \(X_2\) & \(X_3\) & \(\cdots\) & \(X_p\) \\
\midrule()
\endhead
Elemento \(1\) & \(x_{11}\) & \(x_{12}\) & \(x_{13}\) & \(\cdots\) & \(x_{1p}\) \\
Elemento \(2\) & \(x_{21}\) & \(x_{22}\) & \(x_{23}\) & \(\cdots\) & \(x_{2p}\) \\
Elemento \(3\) & \(x_{31}\) & \(x_{32}\) & \(x_{33}\) & \(\cdots\) & \(x_{3p}\) \\
\(\cdots\) & \(\cdots\) & \(\cdots\) & \(\cdots\) & \(\cdots\) & \(\cdots\) \\
Elemento \(n\) & \(x_{n1}\) & \(x_{n2}\) & \(x_{n3}\) & \(\cdots\) & \(x_{3p}\) \\
\bottomrule()
\end{longtable}

En definitiva, la información de partida es una matriz \(\bf X_{\textit n\times \textit p}\) donde cada elemento viene representado por un punto en el espacio \(p\)-dimensional de variables, es decir, una matriz que proporciona los valores de las variables para cada elemento\footnote{También podría observarse \(\bf X\) por columnas (la \emph{j}-ésima columna muestra los valores de la \emph{j}-ésima variable para cada elemento de la muestra). Aparentemente, no hay razón para no poder clasificar las variables que describen cada elemento (cluster por variables en vez de por elementos).}.

Una cuestión a tener en cuenta es el número de variables a considerar en el AC. La exclusión de variables relevantes generará una agrupación deficiente. La inclusión de variables irrelevantes complicará el proceso de agrupamiento sin procurar ganancias sustantivas. Dado que el miedo del investigador vendrá por el lado de la exclusión de variables relevantes, tenderá a incluir un número excesivo de variables (muchas de ellas correlacionadas). Por ello, se recomienda realizar previamente un ACP (véase Capítulo \ref{acp}, lo que reduce la dimensionalidad del problema, y llevar a cabo el AC a partir de las componentes principales retenidas (incorreladas, evitando así redundancias). La eliminación de información redundante es una cuestión importante en el proceso de clusterización, porque dicha información estaría sobreponderada en el resultado obtenido. Una solución menos drástica a este problema es la utilización de la distancia de Mahalanobis, que, como se verá posteriormente, corrige estas redundancias.

Otra cuestión importante en este momento es decidir si las variables (o componentes principales en su caso) seleccionadas se utilizarán estandarizadas o no. No existe consenso sobre la cuestión, si bien se suele recomendar su estandarización para evitar consecuencias no deseadas derivadas de la distinta escala y/o unidades de medida. No obstante, autores tan relevantes como Edelborck (1979) y Everitt (1993), están en contra y proponen las siguientes alternativas: (\(i\)) recategorizar todas las variables en variables binarias, y aplicar a éstas una distancia apropiada para ese tipo de medidas; \((ii)\) realizar distintos AC con grupos de variables homogéneas (en cuanto a su métrica) y sintetizar después los diferentes resultados; y \((iii)\) utilizar la distancia de Gower, que es aplicable con cualquier tipo de métrica.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tic }\OtherTok{\textless{}{-}} \FunctionTok{scale}\NormalTok{(TIC2021)}
\end{Highlighting}
\end{Shaded}

\hypertarget{elecciuxf3n-de-la-distancia-entre-elementos}{%
\section{Elección de la distancia entre elementos}\label{elecciuxf3n-de-la-distancia-entre-elementos}}

Una vez se dispone de la matriz de información \(\bf X_{\textit n\times \textit p}\) , \index{distancia} la segunda etapa en el AC consiste en la creación de una nueva matriz \(\bf D_{\textit n\times \textit n}\) cuyos elementos \(\{d_{ij}\}\) sean las distancias o disimilaridades entre los elementos objeto de agrupamiento.

En caso de variables cuantitativas, la distancia entre dos elementos en un espacio de \(p\) dimensiones, \(d({\bf x}_i;{\bf{x}}_{j})\), se define como una función que a cada dos puntos de \(\mathbb{R}^{p}\) le asocia un número real y que verifica:\footnote{Si se cumple el cuarto requisito la función distancia suele llamarse distancia métrica.}

\begin{itemize}
\item
  \(d({\bf x}_i;{\bf{x}}_{j}) \geq0\),
\item
  \(d({\bf x}_i;{\bf{x}}_{j})=0\) si y sólo si \({\bf{x}}_{i}={\bf x}_{j}\),
\item
  \(d({\bf x}_i;{\bf{x}}_{j})=d({\bf x}_j;{\bf{x}}_{i})\),
\item
  \(d({\bf x}_i;{\bf{x}}_{j})+d({\bf x}_j;{\bf{x}}_{k}) \geq d({\bf x}_i;{\bf{x}}_{k}), \quad \forall{\bf{x}}_{k} \in \mathbb{R}^{p}\),
\end{itemize}

Con variables cualitativas, la similitud entre dos elementos, \(s({\bf x}_i;{\bf{x}}_{j})\), es una función que a cada dos puntos de \(\mathbb{R}^{p}\) le asocia un número real, y que verifica:

\begin{itemize}
\tightlist
\item
  \(s({\bf x}_i;{\bf{x}}_{j}) \leq s_0\), donde \(s_0\) es un número real finito arbitrario (normalmente 1).
\item
  \(s({\bf x}_i;{\bf{x}}_{j})=s_0\) si y sólo si \({\bf{x}}_{i}={\bf x}_{j}\),
\item
  \(s({\bf x}_i;{\bf{x}}_{j})=s({\bf x}_j;{\bf{x}}_{i})\),
\item
  \(|s({\bf x}_i;{\bf{x}}_{j})+s({\bf x}_j;{\bf{x}}_{k})|s({\bf x}_i;{\bf{x}}_{z}) \geq d({\bf x}_i;{\bf{x}}_{k})s({\bf x}_j;{\bf{x}}_{k}) \in \mathbb{R}^{p}\).
\end{itemize}

Son numerosas las formas de medir las distancias o similaridades entre dos elementos, que satisfacen las condiciones expuestas. Las más populares son las siguientes:

\textbf{Variables cuantitativas}

\begin{itemize}
\tightlist
\item
  \textbf{Distancia euclídea.} \index{distancia!euclídea} Se define como: \begin{equation}
  d_{e}({\bf x}_i;{\bf{x}}_{j})=\sqrt{\sum_{k=1}^{p}\left(  x_{ik}-x_{jk}\right)  ^{2}}
  \end{equation}
\end{itemize}

Ignora las unidades de medida de las variables y, en consecuencia, aunque es invariante a los cambios de origen, no lo es a los cambios de escala. También ignora las relaciones entre ellas. Resulta de utilidad con variables cuantitativas incorreladas y medidas en las mismas unidades. El cuadrado de la distancia euclídea \index{distancia!euclídea al cuadrado} también suele utilizarse como distancia.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(factoextra)}
\NormalTok{d\_euclidea }\OtherTok{\textless{}{-}} \FunctionTok{get\_dist}\NormalTok{(}\AttributeTok{x =}\NormalTok{ tic, }\AttributeTok{method =} \StringTok{"euclidea"}\NormalTok{)}
\FunctionTok{as.matrix}\NormalTok{(d\_euclidea)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\CommentTok{\#\textgreater{}          1        2        3        4        5}
\CommentTok{\#\textgreater{} 1 0.000000 6.421631 2.417212 1.870962 2.304686}
\CommentTok{\#\textgreater{} 2 6.421631 0.000000 4.616177 7.988106 4.871235}
\CommentTok{\#\textgreater{} 3 2.417212 4.616177 0.000000 3.765714 1.366011}
\CommentTok{\#\textgreater{} 4 1.870962 7.988106 3.765714 0.000000 3.607589}
\CommentTok{\#\textgreater{} 5 2.304686 4.871235 1.366011 3.607589 0.000000}
\end{Highlighting}
\end{Shaded}

La Fig. \ref{fig:plot-dist-euclidea} muestra un heatmap\footnote{Representación visual de fácil lectura e interpretación basada en un código de colores, típica del analytics de páginas web. Normalmente proporcionan un patrón visual en forma de''F'', que es el del seguimiento ocular de un sitio o plataforma tecnológica.} de distancias euclideas entre los países de la UE27 a partir deen la Uni las estadísticas de uso de las TIC en 2021.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fviz\_dist}\NormalTok{(}\AttributeTok{dist.obj =}\NormalTok{ d\_euclidea, }\AttributeTok{lab\_size =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/plot-dist-euclidea-1} 

}

\caption{Heatmap de  distancias euclídeas: datos `TIC2021` del paquete `cdr`}\label{fig:plot-dist-euclidea}
\end{figure}

\begin{itemize}
\tightlist
\item
  \textbf{Distancia Manhattan o city block.} \index{distancia!Manhattan} Se define como: \begin{equation}
  d_{MAN}({\bf x}_i;{\bf{x}}_{j})=\sum_{k=1}^{p}\left\vert x_{ik}-x_{jk}\right\vert.
  \end{equation}
\end{itemize}

Viene afectada por los cambios de escala en alguna de las variables y es menos sensible que la distancia euclídea a los valores extremos. Por ello, es recomendable cuando las variables son cuantitativas, con las mismas unidades de medida, sin relaciones entre ellas y con valores extremos.

\begin{itemize}
\tightlist
\item
  \textbf{Distancia de Minkowski.} Se define como:\index{distancia!Minkowski} \begin{equation}
  d_{MIN}({\bf x}_i;{\bf{x}}_{j})=\left(  \sum_{k=1}^{p}\left\vert x_{ik}-x_{jk}\right\vert
  ^{\lambda}\right)  ^{\frac{1}{\lambda}}.
  \end{equation}
\end{itemize}

Las distancias euclídea y Manhattan son casos particulares de la distancia de Minkowski. En la distancia euclídea \(\lambda=2\) y en la Manhattan \(\lambda=1.\)

\begin{itemize}
\tightlist
\item
  \textbf{Norma del supremo o distancia de Chebychev.} Su expresión es: \index{distancia!Chebychev} \begin{equation}
  d_{CHE}({\bf x}_i;{\bf{x}}_{j})=\max_{1\leq k\leq p}\sum_{k=1}^{p}\left\vert x_{ik}%
  -x_{jk}\right\vert.
  \end{equation}
\end{itemize}

Únicamente influye en ella la variable con los valores más extremos y, en este sentido, es muy sensible a los cambios de escala en una de las variables.

\begin{itemize}
\tightlist
\item
  \textbf{Distancia de Mahalanobis.} Se define como: \index{distancia!de Mahalanobis} \begin{equation}
  d_{MAH}=({\bf x}_i;{\bf{x}}_{j})=(\mathbf{x}_{i}-\mathbf{x}_{j})^{\prime}\mathbf{S}^{-1} (\mathbf{x}_{i}-\mathbf{x}_{j})
  \end{equation}
\end{itemize}

Coincide con la distancia euclídea calculada sobre las componentes principales. Es invariante a cambios de origen y de escala (por tanto, se puede sustituir \(\bf S\) por \(\bf R\). Además, tiene en cuenta, explícitamente, las correlaciones lineales que puedan existir entre las variables, corrigiendo así el efecto redundancia. Es, por tanto, es apropiada con variables cuantitativas con relaciones aproximadamente lineales. Su principal desventaja es que \(\bf S\) involucra, conjuntamente, a todos los elementos, y no únicamente, y de forma separada, a los elementos de cada cluster.

\begin{itemize}
\tightlist
\item
  \textbf{Coeficiente de correlación de Pearson.} Se define como:
  \begin{equation}
  d_{P}({\bf x}_i;{\bf{x}}_{j})=\frac{\sum_{k=1}^{p}\left(  x_{ik}-\overline{x}_{i}\right)  \left(  x_{jk}-\overline{x}_{j}\right)  }{\sqrt{\sum_{k=1}%
  ^{p}\left(  x_{ik}-\overline{x}_{i}\right)  ^{2}\sum_{k=1}^{p}\left(
  x_{jk}-\overline{x}_{j}\right)  ^{2}}}.
  \end{equation}
\end{itemize}

No es una distancia sino un indicador de similitud. Por tanto, valores altos indican elementos similares y valores bajos elementos distintos.

Su campo de variación es \([-1,1]\), por lo que se toma su valor absoluto. Cuando las variables están centradas, se denomina coeficiente de congruencia o distancia coseno, puesto que coincide con el coseno formado por los vectores representativos de cada pareja de elementos. Tiene un inconveniente importante: un valor unitario no significa que los dos elementos sean iguales puesto que también pueden obtenerse valores unitarios cuando los valores de las \(p\) variables en uno de los elementos sean combinación lineal de los valores de las \(p\) variables del otro.

Se utiliza, en ocasiones, preferentemente con datos cuantitativos y con el algoritmo de distancia mínima. Los coeficientes de correlación por rangos de Kendall y Spearman se utilizan, también, en casos de variables ordinales.

A efectos prácticos, cambiando el argumento \texttt{method} de la función \texttt{get\_dist} (\texttt{euclidea}, `\texttt{maximum}, \texttt{manhattan}, \texttt{minkowski}, \texttt{pearson}, \texttt{spearman}, \texttt{kendall}) se obtienen distintas matrices de distancias entre los elementos.

\textbf{Variables cualitativas (dicotómicas)}

En este caso, se pueden establecer distintas medidas de similaridad en base a la siguiente tabla de contingencia \(2\times 2\):

\begin{longtable}[]{@{}rlccc@{}}
\toprule()
& & Elem. \(j\) & & \\
\midrule()
\endhead
& & Presencia & Ausencia & Total \\
Elem. \(i\) & Presencia & \(n_{11}\) & \(n_{12}\) & \(n_{1\cdot}\) \\
& Ausencia & \(n_{21}\) & \(n_{22}\) & \(n_{2\cdot}\) \\
& Total & \(n_{\cdot1}\) & \(n_{\cdot2}\) & \(p\) \\
\bottomrule()
\end{longtable}

A partir de la tabla anterior, la similaridad entre dos elementos se puede medir a partir de las coincidencias, ya sea de presencias y ausencias como de solo presencias.

Entre las medidas de similaridad que involucran tanto presencias como ausencias comunes están:

\begin{itemize}
\tightlist
\item
  El \textbf{coeficiente de coincidencias simple}: \(c_{cs}=\frac{(n_{11} + n_{22})} {2}\)
\item
  El \textbf{coeficiente de Rogers-Tanimoto}: \(c_{RT}=\frac{(n_{11} + n_{22})} {2 (n_{11} + n_{22})+ n_{12}+n_{21}}\)
\end{itemize}

Estos dos coeficientes tienen una relación monotónica (si la distancia entre dos elementos es igual o superior a la distancia entre otros dos con una de las medidas, también lo es con la otra). Esto es importante dado que algunos procedimientos de agrupación no se ven afectados por la medida utilizada siempre y cuando el ordenamiento establecido por ellas sea el mismo.

Entre aquellas que identifican similaridad con presencias destacan:

\begin{itemize}
\item
  El \textbf{coeficiente de Jacard}: \(c_J=\frac{n_{11}} {n_{11} + n_{12}+ n_{21}}\)
\item
  El \textbf{coeficiente de Czekanowski}: \(c_{C}=\frac{2 n_{11}} {2n_{11} + n_{12}+ n_{21}}\)
\item
  El \textbf{coeficiente de Sokal y Sneath}: \(c_{SS}=\frac{n_{11} } {n_{11} + 2(n_{12}+n_{21})}\)
\item
  El \textbf{coeficiente de Russell y Rao}: \(c_{RR}=\frac {n_{11}}{p}\)
\end{itemize}

Los tres primeros coeficientes disfutan de la relación de monotonicidad en el sentido anteriormente apuntado, siendo las dos primeros las más utilizados en la práctica.

También se usan como indicadores de similitud las medidas de asociación para tablas \(2\times2\), sobre todo \(Q\) y \(\phi\) (capítulo \ref{medidas}).

\textbf{Variables cualitativas (politómicas)}

Cuando todas las variables sean cualitativas y alguna sea politómica, se generan para estas ultimas tantas variables dicotómicas como categorías tienen, denotando con 1 la presencia y con 0 la ausencia.

\textbf{Variables cuantitativas y cualitativas}

Si las variables no son del mismo tipo, se utiliza la medida de similaridad de Gower:

\begin{equation}
    S_{ij}({\bf x}_i;{\bf{x}}_{j})=\frac{\sum_{k=1}^{p}s_{ij}}{\sum_{k=1}^{p}w_{ij}}
    \end{equation}

donde \(w_{ij}\) vale siempre la unidad salvo para variables binarias si los dos elementos presentan el valor cero. En cuanto al valor de \(S_{ij}\), se distinguen tres casos:

\begin{itemize}
\tightlist
\item
  Variables cualitativas de más de dos niveles: 1 si ambos elementos son iguales en la \emph{k}-ésima variable; 0 si son diferentes.
\item
  Variables dicotómicas: 1 si la variable considerada está presente en ambos elementos; 0 en los demás casos.
\item
  Variables cuantitativas: \(1-\frac {|x_{ik}-x_{jk}|}{R_k}\), donde \(R\) es el rango de la variable \emph{k}.
\end{itemize}

No es recomendable cuando las variables cuantitativas sean muy asimétricas. En este caso, hay dos procedimientos aproximados: \((i)\) calcular medidas separadas para las variables cuantitativas y cualitativas y combinarlas estableciendo algún tipo de de ponderación; \((ii)\) pasar las variables cuantitativas a cualitativas y utilizar las medidas propuestas para este tipo de variables.

\hypertarget{tuxe9cnicas-de-agrupaciuxf3n-jeruxe1rquicas}{%
\section{Técnicas de agrupación jerárquicas}\label{tuxe9cnicas-de-agrupaciuxf3n-jeruxe1rquicas}}

\hypertarget{introac}{%
\subsection{Introducción}\label{introac}}

Una vez se han seleccionado las variables en función de las cuales se van a agrupar en clusters o conglomerados los elementos disponibles, así como se ha decidido qué distancia utilizar para tal propósito, el siguiente paso del AC es la selección de un criterio o técnica de agrupamiento para formar los conglomerados. Dichas técnicas se pueden clasificar en \((i)\) jerárquicas y \((ii)\) no jerárquicas.

\begin{infobox}

\textbf{TÉCNICAS DE CLUSTERIZACIÓN}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Jerárquicas}:

  \begin{itemize}
  \tightlist
  \item
    \emph{Aglomerativas}:

    \begin{itemize}
    \tightlist
    \item
      Vecino más cerano o encadenamiento simple
    \item
      Vecino más lejano o encadenamiento completo
    \item
      Método de la distancia media
    \item
      Método de la distancia entre centroides
    \item
      Método de la mediana
    \item
      Método de Ward
    \item
      Encadenamiento intragrupos
    \item
      Método flexible de Lance y Williams
    \end{itemize}
  \item
    \emph{Divisivas}:

    \begin{itemize}
    \tightlist
    \item
      Vecino más cerano o encadenamiento simple
    \item
      Vecino más lejano o encadenamiento completo
    \item
      Método de la distancia media
    \item
      Método de la distancia entre centroides
    \item
      Método de la mediana
    \item
      Método de Ward
    \item
      Encadenamiento intragrupos
    \item
      Análisis de la asociación
    \item
      Detector automático de interacciones
    \end{itemize}
  \end{itemize}
\item
  \textbf{No jerárquicas}:

  \begin{itemize}
  \tightlist
  \item
    \emph{Técnicas de reasignación}:

    \begin{itemize}
    \tightlist
    \item
      Basadas en centroides: Método de Forgy, \(k\)-medias
    \item
      Basadas en medoides: \(k\)-medoides, PAM, CLARA, CLARANS
    \item
      Basadas en medianas: \(k\)-medianas
    \end{itemize}
  \item
    \emph{Técnicas basadas en la densidad de puntos (mode-seeking)}:

    \begin{itemize}
    \tightlist
    \item
      Aproximación tipológica: Análisis modal, métodos taxmap, de Fortin, de Gitman y Levine, de Catel y Coulter
    \item
      Aproximación probabilística: método de Wolf
    \item
      DBSCAN
    \end{itemize}
  \item
    \emph{Otras técnicas no jerárquicas}

    \begin{itemize}
    \tightlist
    \item
      Métodos directos: block-; bi; co-; two-mode clustering
    \item
      Métodos de reducción de la dimensionalidad: modelos
      Q- y R-factorial
    \item
      Clustering difuso
    \item
      Métodos basados en mixturas de modelos
    \end{itemize}
  \end{itemize}
\end{enumerate}

\end{infobox}

Los procedimientos jerárquicos no particionan el conjunto de elementos de una sola vez, sino que realizan particiones sucesivas a distintos niveles de agrupamiento; es decir, establecen una jerarquía de clusters, de ahí su nombre. Forman los conglomerados, bien agrupando los elementos en grupos cada vez más grandes, fusionando grupos en cada paso, (jerárquicos aglomerativos), o bien desagregándolos en conglomerados cada vez más pequeños (jerárquicos divisivos).

Las técnicas no jerárquicas se caracterizan porque \((i)\) el número de clusters se suele determinar a priori; \((ii)\) utilizan directamente los datos originales, si necesidad de calcular una matriz de distancias o similaridades; y \((iii)\) los clusters resultantes no están anidados unos en otros, sino que están separados. La caja informativa proporciona un detalle mayor de la tipología de técnicas de agrupación a la que nos vamos a referir\footnote{Elaboración propia en base a \citet{kassambara2017}.}. En lo que sigue, nos centramos en las técnicas jerárquicas, abordando las no jerárquicas en el Capítulo \ref{no-jerarquico}.

\hypertarget{tuxe9cnicas-jeruxe1rquicas-aglomerativas}{%
\subsection{Técnicas jerárquicas aglomerativas}\label{tuxe9cnicas-jeruxe1rquicas-aglomerativas}}

Las técnicas jerárquicas aglomerativas, de amplia utilización, parten de tantos conglomerados como elementos y llegan a un único conglomerado final.

Se parte de un conglomerado constituido por los dos elementos más próximos, de tal manera que en la segunda etapa el conglomerado formado actuará a modo de elemento (como si se tuviesen \(n-1\) elementos). En la segunda etapa, de nuevo se agrupan de nuevo los dos elementos más cercanos, que pueden ser dos elementos simples o uno simple y otro compuesto (el conglomerado anterior); en el primer caso se tendrían dos conglomerados (cada uno de ellos formado por dos elementos) y en el segundo, un conglomerado con tres elementos y otro con uno. Sea cual sea el caso, al final de la segunda etapa se tienen \(n-2\) elementos, dos de los cuales son conglomerados. En las etapas siguientes se procede de idéntica manera: agrupación de los dos elementos (sean elementos simples o conglomerados formados en las etapas anteriores) más cercanos, y así sucesivamente hasta formar un único conglomerado integrado por todos los elementos. Es importante resaltar que un elemento, una vez forma parte de un conglomerado, ya no sale de él.

La pregunta que surge en este momento es: en el proceso de agrupamiento descrito, ¿cómo se mide la distancia de un elemento a un conglomerado, o entre dos conglomerados?\footnote{El criterio de inclusión de los elementos en dichos conglomerados citado en \ref{origen-cluster}.} Los métodos más populares son los siguientes:

\begin{itemize}
\tightlist
\item
  \textbf{Método del encadenamiento simple o vecino más cercano.} \index{encadenamiento simple} \index{vecino más cercano}
\end{itemize}

Utiliza el criterio de ``\emph{la distancia mas cercana}''. Por tanto, \((i)\) la distancia entre un elemento y un conglomerado es la menor de las distancias entre dicho elemento y cada uno de los elementos del conglomerado; \((ii)\) la distancia entre dos conglomerados viene dada por la distancia entre sus dos elementos más cercanos. Una vez computada la matriz de distancias se seleccionan los conglomerados más cercanos.

\begin{itemize}
\tightlist
\item
  \textbf{Método del encadenamiento completo o vecino más lejano.}
\end{itemize}

Funciona igual que el anterior, \index{encadenamiento completo} \index{vecino más lejano} pero ahora el criterio es ``\emph{la distancia más lejana}''.

Nótese que mientras que con el método del vecino más cerano la distancia entre los elementos más próximos de un cluster es siempre menor que la distancia entre elementos de distintos clusters, con el criterio del vecino más lejano la distancia entre los dos elementos más alejados de un cluster es siempre menor que la distancia entre cualquiera de sus elementos y los elementos más alejados de los demás clusters. Nótese también que mientras que el método del vecino más lejano tiende a separar a los individuos en menor medida que la indicada por sus disimilaridades iniciales (es espacio-contractivo), el criterio del vecino más lejano es espacio-dilatante, es decir, tiende a separar a los individuos en mayor medida que la indicada por sus disimilaridades iniciales \citep{gallardoyvera2004}.

\begin{itemize}
\tightlist
\item
  \textbf{Método de la distancia media.}
\end{itemize}

Surge como una solución a la constricción o dilatación del espacio que provocan los dos métodos anteriores (por eso se dice que es espacio-conservativo y es muy utilizado), utilizando ``\emph{la distancia promedio}'' \index{distancia!media}, es decir, la distancia entre un elemento y un conglomerado es la media aritmética de las distancias de dicho elemento a cada uno de los elementos del conglomerado. En caso de dos conglomerados, la distancia entre ellos viene dada por el promedio aritmético de las distancias, dos a dos, un elemento de cada conglomerado. Igual que los dos métodos precedentes, es invariante a transformaciones monótonas de la distancia utilizada.

En la Fig. \ref{fig:cluster-compara} se puede ver la constricción, dilatación y conservación del espacio que producen los métodos del vecino más cercano, más lejano y de la distancia media, respectivamente. En este caso se utiliza como representación gráfica el dendrograma(diagrama de árbol). En figuras posteriores se utilizarán otras alternativas al dendrograma, con el objetivo de mostrar las más populares.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(factoextra)}
\NormalTok{hc\_simple }\OtherTok{\textless{}{-}} \FunctionTok{hcut}\NormalTok{(tic, }\AttributeTok{k =} \DecValTok{3}\NormalTok{, }\AttributeTok{hc\_method =} \StringTok{"single"}\NormalTok{)}
\NormalTok{hc\_completo }\OtherTok{\textless{}{-}} \FunctionTok{hcut}\NormalTok{(tic, }\AttributeTok{k =} \DecValTok{3}\NormalTok{, }\AttributeTok{hc\_method =} \StringTok{"complete"}\NormalTok{)}
\NormalTok{hc\_promedio }\OtherTok{\textless{}{-}} \FunctionTok{hcut}\NormalTok{(tic, }\AttributeTok{k =} \DecValTok{3}\NormalTok{, }\AttributeTok{hc\_method =} \StringTok{"average"}\NormalTok{)}

\NormalTok{d1 }\OtherTok{\textless{}{-}} \FunctionTok{fviz\_dend}\NormalTok{(hc\_simple, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{k =} \DecValTok{3}\NormalTok{, }\AttributeTok{main =} \StringTok{"Vecino más cercano"}\NormalTok{)}
\NormalTok{d2 }\OtherTok{\textless{}{-}} \FunctionTok{fviz\_dend}\NormalTok{(hc\_completo, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{k =} \DecValTok{3}\NormalTok{, }\AttributeTok{main =} \StringTok{"Vecino más lejano"}\NormalTok{)}
\NormalTok{d3 }\OtherTok{\textless{}{-}} \FunctionTok{fviz\_dend}\NormalTok{(hc\_promedio, }\AttributeTok{cex =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{k =} \DecValTok{3}\NormalTok{, }\AttributeTok{main =} \StringTok{"Distancia promedio"}\NormalTok{)}

\FunctionTok{library}\NormalTok{(patchwork)}
\NormalTok{d1 }\SpecialCharTok{+}\NormalTok{ d2 }\SpecialCharTok{+}\NormalTok{ d3}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/cluster-compara-1} 

}

\caption{Clusterización jerárquica con distancias euclídeas (dendrograma): métodos del vecino más cercano, vecino más lejano y distancia media}\label{fig:cluster-compara}
\end{figure}

\begin{itemize}
\tightlist
\item
  \textbf{Método de la distancia entre centroides.}
\end{itemize}

Según este método\index{distancia!entre centroides}, la distancia entre dos grupos o conglomerados es la distancia entre sus centroides, entendiendo por centroide del grupo \(g\): \(c_{g}=\left(\overline{x}_{1g},\overline{x}_{2g},...,\overline{x}_{pg}\right),\) donde \(\overline{x}_{jg}\) es la media de la \(j\)-ésima variable en dicho grupo.

Igual que el método de la media, este método es también espacio-conservativo. Sin embargo, tiene la limitación de que cuando se agrupan dos conglomerados de diferente tamaño, el conglomerado resultante queda más cerca del conglomerado mayor y más alejado del menor, de forma proporcional a la diferencia de tamaños, lo que lleva a que a lo largo del proceso de clusterización se vayan perdiendo las propiedades de los conglomerados pequeños \citep{gallardo2022}.

\begin{itemize}
\tightlist
\item
  \textbf{Método de la mediana.}
\end{itemize}

\index{método de la mediana} Viene a superar la limitación del método del centroide. Para ello, la estrategia natural es suponer que los grupos son de igual tamaño. Dicha estrategia se plasma en suponer que la distancia entre un elemento (o un conglomerado, \(k\)) y el conglomerado formado por la agrupación de los conglomerados \(i\) y \(j\) viene dada por la mediana del triángulo formado por sus centroides (de ahí su nombre). Se trata de un método espacio conservativo, pero, igual que el método del centroide, no es invariante a transformaciones monótonas de la distancia utilizada.

La Fig. \ref{fig:cluster-compara2}, un tanglegrama o diagrama de laberinto, muestra las agrupaciones producidas por los métodos del centroide y la mediana. En ella se puede observar como el método de la mediana corrije la limitación del método del centroide.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dendextend)}
\FunctionTok{library}\NormalTok{(cluster)}
\NormalTok{hc\_cent\_dend }\OtherTok{\textless{}{-}} \FunctionTok{as.dendrogram}\NormalTok{(}\FunctionTok{hclust}\NormalTok{(d\_euclidea, }\AttributeTok{method =} \StringTok{"centroid"}\NormalTok{))}
\NormalTok{hc\_med\_dend }\OtherTok{\textless{}{-}} \FunctionTok{as.dendrogram}\NormalTok{(}\FunctionTok{hclust}\NormalTok{(d\_euclidea, }\AttributeTok{method =} \StringTok{"median"}\NormalTok{))}
\FunctionTok{tanglegram}\NormalTok{(hc\_cent\_dend, hc\_med\_dend)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/cluster-compara2-1} 

}

\caption{Clusterización jerárquica con distancias euclídeas (tanglegrama): método del centroide vs. método de la mediana}\label{fig:cluster-compara2}
\end{figure}

\begin{itemize}
\tightlist
\item
  \textbf{Método de Ward.}
\end{itemize}

El método de Ward \index{método de Ward} agrupa, en cada etapa, los dos clusters que producen el menor incremento de la varianza total intra-cluster: \(W=\sum_g\sum_{i \in g} (x_{ig}- \bar{x}_g)^{\prime} (x_{ig}- \bar{x}_g)\), donde \(\bar{x}_g\) es el centroide del grupo \(g\). Así, los grupos formados no distorsionan los datos originales.\footnote{Específicamente, la propuesta de Ward es que la pérdida de información que se produce al integrar los distintos individuos en clusters sea la mínima posible.}

Es muy utilizado en la práctica, dado que tiene casi todas las ventajas del método de la media y suele ser más discriminatorio en la determinación de los niveles de agrupación. También suele crear conglomerados muy compactos de tamaño similar. Dado que el menor incremento de \(W\) es proporcional a la distancia euclidea al cuadrado entre los centroides de los grupos fusionados, \(W\) no es decreciente, solventándose los problemas de los otros métodos basados en centroides.

La Fig. \ref{fig:plot-igraph}, muestra el filograma, diagrama filético en forma de árbol filogenético, generado por la librería \texttt{igraph} con el método de agrupación de Ward.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"igraph"}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{5665}\NormalTok{)}
\NormalTok{hc\_ward }\OtherTok{\textless{}{-}} \FunctionTok{hcut}\NormalTok{(tic, }\AttributeTok{k =} \DecValTok{3}\NormalTok{, }\AttributeTok{hc\_method =} \StringTok{"ward.D2"}\NormalTok{)}
\FunctionTok{fviz\_dend}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ hc\_ward,}
  \AttributeTok{k =} \DecValTok{3}\NormalTok{,}
  \AttributeTok{type =} \StringTok{"phylogenic"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/plot-igraph-1} 

}

\caption{Clusterización jerárquica con distancias euclídeas al cuadrado (filograma): método de Ward}\label{fig:plot-igraph}
\end{figure}

\begin{itemize}
\tightlist
\item
  \textbf{Método del encadenamiento intragrupos.}
\end{itemize}

Según el método de la distancia promedio (o vinculación entre grupos)\index{método de vinculación entre grupos} la distancia entre dos conglomerados se obtenía calculando las distancias de cada elemento de uno de los grupos con todos los del otro y computando, posteriormente, la media aritmética de dichas distancias. Con el método de la vinculación intragrupos se computa la distancia media entre la totalidad de los elementos de los conglomerados susceptibles de agrupación, con independencia de si pertenecen al mismo conglomerado inicial o a distinto conglomerado. Por ejemplo: si un conglomerado está formado por los elementos \(a\) y \(b\), y otro por los elementos \(c\) y \(d\), la distancia intergrupos\index{distancia intragrupos} entre los dos conglomerados es:

\[d_{intergrupos}=\frac{d_{(a;c)}+d_{(a;d)}+d_{(b;c)}+d_{(b;d)}}{4}\] mientras que la distancia intragrupos vendrá dada por la media de las distancias entre los elementos \(a,b,c\) y \(d\): \[d_{intergrupos}=\frac{d_{(a;b)}+d_{(a;c)}+d_{(a;d)}+d_{(b;c)}+d_{(b;d)}
     +d_{(c;d)}}{6}\]

\begin{itemize}
\tightlist
\item
  \textbf{Método flexible de Lance y Williams.}
\end{itemize}

Calcula la distancia entre dos conglomerados (el primero formado por la unión de otros dos en la etapa previa) a partir de la siguiente expresión: \[d_{\left(  g_{1}\cup g_{2}\right); g_{3}}=\alpha_{1}d_{(g_{1};g_{3})}
+\alpha_{2}d_{(g_{2};g_{3})}+\beta d_{(g_{1};g_{2})}+\gamma\left\vert
d_{(g_{1};g_{2})}-d_{(g_{2};g_{2})}\right\vert,\] donde \(\alpha_{1}+\alpha_{2}+\beta=1; \alpha_{1}=\alpha_{2};\beta<1;\gamma=0\), si bien Lance y Williams sugieren adicionalmente un pequeño valor negativo de \(\beta\). Por ejemplo \(\beta =-0,25\).

Los métodos anteriormente expuestos son casos particulares de éste. Denominando \(n_1\), \(n_2\) y \(n_3\) a los tamaños de los grupos \(g_1\), \(g_2\) y \(g_3\), respectivamente, se tiene:

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3846}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1538}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1538}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1538}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1538}}@{}}
\caption{Valores de \(\alpha_1\), \(\alpha_2\), \(\beta\) y \(\gamma\) para distintos procedimientos de agrupación}\tabularnewline
\toprule()
\begin{minipage}[b]{\linewidth}\centering
Método
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\alpha_1\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\alpha_2\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\beta\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\gamma\)
\end{minipage} \\
\midrule()
\endfirsthead
\toprule()
\begin{minipage}[b]{\linewidth}\centering
Método
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\alpha_1\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\alpha_2\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\beta\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\gamma\)
\end{minipage} \\
\midrule()
\endhead
Vecino más cercano & 0,5 & 0,5 & 0 & -0,5 \\
Vecino más lejano & 0,5 & 0,5 & 0 & 0,5 \\
Distancia media & \(\frac{n_1}{n_1+n_2}\) & \(\frac{n_2}{n_1+n_2}\) & 0 & 0 \\
Distancia entre centroides & \(\frac{n_1}{n_1+n_2}\) & \(\frac{n_2}{n_1+n_2}\) & \(\frac{-n_1 n_2} {(n_1+n_2)^2}\) & 0 \\
Método de la mediana & 0,5 & 0,5 & -0,25 & 0 \\
Ward & \(\frac {n_1+n_3} {n_1+n_2+n_3}\) & \(\frac {n_2+n_3} {n_1+n_2+n_3}\) & \(\frac{-n_3} {n_1+n_2+n_3}\) & 0 \\
Flexible & \(0,5(1-\beta)\) & \(0,5(1-\beta)\) & \(\beta\) & 0 \\
\bottomrule()
\end{longtable}

\hypertarget{tuxe9cnicas-jeruxe1rquicas-divisivas}{%
\subsection{Técnicas jerárquicas divisivas}\label{tuxe9cnicas-jeruxe1rquicas-divisivas}}

En este caso, la secuencia de acontecimientos es justo la inversa. Se parte de un único conglomerado formado por todos los elementos y se llega a \(n\) conglomerados formados cada uno de ellos por un único elemento (a veces el proceso termina cuando se llega a un número de grupos preestablecido). Ahora bien, dado que ahora se trata de subividir conglomerados, es decir, de identificar los elementos más distantes, o menos similares, para separarlos del resto del conglomerado, la estrategia a seguir estará basada en maximizar las distancias (o minimizar las similitudes). En el proceso disociativo surge una cuestión importante: cuándo debe dejar de dividirse un cluster determinado y pasar a dividir otro, cuestión que se resuelve por el procedimiento propuesto por \citet{macnaughton_et_al1964}. Las técnicas divisivas (también llamadas partitivas o disociativas), pueden ser monotéticas o politéticas. En el primer caso, las divisiones se basan en una sola característica o atributo. En el segundo, se tienen en cuenta todas.

Las técnicas divisivas son menos populares que las aglomerativas. Sin embargo, la probabilidad de que lleven a decisiones equivocadas (debido a la variabilidad estadística de los datos) en las etapas iniciales del proceso, lo cual distorsionaría el resultado final del mismo, es menor que en las aglomerativas. En este sentido, los métodos partitivos, al partir del total de elementos, se consideran más seguros que los aglomerativos. Los métodos disociativos más populares son los siguientes:

\textbf{Método de la distancia promedio}

Dentro de las técnicas politéticas, entre las que se cuentan todas las vistas en la clusterización jerárquica aglomerativa, quizás la más popular es la que utiliza para la partición el método de la distancia promedio. Para ilustrarla, supóngase que se tienen 5 elementos y que su matriz de distancias es la siguiente:

\[\bf X=\left(\begin{matrix} .&.&.&.&.\\
8&.&.&.&.\\
7&4&.&.&.\\
6&1&4&.&.\\
3&4&5&4&.
\end {matrix}\right)\]

En la primera etapa hay que dividir el grupo de cinco elementos en dos conglomerados. Hay \(2^{2n-1}-1\) posibilidades, pero según el método de la distancia promedio, se calcula la distancia de cada elemento a los demás y se promedia, desgajándose el elemento con distancia promedio máxima. En nuestro caso, se desgajaría el primer elemento, y en la segunda etapa se partiría de dos grupos: \(\{e_1 \}\) y \(\{e_2, e_3, e_4, e_5\}\).

A partir de la segunda etapa, se procede como sigue (véase Tabla 2.2):

\begin{itemize}
\tightlist
\item
  \((i)\) Se calculan las (4) distancias promedio de cada elemento del conglomerado principal al elemento desgajado;
\item
  \((ii)\) Se calculan las (4) distancias promedio de cada elemento del conglomerado principal al resto de elementos del mismo;
\item
  \((iii)\) Se computan las diferencias \((i)-(ii)\) para cada uno de los 4 elementos del conglomerado principal;
\item
  \((iv)\) De entre aquéllos elementos del grupo principal en los que \((i)-(ii)<0\) se selecciona aquél para el cual es máxima. Tras esta segunda etapa los conglomerados son \(\{e_1, e_5\}\) y \(\{e_2, e_3, e_4\}\).
\end{itemize}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2603}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}@{}}
\caption{Distancias entre conglomerados: segunda etapa}\tabularnewline
\toprule()
\begin{minipage}[b]{\linewidth}\centering
Elemento
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Distancia promedio al grupo desgajado \(\{e_1\}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Distancia promedio al grupo principal
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Diferencia
\end{minipage} \\
\midrule()
\endfirsthead
\toprule()
\begin{minipage}[b]{\linewidth}\centering
Elemento
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Distancia promedio al grupo desgajado \(\{e_1\}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Distancia promedio al grupo principal
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Diferencia
\end{minipage} \\
\midrule()
\endhead
\(\{e_2\}\) & 8 & 3 & 5 \\
\(\{e_3\}\) & 7 & 3 & 4 \\
\(\{e_4\}\) & 6 & 3 & 3 \\
\(\{e_5\}\) & 3 & 4,33 & -1,33 \\
\bottomrule()
\end{longtable}

En las siguientes etapas se procede de igual manera hasta que todas las diferencias son positivas (en nuestro caso esto ocurre en la tercera etapa; véase Tabla 2.3).

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2603}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}@{}}
\caption{Distancias entre conglomerados: tercera etapa}\tabularnewline
\toprule()
\begin{minipage}[b]{\linewidth}\centering
Elemento
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Distancia promedio al grupo desgajado \(\{e_1,e_5\}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Distancia promedio al grupo principal
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Diferencia
\end{minipage} \\
\midrule()
\endfirsthead
\toprule()
\begin{minipage}[b]{\linewidth}\centering
Elemento
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Distancia promedio al grupo desgajado \(\{e_1,e_5\}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Distancia promedio al grupo principal
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Diferencia
\end{minipage} \\
\midrule()
\endhead
\(\{e_2\}\) & 6 & 2,5 & 3,5 \\
\(\{e_3\}\) & 6 & 4 & 2 \\
\(\{e_4\}\) & 5 & 2,5 & 2,5 \\
\bottomrule()
\end{longtable}

Cuando esto ocurre, es decir, cuando todos los elementos del conglomerado principal están más cerca de los demás que lo componen que de los del conglomerado disociado, se vuelve a iniciar el algoritmo, pero esta vez para cada uno de los dos conglomerados generados \citep{macnaughton_et_al1964}. En nuestro caso, en \(\{e_1, e_5\}\) la única partición posible es \(\{e_1\}\), \(\{e_5\}\). En \(\{e_2, e_3, e_4\}\) se desgaja el elemento con mayor distancia promedio a los demás del grupo. Como \(\frac{d_{(2,3)}+ d_{(2,4)}}{2}=2,5\), \(\frac{d_{(3,2)}+ d_{(3,4)}}{2}=4\) y \(\frac {d_{(4,2)} + d_{(4,3)}} {2}=2,5\), se desgaja \(\{e_3\}\).

A continuación se aplica el algoritmo anteriormente expuesto a cada elemento del grupo principal \(\{e_2, e_4\}\) y \(\{e_3\}\) (Tabla 2.4), y como todas las distancias son positivas, se divide \(\{e_2, e_4\}\) en \(\{e_2 \}\) y \(\{e_4\}\).

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2603}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2466}}@{}}
\caption{Distancia entre conglomerados: etapa final}\tabularnewline
\toprule()
\begin{minipage}[b]{\linewidth}\centering
Elemento
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Distancia promedio al grupo desgajado \(\{e_3\}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Distancia promedio al grupo principal
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Diferencia
\end{minipage} \\
\midrule()
\endfirsthead
\toprule()
\begin{minipage}[b]{\linewidth}\centering
Elemento
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Distancia promedio al grupo desgajado \(\{e_3\}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Distancia promedio al grupo principal
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Diferencia
\end{minipage} \\
\midrule()
\endhead
\(\{e_2\}\) & 4 & 1 & 3 \\
\(\{e_4\}\) & 4 & 1 & 3 \\
\bottomrule()
\end{longtable}

El algoritmo DIvisive ANAlysis (DIANA) permite llevar a cabo la partición anterior utilizando el diámetro de los clusters para decidir el orden de partición clusters cuando se tienen varios con más de un elemento (véase capítulo 6 de \citet{kauf_1990}). Proporciona \((i\)) el coeficiente divisivo (véase \texttt{diana.object}) que mide la cantidad de estructura de agrupamiento encontrada; y \((ii\)) la pancarta, una novedosa presentación gráfica (véase \texttt{plot.diana}).

Para el ejemplo TIC, DIANA proporciona el dendrograma circular de la Fig. \ref{fig:diana}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# compute divisive hierarchical clustering}
\FunctionTok{library}\NormalTok{(cluster)}
\NormalTok{hc\_diana }\OtherTok{\textless{}{-}} \FunctionTok{diana}\NormalTok{(tic, }\AttributeTok{metric =} \StringTok{"euclidea"}\NormalTok{)}
\CommentTok{\# Divise coefficient. \#los valores más cercanos a 1 sugieren una estructura de agrupación fuerte}
\NormalTok{hc\_diana}\SpecialCharTok{$}\NormalTok{dc}
\CommentTok{\#\textgreater{} [1] 0.8043393}

\FunctionTok{library}\NormalTok{(factoextra)}
\FunctionTok{fviz\_dend}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ hc\_diana,}
  \AttributeTok{k =} \DecValTok{3}\NormalTok{,}
  \AttributeTok{type =} \StringTok{"circular"}\NormalTok{,}
  \AttributeTok{ggtheme =} \FunctionTok{theme\_minimal}\NormalTok{()}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/diana-1} 

}

\caption{Clusterización jerárquica divisiva con DIANA}\label{fig:diana}
\end{figure}

\textbf{Análisis de la asociación}

En caso de que los elementos vengan caracterizados por variables cualitativas o factores dicotómicos, \(F_1,F_2,..., F_n\) (si alguno fuese politómico, cada una de sus categorías se consideraría como un factor dicotómico), el método del análisis de la asociación (o suma de estadísticos chi-cuadrado) es una técnica monotética muy utilizada que procede como sigue:

\begin{itemize}
\tightlist
\item
  \((i)\) Considérese \(F_1\) y divídase el conjunto de elementos en dos grupos o categorías: uno con los elementos en los que \(F_1\) esté presente y otro con aquellos en los que esté ausente. Hágase lo mismo con los demás factores.
\item
  \((ii)\) Constrúyanse las \(n\times(n-1)\) tablas de contingencia \(2\times2\) que cruzan cada factor con cada uno de los restantes (véase Capítulo \ref{notac}).
\end{itemize}

\begin{longtable}[]{@{}rrccc@{}}
\toprule()
\endhead
& & Presencia & Factor \emph{j} & \\
& & SI & NO & \textbf{Total} \\
Presencia & SI & \(n_{11}\) & \(n_{21}\) & \(n_{1\cdot}\) \\
Factor \emph{i} & NO & \(n_{21}\) & \(n_{22}\) & \(n_{2\cdot}\) \\
& \textbf{Total} & \(n_{\cdot1}\) & \(n_{\cdot2}\) & \textbf{n} \\
\bottomrule()
\end{longtable}

donde \(i\neq j\).

\begin{itemize}
\tightlist
\item
  \((iii)\) Calcúlese el estadístico chi-cuadrado (\(\chi_{ij}^2=\frac {n(n_{11}n_{22}-n_{12}n_{21})^2}{n_{1\cdot} n_{2\cdot}n_{\cdot1}n_{\cdot2}}\) para una de dichas tablas (véase Capítulo \ref{dise} y compútese \(\sum_{i\neq j}\chi_{ij}^2\).
\item
  \((iii)\) Desgájese del conglomerado inicial en dos: uno con los elementos que contienen el factor con la máxima \(\sum_{i\neq j}\chi_{ij}^2\); y otro con el resto de los elementos (donde dicho factor está ausente).
\item
  \((iv)\) Procédase así iterativamente.
\end{itemize}

\textbf{Método del detector automático de interacciones (AID)}

No es propiamente un método de AC, sino de la esfera de los modelos lineales de rango no completo. Sin embargo, se menciona, siquiera mínimamente, porque se utiliza en algunas ocasiones con la finalidad combinar categorías de los factores utilizados con la finalidad de generar grupos que difieran lo más posible entre sí respecto de los valores de una variable dependiente medida en una escala métrica (con una escala proporcional o de intervalo) o ficticia (dicotómica con valores 0 y 1).
Específicamente, el AID procede con un ANOVA entre las categorías de la variable independiente, que maximiza la varianza secuencial que se realiza mediante divisiones dicotómicas de la variable dependiente que busca en cada etapa la partición intergrupos y minimiza la varianza intragrupos. La agrupación de categorías se efectúa probando todas las combinaciones binarias posibles de las variables. Se utiliza un test \(F\) para seleccionar las mayores diferencias posibles.
En este algoritmo, el proceso de subdivisión del conjunto de elementos en grupos dicotómicos continúa hasta que se verifica algún criterio de parada.

Las limitaciones más importantes del AID son las siguientes:

\begin{itemize}
\tightlist
\item
  Tiende a seleccionar como más explicativas las variables con mayor número de categorías. Por eso no conviene utilizarlo cuando las variables explicativas difieran mucho en el número de categorías.
\item
  Las particiones resultantes dependen de la variable que elegida en primer lugar, condicionando las sucesivas particiones.
\item
  Su naturaleza exclusivamente dicotómica también es una limitación importante. Si se llevasen a cabo particiones con tres o más ramas producirían una mayor reducción de la varianza residual y, además, permitirían una mejor selección de otras variables.
\end{itemize}

El AID basado en tablas de contingencia y el estadístico chi-cuadrado (CHAID) corrige la mayoría de estas limitaciones. Aunque inicialmente fue diseñado para variables categóricas, posteriormente se incluyó la posibilidad de trabajar con variables categóricas nominales, categóricas ordinales y variables continuas, permitiendo generar tanto árboles de decisión, para resolver problemas de clasificación, como árboles de regresión. Además, los nodos se pueden dividir en más de dos ramas.

\hypertarget{calidad-de-la-agrupaciuxf3n-y-nuxfamero-de-clusters}{%
\section{Calidad de la agrupación y número de clusters}\label{calidad-de-la-agrupaciuxf3n-y-nuxfamero-de-clusters}}

\hypertarget{el-coeficiente-de-correlaciuxf3n-lineal-cofenuxe9tico}{%
\subsection{El coeficiente de correlación lineal cofenético}\label{el-coeficiente-de-correlaciuxf3n-lineal-cofenuxe9tico}}

Dado que las técnicas jerárquicas imponen una estructura sobre los datos y pueden producir distorsiones significativas en las relaciones entre los datos originales, una vez realizada la jerarquización de los elementos objeto de clusterización, surge la siguiente pregunta: ¿en qué medida la estructura final obtenida representa las similitudes o diferencias entre dichos objetos? En otros términos, ¿en qué medida el dendrograma representa la matriz de distancias o similitudes original?

El coeficiente de correlación lineal cofenético da respuesta a dichas preguntas. Se define como el coeficiente de correlación lineal entre los \(n(n-1)\) elementos del triangulo superior de la matriz de distancias o similitudes y sus homónimos en la matriz cofenética, \(\bf C\), cuyos elementos \(\{c_{ij}\}\) son las distancias o similitudes entre los elementos \((i,j)\) tras la aplicación de la técnica de jerarquización. Obviamente, se utilizará la técnica jerárquica que origine el mayor coeficiente.

En el ejemplo TIC, el mayor coeficiente cofenético corresponde al método del promedio o del centroide, si bien la magnitud de los correspondientes a otras técnicas de agregación es bastante parecida.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# comparamos con la distancia euclidea: d\_euclidea}
\NormalTok{cof\_simp }\OtherTok{\textless{}{-}} \FunctionTok{cophenetic}\NormalTok{(hc\_simple)}
\NormalTok{cof\_comp }\OtherTok{\textless{}{-}} \FunctionTok{cophenetic}\NormalTok{(hc\_completo)}
\NormalTok{cof\_prom }\OtherTok{\textless{}{-}} \FunctionTok{cophenetic}\NormalTok{(hc\_promedio)}
\NormalTok{cof\_ward }\OtherTok{\textless{}{-}} \FunctionTok{cophenetic}\NormalTok{(hc\_ward)}
\NormalTok{cof\_dia }\OtherTok{\textless{}{-}} \FunctionTok{cophenetic}\NormalTok{(hc\_diana)}
\NormalTok{coef\_cofeneticos }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(d\_euclidea, cof\_simp, cof\_comp, cof\_prom, cof\_dia, cof\_ward)}

\FunctionTok{round}\NormalTok{(}\FunctionTok{cor}\NormalTok{(coef\_cofeneticos)[}\DecValTok{1}\NormalTok{, ], }\DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} d\_euclidea   cof\_simp   cof\_comp   cof\_prom    cof\_dia   cof\_ward }
\CommentTok{\#\textgreater{}       1.00       0.71       0.61       0.77       0.65       0.60}
\end{Highlighting}
\end{Shaded}

\hypertarget{nuxfamero-uxf3ptimo-de-clusters}{%
\subsection{Número óptimo de clusters}\label{nuxfamero-uxf3ptimo-de-clusters}}

Acabado el procedimiento de clusterización de los \(n\) elementos disponibles, sea por un procedimiento jerárquico aglomerativo o divisivo, hay que tomar una decisión sobre el número de óptimo de clusters. Esta decisión es ardua y requiere un delicado equilibrio. Valores grandes de \(k\) pueden mejorar la homogeneidad de los clusters; sin embargo, se corre el riesgo de sobreajuste. Lo contrario ocurre con un \(k\) pequeño.

Para tomar esta decisión, además del sentido común y el conocimiento que se tenga del fenómeno en estudio, se puede echar mano de distintos procedimientos heurísticos. El primero se basa en el \textbf{dendrograma} y, en concreto, en la representación de las distintas etapas del algoritmo y las distancias a la que se producen las agrupaciones o particiones de los clusters. Para cada distancia, el dendrograma produce un numero determinado de clusters que aumenta (o disminuye) con la misma. Por tanto, el número de clusters dependerá de la distancia a la que se corte el dendrograma (eje de abcisas del dendrograma, height). Dicha distancia debería elegirse de tal forma que los conglomerados estuviesen bien determinados y fuesen interpretables. En las primeras etapas del proceso las distancias no varían mucho, pero en las etapas intermedias y, sobre todo, finales, las distancias aumentan mucho entre dos etapas consecutivas. Por ello, se suele cortar el dendrograma a la distancia a la cual las distancias entre dos etapas consecutivas del proceso empiecen a ser muy grandes, indicador de que los grupos empiezan a ser muy distintos. Otra posibilidad es utilizar el \textbf{gráfico de sedimentación} (\ref{numcomp}, que relaciona la variablidad entre clusters (eje de ordenadas) con el el número de clusters (eje de abscisas). Normalmente, decrece bruscamente al principio, y posteriormente más despacio, hasta llegar a la parte de sedimentación (el codo del gráfico), donde el decrecimiento es muy lento. Pues bien, el número óptimo de conglomerados es el correspondiente al codo o comienzo del área de sedimentación del gráfico.

El algoritmo del gráfico de sedimentación es como sigue:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Clusterícese variando el número de grupos, \(k\), por ejemplo, de 1 a 10.
\item
  Para cada valor de \(k\), compútese la suma de cuadrados intragrupo (WSS).
\item
  Trácese la gráfica de WSS vs.~\(k\).
\item
  Determínese el número óptimo de grupos.
\end{enumerate}

Con conjuntos de datos de tamaño pequeño a moderado, este proceso se puede realizar convenientemente con \texttt{factoextra::fviz\_nbclust()}.

Otra opción es el \emph{ancho de silueta promedio}.El coeficiente o ancho de silueta compara, por cociente, la distancia media a elementos en el mismo grupo con la distancia media a elementos en otros grupos.

Este método calcula el ancho de silueta promedio (avg.sil.wid.) de los elementos objeto de agrupación para diferentes valores de \(k\). Como un valor alto del ancho promedio indica una buena agrupación, el número óptimo de conglomerados es el que lo maximiza. El campo de variación del ancho de silueta es {[}-1, 1{]}, donde 1 significa que los elementos están muy cerca de su propio clúster y lejos de otros clústeres, mientras que -1 indica que están cerca de los clústeres vecinos.

El \textbf{criterio del Gap (brecha)}, similar al método del codo, tiene como finalidad encontrar la mayor diferencia o distancia que entre los diferentes grupos de elementos que se van formando en el proceso de clusterización y que se representan normalmente en un dendrograma. Se computan las distancias de cada uno de los enlaces que forman el dendrograma y se observa cuál es la mayor de ellas. El máximo del gráfico de estas diferencias vs.~el número de clusters indica el número óptimo de clusters.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Plot cluster results}
\NormalTok{p1 }\OtherTok{\textless{}{-}} \FunctionTok{fviz\_nbclust}\NormalTok{(tic,}
  \AttributeTok{FUN =}\NormalTok{ hcut, }\AttributeTok{method =} \StringTok{"wss"}\NormalTok{,}
  \AttributeTok{k.max =} \DecValTok{10}
\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Elbow"}\NormalTok{)}
\NormalTok{p2 }\OtherTok{\textless{}{-}} \FunctionTok{fviz\_nbclust}\NormalTok{(tic,}
  \AttributeTok{FUN =}\NormalTok{ hcut, }\AttributeTok{method =} \StringTok{"silhouette"}\NormalTok{,}
  \AttributeTok{k.max =} \DecValTok{10}
\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Silhouette"}\NormalTok{)}
\NormalTok{p3 }\OtherTok{\textless{}{-}} \FunctionTok{fviz\_nbclust}\NormalTok{(tic,}
  \AttributeTok{FUN =}\NormalTok{ hcut, }\AttributeTok{method =} \StringTok{"gap\_stat"}\NormalTok{,}
  \AttributeTok{k.max =} \DecValTok{10}
\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Gap"}\NormalTok{)}

\FunctionTok{library}\NormalTok{(patchwork)}
\NormalTok{p1 }\SpecialCharTok{+}\NormalTok{ p2 }\SpecialCharTok{+}\NormalTok{ p3}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/plot-kresults-hc-1} 

}

\caption{Métodos heurísticos para la determinación del número óptimo de clusters}\label{fig:plot-kresults-hc}
\end{figure}

Finalmente, el \textbf{índice de Dunn} es el cociente entre la mínima distancia intergrupos y la máxima distancia intragrupos. A mayor índice, mayor calidad de clusterización.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(clValid)}
\NormalTok{cut2\_hc\_prom }\OtherTok{\textless{}{-}} \FunctionTok{cutree}\NormalTok{(hc\_promedio, }\AttributeTok{k =} \DecValTok{2}\NormalTok{)}
\NormalTok{cut3\_hc\_prom }\OtherTok{\textless{}{-}} \FunctionTok{cutree}\NormalTok{(hc\_promedio, }\AttributeTok{k =} \DecValTok{3}\NormalTok{)}
\NormalTok{cut4\_hc\_prom }\OtherTok{\textless{}{-}} \FunctionTok{cutree}\NormalTok{(hc\_promedio, }\AttributeTok{k =} \DecValTok{4}\NormalTok{)}
\NormalTok{cut5\_hc\_prom }\OtherTok{\textless{}{-}} \FunctionTok{cutree}\NormalTok{(hc\_promedio, }\AttributeTok{k =} \DecValTok{5}\NormalTok{)}

\FunctionTok{dunn}\NormalTok{(d\_euclidea, cut2\_hc\_prom)}
\CommentTok{\#\textgreater{} [1] 0.4465593}
\FunctionTok{dunn}\NormalTok{(d\_euclidea, cut3\_hc\_prom)}
\CommentTok{\#\textgreater{} [1] 0.3751942}
\FunctionTok{dunn}\NormalTok{(d\_euclidea, cut4\_hc\_prom)}
\CommentTok{\#\textgreater{} [1] 0.4074884}
\FunctionTok{dunn}\NormalTok{(d\_euclidea, cut5\_hc\_prom)}
\CommentTok{\#\textgreater{} [1] 0.4366356}
\end{Highlighting}
\end{Shaded}

En nuestro ejemplo TIC, el gráfico de sedimentación y criterio del gap indican un número óptimo de clusters de 3. El ancho de silueta alcanza su máximo con dos clusters, si bien la altura del gráfico para tres clusters es prácticamente la misma. Por ello, se opta por 3 clusters a pesar de que el índice de Dunn también se decanta por dos. El primero lo forman Rumanía, Bulgaria y Grecia, la franja sudeste de la UE27, que se caracteriza por tener los peores guarismos en dotación y uso de las TIC, tanto a nivel de hogar como de empresa. El segundo lo integran el resto de la franja este más las tres primeras economías de la Unión y Portugal. Tienen unos elevados porcentajes en todas las variables, pero no los mayores, que corresponden a los demás países de la UE27, el tercer conglomerado.

Además de los procedimientos anteriores, hay otros, no tan populares, \((i)\) basados en la contrastación de hipótesis, suponiendo que los datos siguen alguna distribución multivariante (casi siempre la normal) o \((ii)\) procedentes de la abstracción de procedimientos inherentes al análisis multivariante paramétrico; los detalles pueden verse en \citet{gallardo2022}. El paquete \texttt{NbClust} de R contiene la función NbClust() que calcula 30 índices para valorar el número óptimo de clústers.

\begin{infobox_resume}
\textbf{RESUMEN}
El análisis cluster está orientado a la agrupación de un conjunto de elementos en grupos, en función de una serie de características, tal que los elementos de cada grupo sean lo más parecidos posible entre sí y lo más diferentes posible de los de otros grupos. Este proceso implica \((i)\) la selección de las variables en función de las cuales se van a agrupar; \((ii)\) la elección de la distancia o medida de similitud entre ellos; \((iii)\) la elección de la técnica para formar los grupos; y \((iv)\) la determinación del número óptimo de clusters, cuando sea menester. Estas son las cuestiones que se estudian en este capítulo, si bien, por cuestiones de espacio, en \((iii\) solo se abordan las técnicas de clusterización jerárquicas, estudiandose las no jerárquicas en el siguiente capítulo.

\end{infobox_resume}

\hypertarget{no-jerarquico}{%
\chapter{Análisis cluster: clusterización no jerárquica}\label{no-jerarquico}}

\emph{José-María Montero y Gema Fernández-Avilés}

Como se avanzó en \ref{introac}, aunque las técnicas de agrupación jerárquicas son muy utilizadas, existen otras, también muy populares, que se aglutinan bajo la denominación de no jerárquicas y que se pueden clasificar, sin ánimo de exhaustividad, en \((i)\) \textbf{de optimización o reasignación}; \((ii)\) \textbf{basadas en la densidad de elementos}; y \((iii)\) \textbf{otras}, como los \emph{métodos directos} (por ejemplo, el block-; bi-; co-; two mode cluster), los \emph{de reducción de la dimensionalidad} (como el Q- o el R-factorial), los \emph{métodos de clusterización difusa}, o los \emph{basados en mixturas de modelos}.

Las técnicas no jerárquicas proceden con el criterio de la inercia, maximizando la varianza inter-grupos y minimizando la intra-grupos. Se caracterizan porque:

\begin{itemize}
\tightlist
\item
  El número de clusters se suele determinar a priori,
\item
  Utilizan directamente los datos originales, si necesidad de computo de una matriz de distancias o similaridades,
\item
  Los elementos pueden cambiar de cluster, y
\item
  Los clusters resultantes no están anidados unos en otros.
\end{itemize}

\hypertarget{muxe9todos-de-reasignaciuxf3n}{%
\section{Métodos de reasignación}\label{muxe9todos-de-reasignaciuxf3n}}

Los métodos de reasignación permiten que un elemento asignado a un grupo en una determinada etapa del proceso de clusterización sea reasignado a otro grupo, en una etapa posterior, si dicha reasignación implica la optimización del criterio de selección. El proceso finaliza cuando no hay ningún elemento cuya reasignación permita optimizar el resultado conseguido. Estas técnicas suelen asumir un número determinado de clusters a priori y se diferencian entre sí en la manera de obtener la partición inicial y en la medida a optimizar en el proceso. Respecto a esta última, las medidas más populares son \((i)\) la minimización de la traza de la matriz de covarianzas intra-grupos; \((ii)\) la minimización de su determinante; \((iii)\) la maximización de la traza del producto de las matrices de covarianzas inter-grupos e intra-grupos; \((iv)\) medidas de información o de estabilidad.

\hypertarget{tuxe9cnicas-basadas-en-centroides-muxe9todos-de-forgy-y-bf-k-medias}{%
\subsection{\texorpdfstring{Técnicas basadas en centroides: métodos de Forgy y \(\bf k\)-medias}{Técnicas basadas en centroides: métodos de Forgy y \textbackslash bf k-medias}}\label{tuxe9cnicas-basadas-en-centroides-muxe9todos-de-forgy-y-bf-k-medias}}

Los algoritmos de reasignación más populares son el de Forgy y, sobre todo, el \(k\)-medias. La literatura sobre este tipo de técnicas no es clara y, frecuentemente, se confunden el método de Forgy y el \(k\)-medias, así como el \(k\)-medias con algunas de sus otras denominaciones (dándose a entender entender que son técnicas distintas). Sin embargo, la historia es la siguiente: originalmente, \citet{forgy1965cluster} propuso un algoritmo consistente en la iteración sucesiva, hasta obtener convergencia, de las dos operaciones siguientes:\((i)\) representación de los grupos por sus centroides; y \((ii)\) asignación de los elementos al grupo con el centroide más cercano. Posteriormente, \citet{diday1971nouvelle}, \citet{diday1973dynamic}, \citet{anderberg1973}, \citet{bock1974} y \citet{spath1975cluster} desarrollaron una variante del método de Forgy, que solo se diferencia de él en que los centroides se recalculan después de asignar cada elemento (con la técnica de Forgy primero se llevan a cabo todas las asignaciones y posteriormente se recalculan los centroides). Diday la llamó método de las nubes dinámicas o clusters dinámicos, Anderberg se refirió a ella como el criterio de inclusión en el grupo del centroide más cercano, Bock la denominó particionamiento iterativo basado en la mínima distancia, y Späth la llamó HMEANS, una versión por lotes del procedimiento de los autores anteriores. Sin embargo, fue \citet{macqueen1967classification} quien previamente acuñó la denominación de ``\(k\)-medias'' que se usa hasta la fecha.

\(K\)-medias\footnote{Recuérdese que el centro de un conglomerado viene dado por el centroide, vector de medias.} requiere la especificación previa del número de grupos, \(k\), en los que se va a dividir el conjunto de elementos. El algoritmo \((i)\) selecciona \(k\) elementos por algún procedimiento; \((ii)\) asigna los restantes elementos al elemento más cercano de los previamente seleccionados; \((iii)\) sustituye los elementos seleccionados en \((i)\) por los centroides de los grupos que se han formado; \((iv)\) asigna el conjunto de elementos al centroide más cercano del punto \((iii)\); \((v)\) repite iterativamente los dos últimos pasos hasta que la asignación de elementos a los centroides no cambia. Los grupos entonces formados máximizan la distancia inter-grupos y minimizan la distancia intra-grupos.
Recuérdese que con el método de Forgy la etapa \((iii)\) no comienza hasta que no se hayan asignado todos los elementos a un cluster en la etapa \((ii)\), mientras que en ``\(k\)-medias'' los centroides se recomputan cada vez que un elemento es asignado a un grupo.

La partición que se obtiene es un óptimo local (pequeños cambios en la reasignación de elementos no lo mejoran), pero no se puede asegurar que sea el global, pues se trata de un método heurístico. Sí se puede asegurar que la partición es de calidad.

\(K\)-medias es eficiente y sencillo de implementar, pero tiene algunas desventajas: \((i)\) necesita conocer a priori el número de grupos; \((ii)\) la agrupación resultante puede depender de la asignación inicial (normalmente aleatoria) de los centroides, pudiendo converger a mínimos locales, por lo que se recomienda repetir la clusterización 25-50 veces y seleccionar la que tenga menor varianza intra-grupos; \((iii)\) no es robusto a valores extremos; y \((iv)\) no trabaja con datos nominales.

En nuestro ejemplo TIC, se ha usado el algoritmo AS 136 de \citet{hartigan1979}, una versión eficiente del de \citet{hartigan1975clustering} que no busca óptimos locales (varianza intra-grupos mínima en cada grupo), sino soluciones tales que ninguna reasignación de elementos reduzca la varianza (global) intra-grupos (véase Fig. \ref{fig:kmeans}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(factoextra)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{kmeans\_tic }\OtherTok{\textless{}{-}} \FunctionTok{eclust}\NormalTok{(tic, }\StringTok{"kmeans"}\NormalTok{, }\AttributeTok{k =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/kmeans-1} 

}

\caption{Clusterización no jerárquica con $k$-medias}\label{fig:kmeans}
\end{figure}

Algunas versiones del \(k\)-medias como el \(k\)-medias difusa, el \(k\)-medias recortadas, el \(k\)-medias armónicas, el \(k\)-medias sparse y el \(k\)-medias sparse robusto pueden verse en \citet{carrasco2020}.

\hypertarget{tuxe9cnicas-basadas-en-medoides}{%
\subsection{Técnicas basadas en medoides}\label{tuxe9cnicas-basadas-en-medoides}}

\hypertarget{bf-k-medoides-pam}{%
\subsubsection{\texorpdfstring{\(\bf K\)-medoides (PAM)}{\textbackslash bf K-medoides (PAM)}}\label{bf-k-medoides-pam}}

Es un método de clusterización similar al \(k\)-medias que también requiere la especificación a priori del número de grupos. La diferencia es que en \(k\)-medoides, cada grupo está representado por uno de sus elementos, denominados medoides (o centrotipos)\footnote{El medoide de un conglomerado es el elemento del conglomerado con la menor disimilitud promedio entre él y todos los demás miembros del grupo. Es el elemento más céntrico del grupo.}. En el \(k\)-medias están representados por sus centroides, que no tienen por qué coincidir con ninguno de los elementos a agrupar. Se trata pues, de formar grupos \emph{particionando el conjunto de elementos alrededor de los medoides} (PAM).

El algoritmo \(k\)-medoids es más robusto al ruido y a valores grandes de los datos (de hecho es invariante a los outliers) que el \(k\)-medias, ya que minimiza la suma de diferencias por
parejas (utiliza la distancia Manhattan) en lugar de la suma de los cuadrados de las distancias euclídeas\footnote{las técnicas basadas en la minimización de promedios de distancias o de residuos en valor absoluto son más robustas
  que las basadas en sumas de cuadrados.}.
Además, sus agrupaciones no dependen del orden en que han sido introducidos 1os elementos, cosa que puede ocurrir con otras técnicas no-jerárquicas, y propone como centro del cluster un elemento del mismo.

PAM funciona muy bien con conjuntos de datos pequeños (por ejemplo 100 elementos en 5 grupos) y permite un análisis detallado de la partición realizada puesto que proporciona las características del agrupamiento y un gráfico de silueta, así como un índice de validez propio para determinar el número óptimo de clusters. El algoritmo PAM puede verse al completo en \citet{kauf_1990}; para un muy buen resumen véase \citet{amat2017}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{pam\_tic }\OtherTok{\textless{}{-}} \FunctionTok{eclust}\NormalTok{(tic, }\StringTok{"pam"}\NormalTok{, }\AttributeTok{k =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/pam-1} 

}

\caption{Clusterización no jerárquica con PAM}\label{fig:pam}
\end{figure}

\hypertarget{clara}{%
\subsubsection{CLARA}\label{clara}}

La ineficiencia de PAM para bases de datos grandes, junto con su complejidad computacional, llevó al desarrollo de
CLARA (clustering Large Applications)\footnote{No obstante, hay una interesante modificación de PAM cambiando el orden de anidamiento de los bucles. La idea es encontrar el mejor intercambio de elementos para cada medoide y ejecutar tantos como sea posible en cada iteración, lo que reduce el número de iteraciones necesarias para la convergencia sin pérdida de calidad (\citet{shubert2021}). También se puede aplicar a los algoritmos CLARA y CLARANS que se verán a continuación.}

La diferencia entre PAM y CLARA es que el segundo se basa en muestreos. Solo una pequeña porción de los datos totales es seleccionada como representativa de los datos y
los medoids son escogidos de la muestra usando PAM.
CLARA, pues, combina la idea de \(k\)-medoides con el remuestreo para que pueda aplicarse a grandes volúmenes de datos. De acuerdo con \citet{amat2017} (una descripción completa puede verse en \citet{kauf_1990}), CLARA selecciona una muestra aleatoria y le aplica el algoritmo de PAM para encontrar los clusters óptimos dada esa muestra. Alrededor de esos medoides se agrupan los elementos de todo el conjunto de datos. La calidad de los medoides resultantes se cuantifica con la suma total de distancias intra-grupos. CLARA repite este proceso un número predeterminado de veces con el objetivo de reducir el sesgo de muestreo. Por último, se seleccionan como clusters finales los obtenidos con los medoides que minimizaron la suma total de distancias intra-grupo.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{clara\_tic }\OtherTok{\textless{}{-}} \FunctionTok{eclust}\NormalTok{(tic, }\StringTok{"clara"}\NormalTok{, }\AttributeTok{k =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/clara-1} 

}

\caption{Clusterización no jerárquica con CLARA}\label{fig:clara}
\end{figure}

\hypertarget{clarans}{%
\subsubsection{CLARANS}\label{clarans}}

CLARANS (Clustering Large Applications based upon Randomized Search) es una mezcla de PAM y CLARA.
Como CLARA puede dar lugar a una mala clusterización si uno de los medoides de la muestra está lejos de los mejores medoides, CLARANS trata de superar esta limitación. El algoritmo puede verse en \citet{ng2002clarans}.

\hypertarget{tuxe9cnicas-basadas-en-medianas-bf-k-medianas}{%
\subsection{\texorpdfstring{Técnicas basadas en medianas: \(\bf k\)-medianas}{Técnicas basadas en medianas: \textbackslash bf k-medianas}}\label{tuxe9cnicas-basadas-en-medianas-bf-k-medianas}}

Igual que el \(k\)-medoides, es una variante del \(k\)-medias que utiliza como centros las medianas, para que no le afecten ni el ruido ni los valores atípicos. La diferencia con el \(k\)-medoides es que la mediana de un grupo no tiene por qué ser una de las observaciones. \(K\)-medianas utiliza la distancia Manhattan.

Volviendo al ejemplo TIC, como se ha podido comprobar, la clusterización de los países de la UE27 en función del uso del las TIC es prácticamente la misma para técnicas jerárquicas aglomeratativas como el vecino más lejano, el método de Ward, el del centroide algoritmos divisivos como DIANA o las técnicas no jerárquicas con preselección de 3 grupos (\(k\)-medias, PAM y CLARA).

\hypertarget{muxe9todos-basados-en-la-densidad-de-puntos}{%
\section{Métodos basados en la densidad de puntos}\label{muxe9todos-basados-en-la-densidad-de-puntos}}

Utilizan indicadores de frecuencia, construyendo grupos mediante la detección de aquéllas zonas del espacio de las variables que caracterizan a los elementos densamente pobladas (clusters naturales) y de aquellas otras con un escasa densidad de elementos. Los elementos que no forman parte de un conglomerado se consideran ruido. Emulan, pues, el funcionamiento del cerebro humano.

La identificación de los grupos (y los parámetros que los caracterizan, cuando se manejan modelos probabilísticos) se lleva a cabo haciéndolos crecer hasta que la densidad del grupo más próximo sobrepase un cierto umbral. Por tanto, imponen reglas para evitar el problema de obtener un solo grupo cuando existen puntos intermedios. Se suele suponer que la densidad de elementos en los grupos sigue una Normal si las variables son cuantitativas, y una Multinomial si son cualitativas.

Se suelen clasificar en:

-\((i)\) Las que tienen un \textbf{enfoque tipológico}: los grupos se construyen buscando las zonas con mayor concentración de elementos. Pertenecen a este tipo el \emph{Análisis modal de Wishart}, que supone clusters esféricos y dada la complejidad de su algoritmo no tuvo mucho éxito, el \emph{método TaxMap}, que introduce un valor de corte en caso de que los grupos no estén claramente aislados (ello lleva a que los resultados tengan un cierto grado de subjetividad), y el \emph{método de Fortin}, también con escasísisma difusión en la literatura.

-\((ii)\) Las que tienen un \textbf{enfoque probabilístico}: las variables que caracterizan los elementos siguen una distribución de probabilidad cuyos parámetros cambian de un grupo a otro. Se trata, pues, de agrupar los elementos que pertenecen a la misma distribución. Un ejemplo es el \emph{método de las combinaciones de Wolf}.

No obstante, estos algoritmos, y otros como, por ejemplo, los de Gitman y Levine, y Catel y Coulter, aunque muy citados en la literatura en español, tuvieron poco éxito.

Mayor éxito han tenido otros algoritmos como \emph{Expectation Maximization} (EM), \emph{Model based clustering} (MCLUST), \emph{Density-Based Spatial Clustering of Applications with Noise} (DBSCAN), \emph{Ordering Point To Identify Clustering Structure Clustering} (OPTICS), que es una generalización de DBSCAN, \emph{Wavelet-based Cluster} (WAVECLUSTER) y \emph{Density-based Clustering} (DENCLUE), entre otros.

DBSCAN\footnote{La densidad se refiere al número de elementos en una misma zona. Sin embargo, si te paras a pensar, es un concepto subjetivo, porque, \((ii)\) ¿cuántos puntos son necesarios para considerar a una zona como densa? y \((ii)\) ¿cómo de distantes pueden estar dichos elementos entre sí? Por ello, ambos (número de puntos y distancia) son los dos hiperparámetros del modelo.} es, quizás, el más popular. Incluso ha recibido premios por sus numerosísimas aplicaciones a lo largo del tiempo. Soluciona los problemas de los métodos de reasignación, que son buenos para clusters con forma esférica o convexa que no tengan demasiados outliers o ruido, pero que fallan cuando los clusters tienen formas arbitrarias. De acuerdo con \citet{amat2017}, DBSCAN evita este problema siguiendo la idea de que, \((i)\) para que una observación forme parte de un cluster, tiene que haber un mínimo de observaciones vecinas dentro de un radio de proximidad y \((ii)\) y que los clusters están separados por regiones vacías o con pocas observaciones. Consecuentemente, DBSCAN necesita dos parámetros: el radio (\(\epsilon\)) que define la región vecina a una observación (\(\epsilon\)-neighborhood); y el número mínimo de puntos (minPts) u observaciones en ella\(\epsilon\)-neighborhood\footnote{Véase \citet{amat2017} para una discusión sobre el valor de ambos parámetros.}.

Los elementos objeto de agrupación se pueden clasificar, en función de su \(\epsilon\)-neighborhood y minPts, como: \((i)\) \emph{elementos centrales}, si el número de elementos en su \(\epsilon\)-neighborhood es igual o mayor que minPts; \((ii)\) \emph{elementos frontera}, si no son elementos centrales pero pertenecen al \(\epsilon\)-neighborhood de otro elemento que sí es central; y \((iii)\) \emph{elementos atípicos o de ruido}, si no verifican ni \((i)\) ni \((ii)\).

A partir de la clasificación anterior, y para \(\epsilon\)-neighborhood y minPts dados, se origina otra:
\((i)\) un elemento \(Q\) es \emph{denso-alcanzable directamente} desde el elemento \(P\) si \(Q\) está en el \(\epsilon\)-neighborhood de \(P\) y \(P\) es un elemento central; \(Q\) es \emph{denso-alcanzable} desde \(P\) si existe una cadena de objetos \(\{Q_{1}=P, Q_2, Q_3,..., Q_{n}\}\) tal que \(Q_{i+1}\) es denso-alcanzable directamente desde \(Q_{i}\), \(\forall 1 \leq i \leq n\); \((iii)\) \(Q\) está \emph{denso-connectado} con \(P\) si hay un elemento \(R\) desde el cual \(P\) y \(Q\) son denso-alcanzables.

Los pasos del algoritmo DBSCAN son los siguientes (\citet{amat2017}):

\begin{itemize}
\item
  Para cada elemento u observación \(x_i\) calcúlese su distancia con el resto de observaciones. Márquese como central si lo es y como visitado si no lo es.
\item
  Para cada observación marcada como elemento central, si aún no ha sido asignada a ningún grupo, créese un grupo nuevo y asígnesele a él. Búsquense, recursivamente, todas las observaciones denso-conectadas con ella y asígnense al mismo grupo.
\item
  Itérese el mismo proceso para todas todas las observaciones no visitadas.
\item
  Aquellas observaciones que tras haber sido visitadas no pertenecen a ningún cluster se marcan como outliers.
\end{itemize}

Como resultado de del algoritmo DBSCAN se generan clusters que verifican: \((i)\) todos los elementos que forman parte de un mismo cluster están denso conectados entre ellos; y \((ii)\) si un elemento \(P\) es denso-alcanzable desde cualquier otro de un cluster, entonces \(P\) también pertenece al cluster.

El éxito de DBSCAN se debe a sus importantes ventajas. De nuevo siguiendo a \citet{amat2017}, no requiere la especificación previa del número de clusters; no require esfericidad (ni ninguna forma determinada) en los grupos; y puede identificar valores atípicos, por lo que la clusterización resultante no vendrá influenciada por ellos. También tiene algunas desventajas, como que no es un método totalmente determinista puesto que \((i)\) los puntos frontera que son denso-alcanzables desde más de un cluster pueden asignarse a uno u otro dependiendo del orden en el que se procesen los datos; y \((ii)\) no genera buenos resultados cuando la densidad de los grupos es muy distinta, ya que no es posible encontrar un \(\epsilon\)-neighborhood y un minPts válidos para todos a la vez.

Dado el escaso número de datos de la base de datos \texttt{TIC2021} del paquete \texttt{cdr} no se ha podido utilizar DBSCAN. Sin embargo, para ilustrar su utilización, en la Fig. \ref{fig:density-kmeans} muestra la agrupación en 5 clusters de la base de datos \texttt{multishapes} de la librería \texttt{factoextra} mediante DBSCAN (función \texttt{dbscan}) y \(k\)-medias. Se trata de una base de datos que contiene observaciones pertenecientes a 5 grupos distintos y con con cierto ruido (outliers); en consecuencia, los grupos no deberían ser esféricos y DBSCAN sería un algoritmo de agrupación adecuado.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/density-kmeans-1} 

}

\caption{Comparación entre los algorítmos $k$-means y DBSCAN para el conjunto de datos simulado `multishapes`}\label{fig:density-kmeans}
\end{figure}

\hypertarget{otros-muxe9todos}{%
\section{Otros métodos}\label{otros-muxe9todos}}

Por último, en el cajón de sastre de otras técnicas de clusterización no jerárquicas, merece la pena siquiera mencionar los métodos directos, los de reducción de la dimensionalidad, los de clustering difuso y la clusterización basada en modelos.

Los \textbf{métodos directos} agrupan simultáneamente los elementos y las variables. El más conocido es el cluster por bloques (block-; bi-; co-; o two mode-clustering). El paquete `biclust' proporciona varios algoritmos para encontrar clusters en dos dimensiones. Además, es muy recomendable para el preprocesamiento de los datos y para la visualización y validación de los resultados.

\textbf{Las técnicas de reducción de la dimensionalidad} buscan factores en el espacio de los elementos (modelo Q-factorial) o de las variables (modelo R-factorial) haciéndo corresponder un cluster a cada factor. Centrándonos en el modelo Q-factorial, el método parte de la matriz de correlaciones entre los elementos y rota ortogonalmente los factores encontrados. Dado que los elementos pueden pertenecer a varios clusters y, por tanto, los clusters pueden solaparse, su interpretación se hace muy difícil.

\textbf{Las técnicas de clustering difuso} precisamente permiten la pertenencia de un elemento a varios clusters, estableciendo un grado de pertenencia a cada uno de ellos. El algoritmo de clustering difuso más popular es \texttt{Fuzzy\ c-means}, muy similar al \texttt{k-means} pero que calcula los centroides como una media ponderada (la ponderación es la probabilidad de pertenencia) y, lógicamente, proporciona la probabilidad de pertenencia a cada grupo.

La \textbf{clusterización basada en modelos} tiene un enfoque estadístico y consiste en la utilización de una mixtura finita de modelos estocásticos para la construcción de los grupos.
Un vector aleatorio \(\bf X\) procede de una mixtura finita de distribuciones paramétricas si, \(\forall \bf x \subset \bf X\) su función de densidad conjunta se puede escribir como
\(f{(\bf {x} | \bf {\psi})}=\sum_{g=1}^{G} {\pi_g} f_g {(\bf {x}|\bf {\theta_g})},\) donde \(\pi_g\) son las proporciones asignadas a cada grupo en la mixtura, tal que \(\sum_{g=1}^{G} \pi_g =1\); \(f_g(x|\theta_g)\) es la función de densidad correspondiente al \(g\)-ésimo grupo y \(\bf \psi=(\pi_1, \pi_2,..., \pi_G, \bf \theta_1,\bf \theta_2,...\bf \theta_G\)). Las funciones de densidad \(f_g {(\bf {x}|\bf {\theta_g})}\) suelen ser idénticas para todos los grupos.

En términos menos formales, el clustering basado en modelos considera que los datos observados (multivariantes) han sido generados a partir de una combinación finita de modelos componentes (distribuciones de probabilidad, normalmente paramétricas). A modo de ejemplo, en un modelo resultante de una mixtura de normales multivariantes (el caso habitual), cada componente (cluster) es una normal multivariante y el componente responsable de la generación de una observación específica determina el grupo al que pertenece dicha observación. Para la estimación de la media y matriz de covarianzas, se suele recurrir al algoritmo Expectation-Maximization, una extensión del \(k\)-medias\footnote{Esta es la razón para incluir este tipo de clusterización en entre las técnicas no jerárquicas.}. El paquete \texttt{mclust} utiliza la estimación máximo verosímil para estimar dichos modelos con distintos número de clusters, utilizando el Bayesian Information Criterion (BIC) para la selección del mejor.

Sus limitaciones fundamentales son considerar que las características de los elementos son independientes, y que no resulta adecuado para grandes bases de datos o distribuciones de probabilidad que impliquen un elevado coste computacional.

Una revisión de la evolución de la clusterización basada en modelos desde sus orígenes en 1965 puede verse en \citet{mcnicholas2016model}. Una idea intuitiva puede encontrarse en \citet{amat2017}.

\hypertarget{nota-final}{%
\section{Nota final}\label{nota-final}}

La elección de que técnica de clusterización, jerárquica o no, es una decisión del investigador, y dependerá de su deseo de clasificación, la métrica de las variables y la distancia o medida de similaridad elegida. No obstante, ambos tipos de técnicas tienen sus ventajas y desventajas, y deberán ser tenidas en cuenta a la hora de decidir. Las jerárquicas adolecen de cierta inestabilidad, lo que plantea dudas sobre la fiabilidad de sus resultados. Además, a veces es difícil decidir cuántos grupos deben seleccionarse. Suelen recomendarse en caso de conjuntos de datos pequeños. En caso de grandes conjuntos de datos la literatura suele recomendar las no jerárquicas; además, tienen una gran fiabilidad ya que al permitir la reasignación de los elementos, una incorrecta asignación puede ser corregida posteriomente.

\begin{infobox_resume}
\textbf{RESUMEN}
En este capítulo se pasa revista a las principales técnicas y algoritmos de agrupación no jerárquicas. Primeramente se abordan los principales métodos de reasignación, y en particular los basados en centroides (método de Forgy y \(k\)-medias), medoides (\(k\)-medoides, PAM, CLARA, CLARANS) y medianas (\(k\)-medianas). Posteriormente se exponen las técnicas basadas en la densidad de puntos desde las perspectivas tipológica (análisis modal, métodos taxmap, de Fortin, de Gitman y Levine, y de Catel y Coulter) y probabilistica (método de Wolf), así como se estudia el DBSCAN. Finalmente, se muestran otras técnicas de agrupación no jerárquicas como los métodos directos (block-; bi; co-; two-mode clustering), los de reducción de la dimensionalidad (modelos Q- y R-factorial), el clustering difuso y los métodos basados en mixturas de modelos,

\end{infobox_resume}

\hypertarget{acp}{%
\chapter{Análisis de componentes principales}\label{acp}}

\emph{José-María Montero y José Luis Alfaro Navarro}

\hypertarget{introducciuxf3n-17}{%
\section{Introducción}\label{introducciuxf3n-17}}

En el estudio de cualquier problema de interés, lo ideal es tomar información del mayor número de variables posible lo cual, actualmente, no es un impedimento. Sin embargo, trabajar con muchas variables es incómodo (por ejemplo, si fueran 30 y se estuviese interesado en su correlación 2 a 2, habría que calcular 435 coeficientes). Además, tener muchas variables no implica necesariamente tener mucha información. Si están correlacionadas entre ellas (que suele ser el caso en la realidad), parte de la información que proporcionan es redundante. Por consiguiente, el reto es reducir la dimensionalidad del problema sin reducir la cantidad de información proporcionada por las variables originales, midiéndose dicha cantidad de información a través de su variabilidad, en consonancia con el concepto de entropía. En concreto, se adopta como medida de la variabilidad de las variables originales la suma de sus varianzas.

Pues bien, el análisis de componentes principales (ACP, perteneciente al ámbito del aprendizaje no supervisado) \index{Análisis de Componentes Principales} es una técnica de reducción de la dimensionalidad, un problema importante en ciencia de datos, tanto en el aprendizaje supervisado como no supervisado. ACP opera sustituyendo las variables originales por un número reducido de combinaciones lineales de ellas, incorreladas, denominadas componentes principales (c.p.), que capturan un elevado porcentaje de la variabilidad de las variables originales \citetext{\citealp{Hothorn_Everitt2014}; \citealp{Boehmke2020}}. ACP es el primer intento de reducción de la dimensionalidad y el único utilizado a tal fin hasta el advenimiento del escalado mulidimensional (aunque no es su función principal) y otras técnicas más complicadas pertenecientes al ámbito del aprendizaje múltiple (\emph{manifold learning}).

La reducción de la dimensionalidad no solo es útil en el estudio de fenómenos complejos con un elevado número de dimensiones, sino también para facilitar la implementación de otros métodos de aprendizaje no supervisado, como el cluster (reduciendo el número de dimensiones a utilizar para configurar los clusters), o supervisado, como, por ejemplo, la regresión (reduciendo el número de regresores y haciéndolos incorrelados, evitando así información redundante y la colinealidad); o la técnica de partial least squares (PLS, similar a la regresión con \(c.p.\) pero que, en vez de ignorar la variable respuesta en la determinación las combinaciones lineales, busca aquellas que, además de explicar la varianza de las variables originales, predicen la variable respuesta lo mejor posible).\footnote{Una descripción completa de la base de datos está disponible en \url{https://geodacenter.github.io/data-and-lab/Guerry/}} También es muy útil para representar gráficamente relaciones multivariantes.

En R hay varias opciones para la realización de un ACP: `princomp'; `prcomp'; y `PCA' de la librería `FactoMineR' \citep{Leetal2008}, entre otras. Se ha optado por la última porque, entre otras, \((i)\) incorpora notabes mejoras gráficas, \((ii)\) permite el ACP con ``missing values'' imputando dichos valores (paquete `missMDA'); \((iii)\) proporciona una descripción e interpretación automática de los resultados, seleccionando los mejores gráficos, mediante el paquete `FactoInvestigate'; \((iv)\) permite la implementación de técnicas híbridas (por ejemplo, clusterización con \(c.p.\)); y \((vi)\) posibilita la predicción de las coordenadas de individuos y variables adicionales utilizando únicamente inputs del ACP previo.

Como ilustración práctica del ACP, se abordará la reducción de la dimensionalidad de un problema del ámbito de la sociedad de la información en la UE-27, en 2021. Se dispone, para 2021 y a nivel de país, de información sobre 7 variables: 4 relacionadas con el uso de las TIC por parte de las empresas y 3 relativas al uso de dichas tecnologías por parte de las personas y a la equipación TIC de los hogares. Dicha información, así como la descripción de las variables, puede consultarse en la base de datos \texttt{TIC2021} del paquete \texttt{CDR}.

\hypertarget{obtenciuxf3n-de-las-componentes-principales}{%
\section{Obtención de las componentes principales}\label{obtenciuxf3n-de-las-componentes-principales}}

\hypertarget{descripciuxf3n-formal-del-proceso}{%
\subsection{Descripción formal del proceso}\label{descripciuxf3n-formal-del-proceso}}

Sea \(\mathbf{X^\prime}=(X_{1},\dotsc,X_{p})\) un vector \(p\)-dimensional de variables aleatorias con vector de medias \(\boldsymbol{\mu}\) y matriz de covarianzas conocida \(\boldsymbol{\Sigma}\). Puesto que los cambios de origen no afectan a la covarianza, las variables originales se consideran centradas, de tal manera que \(\boldsymbol{\mu}=\mathbf {0}\) y \(\boldsymbol{\Sigma}= E\left (\mathbf{X^\prime} \mathbf{X}\right)\). Se trata de encontrar un conjunto de \(p\) combinaciones lineales incorreladas de dichas variables, \(Y_{j}=a_{1j}X_{1}+a_{2j}X_{2}+\dotsb+a_{pj}X_{p}=\mathbf{a}_{j}^{\prime}\mathbf{X}, \quad{j=1,2,\dotsc,p}\), denominadas \(c.p.\), que recojan la variabilidad existente en los datos. La idea es ordenar las \(c.p.\) tal que \(V(Y_1)> V(Y_2)>...> V(Y_p)\), y seleccionar \(m\) de ellas (las \(m\) primeras), \(m<p\), que capturen un elevado porcentaje de la variabilidad de los datos.\footnote{Geométricamente, estas combinaciones lineales representan un nuevo sistema de coordenadas obtenido mediante la rotación de los ejes originales. Los nuevos ejes representan las direcciones de máxima variabilidad y proporcionan una descripción más simple de la estructura de covarianza.}

La varianza de cada componente y la covarianza entre ellas vienen dadas por:
\begin{equation}
\begin{split}
Var(Y_{j})=\mathbf{a}_{j}^{\prime}\mathbf{\Sigma}\mathbf{a}_{j},    \quad{\forall j=1,2,\dotsc,p} \\
Cov(Y_{j}, Y_{k})=\mathbf{a}_{j}^{\prime}\mathbf{\Sigma}\mathbf{a}_{k},      \quad{\forall j, k,j\neq k, =1,2,\dotsc,p}
\end{split}
\label{eq:ecuacion1acp}
\end{equation}

\textbf{Obtención de la primera componente principal}

La primera \(c.p.\), \(Y_1\), se obtiene seleccionando el vector \(\mathbf{a}_{1}\) que maximice su varianza. Sin embargo, dado que la varianza de cada \(c.p.\) puede incrementarse arbitrariamente multiplicando \(\mathbf{a}_{1}\) por una constante, se impone la condición \(\mathbf{a}_{1}^{\prime}\mathbf{a}_{1}=1\); es decir, se normalizan los vectores, de tal forma que tengan longitud unitaria. Por tanto, se trata de encontrar el vector \(\mathbf{a}_{1}\) que maximiza \(Var(Y_{1})=\mathbf{a}_{1}^{\prime}\mathbf{\Sigma}\mathbf{a}_{1}\) sujeto a que \(\mathbf{a}_{1}^{\prime}\mathbf{a}_{1}=1\). En otros términos, se selecciona el vector \(\mathbf{a}_{1}\) que maximiza el lagrangiano: \begin{equation}
\mathcal{L}(\mathbf {\mathbf a}_{1})=\mathbf{a}_{1}^{\prime}\mathbf{\Sigma}\mathbf{a}_{1}-\lambda (\mathbf{a}_{1}^{\prime}\mathbf{a}_{1}-1)
\label{eq:ecuacion2acp}
\end{equation} Para ello se deriva respecto a \(\mathbf{a}_{1}\) y \(\lambda\) y se igualan a cero dichas derivadas: \begin{equation}
\begin{split}
\frac{\partial\mathcal{L} (\mathbf{a}_{1})}{\partial\mathbf{a}_{1}}=2\mathbf{\Sigma}\mathbf{a}_{1}-2\lambda\mathbf{a}_{1}=(\mathbf{\Sigma}-\lambda\mathbf{I})\mathbf{a}_{1}= \mathbf{0} \\
\frac{\partial \mathcal{L}(\mathbf{a}_{1})}{\partial\lambda}=\mathbf{a}_{1}^{\prime}\mathbf{a}_{1}-1=0
\end{split}
\label{eq:ecuacion3acp}
\end{equation} La primera ecuación tendrá solución distinta del vector nulo cuando \((\mathbf{\Sigma}-\lambda\mathbf{I})\) sea singular. Es decir, cuando \(|\mathbf{\Sigma}-\lambda\mathbf{I}|=0\), o en otros términos, cuando \(\lambda\) sea un autovalor \index{autovalor}de \(\mathbf{\Sigma}\). Dado que,

\begin{itemize}
\tightlist
\item
  \(\boldsymbol{\Sigma}\) es semidefinida positiva y, en general, tendrá \emph{p} autovalores no negativos;
\item
  y que en el proceso de optimización, premultiplicando \((\mathbf{\Sigma}-\lambda\mathbf{I})\mathbf{a}_{1}= \mathbf{0}\), por \(\mathbf{a}^{\prime}_{1}\) y teniendo en cuenta que \(\mathbf{a}_{1}^{\prime}\mathbf{a}_{1}=1\), resulta que \(\lambda= \mathbf{a}_{1}^{\prime}\mathbf{\Sigma}\mathbf{a}_{1}= V({Y_1})\),\footnote{La condición \(\mathbf a_i^\prime \mathbf a_i=1\) lleva a que los autovalores de \(\mathbf \Sigma\) coincidan con las varianzas de las \(c.p.\)}
\end{itemize}

se seleccionará el mayor de los autovalores de \(\boldsymbol{\Sigma}\), obteniéndose el autovector \(\mathbf{a}_{1}\) de tal forma que cumpla la condición \(\mathbf{a}_{1}^{\prime}\mathbf{a}_{1}=1\).

\textbf{Obtención de la segunda componente principal}

\(Y_{2} = \mathbf{a}_{2}^{\prime}\bf{X}\) se obtiene igual que \(Y_{1}\), pero añadiendo la condicion de incorrelación con \(Y_{1}\): \(Cov(Y_{1}, Y_{2}) = \mathbf{a}_{2}^{\prime} \mathbf{\Sigma} \mathbf{a}_{1} = 0,\) o equivalentemente, \(\mathbf{a}_{2}^{\prime}\mathbf{a}_{1}=0\) (\(\mathbf a_1\) y \(\mathbf a_2\) ortogonales).\footnote{Si todos los autovalores de \(\boldsymbol{\Sigma}\) son distintos, los autovectores son ortogonales. En caso contrario, los autovectores asociados a autovalores comunes se eligen de forma que sean ortogonales.}

Por tanto, el lagrangiano a maximizar es:

\begin{equation}
\mathcal{L}(\mathbf {a}_{2})=\mathbf{a}_{2}^{\prime}\mathbf{\Sigma}\mathbf{a}_{2}-
\lambda (\mathbf{a}_{2}^{\prime}\mathbf{a}_{2}-1)- \gamma (\mathbf{a}_{2}^{\prime}\mathbf{a}_{1}-0)
\label{eq:ecuacion4}
\end{equation}

Derivando respecto a \(\bf{a}_{2}\) e igualando a cero:

\begin{equation}
\frac{\partial \mathcal{L}(\mathbf {a}_{2})}{\partial\mathbf{a}_{2}}=
2\mathbf{\Sigma}\mathbf{a}_{2} -2\lambda \mathbf{a}_{2}- \gamma \mathbf{a}_{1}=
2(\mathbf{\Sigma}- \lambda \mathbf{I}) \mathbf{a}_{2}- \gamma\mathbf{a}_{1} =
\mathbf{0}
\label{eq:ecuacion5}
\end{equation}

Premultiplicando por \(\mathbf{a}_{1}^{\prime}\) y considerando la condición de ortogonalidad, se tiene que \(\gamma=2 Cov (Y_1,Y_2)=0\), con lo que \(\frac{\partial \mathcal{L}(\mathbf {a}_{2})}{\partial\mathbf{a}_{2}}= 2{\mathbf \Sigma}{\mathbf {a}_2} - 2 \lambda \mathbf a_2 = 0\), que implica que \((\mathbf \Sigma -\lambda \mathbf I)\mathbf a_2=0\).

Siguiendo el mismo razonamiento que en la obtención de la primera componente, se elige el segundo mayor autovalor de \(\mathbf\Sigma\)\footnote{El mayor no puede ser, ya que coincidirían \(\mathbf a_1\) y \(\mathbf a_2\), en cuyo caso \(Y_1\) e \(Y_2\) no estarían incorrelacionadas.}, \(\lambda_{2}\), siendo \(\bf{a}_{2}\) el autovector asociado a él.

\textbf{Obtención del resto de las componentes principales}

Repitiendo este procedimiento, se obtienen las \emph{p} \(c.p.\), siendo los coeficientes de la \emph{j}-ésima los componentes del autovector asociado al \emph{j}-ésimo mayor autovalor de \(\mathbf \Sigma\).

El vector de componentes principales se puede expresar como \(\mathbf{Y}=\mathbf{A}^{\prime}\mathbf{X}\), donde \(\mathbf{A} = [\bf{a}_{1}, \bf{a}_{2},\dotsc,\bf{a}_{p}]\), es la matriz de autovectores (ortogonales) obtenidos.

\hypertarget{cuestiones-importantes-en-el-acp}{%
\subsection{Cuestiones importantes en el ACP}\label{cuestiones-importantes-en-el-acp}}

\hypertarget{varianza-de-las-variables-originales-y-las-cp}{%
\subsubsection{Varianza de las variables originales y las CP}\label{varianza-de-las-variables-originales-y-las-cp}}

La matriz de covarianzas de las \(c.p\), \(\mathbf V(\mathbf Y)=\mathbf A^\prime \mathbf \Sigma \mathbf A\), coincide con \(\mathbf{\Lambda}\), que es una matriz diagonal, puesto que las \(c.p.\) están incorreladas y sus varianzas (los valores de la diagonal principal) son los autovalores de \(\boldsymbol{\Sigma}\). En consecuencia: \begin{equation}
\begin{split}
\sum_{i=1}^{p}Var(Y_{i})= tr (\mathbf{\Lambda})= tr (\mathbf{A}^{\prime} \mathbf{\Sigma} \mathbf{A}) = tr (\mathbf{\Sigma} \mathbf{A} \mathbf{A}^{\prime})
= tr (\mathbf{\Sigma}) = \sum_{i=1}^{p}Var(X_{i}),
\end{split}
\label{eq:ecuacion6}
\end{equation} pudiéndose comprobar que la suma de las varianzas de las variables originales\footnote{Recuérdese que se adopta como medida de la variabilidad de las variables originales la suma de sus varianzas.} coincide con la suma de las varianzas de las \(c.p\).

Por tanto, la \emph{j}-ésima \(c.p.\) captura un porcentaje de la variabilidad de las variables originales cifrado en \(\frac{\lambda_{j}}{\sum_{j=1}^{p} \lambda_{j}} 100\), siendo la proporción capturada por las \(m\) primeras componentes \(\frac{\sum_{j=1}^m\lambda_{j}}{\sum_{j=1}^{p} \lambda_{j}} 100\).

\hypertarget{cp-a-partir-de-variables-estandarizadas}{%
\subsubsection{CP a partir de variables estandarizadas}\label{cp-a-partir-de-variables-estandarizadas}}

A menudo, no solo se centran las variables originales sino que también se estandarizan, para tener varianza unitaria. La razón es que si las variables originales presentan grandes diferencias en sus escalas de medida o en los rangos de las unidades de medida (edad en años, altura en metros, longitud en kilómetros\ldots), sus combinaciones lineales tendrán poco significado porque las variables que las conforman no son ``igualmente importantes'' y en la primera componente tendrá un gran peso la variable original con mayor magnitud \citep{chatfield1980}. Si no fuera el caso, es mejor partir de \(\bf\Sigma\); además, la teoría muestral de las \(c.p.\) es mucho más compleja cuando las variables están estandarizadas que cuando no lo están \citep{morrison1976}.

El mecanismo de obtención de las \(c.p.\) no cambia en absoluto, pero su punto de arranque ya no es \(\boldsymbol{\Sigma}\) sino \(\bf{P}\), la matriz de correlaciones de dichas variables. Los autovectores de \(\bf{P}\) son, en general distintos a los de \(\boldsymbol{\Sigma}\). Además, la suma de los autovalores, como coincide con la suma de las varianzas de las variables originales, es \emph{p}, luego el porcentaje de la variación total capturada por la componente \emph{j}-ésima \(\frac{\lambda_{j}}{p} 100\), siendo la proporción capturada por las \(m\) primeras componentes \(\frac{\sum_{j=1}^m\lambda_{j}}{p} 100\).

\hypertarget{correlaciuxf3n-entre-las-variables-originales-y-las-cp}{%
\subsubsection{Correlación entre las variables originales y las CP}\label{correlaciuxf3n-entre-las-variables-originales-y-las-cp}}

Considérese la variable original \(X_{i}\) y la \(c.p.\) \(Y_{j}=a_{1j}X_{1}+a_{2j}X_{2}+\dotsb+a_{pj}X_{p}=\mathbf{a}_{j}^{\prime}\mathbf{X}\). Dado que \(\mathbf{X}^{\prime} = [X_{1},\dotsb,X_{p}]\), entonces \(X_{i}=\mathbf{e}_{i}^{\prime}\mathbf{X}\), donde \(\mathbf{e}_{i}\) es un vector de ceros excepto un 1 en la \emph{i}-ésima posición.

Entonces, como \(({\bf \Sigma}-{\lambda_{j}} {\bf I)}{\bf {a}}_j=0\), se tiene que \({\bf\Sigma} {\bf a}_j={\lambda}_j {\bf {a}}_j\) y que

\begin{equation}
Cov(X_{i},Y_{j})=Cov(\mathbf{e}_{i}^{\prime}\prime\mathbf{X},\mathbf{a}_{j}^{\prime}\mathbf{X})= \mathbf{e}_{i}^{\prime} \mathbf{\Sigma} \mathbf{a}_{j}=
\mathbf{e}_{i}^{\prime} \lambda_{j} \mathbf{a}_{j}=
\lambda_{j} a_{ij}
\label{eq:ecuacion7}
\end{equation}

\begin{equation}
r_{X_{i},Y_{j}}=\frac{Cov(X_{i},Y_{j})}{\sqrt{Var(X_{i})}\sqrt{Var(Y_{j})}}= \frac{\lambda_{j} a_{ij}}{\sqrt{\sigma_{ii}} \sqrt{\lambda_{j}}}=
\frac {\sqrt{\lambda_{j}}a_{ij}}{\sigma_{ii}}
\label{eq:ecuacion8}
\end{equation}

donde \(\sigma_{ii}\) es el elemento \emph{i}-ésimo de la diagonal principal de \(\boldsymbol{\Sigma}\).

Si se parte de variables estandarizadas, entonces se tiene que \(r_{Z_{i},Y_{j}}=\sqrt{\lambda_{j}}a_{ij}\), donde ahora \(\lambda_{j}\) es el \emph{j}-ésimo autovalor de \(\bf{P}\) y \(a_{ij}\) es el elemento \emph{i}-ésimo de su autovector asociado. Sin embargo, el coeficiente no varía por el hecho de haber estandarizado las variables originales.

Como se verá posteriormente, estos dos coeficientes serán de gran utilidad en la interpretación de las \(c.p.\) Además, como \(r_{X_{i},Y_{j}}\) coincide con el coseno del ángulo que forma \(X_{i}\) con \(Y_{j}\) (que es la proyección o coordenada de \(X_{i}\) en el eje de \(Y_{j}\)), resulta un elemento de gran ayuda para representar las variables originales en el espacio de las componentes y, por consiguiente, para la interpretación de estas últimas. A mayor coseno mayor correlación lineal entre \(X_{i}\) en el eje de \(Y_{j}\). Matricialmente, y denominando \({\mathbf A}^*\) a la matriz de coeficientes de correlación lineal entre las variabes originales tipificadas y las \(c.p.\), se tiene que \({\mathbf A}^{*}= {\bf A} {\bf\Lambda}^{\frac {1}{2}}\). \(\mathbf{A}^*\), imprescindible en la interpretación de las \(c.p.\), no cambia por el hecho de estandarizar también las \(c.p.\)

Los cuadrados de los elementos de \(\bf A^*\) expresan la proporción de varianza de la variable \(X_i\) explicada por la componente \(Y_j\). Por tanto, la suma de los cuadrados de las filas de \(\bf A^*\) será la unidad. Se denomina \emph{contribución} individual de \(X_{i}\) a la componente \(Y_{j}\) a la cantidad \(\frac{r_{X_i,Y_j}^2} {\sum_{i=1}^{p} r_{{X_{i},Y_{j}}}^2}=\frac{cos(X_i,Y_j)}{\sum_{i=1}^{p}{cos(X_i,Y_j)}}\).

Otras dos expresiones interesantes que involucran \(\bf A^*\) son \(\bf A^*\bf A^*{^\prime}=\bf {R}\) y \(\bf A^*{^\prime}\bf A^*=\bf {\Lambda}.\)

\hypertarget{estimaciuxf3n-de-las-componentes-principales}{%
\section{Estimación de las componentes principales}\label{estimaciuxf3n-de-las-componentes-principales}}

Hasta el momento, se han derivado las \(c.p.\) suponiendo conocida la matriz de covarianzas poblacional \(\boldsymbol{\Sigma}\) (o la de correlaciones \(\bf P\)). Sin embargo, este no suele ser el caso en la práctica, por lo que se sustituyen por sus homónimas muestrales \(\bf{S}=\frac {1}{N}\bf{X}^\prime\bf{X}\) (o \(\bf{R}\)). Nada cambia en el proceso de obtención de las \(c.p.\), salvo que el punto de partida es \(\bf{S}\) (o \(\bf{R}\)) y que los valores de los autovalores y autovectores asociados son estimaciones.

En el ejemplo que nos ocupa, \(\bf R\) puede verse en Fig. \ref{fig:lee-datos-ch11}. Puede apreciarse que la correlación entre las variables es notable en la mayoría de los casos, lo que invita a analizar el problema con menos variables e incorreladas, es decir, mediante ACP.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(CDR)}
\FunctionTok{data}\NormalTok{(}\StringTok{"TIC2021"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(corrplot)}
\FunctionTok{corrplot}\NormalTok{(}\FunctionTok{cor}\NormalTok{(TIC2021), }\AttributeTok{method =} \StringTok{"number"}\NormalTok{, }\AttributeTok{type =} \StringTok{"lower"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/lee-datos-ch11-1} 

}

\caption{Matriz de correlaciones}\label{fig:lee-datos-ch11}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(FactoMineR)}
\NormalTok{acp }\OtherTok{\textless{}{-}} \FunctionTok{PCA}\NormalTok{(TIC2021, }\AttributeTok{ncp =} \DecValTok{7}\NormalTok{, }\AttributeTok{graph =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{numcomp}{%
\section{Número de componentes a retener}\label{numcomp}}

Dado que la finalidad de la técnica de componentes principales es la reducción de la dimensionalidad, una decisión clave es el número \(m\) de componentes a retener. Los criterios más populares para tomar esta decisión son:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  \textbf{Seleccionar un número de componentes que capturen, entre todas, un porcentaje de la variabilidad total determinado}
\end{enumerate}

Dicho porcentaje suele estar alrededor del 80\%, si bien, si el número de \(c.p.\) es elevado la interpretación de las \(c.p.\) es muy difícil.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Criterio de la media aritmética o criterio de Kaiser}
\end{enumerate}

Dado que la variabilidad total coincide con la suma de los autovalores, se seleccionan aquéllas \(c.p.\) cuya varianza exceda la varianza media. Es decir, se selecciona la componente \emph{j}-ésima si \(\lambda_{j}> \bar{\lambda}\) (si se parte de \(\mathbf\Sigma\)) o si \(\lambda_{j}> 1\) (si se parte de \(\mathbf R\)). En caso de outliers es recomendable utilizar la media geométrica en vez de la aritmética.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Criterio de Catell}
\end{enumerate}

Se basa en la representación gráfica de los autovalores vs.~su número de orden, que se denomina \emph{gráfico de sedimentación}\index{Gráfico de Sedimentación} porque se asemeja a la ladera de una montaña con su correspondiente zona de sedimentación. Se seleccionan las \(c.p.\) asociadas a los autovalores previos a la zona de sedimentación. En general, el criterio de Catell tiende a incluir demasiadas \(c.p.\), al contrario que el de la media, que tiende a incluir demasiado pocas (sobre todo si \(p<20\)) \citep{Mardia_et_al1979}.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Otros criterios}
\end{enumerate}

Otros criterios menos populares son la validación cruzada, el test de esfericidad o igualdad de autovalores de Anderson (requiere normalidad multivariante) y el criterio del bastón roto (véase \citet{cuadras2007} para los últimos). Para grandes conjuntos de datos, \citet{jobson1992} propone un criterio basado en la partición de la muestra en submuestras mutuamente excluyentes, similar a la validación cruzada.

La Fig. \ref{fig:autovalores-ch11} muestra el gráfico de sedimentación en el ejemplo que nos ocupa:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(acp}\SpecialCharTok{$}\NormalTok{eig[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{], }\DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{}        eigenvalue percentage of variance}
\CommentTok{\#\textgreater{} comp 1      4.644                 66.341}
\CommentTok{\#\textgreater{} comp 2      1.101                 15.731}
\CommentTok{\#\textgreater{} comp 3      0.547                  7.814}
\CommentTok{\#\textgreater{} comp 4      0.328                  4.679}
\FunctionTok{library}\NormalTok{(factoextra)}
\FunctionTok{fviz\_eig}\NormalTok{(acp, }\AttributeTok{addlables =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/autovalores-ch11-1} 

}

\caption{Gráfico de sedimentación}\label{fig:autovalores-ch11}
\end{figure}

Puede apreciarse que con tan solo las dos primeras \(c.p.\) se captura el 82,07\% de la variabilidad total de las siete variables originales.

\hypertarget{interpretaciuxf3n-de-las-cp}{%
\section{Interpretación de las CP}\label{interpretaciuxf3n-de-las-cp}}

Una primera vía consiste en analizar el signo y la magnitud de los coeficientes (\emph{loadings}) de cada variable original en cada componente.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{loadings }\OtherTok{\textless{}{-}} \FunctionTok{sweep}\NormalTok{(acp}\SpecialCharTok{$}\NormalTok{var}\SpecialCharTok{$}\NormalTok{coord, }\DecValTok{2}\NormalTok{, }\FunctionTok{sqrt}\NormalTok{(acp}\SpecialCharTok{$}\NormalTok{eig[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{, }\DecValTok{1}\NormalTok{]))}
\FunctionTok{round}\NormalTok{(loadings, }\DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{}            Dim.1  Dim.2  Dim.3  Dim.4  Dim.5  Dim.6  Dim.7}
\CommentTok{\#\textgreater{} ebroad    {-}1.410 {-}0.854 {-}1.358 {-}0.561 {-}0.303 {-}0.271 {-}0.259}
\CommentTok{\#\textgreater{} esales    {-}1.604 {-}0.318 {-}0.411 {-}0.404 {-}0.310 {-}0.262 {-}0.225}
\CommentTok{\#\textgreater{} esocmedia {-}1.317 {-}0.858 {-}0.645 {-}1.062 {-}0.536 {-}0.311 {-}0.244}
\CommentTok{\#\textgreater{} eweb      {-}1.264 {-}0.865 {-}0.825 {-}0.356 {-}0.773 {-}0.422 {-}0.284}
\CommentTok{\#\textgreater{} hbroad    {-}1.343 {-}1.550 {-}0.570 {-}0.495 {-}0.413 {-}0.159 {-}0.386}
\CommentTok{\#\textgreater{} hiacc     {-}1.290 {-}1.496 {-}0.675 {-}0.495 {-}0.413 {-}0.348 {-}0.053}
\CommentTok{\#\textgreater{} iuse      {-}1.217 {-}1.135 {-}0.652 {-}0.587 {-}0.255 {-}0.608 {-}0.331}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Una segunda vía es el análisis de los \(r_{X_{i},Y_{j}}, \forall {i,j}\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(acp}\SpecialCharTok{$}\NormalTok{var}\SpecialCharTok{$}\NormalTok{cor, }\DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{}           Dim.1  Dim.2  Dim.3  Dim.4  Dim.5  Dim.6  Dim.7}
\CommentTok{\#\textgreater{} ebroad    0.745  0.195 {-}0.618  0.012  0.134  0.081 {-}0.003}
\CommentTok{\#\textgreater{} esales    0.551  0.731  0.328  0.169  0.128  0.090  0.031}
\CommentTok{\#\textgreater{} esocmedia 0.838  0.191  0.095 {-}0.490 {-}0.099  0.040  0.012}
\CommentTok{\#\textgreater{} eweb      0.891  0.185 {-}0.085  0.217 {-}0.336 {-}0.070 {-}0.028}
\CommentTok{\#\textgreater{} hbroad    0.812 {-}0.501  0.170  0.077  0.024  0.193 {-}0.129}
\CommentTok{\#\textgreater{} hiacc     0.865 {-}0.446  0.065  0.077  0.024  0.003  0.203}
\CommentTok{\#\textgreater{} iuse      0.938 {-}0.086  0.087 {-}0.014  0.183 {-}0.256 {-}0.075}
\FunctionTok{fviz\_pca\_var}\NormalTok{(acp,}
  \AttributeTok{col.var =} \StringTok{"contrib"}\NormalTok{,}
  \AttributeTok{gradient.cols =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#00AFBB"}\NormalTok{, }\StringTok{"\#E7B800"}\NormalTok{, }\StringTok{"\#FC4E07"}\NormalTok{),}
  \AttributeTok{repel =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/corrvariablecp-ch11-1} 

}

\caption{Gráfico de cosenos o coeficientes de correlación variables-componentes}\label{fig:corrvariablecp-ch11}
\end{figure}

Se puede utilizar cualquiera de las dos, pues los resultados no serán contradictorios. Sin embargo, algunos autores recomiendan no utilizar sólo la segunda, pues los \(r_{X_{i},Y_{j}}\) sólo tienen en cuenta la variable original considerada y no el resto; es decir, se estarían interpretando las componentes desde una perspectiva univariante. También es de interés la siguiente consideración: Cuando las variables originales están correlacionadas positivamente, la primera \(c.p.\) tiene todas sus coordenadas del mismo signo y puede interpretarse como un promedio ponderado de todas ellas.

Como puede apreciarse en la matriz de \emph{loadings}, la primera \(c.p.\) es una media ponderada (con ponderaciones no muy distintas) de las variables originales, mientras que \emph{esales}, \emph{hbroad} y \emph{hiacc} cargan fuertemente en la segunda, las primera positivamente y las dos segundas de forma negativa. La interpretación desde la perspectiva univariante de los coeficientes de correlación lineal es prácticamente la misma. Por ello, cabe interpretar la primera \(c.p.\) como un indicador general del uso de las TIC. La segunda \(c.p.\) está directamente relacionada con la dotación TIC de las empresas pero tiene una relación fuertemente negativa con la dotación TIC de los individuos y los hogares. \textbf{PUEDE INTERPRETARSE COMO XXXX}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acp1 }\OtherTok{\textless{}{-}} \FunctionTok{fviz\_contrib}\NormalTok{(acp, }\AttributeTok{choice =} \StringTok{"var"}\NormalTok{, }\AttributeTok{axes =} \DecValTok{1}\NormalTok{, }\AttributeTok{top =} \DecValTok{10}\NormalTok{)}
\NormalTok{acp2 }\OtherTok{\textless{}{-}} \FunctionTok{fviz\_contrib}\NormalTok{(acp, }\AttributeTok{choice =} \StringTok{"var"}\NormalTok{, }\AttributeTok{axes =} \DecValTok{2}\NormalTok{, }\AttributeTok{top =} \DecValTok{10}\NormalTok{)}
\FunctionTok{library}\NormalTok{(patchwork)}
\NormalTok{acp1 }\SpecialCharTok{+}\NormalTok{ acp2}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/gráfico-contribución-ch11-1} 

}

\caption{Contribución de las variables originales a las componentes retenidas}\label{fig:gráfico-contribución-ch11}
\end{figure}

\hypertarget{reproducciuxf3n-de-los-datos-tipificados-y-de-r-a-partir-de-las-cp}{%
\section{Reproducción de los datos tipificados y de R a partir de las CP}\label{reproducciuxf3n-de-los-datos-tipificados-y-de-r-a-partir-de-las-cp}}

En la práctica, el punto de partida del ACP es la matriz \(\bf R\), y en tal caso se suelen estandarizar también las \(c.p.\) Pues bien, se tiene que: \[{\bf Y^*}= {\bf X A} {\bf \Lambda}^{-\frac {1}{2}}= \bf X\bf A \bf{\Lambda^{\frac {1}{2}}} \bf\Lambda^{-1}=\bf X\bf A^*\bf\Lambda^{-1}=\bf X\bf F,\] donde \(\bf F\) es la matriz de puntuaciones de las \(c.p.\). La expresión anterior proporciona las coordenadas de los \(N\) elementos en el espacio de las \(c.p.\) y, por tanto, sirve de ayuda en la interpretación de éstas. La estandarización de las \(c.p.\) asegura que los \(m\) ejes (componentes) tengan una métrica homogénea que facilitará la visualización e interpretación. Dichas coordenadas también constituyen el input de técnicas híbridas como, por ejemplo, regresión con \(c.p.\), cluster o PLS. En nuestro caso:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{puntuaciones }\OtherTok{\textless{}{-}}\NormalTok{ acp}\SpecialCharTok{$}\NormalTok{ind}\SpecialCharTok{$}\NormalTok{coord}
\FunctionTok{round}\NormalTok{(puntuaciones[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, ], }\DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{}    Dim.1  Dim.2  Dim.3  Dim.4  Dim.5  Dim.6  Dim.7}
\CommentTok{\#\textgreater{} 1  1.651  1.053  0.310 {-}0.238 {-}0.196  0.296 {-}0.196}
\CommentTok{\#\textgreater{} 2 {-}4.759 {-}0.127 {-}0.128 {-}0.223  0.023 {-}0.013 {-}0.106}
\CommentTok{\#\textgreater{} 3 {-}0.324  0.875 {-}0.564  0.870 {-}0.082 {-}0.032 {-}0.345}
\CommentTok{\#\textgreater{} 4  3.188  1.331  0.497  0.262  0.343 {-}0.300  0.319}
\CommentTok{\#\textgreater{} 5  0.024  0.144 {-}0.183  0.560 {-}0.871 {-}0.485  0.115}
\end{Highlighting}
\end{Shaded}

De la expresión anterior se deduce que \({\bf X}={\bf Y} {\bf \Lambda}^{-\frac{1}{2}} {\bf\Lambda}^{\frac{1}{2}}{\bf A}^\prime={\bf Y^*}_{N \times m}{\bf A^*}^\prime_{m \times p}\), que es la expresión que permite reproducir la matriz \(\bf X\) a partir de las \(m\) primeras \(c.p.\) tipificadas. En consecuencia, la reproducción de \(\bf R\) a partir de las las \(m\) primeras \(c.p.\) tipificadas se lleva a cabo como sigue:
\begin{eqnarray}
{\bf R}_{p \times p} = \frac {1}{N}  {\bf X}^{\prime}_{p \times N} {\bf X}_{N \times p}= {\bf A}^{*}_{p \times m} \frac {1} {N} {\bf Y}^{*^{\prime}}_{m \times N} {\bf Y}^{*}_{N \times m} {\bf A}^{*^{\prime}}_{m \times p} \nonumber\\
={\bf A}^{*}_{p \times m} {\bf I}_{m\times m}{\bf A}^{*^{\prime}}_{m \times p}={\bf A}^{*}_{p \times m}{\bf A}^{*^{\prime}}_{m \times p}
\end{eqnarray}

Debajo se muestran las tres primeras filas de la reproducción de \(\bf R\) a partir de las dos primeras \(c.p.\) De la comparación de sus valores con los de la verdadera \(\bf R\) (Fig. \ref{fig:lee-datos-ch11}) se concluye que se trata de una buena reproducción.\footnote{Con tres \(c.p.\) la reproducción es casi perfecta. Se han retenido solo dos para poder mostrar representaciones bidimensionales y para evitar la interpretación de una tercera componente.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{matrix }\OtherTok{\textless{}{-}}\NormalTok{ acp}\SpecialCharTok{$}\NormalTok{var}\SpecialCharTok{$}\NormalTok{coord[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{] }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(acp}\SpecialCharTok{$}\NormalTok{var}\SpecialCharTok{$}\NormalTok{coord)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, ]}
\FunctionTok{round}\NormalTok{(matrix[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, ], }\DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{}           ebroad esales esocmedia  eweb hbroad hiacc  iuse}
\CommentTok{\#\textgreater{} ebroad     0.593  0.553     0.662 0.700  0.507 0.557 0.682}
\CommentTok{\#\textgreater{} esales     0.553  0.839     0.602 0.626  0.081 0.151 0.455}
\CommentTok{\#\textgreater{} esocmedia  0.662  0.602     0.740 0.782  0.585 0.640 0.770}
\end{Highlighting}
\end{Shaded}

\hypertarget{limitaciones-del-acp}{%
\section{Limitaciones del ACP}\label{limitaciones-del-acp}}

Una primera limitación es que su implementación solo es posible si todas las variables se trabajan bajo un nivel de análisis numérico. Otra limitación importante es el supuesto subyacente de que los datos observados son combinación lineal de una cierta base. Es decir, solo tiene en cuenta combinaciones lineales de las variables originales. Otros métodos de reducción de la dimensionalidad como, por ejemplo, el t-distributed stochastic neighbor embedding (t-SNE), o la versión Kernel de la técnica, que también funcionan con no linealidad, superan esta limitación.

Además, el hecho que todas las \(c.p.\) sean combinaciones lineales de todas las variables originales, dificulta su interpretación. Para superar esta limitación, han surgido algunas alternativas, como el Sparse PCA, que obtiene las \(c.p.\) como un problema minimización del error de reconstrucción forzando a que los autovectores tengan una gran parte de sus componentes nula.

El t-SNE no es la única alternativa no lineal procedente de la comunidad de machine learning. Otras, denominadas actualmente ``aprendizaje múltiple'' (manifold learning), son el Sammon's mapping, curvilinear component analysis (CCA) y sus variantes, Laplacian eigenmaps y maximum variance unfolding (MVU), entre otros; véase \citet{wismuller_et_al2010}.

Finalmente, señalar que el ACP es una técnica matemática que no requiere que las variables originales sigan una distribución normal multivariante, aunque, si así fuera, se podría dar una interpretación más profunda de las \(c.p.\)

\begin{infobox_resume}
\textbf{RESUMEN}

El ACP es una técnica de reducción de la dimensionalidad que captura un gran porcentaje de la variabilidad de un conjunto de variables correladas a partir de un número mucho menor de componentes latentes (las componentes principales) incorreladas. La piedra angular de la construcción de estas componentes son los autovalores de la matriz de covarianzas (o de correlaciones) de las variables originales. En el ACP son cuestiones importantes, entre otras, \((i)\) la determinación del número de componentes a retener, \((ii)\) su interpretación y \((iii)\) la cuantificación del valor de las componentes para cada observación (puntuaciones), que constituyen el input de técnicas híbridas como, por ejemplo, regresión con componentes principales, cluster o partial least squares.

\end{infobox_resume}

\hypertarget{anuxe1lisis-factorial}{%
\chapter{Análisis factorial}\label{anuxe1lisis-factorial}}

\emph{José-María Montero y José Luis Alfaro Navarro}

\hypertarget{introaf}{%
\section{Introducción}\label{introaf}}

Según el trabajo pionero de \citet{harman1976}, el objeto del Análisis Factorial\index{Análisis Factorial} (AF) es la representación de una variable \(X_j\) en términos de varios factores subyacentes no observables\footnote{Se asumirá la representación lineal, por sencillez, pero puede ser cualquier otra.}. En el marco lineal, y considerando \(p\) variables\footnote{Por las mismas razones que en el análisis de componentes principales (ACP), se trabaja con las variables estandarizadas; véase Cap. \ref{acp}.}, hay varias alternativas dependiendo del objetivo que se pretenda:

\begin{itemize}
\tightlist
\item
  La captura de la mayor cantidad posible de la varianza de dichas variables (o ``explicación'' de su varianza).
\item
  La mejor reproducción (o ``explicación'') de sus correlaciones observadas.
\end{itemize}

A modo introductorio, supónganse dos variables politómicas que surgen de las respuestas de \(N\) futbolistas profesionales a dos preguntas: (1) ¿Está usted a gusto en el club? y (2) ¿Se quedaría en el club la siguiente temporada? Las posibles respuestas son: 1, 2, 3, 4, 5 (1 ``en total desacuerdo'' y 5 ``totalmente de acuerdo'').

Cada variable tiene su varianza (nula si todos los futbolistas opinasen igual y máxima si la mitad marcase el 1 y la otra mitad el 5). Esta varianza puede ser ``común'' o compartida'' por las dos variables, o no. Lo normal es que cuanto más a gusto estén los futbolistas en su club mayor sea su deseo de permanecer en él la siguiente temporada, por lo que gran parte de la variabilidad de cada una de las variables sería compartida (ya que la relación -lineal- entre ellas es positiva). El resto de la variabilidad sería ``específica'' de cada variable (puede que un futbolista esté muy bien en el club pero quiera ir a otro más prestigio; o que esté mal, pero a su familia le encante la ciudad) o ``residual'' (normalmente debida a factores de medida). El porcentaje de varianza compartida se mide a través del coeficiente de determinación lineal, \(r^2\). El resto, hasta la varianza unidad, o el 100\%, es varianza ``única'' de cada variable, que incluye tanto la específica como la residual.

De acuerdo con \citet{santiagodelafuente2011}, en el AF caben dos enfoques\footnote{No son los únicos.}:

\begin{itemize}
\tightlist
\item
  El análisis de toda la varianza (común y no común).
\item
  El análisis, únicamente, de la varianza común.
\end{itemize}

Ambos caben bajo el paraguas genérico del AF; ambos se basan en las relaciones entre las variables para identificar grupos de ellas asociadas entre sí. Sin embargo, del primero se ocupa el ACP (Cap. \ref{acp}) y, si se parte de la matriz de correlaciones (cuyas entradas fuera de la diagonal principal, al cuadrado, indican la proporción de varianza compartida por las variables que se cruzan en dicha entrada), ésta lleva unos en la diagonal principal. Al segundo se le aplica la denominación de AF y en la matriz de correlaciones se sustituyen los unos de la diagonal principal por la varianza que cada variable comparte con las demás (su \(comunalidad\)). Por eso se dice que el objetivo del AF es la explicación de la varianza compartida o común de las variables en estudio mediante una serie de factores comunes latentes.\footnote{Ambos enfoques dan resultados similares cuando hay más de 30 variables y las comunalidades (véase Sec. \ref{modelobasicoaf}) son superiores a 0,70, y se interpretan de manera casi idéntica.}

El AF puede ser exploratorio o confirmatorio. En el primero no se establecen consideraciones a priori sobre el número de factores comunes a extraer, sino que éste se determina a lo largo del análisis. Por el contrario, en el segundo se trata de contrastar hipótesis relativas al número de factores comunes, así como sobre qué variables serán agrupadas o tendrán más peso en cada factor. Una práctica habitual es validar mediante el análisis factorial confirmatorio los modelos teóricos basados en los resultados del análisis factorial exploratorio. Sin embargo, \citet{rodriguez2000validez} alertan de los peligros de esta práctica. Este capítulo considera la versión exploratoria del AF.

A efectos prácticos, se utilizará la base de datos \texttt{TIC2021} del paquete \texttt{CDR}, ya trabajada en Cap. \ref{acp} para el ACP, relativa al uso (por empresas e individuos) y equipación (de los hogares) de las TIC en los países de la UE-27, así como la librería \texttt{psych} \citep{Revelle2022} de R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(psych)}
\FunctionTok{library}\NormalTok{(CDR)}
\FunctionTok{data}\NormalTok{(}\StringTok{"TIC2021"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{elementos-teuxf3ricos-del-anuxe1lisis-factorial}{%
\section{Elementos teóricos del análisis factorial}\label{elementos-teuxf3ricos-del-anuxe1lisis-factorial}}

\hypertarget{modelobasicoaf}{%
\subsection{Modelo básico y terminología}\label{modelobasicoaf}}

Considérense \(p\) variables \(\{X_1, X_2,..., X_p\}\) y \(N\) elementos, objetos o individuos, siendo las matrices de datos, \(\bf X\), y datos estandarizados, \(\bf Z\), las siguientes:

\[\bf X=\left(\begin{matrix} x_{11} & x_{12} & \cdots &x_{1N}\\
x_{21}&x_{22}&\cdots&x_{2N}\\
\vdots&\vdots&\ddots     &\vdots\\
x_{p1}&x_{p2}&\cdots&x_{pN}\\
\end {matrix}\right),\quad
\bf Z=\left(\begin{matrix} z_{11} & z_{12} & \cdots &z_{1N}\\
z_{21}&z_{22}&\cdots&z_{2N}\\
\vdots&\vdots&\ddots     &\vdots\\
z_{p1}&z_{p2}&\cdots&z_{pN}\\
\end {matrix}\right),\]

donde el primer subíndice indica la variable y el segundo el elemento.

Mientras que el enfoque de componentes principales está representado por:
\begin{equation} Z_{j}=a_{j1}F_1+a_{j2}F_2+ \cdots +a_{jp}F_p, \quad j=1,2,\cdots,p,
\label{eq:eqaf1}
\end{equation}
en el enfoque AF clásico el modelo teórico es:
\begin{equation}
Z_{j}=a_{j1}F_1+a_{j2}F_2+ \cdots +a_{jk}F_k + b_jSP_j+c_jE_j, \quad j=1,2,\cdots,p,
\label{eq:eqaf2}
\end{equation}
donde \(Z_{j}, \hspace{0,1cm} j=1,2,\cdots, p\), se modeliza, linealmente, en términos de \((i)\) \(k\ll p\) factores comunes, \(F_m,\hspace{0,1cm} m=1,2,\cdots,k\), que dan cuenta de la correlaciones entre las variables \(Z_{j}, \hspace{0,1cm} j=1,2,\cdots, p\), y \((ii)\) un factor específico, \(SP_j, \hspace{0,1cm} j=1,2,\cdots,p\), y un término de error, \(E_j, \hspace{0,1cm} j=1,2,\cdots,p,\) que dan cuenta de la varianza no compartida (específica y residual, respectivamente). Los coeficientes \(a_{jm}\) se denominan \(cargas factoriales\) y, aunque su notación es igual que en el modelo de componentes principales, no tienen por qué coincidir; el problema básico del AF es precisamente la estimación de dichas cargas. En lo que sigue, se aunarán el factor específico y el término de error de \(Z_{j}\) en un factor único, \(U_{j}\).
\begin{equation}
Z_{j}=a_{j1}F_1+a_{j2}F_2+ \cdots +a_{jk}F_k + d_jU_j, \quad j=1,2,\cdots,p,
\label{eq:eqaf3}
\end{equation}

Los supuestos del modelo \eqref{eq:eqaf3} son los siguientes: \((i)\) como en la práctica los factores comunes y únicos son desconocidos, sin pérdida de generalidad pueden suponerse con media cero y varianza unitaria; \((ii)\) los factores únicos se suponen independientes entre sí y de los factores comunes; y \((iii)\) dado que los factores involucrados en el modelo se consideran variables aleatorias, si se asume normalidad, e independencia de los factores comunes, \(\{F_{1},F_{2},\cdots, F_k\}\) sigue una distribución normal multivariante y \(Z_{j},\hspace{0,1cm} j=1,2,\cdots,p,\) una distribución normal.

En términos de valores observados, el modelo AF \eqref{eq:eqaf3} viene dado por:\footnote{Aunque en el modelo figuran explícitamente los valores de los factores, en la práctica hay que estimarlos. En otras versiones del AF estos valores se estiman conjuntamente con los parámetros.}
\begin{equation}
z_{ji}=a_{j1}f_{1i}+a_{j2}f_{2i}+ \cdots +a_{jk}f_{ki} + d_ju_{ji}, \quad i=1,2,\cdots,N; j=1,2,\cdots,p,
\label{eq:eqaf4}
\end{equation}
El modelo AF es muy parecido al de regresión lineal: una variable se describe como una combinación lineal de otro conjunto de variables más un residuo. Sin embargo, en el análisis de regresión las variables son observables, mientras que en el AF son construcciones hipotéticas que solo pueden estimarse a partir de los datos observados. Los propios factores se estiman en una etapa posterior del análisis.

En términos matriciales, y considerando:
\[\bf z=\left(\begin{matrix}Z{1}\\
Z_{2}\\
\vdots\\
Z_{p}\\
\end{matrix}\right),\quad \bf f=\left(\begin{matrix} 
F_{1}\\
F_{2}\\
\vdots\\
F_{k}\\
\end{matrix}\right),\quad \bf u=\left(\begin{matrix} U_{1}\\
U_{2}\\
\vdots\\
U_{p}\\
\end{matrix}\right),\]
\[\bf A=\left(\begin{matrix} a_{11} & a_{12} & \cdots &a_{1k}\\
a_{21}&a_{22}&\cdots&a_{2k}\\
\vdots&\vdots&\ddots     &\vdots\\
a_{p1}&a_{p2}&\cdots&a_{pk}\\
\end {matrix}\right),\quad \bf D=\left(\begin{matrix} d_{1} & 0 & \cdots &0\\
0&d_{2}&\cdots&0\\
\vdots&\vdots&\ddots     &\vdots\\
0&0&\cdots&d_{p}\\
\end {matrix}\right),\]
el modelo \eqref{eq:eqaf3} puede expresarse como \(\bf z=\bf A \bf f +\bf D \bf u\).

Centrándonos en el modelo \eqref{eq:eqaf3}, la varianza de \(Z_j\) viene dada por:
\begin{equation}
V(Z_j)=1= \sum_{m=1}^{k} a_{jm}^2+
2\sum_{m< q }^{k} a_{jm} a_{jq} r_{(F_{mi},F_{qi})}  +d_j^2
\label{eq:eqaf4b}
\end{equation}
y si los factores comunes están incorrelados, \(V(Z_j)=1= \sum_{m=1}^{k} a_{jm}^2+ d_j^2\).

De la expresión \eqref{eq:eqaf4b} surgen las siguientes definiciones:

\begin{itemize}
\tightlist
\item
  \(a_{jm}^2\) es la contribución de \(F_m\) a la varianza de \(Z_j\).
\item
  \(V_m=\sum_{j=1}^{p}a_{jm}^2\) es la contribución de \(F_m\) a suma de las varianzas de todas las variables \(Z_j,\hspace{0,1cm} j=1,2,\cdots,p\).
\item
  \(V=\sum_{m=1}^{k}V_m\) es la contribuión de todos los factores comunes a la varianza de todas las variables \(Z_j,\hspace{0,1cm} j=1,2,\cdots,p\).
\item
  \(\frac{V} {p}\) es un indicador de la \(completitud\) del análisis factorial.
\item
  \(h_j^2=a_{j1}^2+a_{j1}^2+\cdots+a_{jk}^2\) es la \(comunalidad\) de \(Z_j,\hspace{0,1cm} j=1,2,\cdots,p\), es decir la contribución de los factores comunes a la variabilidad de \(Z_j\).
\item
  \(d_j^2\) es la unicidad de \(Z_j,\hspace{0,1cm} j=1,2,\cdots,p\), o contribución de \(U_j\) a la varianza de \(Z_j\). Es un indicador de la medida en la que los factores comunes fracasan a la hora de representar la varianza (unitaria) de \(Z_j\).
\item
  Cuando se descompone el factor único en sus dos componentes (modelo \eqref{eq:eqaf2}), \(b_j^2\) se denomina \(especificidad\) de \(Z_j\) y es la varianza de \(Z_j\) debida a la particular selección de las variables en el estudio, mientras que \(c_j^2\) es la que se debe al error (normalmente de medida), que mide la ``falta de fiabilidad''.
\end{itemize}

\hypertarget{patruxf3n-y-estructura-factoriales}{%
\subsection{Patrón y estructura factoriales}\label{patruxf3n-y-estructura-factoriales}}

Se denomina \emph{patrón factorial} a la siguiente expansión del modelo \eqref{eq:eqaf2},
\begin{equation}
\begin{split}
Z_{1}= a_{11}F_{1}+ a_{12}F_{2}+ \dotsb + a_{1k}F_{k}+ d_1U_1\\
Z_{2}= a_{21}F_{1} + a_{22}F_{2}+ \dotsb + a_{2k}F_{k}+d_2U_2 \\
\ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \ddots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \vdots \ \ \ \\
Z_{p}= a_{p1}F_{1}+ a_{p2}F_{2}+ \dotsb + a_{pk}F_{k}+ d_pU_p\\
\end{split}
\label{eq:eqaf5}
\end{equation}
o simplemente a la tabla, o matriz, con los coeficientes \(a_{jm}\) y \(d_j\) (o únicamente, caso habitual, a la matriz de cargas \(\bf A\)). \(k\) determina la ``complejidad del modelo''.

Se denomina \emph{estructura factorial} al siguiente conglomerado de \(p\) conjuntos de \(k+1\) ecuaciones lineales en \(\{a_{jm}\}\), las \(p\) primeras, y en \(\{d_{j}\}\), la última, \(\hspace{0,1cm} j=1,2,\cdots, p; \hspace{0,1cm} m=1,2,\cdots, k\):\footnote{En caso de variables dicotómicas se utiliza el coeficiente \(\phi\) (véase \ref{medidas}) como medida de correlación momento-producto.}

\begin{equation}
\begin{split}
r_{Z_jF_1} & = a_{j1}r_{F_1F_1}+ a_{j2}r_{F_1F_2}+ \dotsb +a_{jm}r_{F_1F_m}+\dotsb + a_{jk}r_{F_1F_k}\\
\ \ \ & \vdots \ \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ddots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ddots\ \ \ \ \ \ \ \ \vdots \ \ \ \\
r_{Z_jF_m} & = a_{j1}r_{F_mF_1}+ a_{j2}r_{F_mF_2}+ \dotsb +a_{jm}r_{F_mF_m}+\dotsb + a_{jk}r_{F_mF_k}\\
\ \ \ &  \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ddots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ddots\ \ \ \ \ \ \ \vdots \ \ \ \\
r_{Z_jF_k} & = a_{j1}r_{F_kF_1}+ a_{j2}r_{F_kF_2}+ \dotsb +a_{jm}r_{F_kF_m}+\dotsb + a_{jk}r_{F_kF_k}\\ \\
\ \ \ &  \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ddots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ddots \ \ \ \ \ \ \ \vdots \ \ \ \\
r_{Z_jU_j} & = d_j\\
\ \ \ \ \ \ \ \ \ \ \ \ & \vdots
\end{split}
\label{eq:eqaf6}
\end{equation}
En la práctica, viene dada por una tabla, o matriz, \(\bf \Gamma\), con los coeficientes \(\{r_{jm}\}\). Cuando todos los factores están incorrelados el patrón y la estructura coinciden.

El conjunto patrón factorial más estructura factorial se denomina \emph{solución factorial completa}. El patrón factorial muestra la relación lineal de las variables en términos de los factores, como si de una regresión lineal se tratase, y puede usarse para reproducir la correlación entre las variables (y, por tanto, para determinar la bondad de la solución). La estructura factorial es útil para la identificación de los factores y la posterior estimación de las puntuaciones factoriales.

En términos matriciales, denominando
\[\bf F=\left(\begin{matrix} f_{11} & f_{12} & \cdots &f_{1N}\\
f_{21}&f_{22}&\cdots&f_{2N}\\
\vdots&\vdots&\ddots     &\vdots\\
f_{k1}&f_{k2}&\cdots&f_{kN}\\
\end {matrix}\right),\quad \bf \Gamma=\left(\begin{matrix} r_{Z_1F_1} & r_{Z_1F_2} & \cdots &r_{Z_1F_k}\\
r_{Z_2F_1}&r_{Z_2F_2}&\cdots&r_{Z_2F_k}\\
\vdots&\vdots&\ddots     &\vdots\\
r_{Z_pF_1}&r_{X_p^*Z_pF_2}&\cdots&r_{Z_pF_k}\\
\end {matrix}\right),\]
el patrón factorial viene dado por \(\bf Z=\bf A \bf F + \bf D \bf U\). Multiplicando por \(\bf F^{\prime}\) y realizando simples operaciones se tiene que \(\bf \Gamma = \bf A\bf \Phi\), donde \(\bf \Phi\) es la matriz de correlaciones entre los factores comunes. Si los factores comunes están incorrelados, \(\bf \Gamma=\bf A\).

Por último, resaltar que el AF es indeterminado, es decir, dado un conjunto de correlaciones, los coeficientes del patrón factorial no son únicos (dado \(\bf R\), se pueden encontrar infinitos sistemas de factores incorrelados u ortogonales\footnote{La historia es más larga: el AF es indeterminado porque dada una matriz \({\bf {C}}_{kxk}\) no singular, si se define otro vector de factores comunes \(\bf f^*=\bf C^{-1}\bf f\) y otra matriz \(\bf A^*=\bf A\bf C\), entonces \(\bf Z=\bf A \bf f+\bf D \bf u=\bf A^*\bf C^{-1}\bf C\bf f^*+\bf D \bf u=\bf A^*\bf f\bf^*+ D \bf u\) y ambos son equivalentes. Una solución es exigir la incorrelación de los factores comunes (\(\bf \Phi=\bf I\)), con lo que la indeterminación se reduciría solo a cuando \(\bf C\) sea ortogonal. En este caso, la solución será única salvo rotaciones ortogonales.}) consistentes con ella. Por ello, normalmente, tras obtener una solución que ajuste bien los datos originales, se lleva a cabo una rotación de la misma (que ajusta igual de bien dichos datos) que facilite la interpretación de los factores.\footnote{Una solución determina el espacio \(k\)-dimensional que contiene los \(k\) factores comunes pero no su posición exacta.}

\hypertarget{el-anuxe1lisis-factorial-en-la-pruxe1ctica}{%
\section{El análisis factorial en la práctica}\label{el-anuxe1lisis-factorial-en-la-pruxe1ctica}}

\hypertarget{pre-anuxe1lisis-factorial}{%
\subsection{Pre-análisis factorial}\label{pre-anuxe1lisis-factorial}}

\hypertarget{procede-la-realizaciuxf3n-de-un-anuxe1lisis-factorial}{%
\subsubsection{¿Procede la realización de un análisis factorial?}\label{procede-la-realizaciuxf3n-de-un-anuxe1lisis-factorial}}

Antes de comenzar con el AF conviene determinar si procede o no; es decir, si las variables se encuentran fuertemente intercorrelacionadas o no. En caso negativo, el AF no tendría sentido. Para ello, se pueden utilizar procedimientos sencillos como observar si el determinante de \(\bf R\) es bajo (correlaciones altas) o elevado (correlaciones bajas); o calcular la matriz de correlaciones anti-imagen, cuyos elementos son los coeficientes de correlación parcial cambiados de signo. En la diagonal muestra la medida de adecuación muestral para esa variable \(MSA_j\). Para que se den las condiciones de realización del AF, la mayoría de los elementos no diagonales deben ser pequeños y los diagonales deben estar próximos a la unidad.

Otras alternativas más sofisticadas incluyen las dos siguientes:

\textbf{Contraste de esfericidad de Bartlett}\index{Contraste de esfericidad de Bartlett}

Exige normalidad multivariante. Contrasta la incorrelación de las variables, es decir, \(H_0:\bf R=\bf I\) frente a \(H_1:\bf R\neq \bf I\) (o \(H_{0}:|\bf{\bf R}|=1\) frente a \(H_{1}:|\bf{\bf R}|=1\)). El estadístico de contraste es \(d_{\bf R}= - \left( N-1-\frac{1}{6} (2p+5)\right) ln|\mathbf{R}|\) y, bajo \(H_0\), sigue una \(\chi^2_{\frac{p(p-1)}{2}}\), siendo nulo en caso de incorrelación.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{dim}\NormalTok{(TIC2021)[}\DecValTok{1}\NormalTok{]}
\FunctionTok{cortest.bartlett}\NormalTok{(}\FunctionTok{cor}\NormalTok{(TIC2021), n)}\SpecialCharTok{$}\NormalTok{chisq}
\CommentTok{\#\textgreater{} [1] 149.7113}
\FunctionTok{cortest.bartlett}\NormalTok{(}\FunctionTok{cor}\NormalTok{(TIC2021), n)}\SpecialCharTok{$}\NormalTok{p.value}
\CommentTok{\#\textgreater{} [1] 1.992514e{-}21}
\end{Highlighting}
\end{Shaded}

\textbf{Medida de adecuación muestral de Kaiser, Meyer y Olkin (KMO)} \index{Medida de adecuación muestral de Kaiser, Meyer y Oklin (KMO)}

Se basa en la idea de que, entre cada par de variables, el coeficiente de correlación parcial (que mide la correlación existente entre cada par de ellas eliminando el efecto que el resto de variables tiene sobre las dos consideradas), debe ser cercano a cero, puesto que es una estimación de la correlación entre sus factores específicos, que se suponen incorrelados. Por tanto, si el número de coeficientes de correlación parcial no nulos es elevado, la solución factorial no es compatible con los datos.

En otros términos, cuando las variables incluidas en el análisis comparten gran cantidad de información debido a la presencia de factores comunes, la correlación parcial entre cualquier par de variables debe ser reducida. Por el contrario, cuando dos variables comparten gran cantidad de información entre ellas, pero no la comparten con las restantes variables (ni, consecuentemente, con los factores comunes), la correlación parcial entre ellas será elevada, lo cual es un mal síntoma de cara a la idoneidad del AF.

El índice KMO se define como \(KMO=\frac{\displaystyle\sum_{j}\displaystyle\sum_{i \neq j} r_{ji}^2}{\displaystyle\sum_{j}\displaystyle\sum_{i \neq j} r_{ji}^2+\displaystyle\sum_{j}\displaystyle\sum_{i \neq j}r_{ji}^{*2}}\), donde \(r_{ji}^{*}\) es el coeficiente de correlación parcial entre las variables \(Z_j\) y \(Z_i\). Se considera que valores por encima 0,9 implican elevadísismas correlaciones en \(\bf R\); entre 0,5 y 0,9 permiten el AF; y por debajo de 0,5, resultan inaceptables para el AF.

Las \(MSA_j\) mencionadas anteriormente, son la versión del índice KMO limitado a cada variable: \(MSA_{j}= \frac{\displaystyle\sum_{i \neq j} r_{ji}^2}{\displaystyle\sum_{i \neq j} r_{ji}^2+\displaystyle\sum_{i \neq j} r_{ji}^{*2}}.\)

La interpretación es similar a la de KMO, pero mide la adecuación de cada variable para realizar un AF, lo que permite no considerar aquellas variables con menor MSA de cara a mejorar la KMO. No obstante, para eliminar una variable del estudio es aconsejable tener en cuenta también las comunalidades de cada variable, los residuos del modelo, e interpretar los factores obtenidos.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{KMO}\NormalTok{(TIC2021)}\SpecialCharTok{$}\NormalTok{MSA, }\DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 0.83}
\FunctionTok{round}\NormalTok{(}\FunctionTok{KMO}\NormalTok{(TIC2021)}\SpecialCharTok{$}\NormalTok{MSAi, }\DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{}    ebroad    esales esocmedia      eweb    hbroad     hiacc      iuse }
\CommentTok{\#\textgreater{}     0.850     0.671     0.934     0.856     0.808     0.764     0.875}
\end{Highlighting}
\end{Shaded}

Como puede apreciarse, en nuestro ejemplo TIC, tanto el test de Barlett como el índice \(KMO\) y las \(MSA_j\) indican que el AF se puede llevar a cabo con garantías.

\hypertarget{el-problema-de-la-comunalidad-yo-del-nuxfamero-de-factores-comunes}{%
\subsubsection{El problema de la comunalidad y/o del número de factores comunes}\label{el-problema-de-la-comunalidad-yo-del-nuxfamero-de-factores-comunes}}

El objetivo del AF es encontrar una matriz de correlaciones reproducida a partir de los resultados obtenidos, \({\bf R}^{rep}\), con menor rango que la original, \(\bf R\), tal que su diferencia, la matriz de correlaciones de los residuos, \({\bf R}^{res}\), se atribuya únicamente a errores muestrales. \(\bf R\) es una matriz gramiana: simétrica de números reales y de diagonal principal dominante, con lo cual es semidefinida positiva y sus autovalores son nulos o positivos. Por tanto, el número de factores comunes será igual al de autovalores positivos (\(k\leq p\)). Si el punto de partida en el análisis es \(\bf R\), rara vez se obtienen menos factores comunes que variables originales, con lo cual lo cual el AF realmente es un ACP. Ahora bien, como el número de factores comunes coincide con el rango de \({\bf R}^{rep}\), y éste se ve afectado por los valores de la diagonal principal, al sustituir los unos por las estimaciones de las comunalidades (en este caso se está realizando un AF) \({\bf R}^{rep}\) no será, en general, gramiana y \(k< p\). En conclusión: como la solución factorial (\(k< p\)) pasa por el conocimiento del rango de \(\bf R\) o de las comunalidades, o se hipotetiza sobre dicho rango o se hipotetizan o estiman las comunalidades. Normalmente se sigue uno de estos dos caminos:

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{(\arabic{enumi})}
  \tightlist
  \item
    Se parte de un \(k\) prefijado, se lleva a cabo el AF y se contrasta la hipótesis \(H_0\): Núm.\hspace{0,1cm} factores\hspace{0,1cm} comunes = \(k\).
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{(\arabic{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    Se estiman las comunalidades y se obtienen los factores comunes.
  \end{enumerate}
\end{itemize}

En cuanto prefijar un número \(k\) de factores, se pueden seguir los criterios expuestos en Cap. \ref{acp} para determinar el número de componentes principales a retener (criterio de Kaiser, gráfico de sedimentación, porcentaje mínimo de varianza explicada, \ldots).

En cuanto a la estimación de las comunalidades, de las múltiples posibilidades existentes, las siguientes son interesantes por su sencillez y buenos resultados:

\begin{itemize}
\tightlist
\item
  Una de las más sencillas, si el número de variables es grande, es aproximar la comunalidad de una variable por su correlación más alta con las demás variables: \(\hat {h}_j^2=max(r_{j1},r_{j2},\cdots,r_{j(j-1)},r_{j(j+1)},\cdots, r_{jp})\).
\item
  Otra posibilidad es \(\hat{h}_j^2=\frac{r_{js}r_{jt}}{r_{ts}}\), donde \(Z_{s}\) y \(Z_{t}\) son, por este orden, las dos variables más correlacionadas con \(Z_{j}\). Este procedimiento modera el efecto que tendría en el anterior una correlación excepcionalmente elevada.
\item
  En la misma línea, otra posibilidad es el promedio de los coeficientes de correlación entre la variable en cuestión y las restantes: \(\hat{h}_j^2=\frac{\sum_{j \neq s}r_{js}}{p-1}\).
\item
  Otra alternativa es realizar un ACP y tomar como comunalidad de cada variable la varianza explicada por los factores retenidos con el criterio de autovalor mayor que la unidad.
\item
  También se puede utilizar el coeficiente de determinación lineal múltiple de cada variable con las demás como estimación de la cota inferior de sus comunalidades: \(\hat {h}_j^2 \geq r^2_{{Z_j};(Z_{j1},\cdots,Z_{2},\cdots Z_{j-1},Z_{j+1},\cdots, Z_{p})}=1-\frac{1}{r^{jj}}\), donde \(r^{jj}\) es el \(j\)-ésimo elemento de la diagonal de \({\bf R}^{-1}\).
\end{itemize}

Un valor alto de la comunalidad, próximo a \(V(X_j)\), significa que dicha variable está bien representada en el espacio de factores.

\hypertarget{anuxe1lisis-factorial-1}{%
\subsection{Análisis factorial}\label{anuxe1lisis-factorial-1}}

\hypertarget{metodosdeextraccion}{%
\subsubsection{Métodos de extracción de los factores}\label{metodosdeextraccion}}

\textbf{Método de las componentes principales}

Su objetivo es el análisis de toda la varianza, común y no común, (modelo \eqref{eq:eqaf1}). Por consiguiente, las entradas de la diagonal de \(\bf R\)\footnote{Cuando se utiliza este método se debe utilizar \(\bf R\) porque, de otra forma, los factores comunes no tendrían media cero y varianza unitaria.} son unitarias y no se requiere la estimación a priori de las comunalidades; tampoco requiere la estimación a priori del numero de factores comunes, que se determinan a posteriori. Para la exposición del método, así como para su ejemplificación con la base de datos \texttt{TIC2021} del paquete \texttt{CDR}, nos remitimos al Cap. \ref{acp}. Aunque en Cap. \ref{acp} se utilizó la función \texttt{PCA} de la librería \texttt{FactoMineR}, también se puede utilizar la función \texttt{principal} de la librería \texttt{psych}.

Este método tiene la ventaja de que siempre proporciona una solución. Sin embargo, al no estar basado en el modelo \eqref{eq:eqaf3} puede dar estimaciones de las cargas factoriales muy sesgadas, sobre todo cuando hay variables con comunalidades pequeñas.

\textbf{Método de los factores principales}\index{Método de los factores principales}

Es la aplicación del método de componentes principales a la matriz de correlaciones reducida, \({\bf R}^*\), es decir, con comunalidades en la diagonal en vez de unos. Exige, por tanto, la estimación previa de las comunalidades y su objetivo es el análisis de la varianza compartida por todas las variables (modelo \eqref{eq:eqaf3}). Se trata de un procedimiento iterativo que consta de las siguientes etapas:

1.- Cálculo de la matriz de correlaciones muestrales.

2.- Estimación inicial de las comunalidades utilizando el coeficiente de determinación lineal múltiple de cada variable con las demás.\footnote{Una estimación inicial de las comunalidades equivale a una estimación inicial de las varianzas específicas: \(\hat{d}_{j(0)}^2=\hat{\sigma}_{j}^2-\hat{h}_{j(0)}^2\); y si las variables originales están tipificadas: \(\hat{d}_{j(0)}^2=1-\hat{h}_{j(0)}^2\).}

3.- Cálculo de la matriz de correlaciones reducida:

\[\mathbf{R}^*
=\begin{pmatrix}
\hat{h}_{1(0)}^2 & r_{12} & \dotsb & r_{1p}\\
r_{21} & \hat{h}_{2(0)}^2 & \dotsb & r_{2p}\\
\vdots & \vdots & \dotsb & \vdots \\
r_{p1} & r_{p2} & \dotsb & \hat{h}_{p(0)}^2\\
\end{pmatrix}\].

4.- Cálculo de los autovalores y autovectores asociados a \(\mathbf{R}^*\) (matriz no necesariamente semidefinida positiva) y a partir de ellos obtener las estimaciones de la matriz de cargas factoriales \(\bf{A}_{(0)}\). En este paso hay que determinar el número de factores utilizando los criterios del ACP.

5.- A partir de la estimación de \(\bf{A}_{(0)}\), obtención de una nueva estimación de las comunalidades: \(\hat{h}_{j(1)}^2= \hat{a}_{j1(1)}^2+\hat{a}_{j2(1)}^2+ \dotsb +\hat{a}_{jk(1)}^2\) y, por tanto, una nueva estimación de la varianza específica \(\hat{\psi}_{j(1)}^2 =1 - \hat{h}_{j(1)}^2\).

6.- Comparación de \(\hat{h}_{j(1)}^2\) con \(\hat{h}_{j(0)}^2\), \(j=1,2,\cdots,p\). Si hay diferencia significativa se vuelve al paso 3 y si la discrepancia no supera una cantidad prefijada se aceptan como válidas las últimas estimaciones disponibles.

En R, el método de los factores principales se implementa con la función \texttt{fa} de la librería \texttt{psych}, que parte de \({\bf R}^*\) (véase \citet{harman1976} para el procedimiento iterativo y \citet{Revelle2022} para los detalles sobre la manera cómo \texttt{fa} parte de \({\bf R}^*\) y lleva a cabo la extracción de los factores).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{af\_facprin }\OtherTok{\textless{}{-}} \FunctionTok{fa}\NormalTok{(}\FunctionTok{cor}\NormalTok{(TIC2021), }\AttributeTok{nfactors =} \DecValTok{2}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"none"}\NormalTok{, }\AttributeTok{fm =} \StringTok{"pa"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(af\_facprin, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{} Factor Analysis using method = pa}
\CommentTok{\#\textgreater{} Call: fa(r = cor(TIC2021), nfactors = 2, rotate = "none", fm = "pa")}
\CommentTok{\#\textgreater{} Standardized loadings (pattern matrix) based upon correlation matrix}
\CommentTok{\#\textgreater{}              PA1     PA2    h2    u2   com}
\CommentTok{\#\textgreater{} ebroad     0.678   0.189 0.495 0.5050 1.16}
\CommentTok{\#\textgreater{} esales     0.503   0.547 0.553 0.4474 1.99}
\CommentTok{\#\textgreater{} esocmedia  0.796   0.212 0.678 0.3218 1.14}
\CommentTok{\#\textgreater{} eweb       0.872   0.239 0.818 0.1822 1.15}
\CommentTok{\#\textgreater{} hbroad     0.816  {-}0.452 0.869 0.1306 1.56}
\CommentTok{\#\textgreater{} hiacc      0.888  {-}0.439 0.982 0.0181 1.46}
\CommentTok{\#\textgreater{} iuse       0.935  {-}0.023 0.875 0.1248 1.00}
\CommentTok{\#\textgreater{}}
\CommentTok{\#\textgreater{}                    PA1 PA2}
\CommentTok{\#\textgreater{} SS loadings    4.435 0.835}
\CommentTok{\#\textgreater{} Proportion Var 0.634 0.119}
\CommentTok{\#\textgreater{} Cumulative Var 0.634 0.753}
\end{Highlighting}
\end{Shaded}

En la salida anterior, \(SS loadings\) son los autovalores de \({\bf R}^*\), que coinciden con la suma de los cuadrados de las cargas de las variables en cada factor (suma de las cargas al cuadrado por columnas). \(h2\) son las comunalidades (suma de las cargas al cuadrado por filas; solo se muestran las cargas para los dos primeros factores puesto que entre ambos ya acumulan una varianza explicada de más del 75\%). \(u2\) son las varianzas de los factores únicos; finalmente, \(com_j\) es el número de factores comunes necesarios para explicar la variable \(Z_j\): \(c_j=\frac{\left( \sum_{j=1}^{p} a_{jm}^{2} \right)^2}{\sum_{j=1}^{p} a_{jm}^4}\). Cuanto mayor es \(com\) mejor es la calidad de la variable para participar en la extracción factorial. El promedio de los \(com_j\) se denomina índice de complejidad de Hofmann. Una solución de estructura simple perfecta tiene una complejidad de uno (cada variable carga solo en un factor); una solución con elementos distribuidos uniformemente tiene una complejidad mayor que 1. Interesa que la estructura no sea simple y perfecta porque entonces no tendría sentido la reducción dimensional. Por tanto, el índice de Hofmann deberá ser superior a la unidad.

Las comunalidades y especificidades son:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(af\_facprin2}\SpecialCharTok{$}\NormalTok{communality, }\DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{} ebroad esales esocmedia    eweb hbroad hiacc iuse}
\CommentTok{\#\textgreater{}  0.495 0.553      0.678  0.818   0.869 0.982 0.875}
\FunctionTok{round}\NormalTok{(af\_facprin2}\SpecialCharTok{$}\NormalTok{uniquenesses, }\DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{} ebroad esales esocmedia    eweb hbroad hiacc iuse}
\CommentTok{\#\textgreater{}  0.505 0.447      0.322   0.182 0.131  0.018 0.125}
\end{Highlighting}
\end{Shaded}

Nótese que con el método de los factores principales, al aplicar ACP sobre \({\bf R}^*\), los factores obtenidos están incorrelados y la estructura coincide con el patrón.

Los resultados son, en signo, aunque no tanto en valor, similares a los obtenidos por el método de componentes principales. Además, como era de esperar, no permiten una interpretación clara de los factores comunes. Para facilitar dicha interpretación, dichos factores deberán ser rotados (véase Sec. \ref{rotaciones}).

3.- \textbf{Método de Máxima Verosimilitud} \index{Método de Máxima Verosimilitud}

Exige normalidad multivariante y la determinación a priori del número de factores comunes, pero no la estimación de las comunalidades. Obedece al modelo \eqref{eq:eqaf3} y consiste en obtener las estimaciones máximo verosímiles de \(\bf A\) y \(\bf D\). Dado que cualquier transformación ortogonal de \(\bf{A}\) puede ser una solución, se impone la condición de que \(\bf A^{\prime}(DD^{\prime})^{-1}\bf A\) sea diagonal. La log-verosimilitud viene dada por \(l=-\frac{N}{2}\left( log|2\pi{\bf {\Sigma}}|+ tr{\bf {\Sigma}}^{-1}\bf S \right)\), donde \(\bf\Sigma=\bf A \bf A^{\prime}+\bf D \bf D^{\prime}\) y \(\bf S\) son las matrices de covarianzas poblacional y muestral, respectivamente, de las variables \(X_j\), \(j= 1,2,\cdots,p\). \(\hspace{0,1cm} \bf D \bf D^{\prime}\) es la matriz de covarianzas (diagonal) de los factores únicos, donde los elementos de la diagonal representan la parte de la varianza única de cada variable, y en la literatura sobre AF es conocida como \(\bf \Psi\).

La decisión sobre el número de factores comunes, \(k\), que en este método debe hacerse al principio, es muy importante, pues dos soluciones, una con \(k\) factores y otra con \(k+1\), pueden tener matrices de cargas factoriales muy diferentes, al contrario que en el método de componentes principales, donde los primeros \(k\) componentes son siempre iguales. Pues bien, una ventaja del método de máxima verosimilitud es que lleva asociado un test estadístico secuencial para determinar el número de factores (véase Sec. \ref{postanalisis}). Otra ventaja es que las estimaciones máximo-verosímiles son invariantes ante cambios de escala; en consecuencia, las matrices de covarianzas teórica y muestral de la log-verosimilitud pueden ser sustituidas por sus homónimas de correlación sin variación alguna en los resultados. Una desventaja es que puede haber un problema de grados de libertad; o, en otros términos, el número de factores \(k\) debe ser compatible con un número de grados de libertad positivo.

El método máximo verosímil se puede implementar en R con la librería \texttt{pysch} y la función \texttt{fa} (con \texttt{fm=ml}). Otra posibilidad es utilizar la función \texttt{factanal}. En ambos casos hay comprobar el cumplimiento de la hipótesis de normalidad. En nuestro ejemplo \texttt{TIC} no procede su implementación al no cumplirse tal hipótesis.

\textbf{Otros métodos}

Razones de espacio impiden comentar otros métodos de extracción de los factores. Pero, al menos, queremos señalar que la función \texttt{fa} de la librería \texttt{psych} también permite implementar los métodos \((i)\) minres (mínimo residuo), que estima las cargas minimizando (sin ponderaciones) los cuadrados de los residuos no diagonales de \({\bf R}^{res}\); parte de una estimación de \(k\) y, como el método máximo verosímil, no precisa estimar las comunalidades, que se obtienen como subproducto, tras la estimación de las cargas; y \((ii)\) alpha, que maximiza el alfa de Cronbach para los factores. Aunque \texttt{fa} también proporciona el método del centroide o la descomposición triangular (que exigen la estimación de las comunalidades), o el análisis imagen (que requiere el número de factores), en la actualidad están en desuso.

Otros métodos de extracción de los factores son los métodos de mínimos cuadrados no ponderados y mínimos cuadrados generalizados, que minimizan la suma de las diferencias cuadráticas entre las matrices de correlación observadas y reproducidas, en el último caso ponderando los coeficientes de correlación inversamente a la unicidad de las variables (alta unicidad supone baja comunalidad). Ambos son proporcionados por \texttt{fa} y \texttt{FAiR}, que también es una librería muy recomendable.

\hypertarget{rotaciones}{%
\subsubsection{Rotaciones en el modelo de análisis factorial}\label{rotaciones}}

La interpretación de los factores se lleva a cabo a través de la estructura factorial, que, si los factores comunes están incorrelados, coincide con el patrón factorial. Sin embargo, aunque el modelo obtenido sea representativo de la realidad, en ocasiones la interpretación de los factores es harto dificultosa, porque presentan correlaciones similares con un gran número de variables. Como la solución AF no es única (si \(\bf{A}\) es una solución factorial, también lo es cualquier transformación ortogonal), con la rotación se trata de que cada variable tenga una correlación próxima a 1 con un factor y a 0 con el resto, facilitando la interpretación de los factores.

Geométricamente, la \(j\)-ésima fila de la matriz de cargas contiene las coordenadas de un punto en el espacio de las cargas correspondientes a \(X_j\). Al realizar la rotación se tienen las coordenadas respecto a unos nuevos ejes, siendo el objetivo situarlos lo más cerca posible del mayor número de puntos. Esto asociaría cada grupo de variables con un sólo factor, haciendo la interpretación más objetiva y sencilla.

Sea \(\bf T\), una matriz ortogonal (\(\bf T^{\prime} \bf T=\bf T\bf T^{\prime}=\bf I\)), denominada matriz de transformación. Entonces, el modelo \eqref{eq:eqaf3} puede escribirse como \(\bf Z=\bf A\bf T\bf T^{\prime}\bf F+ \bf U=\bf B \bf T^{\prime}\bf F+ \bf U\). Se trata de llegar a una \emph{estructura simple}, que se caracteriza porque en \(\bf B\):

\begin{itemize}
\tightlist
\item
  Cada fila tiene al menos un cero.
\item
  Cada columna tiene, al menos, tantos ceros como factores comunes (\(k\)).
\item
  Cada par de columnas debe ser tal que, para varias variables, una tenga cargas despreciables y la otra no.
\item
  Si \(k\geq 4\), cada par de columnas debe tener un número elevado de variables cuyas cargas sean nulas en ambas variables.
\item
  Para cada par de columnas, el número de variables con cargas no nulas en ambas columnas debe ser muy pequeño.
\end{itemize}

Como se avanzó, se trata de que las variables se aglomeren lo más posible en
torno a los factores comunes, y de la manera más discriminatoria posible. Así mejora la interpretación de éstos y, por lo general, aumenta su significado teórico.

Las rotaciones pueden ser ortogonales \index{Rotaciones ortogonales} u oblícuas, \index{Rotaciones oblicuas} dependiendo de si los nuevos factores siguen estando incorrelacionados (ejes perpendiculares) o no (ejes oblicuos).

\hypertarget{rotaciones-ortogonales}{%
\paragraph{Rotaciones ortogonales}\label{rotaciones-ortogonales}}

Preservan la perpendicularidad de los ejes y no varían las comunalidades, pues \({\bf{B}}{\bf{B}}^{\prime}= {\bf{A}} \bf{T} \bf{T}^{\prime} {\bf{A}}^{\prime}= \bf{A} \bf{A}^{\prime}\). Tampoco modifican los cuadrados de las comunalidades ni, por tanto, la suma de sus cuadrados (para todas las variables): \(\sum_{j=1}^p\sum_{m=1}^k b_{jm}^4+2\sum_{m<r=1}^k\sum_{m=1}^k b_{jm}^2b_{jr}^2\). Y como esta expresión se mantiene invariante, minimizar el segundo término implica maximizar el primero.

Las rotaciones ortogonales más usadas son:

\textbf{Rotación VARIMAX}\index{Rotación VARIMAX}

Se define simplicidad del factor \(m\)-ésimo como la varianza de los cuadrados de las cargas factoriales (rotadas) \(b_{ji}\), \(j=1,2,\cdots,p\):
\({SMPL}_{m} = \frac{\sum_{j=1}^{p} {b}_{jm}^4}{p}- \left(\frac {\sum_{j=1}^{p}b_{jm}^2}{p}\right)^2\). Cuanto mayor es la simplicidad de los factores, más sencilla es su interpretación. Por ello, el objetivo es que \(\bf{T}\) sea tal que se maximice la varianza del cuadrado de las cargas en cada columna del patrón factorial, es decir, en cada factor.

Dicho lo anterior, el método VARIMAX, consiste en la obtención de una \(\bf{T}\) que maximice la suma de las simplicidades de todos los factores, \(V=\sum_{m=1}^{k}{SMPL}_{m}\).\footnote{Este criterio es compatible con el hecho de que en cada fila uno de los elementos esté próximo a cero y los demás a uno, porque la suma de los elementos de la fila, elevados al cuadrado, es la comunalidad fija de la variable.}

Sin embargo, las variables con mayor comunalidad, y por tanto con mayores cargas factoriales, tendrán mayor influencia en la solución final, porque la comunalidad no se ve afectada por la rotación ortogonal. Para evitar esto, Kaiser propuso la rotación VARIMAX NORMALIZADA\footnote{La normalización Kaiser se aplica también en los demás métodos.}, donde las cargas se dividen entre la raíz cuadrada de la comunalidad de la variable correspondiente. Los valores obtenidos son los elementos de \(\bf B\). El procedimiento de cálculo de las cargas de los factores rotados es iterativo, rotándose los factores por parejas hasta que se consigue maximizar la suma de simplicidades normalizadas.

La rotación VARIMAX es muy popular por la robustez de sus resultados, si bien se recomienda para un número no muy elevado de factores comunes.

\textbf{Rotación QUARTIMAX}\index{Rotación QUARTIMAX}

Su objetivo es maximizar la varianza de los cuadrados de todas las cargas factoriales, es decir, maximizar \(Q=\frac{\sum_{j=1}^{p}\sum_{m=1}^{k}{b}_{jm}^4}{pk}-\left( \frac {\sum_{j=1}^{p}\sum_{m=1}^{k}{b}_{jm}^2}{pk} \right)^2\).

Nótese que, como la rotación ortogonal no modifica las comunalidades, \({h}_{j}=\sum_{m=1}^{k} b_{jm}^2\), el segundo término de la expresión anterior no se verá modificado, por lo que el criterio anterior equivale a maximizar \(\frac{\sum_{j=1}^{p}\sum_{m=1}^{k}{b}_{jm}^4}{pk}\).

QUARTIMAX es recomendable cuando el número de factores es elevado. Tiende a generar un factor general, el primero, sobre el que la mayor parte de las variables tienen cargas elevadas, lo cual contradice los objetivos que persigue la rotación.

\textbf{Rotación ORTOMAX}\index{Rotación ORTOMAX}

Es una clase general de los métodos de rotación ortogonal que se construye como una composición ponderada de las dos rotaciones anteriores: \(\alpha Q+ \beta V\), donde \(V\) se multiplica por \(p\) por conveniencia ya que una constante multiplicativa no afecta a la solución. Por tanto, su objetivo es maximizar la expresión: \(ORT=\sum_{m=1}^{k} \left({\sum_{j=1}^{p} {b}_{ji}^4 - \left( \frac{\theta}{p}\sum_{j=1}^{p} b_{ji}^2 \right)^2}\right),\hspace{0,1cm} 0< \theta=\frac{\alpha}{\alpha+\beta}< 1\).

Si \(\theta=1\), se tiene el criterio VARIMAX; si \(\theta=0\), se tiene el criterio QUARTIMAX; si \(\theta=0,5\), se tiene un criterio igualmente ponderado denominado BIQUARTIMAX; y si \(\theta=\frac{k}{2}\), se tiene el criterio EQUAMAX, recomendado por parte de la literatura.

Nótese que QUARTIMAX pone el énfasis en la simplificación de la descripción por filas (variables) de la matriz factorial, mientras que VARIMAX lo pone en la simplificación por columnas ( factores), para satisfacer los requisitos de ``estructura simple''; así, aunque se pueda conseguir la simplicidad de cada variable y que, a la vez, las cargas respecto del mismo factor sean grandes, tal factor queda excluido por la restricción impuesta por la simplificación sobre cada factor \citep{harman1976}.

\hypertarget{rotaciones-oblicuas}{%
\paragraph{Rotaciones oblicuas}\label{rotaciones-oblicuas}}

Superan la incorrelación u ortogonalidad de los factores y se suelen aplicar cuando \((i)\) se sospecha que, en la población, los factores tienen una fuerte correlación y/o \((ii)\) cierta correlación entre los factores permite una gran ganancia en la interpretación de los mismos. Podrían aplicarse siempre, como norma general, puesto que, en realidad, la ortogonalidad es un caso particular de la oblicuidad.

Los procedimientos que proporcionan soluciones con estructura simple oblicua emanan de los mismos criterios objetivos que los que proporcionan soluciones con estructura simple ortogonal. De hecho, si se relajan las condiciones de ortogonalidad, algunos procedimientos de rotación ortogonal pueden adaptarse al caso oblícuo (tal es el caso, por ejemplo, del método OBLIMAX, a partir del criterio QUARTIMAX). Por otra parte, los métodos de rotación oblicua no solo son directos, sino que también pueden introducir los principios de estructura simple que se requieren para la solución factorial primaria de forma indirecta (métodos indirectos). Las rotaciones oblicuas exigen nuevos conceptos y nueva nomenclatura:

\begin{itemize}
\tightlist
\item
  \emph{Factores de referencia}, \({G}_m\), \(m=1, 2,\cdots, k\): para cada factor rotado se puede encontrar un nuevo factor incorrelado con los rotados. A esos nuevos factores de les llama factores de referencia. En caso de rotación ortogonal, los factores de referencia coinciden con los primeros.
\item
  \emph{Estructura factorial de referencia}: hasta ahora, se denominaba estructura factorial a la matriz de correlaciones entre las variables \(Z_j\), \({j=1,2, \cdots, p}\) y los factores rotados, que en el caso ortogonal coincide con la matriz de cargas factoriales rotadas. Pues bien, se denomina estructura factorial de referencia a la matriz de correlaciones entre las variables \(Z_j\) y los factores de referencia. Si la rotación es ortogonal, coincide con la estructura factorial.
\item
  \emph{Matriz de transformación}: en el caso oblicuo se representa por \(\bf\Lambda\).
\item
  \emph{Estructura factorial oblicua}: \(\bf V\), tal que \(\bf V= \bf A \bf \Lambda\); sus elementos son \(v_{jm}\).
\item
  \emph{Cargas}: en el caso oblicuo el término ``carga''se utiliza para denotar la correlación de la variable con el eje de referencia: \(v_{jm}=r_{Z_j;\Lambda_m}\).
\end{itemize}

Mientras las rotaciones ortogonales intentan encontrar la estructura factorial más simple, las oblicuas hacen lo mismo pero con la estructura de referencia.

El método (directo) OBLIMAX maximiza la expresión \(K=\frac{\sum_{j=1}^{p}\sum_{m=1}^{k}v_{jm}^4}{\left(\sum_{j=1}^{p}\sum_{m=1}^{k}v_{jm}^{2}\right)^2}\). Nótese que se trata del criterio QUARTIMAX ortogonal pero incorporando el denominador, puesto que en la rotación oblicua ya no es constante.

El QUARTIMIN directo, también derivado del QUARTIMAX ortogonal, minimiza el criterio \(N=\sum_{j=1}^{p}\sum_{m\leq q=1}^{k}v_{jm}^2v_{jq}^2\), y recibe este nombre por minimizar términos de cuarto grado.

La generalización del criterio ``minimizar \(H=\sum_{j=1}^{p}\sum _{m<q=1}^k b_{jm}^2b_{jq}^2\)'' para factores oblicuos se denomina OBLIMIN y da lugar a métodos indirectos. Entre ellos destaca el COVARIMIN, que se obtiene relajando la condición de ortogonalidad en el VARIMAX minimizando las covarianzas de los cuadrados de los elementos de \(\bf V\): \(C^*=\sum_{m\leq q=1}^{k}\left(p\sum_{j=1}^{p} v_{jm}^2v_{jq}^2-\sum_{j=1}^ {p}v_{jm}^2\sum_{j=1}^ {p}v_{jq}^2\right)\). La versión COVARIMIN normalizada minimiza \(C=\sum_{m\leq q=1}^{k}\left(p\sum_{j=1}^{p} \frac{v_{jm}^2}{h_j^2}\frac{v_{jq}^2}{h_j^2}-\sum_{j=1}^ {p}\frac {v_{jm}^2}{h_j^2}\sum_{j=1}^ {p}\frac{v_{jq}^2}{h_j^2}\right)\).

Se ha comprobado empíricamente que QUARTIMIN tiende a ser demasiado oblicuo y COVARIMIN demasiado ortogonal. Una solución intermedia es el método BIQUARTIMIN, que consiste en minimizar \(B^*=H+\frac{C^*}{p}\), donde \(\frac{C^*}{p}\) es la expresión completa del COVARIMIN. Una generalización del BIQUARTIMIN es \(B^*=\alpha H+\beta \frac{C^*}{p}\). Sencillas operaciones aritméticas llevan a \(B^*=\sum_{m< q=1}^{k}\left(p \sum_{j=1}^{p} v_{jm}^2 v_{jq}^2-\gamma \sum _{j=1}^{p}v_{jm}^2 \sum_{j=1}^{p}v_{jq}^2\right)\), con \(\gamma= \frac {\beta}{\alpha + \beta}\). El criterio QUARTIMIN se obtiene con \(\gamma=0\), el BIQUARTIMIN con \(\gamma=0,5\) y el COVARIMIN con \(\gamma=1\). También se pueden obtener versiones normalizadas sin más que normalizar las cargas (dividirlas por \(h_{jm}^2\)).

El criterio BINORMALMIN (normalizado) es una alternativa al BIQUARTIMIN para corregir el sesgo de oblicuidad del criterio COVARIMIN. Minimiza \(D=\sum_{m< q=1}^{k}\left( \frac{\sum_{j=1}^{p} \frac {v_{jm}^2}{h_j^2} \frac {v_{jq}^2}{h_j^2}} {\sum _{j=1}^{p} \frac{v_{jm}^2}{h_j^2}\sum _{j=1}^{p} \frac{v_{jq}^2}{h_j^2}}\right)\). BINORMALMIN suele ser mejor con datos muy simples o muy complejos; BIQUARTIMIN es más recomendable con datos moderadamente complejos.

El OBLIMIN directo en vez de proceder como \(B^*\), que depende de los valores de la estructura, minimiza directamente una función de la matriz del patrón factorial primario: \(F{(\bf{A})}= \sum_{m< q=1}^{k}\left(\sum_{j=1}^p a_{jm}^2 a_{jq}^2-\frac{\delta}{p}\sum_{j=1}^pa_{jm}^2\sum_{j=1}^pa_{jq}^2\right)\). Cuando \(\delta=0\) se tiene el QUARTIMIN directo.

Hay otros tipos de transformaciones oblícuas, pero únicamente se mencionarán \((i)\) el método ORTOBLICUO, que llega a la solución oblicua mediante una serie de transformaciones ortogonales intermedias; y \((ii)\) el método PROMAX, muy popular, que actúa alterando los resultados de una rotación ortogonal (concretamente elevando las cargas de la rotación ortogonal a una potencia entre 2 y 4) hasta crear una solución con cargas factoriales lo más próximas a la estructura ideal. Cuanto mayor es esta potencia más oblicua es la solución obtenida.

\hypertarget{rotaciones-ortogonales-u-obluxedcuas}{%
\paragraph{¿Rotaciones ortogonales u oblícuas?}\label{rotaciones-ortogonales-u-obluxedcuas}}

La selección del método de rotación, ortogonal u oblicua, depende del objetivo perseguido. Si se pretende reducir el número de variables originales a un conjunto mucho menor de variables incorrelacionadas para su uso posterior en otra técnica, por ejemplo regresión, la rotación debe ser ortogonal. Si el objetivo es obtener unos factores teóricos significativos, puede resultar apropiada la aplicación de una rotación oblicua.

En R es muy sencillo implementar una rotación ortogonal u oblicua. Basta, por ejemplo, con utilizar librería \texttt{GPArotation} \citep{Coen2005} e indicarlo en el argumento \texttt{rotate} de la función \texttt{fa}. A modo de ejemplo, extrayendo los factores por el método de los factores principales y utilizando una rotación VARIMAX normalizada, sería:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(GPArotation)}
\NormalTok{af\_facprin2 }\OtherTok{\textless{}{-}} \FunctionTok{fa}\NormalTok{(}\FunctionTok{cor}\NormalTok{(TIC2021), }\AttributeTok{nfactors =} \DecValTok{2}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"varimax"}\NormalTok{, }\AttributeTok{fm =} \StringTok{"pa"}\NormalTok{, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{af\_facprin2 }\CommentTok{\# el objeto contiene información adicional no relevante en estos momentos}
\CommentTok{\#\textgreater{} Factor Analysis using method = pa}
\CommentTok{\#\textgreater{} Standardized loadings (pattern matrix) based upon correlation matrix}
\CommentTok{\#\textgreater{}              PA1 PA2   h2    u2 com}
\CommentTok{\#\textgreater{} ebroad     0.38 0.59 0.50 0.505 1.7}
\CommentTok{\#\textgreater{} esales     0.02 0.74 0.55 0.447 1.0}
\CommentTok{\#\textgreater{} esocmedia  0.46 0.68 0.68 0.322 1.7}
\CommentTok{\#\textgreater{} eweb       0.50 0.75 0.82 0.182 1.7}
\CommentTok{\#\textgreater{} hbroad     0.91 0.20 0.87 0.131 1.1}
\CommentTok{\#\textgreater{} hiacc      0.96 0.26 0.98 0.018 1.1}
\CommentTok{\#\textgreater{} iuse       0.72 0.60 0.88 0.125 1.9}
\CommentTok{\#\textgreater{}}
\CommentTok{\#\textgreater{}                 PA1  PA2}
\CommentTok{\#\textgreater{} SS loadings    2.87 2.40}
\CommentTok{\#\textgreater{} Proportion Var 0.41 0.34}
\CommentTok{\#\textgreater{} Cumulative Var 0.41 0.75}
\end{Highlighting}
\end{Shaded}

Nótese que la salida por defecto es la normalizada. También se puede utilizar la libería \texttt{stats} indicando \texttt{T} o \texttt{F} en el argumento \texttt{normalize}, dependiendo de que se quiera o no, respectivamente, una rotación VARIMAX (u otra) normalizada.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(stats)}
\FunctionTok{varimax}\NormalTok{(}\FunctionTok{loadings}\NormalTok{(af\_facprin), }\AttributeTok{normalize =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

En el ejemplo del uso las TIC en los países de la UE-27, la rotación VARIMAX ha conseguido facilitar la interpretación de los factores comunes, ya que, tras la rotación, las variables relacionadas con el uso de las TIC a escala individual y de hogar cargan en el primer factor, mientras que las relacionadas con el uso de las TIC a nivel empresarial cargan en el segundo. Por tanto, ambos factores pueden considerarse indicadores de la dotación y uso de las TIC en los ámbitos familiar y empresarial, respectivamente. El lector puede probar (y comparar) con otras rotaciones sin más que incluirlas en el argumento \texttt{rotate}.

\hypertarget{postanalisis}{%
\subsection{Post-análisis factorial}\label{postanalisis}}

Realizado el AF, los siguientes procedimientos permiten comprobar la bondad del modelo obtenido:

\textbf{Análisis de las correlaciones residuales}

Se entiende por bondad de la solución factorial la medida del grado en el que los factores del modelo explican las correlaciones entre las variables. Por ello, parece natural que tal medida se base en la comparación entre las correlaciones observadas y las que se derivan del modelo factorial (reproducidas) o, en términos matriciales, en la magnitud de las entradas de la matriz de correlaciones residuales \({\bf R}^{res}={\bf R} - {\bf R}^{rep}\), donde \({\bf R}=\frac{1}{N} \bf Z \bf Z^{\prime}\) y \({\bf R}^{rep}=\bf A\bf \Phi\bf A^{\prime}=\bf \Gamma \bf A^{\prime}\) (relación fundamental entre el patrón y la estructura factorial; en caso de incorrelación entre los factores, \(\bf \Phi=\bf I\) y \({\bf R}^{rep}=\bf A {\bf A}^{\prime}\). \({\bf R}^{rep}\) se obtiene sin más que sustituir \(\bf Z\) por \(\bf A \bf F\) en la expresión de \(\bf R\)).

Ahora bien, ¿cuál es el criterio apropiado para concluir si una solución factorial es aceptable o no? Para que sea aceptable, los elementos (los residuos) de \({\bf R}^{res}\) deben ser cercanos a cero, y como todos los factores comunes han sido considerados, se supone que no existen más vínculos entre las variables y que la distribución de dichos residuos debe ser como la de correlación cero en una muestra del mismo tamaño. Por tanto, como \(\sigma_{r=0}=\frac{1}{\sqrt{N-1}}\), \(S_{r_{res}}\leq\frac{1}{\sqrt{N-1}}:\)\footnote{Este criterio tiene como ventaja la simplicidad. Sin embargo, sería conveniente que tuviese en cuenta, al menos, el número de variables.}

\begin{itemize}
\tightlist
\item
  Si \(S_{r_{res}}\gg\frac{1}{\sqrt{N-1}}\) es razonable pensar que existen relaciones adicionales significativas entre las variables y hay que modificar la solución factorial.
\item
  Si \(S_{r_{res}}\ll\frac{1}{\sqrt{N-1}}\) es razonable pensar que la solución factorial incluye relaciones que no están justificadas.
\item
  Si \(S_{r_{res}}\leq pero \hspace{0,1cm} no\ll \frac{1}{\sqrt{N-1}}\) la solución es aceptable.
\end{itemize}

Otra posibilidad, también muy sencilla, propuesta por \citet{Revelle2022}, es \(fit= 1-\frac{\sum \left (\bf R-{\bf FF}^{\prime}\right)^2}{\sum (\bf R)^2}\), que indica la reducción proporcional en la matriz de correlación debida al modelo factorial. Nótese que esta medida es sensible al tamaño de las correlaciones originales.Es decir, si los residuos son pequeños, pero las correlaciones son pequeñas, el ajuste es malo. Las medidas clásicas como el RMS error (raíz cuadrada del error cuadrático medio), o similares, también son susceptibles de uso.

En el ejemplo TIC seguido en este capítulo el ajuste realizado es muy bueno:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(af\_facprin2}\SpecialCharTok{$}\NormalTok{residual, }\DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{}           ebroad esales esocmedia   eweb hbroad  hiacc   iuse}
\CommentTok{\#\textgreater{} ebroad     0.505 {-}0.068     0.008  0.068 {-}0.045  0.002  0.003}
\CommentTok{\#\textgreater{} esales    {-}0.068  0.447     0.026  0.015  0.004 {-}0.012  0.021}
\CommentTok{\#\textgreater{} esocmedia  0.008  0.026     0.322 {-}0.047  0.014 {-}0.005  0.017}
\CommentTok{\#\textgreater{} eweb       0.068  0.015    {-}0.047  0.182  0.012  0.015 {-}0.042}
\CommentTok{\#\textgreater{} hbroad    {-}0.045  0.004     0.014  0.012  0.131 {-}0.005  0.010}
\CommentTok{\#\textgreater{} hiacc      0.002 {-}0.012    {-}0.005  0.015 {-}0.005  0.018  0.002}
\CommentTok{\#\textgreater{} iuse       0.003  0.021     0.017 {-}0.042  0.010  0.002  0.125}
\NormalTok{af\_facprin2}\SpecialCharTok{$}\NormalTok{rms}
\CommentTok{\#\textgreater{} [1] 0.02907475}
\NormalTok{af\_facprin2}\SpecialCharTok{$}\NormalTok{fit}
\CommentTok{\#\textgreater{} [1] 0.9715865}
\end{Highlighting}
\end{Shaded}

\begin{infobox}
\textbf{NOTA IMPORTANTE}

Como se avanzó en la introducción, el AF está enfocado al ajuste de las correlaciones entre las variables observadas mediante el patrón factorial correspondiente al modelo \eqref{eq:eqaf2}(con los factores comunes y el factor único). Pues bien, si en el proceso reproductivo se utiliza el modelo solo con los factores comunes, la matriz de correlaciones que se reproduce es \(\bf R\), lo que implica el modelo ACP (modelo \eqref{eq:eqaf1}). Si se incluye también el factor específico, la matriz de correlaciones que se reproduce es \({\bf R}^*\) (modelo AF). Si en dicha reproducción se utilizasen los factores comunes y el término de error, se reproduciría \(\bf R\) con una diagonal principal cuyas entradas serían la unidad menos las estimaciones de las comunalidades.

\end{infobox}

\textbf{Test de bondad de ajuste}

Se trata de un contraste de razón de verosimilitudes que se puede llevar a cabo cuando se extraigan los factores por el método de máxima verosimilitud. La hipótesis nula es la suficiencia de \(k\) factores comunes para explicación de las correlaciones entre las variables originales y de la varianza que comparten.

El estadístico del contraste es \(-2ln\lambda=np(\hat{a} - ln \hat{g} -1]\), donde \(\hat{a}\) y \(\hat{g}\) son las medias aritmética y geométrica, respectivamente, de los autovalores de la matriz \(\hat{\boldsymbol{\Sigma}}_{H_{0}}^{-1} \mathbf{S}\). Bajo \(H_0\), se distribuye asintóticamente como una \(\chi_{df}^2\), con \(df= \left( p+\frac{p(p+1)}{2}\right) - \left( p+pk+p-\frac{k(k-1)}{2}\right)= \frac{1}{2} (p-k)^2- \frac{1}{2}(p+k)\).\footnote{\(df\) indica la medida en que el modelo factorial ofrece una interpretación más simple que \(\bf \Sigma\).}

Este test se aplica de manera secuencial: se formula como hipótesis nula \(k=0\). Si no se rechaza, no hay factores comunes subyacentes. Si se rechaza, se sigue con \(k=1\). Si no se rechaza \(k=1\), se concluye que el modelo con un factor es una adecuada representación de la realidad; si se rechaza, se formula la hipótesis nula de que \(k=2\), y el proceso continua hasta que no se rechace la hipótesis nula, siempre que el valor de \(k\) sea compatible con un número de grados de libertad positivo.

\hypertarget{puntuaciones-factoriales.}{%
\subsection{Puntuaciones factoriales.}\label{puntuaciones-factoriales.}}

Las puntuaciones factoriales son las estimaciones de los valores de los factores aleatorios no observados, es decir, de los elementos de \({\bf F}_{mxm}\). Así, \(\hat{f}_{im}\) será la estimación del valor del \(m\)-ésimo factor para la \(i\)-ésima observación (elemento, individuo, objeto\ldots). Cuando se extraen los factores por componentes principales las puntuaciones son exactas.

Estas estimaciones pueden ser usadas como inputs para posteriores análisis en los que se trabaje con los mismos elementos o individuos sustituyendo las variables originales por los nuevos factores obtenidos (regresión, cluster, etc.). La cuestión es: ¿cómo calcular estas puntuaciones?, porque tanto los factores como los errores no son observables sino aleatorios.

Los métodos más populares para obtener la estimación de las puntuaciones factoriales son \((i)\) el de regresión por mínimos cuadrados ordinarios (MCO), donde \(\hat{\bf F}=\left (\bf A^{\prime} \bf A \right)^{-1}\bf A^{\prime}\bf Z\); \((ii)\) el de Bartlett, basado en el método de estimación por mínimos cuadrados generalizados (MCG), con \(\hat{\bf F}=\left (\bf A^{\prime} \bf \Psi ^{-1}\bf A \right)^{-1}\bf A^{\prime}\Psi ^{-1}\bf Z\). El mismo estimador se puede obtener por máxima verosimilitud asumiendo normalidad multivariante; \((iii)\) el de Thompson (con un enfoque bayesiano), donde \(\hat{\bf F}=\left (\bf I+\bf A^{\prime} \bf \Psi ^{-1}\bf A \right)^{-1}\bf A^{\prime}\Psi ^{-1}\bf Z\); y \((iv)\) el de Anderson-Rubin (que obtiene estimaciones MCG imponiendo la condición \(\bf F^{\prime}F =I\) (\(\hat{\bf F}=\left (\bf A^{\prime} \bf \Psi ^{-1}\bf R \bf \Psi ^{-1}\bf A \right)^{-1}\bf A^{\prime}\Psi ^{-1}\bf Z\)). Las ventajas y desventajas de cada uno de ellos pueden verse en \citet{mardiaetal1979} y \citet{santiagodelafuente2011}.

En nuestro ejemplo de las TIC, las puntuaciones de los dos factores extraídos con el método de factores principales y rotados con VARIMAX (la rotación no afecta a las puntuaciones), calculadas por el método de regresión, para los países de la UE-27 (se muestran los de Bélgica, Bulgaria y la República Checa), se obtienen en R como sigue:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{af\_facprin3 }\OtherTok{\textless{}{-}} \FunctionTok{fa}\NormalTok{(}\FunctionTok{cor}\NormalTok{(TIC2021), }\AttributeTok{nfactors =} \DecValTok{2}\NormalTok{, }\AttributeTok{rotate =} \StringTok{"VARIMAX"}\NormalTok{, }\AttributeTok{fm =} \StringTok{"pa"}\NormalTok{, }\AttributeTok{scores =} \StringTok{"regression"}\NormalTok{)}
\FunctionTok{factor.scores}\NormalTok{(TIC2021, af\_facprin3)}\SpecialCharTok{$}\NormalTok{scores[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, ]}
\CommentTok{\#\textgreater{}             PA1         PA2}
\CommentTok{\#\textgreater{} [1,]  0.6256359  1.01289866}
\CommentTok{\#\textgreater{} [2,] {-}2.1820404 {-}0.03439974}
\CommentTok{\#\textgreater{} [3,] {-}0.2189723  1.08635525}
\end{Highlighting}
\end{Shaded}

\hypertarget{relaciones-y-diferencias-entre-el-af-y-el-acp}{%
\section{Relaciones y diferencias entre el AF y el ACP}\label{relaciones-y-diferencias-entre-el-af-y-el-acp}}

ACF y AF son aparentemente muy similares, pero en realidad son muy diferentes.
Tanto ACP como AF son técnicas de reducción de la dimensionalidad que aparecen juntas en los paquetes estadísticos y persiguen objetivos muy similares, lo cual, en determinadas ocasiones, lleva al lector a pensar que son intercambiables entre sí, cuando ello no es cierto. Por ello, este capítulo finaliza con un breve comentario sobre las diferencias más relevantes entre ambos enfoques.

La primera es que ACP es una mera transformación de los datos en la que no se hace ningún supuesto sobre la matriz de covarianzas o de correlación. Sin embargo, AF asume que los datos proceden de un modelo bien definido, el modelo \eqref{eq:eqaf3}, en el que los factores subyacentes satisfacen unos supuestos bien definidos.

En segundo lugar, en ACP el énfasis se pone en el paso desde las variables observadas a las componentes principales, mientras que en AF se pone en el paso desde los factores latentes a las variables observadas. Es cierto que en ACP se pueden retener \(k\) componentes y a partir de ellas aproximar (reproducir) las variables observadas; sin embargo, esta manera de proceder parece menos natural que la aproximación de las variables observadas en términos de los factores comunes y, además, al no tener en cuenta la unicidad de las variables, sobrestima las cargas factoriales y la dimensionalidad del conjunto de variables originales.

Una tercera diferencia es que, mientras que ACP obtiene componentes en función de las variables originales (los valores de las variables pueden ser estimados a posteriori en función de dichas componentes o factores), en AF las variables son, ellas mismas, combinaciones lineales de factores desconocidos. Es decir, mientras que en ACP la solución viene de la mano de la descomposición en valores singulares, en AF requiere procedimientos de estimación, normalmente iterativos.

La cuarta es que ACF es un procedimiento cerrado mientras que AF es abierto, en el sentido de que explica la varianza común y no toda la varianza.

Finalmente, como pudo verse en \ref{metodosdeextraccion}, cuando las varianzas de los factores únicos son prácticamente nulas, el método de los factores principales es equivalente a ACP y cuando son pequeñas ambos dan resultados similares. Sin embargo, cuando son grandes, en ACP las componentes principales (tanto las retenidas como las que no se retienen) las absorben, mientras que el AF las considera y les da su lugar.

\begin{infobox_resume}
\textbf{RESUMEN}

El Análisis Factorial es una técnica de reducción de la dimensionalidad que trata de dar una explicación de la varianza compartida, o común, de las variables en estudio (no de toda, como hace el análisis de componentes principales) mediante un número mucho menor de factores comunes latentes. Por consiguiente, solo tiene sentido implementarlo si dichas variables se encuentran fuertemente correlacionadas. Tras introducir al lector en los principales elementos teóricos del Análisis Factorial (el modelo básico y la solución factorial completa), se abordan las distintas etapas del procedimiento en su vertiente práctica: \((i)\) el pre-análisis factorial, que responde a la pregunta de si procede o no llevarlo a cabo; \((ii)\) el análisis factorial propiamente dicho, prestando especial atención a los métodos de extracción de los factores y a las rotaciones de los mismos para facilitar su interpretación; y \((iii)\) el post-análisis factorial, que incluye una serie de procedimientos para determinar si la solución factorial obtenida es o no aceptable. Posteriormente, se aborda la cuestión de cómo estimar los valores de los factores obtenidos para cada elemento o individuo involucrado en el análisis, pues estas estimaciones pueden usarse como inputs en análisis posteriores (regresión, cluster, etc.) sustituyendo las variables originales por los factores obtenidos. El capítulo finaliza con algunos comentarios sobre las diferencias entre el análisis factorial y el de componentes principales, aparentemente muy similares, pero en realidad muy diferentes.

\end{infobox_resume}

\hypertarget{escalamiento-multidimensional}{%
\chapter{Escalamiento multidimensional}\label{escalamiento-multidimensional}}

\emph{José María Montero y José Luis Alfaro Navarro}

\hypertarget{introducciuxf3n-18}{%
\section{Introducción}\label{introducciuxf3n-18}}

El escalado multidimensional \index{Escalamiento Multidimensional} (EMD) fue propuesto por primera vez a la Universidad de Princeton por Warren S. Torgerson a principios de la década de 1950 siendo un investigador importante en este campo Joseph Bernard Kruskal. El EMD engloba una variedad de técnicas multivariables cuya finalidad es obtener la estructura (factores o dimensiones) de los individuos (o variables) subyacente a una matriz de datos empíricos, lo que se consigue al representar dicha estructura en una forma geométrica bi o tridimensional.

Por tanto, la idea del EMD es representar los datos en baja dimensión (usualmente 2) utilizando la información proporcionada por las distancias entre los datos. Esta técnica surgue ya que cada vez con más frecuencia los datos particulares de los que se dispone y el objetivo del análisis hacen difícil su tratamiento con las medidas clásicas, por lo que se han ido diseñando nuevas medidas de distancia entre datos. Estas medidas se pueden utilizar para diferentes tareas: agrupamiento de casos, clasificación, detección de patrones o dimensiones subyacentes, recuperación de información, etc. Por lo tanto, EMD aborda algunas problemáticas que pueden ser analizadas con otras técnicas como, por ejemplo, análisis de componentes principales o factorial cuando el objetivo es representar muchas variables en pocas dimensiones mediante la identificación de la estructura interna de los datos, dimensiones o factores en base a la matriz de correlaciones como medida de proximidad entre las variables o el análisis cluster cuando el objetivo es analizar la proximidad entre los objetos, personas, productos,\ldots. estudiados.

El EMD analiza matrices de proximidad (similitud, disimililitud o distancia), por ello, es una alternativa más flexible que otros métodos multivariantes con los que comparte objetivos, ya que sólo requiere de una matriz con las proximidades entre los datos, que pueden representar valoraciones personales, grado de acuerdo entre juicios, parecido entre objetos, frecuencias de aparición de rasgos, diferencias entre tratamientos, etc. La idea central es que las distancias que median entre los puntos se corresponden con las proximidades entre los objetos por medio de una función de ajuste resultante de un proceso iterativo de optimización, pudiendose describir las relaciones entre los objetos sobre la base de las proximidades observadas \citep{LopezHidalgo2010}

En R, existen distintas funciones para desarrollar el EMD, desde las clásicas funciones cmdscale de la librería \textbf{base} e isoMDS del paquete \textbf{MASS} hasta el enfoque más actual, usado en este documento, recogido en la librería \textbf{smacof} \citep{Leeuw2009, Mair2022} que proporciona al usuario una gran flexibilidad para especificar EMD. Utiliza siempre matrices de disimilaridad y, desde la primera versión, se han implementado varios enfoques adicionales de EMD y despliegue, así como varias extensiones y funciones de utilidad.

A modo de ejemplo se va a usar la información relacionada con 7 variables de la sociedad de la información disponibles para 27 países europeos en la base de datos TIC2021, cuatro relacionadas con el uso de las TIC por parte de las empresas y tres de aspectos relacionados con el uso por parte de las personas y la equipación de los hogares.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(smacof)}
\FunctionTok{library}\NormalTok{(CDR)}
\FunctionTok{data}\NormalTok{(}\StringTok{"TIC2021"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{mediciuxf3n-de-distancias-y-similitudes}{%
\section{Medición de distancias y similitudes}\label{mediciuxf3n-de-distancias-y-similitudes}}

Tanto para el EMD como para muchas otras técnicas multivariantes, el concepto de distancia, entendida como medida de diferenciación entre objetos, constituye la base fundamental de la obtención y presentación de sus resultados. También son frecuentes los conceptos de ``disimilaridad'', muy parecido al de distancia, o de ``similaridad'', dual en su sentido al de distancia. Se nombre como se nombre, la característica que hay que tener siempre presente es si la medida indica ``alejamiento'' entre los objetos (distancia o disimilaridad) o ``cercanía'' (similaridad o proximidad).

Básicamente, se considera una medida de distancia a una función que asigna a cada par de objetos (\(o_i\) y \(o_j\)), que pueden contener mediciones de variables x e y, un número real, \(d(o_i, o_j)=\delta_{ij}\), que debe cumplir las siguientes condiciones:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  No negatividad \(\delta_{ij} \geq 0\)
\item
  Simetría, \(\delta_{ij} = \delta_{ji}\)
\item
  Identificación del objeto, \(\delta_{ii}=0\)
\end{enumerate}

Si además es semidefinida positiva y cumple la desigualdad triangular se dice que \emph{\(\delta\)} es una distancia métrica.

Aunque no se va a profundizar en ello, existen diferencias matemáticas en los requisitos que debe cumplir una medida para ser considerada una distancia o una distancia métrica, así como las condiciones para ser considerada una similaridad. Básicamente, se considera una medida de similaridad a una aplicación que asigna a cada par de objetos (\(o_i\) y \(o_j\)) un número real, \(s_{ij}\), que cumple las mismas condiciones que la distancia salvo la condición \emph{c} para la que tiene que cumplir que \(s_{ij} \leq s_{ii}\).

Esta condición es más díficil de cumplir por lo que se emplean mucho más las medidas de distancia al ser más sencillo formular la propiedad \emph{c} pues simplifica mucho el poder atribuir un valor de referencia cero para definir la distancia de un individuo a sí mismo. La similitud carece de este valor de referencia, siendo posible que la similitud de un individuo a sí mismo sea diferente de unos a otros. A pesar de esta dificultad, las medidas de similitud surgen de modo natural en muchos problemas relacionados con valoraciones subjetivas de similitud.

Para un conjunto finito de objetos, la \emph{matriz de similaridad} \index{matriz de similaridad} es:

\[S= \begin{pmatrix}
s_{11} & s_{12} & \dotsb & s_{1n}\\
s_{21} & s_{22} & \dotsb & s_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
s_{n1} & s_{n2} & \dotsb & s_{nn}\\
\end{pmatrix}
\]

El paso de una medida de similaridad (\(s_{ji}\)) a una distancia (\(\delta_{ij}\)) se puede hacer de diversas formas. Las más usuales son:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  \(\delta_{ij} = 1-s_{ij}\)
\item
  \(\delta_{ij} = \sqrt{1-s_{ij}}\)
\item
  Si los valores de la diagonal de S no son la unidad, ) \(\delta_{ij} = \sqrt{s_{ii}+s_{jj}-2s_{ij}}\)
\end{enumerate}

En general, cuando las características que se miden sobre los objetos son variables
cuantitativas p-dimensionales, las distancias más usadas son:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  La distancia euclídea \index{Distancia euclídea}:
  \[d_e (x,y)=\sqrt{\displaystyle\sum_{i=1}^{p}{(x_i - y_i)^2}}\]
\item
  La distancia absoluta, o city-block \index{Distancia absoluta o city-block}:
  \[d_1 (x,y)= \displaystyle\sum_{i=1}^{p}{|x_i - y_i|}\]
\item
  La distancia de Mahalanobis\index{Distancia Mahalanobis}, usando la matriz de covarianzas de las
  variables:
  \[d_M^2 (x,y)= (x - y)' \boldsymbol{\Sigma}^{-1}(x - y)\]
\end{enumerate}

Cuando las variables son binarias (0 y 1), los coeficientes de similaridad más
utilizados son:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Coeficiente de Jaccard \index{Distancia de Jaccard}:
  \[s_{ij}= \frac{a}{a+b+c}\]
\item
  Coeficiente de Sokal-Michener \index{Distancia de Sokal-Michener}:
  \[s_{ij}= \frac{a+d}{a+b+c+d}\]
\end{enumerate}

donde a, b, c y d son las frecuencias de (1,1), (1,0), (0,1) y (1,1)c, respectivamente.

Por último, en el caso más general en el que existan variables cuantitativas, binarias y/o cualitativas, se suele utilizar la distancia de Gower\index{Distancia de Gower}, \(d_{ij}^2=1-s_{ij}\), donde:

\[s_{ij}=\frac{\displaystyle\sum_{h=1}^{p_1}{(1- |x_{ih}-x_{jh}|G_h)+a+\alpha}}{p_1+(p_2-d)+p_3}\]

siendo \(p_1\) el número de variables cuantitativas y \(G_h\) el rango de la h-ésima variable; a y d son el número de coincidencias y no coincidencias para las \(p_2\) variables binarias; y \(\alpha\) el número de coincidencias para las \(p_3\) variables
cualitativas.

Como se aprecia, las características de los datos que se quieren analizar influyen determinantemente en qué tipo de medida de proximidad utilizar. A su vez, la elección de una medida concreta puede modificar la configuración de los datos y, consecuentemente, los resultados de los análisis que se hagan a partir de ellos.

\hypertarget{modelo-escalamiento-multidimensional.}{%
\section{Modelo escalamiento multidimensional.}\label{modelo-escalamiento-multidimensional.}}

El EMD parte de una matriz de proximidades entre n objetos:

\[\boldsymbol{\Delta} _{nxn}=
\begin{pmatrix}
\delta_{11} & \delta_{12} & \dotsb & \delta_{1n}\\
\delta_{21} & \delta_{22} & \dotsb & \delta_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
\delta_{n1} & \delta_{n2} & \dotsb & \delta_{nn}\\
\end{pmatrix}\]

y busca una representación de los n objetos en un espacio de menor dimensión,
m, donde \(x_{ij}\) es la coordenada del objeto i en la dimensión j:

\[\mathbf{X}_{nxm}=
\begin{pmatrix}
x_{11} & x_{12} & \dotsb & x_{1m}\\
x_{21} & x_{22} & \dotsb & x_{2m}\\
\vdots & \vdots & \ddots & \vdots\\
x_{n1} & x_{n2} & \dotsb & x_{nm}\\
\end{pmatrix}\]

de forma que se puede calcular la distancia euclídea entre cada par de objetos
\[d_{ij}=\sqrt{\displaystyle\sum_{t=1}^{m}{(x_{it} - x_{jt})^2}}\]
y, construir una matriz de distancias ``reproducidas''

\[\mathbf{D}_{nxn}=
\begin{pmatrix}
d_{11} & d_{12} & \dotsb & d_{1n}\\
d_{21} & d_{22} & \dotsb & d_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
d_{n1} & d_{n2} & \dotsb & d_{nn}\\
\end{pmatrix}\]

que aproxime, en la medida de lo posible, a la matriz de proximidades, \(\boldsymbol{\Delta}\).

El concepto básico del EMD es que las distancias entre los objetos en la configuración X, \(d_{ij}\) deben corresponder a las proximidades originales, \(\delta_{ij}\) mediante una transformación, \(d_{ij}=f(\delta_{ij})\), donde \emph{f} es de algún tipo determinado.

En la práctica, no se suele encontrar un ajuste perfecto, por lo que existe un cierto grado de error. Por ello, se define el Stress de Kruskal \index{Stress de Kruskal} como una medida de la bondad de ajuste del modelo:
\[Stress= \sqrt{\frac {\sum{(f(\delta_{ij})-d_{ij})^2}}{\sum d_{ij}^2}}\]
En caso de un ajuste perfecto, el Stress sería 0, aumentando conforme más grandes sean los errores (diferencias entre las distancias ``reproducidas'' y las originales). Así, la solución proporcionada por el EMD será ``mejor'' cuanto más
pequeña sea la medida del Stress. También es frecuente utilizar una variante, llamada S-Stress \index{S-Stress}, definida como:

\[S-Stress= \sqrt{\frac {\sum{(f(\delta_{ij})^2-d_{ij}^2)^2}}{\sum (d_{ij}^2)^2}}\]

Otra medida que se suele utilizar como grado de ajuste es el coeficiente de correlación al cuadrado (RSQ), que indica la proporción de variabilidad de los datos explicada por el modelo:

\[RSQ=\frac{[\sum{(d_{ij}-d_{..})(f(\delta_{ij})-f(\delta_{..}))}]^2}{[\sum{(d_{ij}-d_{..})^2][\sum{(f(\delta_{ij})-f(\delta_{..}))}^2]}}\]

Este valor oscila entre 0 y 1, y se intrepreta de forma contraria a las medidas de Stress: mientras mayor sea el RSQ, mejor ajuste del modelo.

\hypertarget{tipos-de-escalamiento-multidimensional}{%
\section{Tipos de escalamiento multidimensional}\label{tipos-de-escalamiento-multidimensional}}

La elección de la función \emph{f} que relaciona las proximidades originales y las distancias ``reproducidas'' produce dos tipos básicos de EMD, el EMD métrico (o clásico) y el EMD no-métrico. En el primero, se considera que los datos están medidos en escala de intervalo o de razón, mientras que en el segundo se considera que los datos están en escala ordinal.

\hypertarget{escalado-multidimensional-muxe9trico}{%
\subsection{Escalado multidimensional métrico}\label{escalado-multidimensional-muxe9trico}}

En el modelo de escalamiento métrico \index{Escalamiento métrico} se asume que la relación entre las proximidades y las distancias es de tipo lineal, \(d_{ij}=a+b \delta_{ij}\). De esta forma, se conserva la métrica de distancia original, entre puntos, lo mejor posible, siendo adecuado para el caso de variables cuantitativas. También se conoce como EMD clásico o como análisis de coordenadas principales.

En el ejemplo introductorio, se va a aplicar un EMD métrico con un doble objetivo: por un lado se usa la matriz de correlaciones entre la variables con el objetivo de analizar la similitud entre las mismas y, por otro lado, se determina la distancia entre observaciones con el objetivo de analizar la similitud exitente entre observaciones, en este caso los países europeso.

En el primer caso, los ``objetos'' son las siete variables TIC2021 y se ha utilizado como medida de proximidad (similitud) el coeficiente de correlación entre las variables, por lo que se plantea un EMD métrico.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{correlacion }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(TIC2021)}
\NormalTok{correlacion[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, ]}
\CommentTok{\#\textgreater{}              ebroad    esales esocmedia      eweb    hbroad     hiacc      iuse}
\CommentTok{\#\textgreater{} ebroad    1.0000000 0.3768434 0.5874489 0.7043619 0.4223139 0.5211193 0.6320885}
\CommentTok{\#\textgreater{} esales    0.3768434 1.0000000 0.5420147 0.5845905 0.1666307 0.1945647 0.4789584}
\CommentTok{\#\textgreater{} esocmedia 0.5874489 0.5420147 1.0000000 0.6979517 0.5669977 0.6085944 0.7563839}
\end{Highlighting}
\end{Shaded}

Los pasos a seguir son:

• Calcular la matriz de disimilaridades sobre la que actúa smacof. En este ejemplo, se usa la matriz de correlaciones que se debe convertir en matriz de disimilaridades, mediante la función sim2diss.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datos }\OtherTok{\textless{}{-}} \FunctionTok{sim2diss}\NormalTok{(correlacion, }\AttributeTok{method =} \StringTok{"corr"}\NormalTok{, }\AttributeTok{to.dist =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

La conversión de similidades (correlaciones) en disimilaridades se ha hecho por el método ``corr'', que utiliza la expresión general \(\delta_{ij} = 1-s_{ij}\). Existen otros métodos en la función sim2diss para cuando la matriz de proximidades no sea de correlaciones. El argumento to.dist=TRUE permite convertir el resultado en un objeto de la clase dist.

• Una vez que disponemos de la matriz de disimilaridades, aplicamos el EMD métrico mediante la función mds, versión equivalente a la función smacofSym.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{mds}\NormalTok{(datos, }\AttributeTok{ndim =} \DecValTok{2}\NormalTok{, }\AttributeTok{type =} \StringTok{"ratio"}\NormalTok{)}
\NormalTok{res}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} mds(delta = datos, ndim = 2, type = "ratio")}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Model: Symmetric SMACOF }
\CommentTok{\#\textgreater{} Number of objects: 7 }
\CommentTok{\#\textgreater{} Stress{-}1 value: 0.161 }
\CommentTok{\#\textgreater{} Number of iterations: 42}
\end{Highlighting}
\end{Shaded}

El gráfico con la representación de los objetos según sus distancias ``reproducidas'' muestra la configuración final de los siete objetos:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(res)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/graficoobjetos.ch13-1} \end{center}

La información numérica detallada se podria obtener con la información de la salidad dada por: ``res\$conf'' que mostraría las coordenadas de los objetos en las dos dimensiones; ``res\$confdist'' que muestra la matriz de distancias reproducidas; ``res\$stress'' para obtener la medida de stress de Krukal y ``res\$spp'' con la contribución porcentual de cada objeto al stress.

En este caso los resultados muestran que la medida de stress es razonablemente baja con un valor de 0,16, indicando una buena ``reproducción'' de las proximidades originales. Además, la ``contribución'' relativa al stress de cada uno de los objetos (delitos) es bastante homogénea, siendo la variable porcentaje de empresas con banda ancha (EBROAD) la que más contribuyen al stress y el nivel de acceso a internet de los hogares (HIACC), la que menos. Para ver gráficamente el grado de ajuste, se usa el gráfico de Shepard que compara las proximidades originales y las distancias obtenidas:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(res, }\AttributeTok{plot.type =} \StringTok{"Shepard"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/graficoShepard.ch13-1} \end{center}

El diagrama de Shepard incluye las proximidades originales entre pares de objetos (
en gris claro) y las obtenidas por el EMD (en negro). También representa el método elegido; en este ejemplo, al usar el argumento method=``ratio'' estamos imponiendo una relación proporcional entre ambos tipos de similitudes, por lo que aparece una recta (que pasa por el origen). Dadas las diferencias que se ven en el gráfico, quizás la elección del método ``ratio'' (opción por defecto), no sea la más adecuada. Probando con el método ``interval'', que impone una relación lineal entre ambos tipos
de similitudes (recta que no tiene que pasar necesariamente por el origen), se obtendría:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res2 }\OtherTok{\textless{}{-}} \FunctionTok{mds}\NormalTok{(datos, }\AttributeTok{type =} \StringTok{"interval"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(res2, }\AttributeTok{plot.type =} \StringTok{"Shepard"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/mdsinterval.ch13-1} \end{center}

La medida de stress se ha reducido a la mitad, 0,087, indicando mejor ajuste, y el gráfico de Shepard muestra más concordancia entre las proximidades originales de los pares de objetos y las distancias reproducidas. Otra opción sería usar el método ``mspline'', pero en este caso las diferencias son menores. Los tres métodos son métricos, puesto que se fija una forma funcional para relacionar las proximidades originales y las disimilitudes del modelo (un ratio, una función lineal o una función spline).

• Por último, para ``interpretar'' el sentido de las dimensiones en las que se representan los objetos, se recurre a ver cuáles están en los extremos. En la parte izquierda de la dimensión 1 están las variables relacionadas con el uso en las empresas mientras que en la parte derecha están las relacionadas con los hogares y las personas; se podría decir, entonces, que es una dimensión relacionada con el ámbito de uso de las TIC. En la dimensión 2, con menos distancias, y una interpretación menos clara, aparecen en la parte superior las variables relacionadas con el tipo de conexión y la existencia de web en las empresas y en la parte inferior las relacionadas con las redes sociales las ventas o la frecuencia de uso de internet por parte de los individuos; se podría decir, \textbf{que es una dimensión asociada al uso dado a las TIC}.

Los pasos para desarrollar el mismo análisis agrupando países, observaciones en lugar de variables, serían similares pero usando la matriz de distancias en lugar de la matriz de correlaciónes, por lo tanto:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(factoextra)}
\NormalTok{d\_euclidea }\OtherTok{\textless{}{-}} \FunctionTok{get\_dist}\NormalTok{(}\AttributeTok{x =}\NormalTok{ TIC2021, }\AttributeTok{method =} \StringTok{"euclidea"}\NormalTok{)}
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{mds}\NormalTok{(d\_euclidea, }\AttributeTok{ndim =} \DecValTok{2}\NormalTok{, }\AttributeTok{type =} \StringTok{"ratio"}\NormalTok{)}
\NormalTok{res}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} mds(delta = d\_euclidea, ndim = 2, type = "ratio")}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Model: Symmetric SMACOF }
\CommentTok{\#\textgreater{} Number of objects: 27 }
\CommentTok{\#\textgreater{} Stress{-}1 value: 0.112 }
\CommentTok{\#\textgreater{} Number of iterations: 77}
\FunctionTok{plot}\NormalTok{(res)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/analisismdsobservaciones.ch13-1} \end{center}

En este caso, la interpretación de la dimensión 1 muestra la existencia de una diferencia clara entre los países del norte y los de última adhesión a la Unión Europea mientras que en la dimensión 2, con unas diferencias menores, aparecen en la parte superior los países de Chipre y Luxemburgo y en la parte inferior Lituanía, Croacia, Irlanda y la República Checa.

\hypertarget{escalado-multidimensional-no-muxe9trico}{%
\subsection{Escalado multidimensional no-métrico}\label{escalado-multidimensional-no-muxe9trico}}

En el modelo de escalamiento no-métrico \index{Escalamiento no-métrico} (también conocido como EMD ordinal) no se asume ninguna fórmula métrica que relacione las proximidades originales con las distancias reproducidas, sino que sólo describe un patrón creciente (o decreciente) entre ellas. No es significativo el valor de distancia, sino su relación con las distancias entre otros pares de objetos, por lo que construye distancias ajustadas que están en el mismo orden de rango que la proximidad original. Por ejemplo, si la distancia de los objetos separados A y B ocupa el tercer lugar en los datos de proximidades originales ordenadas, entonces también debería ocupar el tercer lugar en los datos de distancias reproducidas ordenadas. Así, el EMD no-métrico busca conservar, no tanto los valores de las proximidades originales, sino la ordenación de los objetos en función de dichas proximidades; es, por tanto, un modelo que se ajusta mejor a datos cualitativos que el métrico.

Continuando con el ejemplo anterior, una alternativa más flexible es asumir una relación no métrica, donde sólo se conserve la ordenación de las proximidades originales. En este caso, se busca que las distancias reproducidas ordenen a los pares de objetos de forma idéntica a la original. Para ello, se debe utilizar el método ``ordinal'' dentro de la función mds:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res3 }\OtherTok{\textless{}{-}} \FunctionTok{mds}\NormalTok{(datos, }\AttributeTok{type =} \StringTok{"ordinal"}\NormalTok{)}
\NormalTok{res3}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} mds(delta = datos, type = "ordinal")}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Model: Symmetric SMACOF }
\CommentTok{\#\textgreater{} Number of objects: 7 }
\CommentTok{\#\textgreater{} Stress{-}1 value: 0.021 }
\CommentTok{\#\textgreater{} Number of iterations: 65}
\end{Highlighting}
\end{Shaded}

La medida de stress desciende hasta 0,021 y el gráfico de Shepard muestra una alta concordancia entre las ordenaciones original y reproducida. La información númerica detallada se puede obtener de forma similar al caso métrico.

Por último, destacar que puede ser interesante analizar la estabilidad de las soluciones del EMD (bien mediante jackknife, bootstrap, o elipses de pseudo-confianza). También puede interesar plantear un modelo de EMD que permita valorar las diferencias individuales (abordable con la función smacofIndDiff ). Otra alternativa puede ser abordar un desplegamiento multidimensional, que representa conjuntamente objetos e individuos.

\begin{infobox_resume}
\textbf{RESUMEN}

La aplicación del análisis EMD implica tres pasos consecutivos:
• La determinación de las proximidades originales entre los objetos. Esta fase depende de las características de los objetos y del tipo de relación que se quiera/pueda establecer entre ellos. Actualmente, R dispone de librerías que permiten estimar las matrices de proximidad a partir de los datos en bruto.
• La conversión de las proximidades en similaridades (si fuese necesario) y el ajuste entre las originales y las reproducidas por el EMD. Se debe elegir el tipo de EMD a utilizar, que depende de la función de ajuste: se puede optar por funciones de tipo ``ratio'', ``interval'' o ``mspline'' (EMD métricos) o ``ordinal'' (EMD no métrico). La elección estará relacionada con el tipo de datos usados y el grado de ajuste (stress) de los modelos.
• La interpretación de los resultados a partir de la configuración obtenida, tanto del significado de las dimensiones como de la estructura de los objetos (cuáles se parecen, si existen grupos, etc.)

\end{infobox_resume}

\hypertarget{correspondencias}{%
\chapter{Análisis de correspondencias}\label{correspondencias}}

\emph{Román Mínguez Salido} y \emph{Manuel Vargas Vargas}

\hypertarget{introducciuxf3n-19}{%
\section{Introducción}\label{introducciuxf3n-19}}

El análisis de correspondencias es un método gráfico descriptivo de reducción de la dimensión incluido entre los algoritmos de aprendizaje no supervisado. La idea principal es equivalente al método de componentes principales, pero aplicado a variables cualitativas. El objetivo es representar los valores (\textbf{niveles} en \textbf{R}) de variables cualitativas (\textbf{factores} en \textbf{R}) en ejes cuantitativos cuyas coordenadas representen la cercanía o lejanía de los niveles de los factores. Es decir, es un método de reducción de la dimensionalidad para factores representables en pocas dimensiones.

El punto de partida es una \textbf{tabla de contingencia RxC} \(T\) (véase capítulo anterior \ref{tablas-contin}) que recoge la frecuencia de cada par de niveles \(A_1,A_2,...,A_R\) del factor \(A\) y \(B_1,B_2,...,B_C\) del factor \(B\):

\begin{longtable}[]{@{}llllll@{}}
\toprule()
& \(B_1\) & \(B_2\) & \ldots{} & \(B_C\) & Total \\
\midrule()
\endhead
\(A_1\) & \(n_{11}\) & \(n_{12}\) & \ldots{} & \(n_{1C}\) & \(n_{1·}\) \\
\(A_2\) & \(n_{21}\) & \(n_{22}\) & \ldots{} & \(n_{2C}\) & \(n_{2·}\) \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} \\
\(A_R\) & \(n_{R1}\) & \(n_{R2}\) & \ldots{} & \(n_{RC}\) & \(n_{R·}\) \\
Total & \(n_{·1}\) & \(n_{·2}\) & \ldots{} & \(n_{·C}\) & \(N\) \\
\bottomrule()
\end{longtable}

Cada fila representa el \textbf{perfil condicional} del nivel \(A_i\), siendo la última el \textbf{perfil marginal} del factor \(A\). Igualmente, cada columna representa el \textbf{perfil condicional} del nivel \(B_j\), siendo la última el \textbf{perfil marginal} del factor \(B\).

Si los factores fueran independientes, el \textbf{valor esperado} en cada casilla sería \(e_ij=\frac{n_{i·}n_{·j}}{N}\), por lo que la diferencia tipificada, \(r_ij=\frac{n_{ij}- e_{ij}}{\sqrt{e_{ij}}}\) es una medida de asociación entre las modalidades \(A_i\) y \(B_j\). La matriz formada por estos ``residuos'', \(R=(r_{ij})\) resume la asociación entre los atributos, y es el objetivo básico del análisis de correspondencias; básicamente, se realiza una proyección de las filas y columnas de la tabla de frecuencias relativas (transformadas) para obtener las coordenadas en ejes cuantitativos, representables en la forma habitual como diagramas de puntos.

Para un estudio en profundidad de esta técnica pueden consultarse la referencias \citet{Greenacre2008} (en español) o \citet{BehLombardo2014}. En el resto del capítulo se hará una breve exposición de la metodología y se ejemplificará con el análisis de una tabla de contingencia.

\hypertarget{metodologuxeda-del-anuxe1lisis-de-correspondencias}{%
\section{Metodología del análisis de correspondencias}\label{metodologuxeda-del-anuxe1lisis-de-correspondencias}}

Dada una tabla de contingencia \(T\), a partir de las frecuencias observadas \(n_ij\), se definen las \textbf{distancias} entre los perfiles,

\begin{itemize}
\item
  para los perfiles fila, \(d_{ii'}= \sum_{k=1}^C \frac {1}{n_{·k}} \left( \frac {n_{ik}}{n_{i·}} - \frac {n_{i'k}}{n_{i'·}} \right)^2\)
\item
  para los perfiles columna, \(d_{jj'}= \sum_{k=1}^R \frac {1}{n_{k·}} \left( \frac {n_{kj}}{n_{·j}} - \frac {n_{kj'}}{n_{·j'}} \right)^2\)
\end{itemize}

Estas distancias aumentan cuanto más se \emph{``diferencien''} unos perfiles de otros. El análisis de correspondencias busca construir \textbf{``dimensiones''} (habitualmente, dos) y obtener las coordenadas de los niveles de ambos factores
\begin{equation}
A = \begin{pmatrix}a'_1\\ \vdots\\a'_R\end{pmatrix} \text{,    con } a_i= (a_{i1} \ a_{i2})' \text{,    y } \ B = \begin{pmatrix}b'_1\\ \vdots\\b'_C\end{pmatrix} \text{,   con } b_j= (b_{j1} \ b_{j2})'
\end{equation}
siendo \(a_i\) las coordenadas del nivel fila \(A_i\) y \(b_j\) las del nivel columna \(B_j\) en el plano, de forma que:

\begin{itemize}
\tightlist
\item
  \emph{``reproduzcan''} las distancias entre perfiles fila y columna y las diferencias tipificadas (asociaciones):
  \begin{equation}
  \begin{array}{crl}
  {d(a_i , a_{i'})= \sqrt {(a_{i1}-a_{i'1})^2+(a_{i2}-a_{i'2})^2} \approx d_{ii'}} \\ {d(b_j , b_{j'})= \sqrt {(b_{j1}-b_{j'1})^2+(b_{j2}-b_{j'2})^2} \approx d_{jj'}} \\ a'_i * b_j \approx r_{ij} \end{array}
  \end{equation}
\end{itemize}

Con las coordenadas contenidas en las matrices \(A\) y \(B\), es posible \emph{``visualizar''} la posición relativa de cada factor. Esta estructura permite ver, tanto las \emph{``distancias''} que hay entre los niveles de cada factor (mediante la distancia de representación en el plano), como las \emph{``asociaciones}'' entre niveles de ambos factores (ya que mientras más asociación haya, más cerca se representarán en el plano).

Para resolver el problema de estimación de las matrices \(A\) y \(B\), se busca una descomposición de la matriz de \(R=(r_{ij})\) en valores singulares. Según la importancia que se de al ajuste de uno de los perfiles o a la matriz de residuos, se obtienen diferentes métodos de selección, llamados \textbf{normalizaciones}, que pueden consultarse en \citet{Greenacre2008}.

\hypertarget{proyecciones-fila-columna-y-simuxe9trica}{%
\subsection{Proyecciones fila, columna y simétrica}\label{proyecciones-fila-columna-y-simuxe9trica}}

El punto de partida es la matriz de frecuencias relativas \(P=T(n_{ij})/N\), también llamada \textbf{matriz de correspondencias}. Definiendo el vector de unos \(\mathbf{1}\), con las dimensiones adecuadas, las masas de filas y columnas, \(r_i = \sum_{j=1}^C p_{ij}\) y \(c_j = \sum_{i=1}^R p_{ij}\) respectivamente, se pueden expresar matricialmente como \(r=P \mathbf{1}\) y \(c=P' \mathbf{1}\) o, en forma de matrices diagonales,
\[D_R=diag(r) \equiv diag(f_{1·},...,f_{R·}) \text{  y } \ D_C=diag(c) \equiv diag(f_{·1},...,f_{·C})\]
Se calcula la matriz de \textbf{residuos estandarizados} como
\begin{equation}
S=D_R^{-\frac {1}{2}} (P-rc') D_C^{-\frac {1}{2}}
\end{equation}

Esta matriz \(S\) se descompone en valores singulares, calculando matrices \(U\), \(D\) y \(V\) tales que:
\begin{equation}
\begin{array}{crl}
{S=UDV'} \\ {UU'=V'V=I \ \text{ , } \ U_{(RxK)}} \ V_{(CxK)} \ K=min(R-1, \ C-1) \\ {D=diag(\mu_1,...,\mu_K)} \end{array}
\end{equation}
donde los \(\mu_i\) son los llamados \textbf{valores singulares}, estando ordenados de forma decreciente \(\mu_1 \geq \mu_2 \geq ... \geq \mu_K\).

A partir de esta descomposición se pueden obtener:

\begin{itemize}
\item
  las \textbf{coordenadas estándares de las filas} \(\Phi=D_R^{-\frac {1}{2}}U\), y sus \textbf{coordenadas principales} \(F=\Phi D\).
\item
  las \textbf{coordenadas estándares de las columnas} \(\Gamma=D_C^{-\frac {1}{2}}V\), y sus \textbf{coordenadas principales} \(G=\Gamma D\).
\item
  las \textbf{inercias principales}, \(\lambda_i=\mu_i^2\).
\end{itemize}

Las coordenadas principales son las utilizadas para definir las \textbf{proyecciones fila} y \textbf{proyecciones columna}, que representan en menor dimensión los perfiles correspondientes, formando los llamados \textbf{mapas asimétricos}.

Por último, las matrices \(A=D_R^{-\frac {1}{2}} UD\) y \(B=D_C^{-\frac {1}{2}} VD\) representan las coordenadas de ambos perfiles en un espacio común, llamado \textbf{mapa simétrico}.

\hypertarget{procedimiento-con-r-la-funciuxf3n-ca}{%
\section{\texorpdfstring{Procedimiento con \textbf{R}: la función \texttt{ca()}}{Procedimiento con R: la función ca()}}\label{procedimiento-con-r-la-funciuxf3n-ca}}

Para realizar un análisis de correspondencias simple con \textbf{R} se puede utilizar la librería \textbf{ca}, que contiene la función \texttt{ca()}. Esta función acepta como argumento de entrada o bien directamente una tabla de contingencia, así como los datos originales como objeto matriz o data-frame. Incluso, el argumento puede ser una fórmula del tipo \textasciitilde{} F1 + F2 donde F1 y F2 son factores. Entre los argumentos adicionales se pueden incluir el número de dimensiones en el output así como la posibilidad de incluir filas o columnas suplementarias.

Como primer ejemplo, se van a utilizar los datos \emph{``housetasks''}, contenidos en la librería \textbf{factoextra} y que representan una tabla de contingencia con la frecuencia de ejecución de 13 tareas del hogar por los miembros de la pareja.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ca)}
\FunctionTok{library}\NormalTok{(factoextra)}
\FunctionTok{data}\NormalTok{(}\StringTok{"housetasks"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

En primer lugar, la aplicación del test \(\chi^2\) permite comprobar el grado de asociación entre ambos factores:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(housetasks)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Pearson\textquotesingle{}s Chi{-}squared test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  housetasks}
\CommentTok{\#\textgreater{} X{-}squared = 1944.5, df = 36, p{-}value \textless{} 2.2e{-}16}
\end{Highlighting}
\end{Shaded}

Con un valor de \(\chi^2 =1944.5\) y un p-valor de 2.2e-16, hay suficiente evidencia como para rechazar la hipótesis nula de independencia, indicando asociación entre ambos factores, por lo que tiene sentido analizar más en profundidad la estructura de dicha asociación.

La aplicación de la función \texttt{ca()} proporciona los valores singulares y, tanto para filas como para columnas, las masas, las distancias chi-cuadrado, y las inercias explicadas, así como las coordenadas en el espacio proyectado:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ca\_house }\OtherTok{\textless{}{-}} \FunctionTok{ca}\NormalTok{(housetasks, }\AttributeTok{k =} \DecValTok{2}\NormalTok{)}
\NormalTok{ca\_house}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Principal inertias (eigenvalues):}
\CommentTok{\#\textgreater{}            1        2        3       }
\CommentTok{\#\textgreater{} Value      0.542889 0.445003 0.127048}
\CommentTok{\#\textgreater{} Percentage 48.69\%   39.91\%   11.4\%   }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Rows:}
\CommentTok{\#\textgreater{}           Laundry Main\_meal    Dinner Breakfeast   Tidying    Dishes  Shopping}
\CommentTok{\#\textgreater{} Mass     0.100917  0.087729  0.061927   0.080275  0.069954  0.064794  0.068807}
\CommentTok{\#\textgreater{} ChiDist  1.152997  1.016747  0.785880   0.715740  0.593815  0.549821  0.466440}
\CommentTok{\#\textgreater{} Inertia  0.134160  0.090692  0.038246   0.041124  0.024667  0.019587  0.014970}
\CommentTok{\#\textgreater{} Dim. 1  {-}1.346122 {-}1.188346 {-}0.939962  {-}0.690273 {-}0.534477 {-}0.256462 {-}0.159717}
\CommentTok{\#\textgreater{} Dim. 2  {-}0.742517 {-}0.734702 {-}0.461866  {-}0.678779  0.651108  0.662533  0.604596}
\CommentTok{\#\textgreater{}          Official   Driving Finances Insurance   Repairs Holidays}
\CommentTok{\#\textgreater{} Mass     0.055046  0.079702 0.064794  0.079702  0.094610 0.091743}
\CommentTok{\#\textgreater{} ChiDist  0.984014  1.128542 0.675490  0.852589  1.818512 1.462801}
\CommentTok{\#\textgreater{} Inertia  0.053300  0.101509 0.029564  0.057936  0.312874 0.196311}
\CommentTok{\#\textgreater{} Dim. 1   0.307586  1.006731 0.367485  0.878213  2.074861 0.342675}
\CommentTok{\#\textgreater{} Dim. 2  {-}0.380181 {-}0.979506 0.926221  0.710229 {-}1.295584 2.151159}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Columns:}
\CommentTok{\#\textgreater{}              Wife Alternating   Husband  Jointly}
\CommentTok{\#\textgreater{} Mass     0.344037    0.145642  0.218463 0.291858}
\CommentTok{\#\textgreater{} ChiDist  0.935393    0.899443  1.321252 1.038436}
\CommentTok{\#\textgreater{} Inertia  0.301019    0.117824  0.381373 0.314725}
\CommentTok{\#\textgreater{} Dim. 1  {-}1.136821   {-}0.084397  1.575600 0.202801}
\CommentTok{\#\textgreater{} Dim. 2  {-}0.547487   {-}0.437116 {-}0.902313 1.538902}
\end{Highlighting}
\end{Shaded}

Las dos primeras dimensiones explican el 48.69\% y 39.91\%, por lo que la representación en un plano engloba al 88.6\% de la inercia. Los valores \emph{``Mass''} corresponden a las masas (frecuencias relativas) de filas y columnas respectivamente.

Las distancias chi-cuadrado representan las distancias en esa métrica de cada fila respecto a la fila centroide (dada por la masa de las columnas, promedio de los vectores fila). Así, estos valores indican lo cerca o lejos que está cada fila respecto al centroide de las mismas. En este ejemplo, la fila más distante del centroide de filas es \emph{``Repairs''} (1.819), mientras que la columna más distantes respecto del centroide de columnas es \emph{``Husband''} (1.321).

Las inercias representan la distancia cuadrática \(\chi^2\) respecto al perfil promedio (sin calcular raíces), ponderada por la masa (de la fila o columna) correspondiente. Como la inercia mide la variabilidad, en este ejemplo, respecto a las filas, el nivel que mayor contribuye es Repairs (0.312874) mientras que por columnas es Husband (0.381373). Esto no es sorprendente ya que ambos niveles eran los más alejados del centro.

Con las coordenadas de las dimensiones se puede realizar un gráfico de las mismas utilizando la función \texttt{plot()}, pudiéndose optar por la proyección sólo de las filas (map= ``rowprincipal'', what=c(``all'',``none'')) o de las columnas (mapa ``colprincipal'', what=c(``none'',``all'')), tal como se muestra en la fig.~\ref{fig:proyecciones}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(ca\_house, }\AttributeTok{map =} \StringTok{"rowprincipal"}\NormalTok{, }\AttributeTok{what =} \FunctionTok{c}\NormalTok{(}\StringTok{"all"}\NormalTok{, }\StringTok{"none"}\NormalTok{), }\AttributeTok{xlab =} \StringTok{"Perfiles fila"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(ca\_house, }\AttributeTok{map =} \StringTok{"colprincipal"}\NormalTok{, }\AttributeTok{what =} \FunctionTok{c}\NormalTok{(}\StringTok{"none"}\NormalTok{, }\StringTok{"all"}\NormalTok{), }\AttributeTok{xlab =} \StringTok{"Perfiles columna"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/proyecciones-1} 

}

\caption{Proyecciones de los perfiles fila y columna}\label{fig:proyecciones}
\end{figure}

Respecto a las filas, se muestran varios grupos: el compuesto por \emph{``Breakfast''}, \emph{``Dinner''}, \emph{``Main\_meal''} y \emph{``Laundry''}; otro por \emph{``Shopping''}, \emph{``Dishes''} y \emph{``Tidying''}; uno tercero por \emph{``Insurance''} y \emph{``Finance''}; y el compuesto por \emph{``Driving''} y \emph{``Official''}. Los niveles \emph{``Holiday''} y \emph{``Repairs''} están alejados del resto.

Las coordenadas simétricas permiten la representación de ambos factores a la vez (mapa ``symmetric'', what=c(``all'',``all'')), como se muestra en la fig.~\ref{fig:simetrica}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(ca\_house, }\AttributeTok{map =} \StringTok{"symmetric"}\NormalTok{, }\AttributeTok{what =} \FunctionTok{c}\NormalTok{(}\StringTok{"all"}\NormalTok{, }\StringTok{"all"}\NormalTok{), }\AttributeTok{xlab =} \StringTok{"Proyección común de ambos factores"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/simetrica-1} 

}

\caption{Proyección simétrica de ambos factores}\label{fig:simetrica}
\end{figure}

El gráfico conjunto permite observar qué niveles de filas y columnas pueden estar más cercanos (aproximación a la asociación entre ellos). El grupo de \emph{``Driving''} y \emph{``Repairs''} está cercano a \emph{``Husband''}; el grupo de \emph{``Dinner\emph{'', }''Breakfast''}, \emph{``Laundry''} y \emph{``Main\_meal''} está cercano a \emph{``Wife''}; mientras que el nivel \emph{``Jointly''} parece estar asociado a \emph{``Holidays''}, \emph{``Finance''}, e \emph{``Insurance''}.

Como segundo ejemplo, se van a utilizar los datos \emph{``accidentes2020\_data''}, contenidos en la librería \textbf{CDR}, en concreto, la información sobre \emph{``tipo\_accidente''} y \emph{``estado\_meteorológico''}. Para evitar pares de niveles con frecuencia nula, se eliminan los niveles \emph{``Atropello a animal''}, \emph{``Despañamiento''} y \emph{``Otro''} del factor \emph{``tipo\_accidente''} y los niveles \emph{``Granizando''}, \emph{``Nevando''}, \emph{``NULL''} y \emph{``Se desconoce''} del factor \emph{``estado\_meteorológico''}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(CDR)}
\CommentTok{\# library(dplyr)}
\FunctionTok{library}\NormalTok{(dtplyr)}
\FunctionTok{data}\NormalTok{(}\StringTok{"accidentes2020\_data"}\NormalTok{)}
\NormalTok{datos }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(accidentes2020\_data}\SpecialCharTok{$}\NormalTok{tipo\_accidente, accidentes2020\_data}\SpecialCharTok{$}\NormalTok{estado\_meteorológico))}
\NormalTok{datos\_depu }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(datos, V1 }\SpecialCharTok{!=} \StringTok{"Atropello a animal"}\NormalTok{)}
\NormalTok{datos\_depu }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(datos\_depu, V1 }\SpecialCharTok{!=} \StringTok{"Atropello a persona"}\NormalTok{)}
\NormalTok{datos\_depu }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(datos\_depu, V1 }\SpecialCharTok{!=} \StringTok{"Caída"}\NormalTok{)}
\NormalTok{datos\_depu }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(datos\_depu, V1 }\SpecialCharTok{!=} \StringTok{"Solo salida de la vía"}\NormalTok{)}
\NormalTok{datos\_depu }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(datos\_depu, V1 }\SpecialCharTok{!=} \StringTok{"Vuelco"}\NormalTok{)}
\NormalTok{datos\_depu }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(datos\_depu, V1 }\SpecialCharTok{!=} \StringTok{"Despeñamiento"}\NormalTok{)}
\NormalTok{datos\_depu }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(datos\_depu, V1 }\SpecialCharTok{!=} \StringTok{"Otro"}\NormalTok{)}
\NormalTok{datos\_depu }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(datos\_depu, V2 }\SpecialCharTok{!=} \StringTok{"Granizando"}\NormalTok{)}
\NormalTok{datos\_depu }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(datos\_depu, V2 }\SpecialCharTok{!=} \StringTok{"Nevando"}\NormalTok{)}
\NormalTok{datos\_depu }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(datos\_depu, V2 }\SpecialCharTok{!=} \StringTok{"NULL"}\NormalTok{)}
\NormalTok{datos\_depu }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(datos\_depu, V2 }\SpecialCharTok{!=} \StringTok{"Se desconoce"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(datos\_depu)}
\CommentTok{\#\textgreater{}                               V2}
\CommentTok{\#\textgreater{} V1                             Despejado Lluvia débil LLuvia intensa Nublado}
\CommentTok{\#\textgreater{}   Alcance                           5525          403             84     449}
\CommentTok{\#\textgreater{}   Choque contra obstáculo fijo      3258          308             43     224}
\CommentTok{\#\textgreater{}   Colisión frontal                   711           42              5      48}
\CommentTok{\#\textgreater{}   Colisión fronto{-}lateral           6359          398             51     494}
\CommentTok{\#\textgreater{}   Colisión lateral                  3241          169             30     277}
\CommentTok{\#\textgreater{}   Colisión múltiple                 1619          173             29     111}
\end{Highlighting}
\end{Shaded}

Se comprueba que existe asociación y se obtienen los resultados del análisis de correspondencias:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tabla }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(datos\_depu)}
\FunctionTok{chisq.test}\NormalTok{(tabla)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Pearson\textquotesingle{}s Chi{-}squared test}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  tabla}
\CommentTok{\#\textgreater{} X{-}squared = 103.99, df = 15, p{-}value = 2.275e{-}15}
\NormalTok{ca\_tabla }\OtherTok{\textless{}{-}} \FunctionTok{ca}\NormalTok{(tabla, }\AttributeTok{k =} \DecValTok{2}\NormalTok{)}
\NormalTok{ca\_tabla}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Principal inertias (eigenvalues):}
\CommentTok{\#\textgreater{}            1        2        3      }
\CommentTok{\#\textgreater{} Value      0.003804 0.000493 2.7e{-}05}
\CommentTok{\#\textgreater{} Percentage 87.97\%   11.4\%    0.62\%  }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Rows:}
\CommentTok{\#\textgreater{}           Alcance Choque contra obstáculo fijo Colisión frontal}
\CommentTok{\#\textgreater{} Mass     0.268637                     0.159370         0.033512}
\CommentTok{\#\textgreater{} ChiDist  0.032004                     0.081686         0.065908}
\CommentTok{\#\textgreater{} Inertia  0.000275                     0.001063         0.000146}
\CommentTok{\#\textgreater{} Dim. 1  {-}0.160159                    {-}1.281759         0.729688}
\CommentTok{\#\textgreater{} Dim. 2   1.365544                    {-}0.920696        {-}1.847953}
\CommentTok{\#\textgreater{}         Colisión fronto{-}lateral Colisión lateral Colisión múltiple}
\CommentTok{\#\textgreater{} Mass                   0.303605         0.154547          0.080329}
\CommentTok{\#\textgreater{} ChiDist                0.044584         0.076822          0.128386}
\CommentTok{\#\textgreater{} Inertia                0.000603         0.000912          0.001324}
\CommentTok{\#\textgreater{} Dim. 1                 0.658572         1.229958         {-}2.081267}
\CommentTok{\#\textgreater{} Dim. 2                {-}0.822076         0.528562          0.121030}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Columns:}
\CommentTok{\#\textgreater{}         Despejado Lluvia débil LLuvia intensa  Nublado}
\CommentTok{\#\textgreater{} Mass     0.861212     0.062076       0.010062 0.066650}
\CommentTok{\#\textgreater{} ChiDist  0.013443     0.214467       0.289545 0.083914}
\CommentTok{\#\textgreater{} Inertia  0.000156     0.002855       0.000844 0.000469}
\CommentTok{\#\textgreater{} Dim. 1   0.205524    {-}3.461906      {-}3.671159 1.122905}
\CommentTok{\#\textgreater{} Dim. 2  {-}0.190070    {-}0.838079       8.057832 2.020069}
\end{Highlighting}
\end{Shaded}

Las dos primeras dimensiones explican el 87.97\% y 11.4\%, por lo que la representación en un plano engloba al 99.37\% de la inercia.

Las representación gráfica de la proyección simétrica se muestra en la fig.~\ref{fig:accid}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(ca\_tabla, }\AttributeTok{map =} \StringTok{"symmetric"}\NormalTok{, }\AttributeTok{what =} \FunctionTok{c}\NormalTok{(}\StringTok{"all"}\NormalTok{, }\StringTok{"all"}\NormalTok{), }\AttributeTok{xlab =} \StringTok{"Proyección común de ambos factores"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/accid-1} 

}

\caption{Proyección simétrica de ambos factores}\label{fig:accid}
\end{figure}

Se observa que \emph{``Lluvia intensa''} no está especialmente asociada a ningún tipo de accidente; \emph{``Lluvia débil''} con \emph{``Colisión múltiple''} y \emph{``Choque con obstáculo fijo''}; \emph{``Despejado''} con \emph{``Colisión fronto-lateral''}, \emph{``Colisión frontal''} y \emph{``Alcance''}; y \emph{``Nublado''} con \emph{``Colisión lateral''}.

\begin{infobox_resume}
\textbf{RESUMEN}

El análisis de correspondencias permite, dada una tabla de contingencia, reproducir las distancias entre niveles de cada factor en un espacio de menor dimensión, permitiendo la comparación gráfica entre ellos, así como la representación en un espacio común de los niveles de ambos factores.

En el primer caso, permite una visualización de la composición interna de cada factor, identificando los niveles que más se distancien del centroide. En el segundo caso, permite la representación de la asociación entre niveles de cada uno de los factores.

\end{infobox_resume}

\hypertarget{part-deep-learning}{%
\part{Deep learning}\label{part-deep-learning}}

\hypertarget{capNN}{%
\chapter{Redes neuronales artificiales}\label{capNN}}

\emph{Noelia Vállez Enano y José Luis Espinosa Aranda}

\hypertarget{quuxe9-es-el-deep-learning}{%
\section{\texorpdfstring{¿Qué es el \emph{deep learning}\index{deep learning}?}{¿Qué es el deep learning?}}\label{quuxe9-es-el-deep-learning}}

La inteligencia artificial\index{inteligencia artificial} es el conjunto de técnicas que hacen que cualquier elemento controlado por un computador sea capaz de realizar acciones similares a las que haría un humano en situaciones determinadas. Entre otros ejemplos podemos encontrar en la actualidad tanto robots que son capaces de realizar tareas de manera similar a un humano en una fábrica, las denominadas como casas inteligentes o los vehículos autónomos.

Dentro de las técnicas utilizadas para la inteligencia artificial\index{inteligencia artificial}, se encuentran las técnicas clásicas de \emph{machine learning}\index{machine learning}, ya explicadas en capítulos anteriores de este libro, las cuales tienen la habilidad de aprender sin haber sido explícitamente programadas para una tarea en particular, pudiendo ser utilizadas para varios fines y aplicaciones.

A su vez, dentro de estos algoritmos, se pueden enmarcar como un subconjunto de las mismas las técnicas de \emph{deep learning}\index{deep learning}, las cuales intentan simular tanto la arquitectura como el comportamiento del sistema nervioso humano, en particular, de las redes de neuronas que componen el encéfalo y que se encargan de realizar tareas específicas (Fig. \ref{fig:iaMLDL}). Para ello, estas técnicas se basan en el concepto de redes neuronales, que intentan emular la forma de aprendizaje de los humanos \citep{goodfellow2016deep}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{img/conc_dl_01_iaMLDL} 

}

\caption{Inteligencia Artificial vs Machine learning vs Deep Learning}\label{fig:iaMLDL}
\end{figure}

\hypertarget{diferencias-entre-las-tuxe9cnicas-de-machine-learning-tradicional-y-el-deep-learning}{%
\subsection{\texorpdfstring{Diferencias entre las técnicas de \emph{machine learning}\index{machine learning} tradicional y el \emph{deep learning}\index{deep learning}}{Diferencias entre las técnicas de machine learning tradicional y el deep learning}}\label{diferencias-entre-las-tuxe9cnicas-de-machine-learning-tradicional-y-el-deep-learning}}

Como se vio en capítulos anteriores, las técnicas de \emph{machine learning}\index{machine learning} tradicional requieren al inicio realizar una selección de las mejores características que representen el problema a resolver, y que puedan ser comprendidas por el algoritmo seleccionado de tal forma que sea capaz de solucionar el problema planteado.

Por ejemplo, en el caso de querer detectar una cara dentro de una imagen, sería necesario definir qué tipo de características servirían para detectar la misma, como podrían ser, a bajo nivel, determinados tipos de bordes de la imagen (Fig. \ref{fig:scharrpitu}). Estas características proporcionarían la base para detectar a medio nivel elementos de la cara como ojos, narices, orejas, etc. y, definitivamente a alto nivel, reconocer donde hay una cara dentro de la imagen.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/conc_dl_02_Pitu} 

}

\caption{Detección de bordes de una imagen mediante el método de Scharr}\label{fig:scharrpitu}
\end{figure}

Esta elección de características requiere en muchas ocasiones de la intervención humana, por lo que puede llevar mucho tiempo y diversos experimentos de prueba y error hasta poder encontrar una combinación de características y algoritmos que permita resolver el problema planteado.

Debido a esto, nacen las técnicas de \emph{deep learning}\index{deep learning}, las cuales tratan de simular el comportamiento de aprendizaje humano. Esto implica que, a diferencia de las técnicas de \emph{machine learning}\index{machine learning} tradicional, son capaces de aprender cuales son las mejores características que permitirán representar el problema que se quiere resolver sin necesidad de la interacción humana a la misma vez que buscan la solución al mismo.

Por ejemplo, continuando con el ejemplo anterior de la detección de caras, mientras que en las técnicas de \emph{machine learning}\index{machine learning} sería necesario explicarle al algoritmo que características base componen una cara para que sea capaz de reconocerlas, al utilizar \emph{deep learning}\index{deep learning} únicamente sería necesario mostrarle suficientes imágenes de caras para conseguir que el algoritmo sea capaz de aprender a identificar una cara por si mismo.

La capacidad de aprender las mejores características necesarias por sí mismo hace que a nivel teórico las técnicas de \emph{deep learning}\index{deep learning} puedan llegar a ser más potentes que el \emph{machine learning}\index{machine learning} clásico, pero debido a la mayor complejidad del problema y, por consiguiente, al proceso de entrenamiento, también lleva a que que sean necesarios muchos más datos iniciales y una mayor potencia de cómputo.

Este hecho explica que, aunque las bases de las técnicas de \emph{deep learning}\index{deep learning} como el algoritmo del descenso del gradiente\index{descenso del gradiente} \citep{kiefer1952stochastic}, el perceptrón\index{perceptrón} \citep{rosenblatt1958perceptron}, los algoritmos de retropropagación y el perceptrón\index{perceptrón} multicapa \citep{rumelhart1986learning} y la primera red neuronal convolucional \citep{lecun1995convolutional}, datan de varios años atrás, no sea hasta hace relativamente poco tiempo cuando ha podido empezar a utilizar estas técnicas gracias a:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{La evolución en el hardware de procesamiento.} En particular, debido a la mejora de la capacidad de paralelismo masivo durante el cómputo que proporcionaron las nuevas tarjetas gráficas al incorporar una gran cantidad de microprocesadores específicos, inicialmente, para representar modelos complejos 3D en los monitores, pero que han podido ser utilizadas para las técnicas de \emph{deep learning}\index{deep learning}, llevando recientemente al desarrollo de tarjetas específicas para este fin .
\item
  \textbf{Big data.} La gran cantidad de datos que se generan y almacenan en la actualidad en el día a día, así como la mayor facilidad a la hora de trabajar con esos conjuntos de datos, han permitido cubrir la necesidad de datos iniciales necesarios.
\item
  \textbf{La evolución del sofware.} Recientemente ha habido un amplio interés tanto en buscar nuevos modelos para resolver todo tipo de problemas, como para mejorar las técnicas utilizadas para entrenar dichas redes neuronales, lo cual ha llevado a la creación y mejora de diversos frameworks y aplicaciones relacionadas con el entrenamiento y despliegue de redes neuronales. Entre ellos, serían destacables Keras, Tensorflow, Pytorch, Caffe2, Matlab y OpenVINO.
\end{enumerate}

\hypertarget{aplicaciones-del-deep-learning}{%
\section{\texorpdfstring{Aplicaciones del \emph{deep learning}\index{deep learning}}{Aplicaciones del deep learning}}\label{aplicaciones-del-deep-learning}}

Las posibles aplicaciones de las técnicas de \emph{deep learning}\index{deep learning} son muy diversas y, gracias a la continua investigación desarrollada en el área en la actualidad, no hacen más que aumentar. A continuación se comentan algunas de ellas:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Clasificación de imágenes.}\index{clasificación} Aunque la clasificación de imágenes dentro del área de la visión por computador lleva años presente, es con las técnicas de \emph{deep learning}\index{deep learning} con las que se han logrado los mayores avances, en particular, utilizando las redes neuronales convolucionales\index{redes neuronales convolucionales}. Estas redes permiten determinar a que clase, perteneciente al conjunto de clases utilizado para entrenar, pertenece una determinada imagen.
\item
  \textbf{Detección de objetos.}\index{detección} Permite localizar los objetos contenidos en una imagen y su tipología marcándose con un rectángulo. Por ejemplo, utilizando una cámara de seguridad instalada en una calle con este tipo de modelos sería posible localizar y diferenciar entre peatones y vehículos (Fig. \ref{fig:camaraTermica}).
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{img/conc_dl_04_camaraTermica} 

}

\caption{Detección de peatones y vehículos utilizando una cámara térmica y técnicas de deep learning}\label{fig:camaraTermica}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  \textbf{Segmentación semántica/de instancias.}\index{segmentación} De forma similar a la detección de objetos, la segmentación permite localizar objetos contenidos en una imagen, además de su tipología, pero en este caso se marcan utilizando una máscara a nivel de píxel. La segmentación de instancias además es capaz de diferenciar entre diferentes instancias de una misma clase aún cuando se encuentren situadas de forma contigua.
\item
  \textbf{Reconocimiento del habla.}\index{reconocimiento} Permite a un computador procesar el habla humana en formato escrito. En la actualidad existen varios asistentes inteligentes basados en esta tecnología que además son capaces de interpretar órdenes o instrucciones sencillas y actuar en consecuencia.
\item
  \textbf{Traducción automática.} Consiste en utilizar las técnicas de \emph{deep learning}\index{deep learning} para traducir un texto automáticamente de una lengua a otra sin la necesidad de intervención humana. En la actualidad, no se limita únicamente a la traducción literal, palabra por palabra, del texto, si no que también tiene en cuenta el significado que tendría en el idioma original para adaptarlo al idioma destino (Fig. \ref{fig:deepL}).
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/conc_dl_05_deepL} 

}

\caption{Traductor automático basado en Deep Learning}\label{fig:deepL}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  \textbf{Generación automática de imágenes/texto.} Permite obtener desde una imagen un texto descriptivo que indique el contenido de la imagen, o al contrario, a partir de un texto descriptivo generar una imagen basada en dicho texto. Un ejemplo de este último caso sería Dall-E \citep{borji2022generated} (Fig. \ref{fig:DallE}).
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/conc_dl_06_DallE} 

}

\caption{Algunas salidas posibles del generador de imágentes a partir de texto Dall-E, para el texto a $``$cat with glasses studying computer vision in the space with the Earth in the background$"$}\label{fig:DallE}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  \textbf{Automóvil autónomo.} Las técnicas de \emph{deep learning}\index{deep learning} están siendo claves para el desarrollo del vehículo autónomo, capaz de viajar sin la necesidad de la interacción de un conductor humano. Para lograr definitivamente un vehículo con estas características, es necesario que sea capaz de ver, tomar decisiones y conducir al mismo tiempo. Esto se consigue en la actualidad integrando la información de gran cantidad de sensores que obtienen datos en tiempo real sobre el entorno, como serían cámaras, LIDAR, radares o ultrasónicos entre otros, y que son procesados por varias redes neuronales con el fin de que sea capaz de tomar una decisión en cuestión de milisegundos (Fig. \ref{fig:camaraTermica}).
\end{enumerate}

\hypertarget{redes-neuronales}{%
\section{Redes Neuronales}\label{redes-neuronales}}

Las redes neuronales artificiales\index{redes neuronales artificiales} (en inglés Artificial Neural Network (ANN)) tienen su origen a finales de los años 50 a partir del diseño del perceptrón\index{perceptrón} por parte de Frank Rosenblatt \citep{rosenblatt1958perceptron}. Cada ANN está formada por un conjunto de elementos conocidos como ``neuronas'' cuya organización está inspirada en la que siguen las redes neuronales de los seres vivos. Entre dos neuronas adyacentes existe una serie de conexiones a través de las cuales se envía la información como si de pulsos eléctricos se tratase. De forma aislada, cada neurona\index{neurona} procesa la información recibida para producir un resultado que será utilizado por las siguientes neuronas con las que está conectada

Cada ANN tiene como objetivo resolver una tarea concreta. Por ejemplo, una ANN podría estar diseñada para reconocer un dígito o una letra a partir de una imagen. Para conseguir resolver dicha tarea, la red sigue un proceso de aprendizaje automático. Este proceso se conoce como ``entrenamiento'' y requiere que se disponga de un conjunto de datos representativos de la tarea a resolver.

\hypertarget{perceptruxf3n-o-neurona}{%
\section{\texorpdfstring{Perceptrón\index{perceptrón} o Neurona\index{neurona}}{Perceptrón o Neurona}}\label{perceptruxf3n-o-neurona}}

El elemento básico de toda ANN es el perceptrón\index{perceptrón} o neurona\index{neurona}. Se trata de un modelo artificial basado en las neuronas biológicas. Cada neurona\index{neurona} tiene una serie de entradas y produce una única salida. Las entradas pueden ser variables extraídas de la tarea que se debe resolver o salidas de otras neuronas de la red.

Para calcular la salida, cada neurona\index{neurona} realiza una suma ponderada de sus entradas utilizando una serie de pesos, \(\boldsymbol w\) donde \(w_i\in \mathbb{R}\), y añade un término constante,\(w_0\in \mathbb{R}\). Por tanto, cada neurona\index{neurona} actúa como un clasificador lineal que puede separar dos conjuntos diferentes dependiendo de si la salida es positiva o negativa (Figura \ref{fig:perceptron}).

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/perceptron} 

}

\caption{Estructura del perceptrón o neurona}\label{fig:perceptron}
\end{figure}

Para cada vector de entrada, \(\boldsymbol x\), la neurona\index{neurona} aplicará los pesos, \(\boldsymbol w\), como el producto escalar de ambos vectores:

\begin{equation} 
\boldsymbol w^{\prime}    \boldsymbol x  = w_0\cdot 1+w_1 \cdot x_1+w_2 \cdot x_2+\dots+w_n \cdot x_n .
\end{equation}

Una vez obtenida la suma ponderada, se puede separar las entradas en dos conjuntos, obteniéndose como salida final un valor binario, siguiendo la fórmula:

\begin{equation} 
f (\boldsymbol w^{\prime}   \boldsymbol x) = \begin{cases}
1 & \text{si $\boldsymbol w^{\prime}   \boldsymbol x>0$}\\
0 & \text{en otro caso}
\end{cases} .
\end{equation}

\hypertarget{aprendizaje}{%
\subsection{Aprendizaje}\label{aprendizaje}}

El proceso de aprendizaje del perceptrón\index{perceptrón} busca el ajuste automático de los valores de los pesos. Estos pesos deben seleccionarse de forma que minimicen el error de clasificación\index{clasificación} cometido sobre un conjunto de entrenamiento. El conjunto de entrenamiento estará compuesto por un conjunto de muestras del que se conoce su clase:

\begin{equation} 
D = \{ (\boldsymbol x_1 , y_1 ), (\boldsymbol x_2 , y_2 ), \dots, (\boldsymbol x_m , y_m ) \},
\end{equation}

donde cada muestra, \(\boldsymbol x_i = (x_{i1},x_{i2},\dots,x_{in})\), pertenece a una de las 2 clases, \(y_j = \{ 0,1 \}\) .

El primer paso del aprendizaje o entrenamiento consiste en la inicialización de cada peso \(w_j\) a 0 o a algún otro valor aleatorio.

Tras ello, se calcula la clase estimada, \(\hat y\), en un momento determinado, \(t\), para cada muestra \(\boldsymbol x_i\) del conjunto de datos:
\begin{equation} 
\hat y_i(t) = f(\boldsymbol w(t)^{T}  \boldsymbol x_i) = f(w_0(t) + w_1(t) \cdot x_{i1} + \dots + w_n(t) \cdot x_{in}) .
\end{equation}

Tras obtener la salida para todas las muestras de entrenamiento, cada uno de los pesos, \(w_j\), de la neurona\index{neurona} se actualiza siguiendo la fórmula:
\begin{equation} 
w_j(t+1) = w_j(t) + \lambda \cdot |y_i-\hat y_i(t)|\cdot x_{ij} .
\end{equation}

donde \(|y_i-\hat y_i(t)|\) será 0 cuando la clase predicha coincida con la clase real de la muestra y \(\lambda\) es la tasa de aprendizaje. La tasa de aprendizaje debe seleccionarse de antemano y controla la variación de los pesos entre iteraciones. En algunos casos el valor de \(r\) es 0.

Los dos pasos anteriores se repiten hasta que el error de clasificación\index{clasificación} es menor que un cierto umbral o el número de iteraciones alcanza un cierto valor fijado. Normalmente se suele utilizar el número de iteraciones como criterio de paro puesto que no siempre es posible alcanzar una tasa de error más baja que la deseada.

\hypertarget{convergencia}{%
\subsection{Convergencia}\label{convergencia}}

El teorema de la convergencia del perceptrón\index{perceptrón} dice que, en los problemas en los que haya dos clases linealmente separables, es siempre posible encontrar unos pesos que realicen la separación en un número finito de iteraciones \citep{novikoff62convergence}. Sin embargo, en la mayoría de los casos no es posible obtener un conjunto de variables que separen perfectamente las muestras de ambas clases. Por ello, es necesario el uso de ciertas estrategias que solucionen el problema de convergencia en estos casos. Algunas de las estrategias más utilizadas son:

\begin{itemize}
\item
  Algoritmo Pocket: Guarda la mejor solución obtenida hasta el final del entrenamiento.
\item
  Algoritmo Maxover: Halla el margen de separación máximo permitiendo clasificaciones incorrectas.
\item
  Algoritmo de Voto: Se utilizan múltiples perceptrones combinando sus salidas.
\end{itemize}

\hypertarget{perceptruxf3n-multiclase}{%
\section{Perceptrón Multiclase}\label{perceptruxf3n-multiclase}}

Una extensión lógica del uso del perceptrón\index{perceptrón} es su empleo en la resolución de tareas de clasificación\index{clasificación} donde existan más de dos clases \citep{haykin1999}. En ese caso se tendrá un conjunto de entrenamiento, \(D\), de \(m\) muestras:

\begin{equation} 
D = \{ (\boldsymbol x_1 , y_1 ), (\boldsymbol x_2 , y_2 ), \dots, (\boldsymbol x_m , y_m ) \},
\end{equation}

donde cada muestra \(\boldsymbol x_i = (x_{i1},x_{i2},\dots,x_{in})\) pertenezca a una de las \(c\) clases posibles:

\begin{equation} 
y_j = \{ 0,1,\dots,c-1 \} .
\end{equation}

A diferencia del problema binario, en su versión multiclase lo que se definen son varios modelos, \(F\), uno para cada una de las \(c\) clases:

\begin{equation} 
F=\{f_0,f_1,\dots,f_{c-1}\}\\
f_j: \mathbb{R}^n \rightarrow \mathbb{R} .
\end{equation}

En este caso la salida no se selecciona en función de si el valor obtenido es positivo o negativo, sino que se asigna la clase del modelo que obtenga el valor más alto tras aplicar los pesos a la muestra. Esta estrategia recibe el nombre de ``uno contra todos'':

\begin{equation} 
\hat y_i = argmax_j(f_j(\boldsymbol x_i))\\
j\in\{0,1,\dots ,c-1\} .
\end{equation}

En muchas ocasiones lo que se obtiene no es un único valor con la clase asignada como salida, sino que se obtiene un vector con las salidas binarias de cada uno de los modelos empleados. En ese caso, el vector contendrá un 1 en la posición de la clase asignada y un 0 en el resto de clases. Por ejemplo, el vector \([0,1,0,0,0]\) representaría que una muestra ha sido asignada a la segunda clase en un problema de clasificación\index{clasificación} donde existen 5 clases posibles:

\begin{equation} 
[(f_1(\boldsymbol x_i)),(f_2(\boldsymbol x_i)),\dots,(f_c(\boldsymbol x_i))] .
\end{equation}

\hypertarget{funciones-de-activaciuxf3n}{%
\section{Funciones de activación}\label{funciones-de-activaciuxf3n}}

Además de los pesos, toda neurona\index{neurona} tiene asociada una función de activación. Esta función se encarga de transformar la suma ponderada de las entradas en el resultado final. En las secciones anteriores se ha utilizado una función de activación con umbral 0, pero existen muchas otras. Algunas de las más utilizadas se enumeran a continuación.

Para algunas de ellas, se ha implementado una función, \emph{plot\_activation\_function}, que permite dibujarlas en \textbf{R}, y que se puede ver a continuación:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{require}\NormalTok{(ggplot2)}
\NormalTok{plot\_activation\_function }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(f, title, range) \{}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =}\NormalTok{ range), }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x)) }\SpecialCharTok{+}
    \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{color =} \StringTok{"black"}\NormalTok{, }\AttributeTok{alpha =} \DecValTok{3} \SpecialCharTok{/} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{color =} \StringTok{"black"}\NormalTok{, }\AttributeTok{alpha =} \DecValTok{3} \SpecialCharTok{/} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{stat\_function}\NormalTok{(}\AttributeTok{fun =}\NormalTok{ f, }\AttributeTok{colour =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{ggtitle}\NormalTok{(title) }\SpecialCharTok{+}
    \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"x"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"f(x)"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme}\NormalTok{(}\AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{hjust =} \FloatTok{0.5}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Función lineal.} Se trata de una función identidad donde la salida tiene el mismo valor que la entrada. Normalmente se aplica en problemas de regresión lineal. Por ejemplo, si se quiere predecir el número de días que lloverá en un mes determinado.
\end{itemize}

\begin{equation} 
f(x)=x
\end{equation}

Y se representa gráficamente de la siguiente forma:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{  x}
\NormalTok{\}}
\FunctionTok{plot\_activation\_function}\NormalTok{(f, }\StringTok{"Lineal"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/unnamed-chunk-146-1} \end{center}

\begin{itemize}
\tightlist
\item
  \textbf{Función umbral.} Esta función recibe también el nombre de función escalón. Si el valor de entrada es menor que el umbral la salida será 0. En caso contrario, la salida será 1. Si el umbral es 0, la función se reduce a mirar el signo del valor analizado.
\end{itemize}

\begin{equation} 
f(x)=\begin{cases}
0 & \text{si $x<u$}\\
1 & \text{en otro caso}
\end{cases}
\end{equation}

Se representa gráficamente mediante el siguiente código, el cual se corresponde con una modificación de la función \emph{plot\_activation\_function}, ya que la versión original no mostraría de forma correcta la gráfica al requerir representar dos valores en la posición 0, el valor 0 y el valor 1 del escalón:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{), }\AttributeTok{f =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ f, }\AttributeTok{group =} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{hjust =} \FloatTok{0.5}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Umbral"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{name =} \StringTok{"f(x)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{color =} \StringTok{"black"}\NormalTok{, }\AttributeTok{alpha =} \DecValTok{3} \SpecialCharTok{/} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{color =} \StringTok{"black"}\NormalTok{, }\AttributeTok{alpha =} \DecValTok{3} \SpecialCharTok{/} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_step}\NormalTok{(}\AttributeTok{color =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/unnamed-chunk-147-1} \end{center}

\begin{itemize}
\tightlist
\item
  \textbf{Función sigmoide.} También conocida como función logística, se trata de una de las funciones más utilizadas para asignar una clase. Si el punto de evaluación de la función es un valor negativo muy bajo, la función dará como resultado un valor muy cercano a 0, si se evalúa en 0, el resultado es 0,5 y si se evalúa en un valor positivo alto el resultado será aproximadamente 1.
\end{itemize}

\begin{equation} 
f(x)=\frac{1}{1-e^{-x}} 
\end{equation}

Representándose gráficamente de la siguiente forma:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
  \DecValTok{1} \SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{x))}
\NormalTok{\}}
\FunctionTok{plot\_activation\_function}\NormalTok{(f, }\StringTok{"Sigmoide"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/unnamed-chunk-148-1} \end{center}

\begin{itemize}
\tightlist
\item
  \textbf{Función tangente hiperbólica.} El rango de valores de salida es {[}-1, 1{]}, donde los valores altos tienden de manera asintótica a 1 y los valores muy bajos tienden de manera asintótica a -1 de forma similar a la sigmoide.
\end{itemize}

\begin{equation} 
f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}} 
\end{equation}

Siendo su representación gráfica de la siguiente forma:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tanh\_func }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
  \FunctionTok{tanh}\NormalTok{(x)}
\NormalTok{\}}
\FunctionTok{plot\_activation\_function}\NormalTok{(tanh\_func, }\StringTok{"Tangente Hiperbólica"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/unnamed-chunk-149-1} \end{center}

\begin{itemize}
\tightlist
\item
  \textbf{Función ReLU.} Se trata de la unidad lineal rectificada (del inglés Rectified Linear Unit). Es posiblemente la función de activación más utilizada actualmente en redes neuronales \citep{nair2010rectified}.
\end{itemize}

\begin{equation} 
f(x)=\begin{cases}
0 & \text{si $x\leq 0$}\\
x & \text{en otro caso}
\end{cases} 
\end{equation}

Y se representaría gráficamente de la siguiente manera:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rec\_lu\_func }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
  \FunctionTok{ifelse}\NormalTok{(x }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, x)}
\NormalTok{\}}
\FunctionTok{plot\_activation\_function}\NormalTok{(rec\_lu\_func, }\StringTok{"ReLU"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/unnamed-chunk-150-1} \end{center}

\hypertarget{perceptruxf3n-multicapa}{%
\section{\texorpdfstring{Perceptrón Multicapa\index{perceptrón multicapa}}{Perceptrón Multicapa}}\label{perceptruxf3n-multicapa}}

Aunque el perceptrón\index{perceptrón} puede representar muchos tipos de lógica, no es posible realizar con él la operación XOR (o exclusiva) que asigna un 1 a la salida cuando las dos entradas son distintas \citep{minsky1969introduction}. El perceptrón multicapa\index{perceptrón multicapa} o, en inglés, Multilayer Perceptron (MLP) surge para dar una solución a este problema.

Un MLP está compuesto por varias capas con neuronas. La primera capa será la capa de entrada, que recibirá la variables que representan los elementos del problema a resolver. Por otro lado, la última capa será, de forma similar a cada neurona\index{neurona} individual, la salida del MLP. Entre ambas capas existirán una o más capas ``ocultas''. Las neuronas de una capa intermedia tienen como entrada la salida de la capa anterior y su salida es la entrada de las neuronas de la siguiente capa (Figura \ref{fig:ann}). Este tipo de capas también son llamadas \emph{densas} o \emph{totalmente conectadas}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/ann} 

}

\caption{Estructura del perceptrón multicapa (MLP)}\label{fig:ann}
\end{figure}

\hypertarget{aprendizaje-1}{%
\subsection{Aprendizaje}\label{aprendizaje-1}}

El MLP entra en la categoría de los algoritmos de propagación hacia adelante o \emph{feedforward} ya que las entradas de las neuronas de una capa se combinan mediante la suma ponderada, pasan por una función de activación y el resultado es propagado a las neuronas de la capa siguiente. Este proceso se lleva a cabo desde la capa de entrada hasta la capa de salida.

Dado un conjunto de muestras de entrenamiento \(\{(\boldsymbol x_1, y_1), (\boldsymbol x_2, y_2), \ldots, (\boldsymbol x_n, y_n)\}\) donde cada \(\boldsymbol x_i \in \mathbb{R}^d\) e \(y_i \in \{0, 1\}\), la salida de la primera capa, \(\boldsymbol z_1\), para una entrada \(\boldsymbol x\) vendrá dada por la expresión:

\begin{equation} 
\boldsymbol z_1 = \boldsymbol W_{(1)}^{\prime}   \boldsymbol x + \boldsymbol b_1 ,
\end{equation}

donde \(\boldsymbol b_1 \in \mathbb{R}^{h}\) es un vector con las constantes de la primera capa y \(\boldsymbol{W}_{(1)} \in \mathbb{R}^{h \times d}\) son los pesos de la capa. Tras aplicar la función de activación, \(g(\cdot)\), al vector intermedio, \(\boldsymbol{z}\in \mathbb{R}^h\), se obtiene:

\begin{equation} 
\boldsymbol{h_1}= g(\mathbf{z_1}) .
\end{equation}

La salida de una capa intermedia, \(\boldsymbol{h_i}\in \mathbb{R}^h\), también está formada por variables intermedias que sirven de entrada a la siguiente capa. La función a calcular en la siguiente capa será por tanto:

\begin{equation} 
\boldsymbol h_2 = g ( \boldsymbol W_{(2)}^{\prime}   \boldsymbol h_1 + \boldsymbol b_2) .
\end{equation}

Siguiendo el mismo razonamiento, la salida de la última capa, \(\hat y\), y por tanto de la red, vendrá dada por:

\begin{equation} 
\hat y = g ( \boldsymbol W_{(n)}^{\prime}   \boldsymbol h_{n-1} + \boldsymbol b_n ) .
\end{equation}

Por ejemplo, si se tiene una red de tres capas la salida podrá calcularse como:

\begin{equation} 
\hat y = g ( \boldsymbol W_{(3)}^{\prime}   \boldsymbol g ( \boldsymbol W_{(2)}^{\prime}   \boldsymbol g ( \boldsymbol W_{(1)}^{\prime}   \boldsymbol x + \boldsymbol b_1)+ \boldsymbol b_2 )+ \boldsymbol b_3  ) .
\end{equation}

Para entrenar y ajustar los pesos de este tipo de redes es necesario realizar el ajuste de la combinación de todos los pesos de la red. De forma similar a la búsqueda de los pesos de una sola neurona\index{neurona}, será necesario encontrar la combinación de valores que clasifiquen bien todas las muestras del conjunto de entrenamiento o, en su defecto, que fallen en el menor número de muestras posible o minimicen alguna otra función de coste. En este punto es donde entra en juego la propagación hacia atrás o \emph{backpropagation}.

La propagación hacia atrás es el mecanismo por el que el MLP ajusta de forma iterativa los pesos de la red con el objetivo de minimizar una función de coste que mide lo bueno o malo que es el resultado obtenido en un momento determinado \citep{rumelhart1986learning}. Su único requisito de aplicación es que todas las operaciones de la red (incluidas las funciones de activación) sean diferenciables ya que se utiliza el algoritmo del descenso del gradiente\index{descenso del gradiente} para optimizar la función de coste.

El MLP utiliza diferentes funciones de coste según el tipo de problema a resolver. Para los problemas de clasificación\index{clasificación}, la función de coste más utilizada es la Entropía Cruzada Media (en inglés Average Cross-Entropy). Para un problema binario esta función de coste se calcula como;

\begin{equation} 
C(\hat{y},y,\boldsymbol W) = -\dfrac{1}{n}\sum_{i=0}^n(y_i \ln {\hat{y_i}} + (1-y_i) \ln{(1-\hat{y_i})}) + \dfrac{\alpha}{2n} ||\boldsymbol W||_2^2 ,
\end{equation}

donde \(\alpha ||W||_2^2\) con \(\alpha > 0\) es un término de regularización, L2, también conocido como penalización ya que penaliza los modelos complejos. \(\alpha\) es un hiperparámetro cuyo valor se establece manualmente.

Para los problemas de regresión, la función de coste se basa en el Error Cuadrático Medio:

\begin{equation} 
C(\hat{y},y,\boldsymbol W) = \frac{1}{2n}\sum_{i=0}^n||\hat{y}_i - y_i ||_2^2 + \frac{\alpha}{2n} ||\boldsymbol W||_2^2 .
\end{equation}

Cada iteración en el proceso de aprendizaje estará compuesta entonces por dos etapas, una de propagación hacia adelante y otra de propagación hacia atrás. En la primera etapa se introducen los valores de entrada a la red y se propagan las operaciones y los resultados hasta obtener la salida final de la red. En la segunda, el gradiente de la función de coste es propagado hacia atrás para actualizar los valores de los pesos de todas las capas y acercarse más a los valores que minimizan la función de coste.

En el algoritmo del descenso del gradiente\index{descenso del gradiente}, \(\nabla C_{\boldsymbol W}\) se calcula y deduce de \(\boldsymbol W\).
Formalmente esto puede expresarse como:

\begin{equation} 
\boldsymbol W^{t+1} = \boldsymbol W^{\prime}   - \lambda \nabla {C}_{\boldsymbol W}^{t} ,
\end{equation}

donde \(t\) es el estado de la red en una iteración determinada y \(\lambda\) es la tasa de aprendizaje cuyo valor debe ser superior a 0.

Al igual que en el caso del perceptrón\index{perceptrón} único, el entrenamiento terminará cuando se alcance un número máximo de iteraciones o la mejora en la función de coste entre dos iteraciones consecutivas no supere cierto umbral.

Durante el proceso de aprendizaje, es necesario guardar en memoria los resultados de cada una de las muestras del conjunto de entrenamiento. Si el número de muestras o el tamaño de la red son grandes, es posible que no se disponga del suficiente espacio. Para resolver este problema, en una iteración no se utiliza todo el conjunto de entrenamiento, sino que se utiliza un subconjunto de él llamado \emph{batch}\index{batch}. El conjunto de entrenamiento se divide, por tanto, en un número de \emph{batches} con un número de muestras por \emph{batch}\index{batch}. Atendiendo a esta división, es posible definir una serie de hiperparámetros:

\begin{itemize}
\tightlist
\item
  Tamaño del \emph{batch}\index{batch}. Número de muestras utilizadas en cada iteración para actualizar los pesos.
\item
  Número de épocas. Número de pasadas completas sobre el conjunto de entrenamiento hasta terminar el proceso de aprendizaje.
\item
  Número de iteraciones por época. Será el resultado de dividir el número total de muestras por el tamaño del \emph{batch}\index{batch}.
\end{itemize}

Por ejemplo, si se tiene un conjunto de 55000 muestras y el tamaño del \emph{batch}\index{batch} es de 100, cada época tendrá 550 iteraciones.

\hypertarget{instalaciuxf3n-de-libreruxedas-de-deep-learning-en-r-tensorflowkeras}{%
\section{\texorpdfstring{Instalación de librerías de \emph{deep learning}\index{deep learning} en \textbf{R}: Tensorflow/Keras}{Instalación de librerías de deep learning en R: Tensorflow/Keras}}\label{instalaciuxf3n-de-libreruxedas-de-deep-learning-en-r-tensorflowkeras}}

El framework que se va a utilizar en este libro para trabajar con técnicas de \emph{deep learning}\index{deep learning} será Tensorflow/Keras, debido a que es uno de los más completos en la actualidad, permitiendo realizar una configuración completa del proceso de entrenamiento y trabajar con diversos tipos de redes neuronales.

Para poder utilizar Tensorflow/Keras en \textbf{R}, es necesario realizar la instalación de la librería fuera de \textbf{R}. Por ello, si ya se dispone de una instalación del mismo sería posible utilizarla. No obstante, se recomienda seguir los pasos indicados a continuación para tener una instalación nativa de Tensorflow/Keras asociada directamente a R.

\begin{itemize}
\tightlist
\item
  \textbf{Paso 1} - Librería de Tensorflow en \textbf{R}
\end{itemize}

El primer paso será instalar el paquete de \textbf{tensorflow} en \textbf{R} {[}{]}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"tensorflow"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A continuación, será necesario tener una instalación de Conda en el sistema. Los usuarios tanto de Windows como de Linux/Mac podrán realizar directamente la instalación de una versión de Conda denominada Mini-Conda en el instalador del siguiente paso, la cual sería la opción recomendada para no tener que realizar una instalación externa de manera adicional.

\begin{infobox}
\textbf{NOTA}

Otra manera disponible para los usuarios de Windows, pero no recomendada por los autores de este libro salvo que ya se disponga de Anaconda instalado, sería la de utilizar el programa y la librería directamente dentro de Anaconda, instalando una versión de R directamente en el sistema a través del siguiente link:

\url{https://docs.anaconda.com/anaconda/install/windows/}

\end{infobox}

\begin{itemize}
\tightlist
\item
  \textbf{Paso 2} - Instalación de \textbf{tensorflow} y \textbf{keras}
\end{itemize}

Para continuar la instalación se activará la librería de Tensorflow y se ejecutará la función \emph{install\_tensorflow}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tensorflow)}
\FunctionTok{install\_tensorflow}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Al ejecutar esta función, los usuarios deberán marcar ``Y'' para aceptar la instalación de Mini-Conda, descartando aceptar la utilización de cualquier otro sistema Conda que pueda estar instalado previamente.

También se puede ejecutar la función \emph{install\_keras} del paquete \textbf{keras} para instalar Tensorflow {[}{]}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"keras"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(keras)}
\FunctionTok{install\_keras}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Paso 3} - Confirmar la instalación
\end{itemize}

Para confirmar la instalación, se puede comprobar con los siguientes comandos (la salida puede variar según el equipo, pero la línea final tiene que ser similar a la indicada):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tensorflow)}
\NormalTok{tf}\SpecialCharTok{$}\FunctionTok{constant}\NormalTok{(}\StringTok{"Hellow Tensorflow"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tf.Tensor}\NormalTok{(b}\StringTok{\textquotesingle{}Hellow Tensorflow\textquotesingle{}}\NormalTok{, }\AttributeTok{shape=}\NormalTok{(), }\AttributeTok{dtype=}\NormalTok{string)}
\end{Highlighting}
\end{Shaded}

\hypertarget{ejemplo-de-red-para-clasificaciuxf3n}{%
\section{\texorpdfstring{Ejemplo de red para clasificación\index{clasificación}}{Ejemplo de red para clasificación}}\label{ejemplo-de-red-para-clasificaciuxf3n}}

En esta sección se entrena una red neuronal artificial para reconocer o clasificar los dígitos manuscritos del conjunto de datos MNIST (\url{https://en.wikipedia.org/wiki/MNIST_database}). Cada una de las imágenes de este conjunto de datos tiene un tamaño de \(28\times28\) píxeles en escala de grises. En vez de extraer una serie de variables a partir de cada imagen, en este caso se utilizan cada uno de los \(28\times28=784\) píxeles como variable de entrada (Figura \ref{fig:mlpmnhist}).

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/mlpmnhist} 

}

\caption{MLP para reconocimiento de dígitos manuscritos}\label{fig:mlpmnhist}
\end{figure}

\hypertarget{carga-y-visualizaciuxf3n-de-los-datos}{%
\subsection{Carga y visualización de los datos}\label{carga-y-visualizaciuxf3n-de-los-datos}}

El primer paso será cargar la librería \textbf{keras} que permite crear redes neuronales y conjunto de imágenes que se encuentra disponible públicamente:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(keras)}
\NormalTok{mnist }\OtherTok{\textless{}{-}} \FunctionTok{dataset\_mnist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

A continuación, se puede ver el contenido de las variables generadas, donde cabe destacar que el conjunto de datos MNIST ya viene separado en dos subconjuntos, uno para entrenamiento y otro para test, compuestos por 60000 y 10000 imágenes respectivamente. En ambos casos, estos datos se almacenan en la variable \emph{x}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(mnist)}
\CommentTok{\#\textgreater{} [1] "train" "test"}
\FunctionTok{dim}\NormalTok{(mnist}\SpecialCharTok{$}\NormalTok{train}\SpecialCharTok{$}\NormalTok{x)}
\CommentTok{\#\textgreater{} [1] 60000    28    28}
\FunctionTok{dim}\NormalTok{(mnist}\SpecialCharTok{$}\NormalTok{train}\SpecialCharTok{$}\NormalTok{y)}
\CommentTok{\#\textgreater{} [1] 60000}
\FunctionTok{dim}\NormalTok{(mnist}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{x)}
\CommentTok{\#\textgreater{} [1] 10000    28    28}
\FunctionTok{dim}\NormalTok{(mnist}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{y)}
\CommentTok{\#\textgreater{} [1] 10000}
\end{Highlighting}
\end{Shaded}

Además, las imágenes de cada subconjunto vienen acompañadas de la clase a la que pertenecen (dígito contenido en la imagen). En ambos casos, esta etiqueta se almacena en la variable \emph{y}. A continuación se muestra un pequeño ejemplo que permitirá mostrar alguna de las imágenes contenidas en el conjunto de datos de entrenamiento junto con la etiqueta representando el dígito contenido:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfcol =} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{0}\NormalTok{), }\AttributeTok{xaxs =} \StringTok{"i"}\NormalTok{, }\AttributeTok{yaxs =} \StringTok{"i"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{16}\NormalTok{) \{}
\NormalTok{  im }\OtherTok{\textless{}{-}}\NormalTok{ mnist}\SpecialCharTok{$}\NormalTok{train}\SpecialCharTok{$}\NormalTok{x[j, , ]}
\NormalTok{  im }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(}\FunctionTok{apply}\NormalTok{(im, }\DecValTok{2}\NormalTok{, rev))}
  \FunctionTok{image}\NormalTok{(}
    \AttributeTok{x =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{28}\NormalTok{, }\AttributeTok{y =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{28}\NormalTok{, }\AttributeTok{z =}\NormalTok{ im, }\AttributeTok{col =} \FunctionTok{gray}\NormalTok{((}\DecValTok{0}\SpecialCharTok{:}\DecValTok{255}\NormalTok{) }\SpecialCharTok{/} \DecValTok{255}\NormalTok{),}
    \AttributeTok{xaxt =} \StringTok{"n"}\NormalTok{, }\AttributeTok{main =} \FunctionTok{paste}\NormalTok{(mnist}\SpecialCharTok{$}\NormalTok{train}\SpecialCharTok{$}\NormalTok{y[j])}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/unnamed-chunk-158-1} 

}

\caption{Algunas imágenes del conjunto de entrenamiento}\label{fig:unnamed-chunk-158}
\end{figure}

\hypertarget{preprocesamiento}{%
\subsection{Preprocesamiento}\label{preprocesamiento}}

Una vez cargados los datos y comprobado su contenido, es posible realizar algún tipo de preprocesado. Dependiendo del tipo de problema se podrán realizar unas operaciones u otras. Por ejemplo, cuando se trabaja con imágenes es muy típico estandarizar los valores de color de las imágenes para mitigar las diferencias producidas por las diferentes condiciones de iluminación.

En este caso, solo se va a transformar los valores originales de la imagen (en rango de 0 a 255) a valores entre 0 y 1 dividiendo cada valor por el máximo, 255:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mnist}\SpecialCharTok{$}\NormalTok{train}\SpecialCharTok{$}\NormalTok{x }\OtherTok{\textless{}{-}}\NormalTok{ mnist}\SpecialCharTok{$}\NormalTok{train}\SpecialCharTok{$}\NormalTok{x }\SpecialCharTok{/} \DecValTok{255}
\NormalTok{mnist}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{x }\OtherTok{\textless{}{-}}\NormalTok{ mnist}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{x }\SpecialCharTok{/} \DecValTok{255}
\end{Highlighting}
\end{Shaded}

\hypertarget{nngen}{%
\subsection{Generación de la red neuronal}\label{nngen}}

El siguiente paso consiste en la generación de la red neuronal. Para ello, se define primero la estructura utilizando la interfaz \emph{sequential} proporcionada por Tensorflow/Keras a través de la función \emph{keras\_model\_sequential}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{layer\_flatten}\NormalTok{(}\AttributeTok{input\_shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{28}\NormalTok{, }\DecValTok{28}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{15}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{layer\_dense}\NormalTok{(}\DecValTok{10}\NormalTok{, }\AttributeTok{activation =} \StringTok{"softmax"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Como se puede observar, la red definida está compuesta por una capa de tipo \emph{flatten} que se encarga de transformar los 28x28 valores a un vector de 784 elementos, para que a continuación una capa oculta \emph{dense} de 15 neuronas con activación \emph{relu} se encargue de realizar las primeras operaciones con esos datos. Al final, una última capa \emph{dense} se encarga de obtener la probabilidad de que la imagen represente cada una de las posibles clases mediante una activación \emph{softmax}\footnote{La activación \emph{softmax} convierte un vector de número reales en una distribución de probabilidad.}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(model, }\AttributeTok{line\_length =} \DecValTok{64}\NormalTok{)}
\CommentTok{\#\textgreater{} Model: "sequential"}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{}  Layer (type)               Output Shape              Param \#   }
\CommentTok{\#\textgreater{} ================================================================}
\CommentTok{\#\textgreater{}  flatten (Flatten)          (None, 784)               0         }
\CommentTok{\#\textgreater{}  dense\_1 (Dense)            (None, 15)                11775     }
\CommentTok{\#\textgreater{}  dense (Dense)              (None, 10)                160       }
\CommentTok{\#\textgreater{} ================================================================}
\CommentTok{\#\textgreater{} Total params: 11,935}
\CommentTok{\#\textgreater{} Trainable params: 11,935}
\CommentTok{\#\textgreater{} Non{-}trainable params: 0}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\end{Highlighting}
\end{Shaded}

Finalmente, es necesario compilar el modelo, indicando algunos de los parámetros de configuración necesarios para el proceso de entrenamiento, como la función de coste o pérdida, el optimizador a utilizar y las métricas a obtener:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{compile}\NormalTok{(}
    \AttributeTok{loss =} \StringTok{"sparse\_categorical\_crossentropy"}\NormalTok{, }\CommentTok{\# función utilizada para problemas de clasificación con varias clases}
    \AttributeTok{optimizer =} \StringTok{"sgd"}\NormalTok{, }\CommentTok{\# stochastic gradient descent}
    \AttributeTok{metrics =} \StringTok{"accuracy"} \CommentTok{\# Precisión}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{nntrain}{%
\subsection{Entrenamiento}\label{nntrain}}

Una vez generada la estructura de la red neuronal y definida la anterior configuración, es posible entrenarla mediante la función \emph{fit()}. Para ello, se le debe indicar el conjunto de imágenes de entrenamiento, \emph{x}, que debe utilizar y sus clases correspondientes, \emph{y}. Además de otros parámetros, se podrá configurar el número de \emph{epochs} a entrenar (pasadas sobre el conjunto completo de entrenamiento), el tamaño del batch que se utilizará en cada iteración con \emph{batch\_size} (número de imágenes por iteración), qué porcentaje de elementos del conjunto de datos se utilizarán para validar el modelo con \emph{validation\_split} (imágenes utilizadas durante el entrenamiento pero solo para obtener una estimación real del error cometido) o la tasa de aprendizaje, \emph{learning\_rate}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{training\_evolution }\OtherTok{\textless{}{-}}\NormalTok{ model }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fit}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ mnist}\SpecialCharTok{$}\NormalTok{train}\SpecialCharTok{$}\NormalTok{x, }\AttributeTok{y =}\NormalTok{ mnist}\SpecialCharTok{$}\NormalTok{train}\SpecialCharTok{$}\NormalTok{y,}
    \AttributeTok{epochs =} \DecValTok{10}\NormalTok{, }\AttributeTok{batch\_size =} \DecValTok{128}\NormalTok{,}
    \AttributeTok{validation\_split =} \FloatTok{0.2}\NormalTok{,}
    \AttributeTok{learning\_rate =} \FloatTok{0.1}\NormalTok{,}
    \AttributeTok{verbose =} \DecValTok{2}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Epoch }\DecValTok{1}\SpecialCharTok{/}\DecValTok{10}
\DecValTok{375}\SpecialCharTok{/}\DecValTok{375} \SpecialCharTok{{-}}\NormalTok{ 2s }\SpecialCharTok{{-}}\NormalTok{ loss}\SpecialCharTok{:} \FloatTok{1.6313} \SpecialCharTok{{-}}\NormalTok{ accuracy}\SpecialCharTok{:} \FloatTok{0.5266} \SpecialCharTok{{-}}\NormalTok{ val\_loss}\SpecialCharTok{:} \FloatTok{1.0455} \SpecialCharTok{{-}}\NormalTok{ val\_accuracy}\SpecialCharTok{:} \FloatTok{0.7510} \SpecialCharTok{{-}}\NormalTok{ 2s}\SpecialCharTok{/}\NormalTok{epoch }\SpecialCharTok{{-}}\NormalTok{ 6ms}\SpecialCharTok{/}\NormalTok{step}
\NormalTok{Epoch }\DecValTok{2}\SpecialCharTok{/}\DecValTok{10}
\DecValTok{375}\SpecialCharTok{/}\DecValTok{375} \SpecialCharTok{{-}}\NormalTok{ 1s }\SpecialCharTok{{-}}\NormalTok{ loss}\SpecialCharTok{:} \FloatTok{0.8433} \SpecialCharTok{{-}}\NormalTok{ accuracy}\SpecialCharTok{:} \FloatTok{0.7881} \SpecialCharTok{{-}}\NormalTok{ val\_loss}\SpecialCharTok{:} \FloatTok{0.6409} \SpecialCharTok{{-}}\NormalTok{ val\_accuracy}\SpecialCharTok{:} \FloatTok{0.8434} \SpecialCharTok{{-}}\NormalTok{ 1s}\SpecialCharTok{/}\NormalTok{epoch }\SpecialCharTok{{-}}\NormalTok{ 3ms}\SpecialCharTok{/}\NormalTok{step}
\NormalTok{Epoch }\DecValTok{3}\SpecialCharTok{/}\DecValTok{10}
\DecValTok{375}\SpecialCharTok{/}\DecValTok{375} \SpecialCharTok{{-}}\NormalTok{ 1s }\SpecialCharTok{{-}}\NormalTok{ loss}\SpecialCharTok{:} \FloatTok{0.6022} \SpecialCharTok{{-}}\NormalTok{ accuracy}\SpecialCharTok{:} \FloatTok{0.8427} \SpecialCharTok{{-}}\NormalTok{ val\_loss}\SpecialCharTok{:} \FloatTok{0.5031} \SpecialCharTok{{-}}\NormalTok{ val\_accuracy}\SpecialCharTok{:} \FloatTok{0.8712} \SpecialCharTok{{-}}\NormalTok{ 1s}\SpecialCharTok{/}\NormalTok{epoch }\SpecialCharTok{{-}}\NormalTok{ 3ms}\SpecialCharTok{/}\NormalTok{step}
\NormalTok{Epoch }\DecValTok{4}\SpecialCharTok{/}\DecValTok{10}
\DecValTok{375}\SpecialCharTok{/}\DecValTok{375} \SpecialCharTok{{-}}\NormalTok{ 1s }\SpecialCharTok{{-}}\NormalTok{ loss}\SpecialCharTok{:} \FloatTok{0.5047} \SpecialCharTok{{-}}\NormalTok{ accuracy}\SpecialCharTok{:} \FloatTok{0.8656} \SpecialCharTok{{-}}\NormalTok{ val\_loss}\SpecialCharTok{:} \FloatTok{0.4381} \SpecialCharTok{{-}}\NormalTok{ val\_accuracy}\SpecialCharTok{:} \FloatTok{0.8830} \SpecialCharTok{{-}}\NormalTok{ 1s}\SpecialCharTok{/}\NormalTok{epoch }\SpecialCharTok{{-}}\NormalTok{ 3ms}\SpecialCharTok{/}\NormalTok{step}
\NormalTok{Epoch }\DecValTok{5}\SpecialCharTok{/}\DecValTok{10}
\DecValTok{375}\SpecialCharTok{/}\DecValTok{375} \SpecialCharTok{{-}}\NormalTok{ 1s }\SpecialCharTok{{-}}\NormalTok{ loss}\SpecialCharTok{:} \FloatTok{0.4526} \SpecialCharTok{{-}}\NormalTok{ accuracy}\SpecialCharTok{:} \FloatTok{0.8767} \SpecialCharTok{{-}}\NormalTok{ val\_loss}\SpecialCharTok{:} \FloatTok{0.4019} \SpecialCharTok{{-}}\NormalTok{ val\_accuracy}\SpecialCharTok{:} \FloatTok{0.8909} \SpecialCharTok{{-}}\NormalTok{ 1s}\SpecialCharTok{/}\NormalTok{epoch }\SpecialCharTok{{-}}\NormalTok{ 3ms}\SpecialCharTok{/}\NormalTok{step}
\NormalTok{Epoch }\DecValTok{6}\SpecialCharTok{/}\DecValTok{10}
\DecValTok{375}\SpecialCharTok{/}\DecValTok{375} \SpecialCharTok{{-}}\NormalTok{ 1s }\SpecialCharTok{{-}}\NormalTok{ loss}\SpecialCharTok{:} \FloatTok{0.4201} \SpecialCharTok{{-}}\NormalTok{ accuracy}\SpecialCharTok{:} \FloatTok{0.8854} \SpecialCharTok{{-}}\NormalTok{ val\_loss}\SpecialCharTok{:} \FloatTok{0.3764} \SpecialCharTok{{-}}\NormalTok{ val\_accuracy}\SpecialCharTok{:} \FloatTok{0.8959} \SpecialCharTok{{-}}\NormalTok{ 1s}\SpecialCharTok{/}\NormalTok{epoch }\SpecialCharTok{{-}}\NormalTok{ 3ms}\SpecialCharTok{/}\NormalTok{step}
\NormalTok{Epoch }\DecValTok{7}\SpecialCharTok{/}\DecValTok{10}
\DecValTok{375}\SpecialCharTok{/}\DecValTok{375} \SpecialCharTok{{-}}\NormalTok{ 1s }\SpecialCharTok{{-}}\NormalTok{ loss}\SpecialCharTok{:} \FloatTok{0.3976} \SpecialCharTok{{-}}\NormalTok{ accuracy}\SpecialCharTok{:} \FloatTok{0.8896} \SpecialCharTok{{-}}\NormalTok{ val\_loss}\SpecialCharTok{:} \FloatTok{0.3593} \SpecialCharTok{{-}}\NormalTok{ val\_accuracy}\SpecialCharTok{:} \FloatTok{0.8996} \SpecialCharTok{{-}}\NormalTok{ 1s}\SpecialCharTok{/}\NormalTok{epoch }\SpecialCharTok{{-}}\NormalTok{ 3ms}\SpecialCharTok{/}\NormalTok{step}
\NormalTok{Epoch }\DecValTok{8}\SpecialCharTok{/}\DecValTok{10}
\DecValTok{375}\SpecialCharTok{/}\DecValTok{375} \SpecialCharTok{{-}}\NormalTok{ 1s }\SpecialCharTok{{-}}\NormalTok{ loss}\SpecialCharTok{:} \FloatTok{0.3809} \SpecialCharTok{{-}}\NormalTok{ accuracy}\SpecialCharTok{:} \FloatTok{0.8939} \SpecialCharTok{{-}}\NormalTok{ val\_loss}\SpecialCharTok{:} \FloatTok{0.3463} \SpecialCharTok{{-}}\NormalTok{ val\_accuracy}\SpecialCharTok{:} \FloatTok{0.9022} \SpecialCharTok{{-}}\NormalTok{ 1s}\SpecialCharTok{/}\NormalTok{epoch }\SpecialCharTok{{-}}\NormalTok{ 3ms}\SpecialCharTok{/}\NormalTok{step}
\NormalTok{Epoch }\DecValTok{9}\SpecialCharTok{/}\DecValTok{10}
\DecValTok{375}\SpecialCharTok{/}\DecValTok{375} \SpecialCharTok{{-}}\NormalTok{ 1s }\SpecialCharTok{{-}}\NormalTok{ loss}\SpecialCharTok{:} \FloatTok{0.3678} \SpecialCharTok{{-}}\NormalTok{ accuracy}\SpecialCharTok{:} \FloatTok{0.8975} \SpecialCharTok{{-}}\NormalTok{ val\_loss}\SpecialCharTok{:} \FloatTok{0.3359} \SpecialCharTok{{-}}\NormalTok{ val\_accuracy}\SpecialCharTok{:} \FloatTok{0.9050} \SpecialCharTok{{-}}\NormalTok{ 1s}\SpecialCharTok{/}\NormalTok{epoch }\SpecialCharTok{{-}}\NormalTok{ 3ms}\SpecialCharTok{/}\NormalTok{step}
\NormalTok{Epoch }\DecValTok{10}\SpecialCharTok{/}\DecValTok{10}
\DecValTok{375}\SpecialCharTok{/}\DecValTok{375} \SpecialCharTok{{-}}\NormalTok{ 1s }\SpecialCharTok{{-}}\NormalTok{ loss}\SpecialCharTok{:} \FloatTok{0.3571} \SpecialCharTok{{-}}\NormalTok{ accuracy}\SpecialCharTok{:} \FloatTok{0.8997} \SpecialCharTok{{-}}\NormalTok{ val\_loss}\SpecialCharTok{:} \FloatTok{0.3289} \SpecialCharTok{{-}}\NormalTok{ val\_accuracy}\SpecialCharTok{:} \FloatTok{0.9064} \SpecialCharTok{{-}}\NormalTok{ 1s}\SpecialCharTok{/}\NormalTok{epoch }\SpecialCharTok{{-}}\NormalTok{ 3ms}\SpecialCharTok{/}\NormalTok{step}
\end{Highlighting}
\end{Shaded}

Tras el entrenamiento es posible ver su evolución mediante las gráficas de coste/pérdida y precisión:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(training\_evolution)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/curve1} 

}

\caption{Evolución durante el entrenamiento de la precisión y la pérdida de los conjuntos de entrenamiento y validación}\label{fig:plot-curve1}
\end{figure}

Como se puede observar, la red entrenada tiene alrededor de un 90\% de precisión para las imágenes en los conjuntos de entrenamiento y validación.

\hypertarget{test}{%
\subsection{Test}\label{test}}

Una vez entrenado el modelo, es posible aplicarlo sobre el conjunto de test. Para ello, se puede realizar la predicción sobre cualquiera de las imágenes mediante la función \emph{predict}, obteniendo la probabilidad de que pertenezca a una determinada clase:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predictions }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(model, mnist}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{x)}
\FunctionTok{head}\NormalTok{(}\FunctionTok{round}\NormalTok{(predictions, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{), }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]
#> [1,] 0.000 0.000 0.000 0.003 0.000 0.000 0.000 0.995 0.000 0.002
#> [2,] 0.009 0.000 0.836 0.024 0.000 0.009 0.119 0.000 0.003 0.000
#> [3,] 0.000 0.962 0.013 0.006 0.001 0.001 0.003 0.002 0.010 0.002
#> [4,] 0.999 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
#> [5,] 0.001 0.000 0.007 0.000 0.836 0.004 0.011 0.012 0.017 0.111
\end{verbatim}

También se puede utilizar la función \emph{evaluate} para calcular tanto el coste o pérdida como la precisión de la red neuronal sobre el conjunto de test. Como se puede observar, se obtienen valores muy similares a los obtenidos durante el entrenamiento:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{evaluate}\NormalTok{(mnist}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{x, mnist}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{y, }\AttributeTok{verbose =} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>      loss  accuracy 
#> 0.3310305 0.9045000
\end{verbatim}

Con la función \emph{predict} se puede también generar la matriz de confusión de la red para evaluar qué pares de clases está confundiendo:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prediction\_matrix }\OtherTok{\textless{}{-}}\NormalTok{ model }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{predict}\NormalTok{(mnist}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{x) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{k\_argmax}\NormalTok{()}
\NormalTok{confusion\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(}\FunctionTok{as.array}\NormalTok{(prediction\_matrix), mnist}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{y)}
\NormalTok{confusion\_matrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>    
#>        0    1    2    3    4    5    6    7    8    9
#>   0  953    0   11    4    2   16   16    3    8    7
#>   1    0 1108   10    2    6    1    3   21   10    5
#>   2    4    3  901   27    5   11   14   27   13    6
#>   3    2    2   16  903    0   46    1    4   29   10
#>   4    1    0   16    0  899   16   12    9   11   43
#>   5    6    1    1   29    1  726    8    1   24   13
#>   6    9    4   19    3   10   21  902    0   10    0
#>   7    2    2   12   17    2   10    0  916   11   18
#>   8    3   15   35   20   10   38    2    3  839    9
#>   9    0    0   11    5   47    7    0   44   19  898
\end{verbatim}

En la diagonal principal podemos observar el número de aciertos que obtiene el modelo entrenado para el conjunto de test, mientras que el resto de valores indican en cuantas ocasiones una clase es clasificada de manera incorrecta como otra diferente. Estos resultados coinciden con el valor de \textbf{accuracy} calculado mediante la función \textbf{evaluate} previa.

\hypertarget{guardado-y-reutilizaciuxf3n-del-modelo}{%
\subsection{Guardado y reutilización del modelo}\label{guardado-y-reutilizaciuxf3n-del-modelo}}

Finalmente, es posible almacenar el modelo entrenado mediante la función \emph{save\_model\_tf}, que genera una carpeta con la red que se puede cargar y reutilizar mediante la función \emph{load\_model\_tf}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{save\_model\_tf}\NormalTok{(}\AttributeTok{object =}\NormalTok{ model, }\AttributeTok{filepath =} \StringTok{"model"}\NormalTok{)}
\NormalTok{reloaded\_model }\OtherTok{\textless{}{-}} \FunctionTok{load\_model\_tf}\NormalTok{(}\StringTok{"model"}\NormalTok{)}
\FunctionTok{round}\NormalTok{(}\FunctionTok{predict}\NormalTok{(reloaded\_model, mnist}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{x[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{28}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{28}\NormalTok{]), }\AttributeTok{digits =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>       [,1] [,2]  [,3]   [,4] [,5]  [,6] [,7]   [,8] [,9] [,10]
#> [1,] 2e-04    0 1e-04 0.0028    0 1e-04    0 0.9948    0 0.002
\end{verbatim}

\hypertarget{ejemplo-de-red-para-regresiuxf3n}{%
\section{Ejemplo de red para regresión}\label{ejemplo-de-red-para-regresiuxf3n}}

En esta sección se entrena una red neuronal artificial para predecir el precio de la vivienda según sus características en Madrid. Para ello se usará el dataset de \textbf{Madrid\_Sale} disponibles en el paquete de \emph{R} \textbf{Idealista18}, con datos inmobiliaros del año 2018 y que fue utilizado en el Capítulo @ref(\#chap-feature). Para ello, se tomarán las siguientes 7 variables que se usarán para realizar la estimación:

\begin{itemize}
\tightlist
\item
  \emph{CONSTRUCTEDAREA}. Metros cuadrados construidos.
\item
  \emph{ROOMNUMBER}. Número de habitaciones.
\item
  \emph{BATHNUMBER}. Número de baños.
\item
  \emph{HASLIFT}. Si tiene ascensor.
\item
  \emph{DISTANCE\_TO\_CITY\_CENTER}. Distancia al centro de la ciudad.
\item
  \emph{DISTANCE\_TO\_METRO}. Distancia a la parada de metro más cercana.
\item
  \emph{DISTANCE\_TO\_CASTELLANA}. Distancia a la Castellana.
\end{itemize}

\hypertarget{carga-y-visualizaciuxf3n-de-los-datos-1}{%
\subsection{Carga y visualización de los datos}\label{carga-y-visualizaciuxf3n-de-los-datos-1}}

Considerando que ya se ha cargado previamente la librería \textbf{keras}, se carga el conjunto de datos indicando las variables a considerar:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(idealista18)}
\FunctionTok{data}\NormalTok{(}\StringTok{"Madrid\_Sale"}\NormalTok{)}

\NormalTok{variables }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}
  \StringTok{"CONSTRUCTEDAREA"}\NormalTok{, }\StringTok{"ROOMNUMBER"}\NormalTok{, }\StringTok{"BATHNUMBER"}\NormalTok{,}
  \StringTok{"HASLIFT"}\NormalTok{, }\StringTok{"DISTANCE\_TO\_CITY\_CENTER"}\NormalTok{, }\StringTok{"DISTANCE\_TO\_METRO"}\NormalTok{,}
  \StringTok{"DISTANCE\_TO\_CASTELLANA"}
\NormalTok{)}
\NormalTok{x\_madrid }\OtherTok{\textless{}{-}}\NormalTok{ Madrid\_Sale[variables]}
\NormalTok{x\_madrid\_mat }\OtherTok{\textless{}{-}} \FunctionTok{unname}\NormalTok{(}\FunctionTok{data.matrix}\NormalTok{(x\_madrid))}
\NormalTok{y\_madrid }\OtherTok{\textless{}{-}}\NormalTok{ Madrid\_Sale}\SpecialCharTok{$}\NormalTok{PRICE}
\NormalTok{y\_madrid\_mat }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(y\_madrid, }\AttributeTok{nrow =} \FunctionTok{length}\NormalTok{(y\_madrid), }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

El conjunto de datos contiene un total de 94815 elementos, que se dividirán en un 90\% para entrenamiento y un 10\% para test:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ind }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\ConstantTok{TRUE}\NormalTok{, }\ConstantTok{FALSE}\NormalTok{), }\FunctionTok{length}\NormalTok{(y\_madrid), }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{prob =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.9}\NormalTok{, }\FloatTok{0.1}\NormalTok{))}
\NormalTok{madrid\_dat\_train\_x }\OtherTok{\textless{}{-}}\NormalTok{ x\_madrid\_mat[ind, ]}
\NormalTok{madrid\_dat\_test\_x }\OtherTok{\textless{}{-}}\NormalTok{ x\_madrid\_mat[}\SpecialCharTok{!}\NormalTok{ind, ]}
\NormalTok{madrid\_dat\_train\_y }\OtherTok{\textless{}{-}}\NormalTok{ y\_madrid\_mat[ind, ]}
\NormalTok{madrid\_dat\_test\_y }\OtherTok{\textless{}{-}}\NormalTok{ y\_madrid\_mat[}\SpecialCharTok{!}\NormalTok{ind, ]}
\end{Highlighting}
\end{Shaded}

\hypertarget{preprocesamiento-1}{%
\subsection{Preprocesamiento}\label{preprocesamiento-1}}

Una vez cargados los datos y comprobado su contenido, es recomendable la normalización de las variables contenidas en el conjunto de datos debido a su heterogeneidad. Aunque sería posible para la red neuronal el adaptarse a esta situación, ciertamente puede complicar el proceso de entrenamiento. Para ello, se utilizará la función \emph{scale} en las variables predictoras y se dividirá la variable del precio entre 100000 para reducir su orden:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{madrid\_dat\_train\_x }\OtherTok{\textless{}{-}} \FunctionTok{scale}\NormalTok{(madrid\_dat\_train\_x)}
\NormalTok{madrid\_dat\_test\_x }\OtherTok{\textless{}{-}} \FunctionTok{scale}\NormalTok{(madrid\_dat\_test\_x)}
\NormalTok{madrid\_dat\_train\_y }\OtherTok{\textless{}{-}}\NormalTok{ madrid\_dat\_train\_y }\SpecialCharTok{/} \DecValTok{100000}
\NormalTok{madrid\_dat\_test\_y }\OtherTok{\textless{}{-}}\NormalTok{ madrid\_dat\_test\_y }\SpecialCharTok{/} \DecValTok{100000}
\end{Highlighting}
\end{Shaded}

\hypertarget{generaciuxf3n-de-la-red-neuronal}{%
\subsection{Generación de la red neuronal}\label{generaciuxf3n-de-la-red-neuronal}}

El siguiente paso consiste en la generación de la red neuronal. Para ello, al igual que en la sección \ref{nngen}, se define primero la estructura utilizando la interfaz \emph{sequential} proporcionada por Tensorflow/Keras a través de la función \emph{keras\_model\_sequential}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{128}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{, }\AttributeTok{input\_shape =} \DecValTok{7}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{64}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{16}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Como se puede observar, la red está compuesta por varias capas ocultas tipo \emph{dense}, en las que las tres primeras tienen una activación \emph{relu}. Al final, una última capa \emph{dense} se encarga de obtener el valor de la estimación y, al contrario que en el ejemplo previo, no incluye ningún tipo de activación debido a que el valor de la misma ya es comprensible tanto para el modelo como para su interpretación, y cualquier función de activación restringiría el rango de valores que podría obtener.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(model, }\AttributeTok{line\_length =} \DecValTok{64}\NormalTok{)}
\CommentTok{\#\textgreater{} Model: "sequential\_1"}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{} Layer (type) Output Shape Param \#}
\CommentTok{\#\textgreater{} ================================================================}
\CommentTok{\#\textgreater{} dense\_5 (Dense) (None, 128) 1024}
\CommentTok{\#\textgreater{} dense\_4 (Dense) (None, 64) 8256}
\CommentTok{\#\textgreater{} dense\_3 (Dense) (None, 16) 1040}
\CommentTok{\#\textgreater{} dense\_2 (Dense) (None, 1) 17}
\CommentTok{\#\textgreater{} ================================================================}
\CommentTok{\#\textgreater{} Total params: 10,337}
\CommentTok{\#\textgreater{} Trainable params: 10,337}
\CommentTok{\#\textgreater{} Non{-}trainable params: 0}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\end{Highlighting}
\end{Shaded}

Finalmente, se compila el modelo indicando los parámetros de configuración necesarios para el proceso de entrenamiento. En este caso la función de coste o pérdida se corresponderá con el error medio cuadrático y la métrica con el error medio absoluto:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{compile}\NormalTok{(}
    \AttributeTok{loss =} \StringTok{"mse"}\NormalTok{, }\CommentTok{\# mean squared error}
    \AttributeTok{optimizer =} \StringTok{"sgd"}\NormalTok{, }\CommentTok{\# stochastic gradient descent}
    \AttributeTok{metrics =} \StringTok{"mae"} \CommentTok{\# mean average error}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{entrenamiento}{%
\subsection{Entrenamiento}\label{entrenamiento}}

Una vez generada la estructura de la red neuronal y definida la anterior configuración, se entrena la misma utilizando la función \emph{fit}, configurando el resto de parámetros de forma similar a como se vio en la sección \ref{nntrain}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{training\_evolution }\OtherTok{\textless{}{-}}\NormalTok{ model }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fit}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ madrid\_dat\_train\_x, }\AttributeTok{y =}\NormalTok{ madrid\_dat\_train\_y,}
    \AttributeTok{epochs =} \DecValTok{50}\NormalTok{, }\AttributeTok{batch\_size =} \DecValTok{512}\NormalTok{,}
    \AttributeTok{validation\_split =} \FloatTok{0.2}\NormalTok{,}
    \AttributeTok{learning\_rate =} \FloatTok{0.1}\NormalTok{,}
    \AttributeTok{verbose =} \DecValTok{2}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Tras el entrenamiento es posible ver su evolución mediante las gráficas de coste/pérdida y error:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(training\_evolution)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/curve2} 

}

\caption{Evolución durante el entrenamiento de la precisión y la pérdida de los conjuntos de entrenamiento y validación}\label{fig:plot-curve2}
\end{figure}

Como se puede observar, en este caso el modelo tiene aún posibilidad de mejora, ya que la pérdida sigue siendo alta y no se ha estancado, por lo que incrementando el número de épocas y el tiempo de entrenamiento se podría obtener un mejor resultado.

\hypertarget{test-1}{%
\subsection{Test}\label{test-1}}

Una vez entrenado el modelo, es posible aplicarlo sobre el conjunto de test mediante la función \emph{predict}, obteniendo la estimación para cada una de las viviendas:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predictions }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(model, madrid\_dat\_test\_x)}
\FunctionTok{head}\NormalTok{(predictions, }\DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{} [,1]}
\CommentTok{\#\textgreater{} [1,] 6.669374}
\CommentTok{\#\textgreater{} [2,] 5.895504}
\CommentTok{\#\textgreater{} [3,] 3.887646}
\CommentTok{\#\textgreater{} [4,] 6.390513}
\CommentTok{\#\textgreater{} [5,] 5.721725}
\end{Highlighting}
\end{Shaded}

Y mediante la función \emph{evaluate} se calcula tanto el coste o pérdida como el error de la red neuronal sobre el conjunto de test, el cual tendremos que multiplicar por 100000 para obtener el resultado en la escala original del conjunto de datos:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{evaluate}\NormalTok{(madrid\_dat\_test\_x, madrid\_dat\_test\_y, }\AttributeTok{verbose =} \DecValTok{0}\NormalTok{)}
\CommentTok{\#\textgreater{} loss mae}
\CommentTok{\#\textgreater{} 2.4195166 0.9227165}
\end{Highlighting}
\end{Shaded}

\begin{infobox_resume}

\textbf{RESUMEN}

\begin{itemize}
\item
  En este capítulo se ha explicado en detalle el concepto de redes neuronales artificiales\index{redes neuronales artificiales}, incluyendo los elementos que la componen, desde el perceptrón\index{perceptrón} o neurona\index{neurona} básica hasta el perceptrón multicapa\index{perceptrón multicapa}, pasando el perceptron multiclase, junto al proceso de aprendizaje de los mismos.
\item
  Además, se han definido las funciones de activación clásicas utilizadas en las redes neuronales artificiales\index{redes neuronales artificiales}, las cuales se encargan de transformar la suma ponderada de las entradas en el resultado final de la capa.
\item
  Finalmente, se han explicado los pasos necesarios para poder entrenar una red neuronal artificial utilizando la librería Tensorflow/Keras en \textbf{R}, resolviendo el problema de clasificación\index{clasificación} de dígitos manuscritos representado en el conjunto de datos MNIST y un problema de regresión para estimar el precio de viviendas según sus características representado en el conjunto de datos de Idealista18.
\end{itemize}

\end{infobox_resume}

\hypertarget{cap-redes-convol}{%
\chapter{Redes neuronales convolucionales}\label{cap-redes-convol}}

\index{redes neuronales convolucionales}

\emph{Noelia Vállez Enano y José Luis Espinosa Aranda}

\hypertarget{introducciuxf3n-20}{%
\section{Introducción}\label{introducciuxf3n-20}}

Las redes neuronales convolucionales (del inglés Convolutional Neural Network, CNN) son una extensión de las redes neuronales artificiales\index{redes neuronales artificiales} en las que se incluyen capas convolucionales para aprender a extraer, de forma automática, las características de los datos de entrenamiento al inicio de la arquitectura (Fig. \ref{fig:cnn}).

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/cnn} 

}

\caption{Estructura general de una CNN}\label{fig:cnn}
\end{figure}

Las primeras capas convolucionales de la red aprenden a extraer características generales de los datos de entrada mientras que las últimas capas convolucionales extraen características mucho más específicas. Cuanto más larga es la red (o más \emph{profunda}) mayor cantidad de detalles podrá aprender a distinguir. Esto es lo que ha propiciado la aparición del término ``aprendizaje profundo'' en los últimos años \citep{goodfellow2016deep}.

Tras las capas convolucionales suelen encontrarse las capas \emph{densas} o \emph{totalmente conectadas} que no son otra cosa que una red neuronal artificial tal y como vimos en el capítulo \ref{capNN} Esta parte de la red será la encargada de realizar la clasificación\index{clasificación} de las muestras según los valores de las características extraídas en la parte convolucional. Por tanto, se dice que este tipo de redes tiene dos partes: una parte de extracción de características y una parte de clasificación (o regresión).

\hypertarget{convoluciuxf3n}{%
\section{Convolución}\label{convoluciuxf3n}}

Aunque las redes neuronales artificiales\index{redes neuronales artificiales} pueden utilizarse con los valores de color de una imagen como variables para reconocer qué hay en ella (ver capítulo \ref{capNN}), no es posible extraer información espacial de esta forma. Para lidiar con este problema, las CNN incorporan capas convolucionales para extraer características de las muestras de entrada, incluyendo información de la estructura espacial \citep{lecun1995convolutional}.

Las convoluciones realizan una tarea similar al sistema visual humano, de hecho, se inspiran en cómo el ser humano percibe y procesa las características de los objetos. Aunque se diseñaron principalmente para ayudar a resolver tareas de visión por computador donde la entrada de la red es una imagen, es posible utilizarlas también con entradas vectoriales o series temporales.

Una convolución aplica un filtro sobre la entrada siguiendo un proceso de ventana deslizante. El filtro (o \emph{kernel}) no es otra cosa que una matriz con unos pesos que se centrará en cada uno de los valores de la entrada para realizar una media ponderada de los valores de la entrada por los valores del filtro \citep{Bueno2015}. El tamaño de los filtros suele ser, por tanto, impar.

La Figura \ref{fig:convolution} muestra el resultado de aplicar la operación de convolución con un filtro de tamaño 3x3 sobre una matriz de entrada de tamaño 5x5. La salida será una matriz, \(\boldsymbol M\), de tamaño 3x3, donde cada elemento será la suma ponderada de multiplicar los elementos del filtro centrado en esa posición de la entrada por los valores de la entrada.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/convolution} 

}

\caption{Ejemplo de convolución. De izquierda a derecha: entrada, filtro y salida.}\label{fig:convolution}
\end{figure}

En general, una convolución en dos dimensiones se define como:

\begin{equation} 
\boldsymbol M[x,y]=\sum_{s=-a}^{a}\sum_{t=-b}^{b}\boldsymbol F[s,t]\boldsymbol I[x-s,y-t] ,
\end{equation}

donde \(\boldsymbol F\) es el filtro a aplicar, \(\boldsymbol I\) es la matriz de entrada, \(\boldsymbol M\) es la matriz de resultado que recibe también el nombre de ``mapa de características'' y \(a\) y \(b\) son los desplazamientos desde el centro del filtro a cualquier otro valor.

Por tanto, cada valor del ejemplo de la Figura \ref{fig:convolution} se obtiene como:

\begin{gather} 
\begin{aligned}
M_{1,1} = 1\cdot0+ 0\cdot 0+1 \cdot 0+0 \cdot 0 + & 1 \cdot 1 + 0 \cdot 1+ 1\cdot 0 +0\cdot 1 +1 \cdot 1=2 \\
M_{1,2} = 1\cdot0+ 0\cdot 0+1 \cdot 0+0 \cdot 1 + & 1 \cdot 1 + 0 \cdot 1+ 1\cdot 1 +0\cdot 1 +1 \cdot 1=3 \\
& \cdots \\
M_{2,2} = 1\cdot1+ 0\cdot 1+1 \cdot 1+0 \cdot 1 + & 1 \cdot 1 + 0 \cdot 1+ 1\cdot 1 +0\cdot 1 +1 \cdot 1=5 \\
& \cdots \\
M_{3,3} = 1\cdot1+ 0\cdot 1+1 \cdot 0+0 \cdot 1 + & 1 \cdot 1 + 0 \cdot 0+ 1\cdot 0 +0\cdot 0 +1 \cdot 0=2
\end{aligned}
\end{gather}

La elección de los valores del filtro obtendrá matrices de salida que realcen o suavicen ciertas partes de la entrada. Por ejemplo, si la entrada es una imagen, es posible definir filtros que realcen los bordes, que los suavicen o incluso que detecten dichos bordes y cómo de marcados están. La Figura \ref{fig:filtros} muestra el resultado de aplicar distintos filtros a una imagen de entrada.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/filtros} 

}

\caption{Resultado de aplicar diferentes filtros de convolución sobre una imagen dada.}\label{fig:filtros}
\end{figure}

Los valores (o pesos) de los filtros, que se ajustaban tradicionalmente de forma manual según el problema a resolver, se ajustan durante el proceso de entrenamiento de la CNN junto al resto de pesos de la red. Esto permite encontrar valores que maximicen la precisión final de la red.

\hypertarget{neuronas-convolucionales}{%
\section{Neuronas convolucionales}\label{neuronas-convolucionales}}

Las capas convolucionales de la CNN no estarán compuestas por perceptrones, sino por neuronas convolucionales que realizan dicha operación. Estas neuronas cuentan con matrices de pesos y no con vectores de pesos como lo hace el perceptrón\index{perceptrón}. En este caso, tanto la entrada como la salida de la neurona\index{neurona} son matrices. Para una neurona\index{neurona} \(j\), la salida \(\boldsymbol Y_j\) se calcula como la combinación lineal de las salidas de las neuronas de la capa anterior \(\boldsymbol Yi\) operando cada una de ellas con el filtro \(\boldsymbol F_{ij}\) correspondiente a esa conexión de forma que:

\begin{equation} 
\boldsymbol Y_j = g ( \boldsymbol B_j + \sum \boldsymbol F_{ij} \otimes + \boldsymbol Y_i ) , 
\end{equation}

donde \(\boldsymbol B_j\) y \(g\) representan el \emph{bias} y la función de activación respectivamente. La mayoría de las CNN utilizan la ReLU como activación o alguna variante de ésta. Esta activación funciona muy bien con el método del descenso del gradiente\index{descenso del gradiente} utilizado para encontrar los pesos.

Cada neurona\index{neurona} dará lugar a un \emph{mapa de activaciones}. La salida de una capa convolucional será entonces un conjunto de estos mapas (Fig. \ref{fig:mapas})

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/mapas} 

}

\caption{Conjunto de mapas de activaciones de una determinada capa. Cada filtro de la capa da lugar a un mapa diferente.}\label{fig:mapas}
\end{figure}

En el caso de que la entrada no sea una matriz 2D sino que sea una matriz 3D como, por ejemplo, una imagen, los filtros contarán con una tercera dimensión. La Figura \ref{fig:relu} muestra algunos de los rellenos más empleados.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/relu} 

}

\caption{Resultado de aplicar un filtro 3D a una imagen antes y después de pasar por el filtro de activación}\label{fig:relu}
\end{figure}

\hypertarget{relleno-del-borde}{%
\section{Relleno del borde}\label{relleno-del-borde}}

Si se aplica el filtro convolucional a una entrada, la salida será algo más pequeña al no poder centrar el filtro en los bordes de la matriz. Para poder hacerlo, se suele incrementar la entrada de la capa con un relleno (en inglés \emph{padding}\index{padding}). El relleno se puede realizar con ceros, con algún valor, con el valor más cercano del borde, etc. La Figura \ref{fig:padding} muestra algunos de los rellenos más empleados.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/padding} 

}

\caption{Distintos tipos de relleno del borde}\label{fig:padding}
\end{figure}

\hypertarget{desplazamiento}{%
\subsection{Desplazamiento}\label{desplazamiento}}

El desplazamiento (en inglés \emph{stride}\index{stride}) básico con el que se aplica un filtro convolucional es de 1. Sin embargo, la aplicación de muchos filtros repartidos en capas a lo largo de la red hace que sea especialmente difícil mantener todos los datos generados en un momento determinado del entrenamiento. Para reducir este volúmen de datos, se suelen aplicar las convoluciones con un desplazamiento mayor que 1. Esto reduce el tamaño del mapa de activaciones obtenido por una determinada capa (Fig. \ref{fig:stride}).

\begin{figure}

{\centering \includegraphics[width=0.3\linewidth]{img/stride} 

}

\caption{Desplazamiento 2x2 del filtro. El punto es el centro de la zona en la que se aplica el filtro en cada momento.}\label{fig:stride}
\end{figure}

\hypertarget{capas-de-agrupaciuxf3n}{%
\section{Capas de agrupación}\label{capas-de-agrupaciuxf3n}}

La ejecución en secuencia de varias capas convolucionales es muy efectiva a la hora de decidir si ciertas características están o no presentes en la entrada. Sin embargo, una de sus ventajas y a la vez limitaciones es que mantiene la localización espacial de las características. Aunque es necesario cierta información espacial como, por ejemplo, saber que hay presentes unos bigotes cerca de una boca sería característico de una imagen que contuviese un gato, pequeños movimientos del contenido de la imagen producirían mapas de características diferentes.

Una forma de mitigar este problema es usar capas de agrupación (en inglés \emph{pooling}\index{pooling}). Estas capas agrupan un número de valores adyacentes de los mapas de características obteniendo un nuevo conjunto de mapas más pequeños. Es posible emplear distintos tipos de operaciones con las que realizar la agrupación. Los más empleados suelen ser el \emph{max pooling} y el \emph{average pooling} que seleccionan el máximo de los valores u obtienen su media respectivamente (Fig. \ref{fig:pooling}). El tamaño más típico es 2x2.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/pooling} 

}

\caption{Resultado de emplear dos métodos de agrupación diferentes para reducir la dimentsión de los datos}\label{fig:pooling}
\end{figure}

\hypertarget{desvanecimiento-del-gradiente}{%
\section{Desvanecimiento del gradiente}\label{desvanecimiento-del-gradiente}}

La primera red convolucional fue propuesta en 1982 \citep{fukushima1982neocognitron}. Esta arquitectura recibió el nombre de Neocognitron y ya constaba de capas convolucionales y capas de \emph{pooling}\index{pooling}. Siguiendo la misma idea, en 1998 se diseñó otra CNN para resolver el problema de reconocimiento de dígitos manuscritos, MNIST \citep{lecun1998gradient}. A esta arquitectura de CNN se la conoce con el nombre de LeNet y es una de las arquitecturas más pequeñas que podemos definir para resolver un problema de clasificación\index{clasificación} (Fig. \ref{fig:lenet}). El extractor de características, consta de dos capas convolucionales alternadas con 2 capas de \emph{pooling}\index{pooling} que obtienen un total de 400 variables. La parte final con el clasificador está compuesta por 3 capas densas de 120, 84 y 10 neuronas.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/lenet} 

}

\caption{Arquitectura LeNet}\label{fig:lenet}
\end{figure}

A pesar de los buenos resultados obtenidos por la arquitectura, el uso de estos métodos para resolver problemas reales estaba aún lejos debido a la carga computacional requerida para su entrenamiento. No fue hasta el año 2012, cuando los ganadores del concurso ImageNet presentaron una nueva arquitectura llamada AlexNet, que las CNN volvieron a estar en el punto de mira de los investigadores \citep{deng2012imagenet}. A partir de ese momento, y teniendo en cuenta los grandes avances computacionales de las tarjetas gráficas que permitían ejecutar operaciones matriciales de forma eficiente, se empezaron a desarrollar cada vez más arquitecturas diferentes.

Durante los primeros años, las arquitecturas desarrolladas tenían cada vez más capas y más filtros en cada una de ellas para extraer la mayor cantidad de información posible de la entrada. Sin embargo, las arquitecturas más profundas se encontraron con un problema: el desvanecimiento del gradiente.

Ciertas funciones de activación como, por ejemplo, la sigmoide, comprimen el espacio de entrada entre 0 y 1. Esto hace que grandes cambios en la entrada produzcan cambios muy pequeños en la salida, haciendo que la derivada sea pequeña. Como los gradientes de la red se calculan durante la propagación hacia atrás capa a capa siguiendo la regla de la cadena, si los valores son muy cercanos a 0, la multiplicación de muchos de estos valores hará que el gradiente de la red caiga rápidamente. Un gradiente muy pequeño hará que los pesos de las capas iniciales apenas se modifiquen con cada iteración y no lleguen a obtener valores adecuados durante el entrenamiento.

Algunas soluciones a este problema son:
- El uso de activaciones tipo ReLU que no obtienen valores muy pequeños en su derivada.
- Capas de normalización. Si se normalizan los datos de entrada ya no habrá grandes cambios entre ellos y los valores estarán lejos de los extremos de la sigmoide.
- Uso de bloques con conexiones residuales que suman el valor de la entrada del bloque a su salida.

\hypertarget{sobreajuste-1}{%
\section{\texorpdfstring{Sobreajuste\index{sobreajuste}}{Sobreajuste}}\label{sobreajuste-1}}

Cuanto mayor es el número de parámetros de la red, mayor probabilidad hay de que ``memorice'' los datos de entrenamiento. Esto se debe a la cantidad de características que la red es capaz de extraer y medir. Si la red es muy profunda, aprenderá cosas muy concretas del conjunto de entrenamiento, lo que dará lugar a modelos que no generalizan bien con nuevos datos \citep{tetko1995neural}.

Además de esto, la no linealidad que añaden las funciones de activación puede hacer que se encuentren fronteras de decisión que modelen datos que no son linealmente separables, pero también facilita que se produzca el sobreajuste (Fig. \ref{fig:overfitting}).

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/overfitting} 

}

\caption{Tipos de ajuste del modelo a los datos}\label{fig:overfitting}
\end{figure}

Para evitar que se produzca el sobreajuste se suelen emplear técnicas de regularización. Se trata de técnicas que impiden que los modelos sean demasiado complejos mejorando su capacidad de generalización. Algunas de estas técnicas son:

\begin{itemize}
\tightlist
\item
  \emph{Dropout}\index{Dropout}. Durante el entrenamiento, algunas activaciones se ponen a 0 de forma aleatoria (entre el 10\% y el 50\%). Esto hace que una capa de la red no dependa siempre de los mismos nodos anteriores.
\item
  \emph{Early Stopping}\index{Early Stopping}. Se trata de parar el entrenamiento antes de que se produzca el sobreajuste y seleccionar ese modelo como final. Para ello se utilizan dos conjuntos: uno de entrenamiento y otro de validación. Cuando las curvas de pérdida de ambos conjuntos comienzan a diverger, se para el entrenamiento y se selecciona el modelo resultante del momento anterior al comienzo de la divergencia (Fig. \ref{fig:early}).
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/early} 

}

\caption{Selección del modelo antes del sobreajuste}\label{fig:early}
\end{figure}

\begin{itemize}
\tightlist
\item
  Regularización L1. Penaliza los pesos grandes por lo que fuerzan a los pesos a tener valores cercanos a 0 (sin ser 0). Añade un término de penalización a la función de coste sumando todos los pesos de la matriz y multiplicado por un valor \(\alpha\) que es otro hiperparámetro que debe ser seleccionado manualmente:
\end{itemize}

\begin{equation} 
\alpha||\boldsymbol W||_1 = \alpha\sum_i\sum_j|w_{ij}| .
\end{equation}

\begin{itemize}
\tightlist
\item
  Regularización L2 o \emph{weight decay}\index{weight decay}. Parecida a la regularización L2 pero con una expresión algo diferente:
\end{itemize}

\begin{equation} 
\frac{\alpha}{2} ||\boldsymbol W||_2^2 = \frac{\alpha}{2}\sum_i\sum_j w^2_{ij} .
\end{equation}

\hypertarget{generaciuxf3n-de-datos-de-entrenamiento-artificiales}{%
\section{Generación de datos de entrenamiento artificiales}\label{generaciuxf3n-de-datos-de-entrenamiento-artificiales}}

Como se ha comentado anteriormente, las técnicas de \emph{deep learning}\index{deep learning} suelen requerir de gran cantidad de datos para su correcto funcionamiento. En muchas situaciones, se dispone de un conjunto limitado para poder entrenar los modelos de forma correcta, por lo que para tratar de suplir la falta de datos se recurre a la generación de datos artificiales, técnica conocida con el nombre de \emph{data augmentation}\index{data augmentation}.

Esta técnica realiza pequeñas variaciones en los datos del conjunto de entrenamiento del que se dispone para obtener nuevos, manteniendo el significado semántico de los mismos. Esto también permite mejorar la generalización de los modelos. Por ejemplo, si se tienen imágenes donde una de ellas contiene un elemento de la clase \emph{gato}, las modificaciones que se realicen deben permitir poder reconocer esa misma clase a partir de las imágenes modificadas.

Algunos ejemplos de técnicas de \emph{data augmentation}\index{data augmentation} en imagen pueden ser: la realización de rotaciones, modificación del contraste o cambios en la iluminación, reescalados, adición/eliminación de ruido o cambio en las proyecciones de las mismas.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/data_augmentation} 

}

\caption{Ejemplos de técnicas de generación de datos artificiales}\label{fig:dataAugmentation}
\end{figure}

Para agregar diferentes tipos de \emph{data augmentation}\index{data augmentation} en \textbf{R}, se puede hacer mediante la inclusión de capas de preprocesado en el modelo secuencial, que serán ejecutadas de manera aleatoria únicamente durante el entrenamiento. En el siguiente ejemplo se realizarán rotaciones aleatorias, volteados horizontales y acercamientos a la imagen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_augmentation }\OtherTok{\textless{}{-}}
  \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{layer\_random\_rotation}\NormalTok{(}\FloatTok{0.1}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{layer\_random\_flip}\NormalTok{(}\StringTok{"horizontal"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{layer\_random\_zoom}\NormalTok{(}\FloatTok{0.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A continuación se muestran los diferentes tipos de preprocesado disponibles para imagen y redes neuronales convolucionales:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{layer\_random\_crop}\NormalTok{()}
\FunctionTok{layer\_random\_flip}\NormalTok{()}
\FunctionTok{layer\_random\_flip}\NormalTok{()}
\FunctionTok{layer\_random\_translation}\NormalTok{()}
\FunctionTok{layer\_random\_rotation}\NormalTok{()}
\FunctionTok{layer\_random\_zoom}\NormalTok{()}
\FunctionTok{layer\_random\_height}\NormalTok{()}
\FunctionTok{layer\_random\_width}\NormalTok{()}
\FunctionTok{layer\_random\_contrast}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{infobox}
\textbf{NOTA}

Otros tipos de \emph{data augmentation}\index{data augmentation} disponibles en \textbf{keras} y \textbf{R} para otro tipo de datos pueden consultarse en

\url{https://tensorflow.rstudio.com/guides/keras/preprocessing_layers}

\end{infobox}

\hypertarget{ejemplo-para-el-conjunto-de-datos-cifar10}{%
\section{\texorpdfstring{Ejemplo para el conjunto de datos \texttt{CIFAR10}}{Ejemplo para el conjunto de datos CIFAR10}}\label{ejemplo-para-el-conjunto-de-datos-cifar10}}

En esta sección se verá cómo entrenar una red neuronal convolucional para ser capaces de clasificar las clases contenidas en el conjunto de datos \href{https://en.wikipedia.org/wiki/CIFAR-10}{CIFAR10}. La descarga debe hacerse a través del siguiente enlace \url{https://drive.google.com/file/d/1-pFGg-bkooss1fNp5UNYR0-hLUmDP_XO/view?usp=sharing} y el conjunto tiene que guardarse en una carpeta \texttt{data} dentro del proyecto de trabajo para asegurar la reproducibilidad del capítulo. Cada una de las imágenes contenidas en el mismo contiene un único elemento que puede ser clasificado como avión, coche, pájaro, gato, ciervo, perro, rana, caballo, barco o camión.

\begin{infobox}
\textbf{NOTA}

Existe otra versión del conjunto de datos denominada como CIFAR100, en la cual se definen un total de 100 posibles categorías en las que las imágenes contenidas pueden ser clasificadas. El ejemplo a continuación puede ser replicado con este mismo conjunto de datos.

\url{https://www.rdocumentation.org/packages/keras/versions/2.9.0/topics/dataset_cifar100}

\end{infobox}

Cada una de las imágenes contenidas en el conjunto tiene un tamaño de 32x32 píxeles en color, representándose mediante los 3 canales RGB, siendo diferente al ejemplo del capítulo \ref{capNN}, en el cual se trabaja con imágenes en escala de grises y, por tanto, un único canal.

A continuación, se verán los pasos seguidos, siendo de forma general muy similares a los ya descritos en el capítulo \ref{capNN}, pero adaptando la red al tipo de dato utilizado.

\hypertarget{visualizacion}{%
\subsection{Carga y visualización de los datos}\label{visualizacion}}

El primer paso será cargar la librería \texttt{keras}, para así poder crear las redes neuronales necesarias y también para cargar el conjunto de imágenes CIFAR10 que se encuentra disponible públicamente:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(keras)}
\FunctionTok{load}\NormalTok{(}\StringTok{"data/cifar10.RData"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A continuación, se puede ver el contenido de las variables generadas, donde se puede observar como el conjunto de datos CIFAR10 ya viene separado en dos subconjuntos que pueden ser utilizados para entrenamiento y para test. Además se puede ver que el conjunto de entrenamiento está compuesto por 50000 imágenes, mientras que el conjunto de test por 10000. En ambos casos, estas imágenes se almacenan en la variable \emph{x}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(cifar)}
\CommentTok{\#\textgreater{} [1] "train" "test"}
\FunctionTok{dim}\NormalTok{(cifar}\SpecialCharTok{$}\NormalTok{train}\SpecialCharTok{$}\NormalTok{x)}
\CommentTok{\#\textgreater{} [1] 50000    32    32     3}
\FunctionTok{dim}\NormalTok{(cifar}\SpecialCharTok{$}\NormalTok{train}\SpecialCharTok{$}\NormalTok{y)}
\CommentTok{\#\textgreater{} [1] 50000     1}
\FunctionTok{dim}\NormalTok{(cifar}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{x)}
\CommentTok{\#\textgreater{} [1] 10000    32    32     3}
\FunctionTok{dim}\NormalTok{(cifar}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{y)}
\CommentTok{\#\textgreater{} [1] 10000     1}
\end{Highlighting}
\end{Shaded}

Además, las imágenes de cada subconjunto tienen definida la clase a la que pertenecen, en este caso, cualquiera de las 10 clases indicadas anteriormente. En ambos casos, esta etiqueta se almacena en la variable \emph{y}. A continuación se muestra un pequeño ejemplo que permitirá mostrar alguna de las imágenes contenidas en el conjunto de datos de entrenamiento junto con su etiqueta:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{class\_names }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}
  \StringTok{"avion"}\NormalTok{, }\StringTok{"coche"}\NormalTok{, }\StringTok{"pajaro"}\NormalTok{, }\StringTok{"gato"}\NormalTok{, }\StringTok{"ciervo"}\NormalTok{,}
  \StringTok{"perro"}\NormalTok{, }\StringTok{"rana"}\NormalTok{, }\StringTok{"caballo"}\NormalTok{, }\StringTok{"barco"}\NormalTok{, }\StringTok{"camion"}
\NormalTok{)}

\NormalTok{index }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{30}

\FunctionTok{par}\NormalTok{(}\AttributeTok{mfcol =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{), }\AttributeTok{mar =} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{), }\AttributeTok{oma =} \FunctionTok{rep}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\NormalTok{cifar}\SpecialCharTok{$}\NormalTok{train}\SpecialCharTok{$}\NormalTok{x[index, , , ] }\SpecialCharTok{|\textgreater{}}
\NormalTok{  purrr}\SpecialCharTok{::}\FunctionTok{array\_tree}\NormalTok{(}\DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  purrr}\SpecialCharTok{::}\FunctionTok{set\_names}\NormalTok{(class\_names[cifar}\SpecialCharTok{$}\NormalTok{train}\SpecialCharTok{$}\NormalTok{y[index] }\SpecialCharTok{+} \DecValTok{1}\NormalTok{]) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  purrr}\SpecialCharTok{::}\FunctionTok{map}\NormalTok{(as.raster, }\AttributeTok{max =} \DecValTok{255}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  purrr}\SpecialCharTok{::}\FunctionTok{iwalk}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ \{}
    \FunctionTok{plot}\NormalTok{(.x)}
    \FunctionTok{title}\NormalTok{(.y)}
\NormalTok{  \})}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/unnamed-chunk-180-1} \end{center}

\hypertarget{preprocesamiento-2}{%
\subsection{\texorpdfstring{Preprocesamiento\index{preprocesamiento}}{Preprocesamiento}}\label{preprocesamiento-2}}

Una vez cargados los datos y comprobado su contenido, al igual que se explicó en el capítulo \ref{capNN}, es posible realizar algún tipo de preprocesado. Al estar trabajando con imágenes, es muy típico estandarizar los valores de color de las imágenes para mitigar las diferencias producidas por las diferentes condiciones de iluminación.

En este caso, al igual que en el capítulo \ref{capNN}, se va a transformar los valores originales de la imagen (en rango de 0 a 255) a valores entre 0 y 1 dividiendo cada valor por el máximo, 255:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cifar}\SpecialCharTok{$}\NormalTok{train}\SpecialCharTok{$}\NormalTok{x }\OtherTok{\textless{}{-}}\NormalTok{ cifar}\SpecialCharTok{$}\NormalTok{train}\SpecialCharTok{$}\NormalTok{x }\SpecialCharTok{/} \DecValTok{255}
\NormalTok{cifar}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{x }\OtherTok{\textless{}{-}}\NormalTok{ cifar}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{x }\SpecialCharTok{/} \DecValTok{255}
\end{Highlighting}
\end{Shaded}

\hypertarget{generaciuxf3n-de-la-red-neuronal-1}{%
\subsection{Generación de la red neuronal}\label{generaciuxf3n-de-la-red-neuronal-1}}

En esta ocasión se creará la red neuronal convolucional en dos pasos, para además mostrar cómo se pueden utilizar las funciones proporcionadas por la librería \textbf{keras} para definir una CNN en varias partes, combinándolas poco a poco.

En el primero, se definirá, utilizando la interfaz \emph{sequential} proporcionada por Tensorflow/Keras a través de la función \emph{keras\_model\_sequential}, la base convolucional de la red combinando varias capas \emph{Conv2d} con \emph{MaxPooling2D}. Esta es la parte de la red que se encargará de aprender las características necesarias que permitirán representar el contenido de la imagen. Otra de las diferencias principales que se puede observar en esta red es que, al aceptar imágenes de 3 canales, RGB, en vez de imágenes en escala de grises, el tamaño de la entrada de la primera capa tiene que reflejar esto \emph{input\_shape = c(32,32,3)}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{layer\_conv\_2d}\NormalTok{(}
    \AttributeTok{filters =} \DecValTok{32}\NormalTok{, }\AttributeTok{kernel\_size =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{,}
    \AttributeTok{input\_shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{32}\NormalTok{, }\DecValTok{32}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{layer\_max\_pooling\_2d}\NormalTok{(}\AttributeTok{pool\_size =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{layer\_conv\_2d}\NormalTok{(}\AttributeTok{filters =} \DecValTok{64}\NormalTok{, }\AttributeTok{kernel\_size =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{layer\_max\_pooling\_2d}\NormalTok{(}\AttributeTok{pool\_size =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{layer\_conv\_2d}\NormalTok{(}\AttributeTok{filters =} \DecValTok{64}\NormalTok{, }\AttributeTok{kernel\_size =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Como se puede observar, en esta parte de la red se reduce la dimensión de la información de manera paulatina en cada capa, obteniendo las características representativas del objeto contenido en cada imagen hasta llegar a un tamaño de \(4\times 4\times 64\).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(model, }\AttributeTok{line\_length =} \DecValTok{64}\NormalTok{)}
\CommentTok{\#\textgreater{} Model: "sequential\_2"}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{}  Layer (type)               Output Shape              Param \#   }
\CommentTok{\#\textgreater{} ================================================================}
\CommentTok{\#\textgreater{}  conv2d\_2 (Conv2D)          (None, 30, 30, 32)        896       }
\CommentTok{\#\textgreater{}  max\_pooling2d\_1 (MaxPoolin  (None, 15, 15, 32)       0         }
\CommentTok{\#\textgreater{}  g2D)                                                           }
\CommentTok{\#\textgreater{}  conv2d\_1 (Conv2D)          (None, 13, 13, 64)        18496     }
\CommentTok{\#\textgreater{}  max\_pooling2d (MaxPooling2  (None, 6, 6, 64)         0         }
\CommentTok{\#\textgreater{}  D)                                                             }
\CommentTok{\#\textgreater{}  conv2d (Conv2D)            (None, 4, 4, 64)          36928     }
\CommentTok{\#\textgreater{} ================================================================}
\CommentTok{\#\textgreater{} Total params: 56,320}
\CommentTok{\#\textgreater{} Trainable params: 56,320}
\CommentTok{\#\textgreater{} Non{-}trainable params: 0}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\end{Highlighting}
\end{Shaded}

Ahora, será necesario añadir capas que permitan transformar los resultados de la parte convolucional de la red implementada a un valor de probabilidad de que la imagen represente cada una de las posibles clases de la imagen.

Para ello, primero se inserta una capa de tipo \emph{flatten} que se encarga de transformar la salida de la última capa convolucional \emph{4x4x64} a un vector de 1024 elementos. A continuación, una capa oculta \emph{dense} de 64 neuronas con activación \emph{relu} se encarga de realizar las primeras operaciones con esos datos y de reducir la dimensionalidad. Finalmente, una última capa \emph{dense} con activación \emph{softmax} se encarga de obtener la probabilidad de que la imagen represente cada una de las 10 posibles clases:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{layer\_flatten}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{64}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{layer\_dense}\NormalTok{(}\AttributeTok{units =} \DecValTok{10}\NormalTok{, }\AttributeTok{activation =} \StringTok{"softmax"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A continuación, se puede observar como quedaría la estructura final del modelo implementado:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(model, }\AttributeTok{line\_length =} \DecValTok{64}\NormalTok{)}
\CommentTok{\#\textgreater{} Model: "sequential\_2"}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\CommentTok{\#\textgreater{}  Layer (type)               Output Shape              Param \#   }
\CommentTok{\#\textgreater{} ================================================================}
\CommentTok{\#\textgreater{}  conv2d\_2 (Conv2D)          (None, 30, 30, 32)        896       }
\CommentTok{\#\textgreater{}  max\_pooling2d\_1 (MaxPoolin  (None, 15, 15, 32)       0         }
\CommentTok{\#\textgreater{}  g2D)                                                           }
\CommentTok{\#\textgreater{}  conv2d\_1 (Conv2D)          (None, 13, 13, 64)        18496     }
\CommentTok{\#\textgreater{}  max\_pooling2d (MaxPooling2  (None, 6, 6, 64)         0         }
\CommentTok{\#\textgreater{}  D)                                                             }
\CommentTok{\#\textgreater{}  conv2d (Conv2D)            (None, 4, 4, 64)          36928     }
\CommentTok{\#\textgreater{}  flatten\_1 (Flatten)        (None, 1024)              0         }
\CommentTok{\#\textgreater{}  dense\_7 (Dense)            (None, 64)                65600     }
\CommentTok{\#\textgreater{}  dense\_6 (Dense)            (None, 10)                650       }
\CommentTok{\#\textgreater{} ================================================================}
\CommentTok{\#\textgreater{} Total params: 122,570}
\CommentTok{\#\textgreater{} Trainable params: 122,570}
\CommentTok{\#\textgreater{} Non{-}trainable params: 0}
\CommentTok{\#\textgreater{} \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
\end{Highlighting}
\end{Shaded}

\begin{infobox}
\textbf{NOTA}

Un detalle a tener en cuenta con respecto al ejemplo del capítulo \ref{capNN} es el parámetro \emph{Total params}. Este valor indica el número de parámetros que contiene nuestra red neuronal y, en cierta manera, el tamaño de la misma. Se puede observar que en este caso tiene un mayor tamaño al contar con un total de 122570 parámetros con respecto a los 11935 del ejemplo previo.

\end{infobox}

Finalmente, es necesario compilar el modelo, indicando algunos de los parámetros de configuración necesarios para el proceso de entrenamiento, como serían el optimizador a utilizar, la función de coste y las métricas a calcular para poder evaluar la red entrenada:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\SpecialCharTok{|\textgreater{}} \FunctionTok{compile}\NormalTok{(}
  \AttributeTok{optimizer =} \StringTok{"sgd"}\NormalTok{, }\CommentTok{\# stochastic gradient descent}
  \AttributeTok{loss =} \StringTok{"sparse\_categorical\_crossentropy"}\NormalTok{, }\CommentTok{\# función utilizada para problemas de clasificación con varias clases}
  \AttributeTok{metrics =} \StringTok{"accuracy"} \CommentTok{\# Precisión}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{entrenamiento-1}{%
\subsection{Entrenamiento}\label{entrenamiento-1}}

Una vez generada la estructura de la red neuronal convolucional, es posible entrenarla para resolver el problema de clasificación\index{clasificación} mediante la función \emph{fit}. Para ello, se le debe indicar el conjunto de imágenes de entrenamiento, \emph{x}, que debe utilizar y sus etiquetas correspondientes, \emph{y}. Además de otros parámetros, se podrá configurar el número de \emph{epochs} a entrenar (pasadas sobre el conjunto completo de entrenamiento), el tamaño del batch que se utilizará en cada iteración con \emph{batch\_size} (número de imágenes por iteración), qué porcentaje de elementos del conjunto de datos se utilizarán para validar el modelo con \emph{validation\_split} (imágenes utilizadas durante el entrenamiento pero solo para obtener una estimación real del error cometido) o la tasa de aprendizaje, \emph{learning\_rate}, entre otros.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{training\_evolution }\OtherTok{\textless{}{-}}\NormalTok{ model }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fit}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ cifar}\SpecialCharTok{$}\NormalTok{train}\SpecialCharTok{$}\NormalTok{x, }\AttributeTok{y =}\NormalTok{ cifar}\SpecialCharTok{$}\NormalTok{train}\SpecialCharTok{$}\NormalTok{y,}
    \AttributeTok{epochs =} \DecValTok{10}\NormalTok{, }\AttributeTok{batch\_size =} \DecValTok{32}\NormalTok{,}
    \AttributeTok{validation\_split =} \FloatTok{0.2}\NormalTok{,}
    \AttributeTok{learning\_rate =} \FloatTok{0.1}\NormalTok{,}
    \AttributeTok{verbose =} \DecValTok{2}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{infobox}
\textbf{NOTA}

Como se puede observar, el \emph{batch\_size} configurado es menor que el del capítulo \ref{capNN} (32 vs 128). Esto es debido a que, el número máximo de imágenes que un mismo equipo utilizado para entrenar podrá procesar en una iteración vendrá determinado por el tamaño de la red neuronal, es decir, por la variable \emph{Total params} indicada en la Nota anterior. Cuanto mayor sea el tamaño de la red, menor será el número máximo de imágenes que podrá tener el batch.

\end{infobox}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Epoch }\DecValTok{1}\SpecialCharTok{/}\DecValTok{10}
\DecValTok{1250}\SpecialCharTok{/}\DecValTok{1250} \SpecialCharTok{{-}}\NormalTok{ 12s }\SpecialCharTok{{-}}\NormalTok{ loss}\SpecialCharTok{:} \FloatTok{2.1097} \SpecialCharTok{{-}}\NormalTok{ accuracy}\SpecialCharTok{:} \FloatTok{0.2316} \SpecialCharTok{{-}}\NormalTok{ val\_loss}\SpecialCharTok{:} \FloatTok{1.9339} \SpecialCharTok{{-}}\NormalTok{ val\_accuracy}\SpecialCharTok{:} \FloatTok{0.2958} \SpecialCharTok{{-}}\NormalTok{ 12s}\SpecialCharTok{/}\NormalTok{epoch }\SpecialCharTok{{-}}\NormalTok{ 9ms}\SpecialCharTok{/}\NormalTok{step}
\NormalTok{Epoch }\DecValTok{2}\SpecialCharTok{/}\DecValTok{10}
\DecValTok{1250}\SpecialCharTok{/}\DecValTok{1250} \SpecialCharTok{{-}}\NormalTok{ 8s }\SpecialCharTok{{-}}\NormalTok{ loss}\SpecialCharTok{:} \FloatTok{1.7478} \SpecialCharTok{{-}}\NormalTok{ accuracy}\SpecialCharTok{:} \FloatTok{0.3667} \SpecialCharTok{{-}}\NormalTok{ val\_loss}\SpecialCharTok{:} \FloatTok{1.6987} \SpecialCharTok{{-}}\NormalTok{ val\_accuracy}\SpecialCharTok{:} \FloatTok{0.3965} \SpecialCharTok{{-}}\NormalTok{ 8s}\SpecialCharTok{/}\NormalTok{epoch }\SpecialCharTok{{-}}\NormalTok{ 6ms}\SpecialCharTok{/}\NormalTok{step}
\NormalTok{Epoch }\DecValTok{3}\SpecialCharTok{/}\DecValTok{10}
\DecValTok{1250}\SpecialCharTok{/}\DecValTok{1250} \SpecialCharTok{{-}}\NormalTok{ 8s }\SpecialCharTok{{-}}\NormalTok{ loss}\SpecialCharTok{:} \FloatTok{1.5464} \SpecialCharTok{{-}}\NormalTok{ accuracy}\SpecialCharTok{:} \FloatTok{0.4399} \SpecialCharTok{{-}}\NormalTok{ val\_loss}\SpecialCharTok{:} \FloatTok{1.4731} \SpecialCharTok{{-}}\NormalTok{ val\_accuracy}\SpecialCharTok{:} \FloatTok{0.4707} \SpecialCharTok{{-}}\NormalTok{ 8s}\SpecialCharTok{/}\NormalTok{epoch }\SpecialCharTok{{-}}\NormalTok{ 7ms}\SpecialCharTok{/}\NormalTok{step}
\NormalTok{Epoch }\DecValTok{4}\SpecialCharTok{/}\DecValTok{10}
\DecValTok{1250}\SpecialCharTok{/}\DecValTok{1250} \SpecialCharTok{{-}}\NormalTok{ 9s }\SpecialCharTok{{-}}\NormalTok{ loss}\SpecialCharTok{:} \FloatTok{1.4304} \SpecialCharTok{{-}}\NormalTok{ accuracy}\SpecialCharTok{:} \FloatTok{0.4866} \SpecialCharTok{{-}}\NormalTok{ val\_loss}\SpecialCharTok{:} \FloatTok{1.3653} \SpecialCharTok{{-}}\NormalTok{ val\_accuracy}\SpecialCharTok{:} \FloatTok{0.5149} \SpecialCharTok{{-}}\NormalTok{ 9s}\SpecialCharTok{/}\NormalTok{epoch }\SpecialCharTok{{-}}\NormalTok{ 7ms}\SpecialCharTok{/}\NormalTok{step}
\NormalTok{Epoch }\DecValTok{5}\SpecialCharTok{/}\DecValTok{10}
\DecValTok{1250}\SpecialCharTok{/}\DecValTok{1250} \SpecialCharTok{{-}}\NormalTok{ 8s }\SpecialCharTok{{-}}\NormalTok{ loss}\SpecialCharTok{:} \FloatTok{1.3477} \SpecialCharTok{{-}}\NormalTok{ accuracy}\SpecialCharTok{:} \FloatTok{0.5199} \SpecialCharTok{{-}}\NormalTok{ val\_loss}\SpecialCharTok{:} \FloatTok{1.3407} \SpecialCharTok{{-}}\NormalTok{ val\_accuracy}\SpecialCharTok{:} \FloatTok{0.5257} \SpecialCharTok{{-}}\NormalTok{ 8s}\SpecialCharTok{/}\NormalTok{epoch }\SpecialCharTok{{-}}\NormalTok{ 6ms}\SpecialCharTok{/}\NormalTok{step}
\NormalTok{Epoch }\DecValTok{6}\SpecialCharTok{/}\DecValTok{10}
\DecValTok{1250}\SpecialCharTok{/}\DecValTok{1250} \SpecialCharTok{{-}}\NormalTok{ 7s }\SpecialCharTok{{-}}\NormalTok{ loss}\SpecialCharTok{:} \FloatTok{1.2784} \SpecialCharTok{{-}}\NormalTok{ accuracy}\SpecialCharTok{:} \FloatTok{0.5437} \SpecialCharTok{{-}}\NormalTok{ val\_loss}\SpecialCharTok{:} \FloatTok{1.2563} \SpecialCharTok{{-}}\NormalTok{ val\_accuracy}\SpecialCharTok{:} \FloatTok{0.5564} \SpecialCharTok{{-}}\NormalTok{ 7s}\SpecialCharTok{/}\NormalTok{epoch }\SpecialCharTok{{-}}\NormalTok{ 6ms}\SpecialCharTok{/}\NormalTok{step}
\NormalTok{Epoch }\DecValTok{7}\SpecialCharTok{/}\DecValTok{10}
\DecValTok{1250}\SpecialCharTok{/}\DecValTok{1250} \SpecialCharTok{{-}}\NormalTok{ 7s }\SpecialCharTok{{-}}\NormalTok{ loss}\SpecialCharTok{:} \FloatTok{1.2118} \SpecialCharTok{{-}}\NormalTok{ accuracy}\SpecialCharTok{:} \FloatTok{0.5705} \SpecialCharTok{{-}}\NormalTok{ val\_loss}\SpecialCharTok{:} \FloatTok{1.2331} \SpecialCharTok{{-}}\NormalTok{ val\_accuracy}\SpecialCharTok{:} \FloatTok{0.5720} \SpecialCharTok{{-}}\NormalTok{ 7s}\SpecialCharTok{/}\NormalTok{epoch }\SpecialCharTok{{-}}\NormalTok{ 6ms}\SpecialCharTok{/}\NormalTok{step}
\NormalTok{Epoch }\DecValTok{8}\SpecialCharTok{/}\DecValTok{10}
\DecValTok{1250}\SpecialCharTok{/}\DecValTok{1250} \SpecialCharTok{{-}}\NormalTok{ 8s }\SpecialCharTok{{-}}\NormalTok{ loss}\SpecialCharTok{:} \FloatTok{1.1539} \SpecialCharTok{{-}}\NormalTok{ accuracy}\SpecialCharTok{:} \FloatTok{0.5954} \SpecialCharTok{{-}}\NormalTok{ val\_loss}\SpecialCharTok{:} \FloatTok{1.1807} \SpecialCharTok{{-}}\NormalTok{ val\_accuracy}\SpecialCharTok{:} \FloatTok{0.5882} \SpecialCharTok{{-}}\NormalTok{ 8s}\SpecialCharTok{/}\NormalTok{epoch }\SpecialCharTok{{-}}\NormalTok{ 6ms}\SpecialCharTok{/}\NormalTok{step}
\NormalTok{Epoch }\DecValTok{9}\SpecialCharTok{/}\DecValTok{10}
\DecValTok{1250}\SpecialCharTok{/}\DecValTok{1250} \SpecialCharTok{{-}}\NormalTok{ 7s }\SpecialCharTok{{-}}\NormalTok{ loss}\SpecialCharTok{:} \FloatTok{1.1015} \SpecialCharTok{{-}}\NormalTok{ accuracy}\SpecialCharTok{:} \FloatTok{0.6135} \SpecialCharTok{{-}}\NormalTok{ val\_loss}\SpecialCharTok{:} \FloatTok{1.1516} \SpecialCharTok{{-}}\NormalTok{ val\_accuracy}\SpecialCharTok{:} \FloatTok{0.5935} \SpecialCharTok{{-}}\NormalTok{ 7s}\SpecialCharTok{/}\NormalTok{epoch }\SpecialCharTok{{-}}\NormalTok{ 6ms}\SpecialCharTok{/}\NormalTok{step}
\NormalTok{Epoch }\DecValTok{10}\SpecialCharTok{/}\DecValTok{10}
\DecValTok{1250}\SpecialCharTok{/}\DecValTok{1250} \SpecialCharTok{{-}}\NormalTok{ 7s }\SpecialCharTok{{-}}\NormalTok{ loss}\SpecialCharTok{:} \FloatTok{1.0526} \SpecialCharTok{{-}}\NormalTok{ accuracy}\SpecialCharTok{:} \FloatTok{0.6286} \SpecialCharTok{{-}}\NormalTok{ val\_loss}\SpecialCharTok{:} \FloatTok{1.1014} \SpecialCharTok{{-}}\NormalTok{ val\_accuracy}\SpecialCharTok{:} \FloatTok{0.6128} \SpecialCharTok{{-}}\NormalTok{ 7s}\SpecialCharTok{/}\NormalTok{epoch }\SpecialCharTok{{-}}\NormalTok{ 6ms}\SpecialCharTok{/}\NormalTok{step}
\end{Highlighting}
\end{Shaded}

Tras el entrenamiento, se puede observar la evolución del mismo mediante las gráficas de coste/perdida y precisión.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(training\_evolution)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/curve3} 

}

\caption{Evolución durante el entrenamiento de la precisión y la pérdida de los conjuntos de entrenamiento y validación}\label{fig:plot-curve3}
\end{figure}

Como se puede observar, la red entrenada es capaz de alcanzar un 60\% de precisión tanto en los conjuntos de entrenamiento como los de validación

\hypertarget{test-2}{%
\subsection{Test}\label{test-2}}

Una vez entrenado el modelo, es posible aplicarlo sobre el conjunto de test proporcionado. Para ello, se puede realizar la predicción sobre cualquiera de las imágenes mediante la función \texttt{predict()}, obteniendo la probabilidad de que pertenezca a una determinada clase:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predictions }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(model, cifar}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{x)}
\FunctionTok{head}\NormalTok{(}\FunctionTok{round}\NormalTok{(predictions, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{), }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
#> [1,] 0.03 0.00 0.18 0.47 0.01 0.14 0.16    0 0.00  0.00
#> [2,] 0.04 0.07 0.00 0.00 0.00 0.00 0.00    0 0.89  0.00
#> [3,] 0.08 0.28 0.00 0.00 0.00 0.00 0.00    0 0.55  0.07
#> [4,] 0.82 0.01 0.04 0.00 0.00 0.00 0.00    0 0.11  0.00
#> [5,] 0.00 0.00 0.05 0.11 0.11 0.03 0.69    0 0.00  0.00
\end{verbatim}

También se puede utilizar la función \texttt{evaluate()} para calcular tanto el coste o pérdida como la precisión de la red neuronal sobre el conjunto de test. Como se puede observar, se obtienen valores muy similares a los obtenidos durante el entrenamiento:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{evaluate}\NormalTok{(model, cifar}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{x, cifar}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{y, }\AttributeTok{verbose =} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>     loss accuracy 
#> 1.094381 0.611100
\end{verbatim}

Con la función \emph{predict} se puede también generar la matriz de confusión de la red para evaluar qué pares de clases está cometiendo un mayor número de errores:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prediction\_matrix }\OtherTok{\textless{}{-}}\NormalTok{ model }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{predict}\NormalTok{(cifar}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{x) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{k\_argmax}\NormalTok{()}
\NormalTok{confusion\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(}\FunctionTok{as.array}\NormalTok{(prediction\_matrix), cifar}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{y)}
\NormalTok{confusion\_matrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#>    
#>       0   1   2   3   4   5   6   7   8   9
#>   0 650  25  57  12  39  12   3  21  82  35
#>   1  43 790  15  18  13  11  16  12  43 179
#>   2 119  20 676 180 325 170 112  94  33  26
#>   3  23  15  57 427  76 184  54  48  18  18
#>   4   1   0  13  20 236  12   5  21   3   2
#>   5   6   7  52 145  41 488  24  82   5  10
#>   6  14   5  71 110 119  42 755  13   4  14
#>   7   7   6  30  46 126  60  16 676   9  18
#>   8 114  58  15  19  20  13   6   5 777  62
#>   9  23  74  14  23   5   8   9  28  26 636
\end{verbatim}

\hypertarget{otros-ejemplos-de-interuxe9s}{%
\subsection{Otros ejemplos de interés}\label{otros-ejemplos-de-interuxe9s}}

\begin{itemize}
\item
  Fine tuning CNN (MobileNet, ejemplo perros y gatos)
  \url{https://tensorflow.rstudio.com/guide/keras/examples/fine_tuning/}
\item
  Regresión básica con redes neuronales (conjunto de datos Boston Housing)
  \url{https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_regression/}
\item
  Transfer learning en \textbf{R}
  \url{https://tensorflow.rstudio.com/tutorials/advanced/images/transfer-learning-hub/}
\end{itemize}

\url{https://tensorflow.rstudio.com/}

\begin{infobox_resume}

\textbf{RESUMEN}

\begin{itemize}
\item
  Este capítulo presenta las características de las redes neuronales convolucionales\index{redes neuronales convolucionales} y sus diferencias con el perceptrón multicapa\index{perceptrón multicapa}.
\item
  Además, se exponen los principales problemas a la hora de diseñar este tipo de redes profundas y sus posibles soluciones.
\item
  Finalmente, se han explicado los pasos necesarios para poder entrenar una red neuronal convolucional utilizando la librería Tensorflow/Keras en \textbf{R}, resolviendo el problema de clasificación\index{clasificación} de las 10 clases del conjunto de datos CIFAR10.
\end{itemize}

\end{infobox_resume}

\hypertarget{part-ciencia-de-datos-de-texto-y-redes}{%
\part{Ciencia de datos de texto y redes}\label{part-ciencia-de-datos-de-texto-y-redes}}

\hypertarget{mineria-textos}{%
\chapter{Minería de textos}\label{mineria-textos}}

\emph{Víctor Casero, Ángela Celis y María Lozano Zahonero\footnote{Universidad de Castilla-La Mancha, Universidad de Castilla-La Mancha y Università di Roma Tor Vergata}}

\hypertarget{introducciuxf3n-21}{%
\section{Introducción}\label{introducciuxf3n-21}}

En la actualidad, entre el 80 \% y el 90 \% de los datos que se generan diariamente son datos no estructurados (vistos en el capítulo \ref{datos-no-sql}). Un ejemplo típico de datos no estructurados son los textos, desde los comentarios o mensajes de las redes sociales, reseñas, blogs y microblogs, chats o whatsapp hasta las noticias periodísticas, los discursos políticos o las obras literarias. En consecuencia, aprender a procesar y analizar datos exige aprender a procesar y analizar textos.

Los textos precisan, sin embargo, un tratamiento especial. A diferencia de la mayoría de los datos que se tratan en este libro, que son datos estructurados, los datos textuales requieren que se les otorgue un orden y estructura para su manejo y análisis con el software \texttt{R}. Además, al utilizar un lenguaje natural --es decir, un idioma como, por ejemplo, el español, el chino o el inglés--, los textos no pueden ser procesados directamente por un ordenador. Es preciso `traducirlos' antes a un lenguaje formal que los ordenadores puedan entender.

La \textbf{minería de textos} \index{minería de textos}(en inglés, \emph{text mining}), también conocida como \textbf{análisis de textos} \index{análisis de textos}(en inglés, \emph{text analysis}), puede definirse como el proceso para detectar, extraer, clasificar, analizar y visualizar la información no explícita que contienen los textos, transformando los datos textuales en datos estructurados y el lenguaje natural en lenguaje formal a fin de determinar, después, de manera automática patrones recurrentes y desviaciones en los mismos. La minería de textos utiliza muchas técnicas y métodos diferentes, la mayor parte de los cuales proceden del \textbf{procesamiento del lenguaje natural}\index{PLN-procesamiento del lenguaje natural} (PLN), un ámbito de la inteligencia artificial que se ocupa de la comunicación entre los seres humanos y las máquinas mediante el tratamiento computacional del lenguaje humano.

Este capítulo constituye una primera aproximación a la minería de textos con \texttt{R}. Su objetivo es proporcionar un marco teórico y aplicado básico de este ámbito. Para ello, en el segundo apartado, se presentan los conceptos y fases fundamentales de la minería de textos, mencionando algunos paquetes de \texttt{R} que permiten realizar análisis textuales de distintos tipos. El tercer apartado está dedicado al análisis de sentimientos, que constituye uno de los campos de la minería de textos de mayor desarrollo en la actualidad. Cierra el capítulo un caso práctico, en el que se aplica y se amplía lo estudiado anteriormente. Unas útiles referencias sobre el tema son \citet{fradejas} y \citet{jockers2014}.

\hypertarget{secCONCEPTOS}{%
\section{Conceptos y tareas fundamentales}\label{secCONCEPTOS}}

Lo primero que se necesita para hacer un análisis de textos son los textos. Esta afirmación podría parecer banal, pero no lo es. El volumen de textos en circulación es ingente, pero, en la mayor parte de los casos, es necesario realizar una serie de operaciones complejas para poder extraer y recopilar los datos textuales que se quiere analizar. Es también difícil muchas veces acceder después a estos datos ya que los textos pueden presentar formatos muy heterogéneos, no siempre interpretables o fáciles de convertir en un formato interpretable. Baste pensar, por ejemplo, en una nota escrita a mano. Dado que este capítulo es una primera aproximación a la minería de textos, se parte del supuesto de que el texto o textos están disponibles ya en un fichero, denominado \textbf{corpus}\index{corpus}, legible por \texttt{R}. En este contexto, \emph{corpus} es la colección de textos con el mismo origen, por ejemplo, el \emph{corpus} de las obras de un autor, que para poder manejarse requieren metadatos con detalles adicionales.

\hypertarget{preparaciuxf3n-de-los-datos}{%
\subsection{Preparación de los datos}\label{preparaciuxf3n-de-los-datos}}

Una vez constituido el corpus, la primera fase es la \textbf{preparación de los datos}. Los textos suelen contener un cierto grado de `suciedad', es decir, elementos que alteran o impiden el análisis. De una buena `limpieza' inicial, dependerá en gran parte la validez de los resultados que se obtengan. Entre las operaciones de `limpieza' generales figuran una serie de transformaciones cuya finalidad es evitar el recuento incorrecto de palabras como el cambio de mayúsculas por minúsculas y la eliminación de los signos de puntuación, los números y los espacios en blanco en exceso.

La siguiente operación de preparación, que tiene un importante peso en el análisis, es la eliminación de las \textbf{stopwords}\index{stopwords} o \textbf{palabras vacías}\index{palabras vacías}. En la lengua no todas las palabras tienen el mismo tipo de significado. Las palabras con significado léxico, como \emph{mesa} o \emph{corpus}, son palabras a las que corresponde un concepto que se puede definir o explicar. Otras palabras, sin embargo, son palabras funcionales, cuyo contenido es puramente gramatical. Son palabras como el artículo \emph{el}, la preposición \emph{de} o la conjunción \emph{o}: se puede explicar cómo se usan, pero no definirlas asociándolas a un concepto porque carecen de contenido léxico-semántico.

Las palabras vacías son, con gran diferencia respecto de las palabras léxicas, las más frecuentes de la lengua, pero, dado su escaso o nulo significado léxico, en los análisis de tipo semántico, como el análisis de sentimientos o el modelado de temas, carecen de valor informativo, por lo que es conveniente eliminarlas. No es aconsejable eliminarlas, sin embargo, en otros tipos de análisis, como los análisis estilométricos, donde tienen un importante valor informativo como se verá en el apartado \ref{secESTILOM}. Las palabras vacías pertenecen a clases cerradas, es decir a clases de palabras con un número de elementos limitado, finito. Es posible confeccionar, por tanto, listas de palabras vacías para permitir su eliminación. En el caso aplicado, se aprenderá a usar estas listas y se podrá apreciar con detalle la diferente información que proporciona una tabla de frecuencias con y sin palabras vacías.

\hypertarget{secTOKEN}{%
\subsection{Segmentación del texto: tokenización}\label{secTOKEN}}

La segunda fase de la minería de textos consiste en la \textbf{segmentación del texto}, denominada también \textbf{tokenización}\index{tokenización}. El texto se divide en \textbf{token}\index{token}, secuencias de texto con valor informativo. De esta manera, se pasa del lenguaje natural a un lenguaje formal comprensible por el software, dándole formato de `vector' o `tabla'. Así se pueden aplicar algunas de las herramientas que se utilizan con datos numéricos para manejar el texto y obtener resúmenes y visualizaciones que muestren la información no explícita contenida en él en forma de patrones recurrentes.

Generalmente, los \emph{token} son \textbf{palabras}, es decir, secuencias de caracteres entre dos espacios en blanco y/o signos de puntuación, pero pueden ser también \textbf{oraciones}, \textbf{líneas}, \textbf{párrafos} o \textbf{n-gramas}. Como se verá en el caso aplicado, un primer análisis del significado consiste en eliminar las palabras vacías y obtener las frecuencias\footnote{Frecuencias relativas si se comparan distintos textos. También será útil para ello obtener la tasa de riqueza léxica (TIR).} de las palabras con valor informativo para responder a la pregunta `¿Qué se dice?' \citep{silge2017}.

\textbf{\emph{N-gramas}}\\
El análisis puede proseguir estudiando la frecuencia de los \textbf{n-gramas}\index{n-gramas}, secuencias de \emph{n} palabras consecutivas en el mismo orden. Se tienen así bigramas o 2-gramas (secuencias de dos palabras), trigramas o 3-trigramas (secuencias de tres palabras), etc. El estudio de los \emph{n-gramas} responde al principio de Firth: `You shall know a word by the company it keeps' \citep[11]{firth}. Este principio es el fundamento del llamado \textbf{análisis de colocaciones}\index{análisis de colocaciones}: para conocer el significado de una palabra es preciso conocer las palabras con las que aparece, el contexto relevante. En un sentido amplio, el análisis de colocaciones consiste en examinar los contextos izquierdo y/o derecho de una palabra. La segmentación en \emph{n-gramas} permite tener en cuenta este contexto relevante que indicará, por ejemplo, que \emph{banco} es con toda probabilidad un asiento en las secuencias \emph{banco de madera} o \emph{banco en la terraza}, pero no lo es en secuencias como \emph{banco de peces}, \emph{banco de arena}, \emph{banco de inversiones}, \emph{banco de datos} o \emph{banco de pruebas}. La división en \emph{n-gramas} permitirá también considerar en el análisis, al menos hasta cierto punto, el peso de la ambigüedad, la negación o el distinto significado que pueden tener las palabras según el ámbito temático. Por ejemplo, la forma \emph{larga} no tiene el mismo significado en los bigramas \emph{falda larga}, \emph{mano larga} y \emph{cara larga}, ni tiene tampoco el mismo valor informativo en \emph{es larga} / \emph{no es larga} o en \emph{de larga experiencia} (valor positivo) y en \emph{se me hizo larga} (valor negativo). En el caso aplicado, se verá en la práctica la segmentación en \emph{n-gramas} y cómo la visualización de redes contribuye a complementar el análisis.

\textbf{\emph{Stemming} y lematización}\\
La tokenización se puede refinar mediante el \emph{stemming}\index{stemming}, o reducción de las palabras `flexionadas' a su raíz, y la lematización\index{lematización}, o extracción del lema de cada palabra. Un ejemplo de \emph{stemming} sería reducir las palabras \emph{texto}, \emph{textos}, \emph{textual} y \emph{textuales}, que \texttt{R} cuenta como cuatro palabras diferentes, a la raíz `text'. El \emph{stemming} puede proporcionar un recuento más preciso en algunos casos, pero en otros, al eliminar los sufijos de las palabras, puede crear confusión. Además, como en el ejemplo anterior, las raíces pueden no coincidir con palabras existentes, lo que hace que sean difíciles de interpretar y resulten extrañas si se visualizan en nubes de palabras. Con la lematización se reducen las formas flexionadas de una misma palabra al lema, que es la forma que encabeza la entrada de la palabra en el diccionario. Por ejemplo, si se quiere buscar el significado de la palabra \emph{niñas} no se encontrará como tal sino bajo el lema \emph{niño} y si se quiere buscar \emph{iremos} se tendrá que buscar el lema \emph{ir}. En el caso anterior, la lematización reduciría las formas \emph{texto}, \emph{textos}, \emph{textual} y \emph{textuales} a dos lemas: \emph{texto} y \emph{textual}. La lematización evita la dispersión de significado en varias formas, pero a veces es compleja y puede conducir a la pérdida de información pertinente.

\hypertarget{secESTILOM}{%
\subsection{Campos de aplicación de la minería de textos}\label{secESTILOM}}

La minería de textos tiene varios campos de aplicación. Entre ellos destacan tres: el \textbf{análisis de sentimientos}\index{análisis de sentimientos}, el \textbf{modelado de temas}\index{modelado de temas} o \textbf{tópicos} y el \textbf{análisis estilométrico} o \textbf{estilometría}. El \emph{análisis de sentimientos} se tratará con detalle en la sección \ref{secSENTIM} y en el caso de aplicación (sección \ref{secSENTYEMO}), mientras que el modelado de temas o tópicos, se ilustrará en el capítulo \ref{nlp-textil}. Dadas las limitaciones del manual sólo se presentan brevemente el modelado de temas y la estilometría.

El \textbf{modelado de temas} (en inglés, \emph{topic modelling}), como su propio nombre indica, tiene por objeto identificar los temas principales sobre los que versa el texto haciendo uso de técnicas de clasificación no supervisada del campo del aprendizaje automático, como por ejemplo LDA (\emph{Latent Dirichlet Allocation}).

La \textbf{estilometría}\index{estilometría} es una aplicación de la minería de textos cuya finalidad consiste en determinar las relaciones existentes entre el estilo de los textos y los metadatos incluidos en ellos. Se utiliza principalmente en la atribución de autoría. El concepto base es el de \emph{huella lingüística}\index{huella lingüistica}, constituida por el conjunto de rasgos lingüísticos que caracterizan el estilo de un autor como un estilo individual y único y permiten identificarlo. Un punto clave es que, contrariamente a lo que podría pensarse, los rasgos que conforman en mayor medida la huella lingüística son los que tienen un mayor índice de frecuencia. La mayor parte de los enfoques utilizan el vector de las `palabras más frecuentes' (MFW, por su sigla en inglés), que son, como se ha visto antes, las palabras vacías y no las palabras con significado léxico, para determinar el estilo de un autor. Esto es debido fundamentalmente a que las palabras vacías se usan de manera involuntaria e inconsciente, configurando de esta manera, sin ningún tipo de filtros racionales, una clave estilística idiosincrásica \citep{zahonero2020}. De lo anterior se deduce fácilmente que en este tipo de análisis no deben eliminarse las palabras vacías.

En la actualidad, el análisis estilométrico se usa en ámbitos muy dispares: desde la criminología o los servicios de inteligencia para identificar a los autores de mensajes o notas en casos de asesinatos, terrorismo, secuestro o acoso, por ejemplo, hasta el derecho civil o la literatura en cuestiones de derechos de autor o detección de plagio, entre muchas otras cuestiones.

\hypertarget{mineruxeda-de-textos-en-r}{%
\subsection{\texorpdfstring{Minería de textos en \texttt{R}}{Minería de textos en R}}\label{mineruxeda-de-textos-en-r}}

En \texttt{R} existen diversos paquetes y funciones que facilitan la minería de textos, entre los que destacan:

\begin{itemize}
\item
  \texttt{tidytext}: con la filosofía del \texttt{tidyverse} puede combinarse con los conocidos paquetes \texttt{dplyr}, \texttt{broom}, \texttt{ggplot2}, etc. Se puede destacar la función \texttt{unnest\_tokens()} que automatiza el proceso de tokenización y el almacenamiento en formato tidy en un único paso.
\item
  \texttt{tm}: destaca por tener soporte back-end de base de datos integrado, gestión avanzada de metadatos y soporte nativo para leer en varios formatos de archivo.
\item
  \texttt{tokenizers}: incluye tokenizadores de palabras, oraciones, párrafos, n-gramas, \emph{tweets}, expresiones regulares, así como funciones para contar caracteres, palabras y oraciones, y para dividir textos más largos en documentos separados, cada uno con el mismo número de palabras.
\item
  \texttt{wordcloud}: permite visualizar \emph{nubes de palabras}\index{nubes de palabras}. Las palabras más frecuentes aparecen en mayor tamaño permitiendo de un vistazo obtener las palabras clave del texto.
\item
  \texttt{quanteda}: maneja matrices de documentos-términos\index{matriz de documentos} y destaca en tareas cuantitativas como recuento de palabras o sílabas.
\item
  \texttt{syuzhet}: incluye distintas funciones que facilitan el análisis de textos, en particular el análisis de sentimientos\index{análisis de sentimientos} de textos literarios.
\item
  \texttt{gutenbergr}: almacena las obras del proyecto Gutenberg\footnote{Proyecto desarrollado por Michael Hart en 1971 para crear una biblioteca de libros electrónicos gratuitos, y accesibles en internet, a partir de libros en soporte físico, generalmente de dominio público. Cuenta con más de 50 000 libros.}; muy útil si se quieren analizar textos literarios.
\end{itemize}

\hypertarget{secSENTIM}{%
\section{Análisis de sentimientos}\label{secSENTIM}}

El análisis de sentimientos\index{análisis de sentimientos} (en inglés, \emph{sentiment analysis}) es una aplicación de la minería de textos que tiene como finalidad la detección, extracción, clasificación, análisis y visualización de la dimensión subjetiva asociada a los temas o tópicos presentes en los textos. La dimensión subjetiva comprende no solo los sentimientos, sino también las emociones, sensaciones y estados afectivos y anímicos, así como las opiniones, creencias, percepciones, puntos de vista, actitudes, juicios y valoraciones. De ahí que reciba también el nombre de \emph{minería de opinión}\index{minería de opinión} (en inglés, \emph{opinion mining}) \citep{zahonero2020}.

El análisis de sentimientos asigna a esta dimensión subjetiva una polaridad, que puede ser positiva o negativa \citep{pang2008opinion}. Algunas técnicas añaden además una polaridad neutra. En algunos casos, el análisis de sentimientos se refina hasta llegar a las emociones básicas: este subcampo del análisis de sentimientos se conoce como \emph{detección de emociones}\index{detección de emociones}.

La primera aplicación del análisis de sentimientos fue la investigación de mercados. A partir del año 2000, se registra un crecimiento exponencial de textos como reseñas, chats, foros, blogs, microblogs o comentarios y mensajes de las redes sociales, en los que predomina la expresión de emociones y opiniones personales. Mediante el análisis de sentimientos se extrae de ellos información que permite conocer los gustos del consumidor y diseñar productos a su medida. Esta idea se extenderá después a otros ámbitos, en especial a aquellos en los que predomina la comunicación persuasiva como las campañas publicitarias o políticas. Recientemente, ha empezado a utilizarse también con fines predictivos y preventivos en muchas esferas: desde cuáles son los políticos, las empresas, las películas, canciones u obras literarias que obtendrán un mayor rendimiento, mejores resultados o más votos o ventas hasta cómo detectar y prevenir, por ejemplo, conductas suicidas mediante el análisis de mensajes en las redes sociales.

En el análisis de sentimientos y la detección de emociones existen dos enfoques principales: el enfoque basado en el aprendizaje automático (\emph{machine learning}), en el que se usan algoritmos de aprendizaje supervisado, y el enfoque semántico basado en diccionarios o lexicones\index{lexicón}. Este último enfoque es el que se verá en detalle en el caso aplicado.

En \texttt{R} están implementados varios lexicones para el análisis de sentimientos. Dos de los más utilizados son \texttt{bing}, de Bing Liu y colaboradores \citep{liu2015sentiment}, y \texttt{NRC}, de Saif Mohammad y Peter Turney, ambos incluidos tanto en el paquete \texttt{tidytext} como en \texttt{syuzhet} \citep{jockers2017}. Estos lexicones tienen en común que están basados en unigramas, es decir, en palabras sueltas, y que tienen como idioma original el inglés, si bien hay disponibles versiones traducidas automáticamente a distintas lenguas. La diferencia principal entre los dos lexicones es que \texttt{bing} clasifica las palabras de forma binaria en polaridad positiva/negativa, mientras que \texttt{NRC} además de la polaridad positiva/negativa permite detectar también ocho emociones básicas (\emph{ira, miedo, anticipación, confianza, sorpresa, tristeza, alegría, asco}). En el caso aplicado se compararán ambos diccionarios. Como se verá, los resultados del análisis dependerán en buena medida del lexicón elegido, así como del idioma del texto y de si el lexicón se elaboró originalmente en ese idioma o es una versión traducida automáticamente de otra lengua.

\hypertarget{caso-de-aplicaciuxf3n}{%
\section{Caso de aplicación}\label{caso-de-aplicaciuxf3n}}

\hypertarget{declaraciuxf3n-institucional-del-estado-de-alarma-2020}{%
\subsection{Declaración institucional del estado de alarma 2020}\label{declaraciuxf3n-institucional-del-estado-de-alarma-2020}}

La `Declaración institucional del presidente del gobierno sobre el estado de alarma en la crisis del coronavirus' (en adelante, `la Declaración'), dada en La Moncloa el 13 de marzo de 2020 es el objeto de análisis. Ésta se puede encontrar en el paquete \texttt{CDR} que acompaña este libro.
Se le van a aplicar las operaciones y técnicas mencionadas en la sección \ref{secCONCEPTOS}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"CDR"}\NormalTok{)}
\FunctionTok{data}\NormalTok{(}\StringTok{"declaracion"}\NormalTok{)}
\CommentTok{\# load("data/declaracion{-}estado{-}alarma.Rdata")}
\end{Highlighting}
\end{Shaded}

\hypertarget{segmentaciuxf3n-en-palabras-y-oraciones}{%
\subsection{Segmentación en palabras y oraciones}\label{segmentaciuxf3n-en-palabras-y-oraciones}}

Las primeras tareas del análisis son la preparación, limpieza y segmentación o tokenización de los textos. A continuación, se verá una segmentación en palabras individuales. La función \texttt{tokenize\_words()} del paquete \texttt{tokenizers} prepara el texto convirtiéndolo a minúsculas, elimina todos los signos de puntuación y finalmente segmenta el texto en palabras.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tokenizers)}
\NormalTok{palabras }\OtherTok{\textless{}{-}} \FunctionTok{tokenize\_words}\NormalTok{(declaracion)}
\NormalTok{tokenizers}\SpecialCharTok{::}\FunctionTok{count\_words}\NormalTok{(declaracion)}
\CommentTok{\#\textgreater{} [1] 922}
\end{Highlighting}
\end{Shaded}

Con la última sentencia se obtiene la longitud de la Declaración, el número de palabras utilizadas: 922.

La frecuencia de cada palabra se pueden obtener y presentar con el código de abajo.
La primera sentencia crea la tabla de frecuencias,
la tercera la transforma en el tipo \texttt{tibble}, creando la columna recuento,
y ordena la tabla de forma descendente, de mayor a menor frecuencia.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tabla }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(palabras[[}\DecValTok{1}\NormalTok{]])}
\NormalTok{(tabla }\OtherTok{\textless{}{-}}\NormalTok{ tabla }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{palabra =} \FunctionTok{names}\NormalTok{(tabla), }\AttributeTok{recuento =} \FunctionTok{as.numeric}\NormalTok{(tabla)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(recuento)))}
\CommentTok{\#\textgreater{} \# A tibble: 390 x 3}
\CommentTok{\#\textgreater{}    tabla       palabra recuento}
\CommentTok{\#\textgreater{}    \textless{}table[1d]\textgreater{} \textless{}chr\textgreater{}      \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 43          de            43}
\CommentTok{\#\textgreater{}  2 41          y             41}
\CommentTok{\#\textgreater{}  3 35          la            35}
\CommentTok{\#\textgreater{}  4 31          a             31}
\CommentTok{\#\textgreater{}  5 26          los           26}
\CommentTok{\#\textgreater{}  6 22          en            22}
\CommentTok{\#\textgreater{}  7 20          que           20}
\CommentTok{\#\textgreater{}  8 17          el            17}
\CommentTok{\#\textgreater{}  9 14          al            14}
\CommentTok{\#\textgreater{} 10 14          para          14}
\CommentTok{\#\textgreater{} \# ... with 380 more rows}
\end{Highlighting}
\end{Shaded}

En la primera fila de la salida se indican las dimensiones de la \texttt{tibble}, por lo que se puede ver que en esta Declaración hay 390 `palabras' distintas (considera los números como palabras).

El resultado son las palabras más utilizadas en el texto, que, como puede apreciarse, son palabras vacías. Esto no debería sorprender porque, como ya se ha visto, estas palabras son las más frecuentes. En el siguiente apartado, se verá cómo eliminarlas para obtener datos con valor informativo.

Para otras formas de segmentar el texto (oraciones, párrafos, \emph{tweets}, etc.): véase \texttt{?tokenize\_words}. Por ejemplo, para segmentar en oraciones:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{oraciones }\OtherTok{\textless{}{-}} \FunctionTok{tokenize\_sentences}\NormalTok{(declaracion)}
\FunctionTok{count\_sentences}\NormalTok{(declaracion)}
\CommentTok{\#\textgreater{} [1] 44}
\end{Highlighting}
\end{Shaded}

Las tres primeras oraciones y la última se obtienen con el siguiente código.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{oraciones[[}\DecValTok{1}\NormalTok{]][}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\CommentTok{\# primeras 3 oraciones}
\CommentTok{\#\textgreater{} [1] "Buenas tardes."                                                                                                                                                                                                                  }
\CommentTok{\#\textgreater{} [2] "Estimados compatriotas."                                                                                                                                                                                                         }
\CommentTok{\#\textgreater{} [3] "En el día de hoy, acabo de comunicar al Jefe del Estado la celebración, mañana, de un Consejo de Ministros extraordinario, para decretar el Estado de Alarma en todo nuestro país, en toda España, durante los próximos 15 días."}
\NormalTok{oraciones[[}\DecValTok{1}\NormalTok{]][}\FunctionTok{count\_sentences}\NormalTok{(declaracion)] }\CommentTok{\# última}
\CommentTok{\#\textgreater{} [1] "Buenas tardes."}
\end{Highlighting}
\end{Shaded}

También podría medirse la longitud de cada oración, en número de palabras, típicamente para comparaciones con otros textos. Para ello se debe separar cada oración en palabras y con la función \texttt{sapply} se puede obtener la longitud de cada oración, que puede verse en la Figura \ref{fig:longitudoraciones}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{palabras\_oracion }\OtherTok{\textless{}{-}} \FunctionTok{tokenize\_words}\NormalTok{(oraciones[[}\DecValTok{1}\NormalTok{]])}
\NormalTok{longitud\_o }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(palabras\_oracion, length)}
\FunctionTok{head}\NormalTok{(longitud\_o)}
\CommentTok{\#\textgreater{} [1]  2  2 39 33 33 32}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/longitudoraciones-1} 

}

\caption{Número de palabras en cada oración de la Declaración}\label{fig:longitudoraciones}
\end{figure}

\hypertarget{anuxe1lisis-exploratorio}{%
\subsection{Análisis exploratorio}\label{anuxe1lisis-exploratorio}}

\textbf{Eliminación de palabras vacías}\\
Se va a hacer uso del paquete \texttt{stopwords} que contiene listas de \emph{palabras vacías}\index{palabras vacías} en diferentes idiomas. Para el ejemplo se define una tabla con la misma estructura que la tabla de la Declaración con las 308 palabras vacías españolas que tiene el paquete:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(stopwords)}
\NormalTok{tabla\_stopwords }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{palabra =} \FunctionTok{stopwords}\NormalTok{(}\StringTok{"es"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

La siguiente sentencia `limpia' la tabla de la Declaración quitando las palabras vacías españolas. Además, se hace uso de la función \texttt{kable} para una visualización más sofisticada de la tabla (con la longitud que se desee):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tabla }\OtherTok{\textless{}{-}}\NormalTok{ tabla }\SpecialCharTok{|\textgreater{}} \FunctionTok{anti\_join}\NormalTok{(tabla\_stopwords)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(tabla[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, ],}
  \AttributeTok{caption =} \StringTok{"Palabras más frecuentes (sin palabras vacías)"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:tabla1}Palabras más frecuentes (sin palabras vacías)}
\centering
\begin{tabular}[t]{r|l|r}
\hline
tabla & palabra & recuento\\
\hline
9 & virus & 9\\
\hline
7 & recursos & 7\\
\hline
5 & social & 5\\
\hline
4 & alarma & 4\\
\hline
4 & conjunto & 4\\
\hline
4 & emergencia & 4\\
\hline
4 & españa & 4\\
\hline
4 & semanas & 4\\
\hline
4 & va & 4\\
\hline
3 & cada & 3\\
\hline
\end{tabular}
\end{table}

El resultado, Tabla \ref{tab:tabla1}, se puede considerar el primer análisis léxico con valor informativo: la palabra más frecuente es \emph{virus}, seguida de \emph{recursos} y \emph{social}. Se podría ver que en total hay 319 palabras distintas.

El método de eliminar palabras con el paquete \texttt{stopwords} no es perfecto. Por ejemplo, \emph{va} y \emph{cada} (posiciones 9 y 10 de la tabla) no son muy informativas. En estos casos, como se ha visto antes, se pueden utilizar listas de palabras vacías de otros paquetes como, por ejemplo \texttt{tidytext} o \texttt{tokenizers} o el listado en español propuesto por \citet{fradejas}, o pueden confeccionarse listas \emph{ad hoc}.

\textbf{Nubes de palabras}\\
Una manera habitual de mostrar la información de forma visual es con las denominadas \emph{nubes de palabras}\index{nubes de palabras}, acudiendo a la función \texttt{wordcloud} del paquete con el mismo nombre. Al contener dicha función un componente aleatorio se fija con \texttt{set.seed()} (para la reproducibilidad del gráfico por parte del lector).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{12}\NormalTok{)}
\FunctionTok{library}\NormalTok{(wordcloud)}
\FunctionTok{wordcloud}\NormalTok{(tabla}\SpecialCharTok{$}\NormalTok{palabra, tabla}\SpecialCharTok{$}\NormalTok{recuento,}
  \AttributeTok{max.words =} \DecValTok{50}\NormalTok{, }\AttributeTok{colors =} \FunctionTok{rainbow}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/nube-palabras-1} 

}

\caption{Nube de palabras más frecuentes de la Declaración}\label{fig:nube-palabras}
\end{figure}

El resultado se muestra en la Figura \ref{fig:nube-palabras}. Como se puede observar, el tamaño de letra de la palabra, y en este caso también el color, están relacionados con su frecuencia.

\hypertarget{secSENTYEMO}{%
\subsection{Análisis de sentimientos y detección de emociones}\label{secSENTYEMO}}

\textbf{Lexicón \texttt{bing}}\\
El lexicón \texttt{bing}, como se ha visto en la sección \ref{secSENTIM}, es uno de los repertorios léxicos para el análisis de sentimientos\index{análisis de sentimientos} que se pueden encontrar en \texttt{R}. Es un diccionario de polaridad (positiva/negativa) cuyo idioma original es el inglés. Se puede obtener con la función \texttt{get\_sentiments} del paquete \texttt{tidytext}. Contiene 2005 palabras positivas y 4781 palabras negativas, por lo que hay un marcado sesgo hacia la polaridad negativa.

Para ilustrar el uso de \texttt{bing}, se ha traducido al inglés (automáticamente) la Declaración. A continuación se carga el texto y se genera el objeto \texttt{tabla} replicando el procedimiento descrito arriba de preparación, limpieza, segmentación en palabras, eliminación de palabras vacías (obviamente, en idioma inglés).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"EN\_declaracion"}\NormalTok{)}
\CommentTok{\# declaracion\_EN \textless{}{-} paste(read\_lines("data/EN{-}declaracion{-}estado{-}alarma.txt"),collapse = "\textbackslash{}n")}

\NormalTok{tabla }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(}\FunctionTok{tokenize\_words}\NormalTok{(EN\_declaracion)[[}\DecValTok{1}\NormalTok{]])}
\NormalTok{tabla }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{word =} \FunctionTok{names}\NormalTok{(tabla),}
  \AttributeTok{recuento =} \FunctionTok{as.numeric}\NormalTok{(tabla)}
\NormalTok{)}
\NormalTok{tabla }\OtherTok{\textless{}{-}}\NormalTok{ tabla }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{anti\_join}\NormalTok{(}\FunctionTok{tibble}\NormalTok{(}\AttributeTok{word =} \FunctionTok{stopwords}\NormalTok{(}\StringTok{"en"}\NormalTok{))) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(recuento))}
\end{Highlighting}
\end{Shaded}

Los sentimientos positivos de la Declaración se pueden obtener con:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidytext)}
\NormalTok{pos }\OtherTok{\textless{}{-}} \FunctionTok{get\_sentiments}\NormalTok{(}\StringTok{"bing"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(sentiment }\SpecialCharTok{==} \StringTok{"positive"}\NormalTok{)}
\NormalTok{pos\_EN }\OtherTok{\textless{}{-}}\NormalTok{ tabla }\SpecialCharTok{|\textgreater{}} \FunctionTok{semi\_join}\NormalTok{(pos)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(pos\_EN)}
\end{Highlighting}
\end{Shaded}

Análogamente, se pueden obtener los sentimientos negativos. Las siete palabras más frecuentes de cada tipo que aparecen en la Declaración se presentan conjuntamente en la Tabla \ref{tab:tablaBingNRC}.

\textbf{Lexicón NRC}\\
Para poder observar las similitudes y diferencias en el análisis según el lexicón elegido, se aplica también NRC a la Declaración (véase la Tabla \ref{tab:tablaBingNRC}).

\begin{table}

\caption{\label{tab:tablaBingNRC}Palabras más frecuentes de la Declaración utilizando `bing` y `NRC`}
\centering
\begin{tabular}[t]{l|r|l|r|l|r|l|r}
\hline
positivas bing & fr & negativas bing & fr & positivas NRC & fr & negativas NRC & fr\\
\hline
extraordinary & 6 & virus & 9 & resources & 7 & virus & 9\\
\hline
protect & 4 & alarm & 4 & extraordinary & 6 & alarm & 4\\
\hline
work & 4 & emergency & 4 & protect & 4 & emergency & 4\\
\hline
like & 3 & vulnerable & 3 & maximum & 3 & government & 3\\
\hline
decisive & 2 & difficult & 2 & public & 3 & discipline & 2\\
\hline
good & 2 & hard & 2 & council & 2 & avoid & 1\\
\hline
adequate & 1 & unfortunately & 2 & good & 2 & combat & 1\\
\hline
\end{tabular}
\end{table}

Con el léxico NRC pueden detectarse emociones\index{detección de emociones}. La misma palabra puede tener asociada distintas emociones/sentimientos. En la Figura \ref{fig:barNRC} se puede observar la dispar frecuencia de palabras de cada tipo:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{emo }\OtherTok{\textless{}{-}} \FunctionTok{get\_sentiments}\NormalTok{(}\StringTok{"nrc"}\NormalTok{)}
\NormalTok{emo }\SpecialCharTok{|\textgreater{}} \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(sentiment)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill =}\NormalTok{ sentiment), }\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/barNRC-1} 

}

\caption{Gráfico de barras con la frecuencia de las emociones del lexicón NRC}\label{fig:barNRC}
\end{figure}

El análisis de sentimientos y la detección de emociones de la Declaración mediante NRC se puede realizar con el siguiente código en el que se obtiene la tabla de frecuencias por emociones y sentimientos:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{emo\_tab }\OtherTok{\textless{}{-}}\NormalTok{ tabla }\SpecialCharTok{|\textgreater{}} \FunctionTok{inner\_join}\NormalTok{(emo)}
\FunctionTok{head}\NormalTok{(emo\_tab, }\AttributeTok{n =} \DecValTok{7}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 7 x 3}
\CommentTok{\#\textgreater{}   word          recuento sentiment}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}            \textless{}dbl\textgreater{} \textless{}chr\textgreater{}    }
\CommentTok{\#\textgreater{} 1 virus                9 negative }
\CommentTok{\#\textgreater{} 2 resources            7 joy      }
\CommentTok{\#\textgreater{} 3 resources            7 positive }
\CommentTok{\#\textgreater{} 4 resources            7 trust    }
\CommentTok{\#\textgreater{} 5 extraordinary        6 positive }
\CommentTok{\#\textgreater{} 6 alarm                4 fear     }
\CommentTok{\#\textgreater{} 7 alarm                4 negative}
\end{Highlighting}
\end{Shaded}

Como se ha mencionado antes, algunas palabras tienen asociados distintos sentimientos, por ejemplo, \emph{resources}. La información de la tabla se puede visualizar bien con gráfico de barras (Figura \ref{fig:emobarras}) bien con nubes de palabras (Figura \ref{fig:emonube}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{emo\_tab }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{count}\NormalTok{(sentiment) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sentiment, }\AttributeTok{y =}\NormalTok{ n)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill =}\NormalTok{ sentiment), }\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ n), }\AttributeTok{vjust =} \SpecialCharTok{{-}}\FloatTok{0.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/emobarras-1} 

}

\caption{Frecuencia de emociones de la Declaración utilizando NRC}\label{fig:emobarras}
\end{figure}

Entre las distintas opciones para dibujar nubes de palabras\index{nubes de palabras} para el análisis de sentimientos es interesante la que se obtiene con el paquete \texttt{syuzhet} dado que permite visualizar las palabras agrupadas por emociones. Su obtención requiere distintos pasos en los que primero las palabras se agrupan por emoción, y después se organizan en una \emph{matriz de documentos}\index{matriz de documentos} con la función \texttt{TermDocumentMatrix} del paquete \texttt{tm}. Finalmente la función \texttt{comparison.cloud} permite visualizar el gráfico (tiene distintos argumentos opcionales que admiten distintas posibilidades). En el ejemplo que figura a continuación solo se han escogido tres emociones\footnote{Se deja al lector el análisis de la Declaración con más emociones, en castellano, etc.}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(syuzhet)}
\NormalTok{palabras\_EN2 }\OtherTok{\textless{}{-}} \FunctionTok{get\_tokens}\NormalTok{(EN\_declaracion)}
\NormalTok{emo\_tab2 }\OtherTok{\textless{}{-}} \FunctionTok{get\_nrc\_sentiment}\NormalTok{(palabras\_EN2, }\AttributeTok{lang =} \StringTok{"english"}\NormalTok{)}
\NormalTok{emo\_vec }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}
  \FunctionTok{paste}\NormalTok{(palabras\_EN2[emo\_tab2}\SpecialCharTok{$}\NormalTok{anger }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{], }\AttributeTok{collapse =} \StringTok{" "}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(palabras\_EN2[emo\_tab2}\SpecialCharTok{$}\NormalTok{anticipation }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{], }\AttributeTok{collapse =} \StringTok{" "}\NormalTok{),}
  \FunctionTok{paste}\NormalTok{(palabras\_EN2[emo\_tab2}\SpecialCharTok{$}\NormalTok{disgust }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{], }\AttributeTok{collapse =} \StringTok{" "}\NormalTok{)}
\NormalTok{)}
\FunctionTok{library}\NormalTok{(tm)}
\NormalTok{corpus }\OtherTok{\textless{}{-}} \FunctionTok{Corpus}\NormalTok{(}\FunctionTok{VectorSource}\NormalTok{(emo\_vec))}
\NormalTok{TDM }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{TermDocumentMatrix}\NormalTok{(corpus))}
\FunctionTok{colnames}\NormalTok{(TDM) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"anger"}\NormalTok{, }\StringTok{"anticipation"}\NormalTok{, }\StringTok{"disgust"}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\FunctionTok{comparison.cloud}\NormalTok{(TDM,}
  \AttributeTok{random.order =} \ConstantTok{FALSE}\NormalTok{,}
  \AttributeTok{colors =} \FunctionTok{c}\NormalTok{(}\StringTok{"firebrick"}\NormalTok{, }\StringTok{"forestgreen"}\NormalTok{, }\StringTok{"orange3"}\NormalTok{),}
  \AttributeTok{title.size =} \FloatTok{1.5}\NormalTok{, }\AttributeTok{scale =} \FunctionTok{c}\NormalTok{(}\FloatTok{3.5}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{rot.per =} \DecValTok{0}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/emonube-1} 

}

\caption{Nube de palabras de tres emociones NRC seleccionadas}\label{fig:emonube}
\end{figure}

\hypertarget{n-gramas}{%
\subsection{\texorpdfstring{\emph{N-gramas}}{N-gramas}}\label{n-gramas}}

El siguiente código muestra la obtención de \emph{n-gramas}\index{n-gramas} con \texttt{tokenizers}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bigramas }\OtherTok{\textless{}{-}} \FunctionTok{tokenize\_ngrams}\NormalTok{(declaracion,}
  \AttributeTok{n =} \DecValTok{2}\NormalTok{,}
  \AttributeTok{stopwords =}\NormalTok{ tabla\_stopwords}\SpecialCharTok{$}\NormalTok{palabra}
\NormalTok{)}
\FunctionTok{head}\NormalTok{(bigramas[[}\DecValTok{1}\NormalTok{]], }\AttributeTok{n =} \DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "buenas tardes"          "tardes estimados"       "estimados compatriotas"}
\NormalTok{trigramas }\OtherTok{\textless{}{-}} \FunctionTok{tokenize\_ngrams}\NormalTok{(declaracion,}
  \AttributeTok{n =} \DecValTok{3}\NormalTok{,}
  \AttributeTok{stopwords =}\NormalTok{ tabla\_stopwords}\SpecialCharTok{$}\NormalTok{palabra}
\NormalTok{)}
\FunctionTok{head}\NormalTok{(trigramas[[}\DecValTok{1}\NormalTok{]], }\AttributeTok{n =} \DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "buenas tardes estimados"       "tardes estimados compatriotas"}
\CommentTok{\#\textgreater{} [3] "estimados compatriotas día"}
\end{Highlighting}
\end{Shaded}

Se ha procedido a eliminar de los bigramas y trigramas aquellas combinaciones con al menos una palabra vacía (\emph{stopword})\index{stopwords}.

Se procede ahora a obtener los bigramas con \texttt{tidytext}. Para el resto de \emph{n-gramas} el procedimiento es análogo haciendo las modificaciones oportunas.
En el último paso se ordenan por frecuencia (de mayor a menor):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{declara2 }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{texto =}\NormalTok{ declaracion)}
\NormalTok{bigramas }\OtherTok{\textless{}{-}}\NormalTok{ declara2 }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{unnest\_tokens}\NormalTok{(bigram, texto, }\AttributeTok{token =} \StringTok{"ngrams"}\NormalTok{, }\AttributeTok{n =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{count}\NormalTok{(bigram, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{bigramas[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, ]}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 2}
\CommentTok{\#\textgreater{}   bigram         n}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}      \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 todos los      6}
\CommentTok{\#\textgreater{} 2 de la          5}
\CommentTok{\#\textgreater{} 3 de los         5}
\CommentTok{\#\textgreater{} 4 del estado     5}
\CommentTok{\#\textgreater{} 5 estado de      5}
\end{Highlighting}
\end{Shaded}

Una forma de eliminar las palabras vacías:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bigramas\_limpios }\OtherTok{\textless{}{-}}\NormalTok{ bigramas }\SpecialCharTok{|\textgreater{}}
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{separate}\NormalTok{(bigram, }\FunctionTok{c}\NormalTok{(}\StringTok{"word1"}\NormalTok{, }\StringTok{"word2"}\NormalTok{), }\AttributeTok{sep =} \StringTok{" "}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{word1 }\SpecialCharTok{\%in\%}\NormalTok{ tabla\_stopwords}\SpecialCharTok{$}\NormalTok{palabra) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{word2 }\SpecialCharTok{\%in\%}\NormalTok{ tabla\_stopwords}\SpecialCharTok{$}\NormalTok{palabra) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{unite}\NormalTok{(bigram, word1, word2, }\AttributeTok{sep =} \StringTok{" "}\NormalTok{)}
\NormalTok{bigramas\_limpios[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, ]}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 2}
\CommentTok{\#\textgreater{}   bigram                       n}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}                    \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 autoridades sanitarias       2}
\CommentTok{\#\textgreater{} 2 buenas tardes                2}
\CommentTok{\#\textgreater{} 3 disciplina social            2}
\CommentTok{\#\textgreater{} 4 haga falta                   2}
\CommentTok{\#\textgreater{} 5 ministros extraordinario     2}
\end{Highlighting}
\end{Shaded}

\textbf{Significado y contexto}\\
Como se ha visto en la sección \ref{secTOKEN}, con los \emph{n-gramas} se puede hacer un análisis de colocaciones para extraer los distintos significados y valores informativos a partir del contexto. En este caso, se puede ver cómo la palabra \emph{atender} cambia de sentido cuando va precedida de \emph{no} o \emph{sin}. A continuación, se filtran los bigramas cuya primera palabra es \emph{no}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bigramas\_no }\OtherTok{\textless{}{-}}\NormalTok{ bigramas }\SpecialCharTok{|\textgreater{}}
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{separate}\NormalTok{(bigram, }\FunctionTok{c}\NormalTok{(}\StringTok{"word1"}\NormalTok{, }\StringTok{"word2"}\NormalTok{), }\AttributeTok{sep =} \StringTok{" "}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(word1 }\SpecialCharTok{==} \StringTok{"no"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{count}\NormalTok{(word1, word2, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{bigramas\_no}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 3}
\CommentTok{\#\textgreater{}   word1 word2       n}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{} \textless{}chr\textgreater{}   \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 no    atiende     1}
\CommentTok{\#\textgreater{} 2 no    cabe        1}
\CommentTok{\#\textgreater{} 3 no    es          1}
\end{Highlighting}
\end{Shaded}

Estos resultados se pueden utilizar para el análisis de sentimientos y la detección de emociones.

\hypertarget{anuxe1lisis-de-redes}{%
\subsection{Análisis de redes}\label{anuxe1lisis-de-redes}}

Se proporcionan las instrucciones para realizar un análisis básico de redes\index{análisis de redes}, utilizando los paquetes \texttt{igraph} y \texttt{ggraph}. Dada la corta extensión de la Declaración no es posible obtener conclusiones.
En la figura \ref{fig:figredes} se pueden ver los gráficos de redes de bigramas, tanto sin palabras vacías como con ellas.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(igraph)}
\FunctionTok{library}\NormalTok{(ggraph)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{graf\_bigramas\_l }\OtherTok{\textless{}{-}}\NormalTok{ bigramas\_limpios }\SpecialCharTok{|\textgreater{}}
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{separate}\NormalTok{(bigram, }\FunctionTok{c}\NormalTok{(}\StringTok{"first"}\NormalTok{, }\StringTok{"second"}\NormalTok{), }\AttributeTok{sep =} \StringTok{" "}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(n }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{graph\_from\_data\_frame}\NormalTok{()}
\NormalTok{g1 }\OtherTok{\textless{}{-}} \FunctionTok{ggraph}\NormalTok{(graf\_bigramas\_l, }\AttributeTok{layout =} \StringTok{"fr"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_edge\_link}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_node\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_node\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ name))}
\NormalTok{graf\_bigramas }\OtherTok{\textless{}{-}}\NormalTok{ bigramas }\SpecialCharTok{|\textgreater{}}
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{separate}\NormalTok{(bigram, }\FunctionTok{c}\NormalTok{(}\StringTok{"first"}\NormalTok{, }\StringTok{"second"}\NormalTok{), }\AttributeTok{sep =} \StringTok{" "}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(n }\SpecialCharTok{\textgreater{}} \DecValTok{2}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{graph\_from\_data\_frame}\NormalTok{()}
\NormalTok{g2 }\OtherTok{\textless{}{-}} \FunctionTok{ggraph}\NormalTok{(graf\_bigramas, }\AttributeTok{layout =} \StringTok{"fr"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_edge\_link}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_node\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_node\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ name))}
\FunctionTok{library}\NormalTok{(patchwork)}
\NormalTok{g1 }\SpecialCharTok{+}\NormalTok{ g2}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/figredes-1} 

}

\caption{Redes de bigramas sin palabras vacías y con ellas}\label{fig:figredes}
\end{figure}

\begin{infobox_resume}

\hypertarget{resumen-11}{%
\subsection*{Resumen}\label{resumen-11}}
\addcontentsline{toc}{subsection}{Resumen}

En este capítulo se ha introducido al lector en la minería de textos, en particular:

\begin{itemize}
\tightlist
\item
  Se han presentado los conceptos y tareas fundamentales de este ámbito, así como sus campos de aplicación principales.
  Se ha puesto de relieve la importancia de la preparación de los datos y su segmentación (a distintos niveles) para obtener buenos resultados, acordes con el objetivo de la investigación.
\item
  Se ha mostrado el uso de \texttt{R} para el análisis de textos y de sentimientos.
\item
  Se ha presentado un caso aplicado para ilustrar las técnicas de minería de textos.
\item
  Se han mencionado otros análisis plausibles de minería de textos como la estilometría o el modelado de temas (véase el capítulo 58).
\end{itemize}

\end{infobox_resume}

\hypertarget{grafos}{%
\chapter{Análisis de grafos y redes sociales}\label{grafos}}

\emph{José Javier Galán}

\hypertarget{introducciuxf3n-22}{%
\section{Introducción}\label{introducciuxf3n-22}}

El origen de la teoría de grafos se debe al problema de los Siete puentes de Königsberg \citep{Euler1736}, es considerado el primer artículo sobre teoría de grafos. El problema se centra en la ciudad Königsberg en Prusia, ahora Kaliningrado (Rusia), donde existen varios puentes y el problema plantea trazar una ruta que cruce todos los puentes una única vez (ver Fig.\ref{fig:puentes}). Euler mediante el uso de grafos demostró que no era posible.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/Fig_g1} 

}

\caption{Siete puentes de Königsberg, Euler.}\label{fig:puentes}
\end{figure}

\hypertarget{teoruxeda-de-grafos}{%
\section{Teoría de grafos}\label{teoruxeda-de-grafos}}

Un \textbf{grafo} es un conjunto de \textbf{nodos} (vértices) que pueden estar unidos por \textbf{aristas} (enlaces).

Si se piensa en cada nodo como una persona y en cada arista como la relación que los une, entonces se puede representar mediante grafos una \textbf{red social} (ver Fig.\ref{fig:grafo}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Cargar el paquete de igraph}
\CommentTok{\# if(!require(\textquotesingle{}igraph\textquotesingle{})) install.packages(\textquotesingle{}igraph\textquotesingle{})}
\FunctionTok{library}\NormalTok{(igraph)}

\NormalTok{datos\_facebook }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"data/DatosFacebook.csv"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{sep =} \StringTok{" "}\NormalTok{, }\AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Origen"}\NormalTok{, }\StringTok{"Destino"}\NormalTok{))}
\NormalTok{grafo\_facebook }\OtherTok{\textless{}{-}} \FunctionTok{graph.data.frame}\NormalTok{(datos\_facebook, }\AttributeTok{directed =}\NormalTok{ T)}
\FunctionTok{plot}\NormalTok{(grafo\_facebook, }\AttributeTok{vertex.label =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/grafo-1} 

}

\caption{Estructura de una red social representada como grafo.}\label{fig:grafo}
\end{figure}

Una \textbf{red social} es una estructura de red invisible que une mediante relaciones a distintos actores a través de sus intereses o valores comunes, estableciendo una relación personal entre individuos o grupos de individuos conectados.

\hypertarget{elementos-de-un-grafo}{%
\section{Elementos de un grafo}\label{elementos-de-un-grafo}}

El análisis de redes sociales mediante le teoría de grafos requiere conocer previamente una serie de conceptos básicos, \citep{Perez2021}.

\begin{itemize}
\tightlist
\item
  Las \textbf{aristas} son la relaciones que unen los nodos. Son \textbf{dirigidas} (ver Fig.\ref{fig:grafodirigido}) si tienen un sentido definido y \textbf{no dirigidas} (ver Fig.\ref{fig:grafonodirigido}) en caso contrario.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datos\_grafo }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"data/DatosGrafos.csv"}\NormalTok{, }\AttributeTok{sep =} \StringTok{";"}\NormalTok{)}
\NormalTok{grafo }\OtherTok{\textless{}{-}} \FunctionTok{graph.data.frame}\NormalTok{(datos\_grafo, }\AttributeTok{directed =}\NormalTok{ T)}
\FunctionTok{plot}\NormalTok{(grafo, }\AttributeTok{edge.label =} \FunctionTok{paste}\NormalTok{(}\FunctionTok{E}\NormalTok{(grafo)}\SpecialCharTok{$}\NormalTok{weight))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/grafodirigido-1} 

}

\caption{Grafo dirigido.}\label{fig:grafodirigido}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grafo }\OtherTok{\textless{}{-}} \FunctionTok{graph.data.frame}\NormalTok{(datos\_grafo, }\AttributeTok{directed =}\NormalTok{ F)}
\FunctionTok{plot}\NormalTok{(grafo, }\AttributeTok{edge.label =} \FunctionTok{paste}\NormalTok{(}\FunctionTok{E}\NormalTok{(grafo)}\SpecialCharTok{$}\NormalTok{weight))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/grafonodirigido-1} 

}

\caption{Grafo no dirigido.}\label{fig:grafonodirigido}
\end{figure}

En una red social como \textbf{LinkedIn} las aristas representan la relación que une las personas. Las personas forman parte de un grupo con intereses comunes, formando un grado no dirigido. Pero también pueden seguir a alguien sin necesariamente ser seguido, en ese caso podemos representar un grafo dirigido.

\begin{itemize}
\item
  Los \textbf{vértices}, vertex , representan nodos que serán unidos mediante aristas. En una red social cada vértice representa una de las personas de dicha red, unidas en ocasiones por intereses comunes a otras.
\item
  Un \textbf{grafo} es un conjunto de vértices y de aristas que podemos representar mediante \(G = (V, E)\). Donde \(V\) es el conjunto de nodos o vértices del grafo y \(E\) es un conjunto de pares de vértices llamado arista, arco o edge.
\item
  Una \textbf{matriz de adyacencia} representa una matriz cuadrada de \(n \times n\), siendo n el número de vértices del grafo. Se inicia con valor 0 donde a cada valor \(a_{ij}\) se suma 1 por cada arista que relaciona los vértices \(i\) y \(j\). Si no existe relación el elemento \(a_{ij}\) vale 0.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{matriz\_adyacencia }\OtherTok{\textless{}{-}} \FunctionTok{get.adjacency}\NormalTok{(grafo, }\AttributeTok{sparse =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{matriz\_adyacencia}
\CommentTok{\#\textgreater{}   1 2 3 4 5}
\CommentTok{\#\textgreater{} 1 0 1 1 1 1}
\CommentTok{\#\textgreater{} 2 1 0 1 2 0}
\CommentTok{\#\textgreater{} 3 1 1 0 2 1}
\CommentTok{\#\textgreater{} 4 1 2 2 0 0}
\CommentTok{\#\textgreater{} 5 1 0 1 0 0}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  El \textbf{grado} de un nodo son el numero de aristas que tiene dicho nodo \(x\), y se representa mediante grado(x), g(x) o gr(x), siendo un vértice de grado 0 un vértice aislado. En un grafo \(G\) existirá un grado máximo \(\Delta (G)\) y un grado mínimo \(\delta (G)\), mientras que el grado del grafo, g(G), es la suma de todos los grados de sus vértices. En una red social representa el número de relaciones que existen, en una red social como Facebook significaría conocer cuántos amigos tienes.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{degree}\NormalTok{(grafo)}
\CommentTok{\#\textgreater{} 1 2 3 4 5 }
\CommentTok{\#\textgreater{} 4 4 5 5 2}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Un \textbf{camino} une dos vértices constituyendo un conjunto de aristas no recursivas. Entre dos vértices puede no existir un camino, puede haber varios y se puede incluir el mismo vértice en el camino mas de una vez. Evidentemente siempre habrá un \textbf{camino mas corto} , aquel que menos aristas ha recorrido. Si entre todos los pares de vértices existe un camino estamos hablando de un grafo conexo.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Camino más corto entre el nodo 2 y el 5}
\NormalTok{caminos }\OtherTok{\textless{}{-}} \FunctionTok{get.shortest.paths}\NormalTok{(grafo, }\AttributeTok{from =} \StringTok{"2"}\NormalTok{, }\AttributeTok{to =} \StringTok{"5"}\NormalTok{)}
\FunctionTok{V}\NormalTok{(grafo)[caminos}\SpecialCharTok{$}\NormalTok{vpath[[}\DecValTok{1}\NormalTok{]]]}
\CommentTok{\#\textgreater{} + 4/5 vertices, named, from d9b4a59:}
\CommentTok{\#\textgreater{} [1] 2 4 3 5}
\end{Highlighting}
\end{Shaded}

\hypertarget{el-paquete-igraph}{%
\section{\texorpdfstring{El paquete \texttt{igraph}}{El paquete igraph}}\label{el-paquete-igraph}}

Existen diversos paquetes, pero el más utilizado y popularizado por sencillez y eficacia es \texttt{igraph} \citep{igraph2022}. Se trata de un paquete que permite crear y manipular grafos para poder analizar redes en \textbf{R} de forma muy sencilla (ver Fig.\ref{fig:grafobasico}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nodes }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\StringTok{"nodos"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"C"}\NormalTok{, }\StringTok{"D"}\NormalTok{, }\StringTok{"E"}\NormalTok{))}
\NormalTok{edges }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\StringTok{"from"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"C"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"A"}\NormalTok{), }\StringTok{"to"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"B"}\NormalTok{, }\StringTok{"D"}\NormalTok{, }\StringTok{"C"}\NormalTok{, }\StringTok{"C"}\NormalTok{, }\StringTok{"D"}\NormalTok{, }\StringTok{"E"}\NormalTok{))}
\NormalTok{red }\OtherTok{\textless{}{-}} \FunctionTok{graph\_from\_data\_frame}\NormalTok{(edges, }\AttributeTok{directed =}\NormalTok{ F, }\AttributeTok{vertices =}\NormalTok{ nodes)}

\FunctionTok{plot}\NormalTok{(red)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/grafobasico-1} 

}

\caption{Ejemplo de grafo con Igraph.}\label{fig:grafobasico}
\end{figure}

Para crear un grafo a partir de un \textbf{dataframe} se han usado los argumentos:

\texttt{graph\_from\_data\_frame(edges,\ directed\ =\ TRUE,\ vertices\ =\ NULL)} donde:

\begin{itemize}
\item
  \textbf{edges} Es un data frame donde las dos primeras columnas representan una lista de aristas.
\item
  \textbf{directed} es un Valor lógico que indica si es un grafo dirigido o no dirigido.
\item
  \textbf{vertices} es un data frame con los valores de los vertices o NULL.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(igraph)}
\NormalTok{nodes }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\StringTok{"actores"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Jim Carrey"}\NormalTok{, }\StringTok{"Arnold Swarzenneger"}\NormalTok{, }\StringTok{"George Cloney"}\NormalTok{, }\StringTok{"Cameron Diaz"}\NormalTok{), }\StringTok{"descripcion"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"actor"}\NormalTok{, }\StringTok{"actor"}\NormalTok{, }\StringTok{"actor"}\NormalTok{, }\StringTok{"actriz"}\NormalTok{))}
\NormalTok{edges }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}
  \StringTok{"from"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Jim Carrey"}\NormalTok{, }\StringTok{"Jim Carrey"}\NormalTok{, }\StringTok{"George Cloney"}\NormalTok{, }\StringTok{"Jim Carrey"}\NormalTok{),}
  \StringTok{"to"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Arnold Swarzenneger"}\NormalTok{, }\StringTok{"George Cloney"}\NormalTok{, }\StringTok{"Arnold Swarzenneger"}\NormalTok{, }\StringTok{"Cameron Diaz"}\NormalTok{),}
  \StringTok{"pelicula"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"Batman y Robin"}\NormalTok{, }\StringTok{"Batman y Robin"}\NormalTok{, }\StringTok{"Batman y Robin"}\NormalTok{, }\StringTok{"La mascara"}\NormalTok{)}
\NormalTok{)}
\NormalTok{red }\OtherTok{\textless{}{-}} \FunctionTok{graph\_from\_data\_frame}\NormalTok{(edges, }\AttributeTok{directed =}\NormalTok{ F, }\AttributeTok{vertices =}\NormalTok{ nodes)}
\FunctionTok{plot}\NormalTok{(red)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/grafoactores-1} 

}

\caption{Grafo representativo de la relacion de actores respecto a peliculas}\label{fig:grafoactores}
\end{figure}

En la Fig. \ref{fig:grafoactores} se puede observar como el actor Jim Carrey tuvo relación con todos los actores de la red propuesta, mientras que la actriz Cameron Diaz solo participo con uno de ellos.

\hypertarget{anuxe1lisis-de-influencia-en-un-grafo-aplicado-a-redes-sociales}{%
\section{Análisis de influencia en un grafo aplicado a redes sociales}\label{anuxe1lisis-de-influencia-en-un-grafo-aplicado-a-redes-sociales}}

Existen paquetes para obtener informacion de distintas redes sociales, por ejemplo, se puede utilizar el paquete \texttt{Rfacebook} para conectarse a \textbf{Facebook} y obtener información de lo contactos existentes, para ello sera necesario activar la API desde \textbf{\url{https://developers.facebook.com}}. La información necesaria podemos encontrarla en su página web \textbf{\url{https://developers.facebook.com/docs}}. Para ilustrar un ejemplo didactico, sin ncesidad de que el lector necesite conocimientos de desarrollo para descargar datos, se ha generado un fichero excel que simula la relacion entre amigos de una red social como podria ser perfertamente Facebook.

\begin{itemize}
\tightlist
\item
  Se incorporan los datos y se muestra su cabecera.
  Los datos han sido recogidos en un fichero csv, constando de dos columnas separadas por espacio. La primera columna contiene el id de la primera persona, la cual establece relación con la persona indicada también por su id en la segunda columna.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datos\_faceboook }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"data/DatosFacebook.csv"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{sep =} \StringTok{" "}\NormalTok{, }\AttributeTok{col.names =} \FunctionTok{c}\NormalTok{(}\StringTok{"Origen"}\NormalTok{, }\StringTok{"Destino"}\NormalTok{))}
\FunctionTok{head}\NormalTok{(datos\_faceboook)}
\CommentTok{\#\textgreater{}   Origen Destino}
\CommentTok{\#\textgreater{} 1   3434    3409}
\CommentTok{\#\textgreater{} 2   3493    3361}
\CommentTok{\#\textgreater{} 3   3329    3324}
\CommentTok{\#\textgreater{} 4   3496    3384}
\CommentTok{\#\textgreater{} 5   3370    3415}
\CommentTok{\#\textgreater{} 6   3383    3490}
\end{Highlighting}
\end{Shaded}

Un grafo no tiene una centralidad real porque no tiene coordenadas, pero tenemos distintas medidas de centralidad que en un grafo social nos ayudara a identificar el poder social de cada individuo, es decir, su influencia.

Se puede decir que la \textbf{centralidad de un grafo} coincide con el grado de vértices a analizar, si un vértice representa una persona de una red social de amigos su centralidad representa el número de amigos que tiene.

En los grafos dirigidos cuantas más aristas dirigidas a un nodo existan significará que más personas intentar interactuar con una persona concreta y esta más prestigio tendrá. Pero si la interacción hacia esta persona no es directa y se realiza a través de un camino más largo pasando por más nodos quiere decir que su influencia es elevada.

\begin{itemize}
\tightlist
\item
  Se convierten los datos a formato igraph y mostramos el grafo (ver Fig.\ref{fig:grafofacebook1}).
  Se compone el data frame de tipo igraph necesario indicando que es un grafo dirigido, finalmente mediante `plot' se muestra el grado al mismo tiempo que se establecen sus propiedades.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grafo\_faceboook }\OtherTok{\textless{}{-}} \FunctionTok{graph.data.frame}\NormalTok{(datos\_faceboook, }\AttributeTok{directed =}\NormalTok{ T)}

\FunctionTok{plot.igraph}\NormalTok{(grafo\_faceboook,}
  \AttributeTok{layout =}\NormalTok{ layout.fruchterman.reingold,}
  \AttributeTok{vertex.label =} \ConstantTok{NA}\NormalTok{,}
  \AttributeTok{vertex.label.cex =} \DecValTok{1}\NormalTok{,}
  \AttributeTok{vertex.size =} \DecValTok{3}\NormalTok{, }\AttributeTok{edge.curved =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{edge.arrow.size =} \DecValTok{0}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/grafofacebook1-1} 

}

\caption{Aplicación de Igraph a Redes Sociales.}\label{fig:grafofacebook1}
\end{figure}

\begin{itemize}
\tightlist
\item
  Relaciones de la red social.
  Se puede observar que el numero de aristas que contiene cada vertice son el numero de personas con las que se relaciona cada individuo, por lo tanto son las relaciones que contiene cada persona.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(}\FunctionTok{degree}\NormalTok{(grafo\_faceboook))}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 }
\CommentTok{\#\textgreater{}  1  2  4  4  7  4  8 14 19 15 14 19 22 15 11 13  9  5  3  3  4  1  4}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Se personalizan los datos (ver Fig.\ref{fig:grafofacebook3}).
  Son muchos nodos y para focalizar este caso de estudio se centra el codigo en los nodos que sean de grado igual o superior a 26 y se les asigna nombre.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bad\_network }\OtherTok{\textless{}{-}} \FunctionTok{V}\NormalTok{(grafo\_faceboook)[}\FunctionTok{degree}\NormalTok{(grafo\_faceboook) }\SpecialCharTok{\textless{}=} \DecValTok{26}\NormalTok{]}
\NormalTok{grafo\_faceboook }\OtherTok{\textless{}{-}} \FunctionTok{delete.vertices}\NormalTok{(grafo\_faceboook, bad\_network)}
\FunctionTok{V}\NormalTok{(grafo\_faceboook)}\SpecialCharTok{$}\NormalTok{name }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Gema"}\NormalTok{, }\StringTok{"Patricia"}\NormalTok{, }\StringTok{"Ramon"}\NormalTok{, }\StringTok{"Jose"}\NormalTok{, }\StringTok{"Maria"}\NormalTok{, }\StringTok{"Angeles"}\NormalTok{, }\StringTok{"Gabriel"}\NormalTok{, }\StringTok{"Javier"}\NormalTok{, }\StringTok{"Victor"}\NormalTok{, }\StringTok{"Leonor"}\NormalTok{, }\StringTok{"Ana"}\NormalTok{, }\StringTok{"Isabel"}\NormalTok{, }\StringTok{"Cristobal"}\NormalTok{, }\StringTok{"Rosa"}\NormalTok{, }\StringTok{"Aurola"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(grafo\_faceboook)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/grafofacebook3-1} 

}

\caption{Aplicación de Igraph a Redes Sociales, grafo dendrograma}\label{fig:grafofacebook3}
\end{figure}

Se pueden observar las relaciones entre las distintas personas que lo componen, como siguen a otras y son seguidas. Tambien se pueden observar los casos extremos como el de Gema a quien nadie sigue, o el de Leonor que no sigue a nadie pero varios podrian llegar hasta ella.

\textbf{Detección de comunidades}

En el análisis de una red social es importante detectar las distintas comunidades que la componen.

\begin{itemize}
\tightlist
\item
  Se puede observar dos comunidades detectadas con \textbf{Walktrap} en la (ver Fig.\ref{fig:grafofacebook5})
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Walktrap\_grafo }\OtherTok{\textless{}{-}} \FunctionTok{walktrap.community}\NormalTok{(grafo\_faceboook, }\AttributeTok{steps =} \DecValTok{5}\NormalTok{, }\AttributeTok{modularity =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(Walktrap\_grafo, grafo\_faceboook, }\AttributeTok{edge.arrow.size =} \FloatTok{0.25}\NormalTok{, }\AttributeTok{vertex.label =}\NormalTok{ (grafo\_faceboook)}\SpecialCharTok{$}\NormalTok{name)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/grafofacebook5-1} 

}

\caption{Walktrap Community on Full dataset}\label{fig:grafofacebook5}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dend\_g\_network }\OtherTok{\textless{}{-}} \FunctionTok{as.dendrogram}\NormalTok{(Walktrap\_grafo)}
\FunctionTok{plot}\NormalTok{(dend\_g\_network)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/grafofacebook4-1} 

}

\caption{Grafo dendrograma de Walktrap}\label{fig:grafofacebook4}
\end{figure}

\textbf{Walktrap} detecta las comunidades basandose en el concepto de que las caminatas aleatorias cortas permanecen en la misma comunidad.
Debe indicarse èl largo de la caminata aleatoria, se recomienda usar 5 caminatas.
Se puede ver su dendrograma en la Fig.\ref{fig:grafofacebook4}.
El dendograma es un tipo de diagrama en forma de arbol, en el cual se subdivide los datos en subcategorias sucesivamente hasta llegar a las vistas deseadas.

\begin{itemize}
\tightlist
\item
  La técnica de \textbf{Centralidad de intermediación (betweenness)} se basa en el número de caminos mínimos en los que un nodo esta involucrado. Por lo tanto, en un red social una persona tendrá mayor influencia cuanto mayor betweenness tenga porque comunicara mucha información a través de los nodos de la red. Si puede llegar a un grupo grande, aunque sea a través de un nodo puede alcanzar un gran nivel de viralización (ver Fig.\ref{fig:grafofacebook6} y Fig.\ref{fig:grafofacebook7}).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{edge\_g\_network }\OtherTok{\textless{}{-}} \FunctionTok{edge.betweenness.community}\NormalTok{(grafo\_faceboook)}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{as.dendrogram}\NormalTok{(edge\_g\_network), }\AttributeTok{main =} \StringTok{"Grafo dendrograma de Betweenness"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/grafofacebook6-1} 

}

\caption{Grafo dendrograma de Betweenness}\label{fig:grafofacebook6}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(edge\_g\_network, grafo\_faceboook, }\AttributeTok{edge.arrow.size =} \FloatTok{0.25}\NormalTok{, }\AttributeTok{main =} \StringTok{"Edge Betweenness Community"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/grafofacebook7-1} 

}

\caption{Grafo dendrograma de Betweenness}\label{fig:grafofacebook7}
\end{figure}

En este ejemplo vemos como se han detectado mediante este algoritmo siete comunidades.

\begin{itemize}
\tightlist
\item
  \textbf{Centralidad de vector propio (eigenvector)} se basa en la centralidad de los nodos con los que se relaciona, la centralidad de un nodo es proporcional a la suma de las centralidades de sus nodos vecinos.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{cl\_g\_network }\OtherTok{\textless{}{-}} \FunctionTok{leading.eigenvector.community}\NormalTok{(grafo\_faceboook)}
\FunctionTok{plot}\NormalTok{(cl\_g\_network, grafo\_faceboook, }\AttributeTok{edge.arrow.size =} \FloatTok{0.25}\NormalTok{, }\AttributeTok{main =} \StringTok{"Leading Eigenvector Community"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/grafofacebook8-1} 

}

\caption{Aplicación de Igraph a Redes Sociales}\label{fig:grafofacebook8}
\end{figure}

Se representa mediante \(c_{i} = \lambda \sum_{j} a_{ij}c_{j}\), donde \(\lambda\) es la constante de proporcionalidad y \(a_{ij}\) es el valor de la fila \(i\) y la columna \(j\) de la matriz de adyacencia \textbf{A} de la red social.

En este ejemplo vemos como se han detectado mediante este algoritmo tres comunidades. Segun este metodo, no es tan importante que tengas muchos amigos, lo importante es que tus amigos sean muy influyentes (ver Fig.\ref{fig:grafofacebook8}).

\hypertarget{otras-utilidades-de-grafos}{%
\section{Otras utilidades de grafos}\label{otras-utilidades-de-grafos}}

En la Fig \ref{fig:accidentalidad} se muestran los tipos de accidente ocurridos en Madrid durante el año 2020 que tiene como relación la zona en la que sucedió, en este caso en ``Barajas''. Ademas se añade a las aristas el numero de accidentes sucedido por cada tipo.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(igraph)}
\FunctionTok{library}\NormalTok{(CDR)}

\NormalTok{datos\_accidentalidad\_2020 }\OtherTok{\textless{}{-}}\NormalTok{ accidentes2020\_data}
\NormalTok{conjunto }\OtherTok{\textless{}{-}}\NormalTok{ datos\_accidentalidad\_2020[, }\FunctionTok{c}\NormalTok{(}\StringTok{"distrito"}\NormalTok{, }\StringTok{"tipo\_accidente"}\NormalTok{)]}

\NormalTok{distrito }\OtherTok{\textless{}{-}}\NormalTok{ datos\_accidentalidad\_2020[, }\FunctionTok{c}\NormalTok{(}\StringTok{"distrito"}\NormalTok{)]}
\NormalTok{tipo\_accidente }\OtherTok{\textless{}{-}}\NormalTok{ datos\_accidentalidad\_2020[, }\FunctionTok{c}\NormalTok{(}\StringTok{"tipo\_accidente"}\NormalTok{)]}

\NormalTok{grupos }\OtherTok{\textless{}{-}} \FunctionTok{group\_by}\NormalTok{(conjunto, distrito, tipo\_accidente)}
\NormalTok{contados }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{summarise}\NormalTok{(grupos,}
  \AttributeTok{num =} \FunctionTok{n}\NormalTok{()}
\NormalTok{)}

\NormalTok{zona\_Barajas }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(contados, distrito }\SpecialCharTok{==} \StringTok{"BARAJAS"}\NormalTok{)}
\NormalTok{g3 }\OtherTok{\textless{}{-}} \FunctionTok{graph\_from\_data\_frame}\NormalTok{(zona\_Barajas)}
\FunctionTok{plot.igraph}\NormalTok{(g3, }\AttributeTok{edge.label =}\NormalTok{ zona\_Barajas}\SpecialCharTok{$}\NormalTok{num)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/accidentalidad} 

}

\caption{Accidentes en Barajas en el año 2020}\label{fig:accidentalidad}
\end{figure}

Ahora mostramos los principales puntos conflictivos de Barajas unidos para ver si existen caminos que pasaern por otros accidentes y evitarlos, Fig \ref{fig:gmapa}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(proj4)}
\FunctionTok{library}\NormalTok{(igraph)}
\FunctionTok{library}\NormalTok{(sp)}
\FunctionTok{library}\NormalTok{(leaflet)}

\FunctionTok{library}\NormalTok{(CDR)}
\NormalTok{datos }\OtherTok{\textless{}{-}}\NormalTok{ accidentes2020\_data}
\NormalTok{datos }\OtherTok{\textless{}{-}}\NormalTok{ datos[, }\FunctionTok{c}\NormalTok{(}\StringTok{"localizacion"}\NormalTok{, }\StringTok{"distrito"}\NormalTok{, }\StringTok{"coordenada\_x\_utm"}\NormalTok{, }\StringTok{"coordenada\_y\_utm"}\NormalTok{)]}
\NormalTok{datos }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(datos, distrito }\SpecialCharTok{==} \StringTok{"BARAJAS"}\NormalTok{)}
\NormalTok{datos }\OtherTok{\textless{}{-}}\NormalTok{ datos[, }\FunctionTok{c}\NormalTok{(}\StringTok{"localizacion"}\NormalTok{, }\StringTok{"coordenada\_x\_utm"}\NormalTok{, }\StringTok{"coordenada\_y\_utm"}\NormalTok{)]}
\NormalTok{datos }\OtherTok{\textless{}{-}} \FunctionTok{distinct}\NormalTok{(datos)}

\NormalTok{meta }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{name =}\NormalTok{ datos[[}\StringTok{"localizacion"}\NormalTok{]], }\AttributeTok{lon =}\NormalTok{ datos[[}\StringTok{"coordenada\_x\_utm"}\NormalTok{]], }\AttributeTok{lat =}\NormalTok{ datos[[}\StringTok{"coordenada\_y\_utm"}\NormalTok{]])}
\NormalTok{proj4string }\OtherTok{\textless{}{-}} \StringTok{"+proj=utm +zone=30 +ellps=WGS84 +datum=WGS84 +units=m +no\_defs "}


\NormalTok{xy }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(meta}\SpecialCharTok{$}\NormalTok{lon, meta}\SpecialCharTok{$}\NormalTok{lat) }\CommentTok{\# data frame con utm}
\NormalTok{pj }\OtherTok{\textless{}{-}} \FunctionTok{project}\NormalTok{(xy, proj4string, }\AttributeTok{inverse =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{meta }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{name =}\NormalTok{ datos[[}\StringTok{"localizacion"}\NormalTok{]], }\AttributeTok{lat =}\NormalTok{ pj}\SpecialCharTok{$}\NormalTok{y, }\AttributeTok{lon =}\NormalTok{ pj}\SpecialCharTok{$}\NormalTok{x)}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{from =}\NormalTok{ datos[[}\StringTok{"localizacion"}\NormalTok{]], }\AttributeTok{to =}\NormalTok{ datos[[}\StringTok{"localizacion"}\NormalTok{]])}
\NormalTok{meta }\OtherTok{\textless{}{-}} \FunctionTok{head}\NormalTok{(}\FunctionTok{distinct}\NormalTok{(meta))}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{head}\NormalTok{(}\FunctionTok{distinct}\NormalTok{(df))}


\NormalTok{mat }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\AttributeTok{ncol =} \DecValTok{0}\NormalTok{, }\AttributeTok{nrow =} \DecValTok{0}\NormalTok{)}
\NormalTok{df2 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(mat)}
\ControlFlowTok{for}\NormalTok{ (f }\ControlFlowTok{in}\NormalTok{ df[, }\DecValTok{1}\NormalTok{])}
\NormalTok{\{}
  \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in}\NormalTok{ df[, }\DecValTok{2}\NormalTok{])}
\NormalTok{  \{}
\NormalTok{    fila\_nueva }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(f, t)}
\NormalTok{    df2 }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(df2, fila\_nueva)}
\NormalTok{  \}}
\NormalTok{\}}
\NormalTok{df }\OtherTok{\textless{}{-}}\NormalTok{ df2}

\NormalTok{g }\OtherTok{\textless{}{-}} \FunctionTok{graph.data.frame}\NormalTok{(df, }\AttributeTok{directed =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{vertices =}\NormalTok{ meta)}
\NormalTok{gg }\OtherTok{\textless{}{-}} \FunctionTok{get.data.frame}\NormalTok{(g, }\StringTok{"both"}\NormalTok{)}
\NormalTok{vert }\OtherTok{\textless{}{-}}\NormalTok{ gg}\SpecialCharTok{$}\NormalTok{vertices}
\FunctionTok{coordinates}\NormalTok{(vert) }\OtherTok{\textless{}{-}} \ErrorTok{\textasciitilde{}}\NormalTok{ lon }\SpecialCharTok{+}\NormalTok{ lat}

\NormalTok{edges }\OtherTok{\textless{}{-}}\NormalTok{ gg}\SpecialCharTok{$}\NormalTok{edges}

\NormalTok{edges }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(edges), }\ControlFlowTok{function}\NormalTok{(i) \{}
  \FunctionTok{as}\NormalTok{(}
    \FunctionTok{rbind}\NormalTok{(}
\NormalTok{      vert[vert}\SpecialCharTok{$}\NormalTok{name }\SpecialCharTok{==}\NormalTok{ edges[i, }\StringTok{"from"}\NormalTok{], ],}
\NormalTok{      vert[vert}\SpecialCharTok{$}\NormalTok{name }\SpecialCharTok{==}\NormalTok{ edges[i, }\StringTok{"to"}\NormalTok{], ]}
\NormalTok{    ),}
    \StringTok{"SpatialLines"}
\NormalTok{  )}
\NormalTok{\})}


\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \FunctionTok{seq\_along}\NormalTok{(edges)) \{}
\NormalTok{  edges[[i]] }\OtherTok{\textless{}{-}} \FunctionTok{spChFIDs}\NormalTok{(edges[[i]], }\FunctionTok{as.character}\NormalTok{(i))}
\NormalTok{\}}

\NormalTok{edges }\OtherTok{\textless{}{-}} \FunctionTok{do.call}\NormalTok{(rbind, edges)}

\FunctionTok{leaflet}\NormalTok{(vert) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{addTiles}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{addMarkers}\NormalTok{(}\AttributeTok{data =}\NormalTok{ vert) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{addPolylines}\NormalTok{(}\AttributeTok{data =}\NormalTok{ edges)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/accidentalidad2} 

}

\caption{Grafo sobre mapa interactivo}\label{fig:gmapa}
\end{figure}

\begin{infobox_resume}
\textbf{RESUMEN}

El uso de los grafos para el estudio de las redes sociales resultad de gran utilidad y aunque existen distintos paquetes uno de los mas potentes sigue siendo \texttt{igraph}. Se ha aprendido que tres componentes son necesarios: obtener datos de una red social, conocer las propiedades de los grafos y aplicarlos mediante el paquete \texttt{igraph}. Gracias a los grafos podemos representar las relaciones que unen a los individuos en una red social, asi como las comunidades que forman e incluso la popularidad de cada uno, sus seguidores y a quien siguen.

\end{infobox_resume}

\hypertarget{part-ciencia-de-datos-espaciales}{%
\part{Ciencia de datos espaciales}\label{part-ciencia-de-datos-espaciales}}

\hypertarget{datos-espaciales}{%
\chapter{Trabajando con datos espaciales}\label{datos-espaciales}}

\emph{Gema Fernández-Avilés\footnote{Quisiera agradecer a Diego Hernangómez toda la ayuda prestada en la elaboración de este capítulo.}}

Los datos espaciales\index{datos espaciales}, también conocidos como datos geográficos\index{datos geográficos}, son aquellos datos relacionados o que contienen información de una localización o área geográfica de la superficie de la Tierra. El primer análisis de datos geoespaciales fue hecho por el médico John Snow\index{John Snow} en 1854. Éste produjo un famoso mapa que muestra las muertes causadas por un brote de cólera (que mató a 127 personas en 3 días en Soho, Londres) y la ubicación de las bombas de agua en el área (ver Fig. \ref{fig:snow-cholera-map-ch40}). Snow descubrió que había un agrupamiento significativo de muertes alrededor de una determinada bomba, y al quitar la manija de la bomba se detuvo el brote. Los datos con los que trabajó Snow y aquellos que contienen coordenadas son considerados datos espaciales. El área que se encarga de estudiar y analizar los datos espaciales es la \textbf{estadística espacial}.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/Snow-cholera-map} 

}

\caption{Mapa de cólrela en Londres según Snow. Fuente: Wikipedia}\label{fig:snow-cholera-map-ch40}
\end{figure}

El análisis espacial de Snow es considerado un caso típico de la ciencia de datos (\citet{Baumer_et_el_2021}): (i) la información clave se obtuvo mediante la combinación de tres fuentes de datos (las muertes por cólera, las ubicaciones de las bombas de agua y el mapa de calles de Londres); (ii) se puede crear un modelo espacial directamente a partir de los datos y (iii) el problema solo se resolvió cuando la evidencia basada en datos se combinó con un modelo plausible que explicaba el fenómeno físico. Es decir, Snow era médico y su conocimiento sobre la transmisión de enfermedades fue suficiente para convencer a sus colegas de que el cólera no se transmitía por el aire.

\hypertarget{conceptos-clave}{%
\section{Conceptos clave}\label{conceptos-clave}}

Visto el contexto original de los datos espaciales y antes de entrar en detalle en su análisis, se debe tener en cuenta una serie de conceptos clave. La Fig. \ref{fig:fig-accidentes1}, representa la localización de los accidentes de tráfico registrados en la ciudad de Madrid durante el año 2020. Sin embargo, la representación no aporta información útil para su análisis.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(CDR)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{ggplot}\NormalTok{(}
  \AttributeTok{data =}\NormalTok{ accidentes2020\_data,}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ coordenada\_x\_utm, }\AttributeTok{y =}\NormalTok{ coordenada\_y\_utm)}
\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/accidentes1} 

}

\caption{Accidentes de Tráfico en Madrid (2020)}\label{fig:fig-accidentes1}
\end{figure}

Ademas de las coordenadas\index{coordenadas}, en la representación de geodatos es importante el marco o contexto espacial, así como el conocimiento del (i) \textbf{Sistema de referencia de coordenadas}\index{Sistema de referencia de coordenadas} o Coordinate reference system\index{Coordinate reference system} (\textbf{CRS}) \index{CRS} en el que están proyectadas las coordenadas coordenadas y (ii) el tipo de datos con el que se está trabajando: vectores o ráster.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sf)}
\NormalTok{accidentes2020\_sf }\OtherTok{\textless{}{-}} \FunctionTok{st\_as\_sf}\NormalTok{(accidentes2020\_data,}
  \AttributeTok{coords =} \FunctionTok{c}\NormalTok{(}\StringTok{"coordenada\_x\_utm"}\NormalTok{, }\StringTok{"coordenada\_y\_utm"}\NormalTok{),}
  \AttributeTok{crs =} \DecValTok{25830} \CommentTok{\# proyección ETRS89/ UTM zone 30N}
\NormalTok{)}

\FunctionTok{library}\NormalTok{(mapSpain)}
\NormalTok{madrid }\OtherTok{\textless{}{-}} \FunctionTok{esp\_get\_munic}\NormalTok{(}\AttributeTok{munic =} \StringTok{"\^{}Madrid$"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{st\_transform}\NormalTok{(}\DecValTok{25830}\NormalTok{)}
\NormalTok{tile }\OtherTok{\textless{}{-}} \FunctionTok{esp\_getTiles}\NormalTok{(madrid, }\StringTok{"IDErioja"}\NormalTok{, }\AttributeTok{zoommin =} \DecValTok{2}\NormalTok{) }\CommentTok{\# Imagen}

\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
\NormalTok{  tidyterra}\SpecialCharTok{::}\FunctionTok{geom\_spatraster\_rgb}\NormalTok{(}\AttributeTok{data =}\NormalTok{ tile) }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ accidentes2020\_sf,}
    \AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{size =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.3}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/accidentes2} 

}

\caption{Accidentes de Tráfico en Madrid proyectados y con mapa de carreteras (2020)}\label{fig:fig-accidentes2}
\end{figure}

La Fig.\ref{fig:fig-accidentes2} permite observar ciertos patrones en la ocurrencia de accidentes. Por ejemplo, apenas se producen accidentes en la Casa de Campo o en el Monte del Pardo, y parece observarse cierta concentración en las autopistas de salida de la ciudad.

\hypertarget{CRS}{%
\subsection{Sistema de Referencia de Coordenadas}\label{CRS}}

Los CRS que permiten identificar con exactitud la posición de los datos sobre el globo terráqueo. Cuando se trabaja con datos espaciales provenientes de distintas fuentes de información es necesario comprobar que dichos datos se encuentran definidos en el mismo CRS. Ésto se consigue transformando (o proyectando) los datos a un CRS común.

En la Fig. \ref{fig:plot-puertos-error} se muestran los puertos en un mapa mundial. Todos los vienen representados por el punto rojo, ¿a qué se debe? A que los datos están en distintos CRS.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(giscoR)}
\FunctionTok{library}\NormalTok{(sf)}
\NormalTok{paises }\OtherTok{\textless{}{-}} \FunctionTok{gisco\_get\_countries}\NormalTok{()}
\NormalTok{puertos }\OtherTok{\textless{}{-}} \FunctionTok{gisco\_get\_ports}\NormalTok{()}
\NormalTok{paises\_robin }\OtherTok{\textless{}{-}} \FunctionTok{st\_transform}\NormalTok{(paises, }\FunctionTok{st\_crs}\NormalTok{(}\StringTok{"ESRI:54030"}\NormalTok{)) }\CommentTok{\#+proj=robin}

\FunctionTok{plot}\NormalTok{(}\FunctionTok{st\_geometry}\NormalTok{(paises\_robin), }\AttributeTok{main =} \StringTok{" "}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{st\_geometry}\NormalTok{(puertos), }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{col =} \StringTok{"2"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{20}\NormalTok{, }\AttributeTok{lwd =} \FloatTok{2.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/mapa-mundi-ko} 

}

\caption{Localización de los puertos en el mapa mundi (distinto CRS puertos y mapa)}\label{fig:plot-puertos-error}
\end{figure}

Los dos grandes tipos de CRS son:

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\item
  \textbf{Geográficos}\index{CRS geográficos}: aquellos en los que los parámetros empleados para localizar una posición espacial son la latitud (Norte-Sur {[}-90º,90º{]}) y la longitud (Este-Oeste {[}-180º,180º{]}). En este caso las distancias entre dos puntos representan \textbf{distancias angulares}\index{distancias angulares}.
\item
  \textbf{Proyectados}\index{CRS proyectados}: permiten reducir la superficie de la esfera terrestre (3D) a un sistema cartesiano (2D). Para ello, es necesario transformar las coordenadas longitud y latitud en coordenadas cartesianas \(X\) e \(Y\). La unidad de distancia, habitualmente, es el \textbf{metro}.
\end{enumerate}

Tras proyectar los puertos al mismo CRS que el mapa mundi (Robinson), la Fig. \ref{fig:plot-puertos-ok} muestra adecuadamente el mapa de la Fig. \ref{fig:plot-puertos-error}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{st\_crs}\NormalTok{(puertos) }\SpecialCharTok{==} \FunctionTok{st\_crs}\NormalTok{(paises\_robin) }\CommentTok{\# Comprueba CRS}
\CommentTok{\#\textgreater{} [1] FALSE}
\NormalTok{puertos\_robin }\OtherTok{\textless{}{-}} \FunctionTok{st\_transform}\NormalTok{(puertos, }\FunctionTok{st\_crs}\NormalTok{(paises\_robin))}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{st\_geometry}\NormalTok{(paises\_robin), }\AttributeTok{main =} \StringTok{" "}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{st\_geometry}\NormalTok{(puertos\_robin), }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{col =} \DecValTok{4}\NormalTok{, }\AttributeTok{pch =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/mapa-muni-ok} 

}

\caption{Localización de los puertos en el mapa mundi (mismo CRS puertos y mapa)}\label{fig:plot-puertos-ok}
\end{figure}

\textbf{¿Qué proyección uso?} El CRS adecuado para cada análisis depende de la localización y el rango espacial de los datos. El paquete \texttt{crsuggest} \citep{R-crsuggest} facilita la labor, sugiriendo el CRS más adecuado para cada zona.

\hypertarget{formatos}{%
\subsection{Formatos de datos espaciales}\label{formatos}}

En el ámbito del análisis espacial se pueden clasificar el formato de datos espaciales en función del modelo de datos . Se pueden distinguir dos tipos de modelos de datos \citep{Lovelance_et_al_2019}: vectores y ráster\footnote{Un análisis detallado puede verse en \href{https://mdsr-2122-visualizacion.netlify.app/}{\citet{hernangomez_fernandezaviles_2022}}}.

\hypertarget{vec}{%
\subsubsection{Datos de vectores}\label{vec}}

Este modelo\index{datos vector} está basado en puntos georeferenciados. Los \textbf{puntos} pueden representar localizaciones específicas, como la localización de los Hospitales y Centros de Salud de la ciudad de Toledo (Fig. \ref{fig:plot-puntos}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(sf)}
\FunctionTok{library}\NormalTok{(CDR)}

\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ hosp\_toledo,}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{fill =} \StringTok{"Hospitales y Centros Sanitarios"}\NormalTok{),}
    \AttributeTok{color =} \StringTok{"blue"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{" "}\NormalTok{, }\AttributeTok{fill =} \StringTok{" "}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"right"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\texttt{\{plot-puntos,\ echo=FALSE,\ fig.cap="Hospitales\ y\ Centros\ de\ Salud\ en\ Toledo"\}\ knitr::include\_graphics("img/centros-toledo.PNG")}

Los puntos también pueden estar conectados entre sí, de manera que formen geometrías más complejas, como \textbf{líneas} y \textbf{polígonos}.

En la Fig. \ref{fig:plot-lineas-pol} el río Tajo está representado como una línea (\texttt{tajo}, sucesión de puntos unidos entre sí) y la ciudad de Toledo como un polígono (\texttt{toledo}, línea de puntos cerrada formando un continuo).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(toledo) }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{fill =} \StringTok{"cornsilk2"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data =}\NormalTok{ tajo, }\AttributeTok{col =} \StringTok{"lightblue2"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.7}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data =}\NormalTok{ hosp\_toledo, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_sf}\NormalTok{(}\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{4.2}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{3.8}\NormalTok{), }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\FloatTok{39.8}\NormalTok{, }\FloatTok{39.95}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/rio-toledo} 

}

\caption{Datos vector: Puntos, líneas y polígonos}\label{fig:plot-lineas-pol}
\end{figure}

Las extensiones más habituales de los archivos que contienen datos de vectores se muestran a continuación:

\begin{longtable}[]{@{}ll@{}}
\caption{Ficheros con datos vector}\tabularnewline
\toprule()
Tipo & Extensión \\
\midrule()
\endfirsthead
\toprule()
Tipo & Extensión \\
\midrule()
\endhead
Shapefile & \texttt{.shp}, \texttt{.shx}, \texttt{.dbf} \\
GeoPackage vector & \texttt{.gpkg} \\
GeoJson & \texttt{.geojson} \\
GPX & \texttt{.gpx} \\
Geography Markup Language & \texttt{.gml} \\
Keyhole Markup Language & \texttt{.kml} \\
Otros & \texttt{.csv}, \texttt{.txt}, \texttt{xls} \\
\bottomrule()
\end{longtable}

\begin{infobox}
ESRI Shapefile surgió como uno de los primeros formatos de intercambio de datos geográficos y en la actualidad es quizá el formato más empleado. Sin embargo, tiene una serie de limitaciones: es un formato multiarchivo y el CRS es opcional.

\end{infobox}

\hypertarget{raster}{%
\subsubsection{Datos ráster}\label{raster}}

Los datos raster\index{datos raster} son datos proporcionados en una rejilla rectangular de píxeles regulares o no (denominada \textbf{matriz}). El caso más cotidiano de un ráster es una fotografía, donde la imagen se representa como una serie de celdas, determinadas por la resolución de la imagen (número total de píxeles) y el color que presenta cada uno de estos píxeles. En el ámbito de los datos espaciales, un archivo ráster está formado por una malla de píxeles georreferenciada, tal y como muestra la Fig. \ref{fig:plot-raster-to}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(terra)}
\FunctionTok{library}\NormalTok{(CDR)}
\FunctionTok{library}\NormalTok{(sf)}
\NormalTok{elev }\OtherTok{\textless{}{-}} \FunctionTok{rast}\NormalTok{(}\FunctionTok{system.file}\NormalTok{(}\StringTok{"external/Toledo\_DEM.asc"}\NormalTok{, }\AttributeTok{package =} \StringTok{"CDR"}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(elev, }\AttributeTok{main =} \StringTok{" "}\NormalTok{)}
\NormalTok{pols }\OtherTok{\textless{}{-}} \FunctionTok{as.polygons}\NormalTok{(elev)}
\FunctionTok{plot}\NormalTok{(pols, }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{border =} \StringTok{"grey90"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{st\_geometry}\NormalTok{(Tol\_prov), }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/elev-toledo} 

}

\caption{Datos ráster. Elevación de la provincia de Toledo}\label{fig:plot-raster-to}
\end{figure}

En la Fig. \ref{fig:plot-raster-to}, el objeto ráster \texttt{elev} tiene únicamente una capa. Eso implica que cada píxel tiene asociado un único valor, en este caso, la altitud media del terreno observado. Las extensiones más habituales de los archivos que contienen datos ráster se muestran a continuación:

\begin{longtable}[]{@{}ll@{}}
\caption{Ficheros con datos raster}\tabularnewline
\toprule()
Tipo & Extensión \\
\midrule()
\endfirsthead
\toprule()
Tipo & Extensión \\
\midrule()
\endhead
ASCII Grid & \texttt{.asc} \\
GeoTIFF & \texttt{.tif}, \texttt{.tiff} \\
Enhanced Compression Wavelet & \texttt{.ecw} \\
\bottomrule()
\end{longtable}

\hypertarget{primer-mapa}{%
\section{Mi primer mapa}\label{primer-mapa}}

Definidas las principales características de los datos espaciales se llevará a cabo la representación en un mapa\index{mapa} de la distribución municipal de la renta neta per cápita (\texttt{renta\_municipio\_data}) por municipio (\texttt{municipios}) en el periodo 2019 en España\footnote{Un análisis detallado puede verse en \citep{hernangomez_fernandezaviles_2022}}.

Ambos conjuntos deben tener, al menos, un campo en común, \texttt{codigo\_ine} en este caso, para su unión.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sf)}
\FunctionTok{library}\NormalTok{(CDR)}
\NormalTok{munis\_renta }\OtherTok{\textless{}{-}}\NormalTok{ municipios }\SpecialCharTok{|\textgreater{}}
  \CommentTok{\# unión de datasets}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{left\_join}\NormalTok{(renta\_municipio\_data) }\SpecialCharTok{|\textgreater{}}
  \CommentTok{\# selección de variables}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(name, cpro, cmun, }\StringTok{\textasciigrave{}}\AttributeTok{2019}\StringTok{\textasciigrave{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{ggplot}\NormalTok{(munis\_renta) }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill =} \StringTok{\textasciigrave{}}\AttributeTok{2019}\StringTok{\textasciigrave{}}\NormalTok{), }\AttributeTok{color =} \ConstantTok{NA}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_continuous}\NormalTok{(}
    \AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{label\_number}\NormalTok{(}
      \AttributeTok{big.mark =} \StringTok{"."}\NormalTok{, }\AttributeTok{decimal.mark =} \StringTok{","}\NormalTok{, }\AttributeTok{suffix =} \StringTok{" €"}
\NormalTok{    )}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/renta-ko} 

}

\caption{Distribución de la renta neta media por persona (€) en 2019}\label{fig:plot-renta-ko}
\end{figure}

La Fig. \ref{fig:plot-renta-ko} presenta una serie de elementos gráficos, característicos de los objetos espaciales, tal y como puede verse en la estructura del objeto \texttt{munis\_renta}: Los datos son de tipo vector, el tipo de geometría es MULTIPOLYGON, el CRS es ETRS89 y una leyenda explica el significado de la variable

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(munis\_renta)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{, ]}
\CommentTok{\#\textgreater{} Simple feature collection with 3 features and 4 fields}
\CommentTok{\#\textgreater{} Geometry type: MULTIPOLYGON}
\CommentTok{\#\textgreater{} Dimension:     XY}
\CommentTok{\#\textgreater{} Bounding box:  xmin: {-}3.140179 ymin: 36.73817 xmax: {-}2.741701 ymax: 37.24562}
\CommentTok{\#\textgreater{} Geodetic CRS:  ETRS89}
\CommentTok{\#\textgreater{}       name cpro cmun  2019                           geom}
\CommentTok{\#\textgreater{} 1     Abla   04  001 10192 MULTIPOLYGON ((({-}2.775594 3...}
\CommentTok{\#\textgreater{} 2 Abrucena   04  002 10021 MULTIPOLYGON ((({-}2.787566 3...}
\CommentTok{\#\textgreater{} 3     Adra   04  003  8192 MULTIPOLYGON ((({-}3.051988 3...}
\end{Highlighting}
\end{Shaded}

\hypertarget{no-mentir}{%
\section{¿Cómo (no) mentir con la visualización?}\label{no-mentir}}

Si se realiza un mapa de coropletas como el de la Fig. \ref{fig:plot-renta-ko} puede que la información quede distorsionada. Algunas consideraciones básicas en visualización son:

\begin{itemize}
\item
  La escala de color.
\item
  La distribución de los datos.
\item
  La definición de intervalos.
\end{itemize}

\textbf{¿Cómo es la distribución de la variable renta?} La variable no sigue una distribución Normal (la renta sigue una distribución Gamma) y será necesario dividir los datos en clases con el paquete \textbf{classInt} \citep{R-classint}. Se utiliza el método de Fisher-Jenks\index{Fisher-Jenks}, desarrollado específicamente para la clasificación de datos espaciales y su visualización en mapas. Además, se eliminan los municipios sin datos y se elige una escala de color adecuada. El mapa de la Fig. \ref{fig:plot-mapa-fisher} proporciona ahora una visualización adecuada de la variable renta.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sf)}
\CommentTok{\# crea etiquetas formateadas}
\NormalTok{label\_fun }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{  l }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(x)}
\NormalTok{  eur }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\FunctionTok{prettyNum}\NormalTok{(}\FunctionTok{round}\NormalTok{(x, }\DecValTok{0}\NormalTok{),}
    \AttributeTok{decimal.mark =} \StringTok{","}\NormalTok{, }\AttributeTok{big.mark =} \StringTok{"."}
\NormalTok{  ), }\StringTok{" €"}\NormalTok{)}
\NormalTok{  labels }\OtherTok{\textless{}{-}} \FunctionTok{paste}\NormalTok{(eur[}\SpecialCharTok{{-}}\NormalTok{l], }\StringTok{"{-}"}\NormalTok{, eur[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{])}
\NormalTok{  labels[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{paste}\NormalTok{(}\StringTok{"\textless{}"}\NormalTok{, eur[}\DecValTok{1}\NormalTok{])}
\NormalTok{  labels[l }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{paste}\NormalTok{(}\StringTok{"\textgreater{}"}\NormalTok{, eur[l }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{])}
  \FunctionTok{return}\NormalTok{(labels)}
\NormalTok{\}}

\FunctionTok{library}\NormalTok{(classInt)}
\NormalTok{munis\_renta\_clean }\OtherTok{\textless{}{-}}\NormalTok{ munis\_renta }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{2019}\StringTok{\textasciigrave{}}\NormalTok{))}

\CommentTok{\# crea Fisher{-}Jenks clases}
\NormalTok{fisher }\OtherTok{\textless{}{-}} \FunctionTok{classIntervals}\NormalTok{(munis\_renta\_clean}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{2019}\StringTok{\textasciigrave{}}\NormalTok{,}
  \AttributeTok{style =} \StringTok{"fisher"}\NormalTok{, }\AttributeTok{n =} \DecValTok{10}
\NormalTok{)}

\NormalTok{breaks\_f }\OtherTok{\textless{}{-}}\NormalTok{ fisher}\SpecialCharTok{$}\NormalTok{brks}
\NormalTok{labels\_f }\OtherTok{\textless{}{-}} \FunctionTok{label\_fun}\NormalTok{(breaks\_f)}

\NormalTok{munis\_renta\_clean}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Fisher{-}Jenks}\StringTok{\textasciigrave{}} \OtherTok{\textless{}{-}} \FunctionTok{cut}\NormalTok{(munis\_renta\_clean}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{2019}\StringTok{\textasciigrave{}}\NormalTok{,}
  \AttributeTok{breaks =}\NormalTok{ breaks\_f, }\AttributeTok{labels =}\NormalTok{ labels\_f, }\AttributeTok{include.lowest =} \ConstantTok{TRUE}
\NormalTok{)}

\CommentTok{\# dibuja renta municipal}
\FunctionTok{ggplot}\NormalTok{(munis\_renta\_clean) }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill =} \StringTok{\textasciigrave{}}\AttributeTok{Fisher{-}Jenks}\StringTok{\textasciigrave{}}\NormalTok{), }\AttributeTok{color =} \ConstantTok{NA}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{hcl.colors}\NormalTok{(}\FunctionTok{length}\NormalTok{(labels\_f),}
    \StringTok{"Inferno"}\NormalTok{,}
    \AttributeTok{rev =} \ConstantTok{TRUE}
\NormalTok{  )) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/renta-fj} 

}

\caption{Renta neta per cápita (€) por tramos según Fisher-Jenks}\label{fig:plot-mapa-fisher}
\end{figure}

\hypertarget{mapas-espacio-temporales}{%
\section{Mapas espacio-temporales}\label{mapas-espacio-temporales}}

La dimensión temporal es cada vez más importante en el ámbito espacial, por ello, es importante representar en el tiempo los procesos espaciales. La Fig. \ref{fig:plot-tmin-st} representa la temperatura mínima registrada en España del 6 al 10 de Enero de 2021, \texttt{tempmin\_data}, durante la \href{https://es.wikipedia.org/wiki/Borrasca_Filomena}{borrasca Filomena}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sf)}
\NormalTok{tmin\_sf }\OtherTok{\textless{}{-}} \FunctionTok{st\_as\_sf}\NormalTok{(tempmin\_data,}
  \AttributeTok{coords =} \FunctionTok{c}\NormalTok{(}\StringTok{"longitud"}\NormalTok{, }\StringTok{"latitud"}\NormalTok{),}
  \AttributeTok{crs =} \DecValTok{4326} \CommentTok{\# coordenadas geográficas longitud/latitud}
\NormalTok{)}
\FunctionTok{library}\NormalTok{(mapSpain)}
\NormalTok{esp }\OtherTok{\textless{}{-}} \FunctionTok{esp\_get\_ccaa}\NormalTok{() }\SpecialCharTok{|\textgreater{}} \CommentTok{\# sf objeto, contorno de España}
  \FunctionTok{filter}\NormalTok{(ine.ccaa.name }\SpecialCharTok{!=} \StringTok{"Canarias"}\NormalTok{) }\CommentTok{\# excluye Canarias}
\end{Highlighting}
\end{Shaded}

¿Tengo el CRS de las estaciones de monitoreo en la misma proyección que el contorno de España?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{st\_crs}\NormalTok{(tmin\_sf) }\SpecialCharTok{==} \FunctionTok{st\_crs}\NormalTok{(esp)}
\CommentTok{\#\textgreater{} [1] FALSE}
\NormalTok{esp2 }\OtherTok{\textless{}{-}} \FunctionTok{st\_transform}\NormalTok{(esp, }\FunctionTok{st\_crs}\NormalTok{(tmin\_sf))}
\FunctionTok{st\_crs}\NormalTok{(tmin\_sf) }\SpecialCharTok{==} \FunctionTok{st\_crs}\NormalTok{(esp2)}
\CommentTok{\#\textgreater{} [1] TRUE}
\end{Highlighting}
\end{Shaded}

La Fig. \ref{fig:plot-aemet} representa las estaciones de monitoreo.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{ggplot}\NormalTok{(esp2) }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data =}\NormalTok{ tmin\_sf) }\SpecialCharTok{+}
  \FunctionTok{theme\_light}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/ms-spain} 

}

\caption{Estaciones de AEMET en la Península Ibérica}\label{fig:plot-aemet}
\end{figure}

\begin{infobox}
\textbf{NOTA} Los paquetes \textbf{tmap} \citep{R-tmap} y \textbf{mapsf} \citep{R-mapsf} son referentes para mapas temáticos.

\end{infobox}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# definición de intervalos}
\NormalTok{cortes }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\ConstantTok{Inf}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{20}\NormalTok{, }\DecValTok{20}\NormalTok{, }\FloatTok{2.5}\NormalTok{), }\ConstantTok{Inf}\NormalTok{)}
\NormalTok{colores }\OtherTok{\textless{}{-}} \FunctionTok{hcl.colors}\NormalTok{(}\DecValTok{15}\NormalTok{, }\StringTok{"PuOr"}\NormalTok{, }\AttributeTok{rev =} \ConstantTok{TRUE}\NormalTok{)}

\NormalTok{tmin\_sf\_sptem }\OtherTok{\textless{}{-}}\NormalTok{ tmin\_sf }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(fecha, }\FunctionTok{desc}\NormalTok{(tmin))}

\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data =}\NormalTok{ esp2, }\AttributeTok{fill =} \StringTok{"grey95"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data =}\NormalTok{ tmin\_sf, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ tmin), }\AttributeTok{size =} \DecValTok{3}\NormalTok{, }\AttributeTok{alpha =}\NormalTok{ .}\DecValTok{7}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(fecha), }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{color =} \StringTok{"Temp. mím"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_gradientn}\NormalTok{(}
    \AttributeTok{colours =}\NormalTok{ colores,}
    \AttributeTok{breaks =}\NormalTok{ cortes,}
    \AttributeTok{labels =} \ControlFlowTok{function}\NormalTok{(x) \{}
      \FunctionTok{paste0}\NormalTok{(x, }\StringTok{"º"}\NormalTok{)}
\NormalTok{    \},}
    \AttributeTok{guide =} \StringTok{"legend"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/st-spain} 

}

\caption{Temperatura mínima en España (6-10 enero  2021)}\label{fig:plot-tmin-st}
\end{figure}

\hypertarget{mapas-interactivos}{%
\section{Mapas interactivos}\label{mapas-interactivos}}

El desarrollo de la informática ha propiciado también el desarrollo de la geocomputación y está permite, entre otras cosas, la representación de mapas interactivo.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(leaflet)}
\FunctionTok{library}\NormalTok{(isdas)}
\FunctionTok{data}\NormalTok{(}\StringTok{"snow\_deaths"}\NormalTok{)}
\FunctionTok{data}\NormalTok{(}\StringTok{"snow\_pumps"}\NormalTok{)}

\DocumentationTok{\#\# crea mapa interactivo}
\NormalTok{snow\_map }\OtherTok{\textless{}{-}} \FunctionTok{leaflet}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{setView}\NormalTok{(}\AttributeTok{lng =} \SpecialCharTok{{-}}\FloatTok{0.136}\NormalTok{, }\AttributeTok{lat =} \FloatTok{51.513}\NormalTok{, }\AttributeTok{zoom =} \DecValTok{16}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{addTiles}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{addMarkers}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ snow\_deaths, }\SpecialCharTok{\textasciitilde{}}\NormalTok{long, }\SpecialCharTok{\textasciitilde{}}\NormalTok{lat,}
    \AttributeTok{clusterOptions =} \FunctionTok{markerClusterOptions}\NormalTok{(),}
    \AttributeTok{group =} \StringTok{"Deaths"}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{addMarkers}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ snow\_pumps, }\SpecialCharTok{\textasciitilde{}}\NormalTok{long, }\SpecialCharTok{\textasciitilde{}}\NormalTok{lat,}
    \AttributeTok{group =} \StringTok{"Pumps"}
\NormalTok{  )}
\NormalTok{snow\_map}
\end{Highlighting}
\end{Shaded}

A modo de ejemplo, el mapa de la Fig. \ref{fig:snow-map-png-ch40} representa el mapa Fig. \ref{fig:snow-cholera-map-ch40} de forma interactiva con la librería \texttt{leaflef}. Éstos mapas dinámicos, ampliables y desplazables son más informativos que los mapas estáticos.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/snow_map} 

}

\caption{Mapa interactivo de las muertes por cólera en Londres según Snow}\label{fig:snow-map-png-ch40}
\end{figure}

\hypertarget{estaduxedstica-para-datos-espaciales}{%
\section{Estadística para datos espaciales}\label{estaduxedstica-para-datos-espaciales}}

Debido a que los datos espaciales surgen en una gran variedad de campos y aplicaciones, también hay una gran variedad de tipos de datos espaciales, estructuras y escenarios \citep[p.~6]{Schabenberger_Gotway_2005}. \citet{cressie1993} proporciona una clasificación de datos espaciales basada en la naturaleza del dominio espacial en estudio y distingue: datos geoestadísticos\index{datos geoestadísticos}, datos de patrones de puntos\index{datos de patrones de puntos} y datos lattice o reticulares\index{datos lattice} \index{datos reticulares}. La Fig. \ref{fig:hengl-cressie} ilustra los distintos procesos espaciales que se abordarán en los capítulos XXXX.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/cressie_simulados} 

}

\caption{Clasificación de datos espaciales propuesta por Cressie (1993)}\label{fig:hengl-cressie}
\end{figure}

\begin{infobox_resume}
\textbf{RESUMEN}

Los datos espaciales son aquellos que contienen información de una zona geográfica de la tierra. Éstos vienen definidos por coordenadas y por un sistema de referencia de coordenadas que debe tenerse en cuenta para su representación.

Existen dos tipos de formatos de datos: vector y ráster.

Los datos espaciales pueden clasificarse en: geoestadísticos, reticulares y puntuales.

\end{infobox_resume}

\hypertarget{geo}{%
\chapter{Geoestadística}\label{geo}}

\emph{Gema Fernández-Avilés y José-María Montero}

\hypertarget{introducciuxf3n-23}{%
\section{Introducción}\label{introducciuxf3n-23}}

El término ``geoestadística'' apareció por primera vez en \citet{Matheron1962}, y en él ``geo'' enfatiza la referencia a las Ciencias de la Tierra, mientras que ``estadística'' se refiere al uso de métodos probabilísticos-inferenciales.

La geoestadística estudia de fenómenos regionalizados, que son aquellos que:

\begin{itemize}
\tightlist
\item
  Se extienden en el espacio, siendo el dominio espacial, \(D\), continuo (se puede observar en cualquiera de sus puntos) y fijo (las ubicaciones observadas no son estocásticas; se seleccionan, por el procedimiento que sea, a juicio del investigador)\footnote{Los datos geoestadísticos son tan solo una parte de los datos espaciales: Otra parte de ellos, son los datos ``laticce'' o regionales, donde \(D\) es discreto (códigos postales, provincias, regiones, países\ldots) y las ubicaciones observadas no son estocásticas. De su estudio se suele encargar la econometría espacial. También hay otro tipo de datos espaciales que surgen en dominios que pueden ser continuos o discretos, pero donde la selección de las ubicaciones observadas no depende del investigador (en este sentido \(D\) es aleatorio). Se trata de los denominados procesos de punto.}.
\end{itemize}

\begin{itemize}
\tightlist
\item
  Presentan una organización o estructura debida a la dependencia espacial existente.
\end{itemize}

El objetivo fundamental de la Geoestadística es sacar provecho de la dependencia espacial existente para llevar a cabo predicciones (interpolaciones) óptimas en ubicaciones o áreas de interés (en este sentido se habla de predicciones puntuales o por bloques, respectivamente), o la realización de mappings sobre todo el dominio o parte de él. Al ser \(D\) continuo no se puede hacer una representación exhaustiva del fenómeno y pero sí, partir de las observaciones disponibles, reconstruirlo.

Las consecuencias de utilizar la estadística clásica, que no considera la dependencia espacial, cuando la hay, son muy graves y pueden verse en \citet{montero_et_al2015}.

El ámbito de aplicación de la geoestadística es enorme: Minería, industria petrolífera, geología, meteorología, control de la calidad del aire, ecología, epidemiología, salud pública, criminología, economía\ldots{} Así, por ejemplo, en el ámbito del control de la calidad del aire en las grandes urbes, la concentración de ozono en aire se mide en una serie de estaciones de seguimiento, y a partir de dichas mediciones se reproduce el comportamiento del proceso sobre toda la urbe.

En conclusión, las dos partes del análisis geoestadístico son: el análisis estructural de la dependencia espacial y la predicción (que se suele acompañar del calificativo ``\emph{krigeada}''. Pero antes de estudiarlas, detengámonos en algunos preliminares.

\hypertarget{preliminares-geo}{%
\section{Preliminares}\label{preliminares-geo}}

Dado que los procedimientos geoestadísticos no pueden ser aplicados directamente sobre los fenómenos regionalizados como tales, porque son realidades físicas, se necesita una descripción matemática de los mismos a la que puedan ser aplicados: la \emph{variable regionalizada} (\emph{v.r.}) o \emph{regionalización}, definida en un espacio geográfico, y que se supone que mide y representa correctamente dicho fenómeno.

Formalmente, cuando \(\mathbf{s}\) recorre \(D\), el conjunto \(z(\mathbf{s}), \mathbf{s}\in D,\) se denomina \emph{v.r.}, siendo \(z(\mathbf{s_i}), i=1,2,3,...\) una colección de valores regionalizados.

Desde la perspectiva probabilística, cada uno de los valores que toma \emph{v.r.} puede interpretarse como el resultado de un mecanismo aleatorio, la variable aleatoria (\emph{v.a.}). Si se toman valores regionalizados en todos los puntos del dominio, \(D\), es decir, si se considera \emph{v.r.}, ésta puede ser vista como un conjunto infinitamente grande de \emph{v.a.}, una en cada punto de \(D\), que se conoce como \emph{función aleatoria (f.a.),} \emph{proceso estocástico} o \emph{campo aleatorio} espacial, \(Z(\mathbf{s}), \mathbf{s}\in D\), donde \(Z\) representa el fenómeno de interés. Pues bien, \emph{v.r.} se interpreta como una realización de una \emph{f.a.} espacial, y esta es una decisión metodológica clave en geoestadística.

Es importante tener en cuenta que, \((i)\) frecuentemente, \emph{v.r.} es muy irregular a escala local, lo que impide su representación mediante una función determinista; y \((ii)\) muestra cierta organización o estructura espacial. La interpretación de \emph{v.r.} como una realización de una \emph{f.a.} espacial permite considerar estos dos aspectos:

\begin{itemize}
\item
  En cada localización \(\mathbf{s}\), \(Z(\mathbf{s})\) es una \emph{v.a.} (de ahí el aspecto errático).
\item
  Para un conjunto de puntos dado, \(\mathbf{s_1},\mathbf{s_2}, ..., \mathbf{s_k}\), las \emph{v.a.} \(Z(\mathbf{s_1}), Z(\mathbf{s_2}), ..., Z(\mathbf{s_k})\) están ligadas por una red de correlaciones espaciales que son las responsables de la similitud en los valores que toman (de ahí el aspecto estructurado).
\end{itemize}

Las \emph{f.a.} \(Z(\mathbf{s})\) pueden ser estacionarias (en sentido estricto o de segundo orden), intrínsecamente estacionarias o no estacionarias, y el hecho de que tengan uno u otro tipo de estacionariedad determina el analísis geoestadístico.

Una \emph{f.a.} espacial es \emph{estrictamente estacionaria} si las familias de \emph{v.a.} \(Z(\mathbf{s}_1),Z(\mathbf{s}_2),...,Z(\mathbf{s}_k)\), tienen la misma distribución de probabilidad conjunta que \(Z(\mathbf{s}_1+\mathbf{h}),Z(\mathbf{s}_2+\mathbf{h}),...,Z(\mathbf{s}_k+\mathbf{h})\), \(\forall k\), \(\forall \mathbf{s}_1, \mathbf{s}_2,...,\mathbf{s}_k\) y \(\forall \mathbf{h} \in \mathbb{R}^d\) (donde \(\mathbf{h}\) es un vector de traslación), siempre que \(\mathbf{s}_1+\mathbf{h}, \mathbf{s}_2+\mathbf{h},...,\mathbf{s}_k+\mathbf{h}\in D\).
Es decir, la distribución de probabilidad conjunta de \(Z(\mathbf{s}_1+\mathbf{h}),Z(\mathbf{s}_2+\mathbf{h}),...,Z(\mathbf{s}_k+\mathbf{h})\) no se ve afectada por una traslación \(\mathbf{h}\), y por tanto, ni ella, ni las funciones de densidad de dimensión inferior a \(k\), dependen de las localizaciones consideradas.

La estacionariedad estricta es una condición muy restrictiva. Por ello, en la practica lo que se suele asumir es la \emph{estacionariedad de segundo orden}, que limita la estacionariedad a los dos primeros momentos de la \emph{f.a.}\footnote{En geoestadística lineal el interés se centra en los dos primeros momentos de la \emph{f.a.}, por cual tan solo es necesaria la estacionariedad de segundo orden. Es decir, la esperanza y la varianza existen, son constantes y no dependen de la localización \(\mathbf{s}\). La covarianza existe para cada par de v.a \(Z(\mathbf{s})\) y \(Z(\mathbf{s}+\mathbf{h})\) y solo depende de \(\mathbf{h}\).}

Si una \emph{f.a.} es estrictamente estacionaria, también es estacionaria de segundo orden. Sin embargo, la relación inversa no tiene por qué ser cierta.

La estacionariedad de segundo orden implica la existencia de la varianza de la \emph{f.a.}, y deja fuera los fenómenos con infinita capacidad de variación. En este caso, si las diferencias \(Z(\mathbf{s}+\mathbf{h})-Z(\mathbf{s})\) son estacionarias de segundo orden, se dice que la \emph{f.a.} es \emph{intrínsecamente estacionaria}.

Aquellas \emph{f.a.} cuya esperanza y/o varianza dependan de la localización (no son invariantes a las traslaciones) se denominan \emph{no estacionarias}.

Salvo indicación de lo contrario, se asumirá la estacionariedad de segundo orden.

Finalmente, unos breves comentarios sobre la importancia de la estacionariedad. Es imposible inferir la ley de probabilidad que gobierna la \emph{f.a.} espacial a partir de una sola realización de la misma (una sola regionalización), pues sería como tener una muestra de tamaño 1. Pero en la práctica ese será el caso. Bueno, ni siquiera eso. Solo se dispondrá de una parte de la regionalización: la correspondiente a las localizaciones observadas. La solución a tal importantísima limitación es adoptar la hipótesis de estacionariedad u homogeneidad espacial. Es decir, sustituir la repetición de realizaciones de la \emph{f.a.} espacial por repeticiones en el espacio; es decir, suponer que los valores observados en distintas localizaciones de \(D\) tienen las mismas características estadísticas y pueden ser considerados, en términos estadísticos, como realizaciones de la misma \emph{f.a.}\footnote{Estas realizaciones no son independientes y se suele asumir también la hipótesis de ergodicidad.} Por tanto, la hipótesis de estacionariedad significa que la ley espacial que gobierna \emph{f.a.}, o parte de ella, es invariante a traslaciones; no depende de las localizaciones específicas observadas sino solo de \(\mathbf{h}\).

La hipótesis de estacionariedad permitirá actuar como si todas las \emph{v.a.} que conforman la \emph{f.a.} tuviesen la misma distribución de probabilidad (o los mismos momentos), haciendo posible el proceso inferencial. Por eso se le da tanta importancia a que la \emph{f.a.} sea estacionaria, del tipo que sea.

\hypertarget{ana-estructural}{%
\section{Análisis estructural de la dependencia espacial}\label{ana-estructural}}

\hypertarget{semivariograma}{%
\subsection{Semivariograma}\label{semivariograma}}

La estadística espacial se basa en la suposición de que las unidades georeferenciadas cercanas están relacionadas (son dependientes) de alguna manera \citep{getis1999}, y tanto más cuanto más cercanas estén \citep{tobler1970}.

Los procesos con dependencia espacial se reconocen, visualmente, porque muestran un patrón en el espacio; en los que no la tienen, el patrón es el de la aleatoriedad. La Fig. \ref{fig:plot-dep-iid-ch41} muestra una simulación de una \emph{f.a.} con dependencia espacial (panel izquierdo) frente a unos datos totalmente aleatorios (panel derecho).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(geoR)}
\FunctionTok{library}\NormalTok{(fields)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{728}\NormalTok{)}

\NormalTok{sim\_dep }\OtherTok{\textless{}{-}} \FunctionTok{grf}\NormalTok{(}\DecValTok{401}\NormalTok{, }\AttributeTok{grid =} \StringTok{"reg"}\NormalTok{, }\AttributeTok{cov.pars =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.8}\NormalTok{), }\AttributeTok{messages =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{points.geodata}\NormalTok{(sim\_dep,}
  \AttributeTok{main =} \StringTok{"Dependencia espacial"}\NormalTok{,}
  \AttributeTok{col =} \FunctionTok{tim.colors}\NormalTok{(), }\AttributeTok{cex.max =} \DecValTok{2}
\NormalTok{)}

\NormalTok{sim\_indep }\OtherTok{\textless{}{-}} \FunctionTok{grf}\NormalTok{(}\DecValTok{401}\NormalTok{, }\AttributeTok{grid =} \StringTok{"reg"}\NormalTok{, }\AttributeTok{cov.pars =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\DecValTok{0}\NormalTok{), }\AttributeTok{messages =} \ConstantTok{FALSE}\NormalTok{)}
\FunctionTok{points.geodata}\NormalTok{(sim\_indep,}
  \AttributeTok{main =} \StringTok{"Aleatoriedad"}\NormalTok{,}
  \AttributeTok{col =} \FunctionTok{tim.colors}\NormalTok{(), }\AttributeTok{cex.max =} \DecValTok{2}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/dep-aleatoriedad} 

}

\caption{Dependencia espacial frente a aleatoriedad}\label{fig:plot-dep-iid-ch41}
\end{figure}

Pasando del terreno de las simulaciones a la realidad, la Fig. \ref{fig:plot-temp-max-ch41} muestra la temperatura máxima en España el 6 de agosto de 2022\footnote{Los datos ya procesados para el análisis se encuentran en \texttt{CDR::tempmax\_data}. Una descripción puede verse con \texttt{summary(CDR::tempmax\_data)}. Estos datos han sido descargados con la librería \texttt{climaemet} y \texttt{mapSpain}. Un desarrollo completo de manipulación de datos espaciales puede verse en \citet{R-climaemet}.}, en plena ola de calor (este es el ejemplo real que se utilizará a lo largo del capítulo). En ella puede observarse claramente una estructura de dependencia espacial, con máximas cercanas a 40 grados en la meseta central y Extremadura, de 30 grados o menos en la cordillera cantábrica y las costas atlántica, cantábrica y andaluza, y entre 30 y 35 grados en el resto del país (básicamente Murcia, Comunidad Valenciana y Cataluña).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(CDR)}
\CommentTok{\# summary(CDR::tempmax\_data)}

\CommentTok{\# renombra objetos por simplicidad en el análisis}
\NormalTok{ESP }\OtherTok{\textless{}{-}}\NormalTok{ tempmax\_data}\SpecialCharTok{$}\NormalTok{ESP}
\NormalTok{ESP\_utm }\OtherTok{\textless{}{-}}\NormalTok{ tempmax\_data}\SpecialCharTok{$}\NormalTok{ESP\_utm}
\NormalTok{grd\_sf }\OtherTok{\textless{}{-}}\NormalTok{ tempmax\_data}\SpecialCharTok{$}\NormalTok{grd\_sf}
\NormalTok{grd\_sp }\OtherTok{\textless{}{-}}\NormalTok{ tempmax\_data}\SpecialCharTok{$}\NormalTok{grd\_sp}
\NormalTok{temp\_max\_utm\_sf }\OtherTok{\textless{}{-}}\NormalTok{ tempmax\_data}\SpecialCharTok{$}\NormalTok{temp\_max\_utm\_sf}
\NormalTok{temp\_max\_utm\_sp }\OtherTok{\textless{}{-}}\NormalTok{ tempmax\_data}\SpecialCharTok{$}\NormalTok{temp\_max\_utm\_sp}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\NormalTok{br\_paper }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\ConstantTok{Inf}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\FloatTok{17.5}\NormalTok{, }\DecValTok{45}\NormalTok{, }\FloatTok{2.5}\NormalTok{), }\ConstantTok{Inf}\NormalTok{)}
\NormalTok{pal\_paper }\OtherTok{\textless{}{-}} \FunctionTok{hcl.colors}\NormalTok{(}\DecValTok{15}\NormalTok{, }\StringTok{"YlOrRd"}\NormalTok{, }\AttributeTok{rev =} \ConstantTok{TRUE}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(ESP\_utm) }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data =}\NormalTok{ temp\_max\_utm\_sf, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{col =}\NormalTok{ tmax), }\AttributeTok{size =} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# temp\_max\_utm}
  \FunctionTok{theme\_light}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_gradientn}\NormalTok{(}\AttributeTok{colours =}\NormalTok{ pal\_paper)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/tem-max-spain} 

}

\caption{Temperatura máxima en España peninsular, 6 de agosto de 2022}\label{fig:plot-temp-max-ch41}
\end{figure}

Ahora bien, para poder llevar a cabo predicciones geoestadísticas es necesario representar los patrones de dependencia espacial observados mediante funciones que indiquen cuál es la estructura de la dependencia espacial, para ser utilizadas en el proceso de predicción. Dichas funciones son los semivariogramas. Dado que la identificación de la estructura de la dependencia espacial existente en el fenómeno de interés es la clave del éxito del proceso predictivo, al semivariograma\index{semivariograma} se le considera la piedra angular de la predicción geoestadística \citep{montero_et_al2015}.

Un semivariograma se define como la semivarianza de los incrementos de la \emph{f.a.}: \begin{equation} \label{eq:vario} 
\gamma(\mathbf{s}_i - \mathbf{s}_j) = \frac{1}{2} V[Z(\mathbf{s}_i)-Z(\mathbf{s}_j)] , \forall \mathbf{s}_i, \mathbf{s}_j\in D.
\end{equation}
que, en el caso habitual de \emph{f.a.} estacionarias de segundo orden o intrínsecamente estacionarias (sin deriva), se transforma en:
\begin{equation}
\gamma (\mathbf{h}) = \frac{1}{2} V \left( Z(\mathbf{s} + \mathbf{h}) - Z(\mathbf{s}) \right) =\frac{1}{2}E\left( {\left( {Z(\mathbf{s} + \mathbf{h}) - Z(\mathbf{s})} \right)^2} \right),
\end{equation}

Nótese que:

\begin{itemize}
\tightlist
\item
  Si hay dependencia espacial (positiva\footnote{Valores cercanos similares. Si es negativa, los valores vecinos son diferentes.}, es lo normal), la diferencia entre los valores de la \emph{f.a.} en los puntos separados por una pequeña distancia será poca y más o menos la misma, es decir, dichas diferencias serán poco variables, y el valor del semivariograma, a pequeñas distancias, será pequeño.
\end{itemize}

\begin{itemize}
\item
  Si aumenta la distancia, la dependencia espacial se reduce y la diferencia entre los valores de la \emph{f.a} en los puntos separados por distancias intermedias y grandes no será tan parecida como en el caso anterior, sino mayor; y variará más: Es decir, el valor del semivariograma aumenta con la distancia.
\item
  Si la distancia aumenta lo suficiente como para que ya no haya dependencia espacial, las diferencias entre los valores de la \emph{f.a.} separados por tal distancia alcanzarán la variabilidad de la \emph{f.a} en estudio, y si ésta es estacionaria de segundo orden, el semivariograma se estabilizará en torno a ella.
\end{itemize}

En el caso estacionario, las funciones de covarianza también pueden ser utilizadas para representar la estructura de la dependencia espacial, si bien se prefiere el semivariograma porque no requiere el conocimiento de la media de la \emph{f.a.} en estudio. Además, el semivariograma cubre un espectro más amplio de fenómenos regionalizados \index{Regionalización} que la función de covarianza, ya que ésta no puede definirse en el caso de estacionariedad intrínseca. Los detalles pueden verse en \citet{montero_et_al2015} y \citet{montero_larraz2008}.

Cuando el semivariograma depende tanto de la dirección como de la longitud del vector \(\mathbf{h}\) que une las localizaciones \(\mathbf{s}\) y \(\mathbf{s}+ \mathbf{h}\), se denomina \emph{anisotrópico}. Cuando solo depende de la distancia (\(\lvert \mathbf{h} \rvert=|| \mathbf{h} ||\), porque en el espacio euclídeo el módulo y la norma coinciden) se denomina \emph{isotrópico}.

Un semivariograma no puede ser cualquier función. Tiene que ser nulo en el origen, no negativo, verificar que \(\gamma(\mathbf{h})=\gamma(-\mathbf{h})\), debe ser una función condicionalmente definida negativa y tener un ritmo de crecimiento inferior a \(|\mathbf h|^2\), es decir, \(lim_{|\mathbf h|\to\infty} \frac {\gamma(\mathbf{h})}{|\mathbf h|^2}=0\) cuando el proceso es no estacionario (sin deriva), siendo finito en caso de procesos estacionarios de segundo orden.

El análisis del comportamiento de un semivariograma a pequeñas, medias y grandes distancias es de sumo interés.

En general, a distancias medias y grandes, los semivariogramas asociados a \emph{f.a.} estacionarias de segundo orden crecen, monótonamente, desde el origen con la distancia, hasta alcanzar un valor límite, la varianza a priori de la \emph{f.a.} (o covarianza para \(\mathbf {h}=0\), \(C(\mathbf{0})\)), bien de forma exacta o asintóticamente. Dicho valor límite se denomina \emph{meseta}, \(m\), y la distancia a la cual se alcanza se conoce como \emph{alcance} o \emph{rango}, \(a^\prime\). Por tanto, el rango es la distancia a partir de la cual ya no hay dependencia espacial. Cuando \(m\) se alcanza asintóticamente, el alcance no queda perfectamente definido y se toma como alcance, a efectos prácticos, \(a^\prime\), la distancia a la cual el semivariograma toma el valor \(0,95m\).

En el caso no estacionario (por ejemplo, si hay deriva) o intrínsecamente estacionario el semivariograma no tiene meseta.

El comportamiento a pequeñas distancias, sobre todo cerca del origen, que es donde más dependencia espacial hay, está muy relacionado con el grado de continuidad y regularidad de \emph{f.a.} Cuanto más continua y regular sea, más suaves y estructuradas serán las realizaciones que produzca, y más regular será el comportamiento del semivariograma cerca del origen (Fig. \ref{fig:gauss-sph-plot}, panel izquierdo; representación bidimensional).

Los semivariogramas con un comportamiento lineal cerca del origen son típicos de \emph{v.r.} continuas, al menos por partes, pero no diferenciables. Su representación gráfica tridemensional está llena de picos. La amplitud de las fluctuaciones aumenta con la distancia entre localizaciones y es proporcional a la pendiente de la tangente en el origen (Fig. \ref{fig:gauss-sph-plot}, panel derecho; representación bidimensional).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(fields)}
\FunctionTok{library}\NormalTok{(geoR)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\NormalTok{fa\_gauss }\OtherTok{\textless{}{-}} \FunctionTok{grf}\NormalTok{(}\DecValTok{1225}\NormalTok{, }\AttributeTok{grid =} \StringTok{"reg"}\NormalTok{, }\AttributeTok{cov.pars =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, .}\DecValTok{25}\NormalTok{), }\AttributeTok{cov.model =} \StringTok{"gaussian"}\NormalTok{)}
\FunctionTok{image}\NormalTok{(fa\_gauss, }\AttributeTok{col =} \FunctionTok{tim.colors}\NormalTok{())}

\NormalTok{fa\_sph }\OtherTok{\textless{}{-}} \FunctionTok{grf}\NormalTok{(}\DecValTok{1225}\NormalTok{, }\AttributeTok{grid =} \StringTok{"reg"}\NormalTok{, }\AttributeTok{cov.pars =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, .}\DecValTok{25}\NormalTok{), }\AttributeTok{cov.model =} \StringTok{"spherical"}\NormalTok{)}
\FunctionTok{image}\NormalTok{(fa\_sph, }\AttributeTok{col =} \FunctionTok{tim.colors}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/gauss-sph} 

}

\caption{Representacion bidimensional de dos f.a.: Con semivariograma parabólico en el origen (izquierda); con semivariograma lineal en el origen (derecha)}\label{fig:gauss-sph-plot}
\end{figure}

Las \emph{v.r.} regulares (aquellas cuya gráfica tridimensional no tiene picos) se identifican con un comportamiento semivariográfico parabólico en el origen. Si dicho comportamiento persiste a largas distancias, puede que exista una fuerte deriva.

Las discontinuidades en el origen (que teóricamente no pueden darse) son frecuentes en la práctica. Su amplitud se denomina ``efecto pepita'' (nugget effect) y son típicas de variables regionalizadas muy irregulares y, quizás, discontinuas. Las causas más frecuentes del ``efecto pepita'' son la existencia de una estructura con alcance inferior a la distancia más corta entre localizaciones y los errores de posicionamiento o de medida (véase \citet{chiles1999} para más detalle).

El caso límite del efecto pepita es el ``efecto pepita puro''. En ese caso, el semivariograma es constante cualquiera que sea la distancia, indicando ausencia de dependencia espacial.

La Fig. \ref{fig:plot-partes-sem} muestra gráficamente los principales elementos de un semivariograma.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(geoR)}
\NormalTok{semivar }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, ...) \{}
  \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{cov.spatial}\NormalTok{(x, ...)}
\NormalTok{\}}
\FunctionTok{curve}\NormalTok{(}\FunctionTok{semivar}\NormalTok{(x, }\AttributeTok{cov.pars =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.4}\NormalTok{), }\AttributeTok{cov.model =} \StringTok{"sph"}\NormalTok{), }\FloatTok{0.0}\NormalTok{, }\DecValTok{1}\NormalTok{,}
  \AttributeTok{xlab =} \StringTok{"Distancia"}\NormalTok{,}
  \AttributeTok{ylab =} \FunctionTok{expression}\NormalTok{(}\FunctionTok{bold}\NormalTok{(}\FunctionTok{gamma}\NormalTok{(}\StringTok{"|h|"}\NormalTok{))), }\AttributeTok{lwd =} \DecValTok{4}\NormalTok{, }\AttributeTok{lty =} \DecValTok{1}\NormalTok{, }\AttributeTok{col =} \StringTok{"4"}\NormalTok{, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.03}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{col =} \DecValTok{2}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{) }\CommentTok{\# alcance}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \DecValTok{1}\NormalTok{, }\AttributeTok{col =} \DecValTok{3}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{) }\CommentTok{\# meseta}
\FunctionTok{legend}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.15}\NormalTok{, }\StringTok{"Efecto pepita"}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.95}\NormalTok{, }\StringTok{"Meseta"}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\StringTok{"Alcance"}\NormalTok{)}
\FunctionTok{legend}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.75}\NormalTok{, }\StringTok{"Ausencia de dependencia"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\StringTok{"img/semivar{-}parts.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/semivar-parts} 

}

\caption{Elementos del semivariograma (meseta unitaria)}\label{fig:plot-partes-sem}
\end{figure}

\hypertarget{modelos-de-semivariogramas-vuxe1lidos}{%
\subsection{Modelos de semivariogramas válidos}\label{modelos-de-semivariogramas-vuxe1lidos}}

Las funciones que verifican las condiciones que debe cumplir un semivariograma (véase \ref{semivariograma}) se conocen como semivariogramas válidos. El incumplimiento de alguna de ellas tiene perversas consecuencias en el proceso predictivo (por ejemplo, varianzas de los errores de predicción negativas). Su tipología, siguiendo el enfoque sugerido en \citet{journel1978}\}\index{semivariograma!isotrópico} y \citet{montero_et_al2015} es la siguiente:\footnote{Aquí se presentan los semivariogramas isotrópicos más utilizados en la práctica. Nos centramos en ellos porque los semivariogramas anisotrópicos pueden ser representados por semivariogramas isotrópicos sin más que llevar a cabo una transformación lineal de las coordenadas, y porque las anisotropías pueden representarse separadamente mediante semivariogramas isotrópicos. Para un análisis detallado, consúltese, por ejemplo, \citet{montero_et_al2015}.}

\hypertarget{semivariogramas-con-meseta}{%
\subsubsection{Semivariogramas con meseta}\label{semivariogramas-con-meseta}}

Están asociados a \emph{f.a.} estacionarias de segundo orden. Los más utilizados son:

\begin{itemize}
\tightlist
\item
  \textbf{Esférico.} \index{semivariograma!esférico} Válido en \(\mathbb{R}^1\), \(\mathbb{R}^2\) y
  \(\mathbb{R}^3\), y viene dado por:
  \begin{equation} 
  \gamma(|\mathbf{h}|)= \left\{
  \begin{array}{ll}
  m \left( 1.5\frac{|\mathbf{h}|}{a}-0.5\left(\frac{|\mathbf{h}|}{a}\right)^{3} \right) & \mbox{si $|\mathbf{h}|\leq a$} \\
  m & \mbox{si $|\mathbf{h}| > a$}
  \end{array},
  \right.
  \end{equation}
\end{itemize}

Tiene un comportamiento lineal cerca del origen, indicando continuidad y cierto grado de irregularidad en la \emph{f.a.} A grandes distancias alcanza la meseta cuando \(|\mathbf{h}|=a\). Estas dos características son propias de muchas regionalizaciones observadas en la realidad; de ahí su popularidad.

\begin{itemize}
\tightlist
\item
  \textbf{Exponencial. } \index{Semivariograma!exponencial} Válido en \(\mathbb{R}^d, d\geq1\) y viene dado por:
  \begin{equation} 
  \gamma (|\mathbf{h}|)= m \left( 1-\exp \left( -\frac{|\mathbf{h}|}{a}
  \right) \right).
  \end{equation}
\end{itemize}

Igual que el esférico, cerca del origen exhibe un comportamiento lineal, siendo menor la pendiente. A diferencia de él, solo alcanza la meseta asintóticamente. A efectos prácticos se toma como alcance la distancia para la cual el semivariograma alcanza el valor del 95\% de la meseta, \(a^\prime \approx 3a\).

\begin{itemize}
\tightlist
\item
  \textbf{Gausiano.} \index{Semivariograma!gausiano} Válido en \(\mathbb{R}^d, d \geq1\). Está definido por:
  \begin{equation}   
  \gamma (|\mathbf{h}|)=m\left( 1-\exp \left(-\frac{|\mathbf{h}|^{2}}{a^{2}}\right) \right).
  \end{equation}
\end{itemize}

A diferencia del esférico y el exponencial, tiene un comportamiento parabólico cerca del origen. Por consiguiente, está asociado con \emph{f.a.} estacionarias de segundo orden infinitamente diferenciables y, en consecuencia, muy regulares. Igual que el modelo exponencial, alcanza la meseta sólo asintóticamente, con \(a^\prime \approx a \sqrt 3\).

\begin{itemize}
\tightlist
\item
  \textbf{Efecto pepita puro.} \index{Semivariograma!nugget} Refleja la ausencia de dependencia espacial:
\end{itemize}

\begin{equation}
\gamma(|\mathbf{h}|) = \left\{ {\begin{array}{l}
 m\;\;\mbox{si}\;|\mathbf{h}| = 0 \\
 0\;\;\mbox{ si}\;|\mathbf{h}| > 0 \\
 \end{array}}, \right.\; \;\;\; m> 0.
\end{equation}

\begin{itemize}
\tightlist
\item
  \textbf{K-Bessel}. Válido in \(\mathbb{R}^d, d\geq1\). Su expresión es:
  \begin{equation}
  \gamma(|\mathbf{h}|)=m \left(1- \frac{1}{2^{\alpha-1} \Gamma(\alpha)}
  \left(\frac{|\mathbf{h}|}{a} \right)^\alpha K_\alpha \left(\frac{|\mathbf{h}|}{a} \right) \right), \quad \alpha >0,
  \end{equation} donde \(K_\alpha\) es la función de segunda especie de orden \(\alpha\). Este modelo es que puede representar cualquier tipo de comportamiento cerca del origen. Por ejemplo, para \(\alpha=0,5\) se obtiene el modelo exponencial.
\end{itemize}

\hypertarget{semivariogramas-con-meseta-y-efecto-de-hoyo}{%
\subsubsection{Semivariogramas con meseta y efecto de hoyo}\label{semivariogramas-con-meseta-y-efecto-de-hoyo}}

\textbf{J-Bessel}. Un semivariograma no tiene por qué ser necesariamente una función monótona no decreciente, sino que puede tener ``ondas'' (efecto hoyo). Tal es el caso del modelo J-Bessel, que puede ser utilizado en presencia de dependencia espacial negativa o, específicamente, en caso de alternancia entre dependencia positiva y negativa. Válido en \(\mathbb{R}^d\), \(d\leq 2(\alpha +1)\), su expresión viene dada por:
\begin{equation}
\gamma(|\mathbf{h}|)=m \left(1- \left(\frac{2a}{|\mathbf{h}|}\right)^{\alpha} \Gamma(\alpha + 1)
J_\alpha \left(\frac{|\mathbf{h}|}{a} \right) \right),
\end{equation} donde \(\alpha\) es un parámetro de forma, \(a\) es un parámetro de escala, \index{Scale parameter} \index{Shape parameter} \(\Gamma\) es la función de Euler que interpola el factorial y \(J_{\alpha}\) es la función J-Bessel de primera especie de orden \(\alpha\).

La Fig. \ref{fig:semivar-teo-sill-ch41} muestra una representación gráfica de los anteriores semivariogramas.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gstat)}
\FunctionTok{show.vgms}\NormalTok{(}\AttributeTok{models =} \FunctionTok{c}\NormalTok{(}\StringTok{"Sph"}\NormalTok{, }\StringTok{"Exp"}\NormalTok{, }\StringTok{"Gau"}\NormalTok{, }\StringTok{"Nug"}\NormalTok{, }\StringTok{"Bes"}\NormalTok{, }\StringTok{"Wav"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/semivar-teo-sill-ch41-1} 

}

\caption{Representación de semivariogramas con meseta válidos  (meseta y alcance efectivo unitarios; a excepción del efecto pepita puro)}\label{fig:semivar-teo-sill-ch41}
\end{figure}

\hypertarget{semivariogramas-sin-meseta}{%
\subsubsection{Semivariogramas sin meseta}\label{semivariogramas-sin-meseta}}

Estos modelos van más allá de la hipótesis estacionaria de segundo
orden y corresponden a \emph{f.a.} con una
capacidad ilimitada de dispersión espacial, es decir, a \emph{f.a} intrínsecamente estacionarias, pero no estacionarias de segundo orden.

\begin{itemize}
\item
  \textbf{Potencial}. Válido en \(\mathbb{R}^d, d\geq1\) y definido por:
  \begin{equation}
  \gamma (|\mathbf{h}|)=  (|\mathbf{h}|)^{\alpha }, \quad \mbox  {con } 0<\alpha <2,
  \end{equation}
\item
  \textbf{Logarítmico}. Válido en \(\mathbb{R}^d, d\geq1\) y con expresión:
  \begin{equation}
  \gamma (|\mathbf{h}|) = b\log |\mathbf{h}|\; \;\mbox{si}\ |\mathbf{h}| \ge 0,
  \end{equation}
  donde \(b\) es una constante.
\end{itemize}

Una representación gráfica de ambos semivariogramas puede verse en la Fig. \ref{fig:semivar-teo-no-sill-ch41}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gstat)}
\FunctionTok{show.vgms}\NormalTok{(}\AttributeTok{models =} \FunctionTok{c}\NormalTok{(}\StringTok{"Pow"}\NormalTok{, }\StringTok{"Log"}\NormalTok{), }\AttributeTok{sill =} \DecValTok{1}\NormalTok{, }\AttributeTok{range =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{nugget =} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/semivar-teo-no-sill-ch41-1} 

}

\caption{Representación de semivariogramas sin meseta válidos  }\label{fig:semivar-teo-no-sill-ch41}
\end{figure}

\hypertarget{semivariograma-empuxedrico}{%
\subsection{Semivariograma empírico}\label{semivariograma-empuxedrico}}

Dado que la única información de la que se dispone es una realización observada de la \emph{f.a} objeto de estudio, en la práctica la estructura de la dependencia espacial se estima mediante el semivariograma empírico \index{Empirical semivariogram}.

En el marco de la estacionariedad intrínseca (que incluye la esacionariedead estricta y de segundo orden), y en \(\mathbb{R}^d, d\geq1\), se estiman (insesgadamente) los valores semivariográficos para un número determinado de distancias, por el método de los momentos:
\begin{equation} 
\hat{\gamma} (\mathbf{h}) = \frac{1}{2\#N(\mathbf{h})}\sum\limits_{N(\mathbf{h})} {\left( {Z(\mathbf{s}_i + \mathbf{h}) - Z({\mathbf{s}}_i )} \right)^2},
\end{equation}
donde \(\#N(\mathbf{h})\) es el número de parejas de localizaciones separadas por el vector \(\mathbf{h}\).

La función que mejor ajusta las estimaciones de los valores semivariograficos anteriormente referidos se denomina \emph{semivariograma empírico}, y también se suele denotar por \(\hat{\gamma} (\mathbf{h})\).

Los valores semivariográficos se suelen computar para distancias inferiores a la mitad del diametro de \emph{D}, porque, para distancias superiores, el número de parejas de localizaciones suele ser pequeño para proporcionar estimaciones fiables. En la práctica, como en muchas de las direcciones no hay un numero de parejas para calcular el semivariograma, lo habitual es construir un \emph{semivariograma empírico omnidireccional}, es decir, que depende solo de la distancia (longitud del vector \textbf{h}) y no de la dirección. Para ello se crean regiones de tolerancia, que no se solapen, basadas en intervalos de distancia (normalmente de la misma longitud) y un angulo de tolerancia. En concreto, la tolerancia se especifica en el módulo de \(\mathbf h\) (\(\pm\Delta |\mathbf h|\)) y su dirección (\(\pm\Delta\theta\)). Para más detalles y ejemplos, véase \citet{montero_et_al2015}.

La Fig. \ref{fig:plot-semivar-emp} muestra la ubicación de los puntos semivariográficos, indicando el número de parejas a cada distancia, en el caso de las temperaturas máximas en España (06/08/2022).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vgm\_tmax }\OtherTok{\textless{}{-}} \FunctionTok{variogram}\NormalTok{(tmax }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, temp\_max\_utm\_sf,}
  \AttributeTok{cutoff =} \DecValTok{250000} \CommentTok{\# 250 km}
\NormalTok{)}
\FunctionTok{plot}\NormalTok{(vgm\_tmax, }\AttributeTok{plot.numbers =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{pch =} \StringTok{"+"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{cex =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/emp-semivar} 

}

\caption{Valores semivariográficos. Temperaturas máximas (06/08/2022)}\label{fig:plot-semivar-emp}
\end{figure}

En el ejemplo ilustrado, las distancia mínima entre dos estaciones meteorológicas es 1.125m y la máxima 1.027.597m. Sin embargo, dada la geometría del mapa de España (aunque de Huelva a Girona hay 987 km en linea recta, más de dos terceras partes de las ciudades españolas no están separadas más de 500 km.), se consideró 250.000m (1/4 de la distancia máxima) como distancia máxima a la hora de calcular los valores semivariográficos, ya que a partir de dicha distancia el número de parejas no es lo suficientemente grande como para obtener resultados fiables. Por convenio,gstat divide la distancia en 15 intervalos (en geoR se divide en 13 porque los autores lo hicieron un viernes 13).

\hypertarget{ajuste-semivariogruxe1fico}{%
\subsection{Ajuste semivariográfico}\label{ajuste-semivariogruxe1fico}}

Cualquier funcion que dependa de una distancia y una dirección no es necesariamente un semivariograma, pues para ello tienen que cumplir los requisitos especificados en \ref{semivariograma}. Esta es la razón por la que el semivariograma empírico no puede utilizarse directamente para realizar predicciones geoestadísticas. Por ello, a los valores semivariográficos estimados se les ajusta una función que represente un semivariograma válido. Sin embargo, esta tarea, clave para el éxito del posterior proceso predictivo, no es sencilla ni existe consenso en torno a ella.
El ajuste puede ser \emph{manual}, utilizando métodos visuales y gráficos, o \emph{automático}, que usa procedimientos estadísticos. Una combinación de ambos es muy recomendable.

El ajuste manual pudiera parecer ``no muy científico'' pero, dado que lo más importante a la hora del ajuste no es tanto la bondad del ajuste para todos los puntos semivariográficos sino lo bien que un semivariograma válido representa las principales características del fenómeno, especialmente el tipo de estacionariedad (comportamiento a largas distancias) y, sobre todo, el tipo de continuidad (comportamiento cerca del origen). En este sentido, cualquier conocimiento sobre el fenómeno en estudio es bienvenido.

El ajuste automatizado mediante procedimientos estadísticos incluye los \emph{métodos de mínimos cuadrados} \index{Mínimos cuadrados ordinarios} (tanto ordinarios como generalizados y ponderados \index{Mínimos cuadrados generalizados} \index{Mínimos cuadrados ponderados}), que son los más populares en la práctica, y los \emph{métodos basados en máxima verosimilitud}, \index{Métodos basados en Máxima Verosimilitud} \index{Máxima Verosimilitud} que incluyen, entre otros, el tradicional método máximo verosímil,la máxima verosimilitud restringida \index{Máxima Verosimilitud Restringida} y el método de la verosimilitud compuesta.

La Fig. \ref{fig:plot-fit-semivar} muestra el semivariograma empírico correspondiente a los puntos semivariográficos de la Fig. \ref{fig:plot-semivar-emp}. De todos los modelos con meseta, el semivariograma ajustado ha sido un exponencial con alcance 76.404,64 metros y meseta 13,74.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit\_vgm\_tmax }\OtherTok{\textless{}{-}} \FunctionTok{fit.variogram}\NormalTok{(vgm\_tmax,}
  \AttributeTok{model =} \FunctionTok{vgm}\NormalTok{(}\AttributeTok{model =} \FunctionTok{c}\NormalTok{(}\StringTok{"Sph"}\NormalTok{, }\StringTok{"Exp"}\NormalTok{, }\StringTok{"Gau"}\NormalTok{, }\StringTok{"Nug"}\NormalTok{, }\StringTok{"Bes"}\NormalTok{, }\StringTok{"Wav"}\NormalTok{)), }\AttributeTok{fit.sills =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{fit.ranges =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{fit.kappa =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{fit.method =} \DecValTok{7}
\NormalTok{)}
\NormalTok{fit\_vgm\_tmax}
\CommentTok{\#\textgreater{}   model    psill    range}
\CommentTok{\#\textgreater{}1   Exp 13.74102 76404.64}
\FunctionTok{attr}\NormalTok{(fit\_vgm\_tmax, }\StringTok{"SSErr"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 6.200657e{-}07}
\FunctionTok{plot}\NormalTok{(vgm\_tmax, fit\_vgm\_tmax, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =} \StringTok{"2"}\NormalTok{, }\AttributeTok{pch =} \StringTok{"*"}\NormalTok{, }\AttributeTok{cex =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/emp-teo-semivar} 

}

\caption{Semivariograma empírico. Temperaturas máximas (06/08/2022)}\label{fig:plot-fit-semivar}
\end{figure}

El método de ajuste ha sido el que figura por defecto: mínimos cuadrados ponderados con ponderaciones \(\frac {N_{\bf |h|}} {|\bf h|^2}\), que funciona bien en la práctica y selecciona el semivariograma que mejor ajuste cuando el número de parejas es elevado y la distancia pequeña, que es la parte del semivariograma que hay que ajustar bien porque a pequeñas distancias es cuando más dependencia espacial hay. Respecto a los parámetros iniciales, aunque el investigador puede especificar los que considere convenientes, se recomienda utilizar los que utiliza la función por defecto: \((i)\) alcance igual a 1/3 de la distancia máxima en la muestra; \((ii)\) como efecto pepita se toma la media de los tres primeros valores semivarigráficos; y \((iii)\) como meseta parcial (meseta menos efecto pepita), la media de los cinco últimos valores semivariográficos.

\hypertarget{kriging}{%
\section{Kriging}\label{kriging}}

Seleccionado el semivariograma válido que mejor se ajusta a los puntos semivariográficos, se aborda el proceso predictivo. El método predictivo que usa la geoestadística es conocido como \emph{kriging} en honor al ingeniero de minas D.G. Krige. \index{Daniel G. Krige}

El kriging tiene como objetivo predecir el valor de una \emph{f.a.}, \(Z(\mathbf{s})\), en uno o más puntos (o bloques) no observados, a partir de la regionalización observada (pueden ser puntos o bloques) en un
de un dominio \emph{D}, y proporciona el mejor predictor lineal insesgado
de la \emph{v.r.} de interés \index{regionalización} en tales puntos o bloques no observados \footnote{En lo que sigue, la exposición se centrará en la predicción puntual a partir de datos puntuales (es decir, en el soporte puntual). La generalización a bloques puede verse en \citet{montero_et_al2015}.}. La limitación a la clase de predictores lineales
obedece a que, bajo estacionariedad de segundo orden, solo se requiere el conocimiento de los momentos de segundo orden de la \emph{f.a.} Con más información estructural, pueden definirse predictores no lineales.

Las principales ventajas del kriging sobre los métodos de interpolación espacial deterministas \index{Interpolation} (método de la distancia inversa, splines, regresión polinomial, etc.) \index{splines}, es que (\(i\)) considera la estructura de la dependencia espacial (dando lugar a mejores predicciones), (\(ii\)) proporciona, junto con la predicción, la varianza del error de predicción y (\(iii\)) es un interpolador exacto.

Dependiendo del tipo de estacionariedad que se considere en la \emph{f.a.} el kriging puede ser: universal (no estacionariedad en media) u ordinario (estacionariedad de segundo orden o intrínseca). Nos centraremos en el kriging ordinario (\emph{KO}). La generalización al caso universal (hay deriva: la media depende de las localizaciones en vez de ser constante) puede verse en \citet{montero_et_al2015}).

En términos formales, \emph{KO} se plantea como sigue: Sea \(Z =\left\{ Z(\mathbf{s}),\,\, \mathbf{s} \in D\right\}\) una \emph{f.a.} con estacionariedad de segundo orden o intrínseca y con media desconocida (cuando se conoce, \emph{KO} se denomina kriging simple). Sea el predictor lineal krigeado \(Z^{\ast} (\mathbf{s}_0 ) = \sum\limits_{i = 1}^n {\lambda _i } Z(\mathbf{s}_i )\), donde las ponderaciones \(\lambda _i, i=1,2,...,n\), se obtienen imponiendo al error de predicción las condiciones de esperanza nula y mínima varianza.\\
El sistema de ecuaciones que proporciona dichas ponderaciones óptimas es:
\begin{equation}  
\left\{ {\begin{array}{l}
 \sum\limits_{j = 1}^n {\lambda _j \gamma (\mathbf{s}_i - \mathbf{s}_j)
 + \alpha = \gamma (\mathbf{s}_i - \mathbf{s}_0 ), \quad \ i = 1,...,n} \\
 \sum\limits_{i = 1}^n {\lambda _i = 1} \; \\
 \end{array}} \right.
\end{equation}
siendo la varianza del error de predicción:
\(\sigma_{OK}^2 (\mathbf{s}_0) = \sum\limits_{i = 1}^n {\lambda _i \gamma (\mathbf{s}_i - \mathbf{s}_0 ) + \alpha }\), donde \(\alpha\) es el multiplicador de Lagrange involucrado en el proceso de optimización.

Retomando el ejemplo de las temperaturas máximas en la España peninsular el 6 de agosto de 2022, a continuación se muestra el código necesario para la creación de un mapping de predicción de dichas temperaturas.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kriged\_tmax }\OtherTok{\textless{}{-}} \FunctionTok{krige}\NormalTok{(tmax }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{,}
\NormalTok{  temp\_max\_utm\_sp,}
\NormalTok{  grd\_sp,}
  \AttributeTok{model =}\NormalTok{ fit\_vgm\_tmax}
\NormalTok{)}
\CommentTok{\#\textgreater{} [using ordinary kriging]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kriged\_df }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(kriged\_tmax, }\AttributeTok{xy =}\NormalTok{ T, }\AttributeTok{na.rm =}\NormalTok{ T)}

\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_tile}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ kriged\_df,}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ coords.x1, }\AttributeTok{y =}\NormalTok{ coords.x2, }\AttributeTok{fill =}\NormalTok{ var1.pred)}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data =}\NormalTok{ ESP\_utm, }\AttributeTok{col =} \StringTok{"black"}\NormalTok{, }\AttributeTok{fill =} \ConstantTok{NA}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_gradientn}\NormalTok{(}
    \AttributeTok{colours =}\NormalTok{ pal\_paper,}
    \AttributeTok{breaks =}\NormalTok{ br\_paper,}
    \AttributeTok{labels =} \ControlFlowTok{function}\NormalTok{(x) \{}
      \FunctionTok{paste0}\NormalTok{(x, }\StringTok{"º"}\NormalTok{)}
\NormalTok{    \},}
    \AttributeTok{guide =} \FunctionTok{guide\_legend}\NormalTok{(}\AttributeTok{reverse =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{title =} \StringTok{"Temp. max."}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{theme\_light}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{panel.background =} \FunctionTok{element\_blank}\NormalTok{(),}
    \AttributeTok{panel.border =} \FunctionTok{element\_blank}\NormalTok{(),}
    \AttributeTok{axis.title =} \FunctionTok{element\_blank}\NormalTok{(),}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/tmax-kriged} 

}

\caption{Mapping de temperaturas máximas (06/08/2022).}\label{fig:plot-kriging-pred}
\end{figure}

El mapping de la Fig. \ref{fig:plot-kriging-pred} tiene poco valor si no se acompaña de otro que muestre la desviación típica de los errores de predicción.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(kriged\_df) }\SpecialCharTok{+}
  \FunctionTok{geom\_contour\_filled}\NormalTok{(}\FunctionTok{aes}\NormalTok{(coords.x1, coords.x2, }\AttributeTok{z =} \FunctionTok{sqrt}\NormalTok{(var1.var)),}
    \AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{2.5}\NormalTok{, }\DecValTok{3}\NormalTok{, }\FloatTok{3.5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\FunctionTok{max}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(kriged\_df}\SpecialCharTok{$}\NormalTok{var1.var)))}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data =}\NormalTok{ ESP\_utm, }\AttributeTok{col =} \StringTok{"black"}\NormalTok{, }\AttributeTok{fill =} \ConstantTok{NA}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data =}\NormalTok{ temp\_max\_utm\_sf, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{shape =} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{( }\CommentTok{\# paleta colores}
    \AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"springgreen"}\NormalTok{, }\FunctionTok{hcl.colors}\NormalTok{(}\DecValTok{8}\NormalTok{, }\StringTok{"PuRd"}\NormalTok{, }\AttributeTok{rev =} \ConstantTok{TRUE}\NormalTok{)),}
    \AttributeTok{guide =} \FunctionTok{guide\_legend}\NormalTok{(}\AttributeTok{title =} \StringTok{"Desv. típica}\SpecialCharTok{\textbackslash{}n}\StringTok{ error predicción"}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{theme\_light}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{panel.background =} \FunctionTok{element\_blank}\NormalTok{(),}
    \AttributeTok{panel.border =} \FunctionTok{element\_blank}\NormalTok{(),}
    \AttributeTok{axis.title =} \FunctionTok{element\_blank}\NormalTok{(),}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/tmax-sd-error} 

}

\caption{Desviaciones típicas del error de predicción}\label{fig:plot-sd-error}
\end{figure}

Como se aprecia en la Fig. \ref{fig:plot-sd-error} cuanto mayor es el número de localizaciones observadas alrededor del punto de predicción, menor es la desviación típica del error de predicción.

\begin{infobox_resume}
\textbf{RESUMEN}. La geoestadística estudia de fenómenos regionalizados, que son aquellos que se extienden en el espacio y presentan una organización o estructura debida a la dependencia espacial existente. Su objetivo es sacar provecho de dicha dependencia espacial para llevar a cabo predicciones (interpolaciones) óptimas en ubicaciones o áreas de interés, o la realización de mappings sobre todo el dominio o parte de él. Las dos partes del análisis geoestadístico son: \((i)\) El análisis estructural de la dependencia espacial y \((ii)\)la predicción ``krigeada''. La estructura de dependencia espacial se representa mediante un semivariograma. La elección del semivariograma entre el elenco de funciones semivariográficas válidas es la clave del éxito de la predicción geoestadística, y por ello al semivariograma se le considera la piedra angular de la geoestadística. La técnica que utiliza la geoestadística para predecir se denomina ``kriging'', y presenta un buen número de ventajas sobre los tradicionales métodos de interpolación espacial deterministas al considerar la estructura espacial de las observaciones.

\end{infobox_resume}

\hypertarget{cap-econom-esp}{%
\chapter{Modelos econométricos espaciales}\label{cap-econom-esp}}

\emph{Andrés Vallone y Coro Chasco}

\hypertarget{la-dependencia-espacial}{%
\section{\texorpdfstring{La dependencia espacial \index{Dependencia espacial}}{La dependencia espacial }}\label{la-dependencia-espacial}}

En muchas ocasiones los fenómenos sociales no son independientes del espacio geográfico en el cual se producen, esto se refleja en la primera ley de la Geografía enunciada por \citet{tobler1970} ``Todas las cosas están relacionadas entre sí, pero las cosas más próximas en el espacio tienen una relación mayor que las distantes'' \citep[p 236]{tobler1970computer}.Esta situación produce una violación del supuesto básico de independencia de las variables aleatorias requerido por el método de estimación de Minimos Cuadrados Ordinarios (MCO)

En este contexto, los MCO ya no son óptimos y, por tanto, los test \(t\) y \(F\) estadísticos pueden llevar a conclusiones erróneas\citep{anselin1988spatial}. Por eso es necesario encontrar la manera de incorporar el espacio geográfico en nuestros modelos de estimación. En este capítulo abordaremos este asunto, mostrando primero los métodos de exploración de datos espaciales, para luego mostrar las formas de modelización del espacio y los métodos de estimación.

Los modelos de econometría espacial se centrar en manejar las situaciones de dependencia espacial\index{Dependencia espacial}. Existe dependencia espacial cuando lo que sucede un una unidad espacial \(i\) esta influenciado por lo que sucede en una unidad espacial \(j\) y viceversa \citep{anselin2013}. La dependencia espacial se traduce en que los valores de la variable de las unidades espaciales \(i\) y \(j\) con \(i\neq j\) estan correlacionados entre sí, hecho que se conoce como autocorrelación espacial \index{Autocorrelación espacial} \citep{anselin2013}. La autocorrelación espacial puede ser positiva, cuando la unidades espaciales con valores similares tienden a estar juntos (altos con altos, bajos con bajos) o negativa, cuando las unidades espaciales tienden a estar rodeadas de vecinos con valores diferentes. Los patrones espaciales formados por la existencia de autocorrelación se muestran en la Figura \ref{fig:dependeciaespacial}. La ausencia de algún tipo de autocorrelación es lo que se entiende como aleatoriedad espacial \citep{anselin2013}.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/plot-dep-W} 

}

\caption{Patrones de autocorrelación espacial}\label{fig:plot-dep-W}
\end{figure}

\hypertarget{modelizaciuxf3n-del-espacio-la-matriz-w}{%
\subsection{Modelización del espacio: La matriz W}\label{modelizaciuxf3n-del-espacio-la-matriz-w}}

El espacio puede jugar un rol importante en la determinación de los procesos a modelizar. Por ello, resulta relevante encontrar una forma de incorporar el espacio en los procesos de estimación. Para modelizar la interacción de una variable consigo misma es natural pensar en el concepto de autocorrelación. No obstante, a diferencia de la autocorrelación temporal, que es unidireccional (sólo el pasado puede afectar el presente), en el caso del espacio la influencia es multidireccional en el entorno o vecindario de la unidad de análisis y, por tanto, es crucial definir el vecindario para poder realizar los análisis.

La matriz de vecindad \(\mathbf{W}_{n \times n}\) muestra la relación entre las \(n\) unidades espaciales analizadas. Esta matriz define la condición de vecindad y, de esta manera, la interacción existente entre las unidades espaciales. Esta matriz es simétrica y binaria, de forma que el elemento \(w_{ij} = 1\) si las unidades espaciales \(i\) y \(j\) son vecinas y cero si no lo son. Por tanto, \(w_{ii}=0\) puesto que una unidad espacial no puede ser vecina de si misma. Existen distintas criterios de definición de esta matriz dependiendo del proceso que se desee modelizar y las características de los datos. Si se cuenta con un mapa de polígonos, entonces podemos utilizar alguno de los criterios que se presentan en la Figura \ref{fig:criterio} para difinir la matriz \(\mathbf{W}\)

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/plot-criterio-W} 

}

\caption{Criterios de vecindad extraido de [@martori2008incorporacion]}\label{fig:plot-criterio-W}
\end{figure}

Las matrices \(\mathbf{W}\) generadas bajo el criterio lineal consideran como vecinos a la unidad espacial \(i\) todas aquellas unidades que compartan un borde situadas en la misma dirección cardinal, norte sur o este oeste, de esta unidad. El resto de los criterios de contigüidad siguen los movimientos de las piezas del ajedrez para definir la vecindad de la unidad espacial \(i\). La construcción de una matriz de vecindad bajo el criterio de la torre implica considerar como vecinos de la unidad espacial \(i\) aquellas unidades espaciales situadas al norte, sur, este u oeste y que compartan un borde en común con dicha unidad. El uso del criterio de alfil considera como vecindad de la unidad \(i\) aquellas unidades situadas al noreste, noroeste, sureste o suroeste de la unidad espacial \(i\) y que tengan, al menos, un punto en común. El criterio de la reina considera como vecindario de la unidad espacial \(i\) alas unidades espaciales en todas las direcciones cardinales y que tengan al menos un punto en común con la unidad espacial \(i\) \citep{martori2008incorporacion}.

Dependiendo del fenómeno que se analice la matriz de contigüidad puede ser construida considerando un vecindario más amplio; por ejemplo, considerando como vecinos de la unidad espacial \(i\) a los vecinos de los vecinos de dicha unidad. En este caso diríamos que la matriz es de orden 2 (los vecinos y los vecinos de los vecinos). Las matrices \(\mathbf{W}\) se utilizan para capturar el efecto del vecindario a partir de medias ponderadas basadas en la cercanía de las unidades espaciales. Es por ello que las matrices de vecindad se estandarizan por filas. Los elementos de la matriz estandarizada se obtienen de la siguiente manera:

\begin{equation}
w^e_{ij}=\frac{w_{ij}}{\sum_{j=1}^n w_{ij}}
\end{equation}

En palabras simples, se divide cada elemento de una fila de la matriz \(\mathbf{W}\) por la suma de dicha fila. Esto asegura que cada elemento de la matriz \(\mathbf{W}\) estandarizada se encuentre entre 0 y 1 y que la suma de cada una de sus filas sea siempre 1. Las matrices de vecindad estandarizadas llevan el nombre de matrices de pesos espaciales, a partir de ahora cuando se haga relación a la matriz \(\mathbf{W}\) estaremos hablando de una matriz de pesos espaciales.

Para el cálculo de las matrices de vecindad utilizaremos el paquete \texttt{spedep} \citep{R-spdep}. La función \texttt{poly2nb} construye la relación de vecindad a partir de los polígonos de un objeto espacial según el criterio y el orden que se indique; la función \texttt{nb2listw} transforma la relación de vecindad en una lista de pesos espaciales. Para el ejemplo, utilizaremos el conjunto de datos del estudio de \citet{guerry1833essai} utilizados en {[}\citet{anselinLocalIndicatorMultivariate2017}{]}\footnote{Una descripción completa de la base de datos está disponible en \url{https://geodacenter.github.io/data-and-lab/Guerry/}}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(spdep)}
\FunctionTok{library}\NormalTok{(}\StringTok{"CDR"}\NormalTok{)}

\NormalTok{reina }\OtherTok{\textless{}{-}} \FunctionTok{poly2nb}\NormalTok{(guerry, }\AttributeTok{queen =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{w\_reina }\OtherTok{\textless{}{-}} \FunctionTok{nb2listw}\NormalTok{(reina, }\AttributeTok{style =} \StringTok{"W"}\NormalTok{, }\AttributeTok{zero.policy =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{w\_reina}
\CommentTok{\#\textgreater{} Characteristics of weights list object:}
\CommentTok{\#\textgreater{} Neighbour list object:}
\CommentTok{\#\textgreater{} Number of regions: 85 }
\CommentTok{\#\textgreater{} Number of nonzero links: 420 }
\CommentTok{\#\textgreater{} Percentage nonzero weights: 5.813149 }
\CommentTok{\#\textgreater{} Average number of links: 4.941176 }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Weights style: W }
\CommentTok{\#\textgreater{} Weights constants summary:}
\CommentTok{\#\textgreater{}    n   nn S0      S1       S2}
\CommentTok{\#\textgreater{} W 85 7225 85 37.2761 347.6683}
\end{Highlighting}
\end{Shaded}

Para construir una matriz de contigüidad de la torre de orden 1 se utilizan las mismas funciones, cambiando el parámetro \texttt{queen} de la función \texttt{poly2nb}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{torre }\OtherTok{\textless{}{-}} \FunctionTok{poly2nb}\NormalTok{(guerry, }\AttributeTok{queen =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{w\_torre }\OtherTok{\textless{}{-}} \FunctionTok{nb2listw}\NormalTok{(torre, }\AttributeTok{style =} \StringTok{"W"}\NormalTok{, }\AttributeTok{zero.policy =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Cuando trabajamos con datos a nivel puntual, o cuando existen situaciones geográficas de no contigüidad como, por ejemplo, una isla, el uso de la matriz de contigüidad no es una condición tan evidente. En estos casos, resulta más oportuno definir la matriz de vecindad a partir de criterios de distancia. Las matrices de \(\mathbf{W}\) basadas en distancias pueden tener configuraciones continuas de la matriz respecto a la distancia \(d\) entre las unidades espaciales \(i\) y \(j\) de tal manera que \(w_{ij}=1/d_{ij}\) o \(w_{ij}=1/d_{ij}^2\) y \(w_{ii}=0\). Otras configuraciones implican considerar un numero \(k\) de vecinos más cercanos a cada unidad espacial de tal manera que:

\begin{align}
    \begin{cases}
    w_{ij}(k)=0 & i=j, \forall k \\
    w_{ij}(k)=1 & d_{ij} \leq d_i(k)\\
    w_{ij}(k)=0 & d_{ij} > d_i(k)
    \end{cases}
\end{align}

donde \(d_i(k)\) es la k-esima menor distancia entre las unidades espaciales \(i\) y \(j\). Utilizando funciones de \citet{R-spdep} y \citet{R-sf} y los datos de \citet{valloneSpatiotemporalMethodsAnalysis2020} calcularemos la matriz de 5 vecinos más próximos de las áreas urbanas chilenas con más de 2000 habitantes.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(sf)}
\CommentTok{\# Se extraen las coordenadas de los puntos}
\NormalTok{coord }\OtherTok{\textless{}{-}} \FunctionTok{st\_coordinates}\NormalTok{(cities)}
\CommentTok{\# Calcula la vecindad de 5 vecinos más cercanos}
\NormalTok{w5knn }\OtherTok{\textless{}{-}} \FunctionTok{knearneigh}\NormalTok{(coord, }\AttributeTok{k =} \DecValTok{5}\NormalTok{, }\AttributeTok{longlat =}\NormalTok{ T) }\SpecialCharTok{|\textgreater{}} \FunctionTok{knn2nb}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

El uso de matrices de \(k\) vecinos puede forzar la vecindad entre unidades espaciales, considerando vecinas a unidades espaciales que estén muy distantes entre ellas. Para solucionar esto podemos usar una configuración de vecindad basada en una distancia limite \(d_{max}\) de tal manera que:

\begin{align}
  \begin{cases}
  w_{ij}=1 & d_{ij} \leq d_{max}\\
  w_{ij}=0 & d_{ij} > d_{max}\\
  \end{cases}
\end{align}

El problema de esta definición es la posibilidad de generar unidades espaciales aisladas debido a una definición muy estrecha de la distancia máxima (\(d_{max}\)). Esto se evita fijando la distancia máxima de tal manera que se asegure que todas las unidades espaciales tengan al menos un vecino.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calcula la k=1 matriz W}
\NormalTok{knn1 }\OtherTok{\textless{}{-}} \FunctionTok{knearneigh}\NormalTok{(coord) }\SpecialCharTok{|\textgreater{}} \FunctionTok{knn2nb}\NormalTok{()}
\CommentTok{\# Obtiene la distancia critica}
\NormalTok{distancia.critica }\OtherTok{\textless{}{-}} \FunctionTok{max}\NormalTok{(}\FunctionTok{unlist}\NormalTok{(}\FunctionTok{nbdists}\NormalTok{(knn1, coord)))}
\CommentTok{\# genera la matriz de vencindad de distancia w\_ij \textless{} d\_max}
\NormalTok{k1 }\OtherTok{\textless{}{-}} \FunctionTok{dnearneigh}\NormalTok{(coord, }\DecValTok{0}\NormalTok{, distancia.critica)}
\NormalTok{w\_dist }\OtherTok{\textless{}{-}} \FunctionTok{nb2listw}\NormalTok{(k1)}
\end{Highlighting}
\end{Shaded}

Debe considerarse que la configuración de \(k\) vecinos y de la distancia censurada dan lugar a matrices binarias, mientras que las matrices \(\mathbf{W}\) basadas en distancias, no lo son. Utilicemos el mismo conjunto de datos para realizar el cálculo de la matriz \(\mathbf{W}\) basado en la distancia inversa. Dos elementos deben considerarse: utilizar una función decreciente respecto distancia (en este caso una hipérbola) para satisfacer la ley de Tobler \citep{tobler1970computer} y segundo, dado que la incidencia que puede tener una unidad espacial \(j\) que se encuentre muy lejana a la unidad \(i\) tiende a cero, \citep{tobler1970computer}, la matriz suele censurarse a una distancia máxima \(d_{max}\) a partir de la cual la incidencia entre unidades espaciales es nula, donde \(d_{max}\) es fijada de tal manera de evitar la existencia de unidades espaciales aisladas.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knn1 }\OtherTok{\textless{}{-}} \FunctionTok{knearneigh}\NormalTok{(coord) }\SpecialCharTok{|\textgreater{}} \FunctionTok{knn2nb}\NormalTok{()}
\NormalTok{distancia.critica }\OtherTok{\textless{}{-}} \FunctionTok{max}\NormalTok{(}\FunctionTok{unlist}\NormalTok{(}\FunctionTok{nbdists}\NormalTok{(knn1, coord)))}
\NormalTok{k1 }\OtherTok{\textless{}{-}} \FunctionTok{dnearneigh}\NormalTok{(coord, }\DecValTok{0}\NormalTok{, distancia.critica)}
\CommentTok{\# Calcula la distancia entre los vecinos}
\NormalTok{dist.list }\OtherTok{\textless{}{-}} \FunctionTok{nbdists}\NormalTok{(k1, }\FunctionTok{st\_coordinates}\NormalTok{(cities))}
\CommentTok{\# Calcula la distancia inversa}
\NormalTok{i.dist.list }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(dist.list, }\ControlFlowTok{function}\NormalTok{(x) }\DecValTok{1} \SpecialCharTok{/}\NormalTok{ x)}
\CommentTok{\# Crea la matriz W}
\NormalTok{w.dist.i }\OtherTok{\textless{}{-}} \FunctionTok{nb2listw}\NormalTok{(k1, }\AttributeTok{glist =}\NormalTok{ i.dist.list, }\AttributeTok{style =} \StringTok{"W"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Hemos indicado que la matriz \(\mathbf{W}\) se utiliza para capturar los efectos del espacio a partir de medias ponderadas, es decir mediante la matriz \(\mathbf{W}\) seremos capaces de construir el \emph{retardo} espacial. El retardo espacial \(\mathbf{W}y\) de la variable \(y\) se obtiene al multiplicar dicha variable por la matriz \(\mathbf{W}\), por tanto, cada elemento del retardo espacial puede ser interpretado como la media ponderada de la variable \(y\) en el vecindario de cada unidad espacial \(i\),

\hypertarget{medidas-de-autocorrelaciuxf3n}{%
\section{\texorpdfstring{Medidas de Autocorrelación \index{Autocorrelación espacial}}{Medidas de Autocorrelación }}\label{medidas-de-autocorrelaciuxf3n}}

Una buena herramienta para entender y comprender las medidas de autocorrelación espacial \index{Autocorrelación espacial} es el diagrama de Moran \citep{anselinMoranScatterplotESDA1996}. El diagrama de Moran \index{Diagrama de Moran} relaciona una variable con lo que sucede en su entorno mediante su rezago espacial en una gráfico de puntos. La Figura @ref\{fig:moranplot\} muestra un diagrama de Moran, la linea horizontal intercortada muestra el promedio del rezago espacial, la linea vertical intercortada muestra el promedio de la variable clergy. El dividir el diagrama a partir de las medias permite generar cuatro zonas en el diagrama, el área ``HH'' que contiene a las unidades espaciales cuyo valor de la variable clergy es superior al promedio y su vecindario tiene valores de dicha variable superiores al promedio. Las unidades espaciales que se sitúen en el área ``LL'' presenta valores de la variable cleargy menores al promedio y su entorno tienen valores inferiores al promedio. El área ``LH'' contendrá a las unidades espaciales cuyo valor de la variable cleargy es inferior al promedio y su vecindario tiene valores de cleargy superiores al promedio y lo inverso sucede en el área ``HL''.

A partir del diagrama es posible observar la situación de una variable respecto a su entorno, si las unidades espaciales se sitúan mayoritariamente en las zonas ``HH'' y ``LL'' significa que la unidades espaciales con altos valores (superiores al promedio) de la variable de interés están rodeadas por unidades espaciales con altos valores de dicha variable, y las unidades espaciales con valores bajos (inferiores al promedio) están rodeadas de unidades espaciales con valores bajos, lo cual es una señal de existencia de autocorrelación espacial positiva. Si la concentración se realiza en las áreas ``HL'' y ``LH'' es indicativo de autocorrelación espacial negativa.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{w\_reina\_francia }\OtherTok{\textless{}{-}} \FunctionTok{poly2nb}\NormalTok{(guerry, }\AttributeTok{queen =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{nb2listw}\NormalTok{()}
\DocumentationTok{\#\# Diagrama de Moran}
\FunctionTok{moran.plot}\NormalTok{(guerry}\SpecialCharTok{$}\NormalTok{clergy, w\_reina\_francia,}
  \AttributeTok{xlab =} \StringTok{"Clergy"}\NormalTok{,}
  \AttributeTok{ylab =} \StringTok{"Rezago espacial de Clergy"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/moranplot-1} 

}

\caption{Diagrama de Moran de la variable Clergy}\label{fig:moranplot}
\end{figure}

La Figura \ref{fig:moranplot} da indicios de la existencia de autocorrelación positiva, es decir que las provincias tienden a estar rodeadas de provincias con cantidades similares de clérigos, altos valores junto a altos valores y bajo valores junto a bajos valores. El diagrama de Moran es una herramienta gráfica, para poder comprobar estadísticamente la existencia de autocorrelación global se utilizará el indicador I de Moran

\hypertarget{el-indicador-i-de-moran}{%
\subsection{\texorpdfstring{El indicador \emph{I} de Moran}{El indicador I de Moran}}\label{el-indicador-i-de-moran}}

Si definimos \(x=y-\bar{y}\), el indicador de Moran \index{I de Morna} se calcula como:

\begin{equation}
I = \dfrac{n}{\sum_{i=1}^n\sum_{j=1}^n w_{ij}}\dfrac{\sum_{i=1}^{n}\sum_{j=1}^{n} x_{i}w_{ij}x_{j}}{\sum_{i=1}^{n}x_{i}^{2}}  \label{eq:moranI}
\end{equation}

Este indicador se encuentra en el rango \([-1,1]\) y el signo del indicador coincide con el tipo de autocorrelación: valores positivos son indicativos de autocorrelación positiva y valores negativos son indicadores de la existencia de autocorrelación negativa. Observando detenidamente la ecuación \eqref{eq:moranI} es posible detectar que el cálculo del indicador es calculado como la ratio de la covarianza de la variable \(Y\) con su retardo espacial y la varianza de la variable \(y\). Por tanto, el indicador coincide con el coeficiente de una regresión lineal de \(\mathbf{W}y\) sobre \(y\). La linea con pendiente positiva presente en \ref{fig:moranplot} es precisamente el resultado de la regresión lineal entre \(\mathbf{W}y\) e \(y\) y por tanto su pendiente es el indicador I de Moran. En este sentido, cuanto mayor sea la pendiente de esta recta mayor será el grado de autocorrelación espacial existente. Es posible calcular la significación estadística del indicador mediante un proceso de aleatoriedad espacial, considerando como hipótesis nula la inexistencia de autocorrelación espacial.\citep{anselin2013}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{moran.test}\NormalTok{((guerry}\SpecialCharTok{$}\NormalTok{clergy), w\_reina\_francia,}
  \AttributeTok{randomisation =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{alternative =} \StringTok{"two.sided"}
\NormalTok{)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{}  Moran I test under randomisation}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} data:  (guerry$clergy)  }
\CommentTok{\#\textgreater{} weights: w\_reina\_francia    }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Moran I statistic standard deviate = 6.1632, p{-}value = 7.13e{-}10}
\CommentTok{\#\textgreater{} alternative hypothesis: two.sided}
\CommentTok{\#\textgreater{} sample estimates:}
\CommentTok{\#\textgreater{} Moran I statistic       Expectation          Variance }
\CommentTok{\#\textgreater{}       0.421118648      {-}0.011904762       0.004936422}
\end{Highlighting}
\end{Shaded}

El p-valor permite rechazar la hipótesis nula de homogeneidad espacial no pudiendo rechazar la existencia de autocorrelación positiva.

Es posible detectar agrupaciones espaciales a partir de los indicadores de autocorrelación local LISA \citep{anselinLocalIndicatorsSpatial1995}. Los métodos LISA descomponen los indicadores globales y verifican en cuánto contribuye cada unidad espacial a la formación del valor general, permitiendo obtener un valor de significación para cada \emph{cluster} formado por los valores similares de cada unidad espacial y sus vecinos\citep{yrigoyen2006analisis}. Es posible medir la significación de cada unidad en particular; por ello es posible que los indicadores globales no indiquen la presencia de autocorrelación espacial y aun así encontrar clústeres estadísticamente significativos.

\hypertarget{modelos-economuxe9tricos-espaciales-de-corte-transversal}{%
\section{\texorpdfstring{Modelos econométricos espaciales de corte transversal \index{Modelos econométricos espaciales}}{Modelos econométricos espaciales de corte transversal }}\label{modelos-economuxe9tricos-espaciales-de-corte-transversal}}

Los modelos espaciales deben ser identificados, antes de proceder a su estimación, inferencia y contraste. Para ello, es importante disponer de una estrategia de identificación propia, que permita al investigador conocer los parámetros poblacionales correctos a partir de la observación de una muestra de datos.

Tradicionalmente, la econometría espacial ha resuelto este problema asumiendo que la especificación de los modelos es algo que se conoce a priori, o bien a partir de la teoría económica existente o bien aplicando ciertas estrategias consistentes en la comparación de varios modelos competitivos. Dentro de esta última opción, podemos destacar dos estrategias de modelización ampliamente utilizadas: la que va de lo particular (modelo básico sin efectos de autocorrelación espacial) a lo general (modelo con variables explicativas espacialmente retardadas), y la que parte de un modelo general para terminar en un modelo de autocorrelación espacial más sencillo. A partir estos los dos enfoques previos, es posible plantear la estrategia híbrida de \citet{elhorstAppliedSpatialEconometrics2010} que tiene en cuenta las buenas propiedades de las propuestas anteriores.

Según se presenta en la Figura \ref{fig:estrat}, la estrategia híbrida comienza con la estimación de un modelo básico sin efectos espaciales:

\begin{equation}
y = \alpha \iota_n + \mathbf{X} \beta +\epsilon
\end{equation}

siendo y el vector de la variable dependiente, de orden \((n\times 1)\); \(\mathbf{X}\) la matriz de variables explicativas, de orden \((n\times k)\); \(\iota_n\) un vector formado por unos, de orden \((n \times 1)\); \(\alpha\), \(\beta\) son el grupo de \((k+1)\) parámetros a estimar; y \(\epsilon\) es la variable de perturbaciones aleatorias, de orden \((n\times 1)\), que se distribuye como \(\epsilon \sim N(0,\sigma_{\epsilon}\mathbf{I}_n)\), siendo \(\mathbf{I}_n\) la matriz identidad de orden \(n\).

Este modelo se estima por mínimos cuadrados ordinarios (MCO) y luego, se calculan los tests LM del Multiplicador de Lagrange sobre la variable de los errores de la regresión para contrastar si son ruido blanco desde el punto de vista espacial. Esta hipótesis básica se rechaza en cuanto que alguno de dichos tests, que se distribuyen como una Ji cuadrado con 1 grado de libertad (\(\chi_1^2\)), resulte estadísticamente significativo. Se trata de dos tests orientados hacia una única hipótesis alternativa: el LMLAG, Multiplicador de Lagrange para la hipótesis de variable dependiente espacialmente retardada, y el LMERR, para la hipótesis de dependencia residual.

En primer lugar, si alguno de los tests LM resulta significativo, se recomienda seleccionar el modelo Durbin espacial (SDM), que es un modelo general \citep{anselin1988spatial}:

\begin{equation}
y = \rho \mathbf{W}y + \alpha \iota_n + \mathbf{X} \beta + \mathbf{WX} \theta +\epsilon
\end{equation}

siendo \(\rho\) un parámetro y \(\theta\) un vector de k parámetros autorregresivos espaciales.

Este modelo general incluye dos tipos de interacción espacial, los efectos endógenos (\(\mathbf{W}y\)) y exógenos (\(\mathbf{WX}\)). La variable endógena espacialmente retardada (\(\mathbf{W}y\)) referida al mismo momento del tiempo que la variable dependiente (\(y\)) produce en los estimadores MCO una situación de simultaneidad y, por tanto, sesgo, ineficiencia e inconsistencia. Por eso, se recomienda su estimación por el método de máxima verosimilitud (MV), que supone normalidad en la distribución de los errores (ver \citet{anselin1988spatial}, cap. 6).

La estimación MV de este modelo permite utilizar la ratio de verosimilitud (LR), cuya distribución sigue una Ji al cuadrado con \(k\) grados de libertad, como test para contrastar las hipótesis nulas \(H_0 (\theta=0)\) y \(H_0(\rho=0)\). En este punto, se pueden dar tres casos:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Si se acepta la primera hipótesis, pero no la segunda, siempre y cuando el test LMLAG \textgreater{} LMERR, el SDM debería simplificarse a un modelo del retardo espacial o modelo autorregresivo espacial de orden 1 (SAR):
\end{enumerate}

\begin{equation}
y = \rho \mathbf{W}y + \alpha \iota_n + \mathbf{X} \beta +\epsilon  \label{eq:sar}
\end{equation}

Este modelo se estima por MV si los errores de la estimación por MCO se distribuyen como una normal.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Si se acepta la segunda hipótesis, pero no la primera, y LMERR \textgreater{} LMLAG, debería seleccionarse el modelo del error espacial (SEM):
\end{enumerate}

\begin{equation}
\begin{cases}
y = \alpha \iota_n + \mathbf{X} \beta +u\\
u=\lambda\mathbf{W} \epsilon + \epsilon
\end{cases}
\end{equation}

siendo \(\lambda\) un parámetro autorregresivo espacial a estimar. La estimación MCO produciría estimadores insesgados, consistentes, pero ineficientes. Por eso, se considera aceptable estimar el modelo SEM por MCO realizando una inferencia robusta de la matriz de varianzas y covarianzas de los estimadores por el método KP-HET propuesto por \citet{kelejianSpecificationEstimationSpatial2010}, que tiene en cuenta la existencia conjunta de heteroscedasticidad y autocorrelación espacial en los errores de la regresión.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Si se rechazan ambas hipótesis nulas o no hubiera acuerdo entre los resultados del test LR y los tests LM, entonces el SDM sería el modelo que mejor describiría los datos.
\end{enumerate}

En segundo lugar, si tras la estimación MCO del modelo básico ninguno de los tests LM fuera estadísticamente significativo, entonces dicho modelo tendría que ser reestimado como un modelo modelo espacial regresivo transversal (SLX):

\begin{equation}
y = \alpha \iota_n + \mathbf{X} \beta + \mathbf{WX} \theta +\epsilon
\end{equation}

Este modelo puede estimarse por el método MCO ya que, si las variables explicativas son exógenas, también lo serán sus correspondientes retardos espaciales. La estimación puede considerar todas las variables exógenas espacialmente retardadas o un subconjunto de ellas, para contrastar la hipótesis nula \(H_0(\theta=0)\). Si esta hipótesis no pudiera ser rechazada debería elegirse el modelo básico como el que mejor describe los datos, es decir, no existiría evidencia alguna de la necesidad de efectos de autocorrelación espacial para explicar la variable dependiente. Pero si, por el contrario, la hipótesis \(H_0(\theta=0)\) tuviera que rechazarse, habría que estimar el modelo SDM con las variables \(\mathbf{WX}\) estadísticamente significativas, para contrastar, de nuevo la hipótesis nula \(H_0(\rho=0)\). Si se rechaza esta hipótesis, el modelo seleccionado sería el SDM, pero si no pudiera rechazarse, sería el modelo SLX el que mejor describiría los datos.

Todos estos modelos pueden también estimarse con metodología bayesiana utilizando el enfoque Markov Chains Monte Carlo (MCMC), tal y como se explica en \citet{lesage2009introduction} cap. 5.

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth,height=\textheight]{img/Estrategia.png}
\caption{\label{fig:estrat} Estrategia de especificación híbrida \citep{elhorstAppliedSpatialEconometrics2010}}
\end{figure}

El siguiente conjunto de secuencias de código muestran como estimar los modelos que intervienen en la estrategia de modelización de Elhorst. Para ello, se utilizan un conjunto de datos de los municipios (NUTS 5) que forman parte de las áreas urbanas de España utilizados en \citet{mellaUrbanGrowthTerritorial2006}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(spatialreg)}
\FunctionTok{library}\NormalTok{(tseries)}

\CommentTok{\# Matriz de pesos espaciales}
\NormalTok{coord }\OtherTok{\textless{}{-}} \FunctionTok{coordinates}\NormalTok{(gdpmap)}
\NormalTok{k6 }\OtherTok{\textless{}{-}} \FunctionTok{knearneigh}\NormalTok{(coord, }\AttributeTok{k =} \DecValTok{6}\NormalTok{)}
\NormalTok{dmins }\OtherTok{\textless{}{-}} \FunctionTok{knn2nb}\NormalTok{(k6) }\SpecialCharTok{|\textgreater{}} \FunctionTok{nb2listw}\NormalTok{(}\AttributeTok{style =} \StringTok{"W"}\NormalTok{)}
\CommentTok{\# Estimación modelo básico por MCO}
\NormalTok{gdpols }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(LPGH }\SpecialCharTok{\textasciitilde{}}\NormalTok{ LGH85 }\SpecialCharTok{+}\NormalTok{ BANK }\SpecialCharTok{+}\NormalTok{ UNI01 }\SpecialCharTok{+}\NormalTok{ PAT00, }\AttributeTok{data =}\NormalTok{ gdpmap)}
\FunctionTok{summary}\NormalTok{(gdpols)}
\FunctionTok{jarque.bera.test}\NormalTok{(gdpols}\SpecialCharTok{$}\NormalTok{res) }\CommentTok{\# Test normalidad de residuos}
\FunctionTok{lm.LMtests}\NormalTok{(gdpols, dmins, }\AttributeTok{test =} \StringTok{"all"}\NormalTok{) }\CommentTok{\# Grupo de tests LM}

\CommentTok{\# Estimación modelo SDM por MV}
\NormalTok{gdpsdm }\OtherTok{\textless{}{-}} \FunctionTok{lagsarlm}\NormalTok{(LPGH }\SpecialCharTok{\textasciitilde{}}\NormalTok{ LGH85 }\SpecialCharTok{+}\NormalTok{ BANK }\SpecialCharTok{+}\NormalTok{ UNI01 }\SpecialCharTok{+}\NormalTok{ PAT00,}
  \AttributeTok{data =}\NormalTok{ gdpmap,}
  \AttributeTok{listw =}\NormalTok{ dmins, }\AttributeTok{type =} \StringTok{"mixed"}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(gdpsdm)}

\CommentTok{\# Estimación modeelo SAR por MV}
\NormalTok{gdpsar }\OtherTok{\textless{}{-}} \FunctionTok{lagsarlm}\NormalTok{(LPGH }\SpecialCharTok{\textasciitilde{}}\NormalTok{ LGH85 }\SpecialCharTok{+}\NormalTok{ BANK }\SpecialCharTok{+}\NormalTok{ UNI01 }\SpecialCharTok{+}\NormalTok{ PAT00,}
  \AttributeTok{data =}\NormalTok{ gdpmap,}
  \AttributeTok{listw =}\NormalTok{ dmins}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(gdpsar)}
\FunctionTok{LR.Sarlm}\NormalTok{(gdpsdm, gdpsar) }\CommentTok{\# Test LR: SDM vs. SAR}

\CommentTok{\# Estimación del modelo SEM por MV}
\NormalTok{gdperr }\OtherTok{\textless{}{-}} \FunctionTok{errorsarlm}\NormalTok{(LPGH }\SpecialCharTok{\textasciitilde{}}\NormalTok{ LGH85 }\SpecialCharTok{+}\NormalTok{ BANK }\SpecialCharTok{+}\NormalTok{ UNI01 }\SpecialCharTok{+}\NormalTok{ PAT00,}
  \AttributeTok{data =}\NormalTok{ gdpmap,}
  \AttributeTok{listw =}\NormalTok{ dmins, }\AttributeTok{tol.solve =} \FloatTok{1e{-}16}
\NormalTok{)}
\FunctionTok{summary}\NormalTok{(gdperr)}
\FunctionTok{LR.Sarlm}\NormalTok{(gdpsdm, gdperr) }\CommentTok{\# Test LR: SDM vs. SEM}

\CommentTok{\# Estimación del modelo SLX por MCO}

\CommentTok{\# Cálculo retardos espaciales}
\NormalTok{gdpmap}\SpecialCharTok{$}\NormalTok{WLGH85 }\OtherTok{\textless{}{-}} \FunctionTok{lag}\NormalTok{(dmins, gdpmap}\SpecialCharTok{$}\NormalTok{LGH85)}
\NormalTok{gdpmap}\SpecialCharTok{$}\NormalTok{WBANK }\OtherTok{\textless{}{-}} \FunctionTok{lag}\NormalTok{(dmins, gdpmap}\SpecialCharTok{$}\NormalTok{BANK)}
\NormalTok{gdpmap}\SpecialCharTok{$}\NormalTok{WUNI01 }\OtherTok{\textless{}{-}} \FunctionTok{lag}\NormalTok{(dmins, gdpmap}\SpecialCharTok{$}\NormalTok{UNI01)}
\NormalTok{gdpmap}\SpecialCharTok{$}\NormalTok{WPAT00 }\OtherTok{\textless{}{-}} \FunctionTok{lag}\NormalTok{(dmins, gdpmap}\SpecialCharTok{$}\NormalTok{PAT00)}

\NormalTok{gdpslx }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(LPGH }\SpecialCharTok{\textasciitilde{}}\NormalTok{ LGH85 }\SpecialCharTok{+}\NormalTok{ BANK }\SpecialCharTok{+}\NormalTok{ UNI01 }\SpecialCharTok{+}\NormalTok{ PAT00 }\SpecialCharTok{+}\NormalTok{ WLGH85 }\SpecialCharTok{+}\NormalTok{ WBANK }\SpecialCharTok{+}\NormalTok{ WUNI01 }\SpecialCharTok{+}\NormalTok{ WPAT00, }\AttributeTok{data =}\NormalTok{ gdpmap)}
\FunctionTok{summary}\NormalTok{(gdpslx) }\CommentTok{\# Modelo SLX completo}

\NormalTok{gdpslx2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(LPGH }\SpecialCharTok{\textasciitilde{}}\NormalTok{ LGH85 }\SpecialCharTok{+}\NormalTok{ BANK }\SpecialCharTok{+}\NormalTok{ UNI01 }\SpecialCharTok{+}\NormalTok{ PAT00 }\SpecialCharTok{+}\NormalTok{ WLGH85 }\SpecialCharTok{+}\NormalTok{ WPAT00, }\AttributeTok{data =}\NormalTok{ gdpmap)}
\FunctionTok{summary}\NormalTok{(gdpslx2) }\CommentTok{\# Modelo SLX parsimonioso}
\FunctionTok{LR.Sarlm}\NormalTok{(gdpsdm, gdpslx2) }\CommentTok{\# Test LR: SDM vs. SLX}
\end{Highlighting}
\end{Shaded}

\hypertarget{estimaciuxf3n-sar}{%
\subsection{Estimación SAR}\label{estimaciuxf3n-sar}}

A continuación se muerta la salida de la estimación del modelo SAR, donde puede notarse que todos los parámetros estimados, incluido el parámetro \(\rho\), son estadísticamente significativos.

\begin{verbatim}
#> 
#> Call:lagsarlm(formula = LPGH ~ LGH85 + BANK + UNI01 + PAT00, data = gdpmap, 
#>     listw = dmins)
#> 
#> Residuals:
#>        Min         1Q     Median         3Q        Max 
#> -0.0184657 -0.0034523  0.0012278  0.0032544  0.0194746 
#> 
#> Type: lag 
#> Coefficients: (asymptotic standard errors) 
#>                Estimate  Std. Error z value  Pr(>|z|)
#> (Intercept)  2.5745e-01  3.0703e-02  8.3851 < 2.2e-16
#> LGH85       -3.2962e-02  4.4472e-03 -7.4119 1.246e-13
#> BANK         6.6493e-05  1.3086e-05  5.0811 3.753e-07
#> UNI01        4.6884e-04  8.4080e-05  5.5762 2.459e-08
#> PAT00        4.7256e-02  1.8113e-02  2.6090  0.009082
#> 
#> Rho: 0.36545, LR test value: 16.353, p-value: 5.2578e-05
#> Asymptotic standard error: 0.078929
#>     z-value: 4.6302, p-value: 3.6538e-06
#> Wald statistic: 21.438, p-value: 3.6538e-06
#> 
#> Log likelihood: 447.309 for lag model
#> ML residual variance (sigma squared): 3.7602e-05, (sigma: 0.006132)
#> Number of observations: 122 
#> Number of parameters estimated: 7 
#> AIC: -880.62, (AIC for lm: -866.27)
#> LM test for residual autocorrelation
#> test value: 6.4996, p-value: 0.01079
\end{verbatim}

\hypertarget{comparando-sar-contra-sdm}{%
\subsection{Comparando SAR contra SDM}\label{comparando-sar-contra-sdm}}

A continuación se muestra el resultado el LR test comparando la estimación del modelo SDM contra el modelo SAR, observando el valor del p-value de la prueba es posible rechazar que el valor de los parámetros restringidos sea cero y por tanto el modelo SDM es más adecuado para explicar este fenómeno que el modelo SAR

\begin{verbatim}
#> 
#>  Likelihood ratio for spatial linear models
#> 
#> data:  
#> Likelihood ratio = 11.559, df = 4, p-value = 0.02095
#> sample estimates:
#> Log likelihood of gdpsdm Log likelihood of gdpsar 
#>                 453.0885                 447.3090
\end{verbatim}

\hypertarget{interpretaciuxf3n-de-los-estimadores-de-los-modelos-de-autocorrelaciuxf3n-espacial}{%
\subsection{\texorpdfstring{Interpretación de los estimadores de los modelos de autocorrelación espacial \index{Interpretación de modelos espaciales}}{Interpretación de los estimadores de los modelos de autocorrelación espacial }}\label{interpretaciuxf3n-de-los-estimadores-de-los-modelos-de-autocorrelaciuxf3n-espacial}}

Sólo en los modelos de autocorrelación espacial en los que el efecto endógeno (\(\mathbf{W}y\)) no está presente en la parte derecha del modelo, los coeficientes estimados (\(\hat{\beta}\)) pueden interpretarse de forma directa, como en el modelo básico sin efectos espaciales, porque la forma reducida coincide con la forma estructural. Es decir, el efecto marginal de un cambio del valor de una variable explicativa continua para una determinada localización coincide con el valor del estimador correspondiente a dicha variable.

\begin{equation}
\frac{\partial y_i}{\partial x_{i,k}}= \beta_k \label{eq:dpdyx}
\end{equation}

En los modelos SAR y SDM, la correcta interpretación de los estimadores implica antes pasar de la forma estructural a la forma reducida. Así, por ejemplo, en el modelo SAR de la expresión \eqref{eq:sar} la forma reducida sería (bajo ciertas condiciones de invertibilidad):

\begin{equation}
y = (\mathbf{I}-\rho\mathbf{W})^{-1}(\alpha \iota_N+\mathbf{X}\beta+\epsilon) \label{eq:multiW}
\end{equation}

El término \((\mathbf{I}-\rho\mathbf{W})^{-1}\) es denominado multiplicador espacial y, utilizando la expansión potencial, puede expresarse también del modo siguiente:

\begin{equation}
(\mathbf{I}-\rho\mathbf{W})^{-1}=\mathbf{I}+\rho\mathbf{W}+\rho^2\mathbf{W}^2+\dots
\end{equation}

Si se utiliza esta nueva expresión en la ecuación \eqref{eq:multiW} para la media condicional, tenemos que el valor de \(y\) en una determinada localización i es función no sólo del valor de las variables explicativas en esa localización, sino también del valor de las explicativas en las localizaciones vecinas (a través de término \(\rho\mathbf{WX}\beta\)), del valor de las explicativas en las localizaciones vecinas a las vecinas (a través del término \(\rho^2\mathbf{W}^2\mathbf{X}\beta\)), etc.

\begin{equation}
E(y|\mathbf{X})=\mathbf{X}\beta+\rho\mathbf{W}\mathbf{X}\beta+\rho^2\mathbf{W}^2\mathbf{X}\beta++\rho^3\mathbf{W}^3\mathbf{X}\beta+\dots
\end{equation}

\citet{lesage2009introduction} presentan los efectos causados por el cambio en el valor de una localización, \(j\), de la variable \(x_k\) sobre otra localización, \(i\), de la variable \(y\) como una matriz completa, \(\mathbf{S}_k(\mathbf{W})_{ij}\), de orden \(n\times n\). Así, cada variable explicativa \(x_k\) del modelo tendrá una matriz completa propia de impactos sobre la variable dependiente.

\begin{equation}
\mathbf{S}_k(\mathbf{W})_{ij}=\frac{\partial y_i}{\partial x'_k}=(\mathbf{I_n}-\rho\mathbf{w})^-1\beta_k
\end{equation}

En este caso, también podemos distinguir efectos directos e indirectos. El efecto directo sería el efecto causado por cambios en el valor de \(x_k\), en una localización \(i\) sobre el valor de \(y\) en esa misma localización \(i\). Estos efectos constituyen los valores de la diagonal principal de la matriz \(\mathbf{S}_k(\mathbf{W})_{ii}\). El efecto indirecto está constituido por el resto de los valores de la matriz \(\mathbf{S}_k(\mathbf{W})_{ij}\), que serían los ``bucles de retroalimentación'' en los que el valor de \(x_k\), en una localización \(j\) afecta al valor de \(y\) en la localización \(i\), y viceversa, pudiéndose dar recorridos más largos en los que el efecto en una localización podría llegar a la última observación \(n\) y luego volver de nuevo al punto de partida.

Por ejemplo, el valor de \(\mathbf{S}_k(\mathbf{W})_{11}\) significaría el efecto directo de un cambio en el valor de la variable \(x_k\) en la localización 1 (\(x_{k1}\)) sobre el valor de la variable y en esa misma localización 1 (\(y_1\)), mientras que el valor de \(\mathbf{S}_k(\mathbf{W})_{12}\) sería el efecto indirecto de un cambio en el valor de la variable \(x_{k1}\) sobre el valor de la variable y en la localización 2 (\(y_2\)). En las filas, la matriz \(\mathbf{S}_k(\mathbf{W})_{ij}\) tiene los efectos de \(x_k\) desde cada localización \(i\) ``hacia'' todas y cada una de las localizaciones \(j\), mientras que las columnas representan el efecto sobre cada localización \(j\) ``desde'' todas y cada una de las localizaciones \(i\).

Dado que no es posible construir un proceso inferencial para todos los impactos de la matriz \(\mathbf{S}_k(\mathbf{W})_{ij}\) LeSage y Pace proponen realizar la inferencia sobre el valor medio de los efectos directos y totales, extrayendo por diferencia los efectos indirectos:
\begin{align}
\bar{M}(k)_{directo}=tr(\mathbf{S}_k(\mathbf{W}))/n \\
\bar{M}(k)_{total}=\mathbf{\iota'_n}\mathbf{S}_k(\mathbf{W})\mathbf{\iota_n}/n \\
\bar{M}(k)_{indirecto}=\bar{M}(k)_{total}- \bar{M}(k)_{directo}
\end{align}

El siguiente conjunto de secuencias presentan el cálculo de las matrices de efectos y la inferencia para los modelos SAR y SDM correspondientes al ejemplo del modelo anteriormente estimado para los municipios urbanos de España.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(coda)}
\CommentTok{\# Cálculo de los efectos para el modelo SAR (LeSage y Pace)}
\NormalTok{Wsp }\OtherTok{\textless{}{-}} \FunctionTok{as}\NormalTok{(}\FunctionTok{as\_dgRMatrix\_listw}\NormalTok{(dmins), }\StringTok{"CsparseMatrix"}\NormalTok{)}
\NormalTok{trMat }\OtherTok{\textless{}{-}} \FunctionTok{trW}\NormalTok{(Wsp, }\AttributeTok{type =} \StringTok{"mult"}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{) }\CommentTok{\# Simulaciones para el proceso inferencial}
\NormalTok{gdpsar\_impacts }\OtherTok{\textless{}{-}} \FunctionTok{impacts}\NormalTok{(gdpsar, }\AttributeTok{tr =}\NormalTok{ trMat, }\AttributeTok{R =} \DecValTok{1000}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(gdpsar\_impacts, }\AttributeTok{zstats =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{short =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{HPDinterval}\NormalTok{(gdpsar\_impacts, }\AttributeTok{choice =} \StringTok{"direct"}\NormalTok{)}
\FunctionTok{HPDinterval}\NormalTok{(gdpsar\_impacts, }\AttributeTok{choice =} \StringTok{"indirect"}\NormalTok{)}
\FunctionTok{HPDinterval}\NormalTok{(gdpsar\_impacts, }\AttributeTok{choice =} \StringTok{"total"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(gdpsar\_impacts, }\AttributeTok{choice =} \StringTok{"direct"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(gdpsar\_impacts, }\AttributeTok{choice =} \StringTok{"indirect"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(gdpsar\_impacts, }\AttributeTok{choice =} \StringTok{"total"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(gdpsar\_impacts, }\AttributeTok{trace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{density =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{choice =} \StringTok{"total"}\NormalTok{)}

\CommentTok{\# Cálculo de la matriz de impactos para la variable LGH85}
\NormalTok{clear.pr }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\FunctionTok{dim}\NormalTok{(gdpmap)[}\DecValTok{1}\NormalTok{])}
\FunctionTok{names}\NormalTok{(clear.pr) }\OtherTok{\textless{}{-}}\NormalTok{ gdpmap}\SpecialCharTok{$}\NormalTok{MUNICIPIO}
\NormalTok{svec }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{dim}\NormalTok{(gdpmap)[}\DecValTok{1}\NormalTok{])}
\NormalTok{eye }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{nrow =} \FunctionTok{dim}\NormalTok{(gdpmap)[}\DecValTok{1}\NormalTok{], }\AttributeTok{ncol =} \FunctionTok{dim}\NormalTok{(gdpmap)[}\DecValTok{1}\NormalTok{])}
\FunctionTok{diag}\NormalTok{(eye) }\OtherTok{\textless{}{-}} \DecValTok{1}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(clear.pr)) \{}
\NormalTok{  cvec }\OtherTok{\textless{}{-}}\NormalTok{ svec}
\NormalTok{  cvec[i] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(eye }\SpecialCharTok{{-}}\NormalTok{ gdpsar[[}\StringTok{"rho"}\NormalTok{]] }\SpecialCharTok{*}\NormalTok{ Wsp) }\SpecialCharTok{\%*\%}\NormalTok{ cvec }\SpecialCharTok{*}\NormalTok{ gdpsar[[}\StringTok{"coefficients"}\NormalTok{]][[}\StringTok{"LGH85"}\NormalTok{]]}
\NormalTok{  clear.pr[i] }\OtherTok{\textless{}{-}}\NormalTok{ res[i]}
\NormalTok{\}}
\NormalTok{mult }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(eye }\SpecialCharTok{{-}}\NormalTok{ gdpsar[[}\StringTok{"rho"}\NormalTok{]] }\SpecialCharTok{*}\NormalTok{ Wsp)}
\NormalTok{deriv\_LGH85 }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(eye }\SpecialCharTok{{-}}\NormalTok{ gdpsar[[}\StringTok{"rho"}\NormalTok{]] }\SpecialCharTok{*}\NormalTok{ Wsp) }\SpecialCharTok{*}\NormalTok{ gdpsar[[}\StringTok{"coefficients"}\NormalTok{]][[}\StringTok{"LGH85"}\NormalTok{]]}

\CommentTok{\# Cálculo de los efectos para el modelo SDM (LeSage y Pace)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{) }\CommentTok{\# Simulaciones para el proceso inferencial}
\NormalTok{gdpsdm\_impacts }\OtherTok{\textless{}{-}} \FunctionTok{impacts}\NormalTok{(gdpsdm, }\AttributeTok{tr =}\NormalTok{ trMat, }\AttributeTok{R =} \DecValTok{1000}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(gdpsdm\_impacts, }\AttributeTok{zstats =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{short =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{HPDinterval}\NormalTok{(gdpsdm\_impacts, }\AttributeTok{choice =} \StringTok{"direct"}\NormalTok{)}
\FunctionTok{HPDinterval}\NormalTok{(gdpsdm\_impacts, }\AttributeTok{choice =} \StringTok{"indirect"}\NormalTok{)}
\FunctionTok{HPDinterval}\NormalTok{(gdpsdm\_impacts, }\AttributeTok{choice =} \StringTok{"total"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(gdpsdm\_impacts, }\AttributeTok{choice =} \StringTok{"direct"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(gdpsdm\_impacts, }\AttributeTok{choice =} \StringTok{"indirect"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(gdpsdm\_impacts, }\AttributeTok{choice =} \StringTok{"total"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(gdpsdm\_impacts, }\AttributeTok{trace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{density =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{choice =} \StringTok{"total"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{impacto-del-sar}{%
\subsection{Impacto del SAR}\label{impacto-del-sar}}

A continuación se presenta la salida del cálculo de los efectos para el modelo SAR \citep{lesage2009introduction} donde puede notarse que todos los impactos son estadísticamente significativos y salvo la variable LGH85 todos tienen efectos positivos sobre la variable LPGH

\begin{verbatim}
#> Impact measures (lag, trace):
#>              Direct      Indirect         Total
#> LGH85 -3.361200e-02 -1.833440e-02 -0.0519463985
#> BANK   6.780328e-05  3.698479e-05  0.0001047881
#> UNI01  4.780827e-04  2.607807e-04  0.0007388635
#> PAT00  4.818749e-02  2.628492e-02  0.0744724116
#> ========================================================
#> Simulation results ( variance matrix):
#> ========================================================
#> Simulated standard errors
#>             Direct     Indirect        Total
#> LGH85 4.330382e-03 5.661226e-03 7.112130e-03
#> BANK  1.299226e-05 1.470493e-05 2.376829e-05
#> UNI01 8.505222e-05 9.339449e-05 1.461224e-04
#> PAT00 1.833567e-02 1.535729e-02 3.175518e-02
#> 
#> Simulated z-values:
#>          Direct  Indirect     Total
#> LGH85 -7.731876 -3.314485 -7.346044
#> BANK   5.181691  2.604510  4.443778
#> UNI01  5.651099  2.901134  5.143554
#> PAT00  2.623368  1.821494  2.395654
#> 
#> Simulated p-values:
#>       Direct     Indirect   Total     
#> LGH85 1.0658e-14 0.00091812 2.0406e-13
#> BANK  2.1988e-07 0.00920058 8.8393e-06
#> UNI01 1.5943e-08 0.00371815 2.6959e-07
#> PAT00 0.0087065  0.06853186 0.016591
\end{verbatim}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/impactoSAR-1} \end{center}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/impactoSAR-2} \end{center}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/impactoSAR-3} \end{center}

\begin{infobox_resume}
\textbf{RESUMEN}
En este capítulo se introduce a la moderación del espacio en la estimación econométrica. Primero, observamos la heterogeneidad espacial de los datos a partir de los mapas temáticos y el uso de indicador de autocorrelación espacial de Moran. Luego, construimos la matriz de pesos espaciales bajo distintas especificaciones. Y, por último, se muestra la taxonomía de los modelos econométricos espaciales presentado la estrategia de especificación híbrida y la interpretación de los coeficientes estimados

\end{infobox_resume}

\hypertarget{procesos-de-punto}{%
\chapter{Procesos de punto}\label{procesos-de-punto}}

\emph{Jorge Mateu y Mehdi Moradi}

\hypertarget{spatial-point-patterns-on-mathbb-r2}{%
\section{\texorpdfstring{Spatial point patterns on \(\mathbb R^2\)}{Spatial point patterns on \textbackslash mathbb R\^{}2}}\label{spatial-point-patterns-on-mathbb-r2}}

Let \({\mathbf x}=\{ x_1, x_2, \ldots, x_n \}, 0 \leq n < \infty,\) be an observed realisation of a simple (i.e no multiple events per location) and finite point process \(X\) on \(\mathbb R^2\) within the window \(W \subset \mathbb R^2\) and with the associated distance metric \(d(u,v)\). For any arbitrary set \(A \subset \mathbb R^2\), the cardinality of \(X\) is governed by the count function

\begin{equation*}
N(X \cap A) 
=
\sum\limits_{x \in X} {\mathbf 1} \{ x \in A \} < \infty
.
\end{equation*}

Moreover, according to the Campbell formula \citep{BRT15}, for any measurable function \(f: \mathbb R^2 \to [0, \infty)\) we have

\begin{equation*}\label{eq:compR2}
\mathbb{E}
\left[
\sum\limits_{x \in X} 
f(x)
\right]
=
\int_{\mathbb R^2}
 f(u)
\lambda(u) 
\mathrm{d}u,
\end{equation*}
where \(\lambda (\cdot)\) is called the intensity function of \(X\), and governs its spatial distribution. In fact, \(\lambda(u)\) gives the expected number of points per unit area in the vicinity of \(u \in \mathbb R^2\). Letting \(f(x) = {\mathbf 1} \{ x \in A \}\), we can easily see the relationship between the intensity function \(\lambda(\cdot)\) and the count function \(N\) as

\begin{equation*}
\mathbb{E}
\left[
N(X \cap A)
\right]
=
\int_A
\lambda(u) 
\mathrm{d}u.
\end{equation*}

If the intensity function \(\lambda(\cdot)\) is constant, i.e.~\(\lambda(\cdot) = \lambda\),
the process \(X\) is called homogeneous, otherwise it is called inhomogeneous, meaning that its spatial distribution varies over its state space. For further details we encourage the interested reader to see \citet{moller2003,illian2008,diggle2013,BRT15}.

In practice one often observes a single realisation, thus it is of great importance to have an estimate of \(\lambda(\cdot)\) that can mimic the spatial distribution of the underlying process, which is supposed to have generated the observed pattern. Hence, we next go through different non-parametric intensity estimators.

\hypertarget{kernel-based-intensity-estimation}{%
\subsection{Kernel-based intensity estimation}\label{kernel-based-intensity-estimation}}

The two most frequently used non-parametric kernel-based intensity estimators for spatial point patterns on \(\mathbb R^2\) are
\begin{equation}
\label{e:kde.2D.unif}
\widehat \lambda^{\text{U}}_{\sigma}(u)
= 
\frac{1}{c_{\sigma,W}(u)} \sum_{i=1}^n \kappa_{\sigma}(u - x_i),
\quad u \in W,
\end{equation}
and
\begin{equation}
\label{e:kde.2D.JD}
\widehat \lambda^{\text{JD}}_{\sigma}(u)
= 
\sum_{i=1}^n \frac{\kappa_{\sigma}(u - x_i)}{c_{\sigma,W}(x_i)},
\quad u \in W,
\end{equation}
where \(\kappa_{\sigma}\) is a probability density function on \(\mathbb R^2\) with bandwidth \(\sigma\), and

\begin{equation}
\label{e:cW}
c_{\sigma,W}(u)
= 
\int_W \kappa_{\sigma}(u - v) \mathrm{d}v,
\quad u \in W,
\end{equation}
is the mass of the kernel centred at \(u \in W\), and plays the role of an edge corrector to compensate for lack of information outside \(W\). Note that we, in practice, only see a realisation of \(X\) within a bounded/limited domain \(W\). Regardless of the choice \(\sigma\), the estimator \eqref{e:kde.2D.unif} is pointwise unbiased if the true intensity function is constant \citep{diggle1985}, while the estimator \eqref{e:kde.2D.JD} conserves the total mass \citep{jones1993simple}. We note that the estimators \eqref{e:kde.2D.unif} and \eqref{e:kde.2D.JD} are often called \texttt{uniformly-edge-corrected} and \texttt{Jones-Diggle} \citep{rakshit2019fast}. Throughout, we consider the well-known Gaussian kernel function \citep[Page. 42]{silverman1986}

Regarding practical concerns, the performance of kernel-based intensity estimators strongly depends on the bandwidth parameter, so that a small bandwidth leads to an under-smoothed estimate (low bias and high variance) whereas a large bandwidth results in an over-smoothed estimate (high bias and low variance). For a given point pattern \({\mathbf x}\), the estimators \eqref{e:kde.2D.unif} and \eqref{e:kde.2D.JD} can be calculated using the function \textsf{density.ppp} from \textsf{spatstat.core} by specifying \textsf{diggle=FALSE} and \textsf{diggle=TRUE}, respectively.

\hypertarget{bandwidth-selection}{%
\subsubsection{Bandwidth selection}\label{bandwidth-selection}}

\cite{scott1992} proposed to choose the bandwidth through the rule of thumb

\[
(s_x n^{-1/6}, s_y n^{-1/6}),
\]
for each of the \(x,y\) Cartesian coordinates, in which \(s_x, s_y\) stand for the sample standard deviation of the \(x,y\) coordinate values of data locations. Despite being a rule of thumb, this procedure can be quite useful for exploratory analysis. The function \textsf{bw.scott} from \textsf{spatstat.core} calculates such rule of thumb.

\cite{cronie2018} proposed to find an optimum bandwidth by minimising

\[
CvL(\sigma)
=
\left(
|W|
-
\sum\limits_{i=1}^n 
1
/
\widehat{\lambda}^*_{\sigma}(x_i)
\right)^2,
\]
where \(\widehat{\lambda}^*_{\sigma}(x_i)\) is a non-corrected intensity estimate (either of \eqref{e:kde.2D.unif} or \eqref{e:kde.2D.JD} without the correction term) at \(x_i\) using the bandwidth \(\sigma\). The idea behind this proposal lies on the fact that, using Campbell formula, we have

\[
\mathbb{E}
\left[
\sum\limits_{x \in X}
1
/
\lambda(x)
\right]
=
\int_W 
(1
/
\lambda(x)
)
\lambda(x)
\mathrm{d}u
=
|W|.
\]
For a given point pattern \({\mathbf x}\), the function \textsf{bw.CvL} from \textsf{spatstat.core} calculates the bandwidth using Cronie and van Lieshout's criterion.

\hypertarget{practical-examples}{%
\subsection{Practical examples}\label{practical-examples}}

We next employ the above-mentioned intensity estimators and bandwidth selection approaches to study the spatial distribution of two datasets: Wildfire in Nepal (Figure \ref{fig:nepal}), and Crime in Medellín, Colombia (Figure \ref{fig:medellin}). Throughout this section, we make use of the \textsf{R} packages \textsf{spatstat,~version 2.3-0,} \citep{spatstat,BRT15} for point pattern analysis and \textsf{raster,~version 3.5-15,} \citep{raster} for graphical representations. We add that the package \textsf{spatstat} is recently divided into a family of sub-packages \textsf{spatstat.utils}, \textsf{spatstat.data}, \textsf{spatstat.sparse}, \textsf{spatstat.geom}, \textsf{spatstat.random}, \textsf{spatstat.core}, \textsf{spatstat.linnet}, so that \textsf{spatstat} itself acts as an umbrella package. We warn the readers to be aware of future changes of \textsf{spatstat} to comply with CRAN's suggestions concerning package size.

\hypertarget{example-1-wildfires-in-nepal}{%
\subsubsection{Example 1: Wildfires in Nepal}\label{example-1-wildfires-in-nepal}}

By courtesy of Ganesh Prasad Sigdel, we present here the following data for Nepal which contains (forest) wildfires in Nepal as geo-referenced point data during 2016, obtained from ICIMOD-Nepal. During the studied period Nepal experienced 5757 forest fires in which 475 of them happened in Surkhet District that is a district in Karnali Province of mid-western Nepal. We below begin with loading the essential \textsf{R} packages for this section.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(spatstat)}
\FunctionTok{library}\NormalTok{(raster)}
\end{Highlighting}
\end{Shaded}

Below we estimate the smoothing bandwidth parameter for the considered forest fire data using the reviewed methods in Section \ref{sec:bw}. Scott's rule gives (50253.47 m, 21158.42 m) and Cronie and van Lieshout's criterion (CvL) estimates the bandwidth as 36513.16 m. We note that one may wish to search for an optimum bandwidth over a finer grid of possible values. This can currently be controlled by arguments \textsf{ns} for \textsf{bw.CvL}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(nepal)}
\NormalTok{scott\_nepal }\OtherTok{\textless{}{-}} \FunctionTok{bw.scott}\NormalTok{(nepal) }\CommentTok{\# Scott’s rule}
\NormalTok{CvL\_nepal }\OtherTok{\textless{}{-}} \FunctionTok{bw.CvL}\NormalTok{(nepal) }\CommentTok{\# Cronie and van Lieshout’s criterio}
\end{Highlighting}
\end{Shaded}

Having estimated bandwidth through different methods, we next estimate the intensity of the wildfire data by employing the intensity estimators \eqref{e:kde.2D.unif} and \eqref{e:kde.2D.JD}. The function \textsf{density.ppp} provides kernel-based intensity estimation for point patterns on \(\mathbb R^2\), noting that as default it currently makes use of the uniformly-edge-corrected estimator \eqref{e:kde.2D.unif} with the Gaussian kernel function. We set \textsf{leaveoneout=FALSE} to not calculate the so-called leave-one-out estimator, and \textsf{positive=TRUE} to force density values to be positive. We note that due to numerical errors inherent in the Fast Fourier Transform (finite-precision arithmetic) we might get quite small negative values in very sparse areas, see the help page of \textsf{density.ppp}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d\_scott\_nepal }\OtherTok{\textless{}{-}} \FunctionTok{density.ppp}\NormalTok{(nepal, }\AttributeTok{sigma =}\NormalTok{ scott\_nepal, }\AttributeTok{leaveoneout =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{positive =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{d\_cvl\_nepal }\OtherTok{\textless{}{-}} \FunctionTok{density.ppp}\NormalTok{(nepal, }\AttributeTok{sigma =}\NormalTok{ CvL\_nepal, }\AttributeTok{leaveoneout =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{positive =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we turn to estimate the intensity through the Jones-Diggle estimator \eqref{e:kde.2D.unif} by setting \textsf{diggle=TRUE} in \textsf{density.ppp}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d\_scott\_dig\_nepal }\OtherTok{\textless{}{-}} \FunctionTok{density.ppp}\NormalTok{(nepal, }\AttributeTok{sigma =}\NormalTok{ scott\_nepal, }\AttributeTok{leaveoneout =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{positive =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{diggle =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{d\_cvl\_dig\_nepal }\OtherTok{\textless{}{-}} \FunctionTok{density.ppp}\NormalTok{(nepal, }\AttributeTok{sigma =}\NormalTok{ CvL\_nepal, }\AttributeTok{leaveoneout =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{positive =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{diggle =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Having obtained different intensity estimates after considering distinct bandwidth selection approaches, we next aim to show the obtained estimates and discuss their possible discrepancies. For a better graphical representation, we convert the obtained intensity images from class \textsf{im} to objects of class \textsf{raster} and then merge them into a \textsf{RasterStack}. Figure \ref{fig:kernelnepal} shows different intensity estimates in which a high intensity is assigned to the south and south-west of Nepal highlighting a non-uniform distribution for the forest fire data. The slight difference between the maps generated by the Scott's rule and CvL is that, unlike CvL, Scott's rule further discloses an area of higher intensity in the south-west.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sp\_int\_nepal }\OtherTok{\textless{}{-}} \FunctionTok{stack}\NormalTok{(}\FunctionTok{raster}\NormalTok{(d\_scott\_nepal), }\FunctionTok{raster}\NormalTok{(d\_cvl\_nepal), }\FunctionTok{raster}\NormalTok{(d\_scott\_dig\_nepal), }\FunctionTok{raster}\NormalTok{(d\_cvl\_dig\_nepal))}
\NormalTok{sp\_int\_nepal }\OtherTok{\textless{}{-}}\NormalTok{ sp\_int\_nepal }\SpecialCharTok{*} \DecValTok{10}\SpecialCharTok{\^{}}\DecValTok{7}
\FunctionTok{names}\NormalTok{(sp\_int\_nepal) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"scott\_gaus\_U"}\NormalTok{, }\StringTok{"CvL\_gaus\_U"}\NormalTok{, }\StringTok{"scott\_gaus\_JD"}\NormalTok{, }\StringTok{"CvL\_gaus\_JD"}\NormalTok{)}

\NormalTok{at }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{1.4}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}
\NormalTok{pts\_nepal }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(nepal)}
\FunctionTok{coordinates}\NormalTok{(pts\_nepal) }\OtherTok{\textless{}{-}} \ErrorTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ y}
\FunctionTok{spplot}\NormalTok{(sp\_int\_nepal, }\AttributeTok{at =}\NormalTok{ at, }\AttributeTok{scales =} \FunctionTok{list}\NormalTok{(}\AttributeTok{draw =} \ConstantTok{FALSE}\NormalTok{), }\AttributeTok{col.regions =} \FunctionTok{rev}\NormalTok{(}\FunctionTok{topo.colors}\NormalTok{(}\DecValTok{20}\NormalTok{)), }\AttributeTok{colorkey =} \FunctionTok{list}\NormalTok{(}\AttributeTok{labels =} \FunctionTok{list}\NormalTok{(}\AttributeTok{cex =} \DecValTok{3}\NormalTok{)), }\AttributeTok{par.strip.text =} \FunctionTok{list}\NormalTok{(}\AttributeTok{cex =} \DecValTok{3}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{layer}\NormalTok{(}\FunctionTok{sp.points}\NormalTok{(pts\_nepal, }\AttributeTok{pch =} \DecValTok{20}\NormalTok{, }\AttributeTok{col =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/nepal2016} 

}

\caption{Kernel-based intensity estimation for the wildfire data in Nepal during 2016. Layer's names start with bandwidth selection approach followed by the employed kernel and the edge-correction.  Intensity values are forest fires per ten thousand km.}\label{fig:kernelnepal}
\end{figure}

\hypertarget{example-2-crimes-in-medelluxedn}{%
\subsubsection{Example 2: Crimes in Medellín}\label{example-2-crimes-in-medelluxedn}}

Medellín is the second most populated city in Colombia\footnote{DANE (2019) Proyecciones de población departamentales y municipales por área 2005–2020. \href{www.dane.gov.co.}{www.dane.gov.co.}} with an urban territory enclosing an area of roughly \(105\)km\(^2\). Medellín has suffered with crime for many years, being known as home of dangerous criminals. In 2018 Secretariat of Security of Medellín reported that \(40\%\) of the citizens felt insecure, having delivered about 20607 theft complaints \footnote{Restrepo V (2019) \href{https://www.elcolombiano.com/antioquia/seguridad/percepcion-de-seguridad-en-medellin-encuesta-de-victimizacion-PC10033581}{¿qué tan segura se siente la gente en medellín? El Colombiano.}}. In addition, the police department recognises the need of hiring almost 2000 more policemen to fight against homicide, theft and micro-traffic\footnote{Monsalve C (2019) \href{https://www.bluradio.com/blu360/antioquia/medellin-necesita-2-000-uniformados-mas-para-reforzar-seguridad-policia}{Medellín necesita 2000 uniformados más para reforzar seguridad. Blu Radio.}}.

In this chapter, we only analyse the spatial distribution of the locations of crimes occurred in Medellín during 2005 \citep{sanabria2022}. During 2005, 910 crimes are committed for which the percentage of men being victims is \(66\%\), \(28\%\) of crimes correspond to weekends, the percentage of hold-up theft is \(42\%\) being the most common type, and the percentage of victims having an age between 20 and 40 increases up to \(60\%\).

We point out that this dataset contains types of crimes which do not necessarily happen over the street network of Medellín such as card cloning, hoax, scopolamine-related, etc. Thus, we consider this dataset as a point pattern on \(\mathbb R^2\). One could, however, make a classification of crime types.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(medellin)}
\NormalTok{scott\_med }\OtherTok{\textless{}{-}} \FunctionTok{bw.scott}\NormalTok{(medellin) }\CommentTok{\# Scott’s rule}
\NormalTok{CvL\_med }\OtherTok{\textless{}{-}} \FunctionTok{bw.CvL}\NormalTok{(medellin) }\CommentTok{\# Cronie and van Lieshout’s criterio}
\end{Highlighting}
\end{Shaded}

The Scott's rule estimates the bandwidth as (691.31m, 954.20m) and CvL leads to 692.31m. Then, similar to the case of wildfires in Nepal, we make use of the function \textsf{density.ppp} to compute the intensity estimators \eqref{e:kde.2D.unif} and \eqref{e:kde.2D.JD} under the same settings as Section \ref{sec:nepalfire}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# uniformly{-}edge{-}corrected estimator and Gaussian kernel}
\NormalTok{d\_scott\_med }\OtherTok{\textless{}{-}} \FunctionTok{density.ppp}\NormalTok{(medellin, }\AttributeTok{sigma =}\NormalTok{ scott\_med, }\AttributeTok{leaveoneout =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{positive =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{d\_cvl\_med }\OtherTok{\textless{}{-}} \FunctionTok{density.ppp}\NormalTok{(medellin, }\AttributeTok{sigma =}\NormalTok{ CvL\_med, }\AttributeTok{leaveoneout =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{positive =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Jones{-}Diggle estimator and Gaussian kernel}
\NormalTok{d\_scott\_dig\_med }\OtherTok{\textless{}{-}} \FunctionTok{density.ppp}\NormalTok{(medellin, }\AttributeTok{sigma =}\NormalTok{ scott\_med, }\AttributeTok{leaveoneout =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{positive =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{diggle =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{d\_cvl\_dig\_med }\OtherTok{\textless{}{-}} \FunctionTok{density.ppp}\NormalTok{(medellin, }\AttributeTok{sigma =}\NormalTok{ CvL\_med, }\AttributeTok{leaveoneout =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{positive =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{diggle =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Figure \ref{fig:medker} shows the estimated intensities using different choices of bandwidth selection methods. At a glance, we can generally see a non-uniform distribution for crimes in Medellín. Regardless of the choice of methodology for choosing bandwidth, the estimates highlight two big hotspots in the central area of Medellín, possibly with different radius, slight difference in other areas. Similar to the case of wildfires in Nepal, one can see that the effect of edge-corrections are marginal.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sp\_int\_med }\OtherTok{\textless{}{-}} \FunctionTok{stack}\NormalTok{(}\FunctionTok{raster}\NormalTok{(d\_scott\_med), }\FunctionTok{raster}\NormalTok{(d\_cvl\_med), }\FunctionTok{raster}\NormalTok{(d\_scott\_dig\_med), }\FunctionTok{raster}\NormalTok{(d\_cvl\_dig\_med))}
\NormalTok{sp\_int\_med }\OtherTok{\textless{}{-}}\NormalTok{ sp\_int\_med }\SpecialCharTok{*} \DecValTok{10}\SpecialCharTok{\^{}}\DecValTok{5}
\FunctionTok{names}\NormalTok{(sp\_int\_med) }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(sp\_int\_nepal)}
\NormalTok{at }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.2}\NormalTok{)}
\NormalTok{pts }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(medellin)}
\FunctionTok{coordinates}\NormalTok{(pts) }\OtherTok{\textless{}{-}} \ErrorTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ y}
\FunctionTok{spplot}\NormalTok{(all, }\AttributeTok{at =}\NormalTok{ at, }\AttributeTok{scales =} \FunctionTok{list}\NormalTok{(}\AttributeTok{draw =} \ConstantTok{FALSE}\NormalTok{), }\AttributeTok{col.regions =} \FunctionTok{rev}\NormalTok{(}\FunctionTok{topo.colors}\NormalTok{(}\DecValTok{20}\NormalTok{)), }\AttributeTok{colorkey =} \FunctionTok{list}\NormalTok{(}\AttributeTok{labels =} \FunctionTok{list}\NormalTok{(}\AttributeTok{cex =} \DecValTok{3}\NormalTok{)), }\AttributeTok{par.strip.text =} \FunctionTok{list}\NormalTok{(}\AttributeTok{cex =} \DecValTok{3}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{layer}\NormalTok{(}\FunctionTok{sp.points}\NormalTok{(pts, }\AttributeTok{pch =} \DecValTok{20}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/med2005} 

}

\caption{Kernel-based intensity estimation for the crime data in Medellín, Colombia, during 2005. Layer's names start with bandwidth selection approach followed by the employed kernel and the edge-correction. Intensity values are crime per haundred km.}\label{fig:medker}
\end{figure}

\hypertarget{kernel-based-intensity-estimation-over-irregular-domains}{%
\subsection{Kernel-based intensity estimation over irregular domains}\label{kernel-based-intensity-estimation-over-irregular-domains}}

The estimators \eqref{e:kde.2D.unif} and \eqref{e:kde.2D.JD} may suffer from important issues such as tunnelling the mass, bias near the boundary, simultaneous over- and under-smoothing artefacts, leading to physically implausible results under certain situations \citep{baddeley2022diffusion}. These problems are more noticeable when data are observed over an irregular domain. As a remedy \cite{baddeley2022diffusion} proposed to estimate the intensity via the heat kernel which can be defined as the transition probability density of a Brownian motion on \(W\) that respects a boundary. Indeed, their proposal, which is called the diffusion estimator, is of the form

\begin{equation}\label{eq:heat}
\widehat \lambda_t (u)
=
\sum\limits_{i=1}^n 
\kappa_t (u|x_i)
,
\end{equation}
where \(t= \sigma^2\) (\(\sigma\) being the smoothing bandwidth in \eqref{e:kde.2D.unif} and \eqref{e:kde.2D.JD}) and \(\kappa_t (\cdot|x_i)\) is the heat kernel. This estimator is both unbiased, under homogeneity, and mass-conserving simultaneously. \cite{baddeley2022diffusion} further extended some of the bandwidth selection approaches to their proposal including the likelihood cross validation and Cronie--Van Lieshout criterion. The diffusion estimator above is currently accessible through the function \textsf{densityHeat.ppp}, the versions of Cronie and Van Lieshout criterion and likelihood cross validation to find an optimum bandwidth are accommodated in the functions \textsf{bw.CvLHeat} and \textsf{bw.pplHeat}. All such functions belong to \textsf{spatstat.core}.

Below we employ the diffusion estimator to briefly see its performance compared to that of \eqref{e:kde.2D.unif} through an application of active fires within USA and Central America (discarding the islands) from 24th February to 3rd March 2022\footnote{\href{https://firms.modaps.eosdis.nasa.gov/active\_fire/}{https://firms.modaps.eosdis.nasa.gov/active\_fire/}}. Active fires refer to Near Real-Time (NRT) fire/thermal anomaly data which represents the centres of some 1km pixels that are flagged by an algorithm as containing one or more fires. The locations do not necessarily confirm fires, but rather susceptible pixels for which the difference between the land cover around them and a potential fire meets a given threshold.

The smoothing bandwidths for the computation of both estimators \eqref{e:kde.2D.unif} and \eqref{eq:heat} are chosen by the Cronie and Van Lieshout criterion. Here, due to dealing with a quite bigger area compared to our previous examples, we consider \textsf{ns=50}, meaning that a vector of size 50 is used to search for an optimum bandwidth (default is 16), and \textsf{dimyx=512} to get intensity images with better resolution (default is an image of size \(128\times128\)). The chosen bandwidths to compute \eqref{e:kde.2D.unif} and \eqref{eq:heat} are approximately 556.3km and 104.9km.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(activefires)}
\NormalTok{CvL\_northcentre }\OtherTok{\textless{}{-}} \FunctionTok{bw.CvL}\NormalTok{(activefires, }\AttributeTok{ns =} \DecValTok{50}\NormalTok{)}
\NormalTok{d\_CvL\_northcentre }\OtherTok{\textless{}{-}} \FunctionTok{density.ppp}\NormalTok{(activefires, }\AttributeTok{sigma =}\NormalTok{ CvL\_northcentre, }\AttributeTok{leaveoneout =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{dimyx =} \DecValTok{512}\NormalTok{)}

\NormalTok{heat\_CvL\_northcentre }\OtherTok{\textless{}{-}} \FunctionTok{bw.CvLHeat}\NormalTok{(activefires, }\AttributeTok{ns =} \DecValTok{50}\NormalTok{)}
\NormalTok{dheat\_CvL\_northcentre }\OtherTok{\textless{}{-}} \FunctionTok{densityHeat}\NormalTok{(activefires, }\AttributeTok{sigma =}\NormalTok{ heat\_CvL\_northcentre, }\AttributeTok{leaveoneout =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{dimyx =} \DecValTok{512}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Both estimates are merged into a \textsf{RasterBrick} object which is represented in Figure \ref{fig:USfiresintensity}. The domain in this example is not regular; the Florida state, (south) lower California state, and all Central America makes the studied region quite irregular. In such a situation, it might be quite an implausible result if our estimate transfers kernel mass across the gulf of California/Mexico between two halves. The intensity map on the left-hand side of Figure \ref{fig:USfiresintensity} shows that the uniformly-edge-corrected estimate suffers from tunnelling the mass across the region. However, the intensity map on the right-hand side, obtained by the diffusion estimator, shows quite less mass migration, and seems to be more realistic if one takes into account the spatial distribution of points.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d\_northcentre\_stack }\OtherTok{\textless{}{-}} \FunctionTok{stack}\NormalTok{(}\FunctionTok{raster}\NormalTok{(d\_CvL\_northcentre), }\FunctionTok{raster}\NormalTok{(dheat\_CvL\_northcentre))}
\FunctionTok{names}\NormalTok{(d\_northcentre\_stack) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"CvL\_gaus\_U"}\NormalTok{, }\StringTok{"Diffusion"}\NormalTok{)}
\NormalTok{pts\_northcentre }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(activefires)}
\FunctionTok{coordinates}\NormalTok{(pts\_northcentre) }\OtherTok{\textless{}{-}} \ErrorTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ y}
\NormalTok{d\_northcentre\_stack }\OtherTok{\textless{}{-}}\NormalTok{ d\_northcentre\_stack }\SpecialCharTok{*} \DecValTok{10}\SpecialCharTok{\^{}}\DecValTok{6}

\FunctionTok{spplot}\NormalTok{(d\_northcentre\_stack, }\AttributeTok{scales =} \FunctionTok{list}\NormalTok{(}\AttributeTok{draw =} \ConstantTok{FALSE}\NormalTok{), }\AttributeTok{col.regions =} \FunctionTok{rev}\NormalTok{(}\FunctionTok{terrain.colors}\NormalTok{(}\DecValTok{20}\NormalTok{)), }\AttributeTok{colorkey =} \FunctionTok{list}\NormalTok{(}\AttributeTok{labels =} \FunctionTok{list}\NormalTok{(}\AttributeTok{cex =} \DecValTok{5}\NormalTok{)), }\AttributeTok{par.strip.text =} \FunctionTok{list}\NormalTok{(}\AttributeTok{cex =} \DecValTok{5}\NormalTok{)) }\SpecialCharTok{+} \FunctionTok{layer}\NormalTok{(}\FunctionTok{sp.points}\NormalTok{(pts\_northcentre, }\AttributeTok{pch =} \DecValTok{20}\NormalTok{, }\AttributeTok{col =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/Fire_NorthCentralAmerica_int} 

}

\caption{Kernel-based intensity estimation for the active fires within USA and Central America (discarding the islands) from 24th February to 3rd March 2022. Left: uniformly-edge-corrected estimate with Gaussian kernel, Right: diffusion estimate. Equivalently optimum bandiwdth for both methods are chosen by the Cronie and Van Lieshout criterion. Intensity values are fires per thousand km }\label{fig:USfiresintensity}
\end{figure}

\hypertarget{voronoi-based-intensity-estimators}{%
\subsection{Voronoi-based intensity estimators}\label{voronoi-based-intensity-estimators}}

As previously noted, the performance of kernel-based intensity estimators strongly depends on the smoothing parameter bandwidth, and even more, so it might be quite hard (if not impossible) to find a single fixed bandwidth which smooths the intensity equally fine over the spatial domain \(W\) when there are abrupt changes in the spatial distribution of points. A spatially-varying (adaptive) bandwidth was then suggested to overcome such issues, but at the cost of increased complexity \citep{davies2018fast,baddeley2022diffusion}. As an alternative approach, and to avoid the need for bandwidth parameter selection in advance, one can employ Voronoi-based intensity estimators which are fully non-parametric \citep{BSV10}.

For any \(x\in{\mathbf x}\), its associated Voronoi/Dirichlet cell \({\mathcal V}_{x}\) that consists of all \(u \in W\) which are closer to \(x\) than any other \(y\in{\mathbf x}\setminus\{x\}\), is given by
\begin{align}
\label{VorCell}
{\mathcal V}_{x}
=
\{ u \in W: d(x,u) \leq d(y,u) \text{ for all } y\in {\mathbf x}\setminus\{x\}\}.
\end{align}
The Voronoi-based intensity estimator, at any arbitrary point \(u \in W\), is then of the form

\begin{align}
\widehat{\lambda}^{V}(u)
=
\sum_{x\in {\mathbf x}}
\frac{{\mathbf 1}\{u\in{\mathcal V}_{x}\}}{|{\mathcal V}_{x}|}.
\label{Vor} 
\end{align}

The estimator \(\widehat{\lambda}^{V}(u)\) conserves mass (similar to \(\widehat \lambda^{\text{JD}}_{\sigma}(u)\)), and is simultaneously unbiased when the true intensity is constant (similar to \(\widehat \lambda^{\text{U}}_{\sigma}(u)\)) which are the properties the diffusion estimator holds as well. However, \cite{moradi2019resample} showed that \(\widehat{\lambda}^{V}(u)\) suffers from a quite high variance which might be indeed a result of under-smoothness in very dense areas and over-smoothness in sparse regions. Thereafter, they proposed to overcome such smoothness issues of \(\widehat{\lambda}^{V}(u)\) by sub-sampling some \(m \geq 1\) rescaled copies of \({\mathbf x}\) through independent \(p\)-thinning. In fact, they proposed to estimate the intensity function through

\begin{align}
\label{SmoothVor}
\widehat{\lambda}_{p,m}^{V}(u)
=
\frac{1}{m}\sum_{i=1}^m
\frac{\widehat{\lambda}_i^{V}(u)}{p}
,
\ u\in W,
\end{align}
where \(\widehat{\lambda}_i^{V}(u)\) corresponds to the Voronoi-based intensity estimator of the \(i\)-th thinned pattern. The estimator \(\widehat{\lambda}_{p,m}^{V}(u)\) is called resample-smoothed, and, in addition to the statistical properties of \(\widehat{\lambda}^{V}(u)\), holds some other favourable statistical properties including a very low variance. Employing such an estimator means that one again needs to deal with parameter selection (\(m,p\)) in advance; however \cite{moradi2019resample} proposed both a rule-of-thumb (\(m=400\) and \(p \leq 0.2\)) and a data-driven cross-validation approach. Both estimators \eqref{Vor} and \eqref{SmoothVor} are accessible through the function \textsf{densityVoronoi.ppp} from \textsf{spatstat.core} in which arguments \textsf{f} and \textsf{nrep} control the probability \(p\) and number of thinnings \(m\), respectively. We note that by setting \textsf{f=1} one can obtain an estimate based on the voronoi estimator \eqref{Vor}.

Next, we estimate the intensity of the wildfire data in Nepal (Section \ref{sec:nepalfire}) using the resample-smoothed Voronoi estimator \eqref{SmoothVor} by considering different choices for the retention probability \(p\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d\_vor\_1\_nepal }\OtherTok{\textless{}{-}} \FunctionTok{densityVoronoi.ppp}\NormalTok{(nepal, }\AttributeTok{f =} \DecValTok{1}\NormalTok{, }\AttributeTok{nrep =} \DecValTok{1}\NormalTok{)}
\NormalTok{d\_vor\_2\_nepal }\OtherTok{\textless{}{-}} \FunctionTok{densityVoronoi.ppp}\NormalTok{(nepal, }\AttributeTok{f =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{nrep =} \DecValTok{400}\NormalTok{)}
\NormalTok{d\_vor\_3\_nepal }\OtherTok{\textless{}{-}} \FunctionTok{densityVoronoi.ppp}\NormalTok{(nepal, }\AttributeTok{f =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{nrep =} \DecValTok{400}\NormalTok{)}
\NormalTok{d\_vor\_4\_nepal }\OtherTok{\textless{}{-}} \FunctionTok{densityVoronoi.ppp}\NormalTok{(nepal, }\AttributeTok{f =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{nrep =} \DecValTok{400}\NormalTok{)}
\NormalTok{d\_vor\_5\_nepal }\OtherTok{\textless{}{-}} \FunctionTok{densityVoronoi.ppp}\NormalTok{(nepal, }\AttributeTok{f =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{nrep =} \DecValTok{400}\NormalTok{)}
\NormalTok{d\_vor\_6\_nepal }\OtherTok{\textless{}{-}} \FunctionTok{densityVoronoi.ppp}\NormalTok{(nepal, }\AttributeTok{f =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{nrep =} \DecValTok{400}\NormalTok{)}
\NormalTok{d\_vor\_7\_nepal }\OtherTok{\textless{}{-}} \FunctionTok{densityVoronoi.ppp}\NormalTok{(nepal, }\AttributeTok{f =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{nrep =} \DecValTok{400}\NormalTok{)}
\NormalTok{d\_vor\_8\_nepal }\OtherTok{\textless{}{-}} \FunctionTok{densityVoronoi.ppp}\NormalTok{(nepal, }\AttributeTok{f =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{nrep =} \DecValTok{400}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The obtained estimates, which similarly to the outcomes of \textsf{density.ppp} are of class \textsf{im}, are then merged into a \textsf{RasterBrick} object and are prepared for graphical representation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sp\_int\_nepal\_v }\OtherTok{\textless{}{-}} \FunctionTok{stack}\NormalTok{(}\FunctionTok{raster}\NormalTok{(d\_vor\_1\_nepal), }\FunctionTok{raster}\NormalTok{(d\_vor\_2\_nepal), }\FunctionTok{raster}\NormalTok{(d\_vor\_3\_nepal), }\FunctionTok{raster}\NormalTok{(d\_vor\_4\_nepal), }\FunctionTok{raster}\NormalTok{(d\_vor\_5\_nepal), }\FunctionTok{raster}\NormalTok{(d\_vor\_6\_nepal), }\FunctionTok{raster}\NormalTok{(d\_vor\_7\_nepal), }\FunctionTok{raster}\NormalTok{(d\_vor\_8\_nepal))}
\FunctionTok{names}\NormalTok{(sp\_int\_nepal\_v) }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\NormalTok{names }\OtherTok{\textless{}{-}} \FunctionTok{as.character}\NormalTok{(}\FunctionTok{sort}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{seq}\NormalTok{(.}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, .}\DecValTok{2}\NormalTok{), }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.5}\NormalTok{), }\AttributeTok{decreasing =} \ConstantTok{TRUE}\NormalTok{))}
\NormalTok{names }\OtherTok{\textless{}{-}} \FunctionTok{paste}\NormalTok{(}\StringTok{"p ="}\NormalTok{, names)}

\NormalTok{sp\_int\_nepal\_v }\OtherTok{\textless{}{-}}\NormalTok{ sp\_int\_nepal\_v }\SpecialCharTok{*} \DecValTok{10}\SpecialCharTok{\^{}}\DecValTok{7}
\NormalTok{at }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DecValTok{30}\NormalTok{)}

\FunctionTok{spplot}\NormalTok{(sp\_int\_nepal\_v, }\AttributeTok{at =}\NormalTok{ at, }\AttributeTok{colorkey =} \FunctionTok{list}\NormalTok{(}\AttributeTok{labels =} \FunctionTok{list}\NormalTok{(}\AttributeTok{cex =} \DecValTok{3}\NormalTok{)), }\AttributeTok{col.regions =} \FunctionTok{topo.colors}\NormalTok{(}\DecValTok{20}\NormalTok{), }\AttributeTok{scales =} \FunctionTok{list}\NormalTok{(}\AttributeTok{draw =} \ConstantTok{FALSE}\NormalTok{), }\AttributeTok{par.strip.text =} \FunctionTok{list}\NormalTok{(}\AttributeTok{cex =} \DecValTok{3}\NormalTok{), }\AttributeTok{names.attr =}\NormalTok{ names)}
\end{Highlighting}
\end{Shaded}

Figure \ref{fig:nepalvor} shows the voronoi-based estimated intensities for the wildfire data in Nepal based on a sequence of retention probabilities. An under-smoothness and high variance for large probabilities is visible. One can also see that retention probabilities less than or equal to 0.2 give rise to smoothed heat-maps that can reflect local variations better than fixed-bandwidth kernel estimators.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{img/nepal2016_V} 

}

\caption{Resample-smoothed Voronoi intensity estimation for the wildfire data in Nepal during 2016 based on different retention probabilities $p$. Intensity values are forest firesd per ten thousand km.}\label{fig:nepalvor}
\end{figure}

\hypertarget{second-order-summary-statistics}{%
\subsection{Second-order summary statistics}\label{second-order-summary-statistics}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d\_nepal }\OtherTok{\textless{}{-}} \FunctionTok{density.ppp}\NormalTok{(nepal, bw.scott, }\AttributeTok{leaveoneout =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{en\_nepal }\OtherTok{\textless{}{-}} \FunctionTok{envelope}\NormalTok{(nepal, }\AttributeTok{fun =}\NormalTok{ Kinhom, }\AttributeTok{correction =} \StringTok{"border"}\NormalTok{, }\AttributeTok{nsim =} \DecValTok{99}\NormalTok{, }\AttributeTok{simulate =} \FunctionTok{expression}\NormalTok{(}\FunctionTok{rpoispp}\NormalTok{(d\_nepal)), }\AttributeTok{sigma =}\NormalTok{ bw.scott, }\AttributeTok{normpower =} \DecValTok{2}\NormalTok{)}

\NormalTok{d\_med }\OtherTok{\textless{}{-}} \FunctionTok{density.ppp}\NormalTok{(medellin, bw.scott, }\AttributeTok{leaveoneout =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{en\_med }\OtherTok{\textless{}{-}} \FunctionTok{envelope}\NormalTok{(medellin, }\AttributeTok{fun =}\NormalTok{ Kinhom, }\AttributeTok{correction =} \StringTok{"border"}\NormalTok{, }\AttributeTok{nsim =} \DecValTok{99}\NormalTok{, }\AttributeTok{simulate =} \FunctionTok{expression}\NormalTok{(}\FunctionTok{rpoispp}\NormalTok{(d\_med)), }\AttributeTok{sigma =}\NormalTok{ bw.scott, }\AttributeTok{normpower =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{en\_nepal}\SpecialCharTok{$}\NormalTok{mmean }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\FunctionTok{plot}\NormalTok{(en\_nepal, }\AttributeTok{main =} \StringTok{""}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{, }\AttributeTok{cex.axis =} \FloatTok{2.5}\NormalTok{, }\AttributeTok{cex.lab =} \FloatTok{2.5}\NormalTok{, }\AttributeTok{legend =} \ConstantTok{FALSE}\NormalTok{)}

\NormalTok{en\_med}\SpecialCharTok{$}\NormalTok{mmean }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\FunctionTok{plot}\NormalTok{(en\_med, }\AttributeTok{main =} \StringTok{""}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{3}\NormalTok{, }\AttributeTok{cex.axis =} \FloatTok{2.5}\NormalTok{, }\AttributeTok{cex.lab =} \FloatTok{2.5}\NormalTok{, }\AttributeTok{legend =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.45\linewidth]{img/nepalk} \includegraphics[width=0.45\linewidth]{img/medk} 

}

\caption{K-functions for Nepal and Medellin}\label{fig:kplots}
\end{figure}

\hypertarget{spatial-point-patterns-on-linear-networks}{%
\section{Spatial point patterns on linear networks}\label{spatial-point-patterns-on-linear-networks}}

Over the last ten years point patterns on linear networks have received a lot of scientific attention. The initial explanation behind the consideration of linear networks, as state spaces of some point processes, might refer to the fact that objects that live on such structures can not use all space, and their movements strongly depend on their freedom over such structures \citep{OS12}. Consequently, among other things, the spatial distribution of points as well as the correlation among points should be studied with respect to the underlying network. However, it has not been that easy to deal with this change of support when aiming to adapt statistical methodologies for the analysis of point patterns on linear networks. The main challenges were not only mathematical/statistical, but also computational \citep{moradi2018spatial,baddeley2021}.

A linear network is a union of line segments \(l_i=[u_i,v_i]=\{tu_i + (1-t)v_i:0\leq t\leq 1\} \subset \mathbb R^2\), and a common choice of metric over such structure has initially been the shortest-path distance \(d_L(u,v)\), although later \cite{rakshit2017second} proposed other types of distances including Euclidean distance. Let \(Y\) be a point process on a linear network \(L\), the Campbell formula \eqref{eq:compR2} is modified as

\begin{equation*}\label{eq:campLin}
\mathbb{E}
\left[
\sum\limits_{y \in Y}
f(y)
\right]
=
\int_{L}
 f(z)
\lambda(z)
\mathrm{d}_1 z,
\end{equation*}
where \(\mathrm{d}_1\) denotes the integration with respect to arc length. Here, \(\lambda(z)\) gives the expected number of points per unit length of \(L\) in the vicinity of \(z \in L\). Several different kernel- and Voronoi-based approaches are developed to estimate the intensity function of point processes on linear networks by considering suitable metrics and handling mathematical challenges. We refer interested readers to \cite{moradi2018spatial} and \cite{baddeley2021} for more details, and briefly go through a fast non-parametric kernel-based intensity estimator using two-dimensional convolution \citep{rakshit2019fast}. Given a realisation \({\mathbf y}= \{ y_1, y_2, \ldots, y_n \}\) from a point process \(Y\) on linear network \(L\), they proposed

\begin{equation}\label{eq:lu}
    \widehat{\lambda}^\text{U}_{\sigma}(z)
    =
    \frac{1}{c_{\sigma,L}(z)}
    \sum_{i=1}^{n}
    \kappa_{\sigma}(z-y_i),
    \qquad
    z \in L,
\end{equation}
with a uniform correction, and

\begin{equation}\label{eq:ljd}
    \widehat{\lambda}^\text{JD}_{\sigma}(z)
    =
    \sum_{i=1}^{n}
    \frac{
    \kappa_{\sigma}(z-y_i)
    }{
    c_{\sigma,L}(y_i)
    },
    \qquad z \in L,
\end{equation}
with Jones-Diggle correction, where \(\kappa_{\sigma}\) is a bivariate kernel function with bandwidth \({\sigma}\), and

\begin{equation*}
    c_{\sigma,L}(z)=\int_L \kappa_{\sigma}(z-v) \mathrm{d}_1 v,
\end{equation*}
is an edge correction. The two estimators above provide similar statistical properties as their counterparts for spatial point patterns in \(\mathbb R^2\) (i.e., estimators \eqref{e:kde.2D.unif} and \eqref{e:kde.2D.JD}), and they can be calculated quickly even on large networks and for large bandwidths. The quick computation is achieved by the fast Fourier transform (FFT) \citep{silverman1982algorithm}. Moreover, \cite{rakshit2019fast} proposed to use the adapted versions of Scott's rule and likelihood cross validation, which are accessible through the functions \textsf{bw.scott.iso} and \textsf{bw.lppl} from \textsf{spatstat.core} and \textsf{spatstat.linnet}, to obtain an optimum bandwidth. We add that the fast calculation of the above estimators further simplifies the calculation of adaptive kernel-based intensity estimators and relative risk over network structures \citep{rakshit2019fast}.

We also recall that \cite{moradi2019resample} proposed their voronoi-based sub-sampling approach for general point processes, and for point patterns on linear networks it can be computed by the function \textsf{densityVoronoi.lpp} from \textsf{spatstat.linnet}.

As a practical example for this section, we study the spatial distribution of street crimes in Valencia, Spain. Valencia is the third-largest city in Spain, being the capital of the autonomous community of Valencia . The urban territory of Valencia encloses an area of 134.65 km\(^2\) with over 800000 inhabitants in the municipality. The dataset consists of the locations of 90247 street crimes such as assault (55610 cases), theft (25342 cases), theft against woman with violence (454 cases), and other types (8841 cases). These crimes are committed between 2010 to 2020. However, in what follows, we only focus on the data corresponding to the year 2020 as a case study which only includes 6868 cases from which 4077 concerns assault, 2060 are theft, and 66 cases relate to crimes against women with violence. This dataset is owned by Generalitat Valenciana (GV) through the emergency phone 112, and was made available to the authors thanks to an agreement between GV and University Jaume I.

Next we turn to estimate the bandwidth using the counterparts of the Scott's rule of thumb whichs gives 584.1m. The function \textsf{densityQuick.lpp} from \textsf{spatstat.linnet} is used to compute either of the estimators \eqref{eq:lu} and \eqref{eq:ljd} in which its default currently computes the uniformly-edge-corrected estimators \eqref{eq:lu}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(valencia)}
\NormalTok{scott\_valencia }\OtherTok{\textless{}{-}} \FunctionTok{bw.scott.iso}\NormalTok{(valencia) }\CommentTok{\# Scott rule}
\NormalTok{d\_scott\_valencia }\OtherTok{\textless{}{-}} \FunctionTok{densityQuick.lpp}\NormalTok{(valencia, }\AttributeTok{sigma =}\NormalTok{ scott\_valencia, }\AttributeTok{leaveoneout =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{positive =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dimyx =} \DecValTok{512}\NormalTok{)}
\NormalTok{d\_scott\_valencia }\OtherTok{\textless{}{-}}\NormalTok{ d\_scott\_valencia }\SpecialCharTok{*} \DecValTok{1000}
\end{Highlighting}
\end{Shaded}

The obtained intensity images in this case are of class \textsf{linim}, which we need to convert them to objects of class \textsf{im} prior to turn them into \textsf{raster} objects.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(w\_vlc}\SpecialCharTok{$}\NormalTok{geometry, }\AttributeTok{lwd =} \DecValTok{4}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(valencia, }\AttributeTok{pch =} \DecValTok{20}\NormalTok{, }\AttributeTok{main =} \StringTok{""}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{4}\NormalTok{, }\AttributeTok{cex =} \DecValTok{1}\NormalTok{, }\AttributeTok{add =}\NormalTok{ T, }\AttributeTok{cols =} \StringTok{"red"}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{raster}\NormalTok{(}\FunctionTok{as.im}\NormalTok{(d\_scott\_valencia)), }\AttributeTok{main =} \StringTok{""}\NormalTok{, }\AttributeTok{axis.args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{cex.axis =} \DecValTok{4}\NormalTok{), }\AttributeTok{legend.width =} \DecValTok{2}\NormalTok{, }\AttributeTok{zlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(w\_vlc}\SpecialCharTok{$}\NormalTok{geometry, }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{4}\NormalTok{)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Below, in Figure \ref{fig:titleV}, we represent the estimated intensity together with the data points. Generally, the estimated intensity indicates the central and north-central parts of Valencia as high-risk areas jointly with some lower-risk hotspots in the east and coast of Valencia.

\begin{figure}

{\centering \includegraphics[width=0.4\linewidth]{img/2020_vlc} \includegraphics[width=0.4\linewidth]{img/2020_vlc_scott} 

}

\caption{Kernel-based intensity estimation (right), using the uniformly-edge-corrected estimators, for the crime data in Valencia, Spain, during 2020 (left). Intensity values are street crimes per km.}\label{fig:titleV}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d\_vlc }\OtherTok{\textless{}{-}} \FunctionTok{densityQuick.lpp}\NormalTok{(valencia, }\AttributeTok{sigma =}\NormalTok{ scott\_valencia, }\AttributeTok{leaveoneout =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{positive =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{at =} \StringTok{"points"}\NormalTok{, }\AttributeTok{dimyx =} \DecValTok{512}\NormalTok{)}
\NormalTok{d\_vlc\_im }\OtherTok{\textless{}{-}} \FunctionTok{densityQuick.lpp}\NormalTok{(valencia, }\AttributeTok{sigma =}\NormalTok{ scott\_valencia, }\AttributeTok{leaveoneout =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{positive =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dimyx =} \DecValTok{512}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_vlc }\OtherTok{\textless{}{-}} \FunctionTok{rpoislpp}\NormalTok{(}\AttributeTok{lambda =}\NormalTok{ d\_vlc\_im, }\AttributeTok{L =}\NormalTok{ net\_vlc, }\AttributeTok{nsim =} \DecValTok{199}\NormalTok{)}
\FunctionTok{library}\NormalTok{(spatstat.Knet)}
\NormalTok{K\_vlc }\OtherTok{\textless{}{-}} \FunctionTok{Knetinhom}\NormalTok{(valencia, }\AttributeTok{lambda =} \FunctionTok{as.numeric}\NormalTok{(d\_vlc))}
\NormalTok{r }\OtherTok{\textless{}{-}}\NormalTok{ K\_vlc}\SpecialCharTok{$}\NormalTok{r}

\NormalTok{K\_sim }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(}\AttributeTok{X =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{199}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(i) \{}
\NormalTok{  sigma }\OtherTok{\textless{}{-}} \FunctionTok{bw.scott.iso}\NormalTok{(sim\_vlc[[i]])}
\NormalTok{  lambda }\OtherTok{\textless{}{-}} \FunctionTok{densityQuick.lpp}\NormalTok{(sim\_vlc[[i]], }\AttributeTok{sigma =}\NormalTok{ sigma, }\AttributeTok{leaveoneout =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{positive =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{at =} \StringTok{"points"}\NormalTok{, }\AttributeTok{dimyx =} \DecValTok{512}\NormalTok{)}
\NormalTok{  Ksim }\OtherTok{\textless{}{-}} \FunctionTok{Knetinhom}\NormalTok{(sim\_vlc[[i]], }\AttributeTok{lambda =} \FunctionTok{as.numeric}\NormalTok{(lambda), }\AttributeTok{r =}\NormalTok{ r)}
  \FunctionTok{return}\NormalTok{(Ksim)}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{K\_nsim\_df }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{do.call}\NormalTok{(cbind, d\_nsim))}
\NormalTok{K\_nsim\_df\_est }\OtherTok{\textless{}{-}}\NormalTok{ K\_nsim\_df[, }\FunctionTok{seq}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{399}\NormalTok{, }\AttributeTok{by =} \DecValTok{2}\NormalTok{)]}

\NormalTok{maxn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(n) }\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{order}\NormalTok{(x, }\AttributeTok{decreasing =} \ConstantTok{TRUE}\NormalTok{)[n]}
\NormalTok{minn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(n) }\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{order}\NormalTok{(x, }\AttributeTok{decreasing =} \ConstantTok{FALSE}\NormalTok{)[n]}

\NormalTok{Kmin }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(K\_nsim\_df\_est, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) x[}\FunctionTok{minn}\NormalTok{(}\DecValTok{5}\NormalTok{)(x)])}
\NormalTok{Kmax }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(K\_nsim\_df\_est, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) x[}\FunctionTok{maxn}\NormalTok{(}\DecValTok{5}\NormalTok{)(x)])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(r, Kmin, }\AttributeTok{type =} \StringTok{"n"}\NormalTok{, }\AttributeTok{col =} \StringTok{"grey"}\NormalTok{, }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{270}\NormalTok{), }\AttributeTok{xlab =} \StringTok{"r"}\NormalTok{, }\AttributeTok{ylab =} \FunctionTok{expression}\NormalTok{(}\FunctionTok{italic}\NormalTok{(K[inhom])))}
\FunctionTok{points}\NormalTok{(r, Kmax, }\AttributeTok{type =} \StringTok{"n"}\NormalTok{, }\AttributeTok{col =} \StringTok{"grey"}\NormalTok{)}
\FunctionTok{polygon}\NormalTok{(}\FunctionTok{c}\NormalTok{(r, }\FunctionTok{rev}\NormalTok{(r)), }\FunctionTok{c}\NormalTok{(Kmax, }\FunctionTok{rev}\NormalTok{(Kmin)), }\AttributeTok{col =} \StringTok{"grey"}\NormalTok{, }\AttributeTok{border =} \StringTok{"grey"}\NormalTok{)}
\FunctionTok{points}\NormalTok{(r, K\_vlc}\SpecialCharTok{$}\NormalTok{est, }\AttributeTok{type =} \StringTok{"l"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.4\linewidth]{img/vlcK} 

}

\caption{K-function fot crime in valencia.}\label{fig:VlcK}
\end{figure}

\hypertarget{part-cuxf3munica-y-colabora}{%
\part{Cómunica y colabora}\label{part-cuxf3munica-y-colabora}}

\hypertarget{120007-informes}{%
\chapter{Informes reproducibles con R Markdown y Quarto}\label{120007-informes}}

\emph{Emilio L. Cano}

\hypertarget{introducciuxf3n-24}{%
\section{Introducción}\label{introducciuxf3n-24}}

\hypertarget{por-quuxe9-informes-reproducibles}{%
\subsection{¿Por qué informes reproducibles?}\label{por-quuxe9-informes-reproducibles}}

El resultado final de un proyecto de análisis de datos terminará
comunicándose a distintos niveles, tanto \emph{aguas arriba} como \emph{aguas
abajo}. Esta comunicación es ``la última milla'' del flujo de análisis que se
esquematizaba en la figura \ref{fig:flujods}. Llamemos genéricamente
``informes'' a cualquiera de estos resultados que se pueden producir en
distintos formatos de destino. Estos informes estarán compuestos de múltiples elementos como
texto, gráficos, resultados numéricos, tablas, etc. Además, es posible que haya
que generarlos en distintos formatos como la web, documentos imprimibles o
presentaciones. Finalmente, con una alta probabilidad varias personas
intervendrán en el proceso, y la \textbf{trazabilidad} (reproducibilidad) del análisis
mejorará el proyecto de forma global.

La forma de abordar el problema es típicamente con un enfoque \emph{corta-pega},
en el que primero se realiza todo el análisis de datos con el software
estadístico y después se utilizan
los resultados del análisis como base de un informe escrito, posiblemente
con algunas iteraciones del proceso si el proyecto tiene cierta envergadura.
El software estadístico comercial suele incluir formas de generar resultados listos
para integrar en un informe, pero siempre bajo este paradigma de incluir el
resultado a posteriori \citep{leisch2002sweave}.

Este enfoque provoca inconsistencias (por ejemplo entre unos grupos y otros,
entre diferentes analistas, etc.), errores, contenidos desactualizados o no
reproducibles, especialmente en la ejecución de software, simulaciones, etc.
Además, cada vez que hay que hacer un cambio, hay que hacerlo en muchos
sitios, con la consiguiente pérdida de tiempo y posibles errores.

El enfoque de la \textbf{investigación reproducible}\footnote{The goal of reproducible research is to tie specific instructions to data
  analysis and experimental data so that reports can be recreated, better
  understood and verified \citep{rrtaskview}.} supera muchos de los obstáculos a la
hora de preparar informes de análisis de datos. El objetivo es
unir instrucciones para análisis de datos con datos experimentales de forma que
los resultados se puedan volver a obtener, entender mejor y verificar.

Un concepto muy relacionado que utilizamos en \textbf{R} es la ``programación
literaria''\footnote{Literate programming is a methodology that combines a programming language with
  a documentation language \citep{knuth1984literate}.}, mediante la cual combinamos un lenguaje de programación como \textbf{R} con documentación de todo tipo (por ejemplo comentarios en el
código).

Con el enfoque de la investigación reproducible, lo que se hace es darle la vuelta
al enfoque \emph{corta-pega}, de forma que se escribe el informe a la vez que se realiza
el análisis, incrustando el código dentro del propio informe. Obviamente es necesario
un sistema que consolide el informe con los resultados del código, y esto es lo
que nos permite R y RStudio mediante archivos R Markdown y su evolución reciente a Quarto.

El flujo de trabajo sería el siguiente: los contenidos se encuentran en ficheros
de texto plano, con código y texto explicativo. Estos ficheros fuente, se compilan y producen los materiales en los
formatos necesarios. Los cambios se hacen una vez, y todos los materiales son
actualizados adecuadamente.

Las ventajas de utilizar un enfoque reproducible se pueden resumir en:

\begin{itemize}
\item
  Si el mismo analista tiene que volver al análisis en el futuro, los
  resultados se pueden volver a obtener de nuevo fácil y comprensiblemente.
\item
  En el caso de que en el proyecto participen más analistas, toda la explicación
  está a mano.
\item
  Cualquier cambio en un punto del análisis (por ejemplo, añadir una
  variable a un modelo) se puede realizar de una sola vez y los cambios
  en los resultados y gráficos se actualizarán automáticamente.
\item
  Los resultados se pueden verificar por terceros en caso necesario. Un
  caso paradigmático fue el escándalo de los ensayos de cáncer en Duke en 2011\footnote{ \url{http://www.nytimes.com/2011/07/08/health/research/08genes.html}}.
  No obstante es un tema que cada vez se demanda más en otros campos
  fuera de la investigación clínica (por ejemplo en publicaciones de
  cualquier tipo).
\end{itemize}

A continuación se abordará el enfoque reproducible. Para el otro enfoque,
simplemente basta copiar los resultados de la consola y los gráficos de la pestaña \emph{Plots}
del panel inferior derecho en cualquier editor de documentos.

\hypertarget{markdown-r-markdown-quarto-y-rstudio}{%
\subsection{Markdown, R Markdown, Quarto y RStudio}\label{markdown-r-markdown-quarto-y-rstudio}}

Markdown es un tipo de ficheros de texto pensado para que se pueda leer bajo
cualquier circunstancia, con una sintaxis muy simple que permite leerlo
directamente por las personas, o ser convertido por un ordenador
en otro formato más elaborado, como por ejemplo HTML
(página web) o Microsoft Word. En RStudio, se pueden crear ficheros \textbf{R Markdown}\footnote{\url{https://rmarkdown.rstudio.com}}
utilizando esta sintaxis para
las explicaciones del análisis, e incluir dentro ``trozos'' (\emph{chunks}) de
código de forma que al generar el informe, el resultado de ese código queda
incluido en el documento de salida. Así, si una vez terminado el informe se ha olvidado,
por ejemplo,
incluir un gráfico, sólo hay que añadir las líneas de código que lo crean y
volver a generar el informe.

R Markdown ha evolucionado a un nuevo formato denominado Quarto\footnote{\url{https://quarto.org}}, que extiende aún más la funcionalidad de Markdown y está pensado para ser usado con otros lenguajes de programación. En esencia, y a los efectos de este capítulo, hay pocas diferencias.

Para poder utilizar las capacidades de R Markdown y Quarto, es necesario tener instalado
el paquete \texttt{knitr} \citep{xie2017dynamic}, que utiliza también otros paquetes como \texttt{rmarkdown}.
Aunque \texttt{knitr} no forma parte del \emph{tidyverse}, sí es un enfoque moderno de R
que vino a hacer más fácil la generación de documentos que se hacía en R base
con la función \texttt{Sweave} \citep{leisch2002sweave}. Para usar Quarto necesitamos también el paquete \texttt{quarto} \citep{R-quarto} y tener instalado el software \textbf{quarto} en el ordenador\footnote{La instalación es trivial para cualquier sistema desde la web de quarto, \url{https://quarto.org}.}.

Para crear un nuevo documento Quarto, se selecciona \emph{Quarto Document\ldots{}} en el
icono de nuevo archivo de la barra de herramientas o en el menú \emph{File}. Entonces
se abre el cuadro de diálogo \emph{New Quarto Document}. Hay varios
tipos de archivos que se pueden crear, que producirán formatos diferentes: \emph{Document} (documento),
\emph{Presentation} (Presentación de diapositivas) e \emph{Interactive} (Aplicación web interactiva con Shiny u Observable JS). De momento se verán los documentos. Podemos crear un archivo Quarto vacío si no queremos que nos cree la estructura. Para que se cree con una estructura mínima,
se necesita un título del documento y un autor, que después se podrán cambiar. También se selecciona un formato de salida por defecto, que puede ser HTML (para ver en el navegador), PDF, o Word. Esto también se podrá cambiar después, por lo que la forma más eficiente de trabajar es empezar con HTML, cuya previsualización es más rápida, y cuando esté el resultado final se genera el archivo en el formato deseado.

Se puede seleccionar la \emph{Engine} entre \emph{knitr} (R) y \emph{Jupyter} (Python), y también elegir si se quiere utilizar el editor visual (por defecto). Con el editor visual se pueden utilizar menús para editar el texto y dar el formato Markdown sin esfuerzo. Al hacer clic en el botón \emph{Create} de la ventana \emph{New Quarto Document}, se abre en
el editor de RStudio un documento quarto (extensión .qmd) con una estructura básica a modo de plantilla. Los elementos principales de un archivo R Markdown aparecen en esta plantilla:

\begin{itemize}
\item
  Encabezado YAML: Constituyen la configuración del documento, y controlan sobre todo las opciones de salida, cómo será el resultado. Este encabezado se encuentra entre dos líneas con tres guiones (\texttt{-\/-\/-}),
  donde se expresan las opciones como \texttt{opcion:\ valor}, y estos valores además se pueden anidar. Dispone de ayuda contextual, de forma que pulsando la combinación de teclas CTRL+ESPACIO aparecen las opciones que se pueden configurar y los posibles valores.
\item
  Texto formateado: Con una sintaxis muy sencilla, se puede dar formato al texto, como negritas, listas, etc. En el editor visual se puede hacer con los menús y botones de la barra de herramientas del editor.
\item
  Fragmentos de código (\emph{chunks}): Al generar el documento, se ejecutará el código dentro de estos
  fragmentos, y en el documento resultante se mostrará el resultado. En cada fragmento de código aparecen dos botones que sirven para ejecutar todos los chunks anteriores y para ejecutar el chunk actual.
\end{itemize}

La barra de herramientas del editor ofrece algunas opciones:

\begin{itemize}
\tightlist
\item
  El botón \emph{Render} ``renderiza'' el documento Quarto produciendo el archivo de salida configurado. Se puede desplegar un menú para cambiar el formato de salida y otras opciones. Al crear el documento solo aparece el formato de salida elegido, pero podemos cambiar el encabezado para poder renderizar el documento a distintos formatos. Por ejemplo si cambiamos el encabezado que se ha creado por defecto por el siguiente, podremos generar el archivo de salida en html o word seleccionando en la lista desplegable junto al botón \emph{Render}:
\end{itemize}

\begin{verbatim}
---
title: "Título del informe"
format: 
  html: default
  docx: default
editor: visual
---
\end{verbatim}

\begin{itemize}
\item
  El botón de opciones permite cambiar la forma en que se mostrarán las salidas al ejecutar el código desde el editor.
\item
  El botón \emph{Insert a new code chunk} nos permite insertar un nuevo fragmento de código.
\item
  Con las flechas de navegación podemos movernos entre los \emph{chunks} del
  documento. También
  podemos usar el selector de esquema en la parte inferior para ir a un fragmento de código o
  apartado concreto del documento.
\item
  Desde el menú \emph{Run} se puede ejecutar el código de los distintos \emph{chunks}.
\item
  El menú \emph{Publish} nos permitiría publicar el documento en algún servicio como \href{https://rpubs.com}{\emph{RPubs}}
\item
  El botón \emph{Outline} muestra un esquema para navegar por el documento, donde aparecerán los encabezados formateados con Markdown.
\item
  Podemos cambiar entre el editor vidual y el del código fuente con los botones \emph{Source} y \emph{Visual} en la parte superior del editor.
\end{itemize}

Para generar el documento, guardamos el archivo en cualquier carpeta de nuestro
proyecto y utilizamos el icono de
``renderizado''. La figura \ref{fig:quarto} muestra en el panel izquierdo el archivo fuente en el editor visual, con alguna opción adicional añadida a la plantilla por defecto, y en el panel derecho el informe renderizado. Si en vez de pulsarlo directamente hacemos clic en el
triángulo de la derecha, podemos seleccionar el formato de salida (html, pdf o Word)
si hemos incluido esos formatos en el encabezado YAML como se ha indicado.
El formato PDF requieren tener instalada una
distribución del sistema de edición libre LaTeX\footnote{Se puede instalar una distribución
  ligera de LaTeX con el paquete \texttt{tinytex}, ejecutando \texttt{tinytex::install\_tinytex()}
  habiendo instalado dicho paquete.}.
El archivo de destino, con extensión .html, .pdf o .docx según el caso,
quedará guardado en la carpeta donde se encuentre el archivo quarto.
Dependiendo de las opciones configuradas, el archivo se abrirá automáticamente
en una ventana nueva de RStudio (por defecto),
o en la pestaña \emph{Viewer} del panel inferior derecho, o en el visor de pdf
integrado en RStudio (pdf). Para poder abrir archivos .docx será necesario tener
instalado Microsoft Word o algún otro programa que pueda abrirlo, como LibreOffice.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\StringTok{"img/cdr\_quarto.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/cdr_quarto} 

}

\caption{Informe Quarto y su renderizado con algunas opciones adicionales}\label{fig:captura-quarto}
\end{figure}

Podemos compilar el informe cuantas veces queramos con el icono \emph{Render}.
Para trabajo en curso, se recomienda ir previsualizando en formato HTML, y una vez
sea definitivo generar el formato de destino final. Para la conversión de
formatos RStudio integra la aplicación
\href{https://pandoc.org}{pandoc}.

Hay una guía rápida de
Markdown (\emph{Markdown Quick Reference}) disponible en el menú de ayuda de RStudio, así como enlaces a dos
\emph{Cheatsheets}: \emph{R Markdown Cheatsheet} y \emph{R Markdown Reference Guide}. Esta última es
la más completa y donde encontraremos todas las opciones disponibles (que sirven
para los documentos Quarto). En los siguientes
apartados repasaremos las opciones más habituales que cubren un amplio
abanico de proyectos.

\hypertarget{documentos-quarto}{%
\section{Documentos Quarto}\label{documentos-quarto}}

\hypertarget{encabezado-yaml-y-configuraciuxf3n}{%
\subsection{Encabezado YAML y configuración}\label{encabezado-yaml-y-configuraciuxf3n}}

Las opciones de configuración del documento se establecen en este encabezamiento.
Al crear el documento con la plantilla, se nos ha creado el siguiente
encabezado:

\begin{verbatim}
---
title: "Título del informe"
format: html
editor: visual
---
\end{verbatim}

Ya hemos visto cómo se pueden añadir más formatos, poniendo uno en cada línea e ``indentando'' con el tabulador las distintas opciones. Cada formato a su vez puede incluir opciones, que de nuevo se indican con nuevas líneas que se ``indentan'' debajo del formato. El siguiente encabezado YAML fijaría el ancho y el alto de las figuras (en pulgadas) para el formato de salida html:

\begin{verbatim}
---
title: "Título del informe"
format: 
  html:
    fig-width: 8
    fig-height: 6
  docx: default
---
\end{verbatim}

Además del título (title), se pueden incluir autor (\texttt{author}) y fecha (\texttt{date}).
Estos elementos son cadenas de texto que aparecerán al principio del documento
de salida. Debemos cuidar que estén entre comillas para evitar
posibles errores. El elemento \texttt{format} indica el formato de salida.

La cantidad de opciones que se pueden incluir en el encabezado YAML es enorme y no tiene cabida en este capítulo. Algunas de las más utilizadas son \texttt{lang} para indicar el idioma del documento (ES para español),
\texttt{bibliography} para indicar un fichero bibtex de bibliografía, o \texttt{toc} para incluir una tabla de contenidos. Algunas son específicas del formato. Por ejemplo, una muy útil es \texttt{reference-doc} para documentos de Word, con la que podemos indicar una plantilla personalizada para usar colores corporativos u otras opciones de diseño del informe. Para documentos html podemos incluir una hoja de estilos con la opción \texttt{css}. La lista completa para cada uno de los formatos soportados por Quarto se puede consultar en la guía de referencia en \url{https://quarto.org/docs/reference/}.

\hypertarget{formateado-de-texto}{%
\subsection{Formateado de texto}\label{formateado-de-texto}}

Incluir títulos, énfasis en el texto y listas
es muy sencillo, y a menudo no se necesita nada más para realizar un informe.
En el sencillo informe que se crea con la plantilla ya vemos algunas opciones:

\begin{itemize}
\item
  Los encabezados se crean poniendo al principio de la línea tantos símbolos
  almohadilla (\texttt{\#\#}) como nivel de título queramos (dos almohadilla, apartado, tres
  almohadillas, subapartado, etc.) Una almohadilla sería para el título del
  informe, si no se especificara en el encabezado.
\item
  Para poner texto en \textbf{negrita}, se rodea con dos asteriscos a cada lado: \texttt{**negrita**}.
\item
  Para poner texto en formato monoespaciado, tipo código, se rodea de acentos
  graves (\emph{backticks}).
\item
  Los enlaces se crean automáticamente si hay una url completa.
\end{itemize}

Existen otras opciones sencillas, que se pueden ver en la \emph{Markdown Quick Reference}
del menú \emph{Help}: Cursiva rodeando el texto con un solo asterisco a cada lado (o guión bajo); listas poniendo al principio de la línea un asterisco, guión o signo más; listas ordenadas poniendo un número y un punto al principio de la línea; enlaces con texto + link como: \texttt{{[}texto{]}(url)}; imágenes de cualquier tipo como: \texttt{!{[}{]}(ruta\_a\_la\_imagen)}; superíndices y subíndices; ecuaciones en formato \texttt{LaTeX}; tablas; saltos de línea; saltos de página.
Con estas opciones se cubren las necesidades de la práctica totalidad de
proyectos de análisis. No obstante, dependiendo del formato de salida se
pueden añadir otras opciones de formato.

\hypertarget{inclusiuxf3n-de-cuxf3digo-en-el-documento}{%
\subsection{Inclusión de código en el documento}\label{inclusiuxf3n-de-cuxf3digo-en-el-documento}}

Se pueden crear archivos Quarto sin incluir nada de código, simplemente
para crear documentos editables fácilmente. Sin embargo, la verdadera potencia
de Quarto es la posibilidad de incluir código de R (y también de otros
lenguajes) en los documentos. Como ya se avazó, el código se incluye, principalmente,
en forma de \emph{chunks} o bloques de código.

Un \emph{chunk} consta de unos marcadores
de inicio y final del \emph{chunk}, entre los cuales se insertan expresiones de R que se
ejecutarán al generar el documento de salida. El marcador de inicio son tres
símbolos de acento grave (\emph{backticks}, \texttt{\textasciigrave{}} ) seguidos de unas llaves con la letra r
dentro. El marcador de cierre del \emph{chunk} son de nuevo
tres acentos graves, sin más. Y dentro del chunk podemos poner expresiones de R como si estuviéramos en un script. Al renderizar el documento, el código se ejecutará con las opciones que se indiquen como se explica más adelante, y el archivo de salida incluirá el resultado de la ejecución del chunk. El siguiente sería un ejemplo de código utilizado en otro capítulo del libro para incluir gráficos en el informe.

\begin{verbatim}
```
library(CDR)
library(corrplot)
mcor_tic <- cor(TIC2021)  
corrplot.mixed(mcor_tic, order = 'AOE')
```
\end{verbatim}

A veces es necesario incluir algún resultado de R en medio del texto
y no como un bloque. En esos casos se puede insertar un bloque en línea
poniendo, dentro de dos acentos graves, la letra \texttt{r} como primer carácter,
y después una expresión de R que se pueda ``imprimir'' como cadena de texto:

\begin{Shaded}
\begin{Highlighting}[]
\InformationTok{\textasciigrave{}r expresion\_de\_R \textasciigrave{}}    
\end{Highlighting}
\end{Shaded}

En todo caso, no hay que escribir los marcadores de inicio y final, ya que
se dispone del atajo de teclado \texttt{CTRL+ALT+I} que lo hace automáticamente, o,
alternativamente, el icono \emph{Insert a new code chunk} de la barra de herramientas del
editor.
Una vez se tiene el cursor dentro de un chunk, podemos ejecutar una expresión
como si estuviéramos en un script (\texttt{CTRL+ENTER}), o el chunk completo
(\texttt{MAYUS+CTRL+ENTER}).

Una opción muy interesante de los informes de Quarto es la parametrización.
Esta opción es muy útil para informes automatizados que pueden
cambiar dependiendo de algún valor, por ejemplo del fichero de datos, la fecha,
o cualquier otro valor. Estos parámetros se crean como elementos del encabezado
YAML de la forma:

\begin{verbatim}
params:
  parametro: valor
\end{verbatim}

que después se pueden usar en los chunks de código como \texttt{params\$parametro}.
La verdadera potencia de esta característica es cuando se renderiza el documento
desde un script en el que los parámetros son resultados de algún tipo de
operación en los datos (por ejemplo, un informe de análisis de inventario solo de una
tienda donde se han producido roturas de stock el día x).
En vez de utilizar el botóh \emph{Render}, en estos casos se usa la función \texttt{quarto\_render()},
una de cuyas opciones es \texttt{execute\_params}, donde se pasarían los valores de los parámetros en forma
de lista cuyos elementos tienen el nombre de los parámetros.

\hypertarget{opciones-de-los-bloques-de-cuxf3digo-chunks}{%
\subsection{\texorpdfstring{Opciones de los bloques de código (\emph{chunks})}{Opciones de los bloques de código (chunks)}}\label{opciones-de-los-bloques-de-cuxf3digo-chunks}}

Al renderizar un informe que contiene chunks sin configurar ninguna opción,
por defecto mostrará en el informe el código de entrada y las salidas
(textos y gráficos) así como todos los mensajes que se pueden producir.

Opcionalmente, justo después del marcador de inicio del chunk se pueden añadir opciones del chunk mediante líneas que comienzan por el llamadao \emph{hashpipe}, que es una almohadilla seguida de la barra vertical, \texttt{\#\textbar{}} y a continuación la opción y su valor de la misma forma que se hacía en el encabezado YAML para las opciones del documento, es decir, \texttt{opcion:\ valor}.

El \emph{chunk} que se muestra a continuación tiene como identificador ``ejemplo'',
y como opciones ``echo=FALSE, fig.align=`center'\,'', lo que indica que el código
no se mostrará en el informe final y que el gráfico producido se alineará en el centro
del texto.

\begin{verbatim}
```
#| label: "Ejemplo"
#| echo: false
#| fig-align: 'center'
plot(cars)
```
\end{verbatim}

Las opciones de chunk se pueden incluir de forma global en el documento estableciéndolas en el encabezado YAML del documento, por ejemplo para mantener las opciones anteriores en todos los chunks por defecto:

\begin{verbatim}
---
title: "Mi documento"
format: html
knitr:
  opts_chunk: 
    echo: false
    fig-align: 'center'
---
\end{verbatim}

Hay varias opciones de chunk que tienen que ver con la presentación en la salida. Por defecto, si se produce un error el proceso se detiene y no se genera
el archivo de destino. Este comportamiento, y otras muchas opciones,
se pueden configurar como opciones del chunk. Las más habituales son: \texttt{error:\ true} para mostrar los errores y no detener la renderización; \texttt{warning:\ false} y \texttt{message:\ false} para no mostrar \emph{warnings} ni mensajes respectivamente; \texttt{include:\ false} para ejecutar el código pero no mostrar ningún tipo de salida; \texttt{eval:\ false} para no ejecutar el codigo; \texttt{results:\ \textquotesingle{}hide\textquotesingle{}} para indicar que no se muestren los resultados (otras opciones son \texttt{asis}, \texttt{hold} o \texttt{markup}); \texttt{comment:\ simbolo} para cambiar el símbolo que se usará como comentario del output (a veces es conveniente simplemente no poner comentario, es decir, \texttt{""}). Estas opciones del chunk se pueden incluir a nivel global en el encabezado YAML como se ha indicado anteriormente. Para ver la lista completa de opciones, accede a la \emph{R Markdown reference guide}
que está disponible en el menú \emph{Help/Cheatsheets}.

Una característica muy cómoda es usar la ayuda contextual al escribir las opciones del chunk. Si empezamos a escribir, o pulsamos \texttt{CTRL+ESPACIO}, se
muestran las opciones disponibles, y al seleccionar una opción, si tiene
varios posibles valores aparecen también para seleccionar.

\hypertarget{referencias-cruzadas-y-formateo-de-tablas}{%
\subsection{Referencias cruzadas y formateo de tablas}\label{referencias-cruzadas-y-formateo-de-tablas}}

La salida tabular por defecto de la consola normalmente no es adecuada para un informe. En su lugar, lo que se desea es tener una tabla formateada adecuadamente para el formato de salida. Se pueden incluir en los informes de Quarto tablas formateadas de calidad. Para ello, se debe utilizar alguna función que formatee la tabla de acuerdo al formato de salida (HTML, PDF, Word), y a veces configurar la opción \texttt{results} del \emph{chunk} como \texttt{\textquotesingle{}asis\textquotesingle{}}. Muchas de estas funciones preparan automáticamente el formato según el fichero de salida que se está generando.
Por ejemplo, el siguiente chunk generaría una tabla en cualquiera de los formatos de salida usando la función \texttt{kable} del paquete \texttt{knitr}:

\begin{verbatim}
 ```{r}
 knitr::kable(TIC2021)
 ```
 
\end{verbatim}

Hay otros paquetes con multitud de opciones de formato y presentación para las tablas como \texttt{xtable}, \texttt{flextable}, \texttt{kableExtra}, o \texttt{gt}. Se anima al lector a consultar la documentación de estos paquetes para aprender a crear tablas de calidad que comuniquen adecuadamente los resultados de los análisis.

Tanto las tablas como las figuras se deben referenciar adecuadamente en los informes. Para ello utilizamos las \textbf{referencias cruzadas} de los informes Quarto. Para poder referenciar un gráfico creado en un chunk, necesitamos: i) que el chunk tenga una etiqueta (\texttt{label:\ \textquotesingle{}etiqueta\textquotesingle{}}); ii) que el chunk tenga una opción \texttt{fig-cap} para el título de la figura. Entonces el gráfico se puede referenciar en cualquier lugar del documento Quarto simplemente excribiendo \texttt{@etiqueta}. Por ejemplo, el siguiente chunk crearía el gráfico de la figura \ref{fig:ref-fig}, y en el texto lo referenciaríamos como ``\texttt{Figura\ @fig-tic}''.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/ref-fig-1} 

}

\caption{Ventas vs. \% Empresas con banda ancha}\label{fig:ref-fig}
\end{figure}

\begin{verbatim}
```{r}
#| label: "fig-tic"
#| fig-cap: "Ventas vs. % Empresas con banda ancha"
TIC2021 |> 
  ggplot(aes(ebroad, esales)) +
  geom_point() +
  geom_smooth(method = "lm")
```
\end{verbatim}

En cuanto a las tablas, igualmente el chunk que las crea debe tener una etiqueta. El título de la tabla en este caso lo proporcionará la propia función que crea la tabla. A modo de ejemplo, el siguiente chunk crearía la tabla \ref{fig:ref-fig} ya formateada con la función \texttt{flextable()} del paquete homónimo \citep{R-flextable}, y en el texto la referenciaríamos como ``\texttt{Tabla\ @tab-tic}''.

\textbf{GEMA DICE: ERROR EN LA TABLA POR LAS FUNCIONES, VER CON LA ACTUALIZACIÓN CDR}

\begin{verbatim}
```{r}
#| label: "tab-tic"
#| fig-cap: "Contaminación media NOx según tipo de estación"
library(dplyr)
library(flextable)
contam_mad |>
  filter(nom_abv == "NOx") |>
  group_by(tipo) |> 
  summarise(Media = mean(daily_mean, na.rm = TRUE), n = n(), 
            Desv.Tip = sd(daily_mean, na.rm = TRUE),
            Perdidos = sum(is.na(daily_mean))) |> 
  flextable() |> 
  set_caption("Contaminación media NOx según tipo de estación.") |> 
  autofit()
```
\end{verbatim}

\hypertarget{otros-formatos}{%
\section{Otros formatos}\label{otros-formatos}}

El formato Quarto es solo uno de los que se pueden utilizar
para aplicar la reproducibilidad que motiva este capítulo. Como se ha comentado, es la nueva generación de R Markdown, archivos que también podemos seguir creando con RStudio. En R base
se pueden crear archivos \texttt{Sweave}, con sintaxis \LaTeX para la
narrativa y bloques de código R. También es posible crear archivos R HTML,
con la narrativa en HTML. Los identificadores de los bloques son ligeramente
distintos, así como las opciones disponibles, aunque la filosofía es la misma.
R Markdown y ahora Quarto han ido desplazando estos otros formatos al ser más versátil (puede
generar cualquiera de los otros formatos, y además otros como Word).

En cuanto a los formatos de salida, hay una cantidad de opciones muy interesante
que queda fuera de los objetivos de este libro. Se relacionan a continuación
algunos de ellos, se puede consultar el libro de \citet{xie2019r} y en la documentación de Quarto para
ver detalles.

\begin{itemize}
\item
  \textbf{Notebooks}: Es un tipo especial de salida HTML, más indicado cuando
  se quieren ir probando cosas guardando el resultado parcial de lo ejecutado en el html.
\item
  \textbf{Presentaciones}: Es posible crear presentaciones PowerPoint y usar una plantilla,
  como se vio con los documentos de Word. Se utiliza una sintaxis muy sencilla
  y se puede incluir código y resultados igual que en un informe. Otros formatos de
  presentaciones son Reveal JS y beamer (LaTeX), y con R Markdown además ioslides, slidify o xaringan (HTML).
\item
  \textbf{Tableros} (\emph{dashboards}): Pueden ser estáticos, usando el paquete \texttt{flexdashboard},
  útiles para comunicar resultados en un par de pantallazos.
\item
  \textbf{Shiny}: Aplicaciones web interactivas que responden a inputs del usuario (\emph{reactive})
\item
  \textbf{Websites}: Websites sencillos con páginas enlazadas en el mismo directorio.
\item
  \textbf{Blog}: Directamente como proyecto Quarto, o con el paquete \texttt{blogdown} se pueden generar sitios web completos al estilo de un blog con páginas.
\item
  \textbf{Libros}: Directamente como proyecto Quarto, o con el paquete \texttt{bookdown} se pueden crear libros en varios formatos. El material de este libro está creado con \texttt{bookdown}.
\item
  \textbf{Tutoriales}: Aparecieron en RStudio 1.3, se pueden crear documentos interactivos con preguntas y navegación usando sintaxis R Markdown.
\item
  \textbf{Tufte Handouts}: Un tipo especial de documento con anotaciones al margen
  que comunica muy bien.
\end{itemize}

\hfill\break

\begin{infobox_resume}

\textbf{RESUMEN}

\begin{itemize}
\tightlist
\item
  En la comunicación de resultados, es esencial que sigamos un enfoque reproducible.
\item
  R Markdown y su evolución Quarto es el formato más versátil para crear informes reproducibles que
  permitan una trazabilidad de los análisis.
\item
  RStudio permite trabajar eficientemente con R Markdown y Quarto a través de ayuda y opciones.
\item
  El encabezado YAML del informe contiene la configuración globa, que puede incluir parámetros para automatización.
\item
  La narrativa del informe se escribe en Markdown, con una sintaxis extremadamente sencilla.
\item
  El código se puede incluir en forma de bloques (\emph{chunks}) o en línea.
\item
  En las opciones del \emph{chunk} se puede personalizar
  la forma en que se ejecutará y mostrará en el informe.
\item
  Los informes R Markdown y Quarto se pueden generar en formatos HTML, PDF y Word.
\item
  En los informes Quarto se pueden hacer referencias cruzadas a tablas, figuras y otros elementos del documento.
\item
  Hay otros formatos más elaborados que merece la pena explorar.
\end{itemize}

\end{infobox_resume}

\hypertarget{shiny}{%
\chapter{Aplicaciones webs interactivas con Shiny}\label{shiny}}

\emph{Aurora González Vidal}

\hypertarget{introducciuxf3n-25}{%
\section{Introducción}\label{introducciuxf3n-25}}

\texttt{Shiny} es un paquete de \textbf{R} que permite crear aplicaciones web interactivas \index{aplicaciones web interactivas} que cuentan con todos los elementos de R. \texttt{Shiny} se ha convertido en un referente ya que, para aquellos que tienen conocimiento de R, es muy sencillo crear una aplicación en cuestión de horas \citep{changshiny}. Para crear una aplicación mínima, no se necesitan conocimientos de HTML (\emph{HyperText Markup Language}), CSS (\emph{Cascading Style Sheets}) o \emph{JavaScript} y sus dependencias. Además, no es necesario pensar en elementos técnicos para hacerla accesible en la web como, por ejemplo, el puerto, ya que \texttt{Shiny} se encarga de esos detalles si no se cambian las opciones por defecto. Éstas son algunas de las razones principales por las cuales \texttt{Shiny} se ha vuelto tan popular a lo largo de los años, ya que con poco esfuerzo se pueden crear pruebas de concepto de un producto, mostrar algoritmos o presentar resultados de investigación de forma elegante a través de interfaces de usuario accesibles, reproducibles y amigables \citep{fay2021engineering}.

El primer paso para disfrutar de \texttt{Shiny} consiste en instalar el paquete que está disponible en CRAN:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"shiny"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Para asegurarse de que la versión instalada es igual o superior a la 1.5.0. use \texttt{packageVersion("shiny")}.
A continuación se puede cargar el paquete y ver algunos ejemplos que se incluyen directamente en el mismo utilizando distintas opciones para el argumento \texttt{example}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"shiny"}\NormalTok{)}
\FunctionTok{runExample}\NormalTok{(}\AttributeTok{example =} \StringTok{"01\_hello"}\NormalTok{)}
\CommentTok{\# otras: 02\_text, 03\_reactivity, 04\_mpg, 05\_sliders, 06\_tabsets, 07\_widgets, 08\_html, 09\_upload, 10\_download,  11\_timer.}
\end{Highlighting}
\end{Shaded}

\hypertarget{partes-muxednimas-de-una-aplicaciuxf3n-shiny-y-disposiciuxf3n-buxe1sica}{%
\section{\texorpdfstring{Partes mínimas de una aplicación \texttt{Shiny} y disposición básica}{Partes mínimas de una aplicación Shiny y disposición básica}}\label{partes-muxednimas-de-una-aplicaciuxf3n-shiny-y-disposiciuxf3n-buxe1sica}}

Las aplicaciones \texttt{Shiny} tienen dos componentes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Una \index{interfaz de usuario} interfaz de usuario \texttt{ui}, que es un script y
\item
  Un \texttt{server} que es un script de \index{servidor} servidor o secuencia de comandos de servidor.
\end{enumerate}

Estas partes pueden encontrarse en el mismo script o estar separadas en dos scripts con nombres fijos: \texttt{ui.R} y \texttt{server.R}. En este caso se ha elegeido la segunda opción para ilustrar los ejemplos con mayor claridad.
Una aplicacion \texttt{Shiny} es un directorio que contiene estos scripts y otros ficheros adicionales (conjuntos de datos, fichero donde se definen funciones no dinámicas, etc)

El código mínimo para crear una aplicación con un título, panel lateral y panel principal es el que sigue:

\begin{itemize}
\tightlist
\item
  \texttt{ui.R}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shinyUI}\NormalTok{(}\FunctionTok{fluidPage}\NormalTok{(}
  \FunctionTok{headerPanel}\NormalTok{(}\StringTok{"TITULO"}\NormalTok{),}
  \FunctionTok{sidebarPanel}\NormalTok{(),}
  \FunctionTok{mainPanel}\NormalTok{()}
\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{server.R}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shinyServer}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(input, output) \{\})}
\end{Highlighting}
\end{Shaded}

Para lanzar la aplicación existen dos maneras, mediante el comando \texttt{runApp} que tiene como argumento la ruta del directorio que almacena los ficheros que componen la aplicación.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(shiny)}
\FunctionTok{runApp}\NormalTok{(}\StringTok{"ruta al directorio"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

O directamente desde Rstudio mediante el botón RunApp que aparece en cualquiera de los dos scripts \texttt{ui.R}, \texttt{server.R} reemplazando al Run habitual.

En este capítulo, además de ver los distintos componentes de \texttt{Shiny}, construiremos una aplicación para la visualización de algunos gráficos presentados en el capítulo ``Perfilado estadístico de parados'' \{\#paro-clm\}.

Además del \texttt{ui.R} y el \texttt{sever.R}, puede ser muy útil tener un fichero \texttt{source.R} en la misma carpeta para cargar los paquetes y datos estáticos necesarios para el funcionamiento de la aplicación.

\hypertarget{diseuxf1o-de-una-aplicaciuxf3n-shiny}{%
\section{\texorpdfstring{Diseño de una aplicación \texttt{Shiny}}{Diseño de una aplicación Shiny}}\label{diseuxf1o-de-una-aplicaciuxf3n-shiny}}

\texttt{Shiny} incluye una serie de opciones para el diseño o la disposición de los distintos componentes de una aplicación. En este apartado veremos dos muy sencillos:

\begin{itemize}
\tightlist
\item
  sidebarLayout(): para colocar un sidebarPanel() de entradas junto a un mainPanel() de contenido de salida.
\item
  tabsetPanel() y navlistPanel() para la segmentación de diseños
\end{itemize}

Hasta ahora se ha utilizado el primero sin introducirlo específicamente para mostrar distintos ejemplos por ser el más sencillo.

\hypertarget{diseuxf1o-de-las-puxe1ginas-fluidpage}{%
\subsection{Diseño de las páginas: fluidPage()}\label{diseuxf1o-de-las-puxe1ginas-fluidpage}}

Un diseño de página fluido \texttt{fluidPage()} \index{`fluidPage()`} consiste en filas que a su vez incluyen columnas. Las filas existen con el propósito de asegurar que sus elementos aparezcan en la misma línea (si el navegador tiene el ancho adecuado). Las columnas existen con el propósito de definir cuánto espacio horizontal dentro de una cuadrícula de 12 unidades de ancho deben ocupar sus elementos. Las páginas fluidas escalan sus componentes en tiempo real para llenar todo el ancho disponible del navegador.

Una \texttt{fluidPage()} presenta 2 argumentos: \texttt{headerPanel()} con el título de la aplicación,
y \texttt{sidebarLayout()}.\texttt{sidebarLayout()} es un punto de partida útil para la mayoría de las aplicaciones. Este a su vez tiene 2 argumentos más: \texttt{sidebarPanel()}, que es una barra lateral para las entradas y \texttt{mainPanel()}, una gran área principal para la salida.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shinyUI}\NormalTok{(}\FunctionTok{fluidPage}\NormalTok{(}
  \FunctionTok{headerPanel}\NormalTok{(}\StringTok{"Evolución del paro"}\NormalTok{),}
  \FunctionTok{sidebarLayout}\NormalTok{(}
    \FunctionTok{sidebarPanel}\NormalTok{(}
      \FunctionTok{radioButtons}\NormalTok{(}
        \StringTok{"vble"}\NormalTok{, }\StringTok{"Variable"}\NormalTok{,}
        \FunctionTok{c}\NormalTok{(}
          \StringTok{"sexo"} \OtherTok{=} \StringTok{"sexo"}\NormalTok{,}
          \StringTok{"tramo\_edad"} \OtherTok{=} \StringTok{"tramo\_edad"}\NormalTok{,}
          \StringTok{"tiempo\_busqueda\_empleo\_agregado"} \OtherTok{=} \StringTok{"tiempo\_busqueda\_empleo\_agregado"}\NormalTok{,}
          \StringTok{"sector"} \OtherTok{=} \StringTok{"sector"}\NormalTok{,}
          \StringTok{"tiempo\_busqueda\_empleo"} \OtherTok{=} \StringTok{"tiempo\_busqueda\_empleo"}
\NormalTok{        ), }\StringTok{"sexo"}
\NormalTok{      )}
\NormalTok{    ),}
    \FunctionTok{mainPanel}\NormalTok{(}
      \FunctionTok{plotOutput}\NormalTok{(}\StringTok{"gra1"}\NormalTok{)}
\NormalTok{    )}
\NormalTok{  )}
\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/evparo1} 

}

\caption{fluidPage, sidebarLayout, sidebarPanel por defecto}\label{fig:foo1}
\end{figure}

La barra lateral puede posicionarse a la izquierda (por defecto) o a la derecha del área principal. Por ejemplo, para posicionar la barra lateral a la derecha se debe utilizar \texttt{position\ =\ \textquotesingle{}right\textquotesingle{}} y éste es el resultado.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shinyUI}\NormalTok{(}\FunctionTok{fluidPage}\NormalTok{(}
  \FunctionTok{headerPanel}\NormalTok{(}\StringTok{"Evolución del paro"}\NormalTok{),}
  \FunctionTok{sidebarLayout}\NormalTok{(}\AttributeTok{position =} \StringTok{"right"}\NormalTok{, ...)}
\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/evparo2} 

}

\caption{fluidPage, sidebarLayout a la derecha}\label{fig:unnamed-chunk-251}
\end{figure}

Las funciones \texttt{radioButtons} y \texttt{plotOutput} se introducirán en detalle en las respectivas secciones de este capítulo.

\hypertarget{segmentaciuxf3n-de-diseuxf1os-tabsetpanel-y-navlistpanel}{%
\subsection{Segmentación de diseños: tabsetPanel() y navlistPanel()}\label{segmentaciuxf3n-de-diseuxf1os-tabsetpanel-y-navlistpanel}}

Para subdividir el panel principal en varias secciones discretas, se puede usar \texttt{tabsetPanel()} \index{tabsetPanel()} y `tabPanel()`` como sigue:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mainPanel}\NormalTok{(}
  \FunctionTok{tabsetPanel}\NormalTok{(}
    \FunctionTok{tabPanel}\NormalTok{(}\StringTok{"Plot1"}\NormalTok{, }\FunctionTok{plotOutput}\NormalTok{(}\StringTok{"gra1"}\NormalTok{)),}
    \FunctionTok{tabPanel}\NormalTok{(}\StringTok{"Plot2"}\NormalTok{, }\FunctionTok{plotOutput}\NormalTok{(}\StringTok{"gra2"}\NormalTok{)),}
    \FunctionTok{tabPanel}\NormalTok{(}\StringTok{"Plot3"}\NormalTok{, }\FunctionTok{plotOutput}\NormalTok{(}\StringTok{"gra3"}\NormalTok{))}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\texttt{navlistPanel()} es una alternativa a \texttt{tabsetPanel()} cuando existan muchas separaciones. Un \texttt{navlist} presenta los distintos componentes como una lista de la barra lateral en lugar de utilizar pestañas y no se hace en el mainPanel.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ui }\OtherTok{\textless{}{-}} \FunctionTok{fluidPage}\NormalTok{(}
  \FunctionTok{titlePanel}\NormalTok{(}\StringTok{"Application Title"}\NormalTok{),}
  \FunctionTok{navlistPanel}\NormalTok{(}
    \StringTok{"Header A"}\NormalTok{,}
    \FunctionTok{tabPanel}\NormalTok{(}\StringTok{"Component 1"}\NormalTok{),}
    \FunctionTok{tabPanel}\NormalTok{(}\StringTok{"Component 2"}\NormalTok{),}
    \StringTok{"Header B"}\NormalTok{,}
    \FunctionTok{tabPanel}\NormalTok{(}\StringTok{"Component 3"}\NormalTok{)}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{elementos-para-entrada-de-datos}{%
\section{Elementos para entrada de datos}\label{elementos-para-entrada-de-datos}}

Para que el usuario de la aplicación \texttt{Shiny} introduzca datos manualmente, hay diversos elementos que se enumeran a continuación:

\begin{itemize}
\tightlist
\item
  \textbf{Control deslizante}: Un \index{control deslizante} control deslizante permite que el usuario seleccione entre un intervalo de valores moviendo un control de posición por una pista. En \texttt{Shiny} se crean con la función \texttt{sliderInput} que tiene, entre otros, los siguientes argumentos autoexplicativos:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sliderInput}\NormalTok{(inputId, label, min, max, value, }\AttributeTok{step =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{animate =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Sus características incluyen:

\begin{verbatim}
- La posibilidad de introducir un único valor y rangos

- Formatos customizaods (por ejemplo para entradas relativas al dinero)

- Pueden ser animados y recorrer los valores de forma automática (argumento `animate`)
\end{verbatim}

Algunos ejemplos son:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sliderInput}\NormalTok{(}\StringTok{"enteros"}\NormalTok{, }\StringTok{"Enteros:"}\NormalTok{, }\AttributeTok{min =} \DecValTok{0}\NormalTok{, }\AttributeTok{max =} \DecValTok{1000}\NormalTok{, }\AttributeTok{value =} \DecValTok{500}\NormalTok{)}
\FunctionTok{sliderInput}\NormalTok{(}\StringTok{"decimales"}\NormalTok{, }\StringTok{"Decimales:"}\NormalTok{, }\AttributeTok{min =} \DecValTok{0}\NormalTok{, }\AttributeTok{max =} \DecValTok{1}\NormalTok{, }\AttributeTok{value =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{step =} \FloatTok{0.1}\NormalTok{)}
\FunctionTok{sliderInput}\NormalTok{(}\StringTok{"rango"}\NormalTok{, }\StringTok{"Rango:"}\NormalTok{, }\AttributeTok{min =} \DecValTok{1}\NormalTok{, }\AttributeTok{max =} \DecValTok{1000}\NormalTok{, }\AttributeTok{value =} \FunctionTok{c}\NormalTok{(}\DecValTok{200}\NormalTok{, }\DecValTok{500}\NormalTok{))}
\FunctionTok{sliderInput}\NormalTok{(}\StringTok{"animacion"}\NormalTok{, }\StringTok{"Animacion:"}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{10}\NormalTok{, }\AttributeTok{step =} \DecValTok{10}\NormalTok{, }\AttributeTok{animate =} \FunctionTok{animationOptions}\NormalTok{(}\AttributeTok{loop =}\NormalTok{ T))}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Botón circular}: Un \index{botón circular} botón circular es un tipo de selector que da una lista de opciones entre las cuales solo se puede seleccionar una. En \texttt{Shiny} se crean con la función \texttt{radioButtons} que tiene, entre otros, los siguientes argumentos autoexplicativos:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{radioButtons}\NormalTok{(inputId, label, choices, }\AttributeTok{selected =} \ConstantTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Un ejemplo donde la variable ``sexo'' está elegida por defecto se puede ver en el primer trozo de código de la subsección 2.1.1.1 sidebarLayout().

\begin{itemize}
\tightlist
\item
  \textbf{Selección múltiple}: Un cuadro de selección múltiple es un tipo de selector que da una lista de opciones entre las cuales se pueden seleccionar varias. En \texttt{Shiny} se crean con la función \texttt{selectInput} que tiene, entre otros, los siguientes argumentos autoexplicativos:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{selectInput}\NormalTok{(inputId, label, choices, }\AttributeTok{multiple =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{selectInput}\NormalTok{(}\StringTok{"año"}\NormalTok{, }\StringTok{"Año:"}\NormalTok{,}
  \FunctionTok{c}\NormalTok{(}
    \StringTok{"año1"} \OtherTok{=} \StringTok{"2007"}\NormalTok{,}
    \StringTok{"año2"} \OtherTok{=} \StringTok{"2013"}\NormalTok{,}
    \StringTok{"año3"} \OtherTok{=} \StringTok{"2019"}\NormalTok{,}
    \StringTok{"año4"} \OtherTok{=} \StringTok{"2022"}
\NormalTok{  ),}
  \AttributeTok{multiple =} \ConstantTok{TRUE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{checkboxGroupInput}
\end{itemize}

Muy similar al anterior, este componente crea un grupo de casillas que se pueden utilizars para alternar varias opciones de forma independiente.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{checkboxGroupInput}\NormalTok{(inputId, label, choices, }\AttributeTok{multiple =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{checkboxGroupInput}\NormalTok{(}
  \StringTok{"variable"}\NormalTok{, }\StringTok{"Variables to show:"}\NormalTok{,}
  \FunctionTok{c}\NormalTok{(}
    \StringTok{"año1"} \OtherTok{=} \StringTok{"2007"}\NormalTok{,}
    \StringTok{"año2"} \OtherTok{=} \StringTok{"2013"}\NormalTok{,}
    \StringTok{"año3"} \OtherTok{=} \StringTok{"2019"}\NormalTok{,}
    \StringTok{"año4"} \OtherTok{=} \StringTok{"2022"}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Entrada numérica}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{numericInput}\NormalTok{(}\StringTok{"obs"}\NormalTok{, }\StringTok{"Numero de observaciones:"}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Entrada de texto}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{helpText}\NormalTok{(}\StringTok{"aclaraciones"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Otras opciones de entrada que se invita al lector a analizar se relacionan con las fechas: \texttt{dateInput()}, \texttt{dateRangeInput()} y con un áreas de texto: \texttt{textAreaInput()}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/entradas2} 

}

\caption{Distintos elementos para la entrada de datos}\label{fig:unnamed-chunk-263}
\end{figure}

\hypertarget{lectura-de-ficheros-de-datos}{%
\subsection{Lectura de ficheros de datos}\label{lectura-de-ficheros-de-datos}}

También es posible introducir información a través de la lectura de ficheros de datos con la función \texttt{fileInput()}. Podemos combinar valores por defecto de la función utilizada para la lectura de datos con algunos de los elementos anteriores para definir las características del dataset (seaprador, decimal, cabecera). En el siguiente ejemplo se utiliza la función \texttt{read.csv} y como separador siempre punto y coma, mientras que los demás elementos se dan a elegir.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shinyUI}\NormalTok{(}\FunctionTok{fluidPage}\NormalTok{(}
  \FunctionTok{headerPanel}\NormalTok{(}\StringTok{"Lectura de datos"}\NormalTok{),}
  \FunctionTok{sidebarPanel}\NormalTok{(}
    \FunctionTok{h4}\NormalTok{(}\StringTok{"Cargar fichero CSV"}\NormalTok{),}
    \FunctionTok{fileInput}\NormalTok{(}\StringTok{"file1"}\NormalTok{, }\StringTok{""}\NormalTok{,}
      \AttributeTok{accept =} \FunctionTok{c}\NormalTok{(}\StringTok{"text/csv"}\NormalTok{, }\StringTok{"text/comma{-}separated{-}values,text/plain"}\NormalTok{, }\StringTok{".csv"}\NormalTok{)}
\NormalTok{    ),}
    \FunctionTok{checkboxInput}\NormalTok{(}\StringTok{"header"}\NormalTok{, }\StringTok{"Header (el csv tiene nombres de variables)"}\NormalTok{, }\ConstantTok{TRUE}\NormalTok{),}
    \FunctionTok{radioButtons}\NormalTok{(}
      \StringTok{"dec"}\NormalTok{, }\StringTok{"Separador de decimales:"}\NormalTok{,}
      \FunctionTok{c}\NormalTok{(}
        \StringTok{"Punto"} \OtherTok{=} \StringTok{","}\NormalTok{,}
        \StringTok{"Coma"} \OtherTok{=} \StringTok{"."}
\NormalTok{      )}
\NormalTok{    )}
\NormalTok{  ),}
  \FunctionTok{mainPanel}\NormalTok{(}
    \FunctionTok{tabsetPanel}\NormalTok{(}
      \FunctionTok{tabPanel}\NormalTok{(}
        \StringTok{"CSV"}\NormalTok{,}
        \FunctionTok{h4}\NormalTok{(}\StringTok{"Vista del fichero CSV"}\NormalTok{),}
        \FunctionTok{tableOutput}\NormalTok{(}\StringTok{"contents"}\NormalTok{)}
\NormalTok{      )}
\NormalTok{    )}
\NormalTok{  )}
\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/lecturadatos} 

}

\caption{Lectura de datos}\label{fig:foo}
\end{figure}

\hypertarget{elementos-para-visualizaciuxf3n-salida}{%
\section{Elementos para visualización (salida)}\label{elementos-para-visualizaciuxf3n-salida}}

Tras la introducción de ciertos parámetros en el \texttt{ui.R}, éstos se pueden utilizar en el script \texttt{server.R} mediante la expresión \texttt{input}. El código de \textbf{R} que construye el objeto basado en esos datos se desarrolla en el servidor y para generar esos objetos se utilizan las funciones \texttt{renderX} donde X es el tipo de objeto a devolver. Por úlitmo, este objeto se referencia nuevamente en el \texttt{ui.R} en el lugar que se desea mostrar (panel) a través de la expresión \texttt{XOutput}.

El hecho de colocar una función en \texttt{ui} le dice a \texttt{Shiny} dónde mostrar su objeto. A continuación, hay que decirle a \texttt{Shiny} cómo construir el objeto.
Esto se hace proporcionando el código \textbf{R} que construye el objeto en la función del servidor.

En concreto, algunas posibildiades son se pueden ver en la Tabla @ref(tab:my\_table).

\begin{table}
\centering
\begin{tabular}{l|l|l}
\hline
Server & Ui & Crea\\
\hline
renderImage & imageOutput & Imagen\\
\hline
renderPlot & plotOutput & Grafico\\
\hline
renderTable & tableOutput & Tabla\\
\hline
renderText & textOutput & Texto\\
\hline
 & htmlOutput & HTML\\
\hline
 & verbatimTextOutput & Texto verbatim\\
\hline
\end{tabular}
\end{table}

\begin{itemize}
\tightlist
\item
  \textbf{Gráficos}: Para generar la aplicación de la Figura \ref{fig:foo1}, se utiliza \texttt{renderPlot} en el server.R como sigue:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{"source.R"}\NormalTok{)}
\FunctionTok{shinyServer}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(input, output) \{}
\NormalTok{  output}\SpecialCharTok{$}\NormalTok{gra1 }\OtherTok{\textless{}{-}} \FunctionTok{renderPlot}\NormalTok{(\{}
    \FunctionTok{graf\_evol}\NormalTok{(input}\SpecialCharTok{$}\NormalTok{vble)}
\NormalTok{  \})}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

Se ha llamado \texttt{gra1} a la variable que es el gráfico y que se crea con \texttt{renderPlot()}. En el interior se utiliza una función denominada \texttt{graf\_evol} que es compleja y se crea en el\texttt{source.R} que se carga al principio. Lo que interesa de esta función es que tiene como único argumento el nombre de la variable por el cual crear el gráfico, que puede ser una de las diversas opciones que se dan a través del radioButton vble creado anteriormente y que, como vemos, se utiliza \texttt{input\$vble} para invocar a la selección realizada en la interfaz de usuario.

\begin{itemize}
\tightlist
\item
  \textbf{Tablas}: Para mostrar la tabla de la Figura \ref{fig:foo} se utiliza \texttt{renderTable()} en el server y \texttt{tableOutput()} en el ui.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shinyServer}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(input, output) \{}
\NormalTok{  output}\SpecialCharTok{$}\NormalTok{contents }\OtherTok{\textless{}{-}} \FunctionTok{renderTable}\NormalTok{(\{}
\NormalTok{    inFile }\OtherTok{\textless{}{-}}\NormalTok{ input}\SpecialCharTok{$}\NormalTok{file1}
    \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{is.null}\NormalTok{(inFile)) \{}
      \FunctionTok{return}\NormalTok{(}\ConstantTok{NULL}\NormalTok{)}
\NormalTok{    \}}
    \FunctionTok{read.csv}\NormalTok{(inFile}\SpecialCharTok{$}\NormalTok{datapath, }\AttributeTok{header =}\NormalTok{ input}\SpecialCharTok{$}\NormalTok{header, }\AttributeTok{dec =}\NormalTok{ input}\SpecialCharTok{$}\NormalTok{dec, }\AttributeTok{sep =} \StringTok{";"}\NormalTok{)}
\NormalTok{  \})}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\hypertarget{reactividad}{%
\section{Reactividad}\label{reactividad}}

El modelo de \index{reactividad} reactividad que utiliza \texttt{Shiny} es el siguiente: hay una fuente reactiva, un conductor reactivo y un punto final de la reactividad.
La fuente reactiva suele ser lo que el usuario introduce y el punto de parada lo que se muestra por pantalla. A lo que el usuario introduce se accede con el objeto input y a lo que se muestra por pantalla con el objeto output.
Un ejemplo que ya se ha usado es el siguiente:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{output}\SpecialCharTok{$}\NormalTok{gra1 }\OtherTok{\textless{}{-}} \FunctionTok{renderPlot}\NormalTok{(\{}
  \FunctionTok{graf\_evol}\NormalTok{(input}\SpecialCharTok{$}\NormalTok{vble)}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

El objeto \texttt{output\$gra1} es un punto final de la reactividad, y usa la fuente reactiva \texttt{input\$vble}. Cuando \texttt{input\$vbles} cambia, a \texttt{output\$gra1} se le notifica que necesita ejecutarse de nuevo.

\hypertarget{conductores-reactivos-y-control-de-la-reactividad}{%
\subsection{Conductores reactivos y control de la reactividad}\label{conductores-reactivos-y-control-de-la-reactividad}}

También es posible crear \index{componentes reactivos} componentes reactivos que conecten los inputs y los outputs. En el siguiente ejemplo se ha creado un objeto reactivo \texttt{datos} que genera datos que siguen una distribución que el usuario selecciona a través del radioButton \texttt{dist} y cuya muestra tiene tantos elementos como el usuario haya especificado en el numericInput \texttt{obs}.

\texttt{Shiny}, además, permite controlar la reactividad a través de los actionButtons. Se pueden modificar las entradas sin obtener una respuesta hasta que se apriete dicho botón.

Se ha creado un panel nuevo dentro del mainPanel y éste contendrá, además del \texttt{plot} previo, un \texttt{actionButton} con la etiqueta \texttt{Presiona}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shinyUI}\NormalTok{(}\FunctionTok{fluidPage}\NormalTok{(}
  \FunctionTok{headerPanel}\NormalTok{(}\StringTok{"Controlar reactividad"}\NormalTok{),}
  \FunctionTok{sidebarPanel}\NormalTok{(}
    \FunctionTok{radioButtons}\NormalTok{(}\StringTok{"dist"}\NormalTok{, }\StringTok{"Tipo de distribucion:"}\NormalTok{,}
      \FunctionTok{c}\NormalTok{(}
        \StringTok{"Normal"} \OtherTok{=} \StringTok{"norm"}\NormalTok{,}
        \StringTok{"Uniforme"} \OtherTok{=} \StringTok{"unif"}\NormalTok{,}
        \StringTok{"Log{-}normal"} \OtherTok{=} \StringTok{"lnorm"}\NormalTok{,}
        \StringTok{"Exponencial"} \OtherTok{=} \StringTok{"exp"}
\NormalTok{      ),}
      \AttributeTok{selected =} \StringTok{"Exponencial"}
\NormalTok{    ),}
    \FunctionTok{numericInput}\NormalTok{(}\StringTok{"obs"}\NormalTok{, }\StringTok{"Numero de observaciones:"}\NormalTok{, }\DecValTok{10}\NormalTok{),}
\NormalTok{  ),}
  \FunctionTok{mainPanel}\NormalTok{(}
    \FunctionTok{tabPanel}\NormalTok{(}
      \StringTok{"Histograma distribucion RadioButton"}\NormalTok{,}
      \StringTok{"Plot"}\NormalTok{, }\FunctionTok{plotOutput}\NormalTok{(}\StringTok{"plot"}\NormalTok{), }\FunctionTok{actionButton}\NormalTok{(}\StringTok{"botonReac"}\NormalTok{, }\StringTok{"Presiona"}\NormalTok{)}
\NormalTok{    )}
\NormalTok{  )}
\NormalTok{))}
\end{Highlighting}
\end{Shaded}

A continuación, se hace referencia a ese botón para cada una de las expresiones reactivas que se aislarán con la función \texttt{isolate}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shinyServer}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(input, output) \{}
\NormalTok{  datos }\OtherTok{\textless{}{-}} \FunctionTok{reactive}\NormalTok{(\{}
    \ControlFlowTok{if}\NormalTok{ (input}\SpecialCharTok{$}\NormalTok{botonReac }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) \{}
      \FunctionTok{return}\NormalTok{(}\FunctionTok{dist}\NormalTok{(}\FunctionTok{rexp}\NormalTok{(input}\SpecialCharTok{$}\NormalTok{obs)))}
\NormalTok{    \}}
    \FunctionTok{isolate}\NormalTok{(\{}
\NormalTok{      dist }\OtherTok{\textless{}{-}} \ControlFlowTok{switch}\NormalTok{(input}\SpecialCharTok{$}\NormalTok{dist,}
        \AttributeTok{norm =}\NormalTok{ rnorm,}
        \AttributeTok{unif =}\NormalTok{ runif,}
        \AttributeTok{lnorm =}\NormalTok{ rlnorm,}
        \AttributeTok{exp =}\NormalTok{ rexp,}
\NormalTok{        rnorm}
\NormalTok{      )}

      \FunctionTok{dist}\NormalTok{(input}\SpecialCharTok{$}\NormalTok{obs)}
\NormalTok{    \})}
\NormalTok{  \})}

\NormalTok{  output}\SpecialCharTok{$}\NormalTok{plot }\OtherTok{\textless{}{-}} \FunctionTok{renderPlot}\NormalTok{(\{}
    \ControlFlowTok{if}\NormalTok{ (input}\SpecialCharTok{$}\NormalTok{botonReac }\SpecialCharTok{==} \DecValTok{0}\NormalTok{) \{}
      \FunctionTok{return}\NormalTok{(}\ConstantTok{NULL}\NormalTok{)}
\NormalTok{    \}}
    \FunctionTok{isolate}\NormalTok{(\{}
      \FunctionTok{hist}\NormalTok{(}\FunctionTok{datos}\NormalTok{(),}
        \AttributeTok{main =} \FunctionTok{paste}\NormalTok{(}\StringTok{"r"}\NormalTok{, input}\SpecialCharTok{$}\NormalTok{dist, }\StringTok{"("}\NormalTok{, input}\SpecialCharTok{$}\NormalTok{obs, }\StringTok{")"}\NormalTok{, }\AttributeTok{sep =} \StringTok{""}\NormalTok{)}
\NormalTok{      )}
\NormalTok{    \})}
\NormalTok{  \})}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{img/controlar} 

}

\caption{Barras}\label{fig:unnamed-chunk-271}
\end{figure}

\hypertarget{publicaciuxf3n-de-la-aplicaciuxf3n-en-la-web}{%
\section{Publicación de la aplicación en la web}\label{publicaciuxf3n-de-la-aplicaciuxf3n-en-la-web}}

Después del desarrollo de una aplicación \texttt{Shiny}, suele ser interesante publicarla para su explotación científica o empresarial. Rstudio ofrece diversas soluciones que se analizarán, con distintos niveles de complejidad y libertad para poder publicar la aplicación web : (i) shinyapps.io, (ii) Shiny Server y (iii) RStudio Connect.

Se dará una introducción muy breve a cada uno de ellos y enlaces para que el lectro pueda indagar en profundidad

\begin{itemize}
\tightlist
\item
  Shinyapps.io
\end{itemize}

Rstudio ofrece un servicio de \emph{hosting} denominado Shinyapps.io que permite subir la aplicación directamente desde la sesión de \textbf{R} a un servidor que se mantiene por Rstudio. Hay un control casi completo sobre la aplicación, incluyendo la administración del servidor.

Lo único que se necesita es:
- Un entorno de desarrollo de R, como RStudio IDE
- La última versión del paquete rsconnect

En la web \url{shinyapps.io} en el apartado ``Dashboard'' se realiza el registro. Shinyapps.io genera de forma automática un token que el paquete \texttt{rsconnect} utiliza para acceder a la cuenta.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rsconnect}\SpecialCharTok{::}\FunctionTok{setAccountInfo}\NormalTok{(}\AttributeTok{name =} \StringTok{"\textless{}ACCOUNT\textgreater{}"}\NormalTok{, }\AttributeTok{token =} \StringTok{"\textless{}TOKEN\textgreater{}"}\NormalTok{, }\AttributeTok{secret =} \StringTok{"\textless{}SECRET\textgreater{}"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Para desplegar la aplicación utiliza \texttt{deployApp()} como sigue

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rsconnect)}
\FunctionTok{deployApp}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Para más información sobre este método, consulta la página \url{https://shiny.rstudio.com/articles/shinyapps.html}

\begin{itemize}
\tightlist
\item
  \texttt{Shiny} Server
\end{itemize}

\texttt{Shiny} server construye un servidor web diseñado para hospedar aplicaciones \texttt{Shiny}. Es gratuito, de código abierto y está disponible en GitHub.

Para usar el \texttt{Shiny} Server, es necesario tener un servidor Linux que tenga soporte explícito para Ubuntu 12.04 or superior (64 bit) y CentOS/RHEL 5 (64 bit). Si no se está utilizando una distribución con soporte explícito, se puede aún así utilizar construyéndolo desde el paqeute fuente.

En el mismo \texttt{Shiny} Server se pueden hospedar múltiples aplicaciones \texttt{Shiny}. Para ver instrucciones detalladas para su instalación y configuración, se recomienda la guía \texttt{Shiny} Server \url{https://docs.rstudio.com/shiny-server}.`

\begin{itemize}
\tightlist
\item
  RStudio Connect
\end{itemize}

Cuando \texttt{Shiny} se utiliza en entornos con fines lucrativos, existen herramientas de servidor que se pueden comprar y que vienen equipadas con los programas habituales de un servidor de pago:

\begin{itemize}
\tightlist
\item
  Soporte SSL
\item
  Herramientas de administrador
\item
  Soporte prioritario
\end{itemize}

Para ello, la plataforma de publicación RStudio Connect puede ser una solución. Esta herramienta permite compartir aplicacciones \texttt{Shiny}, informes RMarkdown, cuadros de mando, gráficos, Jupyter Notebooks y más.
Con RStudio Connect se puede programar la ejecución de informes y políticas de seguridad flexibles.

\hypertarget{extensiones-de-shiny}{%
\section{\texorpdfstring{Extensiones de \texttt{Shiny}}{Extensiones de Shiny}}\label{extensiones-de-shiny}}

\texttt{Shiny} es una herramienta totalmente expansible. Lo que se ha mostrado en este capítulo hasta ahora es un aperitivo en relación a todas las posibilidades que existen en el mundo de \texttt{Shiny}. Hay repositorios que recopilan información sobre paquetes que proveen de mejoras a las aplicaciones \texttt{Shiny} en su estilo y funcionalidad \citep[\citet{awesome2}]{awesome}. En esta sección se mencionarán algunos de ellos pero, sobre todo, se econmienda al lector els visitar dichos repositorios para una mayor profundidad en este tema.

\begin{itemize}
\tightlist
\item
  \texttt{shinydashboard}, \texttt{shinydashboardPlus} y \texttt{flexdashboards}
\end{itemize}

En temas de estilo, se destacan éstos tres paquetes. Los dos primeros presentan una serie de plantillas predefinidas para la creación de las aplicaciones \texttt{Shiny}, de manera que los colores combinan y los elementos visuales tienen cierta armonía.

Por su parte, flexdashboards tiene como base un documento \emph{R Markdown} y los distintos niveles del mismo definen los paneles de la aplicación a crear.

\begin{itemize}
\tightlist
\item
  \texttt{shinyWidgets}
\end{itemize}

Este paquete ofrece \emph{widgets} personalizados y diversos componentes para mejorar las aplicaciones. Se pueden reemplazar los \texttt{checkboxes} por \texttt{switch\ buttons}, añadir colores a los \texttt{radioButtons} y al grupo de casillas de verificación (\texttt{checkboxGroupInput}), etc. Cada widget tiene un método de actualización para cambiar el valor de una entrada del server.

\begin{itemize}
\tightlist
\item
  \texttt{shinycssloaders}
\end{itemize}

Cuando una salida de \texttt{Shiny} (un gráfico, una tabla, etc.) se está calculando, permanece visible pero en gris. Si hay procesos algo más complejos, pueden tardar en mostrarse. Utilizando \texttt{shinycssloaders}, se puede añadir una rueda de carga (\emph{spinner}) a las salidas en lugar de hacerlas grises. Envolviendo una salida \texttt{Shiny} en \texttt{withSpinner()}, el \emph{spinner} aparecerá automáticamente mientras la salida se recalcula. Hay 8 tipos de animación incorporadas y personalizables en color y tamaño, pero también se pueden cargar otras animaciones.

\begin{itemize}
\tightlist
\item
  Visualizaciones interactivas
\end{itemize}

Paquetes como \emph{heatmaply} o \emph{leaflet} se pueden combinar perfectamente con \texttt{Shiny} para crear mapas de calor y mapas geográficos interactivos y utilizarlos en las aplicaciones.

\begin{infobox_resume}
\textbf{RESUMEN}

\texttt{Shiny} es un paquete de \textbf{R} que permite crear aplicaciones web interactivas requiriendo únicamente conocimientos de R. En la primera parte de este capítulo se muestran los elementos básicos de una aplicación \texttt{Shiny}: user interface (ui.R) y servidor (server.R), se muestran posibles diseños en relación a los componentes que una aplicación puede tener: barra lateral, paneles discretos, paneles de navegación, etc. A continuación, se repasan los elementos de entrada de datos en una aplicación \texttt{Shiny}, incluyendo la carga de conjuntos de datos y también los elementos de salida como gráficas y tablas. También se repasa el modelo reactividad, es decir, cómo al cambiar algo en los parámetros de entrada de forma dinámica cambia la salida y cómo controlarlo y se muestran distintas opciones para la publicación de las aplicaciones \texttt{Shiny}. Por último, se mencionan algunas de las posibles extensiones al paquete.

\end{infobox_resume}

\hypertarget{github}{%
\chapter{Git y GitHub en R}\label{github}}

\emph{Michal Kinel}

\hypertarget{quuxe9-es-git}{%
\section{¿Qué es Git?}\label{quuxe9-es-git}}

Git es un sistema de control de versiones distribuido, gratuito y de
código abierto, diseñado para gestionar desde proyectos pequeños hasta
muy grandes con rapidez y eficacia \citep{chacon2009}.

Es esencial usar Git en la colaboración con otros desarrolladores en un
proyecto de codificación o trabajar un proyecto propio.

Para comprobar si Git ya se encuentra instalado en el ordenador basta
con escribir \texttt{git\ -\/-version} en el terminal de RStudio.

Si Git ya está instalado en su equipo, la consola proporcionará su
versión. Si no se encuentra instalado, se puede obtener en el sitio
oficial de \href{https://git-scm.com/}{Git} y seguir
fácilmente las instrucciones de descarga para instalar la versión más
adecuada para el sistema operativo.

\begin{infobox}

\textbf{Comandos de Git}

\begin{itemize}
\tightlist
\item
  \texttt{git\ init} - Crea un repositorio Git vacío en el directorio especificado. Ejecutar sin argumentos para iniciar el directorio actual como un repositorio Git.
\item
  \texttt{git\ clone\ \textless{}repo\textgreater{}} - Clona el repo ubicado en en la máquina local. El repositorio original puede estar ubicado en el sistema de archivos local o en una máquina remota a través de HTTP o SSH.
\item
  \texttt{git\ config\ user.name\ \textless{}nombre\textgreater{}} - Define el nombre del autor que se utilizará para todas las confirmaciones en el repositorio actual. Los desarrolladores suelen utilizar \texttt{-\/-global} para establecer las opciones de configuración para el usuario actual.
\item
  \texttt{git\ add\ \textless{}directorio\textgreater{}} - prepara todos los cambios en \texttt{\textless{}directorio\textgreater{}} para la siguiente confirmación \texttt{\textless{}commit\textgreater{}} de la versión del código. Sustituya \texttt{\textless{}directorio\textgreater{}} por un \texttt{\textless{}fichero\textgreater{}} para cambiar un archivo específico.
\item
  \texttt{git\ commit\ -m\ "\textless{}mensaje\textgreater{}"} - Confirma la instantánea preparada, pero en lugar de lanzar un editor de texto, utilice \texttt{\textless{}mensaje\textgreater{}} como mensaje de la confirmación \texttt{\textless{}commit\textgreater{}}.
\item
  \texttt{git\ status} - Enumera los archivos que están preparados \texttt{\textless{}staged\textgreater{}}, no preparados \texttt{\textless{}unstaged\textgreater{}} y sin seguimiento \texttt{\textless{}untracked\textgreater{}}.
\item
  \texttt{git\ log} - Muestra todo el historial de confirmaciones \texttt{\textless{}commits\textgreater{}} utilizando el formato por defecto. Para la personalización vea las opciones adicionales.
\item
  \texttt{git\ diff} - Muestra los cambios no preparados \texttt{\textless{}unstaged\textgreater{}} entre el índice y el directorio de trabajo.
\item
  \texttt{git\ remote\ add\ \textless{}nombre\textgreater{}\ \textless{}url\textgreater{}} - Crea una nueva conexión a un repositorio remoto. Después de añadir un remoto, se puede usar como un atajo para \texttt{\textless{}url\textgreater{}} en otros comandos.
\item
  \texttt{git\ fetch\ \textless{}remote\textgreater{}\ \textless{}branch\textgreater{}} - Obtiene un \texttt{\textless{}branch\textgreater{}} específico, desde el repositorio. Si se deja un \texttt{\textless{}branch\textgreater{}} para obtener todas las referencias remotas.
\item
  \texttt{git\ pull\ \textless{}remote\textgreater{}} - Obtiene la copia remota especificada de la rama actual y la fusiona inmediatamente con la copia local.
\item
  \texttt{git\ push\ \textless{}remote\textgreater{}\ \textless{}branch\textgreater{}} - Empuja la rama a \texttt{\textless{}remote\textgreater{}}, junto con los commits y objetos necesarios. Crea la rama nombrada en el repo remoto si no existe.
\end{itemize}

\end{infobox}

\hypertarget{quuxe9-es-github}{%
\section{¿Qué es GitHub?}\label{quuxe9-es-github}}

GitHub es un servicio de alojamiento de repositorios Git que ofrece una
interfaz gráfica basada en la web. Es la comunidad más popular para que
los desarrolladores compartan código y trabajen juntos en proyectos. Es
gratuito, fácil de usar y se ha convertido en el centro del movimiento
hacia el software de código abierto. Los programadores pueden encontrar
códigos fuente en muchos lenguajes diferentes, entre ellos R, y utilizar
la interfaz de línea de comandos, Git, para hacer y mantener un
seguimiento de los cambios.

GitHub ayuda a todos los miembros del equipo a trabajar juntos en un
proyecto desde cualquier lugar y facilita la colaboración \citep{astigarraga2022}. También se pueden revisar versiones anteriores
creadas en un momento anterior.

\hypertarget{por-quuxe9-usar-git-y-github}{%
\section{¿Por qué usar Git y GitHub?}\label{por-quuxe9-usar-git-y-github}}

La integración de Git y GitHub en el desarrollo de nuestros proyectos en
R, sean grandes o pequeños. Gracias a Git, todo el proceso de desarrollo
de código está versionado, con el histórico en en el ordenador (en
local) y GitHub centraliza todos los cambios online (en remoto).

Las principales ventajas de utilizar Git y GitHub son:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Seguridad: proporciona una copia de seguridad para nuestro código y
  un control sobre los cambios que vamos introduciendo.
\item
  Contribución a tus proyectos: facilita la colaboración con los
  compañeros de trabajo o la comunidad de GitHub cuando utilizamos los
  repositorios públicos o privados, previa asignación de acceso.
  Además, GitHub es una de las mayores comunidades de codificación que
  existen ahora mismo, por lo que es una amplia exposición para tu
  proyecto.
\item
  Documentación: GitHub posee una excelente documentación. Su sección
  de ayuda y guías pueden resolver casi cualquier duda.
\item
  Markdown: permite utilizar un sencillo editor de texto para escribir
  documentos con formato, de forma que se puede enriquecer los
  proyectos de un texto explicativo para que los demás usuarios o el
  propio desarrollador (pasado un tiempo) pueda identificar
  rápidamente las características del proyecto.
\item
  Seguimiento de los cambios en tu código a través de las versiones:
  cuando varios desarrolladores colaboran en un proyecto, es difícil
  hacer un seguimiento de las versiones: quién ha cambiado qué, cuándo
  y dónde se almacenan estos archivos. GitHub se encarga de este
  problema manteniendo un registro de todos los cambios que se han
  enviado al repositorio.
\item
  Integración: al tratarse de una plataforma tan ampliamente
  utilizada, GitHub puede integrarse fácilmente con plataformas como
  Amazon o Google Cloud o resaltar la sintaxis en más de 200 lenguajes
  de programación diferentes.
\end{enumerate}

\hypertarget{configuraciuxf3n}{%
\section{Configuración}\label{configuraciuxf3n}}

En este apartado va a explicar de forma sintetizada cómo configurar un repositorio de Git en RStudio y vincularlo a un proyecto en GitHub. Antes de comenzar, se va a asumir el proyecto de RStudio se denomina \texttt{mi\_proyecto.Rproj} y que el usuario de GitHub es \texttt{\textbackslash{}@mi\_usuario}.

Git utiliza una terminal llamada Git Bash que en su esencia es un conjunto de programas de utilidad de línea de comandos que están diseñados para ejecutarse en un entorno de línea de comandos estilo Unix. En el caso de que el equipo no tenga instalado Git, está disponible en la página oficial de \href{https://git-scm.com}{Git}, donde se encuentra el software y el manual de ayuda. RStudio permite configurar Git Bash como la terminal por defecto, basta con acceder a Tools -\textgreater{} Global options\ldots{} -\textgreater{} Terminal -\textgreater{} New terminals open with. La forma más fácil de utilizar la línea de comandos en RStudio es utilizar la pestaña Terminal justo al lado de la pestaña de la consola en el panel inferior izquierdo.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/terminal} 

}

\caption{Terminal de RStudio}\label{fig:terminal}
\end{figure}

\begin{infobox}
\textbf{NOTA}

Una cosa a tener en cuenta es que GitHub tiene un límite de tamaño de archivo único de 100mb. Por ello será importante configurar bien el archivo .gitignore y utilizar ficheros comprimidos o compartir los datos por medio de alguno de los servicios almacenamiento de datos en la nube.

\end{infobox}

\hypertarget{configurar-git}{%
\section{Configurar git}\label{configurar-git}}

Git asocia el nombre y dirección de correo electrónico con cada \textbf{commit}, lo que ayuda cuando varias personas colaboran en un proyecto. Para configurar tu nombre y dirección de correo electrónico en git, abra el Terminal y ejecute:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{git config {-}{-}global user.name \textquotesingle{}Tu Nombre\textquotesingle{}}
\NormalTok{git config {-}{-}global user.email tu\_correo@email.com\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

En un macOS, es aconsejable configurar Git para que recuerde la del usuario:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{git config {-}{-}global credential.helper osxkeychain}
\end{Highlighting}
\end{Shaded}

El siguiente paso es generar la llave SSH para el equipo en el que se va a desarrollar el trabajo y añadirla en la cuenta de GitHub. Primero se accede a Tools -\textgreater{} Global Options\ldots{} -\textgreater{} Git/SVN y pulsamos Create SSH key, se selecciona SSH key type como ED25519. Como función opcional se puede incluir una contraseña aunque luego resulta incordioso al hacer commits, por lo que si la llave se configura en un ordenador personal no hace falta. Se copia la dirección donde se crea la llave, que por defecto se crea en la carpeta .ssh de usuario, (por ejemplo en Windows: C:/Users/mi\_user/.ssh/id\_ed25519) y se pulsa en create. Tras realizar estos pasos se introduce en la terminal:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{clip \textless{} C:/Users/mi\_user/.ssh/id\_ed25519.pub }
\NormalTok{\# Copia el contenido del archivo id\_ed25519.pub en el portapapeles}
\NormalTok{\# ruta con instalación de R y RStudio con todas las opciones por defecto: \textasciitilde{}/.ssh/id\_ed25519.pub}
\end{Highlighting}
\end{Shaded}

Una vez copiada la llave SSH se accede a GitHub dando clic en el perfil de usuario a Settings luego en el menú de Access, posteriormente en SSH and GPG keys y se selecciona la opción pulsando New SSh key y aparece el menú de creación de llave SSH. En Title se introduce un nombre identificativo como por ejemplo, Mi pc personal, en Key type se deja la opción por defecto: Authentication Key, y en el recuadro de Key se pega la llave SSH copiada con la función clip del terminal. la llave se tiene que parecer al ejemplo inferior:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ssh{-}ed25519 A44AC3NzaC1lZDI1NTE5UGHJIGVuY9LWIcUX+CbtPKsO6yERjp5fFbCaA8QPM1qbLsGU mi\_user@NOMBRE\_DE\_EQUIPO}
\end{Highlighting}
\end{Shaded}

Para terminar la configuración de la llave se confirma dando clic en Add SSH key. Tras realizar este proceso la autorización del equipo queda finalizada y el equipo se encuentra listo para usar Git en GitHub.

Ahora se procede a convertir el proyecto ejemplo, mi\_proyecto.Rproj, en un repositorio Git, para ello se abre el proyecto proyecto y simplemente se ejecuta en la terminal el siguiente comando:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{git init}
\end{Highlighting}
\end{Shaded}

Esto creará un repositorio git en el directorio de trabajo. Debido a que se está utilizando la línea de comandos, RStudio normalmente no reconocerá inmediatamente el cambio, por lo que se debe cerrar RStudio y volver a abrir el programa. Para reabrir el proyecto, se puede hacer doble clic en el archivo mi\_proyecto.Rproj o utilizar el menú desplegable para la gestión de proyectos en la esquina superior derecha del propio RStudio. Una vez reabierto el proyecto, aparecerá una pestaña adicional ``Git'' situada en el panel superior derecho.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/rstudio_git} 

}

\caption{Pestaña de Git de RStudio}\label{fig:git}
\end{figure}

La pestaña de Git mostrará los ficheros creados y que aún no han sido versionados por git, así como los archivos que han sido modificados o eliminados desde el último commit. El icono del signo de interrogación para el estado de cada archivo me muestra que ninguno de los archivos de mi directorio está siendo versionado actualmente. Se seleccionan los ficheros para el versionado y se pulsa el botón de commit. Se abrirá una ventana con los detalles, se introduce el mensaje del commit que resume el cambio registrado. Como se trata del primer commit del proyecto, el mensaje podría ser el siguiente: Primer commit del proyecto y pulsamos commit.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/commit_window} 

}

\caption{Ventana de confirmación de RStudio}\label{fig:commit-window}
\end{figure}

Una vez obtenido el seguimiento de los cambios en el equipo de trabajo local, el siguiente paso es hacer un repositorio en el perfil de GitHub.

Para configurar un nuevo repositorio en GitHub, basta con dirigirse al menú desplegable ``+'' en la parte superior derecha de tu página de perfil de GitHub y seleccionar la opción New repository. El cuadro de diálogo del nuevo repositorio tiene varias opciones, como requisitos mínimos se introduce un nombre para el repositorio, por ejemplo \textbf{mi\_proyecto.} En esta etapa se puede seleccionar si el proyecto es público o privado, no obstante, ésta opción se puede modificar posteriormente en las opciones del repositorio.

Una vez creado el repositorio, GitHub proporcionará unas instrucciones de cómo comenzar a usarlo. En caso de comenzar con los datos locales las instrucciones se encuentran en \textbf{\ldots or push an existing repository from the command line}, siguiendo con el ejemplo:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{git remote add origin git@github.com:mi\_usuario/mi\_proyecto.git}
\NormalTok{git branch {-}M main}
\NormalTok{git push {-}u origin main}
\end{Highlighting}
\end{Shaded}

Se copia el código proporcionado por GitHub en la terminal de RStudio del proyecto abierto y se ejecuta. Estos comandos de Git configurarán GitHub como el repositorio remoto origin y empujará, push, todos los commits locales existentes a él. A continuación, se puede actualizar la página del repositorio de GitHub y ver que todos los archivos y el código del proyecto se encuentran en el repositorio. Como los comandos fueron ejecutados por la terminal hay que reiniciar RStudio, para que al reiniciar aparecerán operativas las flechas con Pull y Push. Ahora ya está disponible realizar la operativa del flujo de trabajo básico de git de pull-commit-push usando la pestaña git de RStudio.

\hypertarget{workflow}{%
\section{Workflow}\label{workflow}}

El flujo de trabajo típico es el siguiente - se crea/edita/modifica un archivo dentro del repositorio \citep{happygitwithr}. Se hace el commit de los cambios, lo que crea una instantánea permanente del archivo en el directorio Git junto con un mensaje del commit para indicar qué cambio se realiza en el código.

Cuando se empieza un nuevo proyecto, los archivos del directorio de trabajo no tienen seguimiento, primero es necesario añadirlos al repositorio antes de que Git pueda hacer un seguimiento de ellos y de su historia.

Una vez creado el commit con su mensaje correspondiente se debe de hacer push desde el asistente de RStudio o en la terminal con git push.

Cuando se trabaja con otros usuarios o se utilizan varios ordenadores para trabajar con un mismo proyecto es importante comenzar el trabajo ejecutando el comando git pullen la terminal para descargar todos los commits del remoto.

El mensaje del commit es muy importante, porque en el futuro ayudará a identificar los cambios realizados por el desarrollador o un groupon de ellos. Para facilitar la lectura y el seguimiento de los commits se debe de seguir ciertos consejos establecidos por la comunidad:

\begin{itemize}
\tightlist
\item
  Utilizar el modo imperativo en la línea del asunto, al igual que en una receta
  de cocina el cambio tiene un fin particular, por ejemplo: \emph{Fix bug in my\_function}
  \emph{o Corregir una errata en la descripción del proyecto},
\item
  El asunto debe empezar por mayúscula, tener aproximadamente como máximo 50
  caracteres y no terminar en un punto
\item
  Si es necesaria una descripción más detallada, se incluye tras una línea en
  blanco bajo el encabezado. Cuando se escribe el cuerpo de un mensaje de
  confirmación, se debe de tener en cuenta su margen derecho, y envolver el texto
  manualmente. La recomendación es hacerlo a los 72 caracteres, para que Git tenga
  suficiente espacio para sangrar el texto, pero manteniendo todo por debajo de los
  80 caracteres. Además se debe de utilizar la descripción para responder
  preferentemente a qué y el por qué frente al cómo.
\end{itemize}

\hypertarget{cap-geoprocesamiento}{%
\chapter{Geoprocesamiento en nube}\label{cap-geoprocesamiento}}

\emph{Dominic Royé}

Habitualmente se obtiene los datos desde cualquier proveedor lo que implica la descarga de grandes volúmenes. No obstante, en la actualidad existen servicios de geoprocesamiento en nube que nos ayudan a hacer análisis online sin necesidad de descargar los datos ni preocuparnos por el rendimiento del cálculo. Uno de estos servicios es \emph{Google Earth Engine} (GEE) donde se combina un catálogo de varios petabytes de imágenes satelitales y conjuntos de datos geoespaciales multidimensionales con capacidades de análisis a escala planetaria.

GEE consiste en una API con bibliotecas de cliente para JavaScript y Python, que traducen los análisis geoespaciales y hacen posible acceder a los datos. No es necesario descargar grandes volúmenes de datos ni configurar la computación. Para el lenguaje de R se puede hacer uso del paquete \texttt{\{rgee\}} que nos hace puente entre R y la API.

Se hará uso del dataset con el nombre ``NOAA CDR OISST v02r01'', una interpolación óptima diaria de temperatura de la superficie del mar (OISST, por sus siglas en inglés) con una resolución de 1/4 grados (27 km). Los datos proporciona la NOAA con campos completos de temperatura del océano construidos mediante la combinación de observaciones ajustadas por sesgo de diferentes plataformas (satélites, barcos, boyas) en una cuadrícula global regular, con lagunas rellenadas por interpolación (\url{https://developers.google.com/earth-engine/datasets/catalog/NOAA_CDR_OISST_V2_1}) (\citet{reynolds2008}).

El objetivo de este caso práctico es mostrar el potential del uso de APIs directamente en R. El resultado se empleará en el capítulo sobre datos del cambio climático como visualización avanzada.

\hypertarget{sintaxis-de-google-earth-engine}{%
\section{Sintaxis de Google Earth Engine}\label{sintaxis-de-google-earth-engine}}

Con ayuda de GEE se preprocesan los datos de tal manera que el resultado son las anomalías estivales en forma de ráster para cada año entre 1981 y 2022. El primer paso consiste en crear nuestro usuario en earthengine.google.com. Además, es necesario instalar \emph{CLI} de \emph{gcloud} (\url{https://cloud.google.com/sdk/docs/install?hl=es-419}).

Antes se deben conocer algunos conceptos fundamentales sobre la sintaxis en GEE. En general, el lenguaje nativo es Javascript el que se caracteriza por la forma combinando funciones y variables usando el punto, el que se sustuye por el \$ en R. Todas las funciones GEE en R empiezan por el prefijo ee\_* (\texttt{ee\_print()}, \texttt{ee\_image\_to\_drive()}).

\begin{itemize}
\tightlist
\item
  \emph{ImageCollection}: serie temporal de imágenes.
\item
  \emph{Geometry}: dato vectorial.
\item
  \emph{Functions}: \texttt{map()} aplica funciones sobre \emph{ImageCollections}, \texttt{ee.Date()} define una fecha, \texttt{filterDate()} permite filtrar por fecha una \emph{ImageCollection}, \texttt{select()} selecciona una banda, etc.
\end{itemize}

Muchas funciones que se aplican son similares a lo que se conoce de \texttt{\{tidyverse\}}.

Se puede obtener más ayuda en \url{https://r-spatial.github.io/rgee/reference/rgee-package.html} y en la propia página de GEE.

\hypertarget{primeros-pasos}{%
\section{Primeros pasos}\label{primeros-pasos}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{require}\NormalTok{(}\StringTok{"rgee"}\NormalTok{, }\AttributeTok{character.only =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{||} \FunctionTok{install.packages}\NormalTok{(}\StringTok{"rgee"}\NormalTok{, }\AttributeTok{repos =} \StringTok{"https://cloud.r{-}project.org"}\NormalTok{)}

\FunctionTok{library}\NormalTok{(rgee)}
\end{Highlighting}
\end{Shaded}

Después de darse de alta en GEE y haber instalado \emph{CLI gcloud} en el sistema operativo, sea crea un entorno virtual de Python con todas las dependencias de GEE usando la función \texttt{ee\_install()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ee\_install}\NormalTok{() }\CommentTok{\# crear entorno virtual de Python}
\FunctionTok{ee\_check}\NormalTok{() }\CommentTok{\# comprobar si todo está correcto}
\end{Highlighting}
\end{Shaded}

Antes de pasar a programar con la sintaxis propia de GEE se debe autenticar e inicializar GEE empleando la función \texttt{ee\_Initialize()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ee\_Initialize}\NormalTok{() }\CommentTok{\# autenticar e inicializar GEE}
\FunctionTok{ee\_user\_info}\NormalTok{() }\CommentTok{\# inf sobre usuario}
\end{Highlighting}
\end{Shaded}

Hay que tener en cuenta que únicamente cuando se envian tareas GEE ejecuta el cálculo en los servidores enviando todos los objetos creados. En la mayoría de los pasos se crean objetos \emph{EarthEngine}, los que se usan una vez que construimos un mapa interactivo, la exportación o impresión en consola de un objeto.

Por ejemplo, se puede selecionar la banda NDVI del producto MODIS MOD13A2 e imprimir los metadatos de primer día disponible con \texttt{ee\_print()}. Existe un límite 5000 elementos que se podrían ver usando esta función.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# imageCollection NDVI}
\NormalTok{img }\OtherTok{\textless{}{-}}\NormalTok{ ee}\SpecialCharTok{$}\FunctionTok{ImageCollection}\NormalTok{(}\StringTok{"MODIS/006/MOD13A2"}\NormalTok{)}\SpecialCharTok{$}\FunctionTok{select}\NormalTok{(}\StringTok{"NDVI"}\NormalTok{)}

\CommentTok{\# metadatos}
\FunctionTok{ee\_print}\NormalTok{(img}\SpecialCharTok{$}\FunctionTok{first}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\hypertarget{cuxe1lculo-de-anomalias}{%
\section{Cálculo de anomalias}\label{cuxe1lculo-de-anomalias}}

\hypertarget{definiciones-previas}{%
\subsection{Definiciones previas}\label{definiciones-previas}}

Los datos NOAA CDR OISST contienen la temperatura superficial de los océanos a nivel global, por eso, se fija un rectángulo con sus cuatro puntos cubriendo la extensión del Mar Mediterráneo.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# extensión del Mar Mediterráneo}
\NormalTok{geom }\OtherTok{\textless{}{-}}\NormalTok{ ee}\SpecialCharTok{$}\NormalTok{Geometry}\SpecialCharTok{$}\FunctionTok{Polygon}\NormalTok{(}
  \AttributeTok{coords =} \FunctionTok{list}\NormalTok{(}
    \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{6.046418548121442}\NormalTok{, }\FloatTok{46.733937391710846}\NormalTok{),}
    \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{6.046418548121442}\NormalTok{, }\FloatTok{29.680544334046786}\NormalTok{),}
    \FunctionTok{c}\NormalTok{(}\FloatTok{42.469206451878556}\NormalTok{, }\FloatTok{29.680544334046786}\NormalTok{),}
    \FunctionTok{c}\NormalTok{(}\FloatTok{42.469206451878556}\NormalTok{, }\FloatTok{46.733937391710846}\NormalTok{)}
\NormalTok{  ),}
  \AttributeTok{proj =} \StringTok{"EPSG:4326"}\NormalTok{,}
  \AttributeTok{geodesic =} \ConstantTok{FALSE}
\NormalTok{)}

\NormalTok{geom }\CommentTok{\# vemos que es un objeto EarthEngine de tipo geometría}
\FunctionTok{str}\NormalTok{(geom) }\CommentTok{\# construcción javascript}
\end{Highlighting}
\end{Shaded}

En el próximo paso se define el período de interés, desde el año 1982 hasta el 2022.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{startDate }\OtherTok{\textless{}{-}}\NormalTok{ ee}\SpecialCharTok{$}\FunctionTok{Date}\NormalTok{(}\StringTok{"1982{-}01{-}01"}\NormalTok{) }\CommentTok{\# fecha inicio}
\NormalTok{endDate }\OtherTok{\textless{}{-}}\NormalTok{ ee}\SpecialCharTok{$}\FunctionTok{Date}\NormalTok{(}\StringTok{"2022{-}08{-}31"}\NormalTok{) }\CommentTok{\# fecha final}
\end{Highlighting}
\end{Shaded}

Se puede acceder a todas las colecciones (\emph{ImageCollection}) indicando su identifcación. Además, filtramos y recortamos los datos con respecto al periodo y a la extensión fijada. Finalmente se selecciona la banda o variable de de interés `sst' (\emph{surface sea temperature}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{collection\_era5 }\OtherTok{\textless{}{-}}\NormalTok{ ee}\SpecialCharTok{$}\FunctionTok{ImageCollection}\NormalTok{(}\StringTok{"NOAA/CDR/OISST/V2\_1"}\NormalTok{)}\SpecialCharTok{$}
  \FunctionTok{filterDate}\NormalTok{(startDate, endDate)}\SpecialCharTok{$}
  \FunctionTok{filterBounds}\NormalTok{(geom)}\SpecialCharTok{$}
  \FunctionTok{select}\NormalTok{(}\StringTok{"sst"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Procedemos a calcular el número de años en el período fijado.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{numberOfyears }\OtherTok{\textless{}{-}}\NormalTok{ endDate}\SpecialCharTok{$}\FunctionTok{difference}\NormalTok{(startDate, }\StringTok{"years"}\NormalTok{)}\SpecialCharTok{$}\FunctionTok{round}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{promedio-estival}{%
\subsection{Promedio estival}\label{promedio-estival}}

Después de las definiciones necesarias se crea una nueva colección con el promedio estival de cada año en el período. Para ello se hae una lista de los años sobre la que se mapea otra función. Esta función se debe pasar con \texttt{ee\_utilspyfunc()}, la que traduce una función R a una de Python.

En la función personalizada se filtran los meses de verano, se calcula el promedio y se multiplica por 0,01, un factor de escala. Cuando se crea nuevas colecciones es importante fijar la nueva fecha lo que se hace en \texttt{set()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{yearly }\OtherTok{\textless{}{-}}\NormalTok{ ee}\SpecialCharTok{$}\FunctionTok{ImageCollection}\NormalTok{(}
\NormalTok{  ee}\SpecialCharTok{$}\NormalTok{List}\SpecialCharTok{$}\FunctionTok{sequence}\NormalTok{(}\DecValTok{0}\NormalTok{, numberOfyears}\SpecialCharTok{$}\FunctionTok{subtract}\NormalTok{(1L))}\SpecialCharTok{$}
    \FunctionTok{map}\NormalTok{(}\FunctionTok{ee\_utils\_pyfunc}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(dayOffset) \{}
\NormalTok{    start }\OtherTok{\textless{}{-}}\NormalTok{ startDate}\SpecialCharTok{$}\FunctionTok{advance}\NormalTok{(dayOffset, }\StringTok{"years"}\NormalTok{)}
\NormalTok{    end }\OtherTok{\textless{}{-}}\NormalTok{ start}\SpecialCharTok{$}\FunctionTok{advance}\NormalTok{(1L, }\StringTok{"years"}\NormalTok{)}
    \FunctionTok{return}\NormalTok{(collection\_era5}\SpecialCharTok{$}
      \FunctionTok{filterDate}\NormalTok{(start, end)}\SpecialCharTok{$}
      \FunctionTok{filter}\NormalTok{(ee}\SpecialCharTok{$}\NormalTok{Filter}\SpecialCharTok{$}\FunctionTok{calendarRange}\NormalTok{(6L, 8L, }\StringTok{"month"}\NormalTok{))}\SpecialCharTok{$}
      \FunctionTok{mean}\NormalTok{()}\SpecialCharTok{$}
      \FunctionTok{multiply}\NormalTok{(}\FloatTok{0.01}\NormalTok{)}\SpecialCharTok{$}
      \FunctionTok{set}\NormalTok{(}\StringTok{"system:time\_start"}\NormalTok{, start}\SpecialCharTok{$}\FunctionTok{millis}\NormalTok{()))}
\NormalTok{  \}))}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

En el próximo paso se calcula la temperatura media estival entre 1982 y 2010 como período de referencia para las anomalías.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{msst }\OtherTok{\textless{}{-}}\NormalTok{ collection\_era5}\SpecialCharTok{$}\FunctionTok{filterDate}\NormalTok{(}\StringTok{"1982{-}01{-}01"}\NormalTok{, }\StringTok{"2010{-}12{-}31"}\NormalTok{)}\SpecialCharTok{$}
  \FunctionTok{filter}\NormalTok{(ee}\SpecialCharTok{$}\NormalTok{Filter}\SpecialCharTok{$}\FunctionTok{calendarRange}\NormalTok{(6L, 8L, }\StringTok{"month"}\NormalTok{))}\SpecialCharTok{$}
  \FunctionTok{mean}\NormalTok{()}\SpecialCharTok{$}
  \FunctionTok{multiply}\NormalTok{(}\FloatTok{0.01}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Se aplica otra función personalizada sobre las medias estivales de todos los años en la que se resta la temperatura del periodo de referencia.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{anom }\OtherTok{\textless{}{-}}\NormalTok{ yearly}\SpecialCharTok{$}\FunctionTok{map}\NormalTok{(}\FunctionTok{ee\_utils\_pyfunc}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(im) \{}
  \FunctionTok{return}\NormalTok{(im}\SpecialCharTok{$}\FunctionTok{subtract}\NormalTok{(msst)}\SpecialCharTok{$}\FunctionTok{set}\NormalTok{(}
    \StringTok{"system:time\_start"}\NormalTok{,}
\NormalTok{    im}\SpecialCharTok{$}\FunctionTok{get}\NormalTok{(}\StringTok{"system:time\_start"}\NormalTok{)}
\NormalTok{  ))}
\NormalTok{\}))}
\end{Highlighting}
\end{Shaded}

Es posible crear un mapa interactivo de un año concreto aplicando la función \texttt{Map.addLayer()}. En este paso es la primera vez que GEE calcula lo que se ha creado anteriormente.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# metadatos}
\FunctionTok{ee\_print}\NormalTok{(anom}\SpecialCharTok{$}\FunctionTok{first}\NormalTok{()) }\CommentTok{\# año 1982}

\CommentTok{\# mapa interactiva del año 1982}
\NormalTok{Map}\SpecialCharTok{$}\FunctionTok{setCenter}\NormalTok{(}\DecValTok{9}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{5}\NormalTok{) }\CommentTok{\# centrar mapa en el mediterráneo con nivel de zoom 5}

\CommentTok{\# crear mapa con leyenda}
\NormalTok{Map}\SpecialCharTok{$}\FunctionTok{addLayer}\NormalTok{(}
  \AttributeTok{eeObject =}\NormalTok{ anom}\SpecialCharTok{$}\FunctionTok{first}\NormalTok{(),}
  \AttributeTok{visParams =} \FunctionTok{list}\NormalTok{(}
    \AttributeTok{palette =} \FunctionTok{rev}\NormalTok{(RColorBrewer}\SpecialCharTok{::}\FunctionTok{brewer.pal}\NormalTok{(}\DecValTok{11}\NormalTok{, }\StringTok{"RdBu"}\NormalTok{)),}
    \AttributeTok{min =} \SpecialCharTok{{-}}\DecValTok{3}\NormalTok{,}
    \AttributeTok{max =} \DecValTok{3}
\NormalTok{  ),}
  \AttributeTok{name =} \StringTok{"MED\_SST"}
\NormalTok{) }\SpecialCharTok{+}
\NormalTok{  Map}\SpecialCharTok{$}\FunctionTok{addLegend}\NormalTok{(}
    \FunctionTok{list}\NormalTok{(}
      \AttributeTok{min =} \SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\AttributeTok{max =} \DecValTok{3}\NormalTok{,}
      \AttributeTok{palette =} \FunctionTok{rev}\NormalTok{(RColorBrewer}\SpecialCharTok{::}\FunctionTok{brewer.pal}\NormalTok{(}\DecValTok{11}\NormalTok{, }\StringTok{"RdBu"}\NormalTok{))}
\NormalTok{    ),}
    \AttributeTok{name =} \StringTok{"SST Anomaly"}\NormalTok{,}
    \AttributeTok{position =} \StringTok{"bottomright"}\NormalTok{,}
    \AttributeTok{bins =} \DecValTok{4}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/interactivo_mapa_geopro} 

}

\caption{Mapa interactivo del mar Mediterraneo (1982)}\label{fig:interactive-map-plot}
\end{figure}

Hasta este momento no se ha enviado una tarea que debe realizar GEE. Lo que falta por hacer es exportar las anomalías de cada año en formato geotiff. En el bucle \emph{for} se selecciona cada año y se reduce una única capa. La función \texttt{ee\_image\_to\_drive()} exporta los datos a \emph{Google Drive}. El argumento \emph{escale} indica con qué resolución se exporta. Aunque la resolución de los datos originales es de 27 km, se fija una de 2 km lo que implica una interpolación en la exportación por razones de estética en la visualización.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1982}\SpecialCharTok{:}\DecValTok{2022}\NormalTok{) \{}
\NormalTok{  select\_data }\OtherTok{\textless{}{-}}\NormalTok{ anom}\SpecialCharTok{$}\FunctionTok{filter}\NormalTok{(ee}\SpecialCharTok{$}\NormalTok{Filter}\SpecialCharTok{$}\FunctionTok{calendarRange}\NormalTok{(i, i, }\StringTok{"year"}\NormalTok{))}\SpecialCharTok{$}
    \FunctionTok{reduce}\NormalTok{(ee}\SpecialCharTok{$}\NormalTok{Reducer}\SpecialCharTok{$}\FunctionTok{mean}\NormalTok{())}

\NormalTok{  text }\OtherTok{\textless{}{-}} \FunctionTok{paste}\NormalTok{(}\StringTok{"ssd\_anom\_yr"}\NormalTok{, i)}

\NormalTok{  task\_img }\OtherTok{\textless{}{-}} \FunctionTok{ee\_image\_to\_drive}\NormalTok{(}
    \AttributeTok{image =}\NormalTok{ select\_data,}
    \AttributeTok{description =}\NormalTok{ text,}
    \AttributeTok{fileFormat =} \StringTok{"GEO\_TIFF"}\NormalTok{,}
    \AttributeTok{folder =} \StringTok{"sst\_med"}\NormalTok{,}
    \AttributeTok{region =}\NormalTok{ geom,}
    \AttributeTok{scale =} \DecValTok{2000}\NormalTok{,}
    \AttributeTok{fileNamePrefix =}\NormalTok{ text,}
    \AttributeTok{maxPixels =} \FloatTok{10e9}
\NormalTok{  )}

\NormalTok{  task\_img}\SpecialCharTok{$}\FunctionTok{start}\NormalTok{()}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Las siguientes funciones permiten conocer el estado de las tareas.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ee\_monitoring}\NormalTok{(}\AttributeTok{eeTaskList =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{ee\_check\_task\_status}\NormalTok{(}\AttributeTok{quiet =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{part-casos-de-estudio-en-ciencia-de-datos}{%
\part{Casos de estudio en ciencia de datos}\label{part-casos-de-estudio-en-ciencia-de-datos}}

\hypertarget{cap-crimen}{%
\chapter{Análisis de una red criminal}\label{cap-crimen}}

\emph{F. Liberatore, L. Quijano-}Sánchez\emph{, M. Camacho-Collados}

Se plantea la idea de realizar un \textbf{análisis de una red de crimen organizado}.

Para ello, se estudia la red derivada de un \emph{dataset} real, relativo a la operación \emph{Oversize}. Para ello se usa la librería \texttt{igraph}.

\hypertarget{el-dataset-oversize}{%
\section{\texorpdfstring{El \emph{dataset Oversize}}{El dataset Oversize}}\label{el-dataset-oversize}}

Los datos que se van a analizar se han obtenido de la operación \emph{Oversize} \citep{berlusconi2016} \citep{oversize1} \citep{oversize2}, un caso Italiano contra un grupo mafioso. La investigación duró del 2000 al 2006, y se enfocó en mas de 50 sospechosos involucrados en trafico internacional de drogas, homicidios y robos. El juicio empezó en el 2007 y duró hasta el 2009, cuando se dictó la sentencia y los principales sospechosos fueron condenados con penas de 5 a 22 años de cárcel. La mayoría de los sospechosos eran afiliados con la \emph{'Ndrangheta}, una mafia de Calabria (una región del sur de Italia) con ramificaciones en otras regiones y en el extranjero.

En particular, se va a estudiar la red obtenida de las escuchas telefónicas. Los datos consideran todas las conversaciones telefónicas transcritas por la policía y consideradas relevantes. En esta red, los nodos representan sospechosos (los datos son anónimos y los nombres asignados en la red se han generado de forma aleatoria). Las aristas conectan los sospechosos que han tenido al menos una conversación telefónica relevante al caso durante la investigación.

\hypertarget{creaciuxf3n-del-grafo}{%
\section{Creación del grafo}\label{creaciuxf3n-del-grafo}}

El dataset \emph{Oversize\_nodes} contiene el listado de nodos con sus propiedades, en este caso el nombre (ficticio) del sospechoso. \emph{Oversize\_edges} contiene las aristas del grafo representadas como parejas de nodos, identificados por su ID. A partir de estos datasets la librería \emph{igraph} permite crear un grafo, tal y como se ilustra a continuación.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"igraph"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"CDR"}\NormalTok{)}
\FunctionTok{data}\NormalTok{(oversize\_edges, oversize\_nodes)}
\NormalTok{net }\OtherTok{\textless{}{-}} \FunctionTok{graph\_from\_data\_frame}\NormalTok{(}\AttributeTok{d =}\NormalTok{ oversize\_edges, }\AttributeTok{vertices =}\NormalTok{ oversize\_nodes, }\AttributeTok{directed =}\NormalTok{ F)}
\NormalTok{net}
\CommentTok{\#\textgreater{} IGRAPH eaeb052 UN{-}{-} 182 247 {-}{-} }
\CommentTok{\#\textgreater{} + attr: name (v/c)}
\CommentTok{\#\textgreater{} + edges from eaeb052 (vertex names):}
\CommentTok{\#\textgreater{}  [1] Casto Ben          {-}{-}Gustavo Mango          }
\CommentTok{\#\textgreater{}  [2] Casto Ben          {-}{-}Metrofane Abbatiello   }
\CommentTok{\#\textgreater{}  [3] Uranio Natoli      {-}{-}Fidenziano Marcellino  }
\CommentTok{\#\textgreater{}  [4] Lancilotto Di Biasi{-}{-}Romolo Gemignani       }
\CommentTok{\#\textgreater{}  [5] Lancilotto Di Biasi{-}{-}Fidenziano Marcellino  }
\CommentTok{\#\textgreater{}  [6] Senesio Rabito     {-}{-}Pacifico Caliri        }
\CommentTok{\#\textgreater{}  [7] Senesio Rabito     {-}{-}Michelangelo Piccininni}
\CommentTok{\#\textgreater{}  [8] Romolo Gemignani   {-}{-}Alighiero Mazzarella   }
\CommentTok{\#\textgreater{} + ... omitted several edges}
\end{Highlighting}
\end{Shaded}

La vista previa del grafo indica lo siguiente:

\begin{itemize}
\item
  El grafo es no dirigido (\emph{UN}) y está compuesto por 182 nodos y 247 aristas.
\item
  El único atributo es el nombre de los nodos (\emph{attr: name (v/c)}).
\item
  Finalmente, se proporciona una previsualización de un subconjunto de aristas, indicando para cada una los dos nodos conectados (ej. \emph{Casto Ben -\/-Gustavo Mango}).
\end{itemize}

\hypertarget{visualizaciuxf3n-del-grafo}{%
\section{Visualización del grafo}\label{visualizaciuxf3n-del-grafo}}

Para hacernos una idea de que aspecto tiene nuestro grafo, se procede a realizar una visualización, usando el comando \emph{plot} de R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(net, }\AttributeTok{asp =} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/unnamed-chunk-283-1} \end{center}

Como se puede apreciar, el resultado no es muy claro. Todos los nodos tienen el mismo tamaño y se solapan entre ellos. Además se muestran los nombres de todos los actores dentro de la red, lo cual dificulta ulteriormente su interpretación.

Se puede mejorar esta presentación usando unos parámetros de \emph{plot}, específicos de \emph{igraph}. En particular:

\begin{itemize}
\item
  \emph{vertex.size}: determina el tamaño de los nodos.
\item
  \emph{vertex.label}: define el texto asociado a cada nodo. Por defecto se asume que es su nombre. En el ejemplo de abajo, excluimos los nombres de la visualización.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(net, }\AttributeTok{vertex.size =} \DecValTok{2}\NormalTok{, }\AttributeTok{vertex.label =} \FunctionTok{c}\NormalTok{(}\StringTok{""}\NormalTok{), }\AttributeTok{asp =} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/unnamed-chunk-284-1} \end{center}

Se puede ver como el grafo superior permite una mejor valoración de la distribución de los actores dentro de la red. Por ejemplo, hay dos grupos pequeños (de cuatro y dos actores) completamente desconectados de la red principal.

\hypertarget{importancia-de-los-actores}{%
\section{Importancia de los actores}\label{importancia-de-los-actores}}

Las medidas de centralidad permiten asignar un valor a cada actor que establece su importancia relativa a los demás. Existen diversas medidas, cada una con sus características y finalidad. En este ejemplo se van a usar las siguientes:

\begin{itemize}
\item
  \textbf{Grado}: número de aristas incidentes al nodo. Cuanto más alto este valor, más vecinos tendrá el nodo.
\item
  \textbf{Intermediación}: cuantifica el número de veces que un nodo se encuentra en el camino más corto entre otros actores. Cuanto más alto este valor, más información pasará por el nodo.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dgr }\OtherTok{\textless{}{-}}\NormalTok{ igraph}\SpecialCharTok{::}\FunctionTok{degree}\NormalTok{(net) }\CommentTok{\# Centralidad de grado}
\NormalTok{btwn }\OtherTok{\textless{}{-}}\NormalTok{ igraph}\SpecialCharTok{::}\FunctionTok{betweenness}\NormalTok{(net) }\CommentTok{\# Centralidad de intermediacion}
\end{Highlighting}
\end{Shaded}

A continuación, se muestran los actores con los valores más altos en cada medidad de centralidad.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(}\FunctionTok{sort}\NormalTok{(dgr, }\AttributeTok{decreasing =}\NormalTok{ T))}
\CommentTok{\#\textgreater{}        Gustavo Mango      Pacifico Caliri Metrofane Abbatiello }
\CommentTok{\#\textgreater{}                   32                   31                   18 }
\CommentTok{\#\textgreater{}        Olindo Iacona         Arturo Gizzi      Guido Minervini }
\CommentTok{\#\textgreater{}                   17                   16                   16}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(}\FunctionTok{sort}\NormalTok{(btwn, }\AttributeTok{decreasing =}\NormalTok{ T))}
\CommentTok{\#\textgreater{}        Gustavo Mango            Bino Lana      Pacifico Caliri }
\CommentTok{\#\textgreater{}             4602.167             4292.902             4056.435 }
\CommentTok{\#\textgreater{}        Olindo Iacona Metrofane Abbatiello      Donato Di Santi }
\CommentTok{\#\textgreater{}             3397.907             3387.931             2978.427}
\end{Highlighting}
\end{Shaded}

Se pueden usar las medidas de centralidad para mejorar la visualización del grafo. Primero, se filtran todos los nodos que tengan grado menor que dos, ya que representan actores muy marginales en la red. Luego, se representa el tamaño de cada nodo en función de su valor de intermediación.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vertex\_filter }\OtherTok{\textless{}{-}}\NormalTok{ dgr }\SpecialCharTok{\textgreater{}} \DecValTok{1} \CommentTok{\# deteccion actores marginales}
\NormalTok{scaled\_btwn }\OtherTok{\textless{}{-}} \FloatTok{0.1} \SpecialCharTok{+} \FloatTok{4.9} \SpecialCharTok{*}\NormalTok{ btwn }\SpecialCharTok{/} \FunctionTok{max}\NormalTok{(btwn) }\CommentTok{\# Escalado del tamaño del nodo en funcion de la intermediacion}
\NormalTok{net2 }\OtherTok{\textless{}{-}}\NormalTok{ igraph}\SpecialCharTok{::}\FunctionTok{induced.subgraph}\NormalTok{(net, }\FunctionTok{which}\NormalTok{(vertex\_filter)) }\CommentTok{\# creacion subgrafo}
\FunctionTok{plot}\NormalTok{(net2, }\AttributeTok{vertex.size =}\NormalTok{ scaled\_btwn[vertex\_filter], }\AttributeTok{vertex.label =} \FunctionTok{c}\NormalTok{(}\StringTok{""}\NormalTok{), }\AttributeTok{rescale =}\NormalTok{ T, }\AttributeTok{asp =} \DecValTok{0}\NormalTok{) }\CommentTok{\# visualizacion subgrafo}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/unnamed-chunk-288-1} \end{center}

Gracias a la medida de centralidad se puede tener una mejor idea de cómo se configura la red respecto a sus actores más importantes.

\hypertarget{identificaciuxf3n-de-comunidades}{%
\section{Identificación de comunidades}\label{identificaciuxf3n-de-comunidades}}

Se procede a identificar las comunidades existentes en el grafo de la operación \emph{Overdrive}.

\emph{igraph} proporciona un gran variedad de algoritmos para la detección de comunidades en redes sociales. En el siguiente ejemplo, se usa el algoritmo Louvain \citep{blondel2008fast} que es el más popular.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{louvain\_partition }\OtherTok{\textless{}{-}}\NormalTok{ igraph}\SpecialCharTok{::}\FunctionTok{cluster\_louvain}\NormalTok{(net) }\CommentTok{\# Ejecucion del algoritmo Louvain}
\NormalTok{net}\SpecialCharTok{$}\NormalTok{community }\OtherTok{\textless{}{-}}\NormalTok{ louvain\_partition}\SpecialCharTok{$}\NormalTok{membership }\CommentTok{\# Asignacion de las comunidades al grafo}
\FunctionTok{unique}\NormalTok{(net}\SpecialCharTok{$}\NormalTok{community) }\CommentTok{\# Visualizacion de las comunidades encontradas}
\CommentTok{\#\textgreater{} [1] 1 2 3 4 5 6 7 8 9}
\end{Highlighting}
\end{Shaded}

Al algoritmo identifica distintas comunidades, cada una con su número asignado.

\hypertarget{visualizaciuxf3n-de-comunidades}{%
\section{Visualización de comunidades}\label{visualizaciuxf3n-de-comunidades}}

Se procede a visualizar las comunidades detectadas en el subgrafo, representando cada una de ellas en un color distinto. Además, para mejorar la calidad de la información representada, se resalta la importancia de cada actor representando los nodos asociados y sus nombres en tamaños proporcionales a su centralidad en toda la red.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{V}\NormalTok{(net2)}\SpecialCharTok{$}\NormalTok{size }\OtherTok{\textless{}{-}}\NormalTok{ scaled\_btwn[vertex\_filter] }\CommentTok{\# Tamaño del nodo en funcion de su centralidad}
\FunctionTok{V}\NormalTok{(net2)}\SpecialCharTok{$}\NormalTok{frame.color }\OtherTok{\textless{}{-}} \StringTok{"grey"}
\FunctionTok{V}\NormalTok{(net2)}\SpecialCharTok{$}\NormalTok{color }\OtherTok{\textless{}{-}}\NormalTok{ net}\SpecialCharTok{$}\NormalTok{community[vertex\_filter] }\CommentTok{\# Color del nodo en funcion de su comunidad}
\FunctionTok{V}\NormalTok{(net2)}\SpecialCharTok{$}\NormalTok{label }\OtherTok{\textless{}{-}} \FunctionTok{V}\NormalTok{(net2)}\SpecialCharTok{$}\NormalTok{name}
\FunctionTok{V}\NormalTok{(net2)}\SpecialCharTok{$}\NormalTok{label.cex }\OtherTok{\textless{}{-}}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ scaled\_btwn[vertex\_filter]) }\SpecialCharTok{/} \DecValTok{6} \CommentTok{\# Escalado del nombre en funcion de su centralidad}
\FunctionTok{V}\NormalTok{(net2)}\SpecialCharTok{$}\NormalTok{label.color }\OtherTok{\textless{}{-}} \StringTok{"black"}

\CommentTok{\# Definicion del color de las aristas en funcion de la comunidad de origen}
\NormalTok{edge.start }\OtherTok{\textless{}{-}} \FunctionTok{ends}\NormalTok{(net2, }\AttributeTok{es =} \FunctionTok{E}\NormalTok{(net2), }\AttributeTok{names =}\NormalTok{ F)[, }\DecValTok{1}\NormalTok{]}
\FunctionTok{E}\NormalTok{(net2)}\SpecialCharTok{$}\NormalTok{color }\OtherTok{\textless{}{-}} \FunctionTok{V}\NormalTok{(net2)}\SpecialCharTok{$}\NormalTok{color[edge.start]}

\FunctionTok{plot}\NormalTok{(net2, }\AttributeTok{asp =} \DecValTok{0}\NormalTok{) }\CommentTok{\# Los resultados puede ser distintos con cada ejecucion}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/unnamed-chunk-290-1} \end{center}

Se puede mejorar aun más el aspecto del grafo. Para ello, se va a experimentar con una disposición diferente de los nodos. En este ejemplo, se usa el algoritmo Fruchterman-Reingold \citep{fruchterman1991graph}. Además, se aplica un efecto de curvatura a las aristas asignando un valor positivo al parámetro \emph{edge.curved}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l1 }\OtherTok{\textless{}{-}}\NormalTok{ igraph}\SpecialCharTok{::}\FunctionTok{layout\_with\_fr}\NormalTok{(net2) }\CommentTok{\# algoritmo Fruchterman{-}Reingold}
\FunctionTok{plot}\NormalTok{(net2, }\AttributeTok{layout =}\NormalTok{ l1, }\AttributeTok{asp =} \DecValTok{0}\NormalTok{, }\AttributeTok{edge.curved =} \FloatTok{0.5}\NormalTok{) }\CommentTok{\# Los resultados puede ser distintos con cada ejecucion}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=1.2\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/unnamed-chunk-291-1} \end{center}

Finalmente, se puede exportar el grafo como PDF usando el comando \emph{pdf} de R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pdf}\NormalTok{(}\StringTok{"grafo\_final.pdf"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(net2, }\AttributeTok{layout =}\NormalTok{ l1, }\AttributeTok{asp =} \DecValTok{0}\NormalTok{, }\AttributeTok{edge.curved =} \FloatTok{0.5}\NormalTok{) }\CommentTok{\# Los resultados puede ser distintos con cada ejecucion}
\FunctionTok{dev.off}\NormalTok{()}
\CommentTok{\#\textgreater{} agg\_png }
\CommentTok{\#\textgreater{}       2}
\end{Highlighting}
\end{Shaded}

\hypertarget{cap-publidiad}{%
\chapter{Optimización de inversiones publicitarias}\label{cap-publidiad}}

\emph{Carlos Real Ugena}

\hypertarget{metodologuxedas-para-optimizar-las-inversiones-publicitarias}{%
\section{Metodologías para optimizar las inversiones publicitarias}\label{metodologuxedas-para-optimizar-las-inversiones-publicitarias}}

Uno de los principales retos a los que se enfrentan los departamentos de marketing de cualquier compañía es cuantificar el impacto de la publicidad en el negocio. Está medición será clave en la optimización de las inversiones que destinan a cada medio publicitario, existiendo múltiples métodos para medir el ROI (Return On Investment), es decir, el retorno que se obtiene por cada euro invertido en publicidad. Antes de revisar un ejemplo práctico, es necesario entender bien las características que tiene cada uno de ellos. Estas metodologías se pueden clasificar en cuatro grandes grupos como muestra una investigación llevada a cabo por \href{https://www.gartner.com/en/newsroom/press-releases/2020-03-04-gartner-identifies-four-methods-for-measuring-marketi}{Gartner}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Marketing Mix Modeling (MMM)}: son modelos de series temporales que sirven para estimar la contribución del marketing y otros drivers en ventas desde un punto de vista estratégico.
\item
  \textbf{Multitouch Attribution (MTA)}: son modelos de atribución que valoran cada punto de contacto (Touchpoint) del recorrido del cliente (Customer Journey) asignando a cada uno un peso en la conversión. Son modelos tácticos que, normalmente, se centran en el canal online.
\item
  \textbf{Holdout Testing o Experiments(EXP)}: son modelos causales que evalúan el impacto de una campaña publicitaria a partir de una muestra de control y test.
\item
  \textbf{Unified Measurement Approaches (UMA)}: es la combinación de los modelos anteriores (MMM+MTA+EXP) con el objetivo de tener una visión unificada de los resultados.
\end{enumerate}

En la comparativa (figura 14.1) que se muestra a continuación se resume el objetivo de cada uno de los modelos, así como las preguntas que permiten responder:

\begin{figure}
\centering
\includegraphics{img/Comparativa_modelos.PNG}
\caption{Comparativa de metodologías de medición}
\end{figure}

Por otra parte, en los últimos años se han producido una serie de cambios en el entorno de la industria del marketing digital enfocados a garantizar el más estricto control de la privacidad de los usuarios. Desde el lanzamiento de la nueva ley de protección de datos ``GDPR'' en 2018, hasta la reciente confirmación por parte de Google de la prohibición de uso de cookies de tercera parte en ``Chrome'', la posibilidad de acceder a datos de identificación personal para la medición y optimización del impacto de las campañas de publicidad digital está cada vez más limitada.

La situación anterior está provocando que el primer tipo de enfoque, Marketing Mix Modeling (MMM), esté siendo el gran beneficiado, dado que es una técnica que no depende del acceso a datos a nivel de individuo. El ejemplo más claro sobre el protagonismo que está alcanzando el MMM es que grandes compañías como Meta, Google y Uber están desarrollando soluciones open-source basadas en técnicas de Machine Learning que integran grandes avances y mejoras sobre las metodologías tradicionales. Entre las metodologías que han desarrollado, hay claras diferencias en las bases teóricas sobre las que se rigen, así como diferencias en tiempos de computación, lenguaje en el que se ha desarrollado o capacidades funcionales. Se detalla a continuación las principales características de cada una de ellas:

• \href{https://facebookexperimental.github.io/Robyn/}{\textbf{Robyn}}: desarrollado por Meta, está pensado para datasets con gran cantidad de variables independientes dado que trabaja con regresiones ridge, las cuales están pensadas para lidiar con problemas de multicolinealidad, muy presentes en este tipo de análisis. Cabe destacar que, entre las iteraciones que realiza, ofrece una serie de outputs visuales avanzados que permiten al usuario seleccionar el que mejor se adapta al contexto de negocio y necesidad. Requiere tiempos de cómputo alto, pudiendo llegar hasta las tres horas y permite hacer optimizaciones de presupuesto.

• \href{https://github.com/google/lightweight_mmm}{\textbf{Lightweight}}: desarrollado por Google, sus fundamentos teóricos se basan en modelos bayesianos. La particularidad de esta solución es la posibilidad de incluir datos geográficos para su posterior segregación. Lightweight también considera distintos tipos de ad-stock, así como la tendencia y estacionalidad de la serie a explicar. El uso de esta herramienta es más sencillo, aunque los resultados son expuestos en un notebook y no poseen outputs más elaborados como el resto de soluciones. Por último, permite también realizar optimizaciones de presupuesto.

• \href{https://www.uber.com/en-ES/blog/orbit/}{\textbf{Orbit}}: desarrollado por Uber, se basa en modelos bayesianos. Permite al usuario medir los retornos a lo largo del tiempo, lo cual hace que sea un modelo adecuado para compañías con grandes picos de ventas. Incluye análisis de estacionalidad mediante la descomposición de la misma en series de Fourier. Es el modelo más complejo de desarrollar, sin embargo, el tiempo de ejecución es bajo. Cabe destacar que es la librería más estable y por lo tanto se podría utilizar para realizar reportes con más frecuencia. No incluye la posibilidad de optimizar presupuestos.

\hypertarget{robyn-como-alternativa-open-source-en-r}{%
\section{Robyn como alternativa open-source en R}\label{robyn-como-alternativa-open-source-en-r}}

Robyn está ganando en popularidad debido a las mejoras continuas que están aplicando desde Meta, así como la adaptabilidad de la solución a la realidad del anunciante. Además, existe un paquete en R validado y subido al CRAN que lo sitúa como una gran alternativa open-source en R para iniciarse en el campo de la medición y optimización del retorno de las inversiones publicitarias. Se basa en regresiones ridge y la fórmula empleada es la siguiente:

\begin{equation}
y_n= \beta_0 + SCurve(x,j) + \beta_{hol} hol_t + \beta_{sea} sea_t + \beta_{trend} trend_t + ... + \beta_{ETC} ETC_t + \epsilon
\end{equation}

donde:

\begin{equation}
\begin{aligned}
\beta_0 &= \text{Intercepto} \\
S Curve(x,j) &= \beta_j \times  \frac{x_{decay_{ t,j}}^\alpha}{x_{decay_{ t,j}}^\alpha + \gamma^\alpha} \, \text{(curva no lineal)} \\
X_{decay_{ t,j}} &= X_{t,j} + \theta_j X_{decay_{ t,j-1}} \text{(ad-stock)} \\
y_t &= \text{ventas en el instante} \,t \\
t &= \text{instante} \,t\, \text{de las variables (semanas)} \\
j &= \text{indice del medio (por ejemplo,TV o Display)}  \\
\beta,\alpha,\gamma,\theta &= \text{regresores de cada medio}\,j \\
\theta &: \text{implementada en la SCurve, donde} \,\theta_{tran} = cuantil(X_{decay j, \gamma}) \\
hol &=\, \text{festivos},\,sea\,=\,\text{estacionalidad},\,trend\,=\,\text{tendencia} \\
\beta_{ETC} ETC_t &= \text{resto variables independientes (precio, promociones, etc)} \\
\epsilon &= \text{termino de error}
\end{aligned}
\end{equation}

A continuación, se aplicará Robyn sobre un ejemplo con información simulada del sector hotelero. En \href{https://github.com/facebookexperimental/Robyn}{Github} se cuenta con toda la información sobre cómo aplicar esta metodología.

Cabe destacar que en el ejemplo se ha utilizado la versión 3.6.3 que está disponible en el CRAN o se puede descargar desde \href{https://github.com/facebookexperimental/Robyn/releases/tag/v3.6.3}{Github}. El objetivo del ejercicio será revisar el camino más corto que se puede seguir hasta llegar a los principales outputs y a la interpretación de los mismos, sin embargo, para conocer en detalle la metodología se recomienda seguir profundizando a través de la realización de pruebas adicionales más complejas.

Lo primero, cargue el paquete Robyn, chequee que está trabajando con la versión 3.6.4 y fuerce el uso de multicore al utilizar Rstudio:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(Robyn)}
\FunctionTok{packageVersion}\NormalTok{(}\StringTok{"Robyn"}\NormalTok{)}
\FunctionTok{Sys.setenv}\NormalTok{(}\AttributeTok{R\_FUTURE\_FORK\_ENABLE =} \StringTok{"true"}\NormalTok{)}
\FunctionTok{options}\NormalTok{(}\AttributeTok{future.fork.enable =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A continuación, cargue una librería de Python llamada \href{https://pypi.org/project/nevergrad/}{Nevergrad} que utilizará en el proceso de estimación de hiperparámetros, invocándola desde R. Hay varias opciones para llevar a cabo esta instalación, una de ellas es instalando primero el paquete Reticulate e instalarla vía PIP:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"reticulate"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(reticulate)}
\FunctionTok{virtualenv\_create}\NormalTok{(}\StringTok{"r{-}reticulate"}\NormalTok{)}
\FunctionTok{use\_virtualenv}\NormalTok{(}\StringTok{"r{-}reticulate"}\NormalTok{, }\AttributeTok{required =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{py\_install}\NormalTok{(}\StringTok{"nevergrad"}\NormalTok{, }\AttributeTok{pip =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{py\_config}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

En el caso de que encuentre alguna dificultad, puede consultar en este \href{https://github.com/facebookexperimental/Robyn/issues/189}{enlace}, ya que existen distintas alternativas para la instalación de Nevergrad.

Ahora cargue la tabla con la que medirá el impacto de la publicidad:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# hotel\_tablonsemanal \textless{}{-} read\_csv("data/212053\_publicidad\_dataset\_hotel.csv")}
\FunctionTok{library}\NormalTok{(}\StringTok{"CDR"}\NormalTok{)}
\FunctionTok{data}\NormalTok{(}\StringTok{"hotel\_tablonsemanal"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(hotel\_tablonsemanal)}
\end{Highlighting}
\end{Shaded}

Se detalla a continuación lo que contiene cada variable:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{semana}: Lunes de la semana de referencia.
\item
  \emph{reservas}: Número de reservas que ha conseguido la cadena hotelera en cada una de las semanas bajo análisis.
\item
  \emph{turismo}: Número de pernoctaciones.
\item
  \emph{covid\_mov}: Movilidad desde el comienzo de la pandemia.
\item
  \emph{notoriedad}: Conocimiento espontáneo de la marca a lo largo del tiempo.
\item
  \emph{temperatura}: Temperatura media.
\item
  \emph{tv\_grps20} y \emph{tv\_inv}: Métrica de impactos (GRPs) e inversión realizada en TV.
\item
  \emph{resto\_off\_inv}: Resto de inversiones offline realizadas.
\item
  \emph{paidsearch\_imp} y \emph{paidsearch\_inv}: Métrica de impactos (impresiones) e inversión realizada en Paid Search.
\item
  \emph{display\_imp} y \emph{display\_inv}: Métrica de impactos (impresiones) e inversión realizada en Display.
\item
  \emph{onlinevideo\_imp} y \emph{onlinevideo\_inv}: Métrica de impactos (impresiones) e inversión realizada en Online Video
\item
  \emph{competidores}: Inversión realizada por la competencia.
\item
  \emph{eventos}: Recoge eventos que tienen impacto en la serie de reservas. En nuestro caso consideramos el Black Friday.
\end{enumerate}

En resumen, en el ejercicio estimará el impacto de cada una de las variables sobre las reservas, poniendo foco especial en las variables de medios: TV, Resto medios offline, Paid Search, Display y Online Video. Se quiere medir el efecto de cada una de ellas para posteriormente poder optimizar las inversiones.

Cargue los festivos de una tabla auxiliar:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"CDR"}\NormalTok{)}
\FunctionTok{data}\NormalTok{(}\StringTok{"festivos"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(festivos)}
\end{Highlighting}
\end{Shaded}

Ahora fije dónde se guardarán los resultados:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{robyn\_object }\OtherTok{\textless{}{-}} \StringTok{"data/MyRobyn.RDS"}
\NormalTok{ruta\_outputs }\OtherTok{\textless{}{-}} \StringTok{"data"}
\end{Highlighting}
\end{Shaded}

Y trabaje en la configuración inicial del modelo:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hotel\_tablonsemanal}\SpecialCharTok{$}\NormalTok{eventos }\OtherTok{\textless{}{-}}\NormalTok{ hotel\_tablonsemanal}\SpecialCharTok{$}\NormalTok{eventos }\SpecialCharTok{|\textgreater{}} \FunctionTok{replace\_na}\NormalTok{(}\StringTok{"na"}\NormalTok{)}
\NormalTok{InputCollect }\OtherTok{\textless{}{-}} \FunctionTok{robyn\_inputs}\NormalTok{(}
  \AttributeTok{dt\_input =}\NormalTok{ hotel\_tablonsemanal,}
  \AttributeTok{dt\_holidays =}\NormalTok{ festivos,}
  \AttributeTok{date\_var =} \StringTok{"semana"} \CommentTok{\# tiene que tener este formato "2020{-}01{-}01"}
\NormalTok{  , }\AttributeTok{dep\_var =} \StringTok{"reservas"} \CommentTok{\# sólo una variable dependiente}
\NormalTok{  , }\AttributeTok{dep\_var\_type =} \StringTok{"conversion"} \CommentTok{\# "revenue" o "conversion". En nuestro caso son reservas.}
\NormalTok{  , }\AttributeTok{prophet\_vars =} \FunctionTok{c}\NormalTok{(}\StringTok{"trend"}\NormalTok{, }\StringTok{"season"}\NormalTok{, }\StringTok{"holiday"}\NormalTok{) }\CommentTok{\# "trend=tendencia","season=estacionalidad", "weekday=dia de la semana" \& "holiday=festivos"}
\NormalTok{  , }\AttributeTok{prophet\_signs =} \FunctionTok{c}\NormalTok{(}\StringTok{"default"}\NormalTok{, }\StringTok{"default"}\NormalTok{, }\StringTok{"default"}\NormalTok{),}
  \AttributeTok{prophet\_country =} \StringTok{"ES"} \CommentTok{\# seleccione un pais. España en nuestro caso}
\NormalTok{  , }\AttributeTok{context\_vars =} \FunctionTok{c}\NormalTok{(}\StringTok{"eventos"}\NormalTok{, }\StringTok{"turismo"}\NormalTok{, }\StringTok{"covid\_mov"}\NormalTok{, }\StringTok{"notoriedad"}\NormalTok{, }\StringTok{"temperatura"}\NormalTok{, }\StringTok{"competidores"}\NormalTok{) }\CommentTok{\# variables de contexto que no sean medios}
\NormalTok{  , }\AttributeTok{context\_signs =} \FunctionTok{c}\NormalTok{(}\StringTok{"default"}\NormalTok{, }\StringTok{"positive"}\NormalTok{, }\StringTok{"negative"}\NormalTok{, }\StringTok{"positive"}\NormalTok{, }\StringTok{"default"}\NormalTok{, }\StringTok{"negative"}\NormalTok{) }\CommentTok{\# signos fijados a priori (por ejemplo, el turismo debe tener un signo positivo puesto que estamos analizando reservas de hotel)}
\NormalTok{  , }\AttributeTok{paid\_media\_spends =} \FunctionTok{c}\NormalTok{(}\StringTok{"display\_inv"}\NormalTok{, }\StringTok{"onlinevideo\_inv"}\NormalTok{, }\StringTok{"paidsearch\_inv"}\NormalTok{, }\StringTok{"resto\_off\_inv"}\NormalTok{, }\StringTok{"tv\_inv"}\NormalTok{) }\CommentTok{\# variables de inversión}
\NormalTok{  , }\AttributeTok{paid\_media\_signs =} \FunctionTok{c}\NormalTok{(}\StringTok{"positive"}\NormalTok{, }\StringTok{"positive"}\NormalTok{, }\StringTok{"positive"}\NormalTok{, }\StringTok{"positive"}\NormalTok{, }\StringTok{"positive"}\NormalTok{),}
  \AttributeTok{paid\_media\_vars =} \FunctionTok{c}\NormalTok{(}\StringTok{"display\_imp"}\NormalTok{, }\StringTok{"onlinevideo\_imp"}\NormalTok{, }\StringTok{"paidsearch\_imp"}\NormalTok{, }\StringTok{"resto\_off\_inv"}\NormalTok{, }\StringTok{"tv\_grps20"}\NormalTok{) }\CommentTok{\# variables de impacto si están disponibles. Si no están disponibles utilice el coste como en el caso de Resto\_off\_inversion}
\NormalTok{  , }\AttributeTok{factor\_vars =} \FunctionTok{c}\NormalTok{(}\StringTok{"eventos"}\NormalTok{) }\CommentTok{\# especifique variables que son factores. En nuestro caso, sólo la variable de Eventos}
\NormalTok{  , }\AttributeTok{window\_start =} \StringTok{"2018{-}10{-}01"} \CommentTok{\# fecha de inicio del modelo. En nuestro caso, octubre 2010 porque previamente no se tienen datos de reservas}
\NormalTok{  , }\AttributeTok{window\_end =} \StringTok{"2021{-}09{-}27"} \CommentTok{\# fecha fin del modelo}
\NormalTok{  , }\AttributeTok{adstock =} \StringTok{"geometric"} \CommentTok{\# tipo de ad{-}stock. Seleccione el ad{-}stock geométrico para reducir tiempos de cómputo}
\NormalTok{)}
\FunctionTok{print}\NormalTok{(InputCollect)}
\end{Highlighting}
\end{Shaded}

Ahora, obtén los nombres de los hiperparámetros que hay que ajustar:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hyper\_names}\NormalTok{(}\AttributeTok{adstock =}\NormalTok{ InputCollect}\SpecialCharTok{$}\NormalTok{adstock, }\AttributeTok{all\_media =}\NormalTok{ InputCollect}\SpecialCharTok{$}\NormalTok{all\_media)}
\end{Highlighting}
\end{Shaded}

A continuación, defina los rangos en los que se moverán los hiperparámetros. Se utilizarán los mismos límites para \(\alpha\) y \(\gamma\) que controlan la forma de la curva entre retorno decreciente y forma de S, y el punto de inflexión respectivamente. En el caso de \(\theta\) servirá para definir el ad-stock (recuerdo publicitario) que podríamos limitar teniendo en cuanta los intervalos recomendados por Meta: TV (entre 0,3 y 0,8), Resto medios off (entre 0,1 y 0,4) y Digital (entre 0 y 0,3). En nuestro caso, se dejará \(\theta\) libre entre 0 y 0,99 para todos los medios:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hyperparameters }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \AttributeTok{display\_inv\_alphas =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.0001}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{display\_inv\_gammas =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.3}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{display\_inv\_thetas =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{),}
  \AttributeTok{onlinevideo\_inv\_alphas =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.0001}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{onlinevideo\_inv\_gammas =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.3}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{onlinevideo\_inv\_thetas =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{),}
  \AttributeTok{paidsearch\_inv\_alphas =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.0001}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{paidsearch\_inv\_gammas =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.3}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{paidsearch\_inv\_thetas =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{),}
  \AttributeTok{resto\_off\_inv\_alphas =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.0001}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{resto\_off\_inv\_gammas =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.3}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{resto\_off\_inv\_thetas =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{),}
  \AttributeTok{tv\_inv\_alphas =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.0001}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{tv\_inv\_gammas =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.3}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{tv\_inv\_thetas =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.99}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Ahora, añada los hiperparámetros al input que utilizaremos para ajustar los modelos:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{InputCollect }\OtherTok{\textless{}{-}} \FunctionTok{robyn\_inputs}\NormalTok{(}\AttributeTok{InputCollect =}\NormalTok{ InputCollect, }\AttributeTok{hyperparameters =}\NormalTok{ hyperparameters)}
\FunctionTok{print}\NormalTok{(InputCollect)}
\end{Highlighting}
\end{Shaded}

Y ejecute el modelo definiendo el número de iteraciones y trials. En nuestro caso empleará 2000 iteraciones y 10 trials:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{OutputModels }\OtherTok{\textless{}{-}} \FunctionTok{robyn\_run}\NormalTok{(}
  \AttributeTok{InputCollect =}\NormalTok{ InputCollect,}
  \AttributeTok{iterations =} \DecValTok{2000}\NormalTok{,}
  \AttributeTok{trials =} \DecValTok{10}\NormalTok{,}
  \AttributeTok{outputs =} \ConstantTok{FALSE} \CommentTok{\# desactive los outputs que se guardarán después}
\NormalTok{)}
\FunctionTok{print}\NormalTok{(OutputModels)}
\end{Highlighting}
\end{Shaded}

Guarde ahora los resultados de la modelización en la carpeta seleccionada:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{OutputCollect }\OtherTok{\textless{}{-}} \FunctionTok{robyn\_outputs}\NormalTok{(}
\NormalTok{  InputCollect, OutputModels,}
  \AttributeTok{pareto\_fronts =} \DecValTok{3}\NormalTok{,}
  \AttributeTok{csv\_out =} \StringTok{"all"}\NormalTok{,}
  \AttributeTok{clusters =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{plot\_pareto =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{plot\_folder =}\NormalTok{ ruta\_outputs }\CommentTok{\# ruta para exportar los resultados}
\NormalTok{)}
\FunctionTok{print}\NormalTok{(OutputCollect)}
\end{Highlighting}
\end{Shaded}

Para finalizar revisará los distintos modelos obtenidos y gráfico resumen que proporciona Robyn y ya tendrá listo un primer modelo que estima el impacto de cada uno de los medios publicitarios. Se profundizará en la interpretación de uno de los modelos obtenidos con buen ajuste. A través del onepager (figura 14.2), tendrá toda la información relativa al impacto de la publicidad, incluyendo métricas relativas a la bondad del ajuste como el R2 y el NRMSE en la parte superior:

\begin{figure}
\centering
\includegraphics{img/Output_Robyn_7_394_4.PNG}
\caption{One pager de resultados de Robyn}
\end{figure}

Los principales outputs a evaluar están resumidos en cada uno de los gráficos del Onepager:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Response Decomposition Waterfall by Predictor: representa el peso de cada una de las variables en el modelo. En el ejemplo, el peso de la publicidad es del 54,5\% (suma de todas las variables de medios).
\item
  Actual vs.~Predicted Response: muestra el ajuste del modelo. En nuestro caso, la línea del ajuste (azul) y la real (naranja) se aproximan bastante, lo que muestra que ha sido capaz de explicar la mayor parte de la variabilidad de la serie de reservas.
\item
  Share of Spend vs Share of Effect with total CPA: muestra la relación entre la cuota de inversión y la cuota de la contribución generada por dichas inversiones. En nuestro caso, puede observar que tanto Online Video y Display están obteniendo mayor aporte que lo esperado por la inversión, lo que hace que el CPA (valores en azul) sean menores para estos medios que para el resto. La TV se encuentra en el polo opuesto y marca el máximo CPA, es decir, es el medio menos eficiente a la hora de generar reservas.
\item
  Response Curves and Mean Spends by Channel: muestra la relación no lineal existente entre las inveresiones y las reservas. Es el input principal a la hora de optimizar las inversiones.
\item
  Geometric Adstock: Fixed Decay Rate Over Time: muestra el efecto recuerdo (ad-stock) de cada medio publicitario. Puede observar que los medios offline (TV y el resto) son los medios que son capaces de generar un recuerdo más alto. No olvide que en la configuración del modelo podría haber fijado el intervalo en el que se tendría que mover el ad-stock de cada medio.
\item
  Fitted vs.~Residual: muestra la dispersión entre los datos ajustados (eje x) y los residuos (eje y). Este gráfico debe mostrar que los puntos están aleatoriamente ubicados alrededor el eje horizontal.
\end{enumerate}

Y, ¿cómo se pueden optimizar las inversiones en medios empleando estos resultados? Podrá utilizar las curvas obtenidas (parámetros guardados en \emph{all\_hyperparameters.csv}) para simular distintos escenarios y seleccionar el que dé máximas reservas. La otra opción sería utilizar la documentación de \href{https://github.com/facebookexperimental/Robyn/blob/main/demo/demo.R}{código} incluida en el Step 5 para generar el reparto óptimo de inversión basado en un presupuesto fijado.

En este capítulo se han dado los primeros pasos en la medición y optimización del ROI. Le animamos a que siga profundizando dentro de este campo tan apasionante y complejo para basar las decisiones futuras de inversión en análisis desarrollados en R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sessionInfo}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{cap-periodismo}{%
\chapter{Análisis electoral: de Rstudio a su periódico}\label{cap-periodismo}}

\emph{Borja Andrino Turón}

El uso de R en el entorno profesional ha llegado también a los periódicos. Cada vez es más habitual encontrar en los medios analistas de datos que lo utilizan en su día a día. En EL PAÍS, muchos de los contenidos que se publican en la Unidad de Datos surgen de un notebook de RStudio. A continuación, se muestra un análisis sobre las últimas elecciones andaluzas, de RStudio a su periódico favorito.

\hypertarget{obtenciuxf3n-de-los-datos}{%
\section{Obtención de los datos}\label{obtenciuxf3n-de-los-datos}}

Los datos electorales no siempre son igual de accesibles. Los de las elecciones que dependen del Ministerio del Interior se publican en el portal \href{https://infoelectoral.interior.gob.es/opencms/es/inicio/}{Infoelectoral}. En el caso de las elecciones andaluzas, los resultados a nivel de mesa se han publicado en los portales de cada convocatoria, aunque pueden encontrarse entre los contenidos del libro para replicar estos análisis.

En primer lugar se compondrá un diccionario de municipios que se usará para filtrar y agrupar los resultados por provincia. Primero se escrapeará de la web del INE la relación de códigos de provincia con la librería rvest. Se lee el código html de la página y se buscan los elementos table con clase miTabla. A continuación, se usa la función html\_table para convertir las tres tablas en un objeto tibble. La información con los nombres de municipios y provincias se leerá en la web del INE.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{require}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{require}\NormalTok{(rvest)}
\FunctionTok{require}\NormalTok{(lubridate)}
\FunctionTok{require}\NormalTok{(sf)}
\FunctionTok{require}\NormalTok{(ggtext)}
\FunctionTok{require}\NormalTok{(rio)}
\FunctionTok{require}\NormalTok{(janitor)}
\FunctionTok{require}\NormalTok{(here)}

\FunctionTok{load}\NormalTok{(}\FunctionTok{here}\NormalTok{(}\StringTok{"data/data\_periodismo.RData"}\NormalTok{))}

\NormalTok{url\_provincias }\OtherTok{\textless{}{-}}
  \StringTok{"https://www.ine.es/daco/daco42/codmun/cod\_provincia.htm"}

\NormalTok{cod\_provincias }\OtherTok{\textless{}{-}}
  \FunctionTok{read\_html}\NormalTok{(url\_provincias) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{"table.miTabla"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{html\_table}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
\NormalTok{  purrr}\SpecialCharTok{::}\FunctionTok{map\_df}\NormalTok{(as\_tibble) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{codigo\_prov =} \DecValTok{1}\NormalTok{, }\AttributeTok{name\_prov =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{codigo\_prov =}\NormalTok{ stringr}\SpecialCharTok{::}\FunctionTok{str\_pad}\NormalTok{(codigo\_prov, }\AttributeTok{width =} \DecValTok{2}\NormalTok{, }\AttributeTok{pad =} \StringTok{"0"}\NormalTok{, }\AttributeTok{side =} \StringTok{"left"}\NormalTok{))}

\NormalTok{url\_municipios }\OtherTok{\textless{}{-}}
  \StringTok{"https://www.ine.es/daco/daco42/codmun/codmun20/20codmun.xlsx"}

\NormalTok{cod\_municipios }\OtherTok{\textless{}{-}}
  \FunctionTok{import}\NormalTok{(url\_municipios, }\AttributeTok{skip =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{clean\_names}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{transmute}\NormalTok{(}
    \AttributeTok{codigo\_prov =}\NormalTok{ cpro,}
    \AttributeTok{codigo\_mun =}\NormalTok{ stringr}\SpecialCharTok{::}\FunctionTok{str\_glue}\NormalTok{(}\StringTok{"\{codigo\_prov\}\{cmun\}"}\NormalTok{),}
    \AttributeTok{name\_mun =}\NormalTok{ nombre}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{left\_join}\NormalTok{(cod\_provincias) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(codigo\_mun, name\_mun, name\_prov)}
\end{Highlighting}
\end{Shaded}

Se añade la información sobre municipios al dataset de elecciones que tiene los datos de cada sección censal.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datos\_elecciones }\OtherTok{\textless{}{-}}
\NormalTok{  datos\_elecciones }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{left\_join}\NormalTok{(cod\_municipios) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(codigo\_secc, codigo\_mun, name\_mun, name\_prov, convocatoria, }\FunctionTok{everything}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\hypertarget{transformaciuxf3n-y-primeros-gruxe1ficos}{%
\section{Transformación y primeros gráficos}\label{transformaciuxf3n-y-primeros-gruxe1ficos}}

En el primer gráfico se mostrará la evolución de los votos a partidos de izquierda y de derecha en toda Andalucía desde 2015. Primero se calculan los votos válidos en cada convocatoria. Como en la estructura de datos ese dato está repetido para cada combinación de convocatoria-sección-partido se usará la función distinct antes de agrupar y sumar los votos validos de todas las secciones.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datos\_elecciones\_validos\_total }\OtherTok{\textless{}{-}}
\NormalTok{  datos\_elecciones }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{distinct}\NormalTok{(convocatoria, codigo\_secc, }\AttributeTok{.keep\_all =}\NormalTok{ T) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{region =} \StringTok{"Andalucía"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(convocatoria, region) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{validos =} \FunctionTok{sum}\NormalTok{(validos), }\AttributeTok{.groups =} \StringTok{"drop"}\NormalTok{)}

\NormalTok{datos\_elecciones\_validos\_provs }\OtherTok{\textless{}{-}}
\NormalTok{  datos\_elecciones }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{distinct}\NormalTok{(convocatoria, codigo\_secc, }\AttributeTok{.keep\_all =}\NormalTok{ T) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{region =}\NormalTok{ name\_prov) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(convocatoria, region) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{validos =} \FunctionTok{sum}\NormalTok{(validos), }\AttributeTok{.groups =} \StringTok{"drop"}\NormalTok{)}

\NormalTok{datos\_elecciones\_validos }\OtherTok{\textless{}{-}}
\NormalTok{  datos\_elecciones\_validos\_total }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{bind\_rows}\NormalTok{(datos\_elecciones\_validos\_provs)}
\end{Highlighting}
\end{Shaded}

Ahora se calcula la suma de votos de cada bloque en cada convocatoria. En este caso, como cada fila tiene el dato de votos de un partido distinto no es necesaria la función distinct.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datos\_bloques\_total }\OtherTok{\textless{}{-}}
\NormalTok{  datos\_elecciones }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{region =} \StringTok{"Andalucía"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(convocatoria, region, bloque) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{votos\_bloque =} \FunctionTok{sum}\NormalTok{(votos\_partido), }\AttributeTok{.groups =} \StringTok{"drop"}\NormalTok{)}

\NormalTok{datos\_bloques\_provs }\OtherTok{\textless{}{-}}
\NormalTok{  datos\_elecciones }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{region =}\NormalTok{ name\_prov) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(convocatoria, region, bloque) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{votos\_bloque =} \FunctionTok{sum}\NormalTok{(votos\_partido), }\AttributeTok{.groups =} \StringTok{"drop"}\NormalTok{)}

\NormalTok{datos\_bloques }\OtherTok{\textless{}{-}}
\NormalTok{  datos\_bloques\_total }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{bind\_rows}\NormalTok{(datos\_bloques\_provs) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{left\_join}\NormalTok{(datos\_elecciones\_validos) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{votos\_bloque\_pc =}\NormalTok{ votos\_bloque }\SpecialCharTok{/}\NormalTok{ validos)}
\end{Highlighting}
\end{Shaded}

A continuación, se realiza el gráfico con los daton que se han calculado antes. Se definen los colores que representan a cada bloque, las fechas para poder etiquetar en el gráfico los ticks del eje x y se programa el gráfico.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{title }\OtherTok{\textless{}{-}}
  \StringTok{"Evolución de voto a partidos de \textless{}b style=\textquotesingle{}color:\#457b9d;\textquotesingle{}\textgreater{}derecha\textless{}/b\textgreater{}, \textless{}b style=\textquotesingle{}color:\#e63946;\textquotesingle{}\textgreater{}izquierda\textless{}/b\textgreater{}\textless{}br\textgreater{}y \textless{}b style=\textquotesingle{}color:\#676767;\textquotesingle{}\textgreater{}otros\textless{}/b\textgreater{} desde 2015"}

\NormalTok{convocatorias\_dates }\OtherTok{\textless{}{-}}
\NormalTok{  datos\_bloques }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{distinct}\NormalTok{(convocatoria) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pull}\NormalTok{(convocatoria)}

\NormalTok{datos\_bloques }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(region }\SpecialCharTok{==} \StringTok{"Andalucía"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ convocatoria, }\AttributeTok{y =}\NormalTok{ votos\_bloque\_pc,}
    \AttributeTok{color =}\NormalTok{ bloque, }\AttributeTok{group =}\NormalTok{ bloque}
\NormalTok{  )) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{width =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =}\NormalTok{ colors\_bloques) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_date}\NormalTok{(}\AttributeTok{breaks =}\NormalTok{ convocatorias\_dates, }\AttributeTok{date\_labels =} \StringTok{"\%Y"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\NormalTok{percent) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =}\NormalTok{ title, }\AttributeTok{x =} \StringTok{"Convocatoria"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Votos bloque"}\NormalTok{,}
    \AttributeTok{caption =} \StringTok{"Fuente: Junta de Andalucía"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{,}
    \AttributeTok{plot.title =} \FunctionTok{element\_markdown}\NormalTok{(}\AttributeTok{margin =} \FunctionTok{margin}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{img/plot-voto-andalucia} 

}

\caption{ }(\#fig:plot-voto-andalucia )
\end{figure}

Replicar este mismo gráfico para cada provincia no es complicado. Solo se descartarán los datos de toda Andalucía y se usará la función facet\_wrap que realizará el mismo gráfico con el mismo estilo para cada provincia.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{datos\_bloques }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(region }\SpecialCharTok{!=} \StringTok{"Andalucía"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ convocatoria, }\AttributeTok{y =}\NormalTok{ votos\_bloque\_pc,}
    \AttributeTok{color =}\NormalTok{ bloque, }\AttributeTok{group =}\NormalTok{ bloque}
\NormalTok{  )) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{width =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =}\NormalTok{ colors\_bloques) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_date}\NormalTok{(}\AttributeTok{breaks =}\NormalTok{ convocatorias\_dates, }\AttributeTok{date\_labels =} \StringTok{"\%Y"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\NormalTok{percent) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =}\NormalTok{ title, }\AttributeTok{x =} \StringTok{"Convocatoria"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Votos bloque"}\NormalTok{,}
    \AttributeTok{caption =} \StringTok{"Fuente: Junta de Andalucía"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{region, }\AttributeTok{ncol =} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{,}
    \AttributeTok{plot.title =} \FunctionTok{element\_markdown}\NormalTok{(}\AttributeTok{margin =} \FunctionTok{margin}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{img/voto-andalucia-pro} 

}

\caption{ }(\#fig:plot-voto-andalucia-pro )
\end{figure}

Este segundo gráfico cuenta una historia complementaria al primero. El giro no se ha producido igual en toda Andalucía, no es igual el de Almería que el de Sevilla. Para intentar buscar nuevas diferencias territoriales se explorarán los mapas de ganadores a nivel municipal. Se procede de igual manera que con los datos de provincias, salvo que en este caso se agrega a partir de la columna codigo\_mun. Para calcular el ganador se agrupa por esta columna y se usa la función slice\_max, que tomará para cada municipio la fila del partido con el mayor número de votos.

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{datos\_elecciones\_validos\_muns }\OtherTok{\textless{}{-}}
\NormalTok{  datos\_elecciones }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{distinct}\NormalTok{(convocatoria, codigo\_secc, }\AttributeTok{.keep\_all =}\NormalTok{ T) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(convocatoria, codigo\_mun) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{validos =} \FunctionTok{sum}\NormalTok{(validos),}
    \AttributeTok{.groups =} \StringTok{"drop"}
\NormalTok{  )}

\NormalTok{datos\_bloques\_muns }\OtherTok{\textless{}{-}}
\NormalTok{  datos\_elecciones }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(convocatoria, codigo\_mun, bloque) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{votos\_bloque =} \FunctionTok{sum}\NormalTok{(votos\_partido), }\AttributeTok{.groups =} \StringTok{"drop"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{left\_join}\NormalTok{(datos\_elecciones\_validos\_muns) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{votos\_bloque\_pc =}\NormalTok{ votos\_bloque }\SpecialCharTok{/}\NormalTok{ validos, }\DecValTok{1}\NormalTok{)}

\CommentTok{\# Ahora calculamos los ganadores}
\NormalTok{datos\_winners\_muns }\OtherTok{\textless{}{-}}
\NormalTok{  datos\_bloques\_muns }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(convocatoria, codigo\_mun) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_max}\NormalTok{(votos\_bloque, }\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{with\_ties =}\NormalTok{ F) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(convocatoria, codigo\_mun,}
    \AttributeTok{winner =}\NormalTok{ bloque, votos\_bloque\_pc}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Para realizar el gráfico se tomará el objeto sf con los recintos de los municipios andaluces y se les añadirá los datos de ganadores calculados anteriormente con la función left\_join. Se usa el color del bloque para el relleno y el porcentaje de votos que suma el bloque ganador para la transparencia, de forma que de un vistazo se pueden encontrar feudos de uno u otro bloque.

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{map\_munis }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{left\_join}\NormalTok{(datos\_winners\_muns) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{convocatoria =} \FunctionTok{year}\NormalTok{(convocatoria)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill =}\NormalTok{ winner, }\AttributeTok{alpha =}\NormalTok{ votos\_bloque\_pc),}
    \AttributeTok{size =} \FloatTok{0.01}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =}\NormalTok{ colors\_bloques) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{convocatoria, }\AttributeTok{ncol =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =}\NormalTok{ title,}
    \AttributeTok{caption =} \StringTok{"Fuente: Junta de Andalucía"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{coord\_sf}\NormalTok{(}\AttributeTok{label\_graticule =} \StringTok{""}\NormalTok{, }\AttributeTok{ndiscr =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{,}
    \AttributeTok{plot.title =} \FunctionTok{element\_markdown}\NormalTok{(}\AttributeTok{margin =} \FunctionTok{margin}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{img/voto-andalucia-muni} 

}

\caption{ }\label{fig:plot-voto-andalucia-muni}
\end{figure}

En los mapas se encuentran nuevas historias. En 2015 la derecha era fuerte en la costa de Almería y Málaga. Su presencia creció en 2018, aunque la izquierdas seguía ganando el interior de la comunidad. En 2018 el dominio del bloque de derechas se extiende por casi todo el territorio, en especial en las zonas donde ya era fuerte en 2015.

\hypertarget{paro-clm}{%
\chapter{Evolución del paro registrado en Castilla-La Mancha}\label{paro-clm}}

\emph{Isidro Hidalgo Arellano} y \emph{Ángel Jiménez Rojas}\\
Observatorio del Mercado de Trabajo de Castilla-La Mancha

\hypertarget{planteamiento}{%
\section{Planteamiento}\label{planteamiento}}

En los últimos 15 años el mundo ha sufrido dos grandes períodos de \textbf{crisis económicas}: en \textbf{2008}, de tipo financiero; y en \textbf{2020}, a causa de la pandemia de \textbf{COVID-19}\index{COVID-19}. Uno de los parámetros socioeconómicos que se ven más afectados por este tipo de procesos es el paro registrado. El paro registrado se define \citep{TOHARIA} como el conjunto de los demandantes inscritos en las oficinas de empleo, una vez excluidos los inscritos sin disponibilidad para trabajar y los demandantes no parados, tales como estudiantes, desempleados en formación,etc.\\
\textbf{Castilla-La Mancha} no ha sido ajena a las crisis económicas mencionadas, por lo que en este trabajo se quiere analizar el impacto de las mismas en la estructura del \textbf{paro registrado}\index{paro registrado} de la región. Para ello se han tomado las siguientes variables: \textbf{sexo}\index{sexo} y \textbf{edad}\index{edad} de la persona desempleada, \textbf{sector de actividad económica de procedencia}\index{sector} y \textbf{tiempo de búsqueda de empleo}\index{tiempo de búsqueda de empleo}. El conjunto de datos utilizado comprende la media anual del paro registrado desagregado según estas variables a lo largo de los años que van desde 2007 a 2022 (éste último con datos desde enero hasta agosto).

Para este análisis se usan las librerías y variables (paletas de colores para los gráficos) siguientes:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(CDR, tidyverse, ggpubr)}
\NormalTok{paleta\_heatmaps }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rgb}\NormalTok{(.}\DecValTok{7}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, .}\DecValTok{5}\NormalTok{), }\FunctionTok{rgb}\NormalTok{(.}\DecValTok{13}\NormalTok{, .}\DecValTok{22}\NormalTok{, .}\DecValTok{58}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{paleta\_lineas }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"blue4"}\NormalTok{, }\StringTok{"orange"}\NormalTok{, }\StringTok{"darkgreen"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Se cargan los datos, y se muestra la estructura de la \texttt{tibble} \index{tibble}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"parados\_clm"}\NormalTok{)}
\NormalTok{parados\_clm}
\CommentTok{\# A tibble: 92,095 × 8}
\CommentTok{\#    anyo  sexo    edad sector t\_bus\_e   tramo\_edad t\_bus\_e\_agr parados}
\CommentTok{\#    \textless{}ord\textgreater{} \textless{}fct\textgreater{}  \textless{}dbl\textgreater{} \textless{}fct\textgreater{}  \textless{}ord\textgreater{}     \textless{}ord\textgreater{}      \textless{}ord\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#  1 2022  hombre    16 SINACT t\textless{}=7 días \textless{}30 años   t\textless{}=6 meses    7}
\CommentTok{\#  2 2022  mujer     16 SINACT t\textless{}=7 días \textless{}30 años   t\textless{}=6 meses    8.25}
\CommentTok{\#  3 2019  hombre    28 INDUST t\textless{}=7 días \textless{}30 años   t\textless{}=6 meses    8.58}
\CommentTok{\#  4 2019  mujer     28 INDUST t\textless{}=7 días \textless{}30 años   t\textless{}=6 meses    3.83}
\CommentTok{\#  5 2017  hombre    48 CONSTR t\textless{}=7 días \textgreater{}44 años   t\textless{}=6 meses   15.8}
\CommentTok{\# … with 92,085 more rows}
\end{Highlighting}
\end{Shaded}

\hypertarget{evoluciuxf3n-del-nuxfamero-total-de-parados}{%
\section{Evolución del número total de parados}\label{evoluciuxf3n-del-nuxfamero-total-de-parados}}

Para ver la evolución del número total de parados se puede construir un gráfico de evolución:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{resumen }\OtherTok{\textless{}{-}}\NormalTok{ parados\_clm }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(anyo) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{parados =} \FunctionTok{sum}\NormalTok{(parados)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{anyo =} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(anyo)))}
\NormalTok{anyos }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2007}\NormalTok{, }\DecValTok{2013}\NormalTok{, }\DecValTok{2019}\NormalTok{, }\DecValTok{2020}\NormalTok{, }\DecValTok{2022}\NormalTok{)}
\NormalTok{paro\_anyos }\OtherTok{\textless{}{-}}\NormalTok{ resumen }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(anyo }\SpecialCharTok{\%in\%}\NormalTok{ anyos) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(parados) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{parados =} \FunctionTok{round}\NormalTok{(parados, }\DecValTok{0}\NormalTok{))}
\NormalTok{puntos }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(anyos, paro\_anyos)}
\NormalTok{graf }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(resumen, }\FunctionTok{aes}\NormalTok{(anyo, parados)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{linewidth =} \DecValTok{2}\NormalTok{, }\AttributeTok{col =}\NormalTok{ paleta\_lineas[}\DecValTok{1}\NormalTok{], }\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"número de parados"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(puntos, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ anyos, }\AttributeTok{y =}\NormalTok{ parados,}
    \AttributeTok{shape =} \StringTok{"circle filled"}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{fill =}\NormalTok{ paleta\_lineas[}\DecValTok{1}\NormalTok{],}
    \AttributeTok{alpha =} \FloatTok{0.5}
\NormalTok{  )) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{,}
    \AttributeTok{axis.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{face =} \StringTok{"bold"}\NormalTok{, }\AttributeTok{size =} \DecValTok{10}\NormalTok{),}
    \AttributeTok{axis.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{face =} \StringTok{"bold"}\NormalTok{, }\AttributeTok{size =} \DecValTok{10}\NormalTok{),}
    \AttributeTok{strip.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{9}\NormalTok{, }\AttributeTok{face =} \StringTok{"bold"}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{labels =} \ControlFlowTok{function}\NormalTok{(x) \{}
    \FunctionTok{format}\NormalTok{(x,}
      \AttributeTok{big.mark =} \StringTok{"."}\NormalTok{,}
      \AttributeTok{scientific =} \ConstantTok{FALSE}
\NormalTok{    )}
\NormalTok{  \})}
\NormalTok{graf}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/empleo-total} 

}

\caption{Evolución del paro registrado en CLM}\label{fig:empleo-total}
\end{figure}

De un primer análisis visual de la Fig. \ref{fig:empleo-total} se toman como puntos de referencia los años previos a las crisis: 2007 y 2019, y el último año, 2022. Se puede observar que, si bien la crisis de la \textbf{COVID-19} ha tenido profundos efectos sectoriales, principalmente en turismo\index{turismo}, comercio\index{comercio} y restauración\index{restauración}, la \textbf{crisis de 2008} tuvo un impacto enorme y generalizado en toda la economía, por lo que su efecto en el paro registrado fue devastador, multiplicando por un factor mayor de 3 la cifra total de paro en la región desde 2007. Sin embargo, a partir del año 2013 el paro registrado inicia una tendencia a la baja muy pronunciada que aún hoy continúa, después de haber repuntado ligeramente por la crisis de la COVID-19.

\hypertarget{evoluciuxf3n-de-la-edad-y-el-sexo-en-la-poblaciuxf3n-parada}{%
\section{Evolución de la edad y el sexo en la población parada}\label{evoluciuxf3n-de-la-edad-y-el-sexo-en-la-poblaciuxf3n-parada}}

Para ver cómo ha cambiado la estructura del paro registrado en función de la \texttt{edad} y el \texttt{sexo} de los parados se pueden utilizar diferentes gráficos. En este análisis, se usan mapas de calor y gráficos de distribución de densidad. Para hacer un mapa de calor\index{mapa de calor} que permita comparar dos variables simultáneamente, se construye la siguiente función:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{heatmap\_anyos }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(var1, var2, }\AttributeTok{inicio =} \DecValTok{2007}\NormalTok{, }\AttributeTok{intermedio =} \DecValTok{2019}\NormalTok{, }\AttributeTok{fin =} \DecValTok{2022}\NormalTok{) \{}
\NormalTok{  tabla }\OtherTok{\textless{}{-}} \FunctionTok{select}\NormalTok{(parados\_clm, anyo, var1, var2, parados) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{filter}\NormalTok{(anyo }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(inicio, intermedio, fin))}
  \FunctionTok{names}\NormalTok{(tabla) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"anyo"}\NormalTok{, }\StringTok{"var1"}\NormalTok{, }\StringTok{"var2"}\NormalTok{, }\StringTok{"parados"}\NormalTok{)}
\NormalTok{  tabla }\OtherTok{\textless{}{-}}\NormalTok{ tabla }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{group\_by}\NormalTok{(anyo, var1, var2) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{summarise}\NormalTok{(}\AttributeTok{parados =} \FunctionTok{sum}\NormalTok{(parados))}
\NormalTok{  graf }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(tabla, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ var1, }\AttributeTok{y =}\NormalTok{ var2, }\AttributeTok{fill =}\NormalTok{ parados)) }\SpecialCharTok{+}
    \FunctionTok{geom\_raster}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{scale\_fill\_gradientn}\NormalTok{(}\AttributeTok{colours =}\NormalTok{ paleta\_heatmaps) }\SpecialCharTok{+}
    \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{anyo) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{""}\NormalTok{, }\AttributeTok{y =} \StringTok{""}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme}\NormalTok{(}
      \AttributeTok{axis.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{10}\NormalTok{, }\AttributeTok{face =} \StringTok{"bold"}\NormalTok{),}
      \AttributeTok{axis.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{10}\NormalTok{, }\AttributeTok{face =} \StringTok{"bold"}\NormalTok{),}
      \AttributeTok{strip.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{10}\NormalTok{, }\AttributeTok{face =} \StringTok{"bold"}\NormalTok{)}
\NormalTok{    )}
  \FunctionTok{return}\NormalTok{(graf)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Si se lanza la función \texttt{heatmap\_anyos()} para las variables \texttt{edad} y \texttt{sexo}, tomando como años comparativos 2007, 2019 y 2022, se obtiene:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{heatmap\_anyos}\NormalTok{(}\StringTok{"sexo"}\NormalTok{, }\StringTok{"edad"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/empleo-heatmap-edad-sexo} 

}

\caption{Paro registrado según edad y sexo en 2007, 2019 y 2022}\label{fig:empleo-heatmap-edad-sexo}
\end{figure}

Se puede apreciar en la Fig. \ref{fig:empleo-heatmap-edad-sexo} que en los dos procesos críticos se ha producido un \textbf{desplazamiento a mayor edad}, siendo este cambio más pronunciado en las \textbf{mujeres}.\\
El mapa de calor es muy útil para ver inicialmente de forma visual estos cambios, pero si se desea ver detalladamente cómo ha cambiado la distribución\index{distribución} del paro según el \texttt{sexo} y la \texttt{edad}, se programa otra función, que proporciona mayor nivel de detalle:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{densidad\_compara }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(variab, }\AttributeTok{inicio =} \DecValTok{2007}\NormalTok{, }\AttributeTok{medio =} \DecValTok{2019}\NormalTok{,}
                             \AttributeTok{fin =} \DecValTok{2022}\NormalTok{) \{}
\NormalTok{  tabla }\OtherTok{\textless{}{-}} \FunctionTok{select}\NormalTok{(parados\_clm, anyo, variab, edad, parados) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{filter}\NormalTok{(anyo }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(inicio, medio, fin))}
  \FunctionTok{names}\NormalTok{(tabla) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"anyo"}\NormalTok{, }\StringTok{"variable"}\NormalTok{, }\StringTok{"edad"}\NormalTok{, }\StringTok{"parados"}\NormalTok{)}
\NormalTok{  tabla }\OtherTok{\textless{}{-}}\NormalTok{ tabla }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{group\_by}\NormalTok{(anyo, edad, variable) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{summarise}\NormalTok{(}\AttributeTok{parados =} \FunctionTok{sum}\NormalTok{(parados)) }\CommentTok{\# |\textgreater{}}
\NormalTok{  graf }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(tabla, }\FunctionTok{aes}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ edad, }\AttributeTok{y =}\NormalTok{ parados, }\AttributeTok{color =}\NormalTok{ anyo,}
    \AttributeTok{fill =}\NormalTok{ anyo}
\NormalTok{  )) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{variable, }\AttributeTok{ncol =} \FunctionTok{dim}\NormalTok{(}\FunctionTok{table}\NormalTok{(tabla}\SpecialCharTok{$}\NormalTok{variable))[}\DecValTok{1}\NormalTok{]) }\SpecialCharTok{+}
    \FunctionTok{ylab}\NormalTok{(}\StringTok{"número de parados"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{color =} \StringTok{"año"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =}\NormalTok{ paleta\_lineas) }\SpecialCharTok{+}
    \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{labels =} \ControlFlowTok{function}\NormalTok{(x) \{}
      \FunctionTok{format}\NormalTok{(x,}
        \AttributeTok{big.mark =} \StringTok{"."}\NormalTok{,}
        \AttributeTok{scientific =} \ConstantTok{FALSE}
\NormalTok{      )}
\NormalTok{    \}) }\SpecialCharTok{+}
    \FunctionTok{theme}\NormalTok{(}
      \AttributeTok{strip.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{10}\NormalTok{, }\AttributeTok{face =} \StringTok{"bold"}\NormalTok{),}
      \AttributeTok{axis.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{10}\NormalTok{, }\AttributeTok{face =} \StringTok{"bold"}\NormalTok{),}
      \AttributeTok{axis.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{10}\NormalTok{, }\AttributeTok{face =} \StringTok{"bold"}\NormalTok{)}
\NormalTok{    )}
  \FunctionTok{return}\NormalTok{(graf)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

La función \texttt{densidad\_compara()} produce un cuadro de gráficos comparando las distribuciones de la edad dentro de cada estrato de la variable elegida, para tres años diferentes (2007, 2019 y 2022 por defecto). Ejecutando esta función para la variable \texttt{sexo} se obtiene:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{densidad\_compara}\NormalTok{(}\StringTok{"sexo"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/empleo-densidad-sexo} 

}

\caption{Distribución por edad y sexo para 2007, 2019 y 2022}\label{fig:empleo-densidad-sexo}
\end{figure}

Se observa en la Fig. \ref{fig:empleo-densidad-sexo} que las distribuciones del año 2007, anteriores a ambas crisis, que los hombres parados presentan \textbf{dos máximos}, en torno a 25 y 60 años, mientras que las mujeres tienen una distribución bastante centrada entre 30 y 40 años. En cambio, si se observan las distribuciones en 2022, se aprecia el desplazamiento de la distribución de ambos sexos a edades \textbf{mayores de 50 años}. Este desplazamiento es algo más intenso en las mujeres.\\
Se observa igualmente que comparando las distribuciones de las mujeres de 2019 y 2022 la crisis de la COVID-19 ha desplazado entre 5 y 10 años la distribución de la edad de las mujeres paradas. Este desplazamiento es inferior en los hombres, donde supone menos de 5 años.

\hypertarget{evoluciuxf3n-del-tiempo-de-buxfasqueda-de-empleo-en-la-poblaciuxf3n-parada}{%
\section{Evolución del tiempo de búsqueda de empleo en la población parada}\label{evoluciuxf3n-del-tiempo-de-buxfasqueda-de-empleo-en-la-poblaciuxf3n-parada}}

Se define el \textbf{tiempo de búsqueda de empleo} como el tiempo transcurrido ininterrumpidamente desde la última inscripción de la persona en el paro registrado\citep{JIPINFANTE}.\\
Si se toma como simplificación una agregación de cuatro tramos de tiempo de búsqueda de empleo (de los doce originales), ejecutando la función \texttt{densidad\_compara()} para la variable \texttt{t\_bus\_e\_agr} se obtiene:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{densidad\_compara}\NormalTok{(}\StringTok{"t\_bus\_e\_agr"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/empleo-densidad-bus-agr} 

}

\caption{Distribución del paro registrado por edad y tiempo de búsqueda de empleo}\label{fig:empleo-densidad-bus-agr}
\end{figure}

En la Fig. \ref{fig:empleo-densidad-bus-agr} se pone de manifiesto que el tramo con mayor incremento de número de parados es el correspondiente a más de 24 meses de búsqueda de empleo, señalando que después de la crisis financiera de 2008 es más difícil encontrar empleo para una persona en \textbf{paro de larga duración}\index{paro de larga duración}. Se puede afirmar también que los dos períodos de crisis han supuesto la creación de un \textbf{paro estructural de larga duración}.

Dejamos al lector ejecutar la función \texttt{heatmap\_anyos()} para las variables \texttt{sexo} y \texttt{t\_bus\_e}, tomando como años comparativos 2007, 2019 y 2022, observará en el gráfico resultante que el incremento en el paro correspondiente al tiempo de búsqueda de empleo \textbf{mayor de 24 meses} es más profundo en el colectivo de las mujeres. El código a utilizar es:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{heatmap\_anyos}\NormalTok{(}\StringTok{"sexo"}\NormalTok{, }\StringTok{"t\_bus\_e"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{evoluciuxf3n-del-paro-registrado-seguxfan-sexo-edad-y-sector-de-procedencia}{%
\section{Evolución del paro registrado según sexo, edad y sector de procedencia}\label{evoluciuxf3n-del-paro-registrado-seguxfan-sexo-edad-y-sector-de-procedencia}}

La variable sector de procedencia es un tanto particular ya que, cuando un parado lleva mucho tiempo buscando empleo ininterrumpidamente, ``pierde'' el sector de procedencia y se clasifica automáticamente como ``sin actividad''. A la hora de analizar esta variable, por tanto, es importante tener en cuenta que una parte de los parados calificados como ``sin actividad'', realmente tuvieron un trabajo hace mucho tiempo.\\
Si se desea visualizar los cambios producidos en estas variables con un mapa de calor, es posible ejecutar de nuevo la función \texttt{heatmap\_anyos()} para obtener:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{heatmap\_anyos}\NormalTok{(}\StringTok{"sexo"}\NormalTok{, }\StringTok{"sector"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth,height=0.5\textheight]{img/empleo-heatmap-sexo-sector} 

}

\caption{Paro registrado según sexo y sector de procedencia}\label{fig:empleo-heatmap-sexo-sector}
\end{figure}

Destaca en la Fig. \ref{fig:empleo-heatmap-sexo-sector} el incremento de paro registrado correspondiente al sector \textbf{servicios}, siendo este efecto mayor para las mujeres.\\
Ejecutando la función \texttt{densidad\_compara()} para la variable \texttt{sector} se obtiene:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{densidad\_compara}\NormalTok{(}\StringTok{"sector"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth,height=0.5\textheight]{img/empleo-densidad-sector} 

}

\caption{Distribución del paro registrado por edad y tiempo de búsqueda de empleo}\label{fig:empleo-densidad-sector}
\end{figure}

Como se observa en la Fig. \ref{fig:empleo-densidad-sector} las diferencias a lo largo del tiempo de las distribuciones por sector de actividad económica revelan algunas particularidades interesantes. \textbf{Industria}\index{industria} y \textbf{construcción}\index{construcción} se comportan de modo similar: hay un fuerte desplazamiento en edad desde 2007, pero se mantiene el volumen de paro en ambos sectores a lo largo de los 15 años de estudio. El paro en el sector \textbf{agropecuario} y en el sector \textbf{servicios} presenta igualmente desplazamiento en edad, pero además ha subido intensamente en estos 15 años; ambos efectos son mucho más evidentes en el sector \textbf{servicios}. Finalmente, en el colectivo \textbf{sin actividad} se aprecian dos características: en primer lugar, los parados menores de 30 años suponen el mayor volumen en este colectivo, como era de esperar, ya que la población joven que accede al mercado laboral por primera vez, no cuenta con experiencia previa; en segundo lugar, desde 2007 a 2019, y algo menos desde 2019 a 2022 hay un incremento de volumen de paro en los \textbf{mayores de 45 años}\index{mayores de 45 años} que, con toda probabilidad, corresponde a los parados de larga duración de mayor edad.\\
En todos los sectores se aprecia el descenso del volumen total de paro registrado desde 2019 a 2022, a pesar de la crisis sanitaria de la COVID-19.

\hypertarget{conclusiones}{%
\section{Conclusiones}\label{conclusiones}}

La crisis de 2008 tuvo un gran impacto en el paro registrado de Castilla- La Mancha, multiplicándolo por un factor mayor de 3 desde 2007. Sin embargo, a partir del año 2013 el paro registrado inicia una tendencia a la baja muy pronunciada que aún hoy continúa, después de haber sufrido un rebote debido a la crisis de la COVID-19.\\
La estructura interna de la población parada en la región ha cambiado sustancialmente atendiendo a las variables analizadas. En efecto, la población mayor de 45 años, las mujeres, los parados de larga duración y el sector servicios son los grandes perjudicados por ambos procesos de crisis.

\hypertarget{cap-idealista}{%
\chapter{Valoración del precio de la vivienda}\label{cap-idealista}}

\emph{David Rey y Pelayo Arbués}

\hypertarget{introducciuxf3n-26}{%
\section{Introducción}\label{introducciuxf3n-26}}

Este capítulo construye un modelo de valoración de la vivienda utilizando se utiliza el paquete de datos \textbf{idealista18} que contiene datos de precios de vivienda plurifamiliar para las ciudades españolas de Madrid, Valencia y Barcelona \citep{Rey2022Data}. Como se ha indicado en capítulos anteriores del libro este conjunto de datos se ha puesto a disposición de la comunidad investigadora como un conjunto de datos abierto.

La valoración de viviendas es particularmente difícil la naturaleza heterogénea de este tipo de bien, que es consecuencia de los distintos atributos que la conforman, su estado de conservación y su ubicación. Para resolver esta cuestión, se puede utilizar la técnica de estimación de precios, denominada hedónica, que permite controlar los cambios de precio en función de su calidad, características o localización. La teoría de precios hedónicos calcula el precio de un bien mediante la agregación de las contribuciones de los diferentes elementos que lo componen.
Dicho de otro modo, un piso vale más o menos en función de sus metros, es decir más metros suponen una utilidad positiva para el comprador. Pero no solamente existen contribuciones positivas por características físicas, sino que, un piso frente a la playa tiene mayor valor que otro situado en una zona interior de la ciudad. Esta utilidad se puede denominar espacial o de localización y está relacionada con la existencia de heterogeneidad espacial, entendida en este caso como la variación sistemática de los precios hedónicos a través de la geografía.

En la literatura no existe un consenso general sobre cómo especificar las covariables de localización, por lo que a menudo se construyen de manera arbitraria. Una mala especificación espacial conduce a sesgos asociados a varios fenómenos, que son: la heterogeneidad espacial y la dependencia espacial\footnote{ La heterogeneidad espacial existe si las viviendas tienen precios parecidos a viviendas cercanas, y la dependencia espacial cuando el precio de una vivienda afecta a los precios de su entorno.} \citep{lesage2009introduction}.

En este capítulo se presenta una especificación de modelo de tipo hedónico que incluye distintas características de la vivienda a la vez que trata de modelar la naturaleza espacial de los datos utilizando efectos aleatorios espaciales sobre una superficie continua.

\hypertarget{conjunto-de-datos}{%
\section{Conjunto de datos}\label{conjunto-de-datos}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Se recomienda gestionar las dependencias con la librería renv}
\CommentTok{\# https://rstudio.github.io/renv/articles/renv.html}
\FunctionTok{library}\NormalTok{(INLA)}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(sp)}
\FunctionTok{library}\NormalTok{(sf)}
\FunctionTok{library}\NormalTok{(tmap)}
\FunctionTok{library}\NormalTok{(knitr)}
\FunctionTok{library}\NormalTok{(gt)}
\FunctionTok{library}\NormalTok{(idealista18)}
\end{Highlighting}
\end{Shaded}

Se utilizan con dos sistemas de referencia espacial, el original que es \emph{EPSG:4326} y con el que se trabajará, que son coordenadas cartesianas proyectadas en \emph{EPSG:25830}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{projcrs\_src }\OtherTok{\textless{}{-}} \StringTok{"EPSG:4326"}
\NormalTok{projcrs\_dest }\OtherTok{\textless{}{-}} \StringTok{"EPSG:25830"}
\end{Highlighting}
\end{Shaded}

Se cargan los polígonos de las secciones de Madrid. Se añade una nueva variable denominada \emph{district}, que tiene el código de cada distrito a partir del código de sección censal, sobre la que se seleccionan tres distritos sobre los que se trabajará: Centro (01), Arganzuela (02) y Retiro (03).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Carga de areas censales para Madrid, INE 2011 (fuente en formato WKT CRS EPSG:4326)}
\CommentTok{\# El código censal español se compone de 10 dígitos, compuesto por 2 dígitos para provincia,}
\CommentTok{\# 3 para municipio, 2 para distrito y 3 para la sección censal.}
\NormalTok{polygons\_2011 }\OtherTok{\textless{}{-}} \FunctionTok{read\_delim}\NormalTok{(}\StringTok{"data/polygons{-}2011{-}madrid.csv"}\NormalTok{,}
  \AttributeTok{delim =} \StringTok{";"}
\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{st\_as\_sf}\NormalTok{(}\AttributeTok{wkt =} \StringTok{"WKT"}\NormalTok{, }\AttributeTok{crs =}\NormalTok{ projcrs\_src) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{st\_transform}\NormalTok{(projcrs\_dest) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{district =} \FunctionTok{substring}\NormalTok{(CUSEC, }\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{))}

\CommentTok{\# Seleccionamos los datos de la zona central}
\NormalTok{district\_list\_center }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"01"}\NormalTok{, }\StringTok{"02"}\NormalTok{, }\StringTok{"03"}\NormalTok{)}
\NormalTok{polygons\_2011 }\OtherTok{\textless{}{-}}\NormalTok{ polygons\_2011 }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(district }\SpecialCharTok{\%in\%}\NormalTok{ district\_list\_center)}
\end{Highlighting}
\end{Shaded}

En la Figura se muestran las secciones censales y los distritos del análisis.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(polygons\_2011[, }\FunctionTok{c}\NormalTok{(}\StringTok{"district"}\NormalTok{)], }\AttributeTok{main =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/fig-censustracts-districts} 

}

\caption{Secciones censales para los tres distritos de trabajo, cada color indica un distrito.}\label{fig:plot-fig-censustracts-districts}
\end{figure}

Se toman solo los inmuebles dentro de la zona central de la ciudad, previo haber reproyectado los datos al CRS de trabajo (\emph{EPSG:2583}). Se usan solamente datos del último trimestre de 2018 (condición \emph{PERIOD==201812}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample }\OtherTok{\textless{}{-}}\NormalTok{ idealista18}\SpecialCharTok{::}\NormalTok{Madrid\_Sale }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{st\_transform}\NormalTok{(projcrs\_dest) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(PERIOD }\SpecialCharTok{==} \DecValTok{201812}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(ASSETID) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{row\_number}\NormalTok{() }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{st\_join}\NormalTok{(polygons\_2011, }\AttributeTok{left =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\hypertarget{estimaciuxf3n-del-modelo-1}{%
\section{Estimación del Modelo}\label{estimaciuxf3n-del-modelo-1}}

Este ejemplo estima un modelo espacial de efectos aleatorios espaciales sobre una superficie continua. Para ello se utiliza un método de estimación bayesiana, dadas sus propiedades en cuanto a la interpretabilidad de los modelos y la medida de la incertidumbre de las estimaciones. Para reducir el coste computacional de estimar estos modelos, se utiliza Integrated nested Laplace approximation, INLA \citep{rue2009approximate, martins2013bayesian}, un método computacional para la inferencia Bayesiana que es muy eficiente con modelos complejos y conjuntos de datos grandes. Este ejercicio se basa, en parte, del ejemplo del capítulo \emph{``Spatial Models using Stochastic Partial Differential Equations''} del libro \citep{gomez2020bayesian}. En concreto el método aquí descrito está basado en la aproximación \emph{SPDE} que se describe de forma detalla en \citep{bakka2018spatial} que ajusta un modelo con un efecto espacial continuo.

Este modelo espacial se implementa como un efecto latente del tipo \emph{SDPE} para INLA, este modelo requiere la construcción de una malla sobre la región de estudio, que se utilizará para aproximar el proceso espacial. Krainski \citeyearpar{krainski2018advanced} describe en detalle los procesos \emph{SDPE}. El proceso comienza por la construcción de una malla de tipo rectangular de 100 x 100 celdas, sobre las que se calcularán las predicciones del modelo. Es importante no confundir esta malla con la malla del proceso espacial.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Combinamos todos los polígonos}
\NormalTok{madrid\_comb }\OtherTok{\textless{}{-}} \FunctionTok{st\_combine}\NormalTok{(polygons\_2011)}
\CommentTok{\# Construimos la malla de 100 x 100 rectángulos sobre la ciudad}
\NormalTok{madrid\_grid }\OtherTok{\textless{}{-}} \FunctionTok{st\_make\_grid}\NormalTok{(madrid\_comb, }\AttributeTok{n =} \FunctionTok{c}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Dado que esta malla es más amplia que la región de estudio, se recorta para contener solamente las celdas circunscritas en los polígonos de la ciudad.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Seleccionamos solo las celdas que estén dentro de un polígono}
\NormalTok{aux }\OtherTok{\textless{}{-}} \FunctionTok{st\_intersects}\NormalTok{(madrid\_grid, polygons\_2011)}
\NormalTok{madrid\_grid }\OtherTok{\textless{}{-}}\NormalTok{ madrid\_grid[}\FunctionTok{unlist}\NormalTok{(}\FunctionTok{lapply}\NormalTok{(aux, length)) }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{]}

\CommentTok{\# Para ver la malla descomentar la siguiente linea}
\CommentTok{\# plot(madrid\_grid, asp = 1)}
\end{Highlighting}
\end{Shaded}

El siguiente paso es definir la malla sobre la que se va a ajustar el modelo \emph{SPDE}. La malla es en realidad un sistema de coordenadas discreto sobre el que se estimará el modelo, donde los puntos en el espacio sobre los que se trabajan son los vértices del proceso de triangulación que calcula la función \texttt{inla.mesh.2d}, para más información sobre este proceso véase \citep{bakka2018spatial}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m\_coords }\OtherTok{\textless{}{-}} \FunctionTok{st\_coordinates}\NormalTok{(sample)}

\CommentTok{\# Creación de malla}
\NormalTok{mesh }\OtherTok{\textless{}{-}} \FunctionTok{inla.mesh.2d}\NormalTok{(}
  \AttributeTok{loc.domain =}\NormalTok{ m\_coords,}
  \AttributeTok{max.edge =} \FunctionTok{c}\NormalTok{(}\DecValTok{550}\NormalTok{, }\DecValTok{1200}\NormalTok{),}
  \AttributeTok{offset =} \FunctionTok{c}\NormalTok{(}\DecValTok{600}\NormalTok{, }\DecValTok{1200}\NormalTok{)}
\NormalTok{)}

\CommentTok{\# Para ver la malla que usaremos en el hedónico descomentar las siguientes lineas}
\CommentTok{\# plot(mesh)}
\CommentTok{\# points(m\_coords, pch = 19)}
\end{Highlighting}
\end{Shaded}

El modelo bayesiano establece la distribución a-priori para los parámetros de función de covarianza \emph{Mátern} que definen el efecto aleatorio espacial. Cuando se construye la matriz, se establecen los parámetros de rango y \(\sigma\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Creación de SPDE}
\NormalTok{madrid.spde }\OtherTok{\textless{}{-}} \FunctionTok{inla.spde2.pcmatern}\NormalTok{(}
  \AttributeTok{mesh =}\NormalTok{ mesh, }\AttributeTok{alpha =} \DecValTok{2}\NormalTok{,}
  \AttributeTok{prior.range =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\FloatTok{0.95}\NormalTok{), }\CommentTok{\# P(range \textless{} 2) = 0.95}
  \AttributeTok{prior.sigma =} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\FloatTok{0.01}\NormalTok{)}
\NormalTok{) }\CommentTok{\# P(sigma \textgreater{} 10) = 0.01}
\end{Highlighting}
\end{Shaded}

La matriz \emph{A} se utiliza para calcular el efecto aleatorio para los vértices de la malla generada a través del proceso de triangulación anterior, que se corresponden con la situación real de las distintas viviendas del conjunto de datos. El efecto aleatorio espacial de cada vivienda se calcula como el peso de la suma de los efectos aleatorios de los vértices entre los que está cada vivienda \citep{bakka2018spatial}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Creación de la matriz proyectora A}
\NormalTok{a\_madrid }\OtherTok{\textless{}{-}} \FunctionTok{inla.spde.make.A}\NormalTok{(}\AttributeTok{mesh =}\NormalTok{ mesh, }\AttributeTok{loc =}\NormalTok{ m\_coords)}
\end{Highlighting}
\end{Shaded}

El objeto \(s.index\) contiene el índice para el modelo espacial gaussiano latente mediante \texttt{inla.spde.make.index()}. Se deben especificar el nombre del índice \emph{s} y el número de vértices en el modelo SPDE \emph{spde\$n.spde}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s.index }\OtherTok{\textless{}{-}} \FunctionTok{inla.spde.make.index}\NormalTok{(}
  \AttributeTok{name =} \StringTok{"spatial.field"}\NormalTok{,}
  \AttributeTok{n.spde =}\NormalTok{ madrid.spde}\SpecialCharTok{$}\NormalTok{n.spde}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Para introducir los datos en el modelo INLA, es necesario definir una estructura de dato de tipo \emph{stack} que incluya los distintos parámetros: los efectos aleatorios y los efectos fijos. La variable dependiente será el precio de venta en €/m².

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{madrid.stack }\OtherTok{\textless{}{-}} \FunctionTok{inla.stack}\NormalTok{(}
  \AttributeTok{data =} \FunctionTok{list}\NormalTok{(}\AttributeTok{UNITPRICE =}\NormalTok{ sample}\SpecialCharTok{$}\NormalTok{UNITPRICE),}
  \AttributeTok{A =} \FunctionTok{list}\NormalTok{(a\_madrid, }\DecValTok{1}\NormalTok{),}
  \AttributeTok{effects =} \FunctionTok{list}\NormalTok{(}
    \FunctionTok{c}\NormalTok{(s.index, }\FunctionTok{list}\NormalTok{(}\AttributeTok{Intercept =} \DecValTok{1}\NormalTok{)),}
    \FunctionTok{list}\NormalTok{(}
      \AttributeTok{ROOMNUMBER =}\NormalTok{ sample}\SpecialCharTok{$}\NormalTok{ROOMNUMBER,}
      \AttributeTok{BATHNUMBER =}\NormalTok{ sample}\SpecialCharTok{$}\NormalTok{BATHNUMBER,}
      \AttributeTok{HASLIFT =}\NormalTok{ sample}\SpecialCharTok{$}\NormalTok{HASLIFT,}
      \AttributeTok{district =}\NormalTok{ sample}\SpecialCharTok{$}\NormalTok{district,}
      \AttributeTok{CONSTRUCTEDYEAR =}\NormalTok{ sample}\SpecialCharTok{$}\NormalTok{CONSTRUCTIONYEAR,}
      \AttributeTok{CADASTRALQUALITYID =}\NormalTok{ sample}\SpecialCharTok{$}\NormalTok{CADASTRALQUALITYID}
\NormalTok{    )}
\NormalTok{  ),}
  \AttributeTok{tag =} \StringTok{"madrid.data"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Además de la información para ajustar el modelo, se desea estimar el efecto espacial aleatorio (más el intercepto del modelo) sobre un conjunto de puntos definidos mediante una malla rectangular \emph{grid}, para cada una de las celdas se toma el centroide sobre el que se calculará la predicción. Para realizar la inferencia se debe crear un nuevo un nuevo \emph{stack} (variable \(madrid.stack.pred\)).

Para poder hacer la inferencia se deben estimar las variables independientes, como los puntos del grid no se corresponden con una vivienda se imputan los valores de calidad constructivas y año de construcción medianos de su sección censal.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Medidas por sección censal}
\NormalTok{sample\_cusec }\OtherTok{\textless{}{-}}\NormalTok{ sample}

\CommentTok{\# Eliminamos la geometría para hacer que la agregación sea más rápida}
\FunctionTok{st\_geometry}\NormalTok{(sample\_cusec) }\OtherTok{\textless{}{-}} \ConstantTok{NULL}

\NormalTok{sample\_cusec }\OtherTok{\textless{}{-}}\NormalTok{ sample\_cusec }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(CUSEC) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{sscc\_constructedyear =} \FunctionTok{median}\NormalTok{(CONSTRUCTIONYEAR, }\AttributeTok{na.rm =}\NormalTok{ T),}
    \AttributeTok{sscc\_cadastralqualityid =} \FunctionTok{median}\NormalTok{(CADASTRALQUALITYID, }\AttributeTok{na.rm =}\NormalTok{ T),}
    \AttributeTok{sscc\_district =} \FunctionTok{first}\NormalTok{(district)}
\NormalTok{  )}

\NormalTok{polygons\_2011 }\OtherTok{\textless{}{-}}\NormalTok{ polygons\_2011 }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{inner\_join}\NormalTok{(sample\_cusec, }\AttributeTok{by =} \StringTok{"CUSEC"}\NormalTok{)}

\CommentTok{\# Convertimos el objeto sfc a sf}
\CommentTok{\# Tomamos solo los centroides para que solo haya una correspondencia}
\CommentTok{\#  con la seccion censal}

\NormalTok{grid\_attributes }\OtherTok{\textless{}{-}} \FunctionTok{st\_as\_sf}\NormalTok{(}\FunctionTok{st\_centroid}\NormalTok{(madrid\_grid)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{st\_join}\NormalTok{(polygons\_2011)}
\end{Highlighting}
\end{Shaded}

La inferencia se realiza sobre pisos de dos habitaciones, 1 baño y con ascensor, que tengan las mismas características constructivas (año de construcción y calidad constructivas) que las de su sección censal.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grid\_coords }\OtherTok{\textless{}{-}} \FunctionTok{st\_coordinates}\NormalTok{(}\FunctionTok{st\_centroid}\NormalTok{(madrid\_grid))}

\CommentTok{\# Creamos la estructura de datos para la predicción}
\NormalTok{A.pred }\OtherTok{\textless{}{-}} \FunctionTok{inla.spde.make.A}\NormalTok{(}\AttributeTok{mesh =}\NormalTok{ mesh, }\AttributeTok{loc =}\NormalTok{ grid\_coords)}
\NormalTok{madrid.stack.pred }\OtherTok{\textless{}{-}} \FunctionTok{inla.stack}\NormalTok{(}
  \AttributeTok{data =} \FunctionTok{list}\NormalTok{(}\AttributeTok{UNITPRICE =} \ConstantTok{NA}\NormalTok{),}
  \AttributeTok{A =} \FunctionTok{list}\NormalTok{(A.pred, }\DecValTok{1}\NormalTok{),}
  \AttributeTok{effects =} \FunctionTok{list}\NormalTok{(}
    \FunctionTok{c}\NormalTok{(s.index, }\FunctionTok{list}\NormalTok{(}\AttributeTok{Intercept =} \DecValTok{1}\NormalTok{)),}
    \FunctionTok{list}\NormalTok{(}
      \AttributeTok{ROOMNUMBER =} \DecValTok{2}\NormalTok{,}
      \AttributeTok{BATHNUMBER =} \DecValTok{1}\NormalTok{,}
      \AttributeTok{HASLIFT =} \DecValTok{1}\NormalTok{,}
      \AttributeTok{district =}\NormalTok{ grid\_attributes}\SpecialCharTok{$}\NormalTok{district,}
      \AttributeTok{CONSTRUCTEDYEAR =}\NormalTok{ grid\_attributes}\SpecialCharTok{$}\NormalTok{sscc\_constructedyear,}
      \AttributeTok{CADASTRALQUALITYID =}\NormalTok{ grid\_attributes}\SpecialCharTok{$}\NormalTok{sscc\_cadastralqualityid}
\NormalTok{    )}
\NormalTok{  ),}
  \AttributeTok{tag =} \StringTok{"madrid.pred"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Posteriormente, se unen ambas estructuras de tipo \emph{stack} para su uso en el ajuste y predicción.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{join.stack }\OtherTok{\textless{}{-}} \FunctionTok{inla.stack}\NormalTok{(madrid.stack, madrid.stack.pred)}
\end{Highlighting}
\end{Shaded}

En la especificación del modelo se utilizan efectos aleatorios, efectos fijos y el efecto espacial aleatorio del modelo \emph{spde}. El ejercicio pretende mostrar la construcción de un modelo bayesiano espacial en el que intervienen además otras covariables como efectos fijos y efectos aleatorios. Los efectos fijos se asocian al número de habitaciones, baños y ascensor. Los efectos aleatorios, a la combinación entre distrito y calidad constructiva. El modelo, por tanto, se especifica de la siguiente manera:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{form }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(UNITPRICE) }\SpecialCharTok{\textasciitilde{}} \SpecialCharTok{{-}}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ Intercept }\SpecialCharTok{+}
\NormalTok{  ROOMNUMBER }\SpecialCharTok{+}
\NormalTok{  BATHNUMBER }\SpecialCharTok{+}
\NormalTok{  HASLIFT }\SpecialCharTok{+}
  \FunctionTok{f}\NormalTok{(district, CADASTRALQUALITYID, }\AttributeTok{model =} \StringTok{"iid"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{f}\NormalTok{(spatial.field, }\AttributeTok{model =}\NormalTok{ spde)}
\end{Highlighting}
\end{Shaded}

Como se puede observar, se utilizan como variables explicativas del modelo hedónico el número de habitaciones, número de baños y si la finca cuenta con ascensor. Estas variables se introducen como efectos fijos en el modelo lo que significa que este efecto se calcula de forma general para todo el ámbito de estudio, la ventaja de utilizar efectos fijos es interpretabilidad, dado que es conocida la distribución de cada uno de los coeficientes. La desventaja es que se asume que la contribución del número de los efectos fijos es constante para toda la ciudad, lo que no es necesariamente cierto. Este supuesto puede relajarse en aquellos casos que el investigador considere conveniente, introduciendo un efecto aleatorio asociado a la variable oportuna.

Por otra parte se ha incluido un término de efectos aleatorios asociados a distrito y calidad constructiva, el motivo de hacerlo así es equivalente a el uso de ``\emph{dummies} de zonas'' con un desglose por calidad de la construcción. Por tanto este modelo capturará el efecto de estas dos variables en todo el conjunto, estima los efectos fijos de los atributos de los inmuebles y ajusta a través del modelo espacial la contribución de los precios debida a la ubicación. Por este motivo se puede considerar en este modelo que la contribución espacial es compartida por el modelo \emph{spde} y por los códigos de distrito. El modelo espacial puede extenderse para incorporar el aspecto temporal, para más información ver \citep{moraga2019geospatial}.

Para ajustar el modelo se utiliza la función \texttt{inla}, y dado que, se incluye en el \emph{stack} el set para la inferencia, se calculan los datos para la malla rectangular definida.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 }\OtherTok{\textless{}{-}} \FunctionTok{inla}\NormalTok{(form,}
  \AttributeTok{data =} \FunctionTok{inla.stack.data}\NormalTok{(join.stack, }\AttributeTok{spde =}\NormalTok{ madrid.spde),}
  \AttributeTok{family =} \StringTok{"gaussian"}\NormalTok{,}
  \AttributeTok{control.predictor =} \FunctionTok{list}\NormalTok{(}
    \AttributeTok{A =} \FunctionTok{inla.stack.A}\NormalTok{(join.stack),}
    \AttributeTok{compute =} \ConstantTok{TRUE}
\NormalTok{  ),}
  \AttributeTok{control.compute =} \FunctionTok{list}\NormalTok{(}\AttributeTok{cpo =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{dic =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A continuación, se muestra un descriptivo del modelo ajustado, por una parte se tiene el valor de la intersección (\emph{INTERCEPT}) y los coeficientes asociados a los efectos fijos. La salida completa del modelo se puede obtener mediante el comando \texttt{summary(m1)}. Se observa que el coeficiente para \emph{ROOMNUMBER} toma un valor negativo, debido a que, a igualdad de condiciones, el precio por unidad de superficie es mayor en pisos pequeños que en pisos grandes. En los casos de \emph{BATHNUMBER} y \emph{HASLIFT} se observan contribuciones positivas al precio unitario de la vivienda.

El descriptivo también muestra el efecto aleatorio asociado a la combinación entre distrito y calidad constructiva, y al efecto espacial. Los valores de la medida DIC recogen una medida de la calidad del modelo\footnote{ La medida DIC es una generalización del Criterio de Información de Akaike (AIC), son particularmente útiles en modelos bayesianos y representan el compromiso entre la bondad del ajuste y complejidad del modelo.}.

\begin{center}\includegraphics[width=0.6\linewidth]{img/table-idealista} \end{center}

Para mostrar el efecto aleatorio (más la intersección), se toma la media de la posterior de la suma de los valores ajustados. Sobre las predicciones en \(madrid.pred\), se crea un \emph{dataframe} con la media y la desviación estándar transformándolos a la escala original de precios por superficie.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{index.pred }\OtherTok{\textless{}{-}} \FunctionTok{inla.stack.index}\NormalTok{(join.stack, }\StringTok{"madrid.pred"}\NormalTok{)}\SpecialCharTok{$}\NormalTok{data}

\NormalTok{tab }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{SPDE =} \FunctionTok{exp}\NormalTok{(m1}\SpecialCharTok{$}\NormalTok{summary.fitted.values[index.pred, }\StringTok{"mean"}\NormalTok{]),}
  \AttributeTok{SPDE\_SD =} \FunctionTok{exp}\NormalTok{(m1}\SpecialCharTok{$}\NormalTok{summary.fitted.values[index.pred, }\StringTok{"sd"}\NormalTok{])}
\NormalTok{)}
\CommentTok{\# Creamos objecto sf}
\NormalTok{madrid\_grid2 }\OtherTok{\textless{}{-}} \FunctionTok{st\_sf}\NormalTok{(}\AttributeTok{data =}\NormalTok{ tab, }\AttributeTok{geom =}\NormalTok{ madrid\_grid)}
\end{Highlighting}
\end{Shaded}

A continuación, se muestran las medias y desviaciones estándar de las distribuciones posteriores. Se puede observar como el modelo captura el efecto espacial de los precios en la ciudad de Madrid.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(madrid\_grid2[}\StringTok{"data.SPDE"}\NormalTok{],}
  \AttributeTok{main =} \StringTok{" "}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/media-posterior} 

}

\caption{Media posterior}\label{fig:plot-media-posterior}
\end{figure}

Conocer la distribución espacial de la desviación estándar es útil para conocer dónde las estimaciones del modelo tienen mayor incertidumbre. Habitualmente la cuestión está relacionada con la disponibilidad de información en la zona, en la figura se observa como las zonas del norte de la ciudad, donde hay menos viviendas en venta, tienen un mayor nivel de variabilidad, al contrario de lo que sucede en torno al centro (Sol) donde la variabilidad es menor.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(madrid\_grid2[}\StringTok{"data.SPDE\_SD"}\NormalTok{], }\AttributeTok{main =} \StringTok{" "}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(madrid\_comb, }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/sd-posterior} 

}

\caption{Desviación estándar modelo INLA}\label{fig:fig-sd-posterior}
\end{figure}

Como se observa en los resultados obtenidos, la estimación de modelos hedónicos usando aproximaciones bayesianas es muy interesante dado que permite estimar distribuciones de los parámetros de los modelos así como de las predicciones. De esta forma, los resultados permiten analizar la precisión de las estimaciones. Por otro lado, el empleo del método de computación INLA permite la aplicación de modelos complejos en número de parámetros a conjuntos de datos grandes.

\hypertarget{cap-rfm}{%
\chapter{Segmentación de clientes en retail}\label{cap-rfm}}

\emph{Jaime Fierro Martín, Rocío González Martínez y Cristina Sánchez
Figueroa}

\hypertarget{motivaciuxf3n-y-conceptos-clave}{%
\section{Motivación y conceptos clave}\label{motivaciuxf3n-y-conceptos-clave}}

Los comercios minoristas (\emph{retailers}) se mueven en un entorno
turbulento y necesitan acercarse a sus clientes para asegurar su
supervivencia. Su producto, o servicio, es nexo clave en dicho proceso.
En este contexto, conocer el \textbf{perfil de los
clientes}\index{perfil de los clientes} permitirá detectar en qué
momento de su ciclo de vida con la empresa se encuentran y desarrollar
propuestas de valor que convengan en cada momento.

\textbf{Segmentar} \index{segmentar}se define como el proceso de dividir a
los clientes actuales o potenciales, en diferentes grupos o segmentos
consistentes en individuos con características y niveles similares de
interés. Es un proceso creativo e iterativo con el fin de satisfacer con
mayor acierto las necesidades de los clientes, proporcionando una
ventaja competitiva y sostenible a la companía. La segmentación viene
dada por las necesidades de los clientes, no de la compañia, y debería
ser revisada periódicamente.

Este caso práctico de negocio está basado en un proyecto real impulsado
por el departamento de marketing de una empresa del \emph{sector retail} que
necesitaba mejorar el conocimiento de sus clientes, agrupándolos en
función de su comportamiento de compra. Los resultados obtenidos fueron
clave para definir la estrategia de marketing relacional de la compañía.

\hypertarget{del-modelo-rfm-tradicional-al-modelo-rfm-extendido-mejoras-propuestas}{%
\section{Del Modelo RFM tradicional al Modelo RFM extendido, mejoras propuestas}\label{del-modelo-rfm-tradicional-al-modelo-rfm-extendido-mejoras-propuestas}}

El \textbf{Modelo RFM}\index{Modelo RFM} es una técnica popular que se
utiliza para analizar el comportamiento de compra de los clientes: cómo
compran, su frecuencia de compra y cuánto gastan. Es un método útil para
enriquecer la segmentación de los clientes en varios grupos que permitan
la personalización e identificación de los clientes más proclives a
responder a las promociones. El análisis RFM depende de las medidas de
actualidad,\textbf{(\emph{recency})}\index{recency}\textbf{(R)}, frecuencia,
\textbf{(\emph{frequency})}\index{frequency}\textbf{(F)} y mometario,
\textbf{(\emph{monetary})}\index{monetary}\textbf{(M)}, que son tres importantes
variables relacionadas con la compra que influyen en las posibilidades
de compra futura de los clientes.

El \textbf{Modelo RFM tradicional} categoriza el valor de las variables
dividiéndolas en quintiles, a partir de los cuales se calcula una
puntuación única que representa el valor del cliente. Sin embargo, no es
muy preciso. Si el intervalo de frecuencia de compras se fija entre 0 y
20, en términos de negocio podría interpretarse como que un cliente con
una sola compra será igual que otro que tenga 20. Con este sencillo
ejemplo comprobamos que los enfoques de conjuntos clásicos pueden
resultar poco funcionales \citep{martinez2019comparison}. Por ello, se
propone una mejora en la definición de los intervalos mediante la
aplicación del \textbf{ranking de percentiles}\index{ranking de percentiles}.
Este método proporciona un método robusto para tratar los valores
atípicos (\emph{outliers}), y además normaliza las variables entre 0 y 1 para
evitar la diferencias de peso entre las variables, permitiendo así la
correcta implementación del \textbf{algoritmo de segmentación}.

\emph{Ranking de percentiles cliente}
\(x_i =\frac{Número\:de\: valores\: inferiores\: a\: x_i}{Número\: total\: de\: clientes}\)

\hypertarget{modelo-rfm-extendido}{%
\section{Modelo RFM extendido}\label{modelo-rfm-extendido}}

Los autores de este caso práctico recomiendan seguir una metodología de
gestión de proyectos. La \textbf{Metodología
CRISP-DM}\index{Metodología CRISP-DM}\citep{chapman2000crisp} es un estándar
ampliamente utilizado en los proyectos de ciencia de datos.

Una vez definido el problema , la recopilación y comprensión de los
datos se establece como etapa esencial para el desarrollo del proyecto.

\hypertarget{recopilaciuxf3n-y-comprensiuxf3n-de-los-datos}{%
\subsection{Recopilación y comprensión de los datos}\label{recopilaciuxf3n-y-comprensiuxf3n-de-los-datos}}

Hoy en día la mayoría de las empresas de \emph{e-commerce} y comercio
minorista tradicional cuentan con sistemas que permiten registrar los
datos básicos de cada una de sus ventas (fecha, artículo, cantidad e
importe), asociados a un código único de cliente. La información
contenida en estos datos de compra atesora gran valor, ya que carecen
del sesgo y subjetividad propias de otras informaciones obtenidas
mediante encuestas de opinión, estudios de mercado, entrevistas y grupos
de discusión, etc. Estos datos suelen encontrarse en las plataformas ERP
(Enterprise Resource Planning) de gestión de pedidos y ventas, o CRM
(Customer Relationship Management) de las empresas.

El lector es, o será, consciente de que la fase de extracción, carga y
limpieza de los datos es la más exigente del proyecto, y donde se
empleará gran parte de los recursos y tiempo de todo el proyecto. \textbf{R}
cuenta con gran cantidad de paquetes y recursos que facilitan la
extracción desde diferentes tipos de bases de datos, a modo meramente
ilustrativo mencionamos \href{https://rsqlite.r-dbi.org/}{RSQLite},
\href{https://github.com/r-dbi/odbc}{odbc},
\href{https://github.com/jeroen/mongolite/}{mogolite},
\href{https://httr.r-lib.org/}{httr}, entre otras muchas.

Para este caso práctico serán necesarias las siguientes librerías:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lubridate)}
\FunctionTok{library}\NormalTok{(factoextra)}
\FunctionTok{library}\NormalTok{(ggpubr)}
\FunctionTok{library}\NormalTok{(CDR)}
\FunctionTok{data}\NormalTok{(}\StringTok{"datos\_retail"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Cualquier tipo de estudio o proyecto de ciencia de datos requiere
familiarizarse con los datos y determinar si presentan suficiente
exactitud, completitud, consistencia, credibilidad y
actualidad\citep{munoz2018calidad}. Los datos de transacciones de venta
registrados por las empresas pueden contener datos atípicos (p.ej.
valores perdidos, inexactos, outliers, etc.). Para determinar la acción
a tomar, o no, de limpieza o corrección de los datos de partida, es
esencial conocer el negocio y las consecuencias que éstas operaciones
tendrán en el resultado final de la segmentación.

El conjunto de datos de muestra contiene 200 000 observaciones
correspondientes a transacciones de compra. Las siguientes variables
iniciales están explicadas en el set de datos:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(datos\_retail)}
\CommentTok{\#\textgreater{} \# A tibble: 6 x 4}
\CommentTok{\#\textgreater{} id\_ticket fecha importe\_venta codigo\_socio}
\CommentTok{\#\textgreater{} \textless{}chr\textgreater{} \textless{}date\textgreater{} \textless{}dbl\textgreater{} \textless{}chr\textgreater{}}
\CommentTok{\#\textgreater{} 1 num\_1646673 2021{-}10{-}30 12.4 id\_1076134}
\CommentTok{\#\textgreater{} 2 num\_2762559 2021{-}12{-}03 38.8 id\_0552641}
\CommentTok{\#\textgreater{} 3 num\_0309422 2022{-}01{-}07 67.8 id\_0537369}
\CommentTok{\#\textgreater{} 4 num\_2113698 2021{-}08{-}27 37.2 id\_0814765}
\CommentTok{\#\textgreater{} 5 num\_0794486 2020{-}11{-}21 44.2 id\_0494840}
\CommentTok{\#\textgreater{} 6 num\_0184609 2021{-}05{-}22 12.0 id\_0214173}
\end{Highlighting}
\end{Shaded}

\hypertarget{cuxe1lculo-de-las-variables-del-modelo-rfm}{%
\subsection{Cálculo de las variables del Modelo RFM}\label{cuxe1lculo-de-las-variables-del-modelo-rfm}}

Identificadas las variables iniciales, es necesario calcular las
variables del Modelo RFM:

\begin{itemize}
\item
  La variable actualidad, \emph{recency} (R), es el intervalo de tiempo
  transcurrido desde la compra más reciente por un cliente hasta la
  fecha de elaboración del modelo RFM.
\item
  La variable frecuencia, \emph{frequency} (F), se obtiene agrupando las
  compras por cliente y contando el número total de tickets únicos.
\item
  La variable monetario, \emph{monetary} (M), se calcula sumando todos los
  importes de venta por cliente.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fecha\_estudio\_rfm }\OtherTok{\textless{}{-}} \FunctionTok{ymd}\NormalTok{(}\StringTok{"2022{-}08{-}01"}\NormalTok{)}

\NormalTok{rfm }\OtherTok{\textless{}{-}}\NormalTok{ datos\_retail }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(codigo\_socio) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{frecuencia =} \FunctionTok{n\_distinct}\NormalTok{(id\_ticket),}
    \AttributeTok{monetario =} \FunctionTok{sum}\NormalTok{(importe\_venta, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{fecha\_transaccion\_reciente =} \FunctionTok{first}\NormalTok{(fecha, }\AttributeTok{order\_by =} \FunctionTok{desc}\NormalTok{(fecha))}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{actualidad =} \FunctionTok{time\_length}\NormalTok{(}\FunctionTok{interval}\NormalTok{(}\AttributeTok{start =}\NormalTok{ fecha\_transaccion\_reciente, }\AttributeTok{end =}\NormalTok{ fecha\_estudio\_rfm), }\AttributeTok{unit =} \StringTok{"days"}\NormalTok{), }\AttributeTok{.keep =} \StringTok{"unused"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(rfm) }\CommentTok{\# el lector puede ver las variables del Modelo RFM}
\end{Highlighting}
\end{Shaded}

\hypertarget{breve-anuxe1lisis-exploratorio-de-las-variables-del-modelo-rfm}{%
\subsection{Breve análisis exploratorio de las variables del Modelo RFM}\label{breve-anuxe1lisis-exploratorio-de-las-variables-del-modelo-rfm}}

Del análisis de las variables obtenidas puede concluirse que:

\begin{itemize}
\item
  107 929 clientes únicos han realizado una media de 1.85 compras, con
  un importe medio total de 70.56€ y 450.4 días de media desde la
  última compra hasta la fecha de realización del estudio, con una
  fuerte asimetría positiva de los valores \emph{frequency} y \emph{monetary}.
\item
  Se detecta una gran estacionalidad de las compras, como se puede
  apreciar en la agrupación de las observaciones de \emph{recency}.
  Teniendo en cuenta la fecha fijada del análisis, los valores
  obtenidos en la variable \emph{recency,} se pueden interpretar como el
  periodo de ventas de la campaña navideña.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{12345}\NormalTok{)}
\NormalTok{plot\_data }\OtherTok{\textless{}{-}}\NormalTok{ rfm }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{2000}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pivot\_longer}\NormalTok{(}\SpecialCharTok{!}\NormalTok{codigo\_socio, }\AttributeTok{names\_to =} \StringTok{"variable"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"valor"}\NormalTok{)}
\NormalTok{plot\_data }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ variable, }\AttributeTok{y =}\NormalTok{ valor)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{outlier.shape =} \ConstantTok{NA}\NormalTok{, }\AttributeTok{color =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{alpha =} \DecValTok{1} \SpecialCharTok{/} \DecValTok{10}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{variable, }\AttributeTok{ncol =} \DecValTok{6}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{strip.text.x =} \FunctionTok{element\_blank}\NormalTok{(), }\AttributeTok{text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{9}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/Rplot01} 

}

\caption{Box-plot de variables RFM}\label{fig:imagen1}
\end{figure}

\hypertarget{cuxe1lculo-del-ranking-de-percentiles}{%
\subsection{Cálculo del Ranking de percentiles}\label{cuxe1lculo-del-ranking-de-percentiles}}

Los valores de ranking son relativos entre clientes y no pueden ser
utilizados para objetivos de negocio, basados en valores absolutos de
puntuación por cliente.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rfm\_rank }\OtherTok{\textless{}{-}}\NormalTok{ rfm }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\AttributeTok{.cols =} \FunctionTok{c}\NormalTok{(}\StringTok{"frecuencia"}\NormalTok{, }\StringTok{"monetario"}\NormalTok{), percent\_rank, }\AttributeTok{.names =} \StringTok{"rank\_\{.col\}"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\AttributeTok{.cols =} \FunctionTok{c}\NormalTok{(}\StringTok{"actualidad"}\NormalTok{), }\SpecialCharTok{\textasciitilde{}} \FunctionTok{percent\_rank}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{.x), }\AttributeTok{.names =} \StringTok{"rank\_\{.col\}"}\NormalTok{)) }\CommentTok{\# menor recency indica mayor puntuación en rank}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(rfm\_rank) }\CommentTok{\# el lector puede ver la puntuación del ranking}
\end{Highlighting}
\end{Shaded}

\hypertarget{modelado-rfm-mediante-k-means}{%
\subsection{Modelado: RFM mediante k-means}\label{modelado-rfm-mediante-k-means}}

El modelo establecido debe proporcionar una segmentación de clientes con
sentido de negocio. En este caso práctico se opta por el algoritmo de
clustering estándar
\href{https://stat.ethz.ch/R-manual/R-devel/library/stats/html/kmeans.html}{k-means}\index{k-means}
que presenta la ventaja de ser muy intuitivo y permite trabajar con
grandes conjuntos de datos. Como el lector ha podido comprobar, existen
otros muchos algoritmos de aprendizaje no supervisado que pueden ser
empleados.

El número óptimo de \emph{clusters} es uno de los retos a la hora de aplicar
técnicas de \emph{clustering}. No existe una manera exclusiva de encontrar el
número adecuado de clusters. Se trata de un proceso subjetivo que
depende de los datos, del tipo de \emph{clustering} empleado y, en este caso,
de que el número elegido tenga sentido y utilidad en el negocio. Existen
numerosos métodos para facilitar la elección del número de \emph{clusters},
entre ellos destacan \emph{Elbow method, Average silhouette method y Gap
statistic method}, que gracias a la función \texttt{fviz\_nbclust()} del paquete
\texttt{factoextra} se pueden calcular con facilidad para realizar una buena
elección.

El número óptimo de \emph{clusters} se calcula:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{muestra\_clusters }\OtherTok{\textless{}{-}}\NormalTok{ rfm\_rank }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{5000}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\FunctionTok{matches}\NormalTok{(}\StringTok{"rank"}\NormalTok{))}

\FunctionTok{fviz\_nbclust}\NormalTok{(}\AttributeTok{x =}\NormalTok{ muestra\_clusters, }\AttributeTok{FUNcluster =}\NormalTok{ kmeans, }\AttributeTok{method =} \StringTok{"wss"}\NormalTok{, }\AttributeTok{k.max =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/Rplot02} 

}

\caption{Número óptimo de clusters}\label{fig:imagen2}
\end{figure}

Mediante el \emph{método Elbow} \index{método Elbow}se observa que la
varianza total \emph{intra-cluster} apenas mejora a partir del cuarto
\emph{cluster}.

El algoritmo de clustering k-means se entrena con las variables R-F-M
normalizadas con el ranking. La salida de la función \texttt{kmeans()} del
paquete base \texttt{stats} es un objeto que entre otros componentes ofrece un
vector numérico indicativo del \emph{clusters} al que pertenece cada uno de
los clientes.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{km\_fit }\OtherTok{\textless{}{-}} \FunctionTok{kmeans}\NormalTok{(}\AttributeTok{x =}\NormalTok{ rfm\_rank[, }\DecValTok{5}\SpecialCharTok{:}\DecValTok{7}\NormalTok{], }\AttributeTok{centers =} \DecValTok{4}\NormalTok{, }\AttributeTok{nstart =} \DecValTok{10}\NormalTok{)}

\NormalTok{clientes\_segmentos }\OtherTok{\textless{}{-}}\NormalTok{ rfm\_rank }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{segmento =}\NormalTok{ km\_fit}\SpecialCharTok{$}\NormalTok{cluster)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(clientes\_segmentos) }\CommentTok{\# el lector puede ver el segmento al que pertence el cliente}
\end{Highlighting}
\end{Shaded}

\hypertarget{descriptivos-e-interpretaciuxf3n-de-los-segmentos}{%
\subsection{Descriptivos e interpretación de los segmentos}\label{descriptivos-e-interpretaciuxf3n-de-los-segmentos}}

Los segmentos obtenidos deben tener sentido y utilidad de negocio. Para
ello es imprescindible realizar un el descriptivo de cada segmento y
proceder a su interpretación de perfil de cliente.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{descriptivo\_segmentos }\OtherTok{\textless{}{-}}\NormalTok{ clientes\_segmentos }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(segmento) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"monetario"}\NormalTok{, }\StringTok{"frecuencia"}\NormalTok{, }\StringTok{"actualidad"}\NormalTok{),}
    \AttributeTok{.fns =}\NormalTok{ mean, }\AttributeTok{.names =} \StringTok{"md\_\{.col\}"}
\NormalTok{  ), }\AttributeTok{n\_clientes =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{relocate}\NormalTok{(segmento, n\_clientes)}

\FunctionTok{head}\NormalTok{(descriptivo\_segmentos)}
\CommentTok{\#\textgreater{} \# A tibble: 4 x 5}
\CommentTok{\#\textgreater{} segmento n\_clientes md\_monetario md\_frecuencia md\_actualidad}
\CommentTok{\#\textgreater{} \textless{}int\textgreater{} \textless{}int\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 1 23551 36.2 1.07 239.}
\CommentTok{\#\textgreater{} 2 2 23632 77.0 2.26 567.}
\CommentTok{\#\textgreater{} 3 3 28809 128. 3.11 188.}
\CommentTok{\#\textgreater{} 4 4 31937 39.3 1.00 757.}
\end{Highlighting}
\end{Shaded}

Interpretación de los segmentos:

\begin{itemize}
\tightlist
\item
  1-\emph{Nuevos probando}: segmento que agrupa nuevos clientes que están
  realizando compras desde hace poco tiempo y tienen un gran potencial
  de desarrollo. Es un segmento de clientes con interés para la
  empresa.
\item
  2-\emph{No podemos perder}: se trata de los clientes `churn' que fueron
  buenos clientes en términos monetarios y de frecuencia pero que hace
  tiempo que no realizan nuevas compras. La compañía debe dedicar
  esfuerzos en recuperar estos clientes para convertirlos al segmento
  TOP.
\item
  3-\emph{Top}: reúne los mejores clientes de la empresa. Son clientes que
  compran con frecuencia, están activos y aportan ventas a la
  compañía. Es el segmento de clientes con mayor interés para la
  empresa.
\item
  4-\emph{Una compra}: segmento formado por aquellos clientes que han
  realizado una sola compra hace tiempo. Presentan frecuencia,
  actualidad y valor monetario bajo. Se trata de un segmento de
  clientes con escaso interés para la compañía.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{segmentos\_descriptivo }\OtherTok{\textless{}{-}}\NormalTok{ clientes\_segmentos }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{segmento =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    segmento }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \StringTok{"1\_Nuevos probando"}\NormalTok{,}
\NormalTok{    segmento }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \StringTok{"2\_No perder"}\NormalTok{,}
\NormalTok{    segmento }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \StringTok{"3\_Top"}\NormalTok{,}
\NormalTok{    segmento }\SpecialCharTok{==} \DecValTok{4} \SpecialCharTok{\textasciitilde{}} \StringTok{"4\_Una compra"}
\NormalTok{  )) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(segmento) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}
    \FunctionTok{across}\NormalTok{(}
      \AttributeTok{.cols =} \FunctionTok{where}\NormalTok{(is.numeric),}
      \AttributeTok{.fns =}\NormalTok{ mean}
\NormalTok{    ),}
    \AttributeTok{n\_clientes =} \FunctionTok{n}\NormalTok{()}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{relocate}\NormalTok{(segmento, n\_clientes)}

\NormalTok{table\_dot\_plot }\OtherTok{\textless{}{-}}\NormalTok{ segmentos\_descriptivo }\SpecialCharTok{|\textgreater{}}
  \CommentTok{\# select(starts\_with("rank")) |\textgreater{}}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(}\StringTok{"rank\_monetario"}\NormalTok{, }\StringTok{"rank\_frecuencia"}\NormalTok{, }\StringTok{"rank\_actualidad"}\NormalTok{), }\AttributeTok{names\_to =} \StringTok{"Variable RFM"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Puntuación"}\NormalTok{)}

\FunctionTok{ggdotchart}\NormalTok{(}
\NormalTok{  table\_dot\_plot,}
  \AttributeTok{x =} \StringTok{"Variable RFM"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Puntuación"}\NormalTok{,}
  \AttributeTok{group =} \StringTok{"segmento"}\NormalTok{, }\AttributeTok{color =} \StringTok{"segmento"}\NormalTok{, }\AttributeTok{palette =} \StringTok{"jco"}\NormalTok{,}
  \AttributeTok{add =} \StringTok{"segment"}\NormalTok{, }\AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{(}\FloatTok{0.3}\NormalTok{),}
  \AttributeTok{sorting =} \StringTok{"none"}\NormalTok{, }\AttributeTok{facet.by =} \StringTok{"segmento"}\NormalTok{, }\AttributeTok{dot.size =} \DecValTok{5}\NormalTok{,}
  \AttributeTok{rotate =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{legend =} \StringTok{"none"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/Rplot03} 

}

\caption{Box-plot de variables RFM}\label{fig:imagen3}
\end{figure}

\hypertarget{puesta-en-producciuxf3n}{%
\subsection{Puesta en producción}\label{puesta-en-producciuxf3n}}

Calculado el modelo RFM k-means la compañía puede incorporan
periódicamente los datos de los clientes nuevos o actualizados. De este
modo los segmentos de clientes se actualizarán y, más allá de las
acciones de \emph{marketing mix} que realicen las compañías gracias a la
segmentación, podrán analizarse las migraciones de clientes entre los
diferentes segmentos en el periodo estudiado. La función \texttt{cl\_predict()}
del paquete en
\href{https://cran.r-project.org/web/packages/clue/index.html}{\texttt{R\ clue:\ Cluster\ Ensembles}}
facilita realizar la actualización periódica de los segmentos con el
modelo entrenado.

\hypertarget{cap-medicina}{%
\chapter{Ciencia de datos para la investigación y ensayos clínicos}\label{cap-medicina}}

Autores: \emph{Alberto M. Borobia y María Jiménez-González}

\hypertarget{justificaciuxf3n}{%
\section{Justificación}\label{justificaciuxf3n}}

La aplicación de la estadística en la investigación clínica ha sido una de las herramientas clave en los últimos dos años. La pandemia mundial causada por la enfermedad por coronavirus (COVID‑19) es una enfermedad infecciosa provocada por el virus SARS-CoV-2. Drante el año 2020, más de 13 millones de casos diagnósticados en España arrojaban un diagnóstico claro: se necesita más investigación.

El objetivo de este capítulo es construir un esquema sobre el uso de datos en investigación clínica y ensayos clínicos a través de teoría y ejemplos prácticos.

\hypertarget{introducciuxf3n-al-uso-de-datos-en-investigaciuxf3n-cluxednica-y-ensayos-cluxednicos}{%
\section{Introducción al uso de datos en investigación clínica y ensayos clínicos}\label{introducciuxf3n-al-uso-de-datos-en-investigaciuxf3n-cluxednica-y-ensayos-cluxednicos}}

\hypertarget{quuxe9-es-un-ensayo-cluxednico}{%
\subsection{¿Qué es un ensayo clínico?}\label{quuxe9-es-un-ensayo-cluxednico}}

En la investigación clínica, existen dos tipos de estudios: \textbf{estudios observacionales} y \textbf{ensayos clínicos}. Los ensayos clínicos aleatorios se definen como el diseño experimental óptimo para proporcionar evidencia, eficacia y seguridad de una intervención \citep{Lium3164}. Los tratamientos estudiados o investigados son asignados aleatoriamente en grupos que garantizan que las diferencias en los resultados después del tratamiento reflejen los efectos del mismo \citep{rosenbaum2005observational}. Cuando estas condiciones ideales no son posibles (falta de recursos, financiación, tiempo, etc), se definen como estudios observacionales.

Previo a la puesta en marcha de un ensayo clínico, es imprescindible la redacción de un \emph{Protocolo} y un \emph{Plan de Análisis Estadístico (PAE)}.

\begin{itemize}
\item
  El protocolo, elaborado por los investigadores del estudio, precisa y justifica los métodos y planes del proceso que se llevará a cabo en el ensayo clínico \citep{CRUZRIVERA2020e549}.
\item
  El PAE detalla las características principales del eventual análisis estadístico de los datos deben describirse en la sección estadística del protocolo \citep{lewis1999statistical}.
\end{itemize}

Los documentos anteriormente mencionados y el resto de directrices necesarias para un ensayo clínico están regulados por la ``Conferencia Internacional sobre armonización de requisitos técnicos para el registro de productos farmacéuticos para uso humano'' (ICH).

\hypertarget{limitaciones-de-los-estudios-observacionales}{%
\subsection{Limitaciones de los estudios observacionales}\label{limitaciones-de-los-estudios-observacionales}}

En el apartado anterior, se pone de manifiesto la importancia de los ensayos clínicos aleatorizados. Sin embargo, la posible falta de recursos, financiación, tiempo o materiales, dificultan la puesta en marcha y realización de los mismos.

En consecuencia, la puesta en práctica de la investigación puede no ser la ideal. Los estudios observacionales, sin embargo, son una herramienta elemental en circunstancias no tan óptimas, ya que permiten analizar e investigar (contra viento y marea).

La limitación principal de los estudios observacionales es la introducción del \emph{sesgo}. Los ensayos clínicos tienen como principal objetivo eleminar el \emph{sesgo de selección}: cuando los sujetos no son asignados aleatoriamente, por ejemplo, los resultados diferentes pueden reflejar estas diferencias iniciales en lugar de los efectos de los tratamientos \citep{rosenbaum2005observational}.

\hypertarget{propensity-score}{%
\subsection{Propensity Score}\label{propensity-score}}

Una solución aconsejable y recomendada ante los sesgos ``escondidos'' en los estudios observacionales es la técnica \emph{propensity Score} o la puntuación de propensión. Esta técnica de emparejamiento equilibra las covariables observadas sesgadas ajustando por su puntuación de propensión, eliminando presumiblemente el sesgo.

\hypertarget{ejemplo-pruxe1ctico-de-un-estudio-observacional}{%
\section{Ejemplo práctico de un estudio observacional}\label{ejemplo-pruxe1ctico-de-un-estudio-observacional}}

El \emph{dataset} sintético \texttt{datos\_observacional} reproduce los datos de un hipotético estudio observacional sobre una enfermedad \textbf{X}. El objetivo del estudio es determinar los factores de riesgos asociados a la mortalidad causada por esa enfermedad.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(readxl)}
\NormalTok{datos\_observacional }\OtherTok{\textless{}{-}} \FunctionTok{read\_excel}\NormalTok{(}\StringTok{"data/datos\_observacional.xlsx"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(datos\_observacional, }\DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 8}
\CommentTok{\#\textgreater{}      ID fecha\_hospitalizacion sexo    edad comorbilidades fecha\_alta exitus}
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{} \textless{}dttm\textgreater{}                \textless{}chr\textgreater{}  \textless{}dbl\textgreater{} \textless{}chr\textgreater{}          \textless{}chr\textgreater{}       \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1     1 2015{-}04{-}17 00:00:00   Mujer     76 1 o más        17/04/2020      1}
\CommentTok{\#\textgreater{} 2     2 2015{-}03{-}21 00:00:00   Mujer     64 1 o más        31/03/2020      0}
\CommentTok{\#\textgreater{} 3     3 2015{-}04{-}09 00:00:00   Hombre    65 1 o más        16/04/2020      0}
\CommentTok{\#\textgreater{} 4     4 2015{-}04{-}04 00:00:00   Hombre    77 1 o más        13/04/2020      0}
\CommentTok{\#\textgreater{} 5     5 2015{-}03{-}24 00:00:00   Mujer     66 1 o más        27/03/2020      0}
\CommentTok{\#\textgreater{} \# ... with 1 more variable: fecha\_exitus \textless{}dttm\textgreater{}}
\end{Highlighting}
\end{Shaded}

Anteriormente, se ha evidenciado que las mujeres de mayor edad y con 1 o más comorbilidades tienen más riesgo de fallecer por la enferdad \textbf{X}.

En investigación, la estructura de los resultados en un \emph{paper} o en un informe estadístico, independientemente de la revista o PAE, comienza en el mismo punto: una tabla resumen de las características basales de la población de estudio. El paquete \emph{tableone} integra funciones específicas para la creación de esas tablas. La función principal de este paquete es \texttt{CreateTableOne()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tableone)}

\NormalTok{myVars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"sexo"}\NormalTok{, }\StringTok{"edad"}\NormalTok{, }\StringTok{"comorbilidades"}\NormalTok{)}
\NormalTok{nonnormal }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"edad"}\NormalTok{)}
\NormalTok{factorVars }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"sexo"}\NormalTok{, }\StringTok{"comorbilidades"}\NormalTok{)}
\NormalTok{tab1 }\OtherTok{\textless{}{-}} \FunctionTok{CreateTableOne}\NormalTok{(}
  \AttributeTok{vars =}\NormalTok{ myVars, }\AttributeTok{factorVars =}\NormalTok{ factorVars,}
  \AttributeTok{strata =} \StringTok{"exitus"}\NormalTok{, }\AttributeTok{data =}\NormalTok{ datos\_observacional}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Muestra la tabla con código \textbf{R} (se omite la salida por simplicidad)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tab1 }\OtherTok{\textless{}{-}} \FunctionTok{print}\NormalTok{(tab1,}
  \AttributeTok{showAllLevels =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{formatOptions =} \FunctionTok{list}\NormalTok{(}\AttributeTok{big.mark =} \StringTok{","}\NormalTok{),}
  \AttributeTok{exact =} \StringTok{"stage"}\NormalTok{, }\AttributeTok{nonnormal =}\NormalTok{ nonnormal}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Presenta la tabla de resultados formateada:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(tab1,}
  \AttributeTok{caption =} \StringTok{"Características basales de la población"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:table}Características basales de la población}
\centering
\begin{tabular}[t]{l|l|l|l|l|l}
\hline
  & level & 0 & 1 & p & test\\
\hline
n &  & 79 & 21 &  & \\
\hline
sexo (\%) & Hombre & 28 (35.4) & 2 ( 9.5) & 0.042 & \\
\hline
 & Mujer & 51 (64.6) & 19 (90.5) &  & \\
\hline
edad (median [IQR]) &  & 64.00 [53.00, 73.00] & 82.00 [72.00, 85.00] & <0.001 & nonnorm\\
\hline
comorbilidades (\%) & 1 o más & 43 (54.4) & 18 (85.7) & 0.018 & \\
\hline
 & No & 36 (45.6) & 3 (14.3) &  & \\
\hline
\end{tabular}
\end{table}

En la Tabla \ref{table}, se confirma el sesgo y el desequilibrio existente en las variables \emph{sexo, edad y comorbilidades}, por lo que se procede al ajuste de la técnica \emph{propensity score}.

El paquete \texttt{MatchIt} integra las funciones principales para el ajuste de la técnica:

\begin{itemize}
\item
  La función \texttt{MatchIt()} integra la teoría de \citep{ho2007matching} para el emparejamiento óptimo de los grupos estudiados. Los argumentos más importantes de esta función son:

  \begin{itemize}
  \tightlist
  \item
    \texttt{formula}: modelo de regresión que estudia la relación entre la variable principal de estudio (exitus) con las variables sesgadas (sexo, edad y comorbilidades).
  \item
    \texttt{method}: especifica el método de \emph{matching}.
  \item
    \texttt{distance}: especifica el método para la estimación de la puntuación de propensión.
  \end{itemize}
\item
  La función \texttt{get\_matches}, empareja las coincidencias que resultan del \texttt{MatchIt}.
\end{itemize}

\emph{Nota:} es imprescindible que los casos del \emph{dataset} estén completos.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MatchIt)}
\NormalTok{match }\OtherTok{\textless{}{-}} \FunctionTok{matchit}\NormalTok{(exitus }\SpecialCharTok{\textasciitilde{}}\NormalTok{ edad }\SpecialCharTok{+} \FunctionTok{as.factor}\NormalTok{(sexo) }\SpecialCharTok{+} \FunctionTok{as.factor}\NormalTok{(comorbilidades),}
  \AttributeTok{method =} \StringTok{"nearest"}\NormalTok{, }\AttributeTok{distance =} \StringTok{"mahalanobis"}\NormalTok{,}
  \AttributeTok{data =}\NormalTok{ datos\_observacional}
\NormalTok{)}
\NormalTok{datos\_observacional }\OtherTok{\textless{}{-}} \FunctionTok{get\_matches}\NormalTok{(match, datos\_observacional)}
\end{Highlighting}
\end{Shaded}

Para comprobar que el sesgo evidenciado en estudios anteriores ha desaparecido, es posible reproducir la Tabla \ref{table}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tab2 }\OtherTok{\textless{}{-}} \FunctionTok{CreateTableOne}\NormalTok{(}
  \AttributeTok{vars =}\NormalTok{ myVars, }\AttributeTok{factorVars =}\NormalTok{ factorVars,}
  \AttributeTok{strata =} \StringTok{"exitus"}\NormalTok{, }\AttributeTok{data =}\NormalTok{ datos\_observacional}
\NormalTok{)}

\NormalTok{tab2 }\OtherTok{\textless{}{-}} \FunctionTok{print}\NormalTok{(tab1,}
  \AttributeTok{showAllLevels =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{formatOptions =} \FunctionTok{list}\NormalTok{(}\AttributeTok{big.mark =} \StringTok{","}\NormalTok{),}
  \AttributeTok{exact =} \StringTok{"stage"}\NormalTok{, }\AttributeTok{nonnormal =}\NormalTok{ nonnormal}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(tab2,}
  \AttributeTok{caption =} \StringTok{"Características basales de la población emparejadas"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:tabla-sesgo-formateada}Características basales de la población emparejadas}
\centering
\begin{tabular}[t]{l|l|l|l|l|l}
\hline
  & level & 0 & 1 & p & test\\
\hline
n &  & 79 & 21 &  & \\
\hline
sexo (\%) & Hombre & 28 (35.4) & 2 ( 9.5) & 0.042 & \\
\hline
 & Mujer & 51 (64.6) & 19 (90.5) &  & \\
\hline
edad (median [IQR]) &  & 64.00 [53.00, 73.00] & 82.00 [72.00, 85.00] & <0.001 & nonnorm\\
\hline
comorbilidades (\%) & 1 o más & 43 (54.4) & 18 (85.7) & 0.018 & \\
\hline
 & No & 36 (45.6) & 3 (14.3) &  & \\
\hline
\end{tabular}
\end{table}

\hypertarget{anuxe1lisis-de-supervivencia}{%
\subsection{Análisis de supervivencia}\label{anuxe1lisis-de-supervivencia}}

Durante la pandemia ocasionada por el SARS-CoV-2, la pregunta principal de los investigadores clínicos se centró en un mismo objetivo: factores de riesgo asociados a la mortalidad causada por COVID-19. El análisis de supervivencia ha permitido a los investigadores intentar explicar las causas más factibles que producen esa mayor probabilidad de fallecer. El análisis de supervivencia permite estudiar los factores de riesgo asociados a la mortalidad.

El \emph{dataset} está formado por 301 pacientes, 101 diagnósticados con infección por SARS-CoV-2 y 100 exitus.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(readxl)}
\NormalTok{datos\_supervivencia }\OtherTok{\textless{}{-}} \FunctionTok{read\_excel}\NormalTok{(}\StringTok{"data/datos\_supervivencia.xlsx"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(datos\_supervivencia, }\DecValTok{5}\NormalTok{)}
\CommentTok{\#\textgreater{} A tibble: 5 × 7}
\CommentTok{\#\textgreater{} id EXITUS\_TIME DIAG\_COVID EXITUS N\_COMORBIDITIES SEX     EDAD}
\CommentTok{\#\textgreater{}  \textless{}dbl\textgreater{}       \textless{}dbl\textgreater{}      \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}       \textless{}dbl\textgreater{} \textless{}chr\textgreater{}  \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 262           0          1      1               5 Hombre    83}
\CommentTok{\#\textgreater{} 236           1          1      1               5 Hombre    72}
\CommentTok{\#\textgreater{} 170          11          0      0               2 Mujer     65}
\CommentTok{\#\textgreater{} 204          11          1      1               4 Hombre    80}
\CommentTok{\#\textgreater{}  46          14          1      1               5 Hombre    90}
\end{Highlighting}
\end{Shaded}

\hypertarget{estimaciuxf3n-y-comparaciuxf3n-de-curvas-de-supervivencia}{%
\subsection{Estimación y comparación de curvas de supervivencia}\label{estimaciuxf3n-y-comparaciuxf3n-de-curvas-de-supervivencia}}

La función (o curva) de supervivencia estudia la probabilidad de que la variable, en este caso el paciente o sujeto, sobreviva a un tiempo \textbf{X}. El estimador más común utilizado para el ajuste de la función de supervivencia es el estimador no paramétrico \emph{Kaplan-Meier} y su función escalonada. Una vez generadas estas curvas de supervivencia, existen diferentes métodos (paramétricos y no paramétricos) para su comparación. En este apartado, se utiliza la prueba de Mantel-Cox (o test \emph{Log-Rank}) para el contraste de funciones.

Los paquetes \emph{survival} y \emph{survminer} integran las funciones principales para el ajuste de la técnica:

\begin{itemize}
\item
  La función \texttt{Surv()}, crea un objeto de supervivencia formado por el tiempo hasta la ocurrencia del evento y el evento (exitus).
\item
  La función \texttt{survfit}, estima la función de supervivencia mediante el método \emph{Kaplan-Meier} del objeto \texttt{Surv} y los factores de riesgo asociados.
\item
  La función \texttt{ggsurvplot()}, genera el gráfico de la curva de supervivencia (basada en la librería \texttt{ggplot2}). El argumento principal de la función es la función de supervivencia estimada (\texttt{survfit}). Los argumentos más importantes (y recomendables) a la hora de graficar la función de supervivencia son:

  \begin{itemize}
  \item
    \texttt{pval}: muestra el p-valor correspondiente a la comparación a través del test \emph{Log-Rank}.
  \item
    \texttt{conf.int}: muestra los intervalos de confianza de la(s) curva(s) de supervivencia.
  \item
    \texttt{risk.table}: añade el número de sujetos (absoluto o relativo) en riesgo en cada momento del periodo del estudio.
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(survival)}
\FunctionTok{library}\NormalTok{(survminer)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{survfit}\NormalTok{(}\FunctionTok{Surv}\NormalTok{(EXITUS\_TIME, EXITUS) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ DIAG\_COVID,}
  \AttributeTok{data =}\NormalTok{ datos\_supervivencia}
\NormalTok{)}

\FunctionTok{ggsurvplot}\NormalTok{(fit,}
  \AttributeTok{data =}\NormalTok{ datos\_supervivencia,}
  \AttributeTok{pval =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{conf.int =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{ggtheme =} \FunctionTok{theme\_bw}\NormalTok{(),}
  \AttributeTok{palette =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#E7B800"}\NormalTok{, }\StringTok{"\#2E9FDF"}\NormalTok{),}
  \CommentTok{\# añade tabla de supervivencia}
  \AttributeTok{risk.table =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{tables.height =} \FloatTok{0.2}\NormalTok{,}
  \AttributeTok{tables.theme =} \FunctionTok{theme\_cleantable}\NormalTok{(),}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/medicina-superviv} 

}

\caption{Curva de supervivencia XXXXXXX}\label{fig:plot-supervivencia}
\end{figure}

\hypertarget{regresiuxf3n-de-cox}{%
\subsection{Regresión de COX}\label{regresiuxf3n-de-cox}}

La regresión de Cox o modelo de riesgos proporcionales es una técnica utilizada para el estudio del efecto de covariables sobre el tiempo hasta la ocurrencia de un evento (exitus, recaída, progresión, etc). Es una técnica semi-paramétrica ya que no asume \emph{a priori} ningún modelo de supervivencia pero sí en los efectos de las variables predictoras.

La función principal para el ajuste de un modelo de regresión de Cox es \texttt{coxph}. Esta función, al igual que la función \texttt{survfit}, está formada por un objeto \texttt{Surv} y las covariables del modelo.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitCOX }\OtherTok{\textless{}{-}} \FunctionTok{coxph}\NormalTok{(}\FunctionTok{Surv}\NormalTok{(EXITUS\_TIME, EXITUS) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ DIAG\_COVID }\SpecialCharTok{+}\NormalTok{ EDAD }\SpecialCharTok{+}\NormalTok{ SEX }\SpecialCharTok{+}\NormalTok{ N\_COMORBIDITIES,}
  \AttributeTok{data =}\NormalTok{ datos\_supervivencia}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

El \emph{output} principal de una regresión de Cox son los \emph{Hazard Ratio} (HR), que es la interpretación de la exponencial de los coeficientes de regresión del modelo. Esta razón de riesgos oscila entre 0 a \(\infty\) , siendo el intervalo {[}0,1{]} una relación de riesgo menor y {[}1, \(\infty\) {]} una relación de riesgo mayor.

Nota para facilitar la interpretación:

\begin{itemize}
\item
  Los HR localizados entre {[}1,2{]} se interpretan en porcentaje. Es decir, HR = 1.5 es igual a un aumento del riesgo del 50\%.
\item
  Los HR localizados entre {[}2, \(\infty\) {]} se interpretan en ``veces''. Es decir, HR = 3 es igual a un aumento del riesgo de 3 veces.
\item
  Los HR localizados entre {[}0,1{]} es recomendable hacer su inversa. Es decir, HR = 0.8 es igual a una disminución del riesgo del 20\%.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(fitCOX)}
\CommentTok{\#\textgreater{} Call:}
\CommentTok{\#\textgreater{} coxph(formula = Surv(EXITUS\_TIME, EXITUS) \textasciitilde{} DIAG\_COVID + EDAD +}
\CommentTok{\#\textgreater{}    SEX + N\_COMORBIDITIES, data = datos\_supervivencia)}

\CommentTok{\#\textgreater{}  n= 271, number of events= 100}
\CommentTok{\#\textgreater{}   (30 observations deleted due to missingness)}

\CommentTok{\#\textgreater{}                      coef  exp(coef)   se(coef)      z Pr(\textgreater{}|z|)}
\CommentTok{\#\textgreater{} DIAG\_COVID       1.3023581  3.6779594  0.5184547  2.512   0.0120 *}
\CommentTok{\#\textgreater{} EDAD             0.0006006  1.0006008  0.0116113  0.052   0.9587}
\CommentTok{\#\textgreater{} SEXMujer        {-}1.1256901  0.3244285  0.2360183 {-}4.770 1.85e{-}06 ***}
\CommentTok{\#\textgreater{} N\_COMORBIDITIES  0.1643743  1.1786554  0.0774043  2.124   0.0337 *}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1}

\CommentTok{\#\textgreater{}                 exp(coef) exp({-}coef) lower .95 upper .95}
\CommentTok{\#\textgreater{} DIAG\_COVID         3.6780     0.2719    1.3314   10.1605}
\CommentTok{\#\textgreater{} EDAD               1.0006     0.9994    0.9781    1.0236}
\CommentTok{\#\textgreater{} SEXMujer           0.3244     3.0823    0.2043    0.5153}
\CommentTok{\#\textgreater{} N\_COMORBIDITIES    1.1787     0.8484    1.0127    1.3717}

\CommentTok{\#\textgreater{} Concordance= 0.815  (se = 0.025 )}
\CommentTok{\#\textgreater{} Likelihood ratio test= 130.1  on 4 df,   p=\textless{}2e{-}16}
\CommentTok{\#\textgreater{} Wald test            = 117.9  on 4 df,   p=\textless{}2e{-}16}
\CommentTok{\#\textgreater{} Score (logrank) test = 165.9  on 4 df,   p=\textless{}2e{-}16}
\end{Highlighting}
\end{Shaded}

Por tanto, de este modelo se pueden concluir las siguientes interpretaciones:

\begin{itemize}
\tightlist
\item
  Un paciente diagnosticado de COVID-19 tiene 3.6 veces más riesgo de fallecer que un paciente sano.
\item
  Una mujer tiene un 67.6\% menos riesgo de fallecer que un hombre.
\item
  Por cada comorbilidad, el riesgo de fallecer aumenta un 17.9\%.
\end{itemize}

\hypertarget{conclusiuxf3n}{%
\section{Conclusión}\label{conclusiuxf3n}}

Ha sido necesaria una pandemia mundial para que la sociedad empiece a dar visibilidad y reconocimiento no sólo a la bioestadística, si no a la investigación clínica y a la necesidad de gestionar el uso masivo de datos en salud. A pesar de los múltiples estudios y experiencias pasadas que llamaban a la prudencia y a la acción concreta si se daba una situación similar, el mundo ha sido incapaz de actuar convenientemente. Esto último se ve reflejado en el mínimo aumento de inversión, reconocimiento y notoriedad no sólo en investigación o desarrollo, si no en el apoyo a la ciencia.

Es, quizá, la paradoja más extraña pero que representa el dicho popular:

La sociedad no avanzará si la ciencia no lo hace primero.

\hypertarget{cap-climatico}{%
\chapter{Lo que nos cuentan los datos sobre el cambio climático}\label{cap-climatico}}

\emph{Dominic Royé}

La temperatura media global en la superficie ha aumentado en 1.1 ºC desde la era preindustrial (1880-1900), puede parecer poco pero implica un aumento significativo en el calor acumulado del sistema tierra. Cuando se combinan el aumento de de la temperatura con respecto a la superficie terrestre y el océano la tasa promedia es de 0,08 ºC por década desde 1880. Sin embargo, la tasa promedio de aumento desde 1981 ha sido más del doble, con 0,18ºC. Los océanos se caracterizan por una menor tasa de calentamiento debido a su capacidad calorífica, no obstante, son los océanos los que absorben la mayoría del calor adicional del planeta debido al cambio climático\footnote{ \url{https://www.ncei.noaa.gov/news/global-climate-202112}}.

Entre todas las regiones, la región mediterránea se está calentando un 20\% más rápido que el promedio mundial. Este lugar representa actualmente el punto crítico más importante del cambio climático, donde se percibe un significativo aumento de las vulnerabilidades. La temperatura de las aguas superficiales en el Mediterráneo ha estado subiendo 0,34ºC cada década desde principios de los 80 (\citet{cramer2020climate}).

En este caso práctico con datos sobre el cambio climátio se tratan las anomalías de la temperatura superficial del mar Mediterráneo en los meses estivales desde el año 1982 a 2022. Se hará uso del dataset con el nombre ``NOAA CDR OISST v02r01'', una interpolación óptima diaria de temperatura de la superficie del mar (OISST, por sus siglas en inglés) con una resolución de 1/4 grados (27 km). Los datos proporciona la NOAA con campos completos de temperatura del océano construidos mediante la combinación de observaciones ajustadas por sesgo de diferentes plataformas (satélites, barcos, boyas) en una cuadrícula global regular, con lagunas rellenadas por interpolación (\url{https://developers.google.com/earth-engine/datasets/catalog/NOAA_CDR_OISST_V2_1}). El geoprocesamiento en nube está explicado en el capítulo XXX.

\hypertarget{consideraciones-iniciales}{%
\section{Consideraciones iniciales}\label{consideraciones-iniciales}}

La información espacio-temporal es clave en muchas disciplinas, especialmente en la climatología o la meteorología, y ello hace necesario disponer de un formato que permita una estructura multidimensional. Además es importante que ese formato tenga un alto grado de compatibilidad de intercambio y pueda almacenar un elevado número de datos. Estas características llevaron al desarrollo del estándar abierto netCDF (NetworkCommon Data Form). El formato netCDF es un estándar abierto de intercambio de datos científicos multidimensionales que se utiliza con datos de observaciones o modelos, principalmente en disciplinas como la climatología, la meteorología y la oceanografía. Se trata de un formato espacio-temporal con una cuadrícula regular o irregular. La estructura multidimensional en forma de matriz (array) permite usar no sólo datos espacio-temporales, sino también multivariables. Los datos multidimensionales en formato \emph{geotiff} son menos comúnes, pero también se pueden llegar a usar en ocaciones. Además, es posible crear objetos multidimensionales importando multiples archivos ráster.

\hypertarget{paquetes}{%
\section{Paquetes}\label{paquetes}}

El manejo de datos en formato netCDF o múltiples archivos ráster es posible a través de varios paquetes de forma directa o indirecta. Destaca el paquete \texttt{\{ncdf4\}} específicamente diseñado, del que hacen uso también otros paquetes aunque no se ve. El manejo con \texttt{\{ncdf4\}} es algo complejo, particularmente por la necesidad de gestionar la memoria RAM cuando tratamos grandes conjuntos de datos o también por la forma de manejar la clase \emph{array}. Otro paquete muy potente es \texttt{\{terra\}}, que conocemos cuando trabajamos con datos ráster y permite usar sus funciones también para el manejo del formato netCDF.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# instalar los paquetes si hace falta}
\FunctionTok{library}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"rmapshaper"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"giscoR"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"sf"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"lubridate"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"terra"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"fs"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"patchwork"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"scales"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"rmapshaper"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"RColorBrewer"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"CDR"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{visualizaciuxf3n-de-mapas-de-pequeuxf1os-muxfaltiples}{%
\section{Visualización de mapas de pequeños múltiples}\label{visualizaciuxf3n-de-mapas-de-pequeuxf1os-muxfaltiples}}

Una forma muy efectiva para mostrar cambios espacio-temporales son los mapas de pequeños múltiples, donde se representan en una rejilla para cada año las anomalías observadas, lo que permite una comparación sencilla.

\hypertarget{datos}{%
\subsection{Datos}\label{datos}}

Importamos el polígono del Mar Mediterráneo para poder limitar los datos al área de interés.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"med\_limit"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A continuación se importa todos los años empleando la función \texttt{dir\_ls()} del paquete \texttt{\{fs\}} y la función \texttt{rast()} de \texttt{\{terra\}}. La primera función crea un vector de todos los archivos ubicados en la carpeta ``data''. Finalmente se renombran todas las capas con los correspondientes años. Es importante que se garantice el correcto orden de los archivos. Siempre cuando hayamos realizado el geoprocesamiento en nube de las anomalías (Capítulo X) podemos usar la alternativa: \texttt{anom\ \textless{}-\ dir\_ls("data-cc",\ regexp\ =\ "tif")\ \textbar{}\textgreater{}\ rast()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# importar}
\NormalTok{anom }\OtherTok{\textless{}{-}} \FunctionTok{dir\_ls}\NormalTok{(}\FunctionTok{system.file}\NormalTok{(}\StringTok{"external/data{-}cc/"}\NormalTok{, }\AttributeTok{package =} \StringTok{"CDR"}\NormalTok{), }\AttributeTok{regexp =} \StringTok{"tif"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{rast}\NormalTok{()}

\CommentTok{\# renombrar las capas}
\FunctionTok{names}\NormalTok{(anom) }\OtherTok{\textless{}{-}} \DecValTok{1982}\SpecialCharTok{:}\DecValTok{2022}
\end{Highlighting}
\end{Shaded}

\hypertarget{preparar-los-datos}{%
\subsection{Preparar los datos}\label{preparar-los-datos}}

Todavía se debe recortar y enmascarar el área de interés. Para ello, se usa primero la función \texttt{crop()} con los límites del Mar Mediterraneo y se pasa el resultado a la función \texttt{mask()}. El paquete \texttt{\{terra\}} necesita los datos vectoriales en su propia clase \emph{SpatVector}, por eso, se pasa con la función \texttt{vect()}, la que lo convierte de la clase \emph{sf} a \emph{SpatVector}. Finalmente se reproyectan los rásters a \emph{ETRS89-extended / LAEA Europe} con el código EPSG:3035.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{anom }\OtherTok{\textless{}{-}} \FunctionTok{crop}\NormalTok{(anom, med\_limit) }\SpecialCharTok{|\textgreater{}} \FunctionTok{mask}\NormalTok{(}\FunctionTok{vect}\NormalTok{(med\_limit))}
\NormalTok{anom }\OtherTok{\textless{}{-}} \FunctionTok{project}\NormalTok{(anom, }\StringTok{"EPSG:3035"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot}
\FunctionTok{plot}\NormalTok{(anom)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/anom-med} 

}

\caption{Selección de anomalías 1982 a 1997 de los datos brutos.}\label{fig:raster-crop-mask-plot}
\end{figure}

Un ráster consiste en latitud, longitud y un único o múltiples valores, también llamados capas. Para poder visualizarlo en \emph{ggplot2}, es necesario convertirlo en un \emph{data.frame}. En este caso, se obtienen 41 columnas para las anomalías, además de las primeras dos que corresponden a la longitud y latitud.

Todavía falta un cambio importante. Ahora mismo se tiene la misma variable, la anomalía, distribuida en muchas columnas, no obstante la estructura adecuada debe ser un conjunto total de dos columnas: una que represente las anomalías y una segunda que contenga los años. Para conseguirlo, se hace uso de \texttt{pivot\_longer()}, indicando el total de columnas que deben ser fusionadas y los nombres de las dos columnas resultantes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(anom, }\AttributeTok{xy =} \ConstantTok{TRUE}\NormalTok{)}

\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{pivot\_longer}\NormalTok{(df, }\DecValTok{3}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(df),}
  \AttributeTok{names\_to =} \StringTok{"yr"}\NormalTok{,}
  \AttributeTok{values\_to =} \StringTok{"anom"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Se añaden los años de la década de los 80 que faltan (1980, 1981) y se limitan las anomalías a un rango entre -2 y +2.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(}
\NormalTok{  df, }\FunctionTok{filter}\NormalTok{(df, yr }\SpecialCharTok{==} \StringTok{"1982"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{yr =} \StringTok{"1981"}\NormalTok{, }\AttributeTok{anom =} \ConstantTok{NA}\NormalTok{),}
  \FunctionTok{filter}\NormalTok{(df, yr }\SpecialCharTok{==} \StringTok{"1982"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{yr =} \StringTok{"1980"}\NormalTok{, }\AttributeTok{anom =} \ConstantTok{NA}\NormalTok{)}
\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{anom2 =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    anom }\SpecialCharTok{\textgreater{}} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \DecValTok{2}\NormalTok{,}
\NormalTok{    anom }\SpecialCharTok{\textless{}} \SpecialCharTok{{-}}\DecValTok{2} \SpecialCharTok{\textasciitilde{}} \SpecialCharTok{{-}}\DecValTok{2}\NormalTok{,}
    \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}}\NormalTok{ anom}
\NormalTok{  ))}
\end{Highlighting}
\end{Shaded}

Antes de pasar a construir el gráfico se estima la media de la anomalía global para toda la cuenca mediterránea de cada año. Estos datos se añadirá como texto a cada mapa. Con el objetivo de obtener las coordenadas de la posición en la proyección EPSG:3035 se fija un punto vectorial que reproyectamos.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# media global}
\NormalTok{med\_anom }\OtherTok{\textless{}{-}} \FunctionTok{global}\NormalTok{(anom, }\AttributeTok{fun =} \StringTok{"mean"}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{med\_anom }\OtherTok{\textless{}{-}} \FunctionTok{rownames\_to\_column}\NormalTok{(med\_anom, }\StringTok{"yr"}\NormalTok{)}

\CommentTok{\# posición}
\NormalTok{pos\_global }\OtherTok{\textless{}{-}} \FunctionTok{st\_point}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{34.24}\NormalTok{, }\FloatTok{41.5}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{st\_sfc}\NormalTok{(}\AttributeTok{crs =} \DecValTok{4326}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{st\_transform}\NormalTok{(}\DecValTok{3035}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{st\_coordinates}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{construir-el-gruxe1fico-de-muxfaltiples-mapas}{%
\subsection{Construir el gráfico de múltiples mapas}\label{construir-el-gruxe1fico-de-muxfaltiples-mapas}}

En el primer paso se definen los estilos partiendo de \texttt{theme\_void()} configurando los títulos, la leyenda y el color de fondo en \texttt{theme()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theme\_SST\_facet }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{base\_family =} \StringTok{"Bahnschrift"}\NormalTok{,}
                            \AttributeTok{base\_size =} \DecValTok{11}\NormalTok{,}
                            \AttributeTok{base\_line\_size =}\NormalTok{ base\_size }\SpecialCharTok{/} \DecValTok{22}\NormalTok{,}
                            \AttributeTok{base\_rect\_size =}\NormalTok{ base\_size }\SpecialCharTok{/} \DecValTok{22}\NormalTok{) \{}
  \FunctionTok{theme\_void}\NormalTok{(}
    \AttributeTok{base\_family =}\NormalTok{ base\_family, }\AttributeTok{base\_size =}\NormalTok{ base\_size,}
    \AttributeTok{base\_line\_size =}\NormalTok{ base\_line\_size, }\AttributeTok{base\_rect\_size =}\NormalTok{ base\_rect\_size}
\NormalTok{  ) }\SpecialCharTok{+}
    \FunctionTok{theme}\NormalTok{(}
      \AttributeTok{strip.text =} \FunctionTok{element\_text}\NormalTok{(}
        \AttributeTok{colour =} \StringTok{"white"}\NormalTok{,}
        \AttributeTok{face =} \StringTok{"bold"}\NormalTok{,}
        \AttributeTok{size =} \DecValTok{12}\NormalTok{,}
        \AttributeTok{margin =} \FunctionTok{margin}\NormalTok{(}\AttributeTok{b =} \DecValTok{15}\NormalTok{)}
\NormalTok{      ),}
      \AttributeTok{legend.text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{colour =} \StringTok{"white"}\NormalTok{),}
      \AttributeTok{legend.position =} \StringTok{"top"}\NormalTok{,}
      \AttributeTok{legend.justification =}\NormalTok{ .}\DecValTok{48}\NormalTok{,}
      \AttributeTok{plot.margin =} \FunctionTok{margin}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{20}\NormalTok{),}
      \AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{colour =} \StringTok{"white"}\NormalTok{, }\AttributeTok{size =} \DecValTok{30}\NormalTok{, }\AttributeTok{hjust =}\NormalTok{ .}\DecValTok{5}\NormalTok{),}
      \AttributeTok{plot.subtitle =} \FunctionTok{element\_text}\NormalTok{(}
        \AttributeTok{colour =} \StringTok{"white"}\NormalTok{, }\AttributeTok{size =} \DecValTok{15}\NormalTok{, }\AttributeTok{hjust =}\NormalTok{ .}\DecValTok{5}\NormalTok{,}
        \AttributeTok{margin =} \FunctionTok{margin}\NormalTok{(}\AttributeTok{t =} \DecValTok{5}\NormalTok{, }\AttributeTok{b =} \DecValTok{5}\NormalTok{)}
\NormalTok{      ),}
      \AttributeTok{plot.caption =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{colour =} \StringTok{"white"}\NormalTok{, }\AttributeTok{size =} \DecValTok{10}\NormalTok{, }\AttributeTok{hjust =} \DecValTok{0}\NormalTok{),}
      \AttributeTok{plot.background =} \FunctionTok{element\_rect}\NormalTok{(}\AttributeTok{fill =} \StringTok{"grey10"}\NormalTok{, }\AttributeTok{colour =} \ConstantTok{NA}\NormalTok{),}
      \AttributeTok{panel.spacing =} \FunctionTok{unit}\NormalTok{(}\DecValTok{2}\NormalTok{, }\StringTok{"lines"}\NormalTok{),}
      \AttributeTok{panel.background =} \FunctionTok{element\_rect}\NormalTok{(}\AttributeTok{fill =} \StringTok{"grey10"}\NormalTok{, }\AttributeTok{colour =} \ConstantTok{NA}\NormalTok{)}
\NormalTok{    )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Para representar datos ráster en forma de xyz empleamos \texttt{geom\_tile()} o \texttt{geom\_raster()} en \emph{ggplot2}. La última geometría requiere una rejilla regular. Para este primer ensayo se filtra sólo el año 2003, además se añade con \texttt{geom\_sf()} el límite del Mar Mediterráneo. La función \texttt{coord\_sf()} permite fijar una proyección para objetos \emph{sf} y por último se cambia el estilo definido anteriormente.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{filter}\NormalTok{(df, yr }\SpecialCharTok{==} \StringTok{"2003"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_tile}\NormalTok{(}\FunctionTok{aes}\NormalTok{(x, y, }\AttributeTok{fill =}\NormalTok{ anom2)) }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ med\_limit,}
    \AttributeTok{fill =} \ConstantTok{NA}\NormalTok{, }\AttributeTok{colour =} \StringTok{"white"}\NormalTok{, }\AttributeTok{size =}\NormalTok{ .}\DecValTok{1}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{coord\_sf}\NormalTok{(}\AttributeTok{crs =} \DecValTok{3035}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_SST\_facet}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Manteniéndose en el ejemplo, se modifica la gama de colores con \texttt{scale\_fill\_gradientn()}, en la que se pasa la paleta de colores, los extremos de valores, se reajustan los valores a una escala divergente, se define las etiquetas y sus posiciones. Dentro de la función \texttt{guides()} se cambia el ancho y altura de la barra colores empleando la función \texttt{guide\_colorbar()}.

Las geometrías \texttt{geom\_point()} y \texttt{geom\_text()} añadirán la información de la anomalía global. La posición se pasa de forma directa en \texttt{aes()}, además se definen el color y el tamaño de texto. La idea es situar el texto a la derecha del punto, por eso, se hace un ajuste en longitud indicando un valor correspondiente en el argumento \emph{nudge\_x} en la unidad del sistema de coordenadas (SC). Recuerde que el SC está en metros.

La función \texttt{number()} del paquete \texttt{\{scales\}} facilita formatear las cifras con un decimal y el símbolo negativo y positivo.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# gama de colores}
\NormalTok{rdbu\_pal }\OtherTok{\textless{}{-}} \FunctionTok{rev}\NormalTok{(}\FunctionTok{brewer.pal}\NormalTok{(}\DecValTok{11}\NormalTok{, }\StringTok{"RdBu"}\NormalTok{))}

\CommentTok{\# mapa 2003}
\FunctionTok{filter}\NormalTok{(df, yr }\SpecialCharTok{==} \StringTok{"2003"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_tile}\NormalTok{(}\FunctionTok{aes}\NormalTok{(x, y, }\AttributeTok{fill =}\NormalTok{ anom2)) }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ med\_limit,}
    \AttributeTok{fill =} \ConstantTok{NA}\NormalTok{,}
    \AttributeTok{colour =} \StringTok{"white"}\NormalTok{,}
    \AttributeTok{size =}\NormalTok{ .}\DecValTok{1}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}
    \AttributeTok{data =} \FunctionTok{filter}\NormalTok{(med\_anom, yr }\SpecialCharTok{==} \StringTok{"2003"}\NormalTok{),}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ pos\_global[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{], }\AttributeTok{y =}\NormalTok{ pos\_global[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{], }\AttributeTok{fill =}\NormalTok{ mean),}
    \AttributeTok{size =} \FloatTok{3.5}\NormalTok{, }\AttributeTok{shape =} \DecValTok{21}\NormalTok{, }\AttributeTok{colour =} \StringTok{"white"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}
    \AttributeTok{data =} \FunctionTok{filter}\NormalTok{(med\_anom, yr }\SpecialCharTok{==} \StringTok{"2003"}\NormalTok{),}
    \FunctionTok{aes}\NormalTok{(}
      \AttributeTok{x =}\NormalTok{ pos\_global[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{], }\AttributeTok{y =}\NormalTok{ pos\_global[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{],}
      \AttributeTok{label =} \FunctionTok{number}\NormalTok{(mean, .}\DecValTok{1}\NormalTok{, }\AttributeTok{style\_positive =} \StringTok{"plus"}\NormalTok{)}
\NormalTok{    ),}
    \AttributeTok{size =} \FloatTok{3.5}\NormalTok{, }\AttributeTok{nudge\_x =} \DecValTok{700000}\NormalTok{, }\AttributeTok{colour =} \StringTok{"white"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_gradientn}\NormalTok{(}
    \AttributeTok{colours =}\NormalTok{ rdbu\_pal, }\AttributeTok{na.value =} \ConstantTok{NA}\NormalTok{,}
    \AttributeTok{values =} \FunctionTok{rescale}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{)),}
    \AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{),}
    \AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{1.5}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{, }\DecValTok{0}\NormalTok{, .}\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{2}\NormalTok{),}
    \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}
      \StringTok{"\textless{} {-}2.0"}\NormalTok{, }\StringTok{"{-}1.5"}\NormalTok{, }\StringTok{"{-}1.0"}\NormalTok{, }\StringTok{"{-}0.5"}\NormalTok{, }\StringTok{"0.0"}\NormalTok{,}
      \StringTok{"0.5"}\NormalTok{, }\StringTok{"1.0"}\NormalTok{, }\StringTok{"1.5"}\NormalTok{, }\StringTok{"\textgreater{} 2.0"}
\NormalTok{    )}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{guides}\NormalTok{(}\AttributeTok{fill =} \FunctionTok{guide\_colorbar}\NormalTok{(}
    \AttributeTok{barwidth =} \DecValTok{20}\NormalTok{,}
    \AttributeTok{barheight =}\NormalTok{ .}\DecValTok{5}
\NormalTok{  )) }\SpecialCharTok{+}
  \FunctionTok{coord\_sf}\NormalTok{(}\AttributeTok{crs =} \DecValTok{3035}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_SST\_facet}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

En los datos se ha añadido dos años con valores perdidos (1980 y 1981) con el objetivo de obtener por cada fila 10 años, una década. Si no se hiciese el \emph{facet grid} empezaría por 1982 sin posibilidad de mantener en cada fila la década correspondiente. A todos los mapas se añadirán los límites de la cuenca mediterránea, no obstante, para que no aparezca en las facetas de los años 1980/81, se debe repetir la geometría para todos los años.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{med }\OtherTok{\textless{}{-}} \FunctionTok{slice}\NormalTok{(med\_limit, }\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{41}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(geometry) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{yr =} \FunctionTok{as.character}\NormalTok{(}\DecValTok{1982}\SpecialCharTok{:}\DecValTok{2022}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

A continuación se construye todo el gráfico con todos las facetas de mapas. Lo único nuevo es la función \texttt{facet\_wrap()} en la que se indica la variable por la que se crean las facetas. A diferencia de \texttt{facet\_grid()} esta variante permite fijar el número de filas o/y columnas. Además, se pasa una función menor en la función \texttt{labeller()} en el mismo argumento. Esta función permite modificar las etiquetas de las facetas, aquí únicamente borramos el texto de los años 1980/81.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# paso 1}
\NormalTok{g }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df) }\SpecialCharTok{+}
  \FunctionTok{geom\_tile}\NormalTok{(}\FunctionTok{aes}\NormalTok{(x, y, }\AttributeTok{fill =}\NormalTok{ anom2)) }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data =}\NormalTok{ med, }\AttributeTok{fill =} \ConstantTok{NA}\NormalTok{, }\AttributeTok{colour =} \StringTok{"white"}\NormalTok{, }\AttributeTok{size =}\NormalTok{ .}\DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ med\_anom,}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ pos\_global[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{], }\AttributeTok{y =}\NormalTok{ pos\_global[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{], }\AttributeTok{fill =}\NormalTok{ mean),}
    \AttributeTok{size =} \FloatTok{3.5}\NormalTok{, }\AttributeTok{shape =} \DecValTok{21}\NormalTok{, }\AttributeTok{colour =} \StringTok{"white"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ med\_anom,}
    \FunctionTok{aes}\NormalTok{(}
      \AttributeTok{x =}\NormalTok{ pos\_global[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{], }\AttributeTok{y =}\NormalTok{ pos\_global[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{],}
      \AttributeTok{label =} \FunctionTok{number}\NormalTok{(mean, .}\DecValTok{1}\NormalTok{, }\AttributeTok{style\_positive =} \StringTok{"plus"}\NormalTok{)}
\NormalTok{    ),}
    \AttributeTok{size =} \FloatTok{3.5}\NormalTok{, }\AttributeTok{nudge\_x =} \DecValTok{700000}\NormalTok{, }\AttributeTok{colour =} \StringTok{"white"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_gradientn}\NormalTok{(}
    \AttributeTok{colours =}\NormalTok{ rdbu\_pal,}
    \AttributeTok{na.value =} \ConstantTok{NA}\NormalTok{, }\AttributeTok{values =} \FunctionTok{rescale}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{)),}
    \AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{),}
    \AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}
      \SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{1.5}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{, }\DecValTok{0}\NormalTok{,}
\NormalTok{      .}\DecValTok{5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{2}
\NormalTok{    ),}
    \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}
      \StringTok{"\textless{} {-}2.0"}\NormalTok{, }\StringTok{"{-}1.5"}\NormalTok{, }\StringTok{"{-}1.0"}\NormalTok{, }\StringTok{"{-}0.5"}\NormalTok{, }\StringTok{"0.0"}\NormalTok{,}
      \StringTok{"0.5"}\NormalTok{, }\StringTok{"1.0"}\NormalTok{, }\StringTok{"1.5"}\NormalTok{, }\StringTok{"\textgreater{} 2.0"}
\NormalTok{    )}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{guides}\NormalTok{(}\AttributeTok{fill =} \FunctionTok{guide\_colorbar}\NormalTok{(}
    \AttributeTok{barwidth =} \DecValTok{20}\NormalTok{,}
    \AttributeTok{barheight =}\NormalTok{ .}\DecValTok{5}
\NormalTok{  )) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(yr }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
    \AttributeTok{ncol =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{labeller =} \FunctionTok{labeller}\NormalTok{(}\AttributeTok{yr =} \ControlFlowTok{function}\NormalTok{(lab) \{}
      \FunctionTok{ifelse}\NormalTok{(lab }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"1980"}\NormalTok{, }\StringTok{"1981"}\NormalTok{), }\StringTok{""}\NormalTok{, lab)}
\NormalTok{    \})}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Finalmente, se combinan el objeto con definiciones finales como los títulos, el sistema de coordenadas y el estilo. Es importante indicar \emph{clip = ``off''}, dado que en caso contrario se cortan visualmente los valores de las anomalías globales al encontrarse fuera de los límites de cada mapa.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# paso 2}
\NormalTok{g }\OtherTok{\textless{}{-}}\NormalTok{ g }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"ANOMALÍA ESTIVAL DE LA TEMPERATURA DE SUPERFICIE DEL}\SpecialCharTok{\textbackslash{}n}\StringTok{Mar Mediterráneo"}\NormalTok{, }\AttributeTok{subtitle =} \StringTok{"Periodo de referencia 1982{-}2010."}\NormalTok{, }\AttributeTok{fill =} \StringTok{""}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_sf}\NormalTok{(}\AttributeTok{crs =} \DecValTok{3035}\NormalTok{, }\AttributeTok{clip =} \StringTok{"off"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_SST\_facet}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

En principio, se podría quedarse aquí y exportar el resultado, no obstante, lo que faltaría es un mapa de orientación.

\hypertarget{mapa-de-orientaciuxf3n}{%
\subsection{Mapa de orientación}\label{mapa-de-orientaciuxf3n}}

A través del paquete \texttt{\{giscoR\}} se obtiene los límites administrativos de los que únicamente se queda con una selección. También se limita la extensión a aproxidamente la de la cuenca mediterreánea. La función \texttt{ms\_innerlines()} del paquete \texttt{\{rmapshaper\}} facilita la obtención de los límites compartidos o interiores de los países selecionados. En la construcción del mapa de orientación se incluye con ayuda de \texttt{geom\_sf\_text()} los nombres de los países en forma de código ISO-3.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# límites de países}
\NormalTok{countries\_med }\OtherTok{\textless{}{-}} \FunctionTok{gisco\_get\_countries}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(ISO3\_CODE }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}
    \StringTok{"ESP"}\NormalTok{, }\StringTok{"MAR"}\NormalTok{, }\StringTok{"FRA"}\NormalTok{, }\StringTok{"ITA"}\NormalTok{,}
    \StringTok{"GRC"}\NormalTok{, }\StringTok{"TUR"}\NormalTok{, }\StringTok{"DZA"}\NormalTok{, }\StringTok{"TUN"}\NormalTok{,}
    \StringTok{"LBY"}\NormalTok{, }\StringTok{"EGY"}\NormalTok{, }\StringTok{"ALB"}
\NormalTok{  )) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{st\_crop}\NormalTok{(}\AttributeTok{xmin =} \SpecialCharTok{{-}}\DecValTok{6}\NormalTok{, }\AttributeTok{xmax =} \DecValTok{36}\NormalTok{, }\AttributeTok{ymin =} \DecValTok{28}\NormalTok{, }\AttributeTok{ymax =} \DecValTok{45}\NormalTok{)}

\CommentTok{\# límites inernos}
\NormalTok{innerlimit }\OtherTok{\textless{}{-}} \FunctionTok{ms\_innerlines}\NormalTok{(countries\_med)}

\CommentTok{\# mapa}
\NormalTok{insetp }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data =}\NormalTok{ med, }\AttributeTok{size =}\NormalTok{ .}\DecValTok{4}\NormalTok{, }\AttributeTok{colour =} \ConstantTok{NA}\NormalTok{, }\AttributeTok{fill =} \StringTok{"grey90"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data =}\NormalTok{ innerlimit, }\AttributeTok{size =}\NormalTok{ .}\DecValTok{2}\NormalTok{, }\AttributeTok{colour =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_sf\_text}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ countries\_med,}
    \FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ ISO3\_CODE),}
    \AttributeTok{size =} \DecValTok{2}\NormalTok{, }\AttributeTok{colour =} \StringTok{"white"}\NormalTok{, }\AttributeTok{fontface =} \StringTok{"bold"}\NormalTok{, }\AttributeTok{nudge\_y =}\NormalTok{ .}\DecValTok{1}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{coord\_sf}\NormalTok{(}\AttributeTok{crs =} \DecValTok{3035}\NormalTok{, }\AttributeTok{expand =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_void}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{plot.background =} \FunctionTok{element\_blank}\NormalTok{(),}
    \AttributeTok{panel.background =} \FunctionTok{element\_blank}\NormalTok{()}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\hypertarget{exportar-mapa-final}{%
\subsection{Exportar mapa final}\label{exportar-mapa-final}}

Se insertará el mapa de orientación en el gráfico como elemento adicional en la esquina derecha-arriba. El paquete \texttt{\{patchwork\}} puede ayudar a crear composiciones de distinos gráficos. Aquí empleamos la función \texttt{inset\_element()} indicando la posición relativa en xmin, ymin, xmax, y ymax. Es importante recordar que cualquier modificación del tamaño de impresión (veáse \emph{height} y \emph{width} en \texttt{ggsave()}), puede llevar a ajustes en la posición. El argumento \emph{align\_to = ``full''} permite posicionar sobre todo el lienzo.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# paso 3}
\NormalTok{p\_final }\OtherTok{\textless{}{-}}\NormalTok{ g }\SpecialCharTok{+} \FunctionTok{inset\_element}\NormalTok{(insetp, }\DecValTok{0}\NormalTok{, .}\DecValTok{75}\NormalTok{, .}\DecValTok{25}\NormalTok{, .}\DecValTok{95}\NormalTok{,}
  \AttributeTok{align\_to =} \StringTok{"full"}
\NormalTok{)}

\FunctionTok{ggsave}\NormalTok{(}\StringTok{"sst\_anom\_med2.png"}\NormalTok{,}
\NormalTok{  p\_final,}
  \AttributeTok{bg =} \StringTok{"grey10"}\NormalTok{,}
  \AttributeTok{height =} \DecValTok{10}\NormalTok{,}
  \AttributeTok{width =} \DecValTok{20}\NormalTok{,}
  \AttributeTok{unit =} \StringTok{"in"}\NormalTok{,}
  \AttributeTok{type =} \StringTok{"cairo{-}png"}\NormalTok{,}
  \AttributeTok{dpi =} \DecValTok{400}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/sst_anom_med2} 

}

\caption{Anomalía estival de la temperatura de superficie del mar}\label{fig:sst-anom-med2}
\end{figure}

\hypertarget{cap-ree}{%
\chapter{Predicción de demanda eléctrica con deep learning}\label{cap-ree}}

\emph{Jose Manuel Sanz Candales}

\hypertarget{introducciuxf3n-27}{%
\section{Introducción}\label{introducciuxf3n-27}}

Red Eléctrica, como Operador del Sistema, tiene como principal misión garantizar la continuidad del suministro eléctrico en España. Para ello, entre otras muchas tareas, se desarrollan, evolucionan y mantienen algoritmos de previsión del consumo eléctrico y de la producción con las principales energías renovables (Eólica y Solar) para distintos horizontes (próximas horas, días, meses, años, etc.) y periodos temporales (anual, horario, quinceminutal).

Este caso de uso se sitúa en el departamento de Ciencia de Datos del Operador del Sistema. Es el principio del año 2018, y el área de planificación de la empresa solicita una \textbf{predicción de la demanda eléctrica en España} para el año actual y el siguiente (2018 y 2019).

\textbf{IMPORTANTE:} Este desarrollo no estaba previsto en el presupuesto del año , por lo que tanto el software como los datos de entrada deben ser, a ser posible, gratuitos.

\hypertarget{datos-de-entrada}{%
\section{Datos de entrada}\label{datos-de-entrada}}

Respecto a los datos de entrada para el modelo, en este caso de uso será necesario utilizar un modelo regresor de aprendizaje supervisado, dado que la demanda eléctrica es una variable numérica contínua. Por ello, se necesitará una serie histórica de demanda eléctrica anual real en España de los años anteriores a 2018 y alguna/s variable/s predictora/s a partir de la/s cual/es los modelos de Machine Learning puedan encontrar relaciones para predecir con una precisión aceptable la demanda eléctrica.

Respecto a la serie histórica de demanda anual, Red Eléctrica, en su Web corporativa, publica datos estadísticos accesible de forma abierta. Sin embargo, el histórico publicado comienza en 2012 y sería conveniente tener un periodo de tiempo más amplio para un entrenamiento adecuado de nuestros modelos. Se realiza una búsqueda de otras fuentes y, afortunadamente, se encuentra que el IDAE publica datos desde 1990 del consumo final de energía eléctrica en miles de toneladas equivalentes de petróleo -ktep-.

Como los datos se deben entregar en MWh, las unidades de la predicción resultante se tendrá que convertir con el coeficiente que indican en la Web del IDAE (1 MWh = 0,086 tep), pero en los modelos se utilizarán las unidades originales porque más adelante se comprobará que dichas unidades resultan muy útiles para ver cómo se relaciona la demanda eléctrica con la variable predictora que se va a utilizar.

Ya se tiene, por tanto, una parte de los datos necesarios para entrenar nuestros modelos de predicción, que es \textbf{la serie histórica de nuestra target (o variable a pedecir)}

La otra parte que se requiere son \textbf{las features o variables predictoras}. Se sabe que, históricamente, las variaciones interanuales de la demanda eléctrica dependen del comportamiento de la economía de una forma directa: si la economía crece, también crece la demanda (y viceversa). Se busca qué variables econométricas relevantes es posible encontrar de forma pública y, tras un detallado análisis, se concluye que la más adecuada para el modelo puede ser el PIB per cápita que se puede encontrar en el siguiente enlace, disponible de forma pública en Expansión - datos macro.

Ya se tiene el dataset de entrada para los modelos (como se verá a continuación, es un conjunto de datos muy pequeño y sencillo), y se puede comenzar a construir nuestro modelo en R.

\hypertarget{caso-de-estudio}{%
\section{Caso de estudio}\label{caso-de-estudio}}

Lo primero que se hará en la siguiente celda de código es leer el fichero que contiene el conjunto de datos, convertirlo de formato data.table a data.frame y visualizar sus primeras filas:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(CDR)}
\NormalTok{df }\OtherTok{\textless{}{-}}\NormalTok{ CDR}\SpecialCharTok{::}\NormalTok{consumoelectricoanual}
\FunctionTok{class}\NormalTok{(df) }\OtherTok{\textless{}{-}} \FunctionTok{class}\NormalTok{(}\FunctionTok{as.data.frame}\NormalTok{(df))}
\FunctionTok{head}\NormalTok{(df, }\DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{} Año PIB Consumo}
\CommentTok{\#\textgreater{} 1 1990 10.331 10.817}
\CommentTok{\#\textgreater{} 2 1991 11.400 11.061}
\CommentTok{\#\textgreater{} 3 1992 11.800 11.244}
\end{Highlighting}
\end{Shaded}

En este caso, ya se dispone de los datos reales de 2018 y 2019 (esto permitirá ver cómo ha sido la precisión del modelo), pero en un caso real, a principios de 2018 el PIB per cápita de 2018 y 2019 será una predicción y la demanda eléctrica anual es desconocida, ya que es lo que se necesita predecir). Es decir, los datos de demanda eléctrica de 2018 y 2019 para el modelo que vamos a construir no existen, no se van a utilizar para entrenar ni para evaluar la precisión del modelo.

En la siguiente celda de código, se puede ver la correlación entre la variable predictora (PIB per cápita) y la variable a predecir (Demanda eléctrica anual):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cormat }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{cor}\NormalTok{(df[}\FunctionTok{c}\NormalTok{(}\StringTok{"PIB"}\NormalTok{, }\StringTok{"Consumo"}\NormalTok{)]), }\DecValTok{2}\NormalTok{)}
\FunctionTok{head}\NormalTok{(cormat)}
\CommentTok{\#\textgreater{} PIB Consumo}
\CommentTok{\#\textgreater{} PIB 1.00 0.96}
\CommentTok{\#\textgreater{} Consumo 0.96 1.00}
\end{Highlighting}
\end{Shaded}

Como se puede observar, el coeficiente de correlación lineal entre ambas variables es muy alto y positivo (es decir, cuando crece una también crece la otra). Si, además, se visualiza la gráfica de ambas variables en el tiempo, se apreciará de forma aún más clara:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(reshape2)}
\NormalTok{df\_m }\OtherTok{\textless{}{-}} \FunctionTok{melt}\NormalTok{(df, }\AttributeTok{id.vars =} \StringTok{"Año"}\NormalTok{)}
\FunctionTok{options}\NormalTok{(}\AttributeTok{repr.plot.width =} \DecValTok{15}\NormalTok{, }\AttributeTok{repr.plot.height =} \DecValTok{8}\NormalTok{)}
\FunctionTok{ggplot}\NormalTok{(df\_m, }\FunctionTok{aes}\NormalTok{(Año, value, }\AttributeTok{col =}\NormalTok{ variable)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{size =} \FloatTok{2.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Hasta 2005 se observa que las líneas están prácticamente superpuestas, pero desde 2006 las líneas se separan, ¿a qué puede deberse este cambio de tendencia? Uno de los motivos más relevantes probablemente sean las medidas de eficiencia energética que se han ido introduciendo en los últimos lustros (iluminación led, electrodomésticos y dispositivos con menor consumo, etc.).

Una vez se han explorado los datos (en este caso ha sido muy breve, pero es muy habitual en proyectos reales que la exploración y limpieza de los datos requiera en torno al 80\% del tiempo), se procederá a dividir el conjunto de datos en dos partes: i. entrenamiento+validación (desde 1990 hasta 2017) y ii. test (2018 y 2019):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{df\_aux }\OtherTok{\textless{}{-}}\NormalTok{ df[df}\SpecialCharTok{$}\NormalTok{Año }\SpecialCharTok{\textless{}} \DecValTok{2018}\NormalTok{, ]}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(df\_aux)}
\NormalTok{trainIndex }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, }\AttributeTok{size =} \FunctionTok{round}\NormalTok{(}\FloatTok{0.9} \SpecialCharTok{*}\NormalTok{ n), }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{df\_train }\OtherTok{\textless{}{-}}\NormalTok{ df\_aux[trainIndex, ]}
\NormalTok{df\_test }\OtherTok{\textless{}{-}}\NormalTok{ df\_aux[}\SpecialCharTok{{-}}\NormalTok{trainIndex, ]}
\end{Highlighting}
\end{Shaded}

Ahora se deberían probar distintos modelos de Machine Learning y comparar sus resultados para determinar cuál es el más preciso para este conjunto de datos. En este ejemplo, por simplicidad solo se probará un modelo, una Red Neuronal simple (Perceptrón Multicapa).

En realidad, no se va a entrenar un solo modelo sino varias Redes Neuronales, ya que se van a utilizar dos técnicas que son Grid Search (para probar distintas combinaciones de hiperparámetros) y Cross Validation (para entrenar y validar aprovechando todos los registros del conjunto de entrenamiento-validación).

En este caso, se van a probar distintas redes combinando el número de capas ocultas y de neuronas por cada capa oculta:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(neuralnet)}
\FunctionTok{library}\NormalTok{(caret)}
\NormalTok{grid }\OtherTok{\textless{}{-}} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{layer1 =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{), }\AttributeTok{layer2 =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{), }\AttributeTok{layer3 =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{grid}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\CommentTok{\# define training control}
\NormalTok{train\_control }\OtherTok{\textless{}{-}} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"cv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{2}\NormalTok{, }\AttributeTok{verbose =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\# train the model}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(Consumo }\SpecialCharTok{\textasciitilde{}}\NormalTok{ PIB, }\AttributeTok{data =}\NormalTok{ df\_train, }\AttributeTok{trControl =}\NormalTok{ train\_control, }\AttributeTok{method =} \StringTok{"neuralnet"}\NormalTok{, }\AttributeTok{tuneGrid =}\NormalTok{ grid)}
\end{Highlighting}
\end{Shaded}

Se muestran los resultados:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(model)}
\CommentTok{\#\textgreater{} Neural Network}
\CommentTok{\#\textgreater{}}
\CommentTok{\#\textgreater{} 25 samples}
\CommentTok{\#\textgreater{} 1 predictor}
\CommentTok{\#\textgreater{}}
\CommentTok{\#\textgreater{} No pre{-}processing}
\CommentTok{\#\textgreater{} Resampling: Cross{-}Validated (2 fold)}
\CommentTok{\#\textgreater{} Summary of sample sizes: 13, 12}
\CommentTok{\#\textgreater{} Resampling results across tuning parameters:}
\CommentTok{\#\textgreater{}}
\CommentTok{\#\textgreater{} layer3 RMSE Rsquared MAE}
\CommentTok{\#\textgreater{} 2 3.906921 0.9801918 3.5864138}
\CommentTok{\#\textgreater{} 5 1.161004 0.9349297 0.9872479}
\CommentTok{\#\textgreater{}}
\CommentTok{\#\textgreater{} Tuning parameter \textquotesingle{}layer1\textquotesingle{} was held constant at a value of 5}
\CommentTok{\#\textgreater{} Tuning}
\CommentTok{\#\textgreater{} parameter \textquotesingle{}layer2\textquotesingle{} was held constant at a value of 5}
\CommentTok{\#\textgreater{} RMSE was used to select the optimal model using the smallest value.}
\CommentTok{\#\textgreater{} The final values used for the model were layer1 = 5, layer2 = 5 and layer3 = 5.}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OtherTok{\textless{}{-}}\NormalTok{ df\_test[}\FunctionTok{c}\NormalTok{(}\StringTok{"PIB"}\NormalTok{)]}
\CommentTok{\#\textgreater{} Año PIB Consumo}
\CommentTok{\#\textgreater{} 2 1991 11.40 11.061}
\CommentTok{\#\textgreater{} 12 2001 17.20 17.279}
\CommentTok{\#\textgreater{} 27 2016 23.98 19.993}
\FunctionTok{predict}\NormalTok{(model, X)}
\CommentTok{\#\textgreater{} 2 12 27}
\CommentTok{\#\textgreater{} 11.48053 17.11479 21.11972}
\end{Highlighting}
\end{Shaded}

El modelo con mejor resultado (Red Neuronal con tres capas de 5 neuronas cada una) es conveniente volverlo a entrenar con el conjunto de entrenamiento-validación completo:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{122}\NormalTok{)}
\NormalTok{nn }\OtherTok{\textless{}{-}} \FunctionTok{neuralnet}\NormalTok{(Consumo }\SpecialCharTok{\textasciitilde{}}\NormalTok{ PIB, }\AttributeTok{data =}\NormalTok{ df\_train, }\AttributeTok{hidden =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Ahora se predice con el modelo entrenado para la parte del conjunto de datos que se habían reservado para test (años 2018 y 2019):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(nn, X)}
\NormalTok{pred}
\CommentTok{\#\textgreater{} [,1]}
\CommentTok{\#\textgreater{} 2 11.61718}
\CommentTok{\#\textgreater{} 12 16.94962}
\CommentTok{\#\textgreater{} 27 21.75289}
\end{Highlighting}
\end{Shaded}

El objetivo del conjunto de test es comprobar la precisión del modelo con datos totalmente desconocidos para él (es decir, no utilizados en la fase de entrenamiento-validación), principalmente para asegurar que el modelo generaliza bien (esto es, no ha sobreentrenado ni subentrenado)

Ahora sí, se va a predecir el consumo eléctrico anual para 2018 y 2019, que es el dato que solicitaron desde el área de planificación de la empresa. Para ello, simplemente se utilizará el método predict del mejor modelo. Previamente, se va a añadir una columna en el conjunto de datos original -df- para luego comprobar gráficamente el valor predicho frente al real que, en este caso ficticio, ya es conocido:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df[}\StringTok{"Prediccion\_MLP"}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\end{Highlighting}
\end{Shaded}

Ahora se hace la predicción del año 2018 y se añade el resultado a esta nueva columna del conjunto de datos:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_pred\_2018 }\OtherTok{\textless{}{-}}\NormalTok{ df[df}\SpecialCharTok{$}\NormalTok{Año }\SpecialCharTok{==} \DecValTok{2018}\NormalTok{, ]}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{Prediccion\_MLP[df}\SpecialCharTok{$}\NormalTok{Año }\SpecialCharTok{==} \DecValTok{2018}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(nn, df\_pred\_2018[}\FunctionTok{c}\NormalTok{(}\StringTok{"PIB"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

Se hace también la predicción para el año 2019 y se visualizan las predicciones añadidas al conjunto de datos para ambos años:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_pred\_2019 }\OtherTok{\textless{}{-}}\NormalTok{ df[df}\SpecialCharTok{$}\NormalTok{Año }\SpecialCharTok{==} \DecValTok{2019}\NormalTok{, ]}
\NormalTok{df}\SpecialCharTok{$}\NormalTok{Prediccion\_MLP[df}\SpecialCharTok{$}\NormalTok{Año }\SpecialCharTok{==} \DecValTok{2019}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(nn, df\_pred\_2019[}\FunctionTok{c}\NormalTok{(}\StringTok{"PIB"}\NormalTok{)])}
\FunctionTok{tail}\NormalTok{(df, }\DecValTok{3}\NormalTok{)}
\CommentTok{\#\textgreater{} Año PIB Consumo Prediccion\_MLP}
\CommentTok{\#\textgreater{} 28 2017 24.97 20.559 NA}
\CommentTok{\#\textgreater{} 29 2018 25.75 20.504 20.78726}
\CommentTok{\#\textgreater{} 30 2019 26.42 20.166 20.89161}
\end{Highlighting}
\end{Shaded}

Estos son los datos que se entregarían como resultado de la petición de información (convertidos a MWh aplicando el coeficiente que se citó anteriormente).

Claro está, en el momento que se entregan las predicciones para 2018 y 2019 todavía no se sabría cómo de buenas han sido, pero a principios de 2020 sí es posible calcular los errores de nuestro modelo, y es lo que se hará en las siguientes chunks:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(reshape)}
\NormalTok{df\_m\_mlp }\OtherTok{\textless{}{-}} \FunctionTok{melt}\NormalTok{(df, }\AttributeTok{id.vars =} \StringTok{"Año"}\NormalTok{)}
\CommentTok{\# options(repr.plot.width=15, repr.plot.height=8)}
\FunctionTok{ggplot}\NormalTok{(df\_m\_mlp, }\FunctionTok{aes}\NormalTok{(Año, value, }\AttributeTok{col =}\NormalTok{ variable)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/plot-ree1} 

}

\caption{xxxxx}\label{fig:reshape-plot-ree1}
\end{figure}

Las predicciones (puntos azules) tienen unos errores del orden de los que se habían visto en el conjunto de test cuando se hizo el entrenamiento de los modelos, por lo que parece que no hay sobreentrenamiento en el modelo.Parece que el modelo ha tenido una precisión bastante aceptable.

\hypertarget{cap-sist-experto}{%
\chapter{Sistemas expertos en el ámbito pediátrico}\label{cap-sist-experto}}

\emph{Arturo Peralta, José Ángel Olivas y Eusebio Angulo}

\hypertarget{introducciuxf3n-28}{%
\section{Introducción}\label{introducciuxf3n-28}}

Sin lugar a duda, el análisis de situaciones complejas para la evaluación y toma de decisiones es un proceso para el que tradicionalmente se requiere el apoyo de un especialista dispuesto a poner en uso todo su conocimiento. Sin embargo, el desarrollo de sistemas automáticos capaces de modelar el conocimiento que un experto podría tener sobre un ámbito concreto, y de procesarlo para alcanzar una respuesta adecuada a una consulta relacionada, resulta cada día más extendido como mecanismo de ayuda. A este tipo de herramientas se los denomina Sistemas Expertos (SE).

En este capítulo se introducen los conceptos teóricos fundamentales de la Ingeniería del Conocimiento, los componentes y el funcionamiento de los SE para, posteriormente, presentar cómo su aplicación puede apoyar en el proceso de evaluación clínica en el ámbito pediátrico de atención primaria. Finalmente, se incluye una sencilla implementación en \textbf{R} del SE enfocado a la ayuda en esta problemática.

\hypertarget{marco-teuxf3rico}{%
\section{Marco teórico}\label{marco-teuxf3rico}}

Un \textbf{Sistema Experto (SE)} es un programa de ordenador que trata de emular el comportamiento de una persona experta en un dominio de conocimiento específico ante un problema que se plantee en dicho dominio y cómo llega a su solución.

La \textbf{Ingeniería del Conocimiento} se ocupa entre otras cosas del proceso de especificación, análisis y desarrollo de un sistema experto \citep{MaMoCa2005}.

Los principales componentes de un SE son la \textbf{Base de Hechos}, que contiene la definición del entorno sobre el que se van a resolver problemas. Hace el papel de ``ojos'' del Sistema Experto. Además, cuenta con la \textbf{Base de Conocimientos}, que contiene el conocimiento del dominio específico convenientemente representado capaz de resolver problemas, y también puede considerar la representación de incertidumbre. Otro componente destacado es el \textbf{Motor de Inferencias}, que es el proceso de razonamiento que usa el sistema experto, combina hechos y conocimiento para emitir una conclusión. Por último, también debe tener un \textbf{Interfaz de entrada/salida} para comunicarse con los usuarios y/o expertos.

Las principales limitaciones en la construcción de SE vienen dadas porque el conocimiento experto humano es experiencia compilada, es Heurístico, esto es, basado en experiencia, reglas prácticas. Es extenso y también redundante e incluye heurística acerca de la heurística y del razonamiento. Es incompleto, impreciso e incierto, (a veces inconsistente y con errores o imprecisiones). Es por ello que las limitaciones de todo SE pueden ser que no conoce lo que conoce ni por qué, carece de imaginación, emociones, inteligencia innata, sentido común, etc. Tiene poco conocimiento de sí mismo, del usuario y del contexto de cada interacción y capacidad de razonamiento limitada por su estrategia de construcción.

Los \textbf{sistemas de producción} son modelos de cálculo que han probado su eficiencia en la Ingeniería del Conocimiento tanto en el desarrollo de algoritmos de búsqueda como en el modelado de problemas del dominio humano. Su origen se remonta a 1943 y consta de:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Las \textbf{reglas de producción} son la forma más extendida de representar el conocimiento, constan de Condiciones (Hipótesis) y Acciones (Conclusiones) y tienen la forma:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
   \FunctionTok{Si}\NormalTok{ (If)}
\NormalTok{     Condición }\DecValTok{1}
\NormalTok{     y Condición }\DecValTok{2}
\NormalTok{     ...}
\NormalTok{     y Condición n}
   \FunctionTok{Entonces}\NormalTok{ (Then)}
\NormalTok{     Conclusión }\DecValTok{1}
\NormalTok{     Conclusión }\DecValTok{2}
\NormalTok{     ...}
\NormalTok{     y Conclusión m}
\end{Highlighting}
\end{Shaded}

Es la forma más extendida de representar el conocimiento. Ejemplo de Regla de producción:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{   Si}
\NormalTok{     ha fallado la bombilla}
\NormalTok{     y hay una de repuesto}
\NormalTok{     y está útil}
\NormalTok{   Entonces}
\NormalTok{     cambiar la bombilla por la de repuesto}
\NormalTok{     y seguir trabajando}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Los \textbf{sistemas de producción} son modelos de cálculo que han probado su eficiencia en la Ingeniería del Conocimiento tanto en el desarrollo de algoritmos de búsqueda como en el modelado de problemas del dominio humano. Su origen se remonta a 1943 y consta de:
\end{enumerate}

\begin{itemize}
\item
  Conjunto de reglas de producción.
\item
  Memoria de trabajo (Memoria temporal).
\item
  Ciclo de reconocer y actuar.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\item
  \textbf{Memoria de trabajo}: Contiene una descripción del estado actual del mundo o entorno de la aplicación en cada paso del proceso de razonamiento. Esta descripción es un modelo que servirá para asociar las partes condición de las reglas con las observaciones del mundo con objeto de seleccionar o producir las apropiadas acciones. En el momento en que se cumplen todas las condiciones de una regla se produce el ``disparo'' de la misma ejecutándose la acción. Esta operación alterará el contenido de la memoria de trabajo.
\item
  \textbf{Ciclo de reconocimiento y actuación}: Es el procedimiento de control de un sistema de producción. Es un procedimiento de marcha hacia adelante. La memoria de trabajo se inicializa con la descripción del problema. Los modelos guardados en la memoria de trabajo se tratan de superponer en las condiciones de las producciones. Se crea un conjunto ``conflicto''. Este es un subconjunto de producciones cuyas condiciones se cumplen. Se escoge una producción y se ``dispara'' o se activa. La acción de la regla es disparada cambiando el contenido de la memoria de trabajo. Se repite todo el proceso descrito con la memoria de trabajo modificada. El proceso continúa hasta que no hay condiciones en las reglas que cumplan el contenido de los modelos de la memoria de trabajo.
\end{enumerate}

La utilización de los sistemas de producción en SE se remonta a los trabajos de Newell y Simon de la Universidad Carnegie Mellon entre 1960 y 1970. Vieron que los sistemas de producción podían utilizarse para modelar el conocimiento humano y hacer un posterior uso del mismo en la inferencia.

Las principales ventajas de los sistemas de producción en los SE son la separación del conocimiento y del control. Se pueden hacer cambios fáciles de reglas sin cambiar el control y viceversa. Modularidad de las reglas de producción e independencia del lenguaje de programación usado.

\hypertarget{razonamiento}{%
\subsection{Razonamiento}\label{razonamiento}}

El razonamiento se puede definir como el proceso de obtención de inferencias o conclusiones a partir de unos hechos u observaciones reales o asumidos y de un conocimiento previo. La inferencia es el proceso por el que a partir de unos hechos conocidos se obtienen conclusiones acerca de otros desconocidos \citep{Fleitas2017} y \citep{BeGu04}. La realización de este tipo de procesamiento es llevada a cabo por el denominado \textbf{motor de inferencia}.

El razonamiento automático ya se usaba en los 50 en juegos. En 1963 se presentó el sistema \emph{``General Problem Solver''} capaz de hacer inferencias lógicas (Newel y Simon). Aparición del razonamiento heurístico.

Tipos de razonamiento en SE:

\begin{itemize}
\item
  \textbf{Forward chaining} (encadenamiento hacia delante, deductivo, progresivo, dirigido por datos o hechos) Síntomas \(\rightarrow\) Causas
\item
  \textbf{Backward chaining} (encadenamiento hacia atrás, inductivo, regresivo, dirigido por metas u objetivos) Síntomas \(\leftarrow\) Causas
\item
  \emph{Forward/Backward chaining}.
\end{itemize}

Pasos del motor de inferencia:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Elaboración de un \textbf{conjunto conflicto} con todas las reglas cuyas condiciones se cumplen.
\item
  \textbf{Detección} (filtro) de reglas pertinentes o \textbf{selección} de reglas a partir de unos hechos. Se trata de obtener de la BC el conjunto de reglas aplicables en una situación determinada o estado de la BH.
\item
  \textbf{Aplicación de reglas} o \textbf{resolución del conflicto}. Consiste en seleccionar una regla del conjunto conflicto y dispararla (ejecutar su conclusión). Se altera la BH o memoria de trabajo.
\item
  \textbf{Vuelta a 1} hasta que el conjunto \textbf{conflicto} esté \textbf{vacío}.
\end{enumerate}

Ciclo de ``razonamiento hacia delante'':

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Parte de unas observaciones (\textbf{hechos}).
\item
  A partir de los hechos observados se \textbf{seleccionan las reglas} cuyas condiciones están relacionadas con ellos.
\item
  Las reglas seleccionadas son examinadas para ver si verifican todas sus condiciones. Aquéllas que las verifican constituyen el \textbf{``conjunto conflicto''}.
\item
  De todas aquellas que forman el conjunto conflicto se selecciona una sola y se activa (se dispara). La selección de una regla del conjunto conflicto es la \textbf{``resolución del conflicto''}
\item
  La activación de la regla provocará la \textbf{aparición de otros hechos} que se añaden a los observados y se \textbf{actualiza} la base de hechos.
\item
  Volver al \textbf{paso 2} hasta analizar todos los hechos observados y deducidos.
\end{enumerate}

En la Fig. \ref{fig:FiguraRazonamientoHaciaDelante} se muestra el algoritmo correspondiente al ciclo de inferencia hacia adelante.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/Figura1CicloRazonamientoHaciaDelante} 

}

\caption{Ciclo de razonamiento hacia delante}\label{fig:FiguraRazonamientoHaciaDelante}
\end{figure}

\hypertarget{sistema-experto-para-el-uxe1mbito-pediuxe1trico-en-atenciuxf3n-primaria}{%
\section{Sistema experto para el ámbito pediátrico en atención primaria}\label{sistema-experto-para-el-uxe1mbito-pediuxe1trico-en-atenciuxf3n-primaria}}

En la actualidad, uno de los principales problemas a los que se enfrentan los profesionales de la sanidad en el ámbito pediátrico de atención primaria en España es la falta del tiempo suficiente para realizar una evaluación clínica del estado del paciente. La necesidad de un mayor número de médicos especialistas en determinadas regiones y la aparición de picos de demanda motivados fenómenos como el COVID, o de modo estacional por otras enfermedades recurrentes, favorecen esta situación.

Ante esta problemática, los centros de salud tratan de optimizar sus recursos mediante diferentes vías, poniendo especial interés en realizar procesos de triaje que les permitan priorizar la atención a los pacientes según su nivel de urgencia. Para ello, en España se utilizan escalas como el MTS (Manchester Triage System), el SET (Sistema Español de Triaje) y el CPTAS (Canadian Pediatric Triage and Acuity Scale) para establecer el tiempo que un paciente puede esperar para recibir atención médica en base a sus síntomas y evolución.

Sin embargo, realizar un correcto proceso de triaje, además de requerir de un gran conocimiento experto, hace necesario un tiempo para una evaluación clínica que a veces resulta difícil de destinar. En este contexto se plantea el desarrollo de un Sistema Experto capaz de ayudar en el proceso de evaluación médica, con el objetivo de facilitar el proceso de triaje.

Como ya se sabe, el primer paso para el desarrollo de un Sistema Experto es la definición de un conjunto de reglas que modelen el conocimiento con el que se nutrirá. Para ello, es posible recurrir al apoyo de expertos, capaces de definir su conocimiento como reglas, o la aplicación de mecanismos de extracción de conocimiento a partir del procesamiento de conjuntos de datos y sucesos.

En este caso se recopilaron y procesaron un conjunto de datos relativos a motivos de consulta pediátrica en un centro de salud, donde la escala de triaje utilizada fue el CPTAS. Los datos fueron anonimizados, seleccionando únicamente aquellos campos que pudieran resultar clave para la extracción de conocimiento y la conformación de reglas. Adicionalmente, se contó con el apoyo de profesionales especialistas del ámbito pediátrico, para revisar y complementar algunas de las reglas extraídas. Un extracto de los datos utilizados para el proceso de extracción de reglas se muestra en la Tab. \ref{fig:TablaConsultaTriaje}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1127}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1408}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2535}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2535}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2394}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Sexo
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Edad
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Tiempo Evolución
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Causa
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Triaje
\end{minipage} \\
\midrule()
\endhead
Hombre & 1-3 años & 25-72 horas & Dermatológica & 5 (120 minutos) \\
Mujer & 1-3 años & 25-72 horas & Respiratoria & 3 (30 minutos) \\
Hombre & 4-6 años & 7-12 horas & Gastrointestinal & 5 (120 minutos) \\
Hombre & 4-6 años & 2-6 horas & Ocular & 4 (60 minutos) \\
Mujer & 4-6 años & 13-24 horas & Fiebre & 5 (120 minutos) \\
\bottomrule()
\end{longtable}

\label{tab:TablaConsultaTriaje} Ejemplo de datos de motivos de consulta y Triaje.

A partir del procesamiento de un total de 400 visitas médicas mediante un algoritmo de extracción de reglas de asociación como ``Magnum Opus'', basado en la definición original de \citet{Webb2011}, y la aplicación de conocimiento experto, se extrajeron un conjunto de reglas con suficiente calidad. A continuación, se se muestra un ejemplo del conjunto de reglas con \(10\) de ellas.

\begin{itemize}
\item
  R1: Si Causa Ginecologica o Edad mayor de 12 años \(\rightarrow\) Tiempo de Evolución mayor de 73h
\item
  R2: Si Causa Ginecologica \(\rightarrow\) Sexo Mujer
\item
  R3: Si Edad menor de 7 días o Causa Fiebre \(\rightarrow\) Tiempo de evolución de 1h
\item
  R4: Si Edad mayor de 12 años y Tiempo de Evolución mayor de 73h \(\rightarrow\) Causa Respiratoria
\item
  R5: Si Tiempo Evolución mayor de 73h y Causa Ocular \(\rightarrow\) Triaje 1
\item
  R6: Si Causa Respiratoria y Sexo Mujer \(\rightarrow\) Triaje 3
\item
  R7: Si Tiempo de Evolución es 2-6h o Causa Neurológica \(\rightarrow\) Triaje 2
\item
  R8: Si Causa Neurológica o Tiempo de Evolución es 2-6h \(\rightarrow\) Triaje 4
\item
  R9: Si Tiempo de Evolución es 13-24h y Causa Ginecológica y Sexo Mujer \(\rightarrow\) Triaje 5
\item
  R10: Si Tiempo de Evolución mayor de 73h \(\rightarrow\) Triaje 4
\end{itemize}

A continuación, se muestra el código en R para la implementación de un Sistema Experto capaz de procesar reglas como las anteriores, realizando una ejecución para tratar de obtener el valor de triaje más posible en caso, por ejemplo, de que el motivo de consulta de una niña sea ginecológico, su edad sea mayor de \(12\) años, y el tiempo de evolución sea superior a \(73\) horas.

Es importante señalar que, habitualmente, un sistema experto partirá de una base de hechos compuesta por decenas o cientos de reglas. No obstante, para simplificar el siguiente fragmento de código se considera únicamente la carga de \(10\) reglas a modo de ejemplo.

Se declaran las reglas que conformarán la base conocimiento del Sistema Experto: - La Base de Conocimientos del SE contiene 10 reglas.

\begin{itemize}
\item
  Cada regla se modela como una lista de antecedentes y un consecuente.
\item
  La relación entre los consecuentes se modela con el atributo ``operador'' del siguiente modo: \texttt{1=Y=\^{},\ 0=o=V,\ -1=no\ hay\ operaciones}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r1 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{antecedentes =} \FunctionTok{list}\NormalTok{(}\StringTok{"Causa\_Ginecologica"}\NormalTok{, }\StringTok{"Edad\_\textgreater{}12a"}\NormalTok{), }\AttributeTok{consecuente =} \FunctionTok{list}\NormalTok{(}\StringTok{"TiempoEvolucion\_\textgreater{}73h"}\NormalTok{), }\AttributeTok{operador =} \DecValTok{0}\NormalTok{)}
\NormalTok{r2 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{antecedentes =} \FunctionTok{list}\NormalTok{(}\StringTok{"Causa\_Ginecologica"}\NormalTok{), }\AttributeTok{consecuente =} \FunctionTok{list}\NormalTok{(}\StringTok{"Sexo\_Mujer"}\NormalTok{), }\AttributeTok{operador =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\NormalTok{r3 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{antecedentes =} \FunctionTok{list}\NormalTok{(}\StringTok{"Edad\_\textless{}7d"}\NormalTok{, }\StringTok{"Causa\_Fiebre"}\NormalTok{), }\AttributeTok{consecuente =} \FunctionTok{list}\NormalTok{(}\StringTok{"TiempoEvolucion\_1h"}\NormalTok{), }\AttributeTok{operador =} \DecValTok{0}\NormalTok{)}
\NormalTok{r4 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{antecedentes =} \FunctionTok{list}\NormalTok{(}\StringTok{"Edad\_\textgreater{}12a"}\NormalTok{, }\StringTok{"TiempoEvolucion\_\textgreater{}73h"}\NormalTok{), }\AttributeTok{consecuente =} \FunctionTok{list}\NormalTok{(}\StringTok{"Causa\_Respiratoria"}\NormalTok{), }\AttributeTok{operador =} \DecValTok{1}\NormalTok{)}
\NormalTok{r5 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{antecedentes =} \FunctionTok{list}\NormalTok{(}\StringTok{"TiempoEvolucion\_\textgreater{}73h"}\NormalTok{, }\StringTok{"Causa\_Ocular"}\NormalTok{), }\AttributeTok{consecuente =} \FunctionTok{list}\NormalTok{(}\StringTok{"Triaje\_1"}\NormalTok{), }\AttributeTok{operador =} \DecValTok{1}\NormalTok{)}
\NormalTok{r6 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{antecedentes =} \FunctionTok{list}\NormalTok{(}\StringTok{"Causa\_Respiratoria"}\NormalTok{, }\StringTok{"Sexo\_Mujer"}\NormalTok{), }\AttributeTok{consecuente =} \FunctionTok{list}\NormalTok{(}\StringTok{"Triaje\_3"}\NormalTok{), }\AttributeTok{operador =} \DecValTok{1}\NormalTok{)}
\NormalTok{r7 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{antecedentes =} \FunctionTok{list}\NormalTok{(}\StringTok{"TiempoEvolucion\_2{-}6h"}\NormalTok{, }\StringTok{"Causa\_Neurologica"}\NormalTok{), }\AttributeTok{consecuente =} \FunctionTok{list}\NormalTok{(}\StringTok{"Triaje\_2"}\NormalTok{), }\AttributeTok{operador =} \DecValTok{0}\NormalTok{)}
\NormalTok{r8 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{antecedentes =} \FunctionTok{list}\NormalTok{(}\StringTok{"Causa\_Neurológica"}\NormalTok{, }\StringTok{"TiempoEvolucion\_2{-}6h"}\NormalTok{), }\AttributeTok{consecuente =} \FunctionTok{list}\NormalTok{(}\StringTok{"Triaje\_4"}\NormalTok{), }\AttributeTok{operador =} \DecValTok{0}\NormalTok{)}
\NormalTok{r9 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{antecedentes =} \FunctionTok{list}\NormalTok{(}\StringTok{"TiempoEvolucion\_13{-}24h"}\NormalTok{, }\StringTok{"Causa\_Ginecologica"}\NormalTok{, }\StringTok{"Sexo\_Mujer"}\NormalTok{), }\AttributeTok{consecuente =} \FunctionTok{list}\NormalTok{(}\StringTok{"Triaje\_5"}\NormalTok{), }\AttributeTok{operador =} \DecValTok{1}\NormalTok{)}
\NormalTok{r10 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{antecedentes =} \FunctionTok{list}\NormalTok{(}\StringTok{"TiempoEvolucion\_\textgreater{}73h"}\NormalTok{), }\AttributeTok{consecuente =} \FunctionTok{list}\NormalTok{(}\StringTok{"Triaje\_4"}\NormalTok{), }\AttributeTok{operador =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}

\CommentTok{\# r1 regla completa}
\CommentTok{\# r1[1] lista antecedentes}
\CommentTok{\# r1[[1]] [1] primero de los antecedentes}
\CommentTok{\# r1[2] lista consecuentes}
\CommentTok{\# r1[[2]] [1] primero de los consecuentes}

\NormalTok{b\_hechos }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\StringTok{"Causa\_Ginecologica"}\NormalTok{, }\StringTok{"Edad\_\textgreater{}12a"}\NormalTok{, }\StringTok{"TiempoEvolucion\_\textgreater{}73h"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Se inicializa la Base de Conocimiento con el conjunto de Reglas y la Base de Hechos con la circunstancia a evaluar

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Se añaden todas las reglas a la Base de Conocimiento}
\NormalTok{b\_conocimiento }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(r1, r2, r3, r4, r5, r6, r7, r8, r9, r10)}
\end{Highlighting}
\end{Shaded}

Se define una función para comprobar la existencia de un número en una lista. Esta función será usada por el motor del Sistema Experto.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Función para comprobar si una lista contiene un número}

\NormalTok{contiene }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(numero, lista) \{}
\NormalTok{  existe }\OtherTok{\textless{}{-}} \ConstantTok{FALSE}
  \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{length}\NormalTok{(lista) }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) \{}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(lista)) \{}
      \ControlFlowTok{if}\NormalTok{ (numero }\SpecialCharTok{==}\NormalTok{ lista[[i]]) existe }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\NormalTok{    \}}
\NormalTok{  \}}
  \FunctionTok{return}\NormalTok{(existe)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Se implementa el motor del Sistema Experto.

El algoritmo ejecuta un bucle en el que, en cada iteración, evalúa las reglas disponibles contenidas en la Base de Conocimiento. Considerando los items de la Base de Hechos, si una regla puede ser disparada, se añade al Conjunto Conflicto. La regla disparada en una iteración será la primera disponible en el Conjunto Conflicto. El Conjunto Conflicto se inicializa en cada iteración. El consecuente de la regla disparada se añade a la Base de Hechos.Cada regla solo puede dispararse una vez, por lo que se actualiza una lista de reglas risparadas. El algoritmo finaliza cuando el Conflicto queda vacío, al haber sido disparadas todas las reglas o no existir más candidatas a ser disparadas.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Motor del Sistema Experto}

\NormalTok{AlgoritmoSE }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(b\_hechos, b\_conocimiento) \{}
\NormalTok{  c\_conflicto }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\NormalTok{  r\_disparadas }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\NormalTok{  condicion }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\NormalTok{  iteracion }\OtherTok{\textless{}{-}} \DecValTok{0}
  \ControlFlowTok{while}\NormalTok{ (condicion) \{}
\NormalTok{    c\_conflicto }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Iteración: "}\NormalTok{, iteracion, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{    iteracion }\OtherTok{\textless{}{-}}\NormalTok{ iteracion }\SpecialCharTok{+} \DecValTok{1}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(b\_conocimiento)) \{}
      \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{contiene}\NormalTok{(i, r\_disparadas)) \{}
        \ControlFlowTok{if}\NormalTok{ (b\_conocimiento[[i]][[}\DecValTok{3}\NormalTok{]][[}\DecValTok{1}\NormalTok{]] }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) \{}
\NormalTok{          r\_disparada }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\NormalTok{        \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{          r\_disparada }\OtherTok{\textless{}{-}} \ConstantTok{FALSE}
\NormalTok{        \}}
        \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(b\_conocimiento[[i]][[}\DecValTok{1}\NormalTok{]])) \{}
\NormalTok{          antecedente }\OtherTok{\textless{}{-}} \ConstantTok{FALSE}
          \ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(b\_hechos)) \{}
            \ControlFlowTok{if}\NormalTok{ (b\_conocimiento[[i]][[}\DecValTok{1}\NormalTok{]][[j]] }\SpecialCharTok{==}\NormalTok{ b\_hechos[[k]]) \{}
              \ControlFlowTok{if}\NormalTok{ (b\_conocimiento[[i]][[}\DecValTok{3}\NormalTok{]][[}\DecValTok{1}\NormalTok{]] }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{||}\NormalTok{ b\_conocimiento[[i]][[}\DecValTok{3}\NormalTok{]][[}\DecValTok{1}\NormalTok{]] }\SpecialCharTok{==} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{) r\_disparada }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\NormalTok{              antecedente }\OtherTok{\textless{}{-}} \ConstantTok{TRUE}
\NormalTok{            \}}
\NormalTok{          \}}
          \ControlFlowTok{if}\NormalTok{ (b\_conocimiento[[i]][[}\DecValTok{3}\NormalTok{]][[}\DecValTok{1}\NormalTok{]] }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) \{}
            \ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\NormalTok{antecedente) r\_disparada }\OtherTok{\textless{}{-}} \ConstantTok{FALSE}
\NormalTok{          \}}
\NormalTok{        \}}
        \ControlFlowTok{if}\NormalTok{ (r\_disparada) \{}
          \FunctionTok{cat}\NormalTok{(}\StringTok{"Regla"}\NormalTok{, i, }\StringTok{"añadida a Conjunto conflicto}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{          c\_conflicto[}\FunctionTok{length}\NormalTok{(c\_conflicto) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ i}
\NormalTok{        \}}
\NormalTok{      \}}
\NormalTok{    \}}
    \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{length}\NormalTok{(c\_conflicto) }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) \{}
      \CommentTok{\# str("Conjunto conflicto:")}
      \CommentTok{\# str(c\_conflicto)}
\NormalTok{      r\_disparadas[}\FunctionTok{length}\NormalTok{(r\_disparadas) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ c\_conflicto[}\DecValTok{1}\NormalTok{]}
      \FunctionTok{cat}\NormalTok{(}\StringTok{"Regla"}\NormalTok{, r\_disparadas[[}\FunctionTok{length}\NormalTok{(r\_disparadas)]], }\StringTok{"disparada}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{      b\_hechos[}\FunctionTok{length}\NormalTok{(b\_hechos) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ b\_conocimiento[[r\_disparadas[[}\FunctionTok{length}\NormalTok{(r\_disparadas)]]]][[}\DecValTok{2}\NormalTok{]][[}\DecValTok{1}\NormalTok{]]}
      \FunctionTok{str}\NormalTok{(}\StringTok{"Base de hechos:"}\NormalTok{)}
      \FunctionTok{str}\NormalTok{(b\_hechos)}
      \FunctionTok{cat}\NormalTok{(}\StringTok{"Consecuente:"}\NormalTok{, b\_conocimiento[[r\_disparadas[[}\FunctionTok{length}\NormalTok{(r\_disparadas)]]]][[}\DecValTok{2}\NormalTok{]][[}\DecValTok{1}\NormalTok{]], }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{      condicion }\OtherTok{\textless{}{-}} \ConstantTok{FALSE}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Ahora considerese, por ejemplo, una posible paciente con más de 12 años de Edad, que acude a consulta por causa Ginecológica con un Tiempo de Evolución de los síntomas mayor de 73h. ¿Cuál será el triaje correspondiente? Para conocerlo, se inicializa la base de hechos con la situación propuesta (Causa\_Ginecologica, Edad\_\textgreater12a y TiempoEvolucion\_\textgreater73h), y se ejecuta el motor del Sistema Experto.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Se inicializa la base de hechos con la situación propuesta (paciente de más de 12 años, por causa ginecológica con tiempo de evolución mayor de 73h)}
\NormalTok{b\_hechos }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\StringTok{"Causa\_Ginecologica"}\NormalTok{, }\StringTok{"Edad\_\textgreater{}12a"}\NormalTok{, }\StringTok{"TiempoEvolucion\_\textgreater{}73h"}\NormalTok{)}

\CommentTok{\# Se lanza la ejecución del motor del Sistema Experto}
\FunctionTok{AlgoritmoSE}\NormalTok{(b\_hechos, b\_conocimiento)}
\end{Highlighting}
\end{Shaded}

El resultado del algoritmo, en este caso (motivo de consulta ginecológico y mayor de \(12\) años y con un tiempo de evolución de los síntomas mayor de 73h) es un triaje de valor \(4\). Es decir, su tiempo de espera máximo para recibir atención médica debería ser inferior a \(60\) minutos. La Tab. \ref{tab:TablaIteracionesAlgoritmo} muestra el proceso realizado por el algoritmo en cada iteración.

\begin{center}\includegraphics[width=0.6\linewidth]{img/Tabla2-sist-exp} \end{center}

\label{tab:TablaIteracionesAlgoritmo} Proceso de ejecución del Sistema Experto.

Como puede observarse, el algoritmo finaliza tras \(5\) iteraciones al alcanzar un conjunto conflicto vacío, dando como resultado un valor de \(4\) para el triaje. A continuación, se describe el proceso ejecutado en cada una de las iteraciones.

\begin{itemize}
\item
  \textbf{Iteración 0:} La Base de Hechos se inicializa con las condiciones establecidas en el ejemplo de consulta médica considerado, es decir, \texttt{Causa\_Ginecologica}, \texttt{Edad\_\textgreater{}12a} y \texttt{TiempoEvolucion\_\textgreater{}73h}. En el Conjunto Conflicto se incluyen aquellas reglas que podrían ser lanzadas con los elementos contenidos en la Base de Hechos es decir, las reglas R1, R2, R4, R10. Se dispara la regla R1, al ser la primera de la lista de reglas del Conjunto Conflicto. El consecuente de la regla disparada (\texttt{TiempoEvolucion\_\textgreater{}73h}) se añade a la Base de Hechos para la siguiente iteración, aunque en este caso no es necesario porque ya lo contiene. La iteración finaliza estableciendo como conclusión el consecuente de la regla disparada, es decir, \texttt{TiempoEvolucion\_\textgreater{}73h}.
\item
  \textbf{Iteración 1:} El Conjunto Conflicto se inicializa con las reglas que podrían ser lanzadas, excluyendo las ya ejecutadas (R1), a partir de los elementos incluidos en la Base de Hechos tras la iteración anterior, es decir, las reglas R2, R4 y R10. Se lanza la primera de las reglas del Conjunto Conflicto, es decir, R2. El consecuente de la regla disparada (\texttt{Sexo\_Mujer}) se añade a la Base de Hechos. La iteración finaliza estableciendo como conclusión el consecuente de la regla disparada, es decir, \texttt{Sexo\_Mujer}.
\item
  \textbf{Iteración 2:} El Conjunto Conflicto se inicializa con las reglas que podrían ser lanzadas, excluyendo las ya ejecutadas (R1 y R2), a partir de los elementos contenidos en la Base de Hechos tras la iteración anterior, es decir, las reglas R4 y R10. Se lanza la primera de las reglas del Conjunto Conflicto, es decir, R4. El consecuente de la regla disparada (Causa\_Respiratoria) se añade a la Base de Hechos. La iteración finaliza estableciendo como conclusión el consecuente de la regla disparada, es decir, \texttt{Causa\_Respiratoria}.
\item
  \textbf{Iteración 3:} El Conjunto Conflicto se inicializa con las reglas que podrían ser lanzadas, excluyendo las ya ejecutadas (R1, R2 y R4), a partir de los elementos contenidos en la Base de Hechos tras la iteración anterior, es decir, las reglas R6 y R10. Se lanza la primera de las reglas del Conjunto Conflicto, es decir, R6. El consecuente de la regla disparada (\texttt{Triaje\_3}) se añade a la Base de Hechos. La iteración finaliza estableciendo como conclusión el consecuente de la regla disparada, es decir, \texttt{Triaje\_3}.
\item
  \textbf{Iteración 4:} El Conjunto Conflicto se inicializa con las reglas que podrían ser lanzadas, excluyendo las ya ejecutadas (R1, R2, R4 y R6), a partir de los elementos contenidos en la Base de Hechos tras la iteración anterior, en este caso únicamente R10. Se lanza por tanto la regla R10. El consecuente de la regla disparada (\texttt{Triaje\_4}) se añade a la Base de Hechos. La iteración finaliza estableciendo como conclusión el consecuente de la regla disparada, es decir, \texttt{Triaje\_4}.
\item
  \textbf{Iteración 5:} El Conjunto Conflicto queda vacío, al no ser posible incluir en él reglas no disparadas aún y que pudieran ser ejecutadas a partir de los elementos contenidos en la Base de Hechos. Por tanto, el algoritmo finaliza concluyendo como resultado el consecuente de la última regla lanzadas, es decir, \texttt{Triaje\_4}.
\end{itemize}

Sin lugar a duda, la implementación del algoritmo, las reglas y el modelado considerado para este caso de estudio resulta una simplificación intencionada del problema, con el único objetivo de facilitar su comprensión desde una perspectiva académica y docente.

En un escenario de aplicación real, el volumen y la complejidad de las reglas debería ser mayor, considerando además la posibilidad de incorporar otras características que permitieran modelar de un modo más completo el estado de salud del paciente y su motivo de consulta.

Actualmente resultan innumerables los ámbitos donde la aplicación de Sistemas Expertos puede ser de ayuda. En este caso de estudio, el objetivo ha sido facilitar y mejorar el proceso de triaje del nivel de urgencia en el ámbito pediátrico en atención primaria. Pero, en este mismo contexto, podría valorarse su uso como herramienta de apoyo para, por ejemplo, el diagnóstico de enfermedades o la prescripción de tratamientos.

La tecnología actual y lenguajes de programación como R facilitan la implementación de Sistemas Expertos de un modo rápido y sencillo. Considérese como herramienta de ayuda para situaciones en las que aplicar conocimiento experto resulte clave para la solución de problemas.

\hypertarget{nlp-textil}{%
\chapter{El NLP y las tendencias en el mundo de la moda}\label{nlp-textil}}

\emph{Ambrosio Nguema Ansue}

\hypertarget{introducciuxf3n-29}{%
\section{Introducción}\label{introducciuxf3n-29}}

El modelado de temas es el nombre colectivo de las técnicas que ayudan a dividir los textos en temas, por lo que la determinación de los temas relevantes es parte del análisis. Como se trata de una técnica no supervisada, requiere muchas iteraciones.
Las técnicas de PNL más novedosas podrían superar a los modelos temáticos en términos de precisión. Sin embargo, una gran ventaja de usar los temas extraídos del texto en una tarea posterior es que la contribución de cada tema (y otras características que tiene disponibles) en el modelo de predicción se analizan y visualizan fácilmente. Esto es útil para cualquier empresa y para cualquiera a quien le esté explicando el modelo predictivo.

\hypertarget{nlp-para-tendencias-de-moda-en-textil}{%
\section{NLP para tendencias de moda en textil}\label{nlp-para-tendencias-de-moda-en-textil}}

El conjunto \texttt{clothes} de datos de reseñas y calificaciones de ropa de comercio electrónico para mujeres contiene 23.486 entradas relacionadas con la edad y la revisión dada por el cliente y sus opiniones sobre la ropa de mujer de varios minoristas.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(CDR)}
\FunctionTok{library}\NormalTok{(readr)}
\FunctionTok{library}\NormalTok{(tidytext)}
\end{Highlighting}
\end{Shaded}

El primer registro presenta la siguiente estructura la información:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(clothes)[}\DecValTok{1}\NormalTok{, ]}
\CommentTok{\#\textgreater{}    ID Age Title                                                Review Rating}
\CommentTok{\#\textgreater{} 1 767  33  \textless{}NA\textgreater{} Absolutely wonderful {-} silky and sexy and comfortable      4}
\CommentTok{\#\textgreater{}   Recommend Liked  Division     Dept     Class}
\CommentTok{\#\textgreater{} 1         1     0 Initmates Intimate Intimates}
\end{Highlighting}
\end{Shaded}

Respecto al \textbf{Departamento} con mayor porcentaje de revisiones/calificaciones
puede verse que es \texttt{Tops} y el que obtiene el menor porcentaje \texttt{Jackets}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{ggplot}\NormalTok{(}
  \FunctionTok{data.frame}\NormalTok{(}\FunctionTok{prop.table}\NormalTok{(}\FunctionTok{table}\NormalTok{(clothes}\SpecialCharTok{$}\NormalTok{Dept))),}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Var1, }\AttributeTok{y =}\NormalTok{ Freq }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\AttributeTok{fill =} \StringTok{"blue"}\NormalTok{)}
\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Department Name"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Percentage of Reviews/Ratings (\%)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =} \FunctionTok{round}\NormalTok{(Freq }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{2}\NormalTok{)), }\AttributeTok{vjust =} \SpecialCharTok{{-}}\FloatTok{0.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.4\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/unnamed-chunk-351-1} 

}

\caption{Percentage of Reviews By Department}\label{fig:unnamed-chunk-351}
\end{figure}

En cuanto a las \textbf{calificaciones por departamento}, excluida la tendencia,
ya que contiene una mezcla de ropa que puede encajar en las otras categorías del Departamento, el análisis se centra en 5 departamentos: \texttt{Bottoms}, \texttt{Dresses}, \texttt{Intimate}, y \texttt{Tops}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{phisto }\OtherTok{\textless{}{-}}\NormalTok{ clothes }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(Dept), Dept }\SpecialCharTok{!=} \StringTok{"Trend"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{Dept =} \FunctionTok{factor}\NormalTok{(Dept)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(Dept) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{count}\NormalTok{(Rating) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{perc =}\NormalTok{ n }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(n))}

\NormalTok{phisto }\SpecialCharTok{|\textgreater{}} \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Rating, }\AttributeTok{y =}\NormalTok{ perc }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\AttributeTok{fill =}\NormalTok{ Dept)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{, }\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Dept) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Percentage of reviews (\%)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =} \FunctionTok{round}\NormalTok{(perc }\SpecialCharTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{2}\NormalTok{)), }\AttributeTok{vjust =} \SpecialCharTok{{-}}\NormalTok{.}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{65}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/phisto-1} \end{center}

En cada Departamento, la calificación dominante otorgada es de 5 estrellas.
\texttt{Jacket} tiene el mayor número de calificaciones de 5 estrellas de todos los departamentos aunque \texttt{Jacket} fue el que tuvo la menor cantidad de reseñas.

Si se analizan los \textbf{departamentos por edad} se observa que la distribución del número de reseñas por Departamento (i.e.~\texttt{Tops} con el mayor número de reseñas,
\texttt{Dresses} con el segundo mayor número de reseñas, etc.)
se mantienen dentro de cada uno de los grupos de edad mostrados.
Las personas de 30 años dejaron la mayoría de las reseñas,
seguidas por las personas de 40 y 50 años.
Esto les da a las empresas una idea de quién es el grupo demográfico objetivo y
qué tipo de ropa (camisas, vestidos) tienen demanda.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ages }\OtherTok{\textless{}{-}}\NormalTok{ clothes }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(Age), }\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(Dept), Dept }\SpecialCharTok{!=} \StringTok{"Trend"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(ID, Age, Dept) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{Age\_group =} \FunctionTok{ifelse}\NormalTok{(Age }\SpecialCharTok{\textless{}} \DecValTok{30}\NormalTok{, }\StringTok{"18{-}29"}\NormalTok{, }\FunctionTok{ifelse}\NormalTok{(Age }\SpecialCharTok{\textless{}} \DecValTok{40}\NormalTok{, }\StringTok{"30{-}39"}\NormalTok{, }\FunctionTok{ifelse}\NormalTok{(Age }\SpecialCharTok{\textless{}} \DecValTok{50}\NormalTok{, }\StringTok{"40{-}49"}\NormalTok{, }\FunctionTok{ifelse}\NormalTok{(Age }\SpecialCharTok{\textless{}} \DecValTok{60}\NormalTok{, }\StringTok{"50{-}59"}\NormalTok{, }\FunctionTok{ifelse}\NormalTok{(Age }\SpecialCharTok{\textless{}} \DecValTok{70}\NormalTok{, }\StringTok{"60{-}69"}\NormalTok{, }\FunctionTok{ifelse}\NormalTok{(Age }\SpecialCharTok{\textless{}} \DecValTok{80}\NormalTok{, }\StringTok{"70{-}79"}\NormalTok{, }\FunctionTok{ifelse}\NormalTok{(Age }\SpecialCharTok{\textless{}} \DecValTok{90}\NormalTok{, }\StringTok{"80{-}89"}\NormalTok{, }\StringTok{"90{-}99"}\NormalTok{))))))))}

\NormalTok{ages }\OtherTok{\textless{}{-}}\NormalTok{ ages }\SpecialCharTok{|\textgreater{}} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Age\_group =} \FunctionTok{factor}\NormalTok{(Age\_group), }\AttributeTok{Dept =} \FunctionTok{factor}\NormalTok{(Dept, }\AttributeTok{levels =} \FunctionTok{rev}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Tops"}\NormalTok{, }\StringTok{"Dresses"}\NormalTok{, }\StringTok{"Bottoms"}\NormalTok{, }\StringTok{"Intimate"}\NormalTok{, }\StringTok{"Jackets"}\NormalTok{))))}

\NormalTok{ages }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(Age }\SpecialCharTok{\textless{}} \DecValTok{80}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(Age\_group) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{count}\NormalTok{(Dept) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(Dept, n, }\AttributeTok{fill =}\NormalTok{ Age\_group)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{, }\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Age\_group, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Department"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Number of Reviews"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ n), }\AttributeTok{hjust =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(.}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/ages-1} \end{center}

Para hacer un \textbf{análisis de bigrama}, se eliminan las entradas que no tienen reseñas.
Hubo 845 revisiones NA, por lo que 845/23486 * 100 = 3,6 \% de calificaciones no se tendrán en cuenta. Además, se combinó el título con la reseña para reunir todas las palabras en una sola sección.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{clothesr }\OtherTok{\textless{}{-}}\NormalTok{ clothes }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(Review))}
\NormalTok{notitle }\OtherTok{\textless{}{-}}\NormalTok{ clothesr }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(Title)) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Title)}
\NormalTok{wtitle }\OtherTok{\textless{}{-}}\NormalTok{ clothesr }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(Title)) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  tidyr}\SpecialCharTok{::}\FunctionTok{unite}\NormalTok{(Review, }\FunctionTok{c}\NormalTok{(Title, Review), }\AttributeTok{sep =} \StringTok{" "}\NormalTok{)}
\NormalTok{main }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(notitle, wtitle)}
\end{Highlighting}
\end{Shaded}

Se ordenan las palabras vacías y se eliminan los dígitos. Se agrupan las palabras de acuerdo con sus calificaciones y se trazaron los 10 bigramas más comunes para cada calificación.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bigramming }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data) \{}
\NormalTok{  cbigram }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{|\textgreater{}} \FunctionTok{unnest\_tokens}\NormalTok{(bigram, Review, }\AttributeTok{token =} \StringTok{"ngrams"}\NormalTok{, }\AttributeTok{n =} \DecValTok{2}\NormalTok{)}
\NormalTok{  cbigram\_sep }\OtherTok{\textless{}{-}}\NormalTok{ cbigram }\SpecialCharTok{|\textgreater{}}
\NormalTok{    tidyr}\SpecialCharTok{::}\FunctionTok{separate}\NormalTok{(bigram, }\FunctionTok{c}\NormalTok{(}\StringTok{"first"}\NormalTok{, }\StringTok{"second"}\NormalTok{), }\AttributeTok{sep =} \StringTok{" "}\NormalTok{)}
\NormalTok{  cbigram2 }\OtherTok{\textless{}{-}}\NormalTok{ cbigram\_sep }\SpecialCharTok{|\textgreater{}}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(}
      \SpecialCharTok{!}\NormalTok{first }\SpecialCharTok{\%in\%}\NormalTok{ stop\_words}\SpecialCharTok{$}\NormalTok{word, }\SpecialCharTok{!}\NormalTok{second }\SpecialCharTok{\%in\%}\NormalTok{ stop\_words}\SpecialCharTok{$}\NormalTok{word,}
      \SpecialCharTok{!}\NormalTok{stringr}\SpecialCharTok{::}\FunctionTok{str\_detect}\NormalTok{(first, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d"}\NormalTok{), }\SpecialCharTok{!}\NormalTok{stringr}\SpecialCharTok{::}\FunctionTok{str\_detect}\NormalTok{(second, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d"}\NormalTok{)}
\NormalTok{    ) }\SpecialCharTok{|\textgreater{}}
\NormalTok{    tidyr}\SpecialCharTok{::}\FunctionTok{unite}\NormalTok{(bigram, }\FunctionTok{c}\NormalTok{(first, second), }\AttributeTok{sep =} \StringTok{" "}\NormalTok{)}
  \FunctionTok{return}\NormalTok{(cbigram2)}
\NormalTok{\}}

\NormalTok{top\_bigrams }\OtherTok{\textless{}{-}} \FunctionTok{bigramming}\NormalTok{(main) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Rating =} \FunctionTok{factor}\NormalTok{(Rating, levels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\SpecialCharTok{:}\DecValTok{1}\NormalTok{))) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{bigram =} \FunctionTok{factor}\NormalTok{(bigram, }\AttributeTok{levels =} \FunctionTok{rev}\NormalTok{(}\FunctionTok{unique}\NormalTok{(bigram)))) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(Rating) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{count}\NormalTok{(bigram, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{top\_n}\NormalTok{(}\DecValTok{10}\NormalTok{, n) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{()}

\NormalTok{top\_bigrams }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(bigram, n, }\AttributeTok{fill =}\NormalTok{ Rating)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Rating, }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{y =} \StringTok{"frequency"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Most Common Bigrams (By Ratings)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.6\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/bigrama-1} \end{center}

No hace falta decir que hay frases positivas para las calificaciones más altas
y frases negativas para las calificaciones más bajas.

Finalmente, se lleva a cabo una visualización mediante una \textbf{nube de palabras} para
mostrar las palabras compartidas dentro de los bigramas más comunes. Se analizan
las reseñas de 5 estrellas y las reseñas de 1 estrella para analizar lo bueno o lo malo de las prendas.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{onerating }\OtherTok{\textless{}{-}}\NormalTok{ main }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(Rating }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)}
\NormalTok{fiverating }\OtherTok{\textless{}{-}}\NormalTok{ main }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(Rating }\SpecialCharTok{==} \DecValTok{5}\NormalTok{)}
\NormalTok{one }\OtherTok{\textless{}{-}} \FunctionTok{bigramming}\NormalTok{(onerating) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{count}\NormalTok{(bigram, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{five }\OtherTok{\textless{}{-}} \FunctionTok{bigramming}\NormalTok{(fiverating) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{count}\NormalTok{(bigram, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Nube de palabras para 1 estrella:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(wordcloud2)}
\FunctionTok{wordcloud2}\NormalTok{(}
\NormalTok{  one }\SpecialCharTok{|\textgreater{}}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(n }\SpecialCharTok{\textgreater{}} \DecValTok{5}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{n =} \FunctionTok{sqrt}\NormalTok{(n)),}
  \AttributeTok{size =}\NormalTok{ .}\DecValTok{4}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.5\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/nube-palabras-nlp-1} \end{center}

Nube de palabras para 5 estrellas:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{wordcloud2}\NormalTok{(five }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(n }\SpecialCharTok{\textgreater{}} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{n =} \FunctionTok{sqrt}\NormalTok{(n)), }\AttributeTok{size =}\NormalTok{ .}\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.5\linewidth]{Ciencia_de_datos_con_r_files/figure-latex/nube-5estrellas-1} \end{center}

Los \textbf{puntos clave} del análisis se resumen en: (i) Las reseñas de 5 estrellas son dominantes en cada departamento y las chaquetas tienen la proporción más alta de todos los departamento. (ii) Los clientes de entre 30 y 40 años dejan la mayoría de las reseñas.
El ajuste, la comodidad/calidad del material, la estética de la prenda influyen en la calificación. (iii) Al realizar análisis de datos exploratorio y análisis de bigramas,
las empresas pueden concentrarse en lo que funciona y lo que no. Conocer la demografía de los revisores puede informar las decisiones de marketing (por ejemplo, anuncios en línea en los sitios más visitados por personas de 30 y 40 años). Seleccionar artículos que tengan telas flexibles y cómodas puede conducir a una mayor satisfacción del cliente. Un mayor número de críticas positivas se convierte en una forma de publicidad y
puede conducir a mayores ventas.

\hypertarget{cap-fraude}{%
\chapter{Detección de fraude de tarjetas de crédito}\label{cap-fraude}}

\emph{Pedro Albarracín}

\hypertarget{introducciuxf3n-30}{%
\section{Introducción}\label{introducciuxf3n-30}}

En un informe publicado en diciembre de 2021 por Nilson Report (\url{https://nilsonreport.com/upload/content_promo/NilsonReport_Issue1209.pdf}) se informó que los emisores de tarjetas de crédito, comerciantes y consumidores sufrieron un total de 28.580 millones de dólares de pérdidas por fraude en 2020, es decir, 6,8 centavos por cada 100 dólares en volumen de compras. El fraude sólo en USA representa el 35,83\% del total mundial.

En Europa la situación no es más alentadora. Según un informe del Banco Central Europeo publicado en 2020 (\url{https://www.ecb.europa.eu/pub/cardfraud/html/ecb.cardfraudreport202008~521edb602b.en.html\#toc2}), el valor total de las transacciones con tarjeta en la zona SEPA ascendieron a 4.84 billones de euros en 2018, de los cuales 1.800 millones correspondieron a operaciones fraudulentas.

Las entidades financieras trabajan a diario en el desarrollo de modelos de machine learning y deep learning que les permitan detectar, con la mayor precisión posible, aquellas operaciones de compra con tarjetas de crédito, débito o prepago que puedan ser sospechosas de fraude, o que al menos puedan ser identificadas como anómalas. En este sentido es importante destacar que no existe una única solución posible ya que el problema presenta, en la mayoría de los casos, múltiples variantes que hacen de éste, un problema complejo y que puede y debe ser abordado desde múltiples perspectivas y con diferentes enfoques.

En primer lugar, es posible identificar dos tipos de fraude. Por un lado, el que se comete físicamente, como, por ejemplo, la compra o la retirada de efectivo con tarjetas robadas o falsas. Por otro lado, están aquellas transacciones fraudulentas que se cometen online, en las que no es necesaria la tarjeta física, y en las que se utilizan los datos de las tarjetas obtenidas por los delincuentes mediante técnicas como el phishing y utilizados posteriormente para realizar pagos online.

Otro hándicap asociado a este tipo de escenarios es el derivado de la gran diversidad de fuentes de datos que forman parte de una transacción y que pueden dar lugar a divergencias metodológicas, tanto en la recogida y transmisión de los datos, como en su posterior almacenaje, lo que ocasiona que en muchos casos la calidad de los datos disponibles no sea la esperada o simplemente nos encontremos ante datasets inconsistentes. Los datos requeridos para este caso de uso pueden categorizarse en variables relativas a:

• Cliente

• Transacción

• Geolocalización

• Comercio

• Tarjetas

• Hábitos de compra

Cada una de estas categorías y otras que puedan aparecer aportan información que permite abordar el problema desde diferentes ángulos. Por un lado, es posible enfocar el problema desde el punto de vista del cliente y sus hábitos de compra para ver si existe alguna característica anómala en una transacción, tal vez la hora de la compra, o quizás analizar los datos de geolocalización junto con los del comercio para analizar si es una compra en un comercio habitual y desde una localización conocida, etc.

\hypertarget{modelizaciuxf3n-del-fraude-en-la-compra-con-tarjetas-de-cruxe9rdito}{%
\section{Modelización del fraude en la compra con tarjetas de crérdito}\label{modelizaciuxf3n-del-fraude-en-la-compra-con-tarjetas-de-cruxe9rdito}}

El objetivo que se presenta en este capítulo es la construcción de un modelo que permita detectar si una operación de compra realizada con tarjeta de crédito es fraudulenta o no. Para ello se utilizará un dataset anonimizado de operaciones con tarjeta de crédito ya etiquetadas disponible desde la web de \href{https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud}{Kaggle} y contenido en el paquete \texttt{CDR} con el nombre de \texttt{creditcard}.

Es un dataset con 284.807 transacciones de las cuales 492 están etiquetadas como fraudulentas, es decir, sólo un 0.172\% del total de las transacciones. Es un dataset, por lo tanto, muy desbalanceado, lo que añade cierto grado de dificultad al modelo. El dataset un conjunto de 31 variables, de las cuales 28 están identificadas como V1 a V28, una variable ``Time'' que registra los segundos transcurridos desde esa transacción y la primera, la variable ``Amount'' que registra el importe de la transacción y la variable dependiente ``Clase'' que indica con valor 0 que la operación es ``no fraudulenta'' y con valor 1 las operaciones fraudulentas.

Para concluir esta breve descripción del dataset es necesario recordar que todos los valores de entrada son numéricos y que ya han sufrido algunas transformaciones. Por motivos de confidencialidad las variables V1 a V28 no incluyen sus nombres originales ni se añade más información de contexto.

\textbf{Carga de los datos y obtención de descriptivos}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Carga del paquete readr}
\FunctionTok{library}\NormalTok{(readr)}

\CommentTok{\# Se crea una variable de tipo dataframe que contendrá los datos del CSV}
\NormalTok{creditcard }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/creditcard.csv"}\NormalTok{, }\AttributeTok{show\_col\_types =} \ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# Se obtienen unos datos descriptivos del dataset}
\FunctionTok{library}\NormalTok{(psych)}
\FunctionTok{describe}\NormalTok{(creditcard)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\textbf{División de los datos}

A continuación es necesario dividir los datos en dos dataframes que denominados ``creditcard\_X'' y ``creditcard\_y'', de esta forma se separan las variables independientes de la variable dependiente o ``Class''.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Se dividen los datos para X e y}
\NormalTok{creditcard\_X }\OtherTok{\textless{}{-}}\NormalTok{ creditcard[, }\SpecialCharTok{{-}}\DecValTok{31}\NormalTok{]}
\NormalTok{creditcard\_y }\OtherTok{\textless{}{-}}\NormalTok{ creditcard}\SpecialCharTok{$}\NormalTok{Class}
\end{Highlighting}
\end{Shaded}

\textbf{Tratamiento de datos desbalanceados}

Uno de los principales problemas a la hora de abordar este tipo de escenarios, es lo que se conoce como ``datos desbalanceados''. Se deice que un dataset está desbalanceado cuando la variable dependiente presenta más observaciones de una clase que de otra. En el caso de transacciones fraudulentas con tarjeta de crédito es evidente que la mayoría de las operaciones son legítimas o benignas, y que sólo un pequeño porcentaje resultan ser maliciosas. ¿Cuál es el problema?

Por lo general, los modelos entrenados con datasets desbalanceados no se comportan bien cuando tienen que generalizar, es decir, cuando tienen que realizar predicciones sobre conjuntos de datos que no han sido vistos anteriormente por el modelo. El desbalanceo de los datos es un sesgo hacia la clase mayoritaria, por lo que, en última instancia, muestra una tendencia al sobreajuste u overfitting hacia esa clase.
Existen diversas técnicas que permiten corregir esta situación:

• Undersampling o Submuestreo. Esta técnica consiste en reducir el número de observaciones de la clase mayoritaria, estableciendo quizás una ratio de 60/40. Esta técnica resulta efectiva si se respetan los grupos naturales que existen en los datos, así como el resto de las características presentes en la clase mayoritaria.
• Oversampling o Sobremuestreo. Esta técnica consiste en aumentar el número de observaciones de la clase minoritaria mediante la creación de datos sintéticos que, al igual que la técnica anterior, respeten las características de esa clase.

Para la creación de datos sintéticos en escenarios de oversampling existen varios algoritmos con los que se obtienen buenos resultados, quizás el más conocido y utilizado sea SMOTE. SMOTE no realiza una copia de las observaciones del dataset, sino que en su lugar genera nuevos datos de forma sintética utilizando los vecinos más cercanos de esos casos, respetando las características estadísticas de la clase. Además, los ejemplos de la clase mayoritaria también son submuestreados, lo que da lugar a un conjunto de datos más equilibrado. En R, el algoritmo SMOTE pertenece al paquete ``Data Mining with R'' o DMwR.

Para este caso particular se utilizará una técnica simple de Undersampling basada en el paquete ``unbalanced''. Unbalanced es un paquete que actualmente no se encuentra disponible en el repositorio de CRAN por lo que para su instalación deberás ejecutar el siguiente código:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("devtools") // Instalar si no se encuentra entre las librerías del sistema}
\FunctionTok{library}\NormalTok{(devtools)}
\NormalTok{devtools}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"dalpozz/unbalanced"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Una vez instalado, al igual que todas sus dependencias, se realiza el Undersampling del dataset siguiendo los siguientes pasos:

1.- Convertir la variable dependiente ``Class'' en factor

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(unbalanced)}
\NormalTok{creditcard}\SpecialCharTok{$}\NormalTok{Class }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(creditcard}\SpecialCharTok{$}\NormalTok{Class)}
\FunctionTok{levels}\NormalTok{(creditcard}\SpecialCharTok{$}\NormalTok{Class) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"0"}\NormalTok{, }\StringTok{"1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

2.- A continuación ejecutar la función de Undersampling

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{undersampled\_creditcard }\OtherTok{\textless{}{-}} \FunctionTok{ubBalance}\NormalTok{(creditcard\_X, creditcard}\SpecialCharTok{$}\NormalTok{Class, }\AttributeTok{type =} \StringTok{"ubUnder"}\NormalTok{, }\AttributeTok{verbose =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{undersampled\_combined }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}
\NormalTok{  undersampled\_creditcard}\SpecialCharTok{$}\NormalTok{X,}
\NormalTok{  undersampled\_creditcard}\SpecialCharTok{$}\NormalTok{Y}
\NormalTok{)}

\FunctionTok{names}\NormalTok{(undersampled\_combined)[}\FunctionTok{names}\NormalTok{(undersampled\_combined) }\SpecialCharTok{==} \StringTok{"undersampled\_creditcard$Y"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"Class"}
\FunctionTok{levels}\NormalTok{(undersampled\_combined}\SpecialCharTok{$}\NormalTok{Class) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Legítima"}\NormalTok{, }\StringTok{"Fraude"}\NormalTok{)}
\CommentTok{\#\textgreater{} Proportion of positives after ubUnder : 50 \% of 984 observations}
\end{Highlighting}
\end{Shaded}

3.- Comprobar el número de casos en el dataset sobre el que se ha ejecutado la función de Undersampling

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{creditcard}\SpecialCharTok{$}\NormalTok{Class }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(creditcard}\SpecialCharTok{$}\NormalTok{Class)}
\FunctionTok{levels}\NormalTok{(creditcard}\SpecialCharTok{$}\NormalTok{Class) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"0"}\NormalTok{, }\StringTok{"1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

4.- Realizar la gráfica para visualizar el número de muestras de cada clase después de realizar el Undersampling

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ undersampled\_combined, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill =}\NormalTok{ Class)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Class)) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Muestras"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_discrete}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{legend.position =} \StringTok{"ninguna"}\NormalTok{,}
    \AttributeTok{legend.title =} \FunctionTok{element\_blank}\NormalTok{(),}
    \AttributeTok{panel.grid.major =} \FunctionTok{element\_blank}\NormalTok{(),}
    \AttributeTok{panel.grid.minor =} \FunctionTok{element\_blank}\NormalTok{(),}
    \AttributeTok{panel.background =} \FunctionTok{element\_blank}\NormalTok{()}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/muestras-cred} 

}

\caption{Número de muestras de cada clase después de undersampling (984)}\label{fig:plot-muestras-cred}
\end{figure}

\textbf{Modelo de Clasificación mediante Regresión Lógística}

A continuación se procederá a la construcción de un modelo de regresión logística para una clasificación binaria en relación al fraude en transacciones con tarjeta de crédito a partir de los datos balanceados obtenidos anteriormente. El dataframe que se utilizará, por lo tanto, será ``undersampled\_combined'' que contiene 984 observaciones, un 50\% de las cuales son transacciones identificadas como fraude.

Lo primero será realizar un par de pequeños cambios en el dataset, es decir, eliminar, por un lado, las variables ``Time'' y ``Amount'' ya que no van a ser relevantes para el modelo, y cambiar las etiquetas ``Legitima'' y ``Fraude'' por 0 y 1 respectivamente.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{undersampled\_combined }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(undersampled\_combined, }\AttributeTok{select =} \SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(Time, Amount))}
\NormalTok{undersampled\_combined}\SpecialCharTok{$}\NormalTok{Class }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(undersampled\_combined}\SpecialCharTok{$}\NormalTok{Class }\SpecialCharTok{==} \StringTok{"Fraude"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Lo siguiente será dividir el conjunto de datos en los datasets de entrenamiento y test, para lo cual se aplicará la función ``split()'' con un SplitRatio de 0.80, es decir, un 80\% de los datos irán de forma aleatoria al dataset de entrenamiento, 788 observaciones, frente a las 196 observaciones que formarán el dataset de testing.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("caTools") // Instalar si no se encuentra entre las librerías del sistema}
\FunctionTok{library}\NormalTok{(caTools)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{split }\OtherTok{\textless{}{-}} \FunctionTok{sample.split}\NormalTok{(undersampled\_combined}\SpecialCharTok{$}\NormalTok{Class, }\AttributeTok{SplitRatio =} \FloatTok{0.80}\NormalTok{)}
\NormalTok{training }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(undersampled\_combined, split }\SpecialCharTok{==} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{test }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(undersampled\_combined, split }\SpecialCharTok{==} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Con los datasets necesarios ya disponibles, el siguiente paso es entrenar el modelo de regresión logística que clasificará las transacciones en legítimas o fraudulentas. Para ello se utilizará el algoritmo GLM creando un clasificador que se identificará como ``undersampledModel'' y al que se le pasarán los parámetros siguientes:

• Formula: Con este parámetro se indica la variable dependiente seguida del simbolo \textasciitilde{} y un punto (con el punto se hace referencia al resto de variables del dataset)

• Data: El dataset con los datos de entrenamiento

• Family: Al ser un clasificador con dos valores posibles 0, 1, se indica que será de tipo ``binomial''

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{undersampledModel }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(Class }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ training, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

Para ver los resultados del modelo ejecutar:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(undersampledModel)}
\end{Highlighting}
\end{Shaded}

Con el modelo ya entrenado se realizan las predicciones para los datos del conjunto de testing utilizando para ello la función ``predict()''. Los parámetros son simples, el primero es el modelo o clasificador que se va a utilizar y que será ``undersampledModel'', a continuación el tipo de dato que devolverá, en este caso ``response'', el cual indica que el algoritmo devolverá las probabilidades listadas en un único vector, el cual estará disponible a partir de la variable ``fraud\_prob'', y por último el parámetro ``newdata'' que hace referencia al dataset en el que se descarta la última columna por ser la que representa la variable dependiente.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fraud\_prob }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(undersampledModel, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{, }\AttributeTok{newdata =}\NormalTok{ test[, }\SpecialCharTok{{-}}\DecValTok{29}\NormalTok{])}
\FunctionTok{head}\NormalTok{(fraud\_prob)}
\end{Highlighting}
\end{Shaded}

La visualización del vector con las predicciones puede parecer algo confusa, por lo que a menudo es preciso realizar una conversión de esas predicciones en valores O y 1, dependiendo del rango a partir del cual se estime que una transacción es fraudulenta, por ejemplo, a partir del 60\% de probabilidad la transacción será etiquetada como ``1'', en caso contrario será etiquetada como ``0''. Para ello se utilizará el siguiente código ``ifelse'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y\_pred }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(fraud\_prob }\SpecialCharTok{\textgreater{}} \FloatTok{0.6}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{La matriz de confusión}

Como último paso del presente estudio se creará la matriz de confusión con el fin de visualizar que tal se ha comportado el algoritmo, es decir, cuántos verdaderos positivos y negativos ha logrado predecir correctamente. Para ello se creará la variable ``confusionMatrix'' en la cual se almacenará el resultado de la comparación entre el vector del dataset de testing, es decir, los datos etiquetados originalmente, y el vector de probabilidades resultado del algoritmo. El resultado, como se puede comprobar es que ha evaluado correctamente 184 de las 196 observaciones.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{confusionMatrix }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(test[, }\DecValTok{29}\NormalTok{], y\_pred)}
\NormalTok{confusionMatrix}
\CommentTok{\#\textgreater{} y\_pred}
\CommentTok{\#\textgreater{}   0  1}
\CommentTok{\#\textgreater{} 0 96 2}
\CommentTok{\#\textgreater{} 1 8 90}
\end{Highlighting}
\end{Shaded}

\hypertarget{appendix-appendix}{%
\appendix}


si hace falta, si no se obvia.

\hypertarget{informaciuxf3n-de-la-sesiuxf3n}{%
\section{Información de la sesión}\label{informaciuxf3n-de-la-sesiuxf3n}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sessionInfo}\NormalTok{()}
\CommentTok{\#\textgreater{} R version 4.2.1 (2022{-}06{-}23 ucrt)}
\CommentTok{\#\textgreater{} Platform: x86\_64{-}w64{-}mingw32/x64 (64{-}bit)}
\CommentTok{\#\textgreater{} Running under: Windows 10 x64 (build 19045)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Matrix products: default}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} locale:}
\CommentTok{\#\textgreater{} [1] LC\_COLLATE=Spanish\_Spain.utf8  LC\_CTYPE=Spanish\_Spain.utf8   }
\CommentTok{\#\textgreater{} [3] LC\_MONETARY=Spanish\_Spain.utf8 LC\_NUMERIC=C                  }
\CommentTok{\#\textgreater{} [5] LC\_TIME=Spanish\_Spain.utf8    }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} attached base packages:}
\CommentTok{\#\textgreater{} [1] splines   grid      stats     graphics  grDevices utils     datasets }
\CommentTok{\#\textgreater{} [8] methods   base     }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} other attached packages:}
\CommentTok{\#\textgreater{}   [1] wordcloud2\_0.2.1        readr\_2.1.3             MatchIt\_4.5.0          }
\CommentTok{\#\textgreater{}   [4] tableone\_0.13.2         readxl\_1.4.1            ggpubr\_0.4.0           }
\CommentTok{\#\textgreater{}   [7] kableExtra\_1.3.4        magrittr\_2.0.3          knitr\_1.39             }
\CommentTok{\#\textgreater{}  [10] coda\_0.19{-}4             spatialreg\_1.2{-}6        spdep\_1.2{-}5            }
\CommentTok{\#\textgreater{}  [13] spData\_2.2.1            sp\_1.5{-}1                gstat\_2.0{-}9            }
\CommentTok{\#\textgreater{}  [16] sf\_1.0{-}9                ggraph\_2.0.6            tm\_0.7{-}8               }
\CommentTok{\#\textgreater{}  [19] NLP\_0.2{-}1               syuzhet\_1.0.6           textdata\_0.4.4         }
\CommentTok{\#\textgreater{}  [22] tidytext\_0.3.4          wordcloud\_2.6           RColorBrewer\_1.1{-}3     }
\CommentTok{\#\textgreater{}  [25] stopwords\_2.3           tokenizers\_0.2.3        keras\_2.9.0            }
\CommentTok{\#\textgreater{}  [28] dtplyr\_1.2.1            ca\_0.71.1               smacof\_2.1{-}5           }
\CommentTok{\#\textgreater{}  [31] colorspace\_2.0{-}3        GPArotation\_2022.10{-}2   psych\_2.2.9            }
\CommentTok{\#\textgreater{}  [34] FactoMineR\_2.6          clValid\_0.7             igraph\_1.3.4           }
\CommentTok{\#\textgreater{}  [37] cluster\_2.1.3           dendextend\_1.16.0       factoextra\_1.0.7       }
\CommentTok{\#\textgreater{}  [40] reprtree\_0.6            plotrix\_3.8{-}2           tree\_1.0{-}42            }
\CommentTok{\#\textgreater{}  [43] devtools\_2.4.5          usethis\_2.1.6           vip\_0.3.2              }
\CommentTok{\#\textgreater{}  [46] questionr\_0.7.7         epiR\_2.0.50             conjoint\_1.41          }
\CommentTok{\#\textgreater{}  [49] mda\_0.5{-}3               class\_7.3{-}20            quantmod\_0.4.20        }
\CommentTok{\#\textgreater{}  [52] TTR\_0.24.3              xts\_0.12.1              zoo\_1.8{-}10             }
\CommentTok{\#\textgreater{}  [55] foreign\_0.8{-}82          lubridate\_1.9.0         timechange\_0.1.1       }
\CommentTok{\#\textgreater{}  [58] forecast\_8.18           astsa\_1.16              tseries\_0.10{-}51        }
\CommentTok{\#\textgreater{}  [61] glmnet\_4.1{-}4            leaps\_3.1               ISLR2\_1.3{-}1            }
\CommentTok{\#\textgreater{}  [64] mlmRev\_1.0{-}8            lme4\_1.1{-}30             maps\_3.4.0             }
\CommentTok{\#\textgreater{}  [67] sm\_2.2{-}5.7.1            tidymv\_3.3.2            mgcv\_1.8{-}40            }
\CommentTok{\#\textgreater{}  [70] nlme\_3.1{-}157            SemiPar\_1.0{-}4.2         Epi\_2.47               }
\CommentTok{\#\textgreater{}  [73] ResourceSelection\_0.3{-}5 GGally\_2.1.2            ggfortify\_0.4.14       }
\CommentTok{\#\textgreater{}  [76] patchwork\_1.1.2         boot\_1.3{-}28             plyr\_1.8.8             }
\CommentTok{\#\textgreater{}  [79] samplingbook\_1.2.4      survey\_4.1{-}1            Matrix\_1.5{-}1           }
\CommentTok{\#\textgreater{}  [82] sampling\_2.9            pps\_1.0                 DescTools\_0.99.46      }
\CommentTok{\#\textgreater{}  [85] fitdistrplus\_1.1{-}8      survival\_3.3{-}1          Rlab\_4.0               }
\CommentTok{\#\textgreater{}  [88] rsample\_1.1.0           mlbench\_2.1{-}3           gridExtra\_2.3          }
\CommentTok{\#\textgreater{}  [91] idealista18\_0.1.1       corrplot\_0.92           ggmosaic\_0.3.3         }
\CommentTok{\#\textgreater{}  [94] gplots\_3.1.3            waffle\_0.7.0            ggstatsplot\_0.9.4      }
\CommentTok{\#\textgreater{}  [97] gapminder\_0.3.0         visdat\_0.5.3            dlookr\_0.6.1           }
\CommentTok{\#\textgreater{} [100] mongolite\_2.6.2         CDR\_0.0.3.0             reshape2\_1.4.4         }
\CommentTok{\#\textgreater{} [103] dplyr\_1.0.10            klaR\_1.7{-}1              MASS\_7.3{-}57            }
\CommentTok{\#\textgreater{} [106] e1071\_1.7{-}12            randomForest\_4.7{-}1.1    ipred\_0.9{-}13           }
\CommentTok{\#\textgreater{} [109] rpart.plot\_3.1.1        rpart\_4.1.16            caret\_6.0{-}93           }
\CommentTok{\#\textgreater{} [112] lattice\_0.20{-}45         ggplot2\_3.4.0          }
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} loaded via a namespace (and not attached):}
\CommentTok{\#\textgreater{}   [1] styler\_1.8.1           remotes\_2.4.2          slam\_0.1{-}50           }
\CommentTok{\#\textgreater{}   [4] paletteer\_1.4.1        labelled\_2.10.0        summarytools\_1.0.1    }
\CommentTok{\#\textgreater{}   [7] expm\_0.999{-}6           vctrs\_0.5.1            stats4\_4.2.1          }
\CommentTok{\#\textgreater{}  [10] utf8\_1.2.2             R.oo\_1.25.0            withr\_2.5.0           }
\CommentTok{\#\textgreater{}  [13] gdtools\_0.2.4          uuid\_1.1{-}0             lifecycle\_1.0.3       }
\CommentTok{\#\textgreater{}  [16] emmeans\_1.8.1{-}1        matrixStats\_0.62.0     stringr\_1.4.1         }
\CommentTok{\#\textgreater{}  [19] cellranger\_1.1.0       munsell\_0.5.0          ragg\_1.2.2            }
\CommentTok{\#\textgreater{}  [22] fontawesome\_0.4.0      codetools\_0.2{-}18       lmtest\_0.9{-}40         }
\CommentTok{\#\textgreater{}  [25] furrr\_0.3.1            flashClust\_1.01{-}2      magick\_2.7.3          }
\CommentTok{\#\textgreater{}  [28] parallelly\_1.32.1      fs\_1.5.2               ellipse\_0.4.3         }
\CommentTok{\#\textgreater{}  [31] stringi\_1.7.8          polyclip\_1.10{-}0        productplots\_0.1.1    }
\CommentTok{\#\textgreater{}  [34] pkgconfig\_2.0.3        prettyunits\_1.1.1      data.table\_1.14.6     }
\CommentTok{\#\textgreater{}  [37] correlation\_0.8.2      estimability\_1.4.1     nnls\_1.4              }
\CommentTok{\#\textgreater{}  [40] httr\_1.4.3             flextable\_0.8.1        hrbrthemes\_0.8.0      }
\CommentTok{\#\textgreater{}  [43] modeltools\_0.2{-}23      heplots\_1.4{-}2          mitools\_2.4           }
\CommentTok{\#\textgreater{}  [46] graphlayouts\_0.8.2     haven\_2.5.1            parameters\_0.18.2     }
\CommentTok{\#\textgreater{}  [49] htmltools\_0.5.3        miniUI\_0.1.1.1         SnowballC\_0.7.0       }
\CommentTok{\#\textgreater{}  [52] viridisLite\_0.4.1      yaml\_2.3.5             prodlim\_2019.11.13    }
\CommentTok{\#\textgreater{}  [55] pillar\_1.8.1           later\_1.3.0            glue\_1.6.2            }
\CommentTok{\#\textgreater{}  [58] DBI\_1.1.3              foreach\_1.5.2          robustbase\_0.95{-}0     }
\CommentTok{\#\textgreater{}  [61] gtable\_0.3.1           caTools\_1.18.2         latticeExtra\_0.6{-}30   }
\CommentTok{\#\textgreater{}  [64] rsvg\_2.3.2             fastmap\_1.1.0          extrafont\_0.18        }
\CommentTok{\#\textgreater{}  [67] broom\_1.0.0            checkmate\_2.1.0        promises\_1.2.0.1      }
\CommentTok{\#\textgreater{}  [70] webshot\_0.5.3          FNN\_1.1.3.1            rapportools\_1.1       }
\CommentTok{\#\textgreater{}  [73] textshaping\_0.3.6      mnormt\_2.1.1           hms\_1.1.2             }
\CommentTok{\#\textgreater{}  [76] ggforce\_0.4.1          askpass\_1.1            png\_0.1{-}7             }
\CommentTok{\#\textgreater{}  [79] lazyeval\_0.2.2         Formula\_1.2{-}4          profvis\_0.3.7         }
\CommentTok{\#\textgreater{}  [82] crayon\_1.5.2           extrafontdb\_1.0        svglite\_2.1.0         }
\CommentTok{\#\textgreater{}  [85] tidyselect\_1.2.0       xfun\_0.35              performance\_0.9.2     }
\CommentTok{\#\textgreater{}  [88] kernlab\_0.9{-}31         purrr\_0.3.5            pander\_0.6.5          }
\CommentTok{\#\textgreater{}  [91] etm\_1.1.1              spacetime\_1.2{-}8        AlgDesign\_1.2.1       }
\CommentTok{\#\textgreater{}  [94] rappdirs\_0.3.3         xgboost\_1.6.0.1        rootSolve\_1.8.2.3     }
\CommentTok{\#\textgreater{}  [97] jpeg\_0.1{-}9             MatrixModels\_0.5{-}1     pagedown\_0.19         }
\CommentTok{\#\textgreater{} [100] combinat\_0.0{-}8         ggsignif\_0.6.3         R.methodsS3\_1.8.2     }
\CommentTok{\#\textgreater{} [103] htmlTable\_2.4.1        xtable\_1.8{-}4           DT\_0.25               }
\CommentTok{\#\textgreater{} [106] cachem\_1.0.6           gdata\_2.18.0.1         statsExpressions\_1.3.3}
\CommentTok{\#\textgreater{} [109] abind\_1.4{-}5            systemfonts\_1.0.4      mime\_0.12             }
\CommentTok{\#\textgreater{} [112] weights\_1.0.4          gld\_2.6.5              ggrepel\_0.9.1         }
\CommentTok{\#\textgreater{} [115] rstatix\_0.7.0          processx\_3.7.0         insight\_0.18.4        }
\CommentTok{\#\textgreater{} [118] numDeriv\_2016.8{-}1.1    tools\_4.2.1            cli\_3.4.1             }
\CommentTok{\#\textgreater{} [121] quadprog\_1.5{-}8         proxy\_0.4{-}27           tibble\_3.1.8          }
\CommentTok{\#\textgreater{} [124] janeaustenr\_1.0.0      future.apply\_1.10.0    libcoin\_1.0{-}9         }
\CommentTok{\#\textgreater{} [127] assertthat\_0.2.1       officer\_0.4.4          urca\_1.3{-}3            }
\CommentTok{\#\textgreater{} [130] lpSolve\_5.6.17         pbapply\_1.5{-}0          s2\_1.1.1              }
\CommentTok{\#\textgreater{} [133] plotly\_4.10.0          R.utils\_2.12.2         tweenr\_2.0.2          }
\CommentTok{\#\textgreater{} [136] BiasedUrn\_1.07         effectsize\_0.7.0.5     tensorflow\_2.9.0      }
\CommentTok{\#\textgreater{} [139] zip\_2.2.1              bayestestR\_0.13.0      tzdb\_0.3.0            }
\CommentTok{\#\textgreater{} [142] tfruns\_1.5.1           ps\_1.7.1               lmom\_2.9              }
\CommentTok{\#\textgreater{} [145] fansi\_1.0.3            tidygraph\_1.2.2        KernSmooth\_2.23{-}20    }
\CommentTok{\#\textgreater{} [148] backports\_1.4.1        scatterplot3d\_0.3{-}42   sysfonts\_0.8.8        }
\CommentTok{\#\textgreater{} [151] interp\_1.1{-}3           farver\_2.1.1           shiny\_1.7.3           }
\CommentTok{\#\textgreater{} [154] hardhat\_1.2.0          whisker\_0.4            partykit\_1.2{-}16       }
\CommentTok{\#\textgreater{} [157] pROC\_1.18.0            viridis\_0.6.2          rstudioapi\_0.14       }
\CommentTok{\#\textgreater{} [160] minqa\_1.2.4            iterators\_1.0.14       intervals\_0.15.2      }
\CommentTok{\#\textgreater{} [163] prismatic\_1.1.1        shape\_1.4.6            gtools\_3.9.3          }
\CommentTok{\#\textgreater{} [166] inum\_1.0{-}4             rematch2\_2.1.2         listenv\_0.8.0         }
\CommentTok{\#\textgreater{} [169] generics\_0.1.3         base64enc\_0.1{-}3        pkgbuild\_1.3.1        }
\CommentTok{\#\textgreater{} [172] ModelMetrics\_1.2.2.2   BayesFactor\_0.9.12{-}4.4 pryr\_0.1.5            }
\CommentTok{\#\textgreater{} [175] R.cache\_0.16.0         timeDate\_4021.106      evaluate\_0.16         }
\CommentTok{\#\textgreater{} [178] memoise\_2.0.1          candisc\_0.8{-}6          doParallel\_1.0.17     }
\CommentTok{\#\textgreater{} [181] httpuv\_1.6.6           Rttf2pt1\_1.3.10        Rcpp\_1.0.9            }
\CommentTok{\#\textgreater{} [184] polynom\_1.4{-}1          openssl\_2.0.4          classInt\_0.4{-}8        }
\CommentTok{\#\textgreater{} [187] diptest\_0.76{-}0         dbscan\_1.1{-}10          pkgload\_1.3.0         }
\CommentTok{\#\textgreater{} [190] cmprsk\_2.2{-}11          jsonlite\_1.8.3         Hmisc\_4.7{-}2           }
\CommentTok{\#\textgreater{} [193] fracdiff\_1.5{-}1         digest\_0.6.30          showtextdb\_3.0        }
\CommentTok{\#\textgreater{} [196] bookdown\_0.28          rprojroot\_2.0.3        LearnBayes\_2.15.1     }
\CommentTok{\#\textgreater{} [199] bitops\_1.0{-}7           here\_1.0.1             rmarkdown\_2.14        }
\CommentTok{\#\textgreater{} [202] globals\_0.16.2         compiler\_4.2.1         nnet\_7.3{-}17           }
\CommentTok{\#\textgreater{} [205] reticulate\_1.26        carData\_3.0{-}5          tcltk\_4.2.1           }
\CommentTok{\#\textgreater{} [208] rlang\_1.0.6            urlchecker\_1.0.1       nloptr\_2.0.3          }
\CommentTok{\#\textgreater{} [211] prabclus\_2.3{-}2         reactable\_0.3.0        wk\_0.7.0              }
\CommentTok{\#\textgreater{} [214] sessioninfo\_1.2.2      fpc\_2.2{-}9              lava\_1.7.0            }
\CommentTok{\#\textgreater{} [217] multcompView\_0.1{-}8     rvest\_1.0.2            recipes\_1.0.3         }
\CommentTok{\#\textgreater{} [220] future\_1.29.0          mvtnorm\_1.1{-}3          htmlwidgets\_1.5.4     }
\CommentTok{\#\textgreater{} [223] forcats\_0.5.2          labeling\_0.4.2         callr\_3.7.1           }
\CommentTok{\#\textgreater{} [226] flexmix\_2.3{-}18         datawizard\_0.6.1       curl\_4.3.2            }
\CommentTok{\#\textgreater{} [229] parallel\_4.2.1         highr\_0.9              DEoptimR\_1.0{-}11       }
\CommentTok{\#\textgreater{} [232] scales\_1.2.1           showtext\_0.9{-}5         deldir\_1.0{-}6          }
\CommentTok{\#\textgreater{} [235] Exact\_3.1              mice\_3.15.0            car\_3.1{-}0             }
\CommentTok{\#\textgreater{} [238] zeallot\_0.1.0          tidyr\_1.2.1            ellipsis\_0.3.2        }
\CommentTok{\#\textgreater{} [241] xml2\_1.3.3             gower\_1.0.0            reshape\_0.8.9         }
\CommentTok{\#\textgreater{} [244] R6\_2.5.1               mclust\_5.4.10          units\_0.8{-}0}
\end{Highlighting}
\end{Shaded}


  \bibliography{bib/book-ch62-BL-FILO.bib,bib/book-ELC.bib,bib/book-RPC-ICM.bib,bib/book-mv.bib,bib/book-MLMS-MVV.bib,bib/book-Mdurban-Vcasero.bib,bib/book-MCGC.bib,bib/book-ch22-JMM.bib,bib/book-ml-supervisado.bib,bib/book-cluster-JMM-GFAC.bib,bib/bookJMML-JLAN.bib,bib/book-NV-JLE.bib,bib/book-ch40-GFAC.bib,bib/book-JM-MM-pp.bib,bib/packages-AVCC.bib,bib/book-AVCC.bib,bib/book-Vcasero-Acelis-Mloz.bib,bib/book-JJG.bib,bib/bookAGV.bib,bib/book-git-NK.bib,bib/book-ch99-DR.bib,bib/book-policia.bib,bib/empleo-IHA.bib,bib/book-CSF.bib,bib/book-idealista.bib,bib/book-medicina.bib,bib/book-ch21-DR.bib,bib/book-EA-JAO-AP.bib}

\backmatter
\printindex

\end{document}
