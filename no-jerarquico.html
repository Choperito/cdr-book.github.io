<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Capítulo 31 Análisis clúster: clusterización no jerárquica | Fundamentos de ciencia de datos con R</title>
<meta name="author" content="Gema Fernández-Avilés y José-María Montero">
<meta name="description" content="José-María Montero\(^{a}\) y Gema Fernández-Avilés\(^{a}\) \(^{a}\)Universidad de Castilla-La Mancha  31.1 Introducción Como se avanzó en la Sec. 30.4.1, aunque las técnicas de agrupación...">
<meta name="generator" content="bookdown 0.37 with bs4_book()">
<meta property="og:title" content="Capítulo 31 Análisis clúster: clusterización no jerárquica | Fundamentos de ciencia de datos con R">
<meta property="og:type" content="book">
<meta property="og:image" content="/img/cover.png">
<meta property="og:description" content="José-María Montero\(^{a}\) y Gema Fernández-Avilés\(^{a}\) \(^{a}\)Universidad de Castilla-La Mancha  31.1 Introducción Como se avanzó en la Sec. 30.4.1, aunque las técnicas de agrupación...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Capítulo 31 Análisis clúster: clusterización no jerárquica | Fundamentos de ciencia de datos con R">
<meta name="twitter:description" content="José-María Montero\(^{a}\) y Gema Fernández-Avilés\(^{a}\) \(^{a}\)Universidad de Castilla-La Mancha  31.1 Introducción Como se avanzó en la Sec. 30.4.1, aunque las técnicas de agrupación...">
<meta name="twitter:image" content="/img/cover.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><link href="libs/tabwid-1.1.3/tabwid.css" rel="stylesheet">
<script src="libs/tabwid-1.1.3/tabwid.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css">
<link rel="stylesheet" href="bs4_book.css">
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Fundamentos de ciencia de datos con <strong>R</strong></a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Buscar" aria-label="Buscar">
</form>

      <nav aria-label="Contenido"><h2>Contenido</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Prefacio</a></li>
<li><a class="" href="pr%C3%B3logo-by-julia-silge.html">Prólogo (by Julia Silge)</a></li>
<li><a class="" href="pr%C3%B3logo-por-yanina-bellini.html">Prólogo (por Yanina Bellini)</a></li>
<li class="book-part">Ciencia, datos, software… y científicos</li>
<li><a class="" href="ciencia-datos.html"><span class="header-section-number">1</span> ¿Es la ciencia de datos una ciencia?</a></li>
<li><a class="" href="metodologia.html"><span class="header-section-number">2</span> Metodología en ciencia de datos</a></li>
<li><a class="" href="ch-110003.html"><span class="header-section-number">3</span> R para ciencia de datos</a></li>
<li><a class="" href="cap-etica.html"><span class="header-section-number">4</span> Ética en la ciencia de datos</a></li>
<li class="book-part">Bienvenidos a la jungla de datos</li>
<li><a class="" href="datos-sql.html"><span class="header-section-number">5</span> Gestión de bases de datos relacionales</a></li>
<li><a class="" href="cap-nosql.html"><span class="header-section-number">6</span> Gestión de bases de datos NoSQL</a></li>
<li><a class="" href="DGDQM.html"><span class="header-section-number">7</span> Gobierno, gestión y calidad del dato</a></li>
<li><a class="" href="cap-130009.html"><span class="header-section-number">8</span> Integración y limpieza de datos</a></li>
<li><a class="" href="chap-feature.html"><span class="header-section-number">9</span> Selección y transformación de variables</a></li>
<li><a class="" href="chap-herramientas.html"><span class="header-section-number">10</span> Herramientas para el análisis en ciencia de datos</a></li>
<li><a class="" href="cap-120006-aed.html"><span class="header-section-number">11</span> Análisis exploratorio de datos</a></li>
<li class="book-part">Fundamentos de estadística</li>
<li><a class="" href="Funda-probab.html"><span class="header-section-number">12</span> Probabilidad</a></li>
<li><a class="" href="Fundainfer.html"><span class="header-section-number">13</span> Inferencia estadística</a></li>
<li><a class="" href="muestreo.html"><span class="header-section-number">14</span> Muestreo y remuestreo</a></li>
<li class="book-part">Modelización estadística</li>
<li><a class="" href="cap-lm.html"><span class="header-section-number">15</span> Modelización lineal</a></li>
<li><a class="" href="cap-glm.html"><span class="header-section-number">16</span> Modelos lineales generalizados</a></li>
<li><a class="" href="cap-gam.html"><span class="header-section-number">17</span> Modelos aditivos generalizados</a></li>
<li><a class="" href="cap-mxm.html"><span class="header-section-number">18</span> Modelos mixtos</a></li>
<li><a class="" href="cap-sparse.html"><span class="header-section-number">19</span> Modelos \(\textit{sparse}\) y métodos penalizados de regresión</a></li>
<li><a class="" href="cap-series-temp.html"><span class="header-section-number">20</span> Modelización de series temporales</a></li>
<li><a class="" href="cap-discriminante.html"><span class="header-section-number">21</span> Análisis discriminante</a></li>
<li><a class="" href="cap-conjunto.html"><span class="header-section-number">22</span> Análisis conjunto</a></li>
<li><a class="" href="tablas-contingencia.html"><span class="header-section-number">23</span> Análisis de tablas de contingencia</a></li>
<li class="book-part">Machine learning supervisado</li>
<li><a class="" href="cap-arboles.html"><span class="header-section-number">24</span> Árboles de clasificación y regresión</a></li>
<li><a class="" href="cap-svm.html"><span class="header-section-number">25</span> Máquinas de vector soporte</a></li>
<li><a class="" href="cap-knn.html"><span class="header-section-number">26</span> Clasificador \(k\)-vecinos más próximos</a></li>
<li><a class="" href="cap-naive-bayes.html"><span class="header-section-number">27</span> Naive Bayes</a></li>
<li><a class="" href="cap-bagg-rf.html"><span class="header-section-number">28</span> Métodos ensamblados: \(\bf \textit {bagging}\) y \(\bf \textit{random}\) \(\bf \textit{forest}\)</a></li>
<li><a class="" href="cap-boosting-xgboost.html"><span class="header-section-number">29</span> \(\bf \textit{Boosting}\) y el algoritmo XGBoost</a></li>
<li class="book-part">Machine learning no supervisado</li>
<li><a class="" href="cap-cluster.html"><span class="header-section-number">30</span> Análisis clúster: clusterización jerárquica</a></li>
<li><a class="active" href="no-jerarquico.html"><span class="header-section-number">31</span> Análisis clúster: clusterización no jerárquica</a></li>
<li><a class="" href="acp.html"><span class="header-section-number">32</span> Análisis de componentes principales</a></li>
<li><a class="" href="af.html"><span class="header-section-number">33</span> Análisis factorial</a></li>
<li><a class="" href="mds.html"><span class="header-section-number">34</span> Escalamiento multidimensional</a></li>
<li><a class="" href="correspondencias.html"><span class="header-section-number">35</span> Análisis de correspondencias</a></li>
<li class="book-part">Deep learning</li>
<li><a class="" href="capNN.html"><span class="header-section-number">36</span> Redes neuronales artificiales</a></li>
<li><a class="" href="cap-redes-convol.html"><span class="header-section-number">37</span> Redes neuronales convolucionales</a></li>
<li class="book-part">Ciencia de datos de texto y redes</li>
<li><a class="" href="mineria-textos.html"><span class="header-section-number">38</span> Minería de textos</a></li>
<li><a class="" href="grafos.html"><span class="header-section-number">39</span> Análisis de grafos y redes sociales</a></li>
<li class="book-part">Ciencia de datos espaciales</li>
<li><a class="" href="datos-espaciales.html"><span class="header-section-number">40</span> Trabajando con datos espaciales</a></li>
<li><a class="" href="geo.html"><span class="header-section-number">41</span> Geoestadística</a></li>
<li><a class="" href="cap-econom-esp.html"><span class="header-section-number">42</span> Modelos econométricos espaciales</a></li>
<li><a class="" href="cap-pp.html"><span class="header-section-number">43</span> Procesos de puntos</a></li>
<li class="book-part">Comunica y colabora</li>
<li><a class="" href="cap-120007-informes.html"><span class="header-section-number">44</span> Informes reproducibles con R Markdown y Quarto</a></li>
<li><a class="" href="shiny.html"><span class="header-section-number">45</span> Creación de aplicaciones web interactivas con Shiny</a></li>
<li><a class="" href="github.html"><span class="header-section-number">46</span> Git y GitHub R</a></li>
<li><a class="" href="geoproces.html"><span class="header-section-number">47</span> Geoprocesamiento en nube</a></li>
<li class="book-part">Casos de estudio en ciencia de datos</li>
<li><a class="" href="cap-crimen.html"><span class="header-section-number">48</span> Análisis de una red criminal</a></li>
<li><a class="" href="cap-publicidad.html"><span class="header-section-number">49</span> Optimización de inversiones publicitarias</a></li>
<li><a class="" href="cap-twitter.html"><span class="header-section-number">50</span> ¿Cómo tuitea Elon Musk?</a></li>
<li><a class="" href="cap-periodismo.html"><span class="header-section-number">51</span> Análisis electoral: de RStudio a su periódico favorito</a></li>
<li><a class="" href="paro-clm.html"><span class="header-section-number">52</span> El impacto de las crisis financiera y de la COVID-19 en el paro de CLM</a></li>
<li><a class="" href="cap-rfm.html"><span class="header-section-number">53</span> Segmentación de clientes en el comercio minorista</a></li>
<li><a class="" href="cap-medicina.html"><span class="header-section-number">54</span> Análisis de datos en medicina</a></li>
<li><a class="" href="cap-futbol.html"><span class="header-section-number">55</span> Messi y Ronaldo: dos ídolos desde la perspectiva de los datos</a></li>
<li><a class="" href="cambioclimatico.html"><span class="header-section-number">56</span> Una nota sobre el cambio climático</a></li>
<li><a class="" href="cap-sist-exp.html"><span class="header-section-number">57</span> Implementación de un sistema experto en el ámbito pediátrico</a></li>
<li><a class="" href="cap-ree.html"><span class="header-section-number">58</span> Predicción de consumo eléctrico con redes neuronales artificiales</a></li>
<li><a class="" href="nlp-textil.html"><span class="header-section-number">59</span> El procesamiento del lenguaje natural para tendencias de moda en textil</a></li>
<li><a class="" href="cap-fraude.html"><span class="header-section-number">60</span> Detección de fraude de tarjetas de crédito</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="info-session.html"><span class="header-section-number">A</span> Información de la sesión</a></li>
<li><a class="" href="referencias.html">Referencias</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="no-jerarquico" class="section level1" number="31">
<h1>
<span class="header-section-number">Capítulo 31</span> Análisis clúster: clusterización no jerárquica<a class="anchor" aria-label="anchor" href="#no-jerarquico"><i class="fas fa-link"></i></a>
</h1>
<p><em>José-María Montero<span class="math inline">\(^{a}\)</span> y Gema Fernández-Avilés</em><span class="math inline">\(^{a}\)</span></p>
<p><span class="math inline">\(^{a}\)</span>Universidad de Castilla-La Mancha</p>
<div id="introducción-13" class="section level2" number="31.1">
<h2>
<span class="header-section-number">31.1</span> Introducción<a class="anchor" aria-label="anchor" href="#introducci%C3%B3n-13"><i class="fas fa-link"></i></a>
</h2>
<p>Como se avanzó en la Sec. <a href="cap-cluster.html#introac">30.4.1</a>, aunque las técnicas de agrupación jerárquicas son muy utilizadas, existen otras, también muy populares, que se aglutinan bajo la denominación de no jerárquicas y que se pueden clasificar, sin ánimo de exhaustividad, en <span class="math inline">\((i)\)</span> <strong>de optimización o reasignación</strong>; <span class="math inline">\((ii)\)</span> <strong>basadas en la densidad de elementos</strong>; y <span class="math inline">\((iii)\)</span> <strong>otras</strong>, como los <em>métodos directos</em> (por ejemplo, el <em>block</em>-; bi-; co-; <em>two-mode clustering</em>), los <em>de reducción de la dimensionalidad</em> (como el <em>Q</em>- y el <em>R</em>-factorial), los <em>métodos de clusterización difusa</em>, o los <em>basados en mixturas de modelos</em>.</p>
<p>
</p>
<p>Las técnicas no jerárquicas proceden con el criterio de la inercia, maximizando la varianza intergrupos y minimizando la intragrupos. Se caracterizan porque:
</p>
<ul>
<li>El número de clústeres se suele determinar <em>a priori</em>.</li>
<li>Utilizan directamente los datos originales, sin necesidad de cómputo de una matriz de distancias o similaridades.</li>
<li>Los elementos pueden cambiar de clúster.</li>
<li>Los clústeres resultantes no están anidados unos en otros.</li>
</ul>
</div>
<div id="métodos-de-reasignación" class="section level2" number="31.2">
<h2>
<span class="header-section-number">31.2</span> Métodos de reasignación<a class="anchor" aria-label="anchor" href="#m%C3%A9todos-de-reasignaci%C3%B3n"><i class="fas fa-link"></i></a>
</h2>
<p>Los métodos de reasignación permiten que un elemento asignado a un grupo en una determinada etapa del proceso de clusterización sea reasignado a otro grupo, en una etapa posterior, si dicha reasignación implica la optimización del criterio de selección. El proceso finaliza cuando no hay ningún elemento cuya reasignación permita optimizar el resultado conseguido. Estas técnicas suelen asumir un número determinado de clústeres <em>a priori</em> y se diferencian entre sí en la manera de obtener la partición inicial y en la medida a optimizar en el proceso. Respecto a esta última cuestión, los procedimientos más populares son: <span class="math inline">\((i)\)</span> la minimización de la traza de la matriz de covarianzas intragrupos; <span class="math inline">\((ii)\)</span> la minimización de su determinante; <span class="math inline">\((iii)\)</span> la maximización de la traza del producto de las matrices de covarianzas intergrupos e intragrupos; y <span class="math inline">\((iv)\)</span> medidas de información o de estabilidad.</p>
<div id="técnicas-basadas-en-centroides-métodos-de-forgy-y-k-medias" class="section level3" number="31.2.1">
<h3>
<span class="header-section-number">31.2.1</span> Técnicas basadas en centroides: métodos de Forgy y <span class="math inline">\(k\)</span>-medias<a class="anchor" aria-label="anchor" href="#t%C3%A9cnicas-basadas-en-centroides-m%C3%A9todos-de-forgy-y-k-medias"><i class="fas fa-link"></i></a>
</h3>
<p> </p>
<p>Los algoritmos de reasignación más populares son el de Forgy y, sobre todo, el <span class="math inline">\(k\)</span>-medias. La literatura sobre este tipo de técnicas no es clara y, frecuentemente, se confunden el método de Forgy y el <span class="math inline">\(k\)</span>-medias, así como el <span class="math inline">\(k\)</span>-medias con algunas de sus otras denominaciones (dándose a entender que son técnicas distintas). Sin embargo, la historia es la siguiente: originalmente, <span class="citation">Forgy (<a href="referencias.html#ref-forgy1965cluster">1965</a>)</span> propuso un algoritmo consistente en la iteración sucesiva, hasta obtener convergencia, de las dos operaciones siguientes: <span class="math inline">\((i)\)</span> representación de los grupos por sus centroides; y <span class="math inline">\((ii)\)</span> asignación de los elementos al grupo con el centroide más cercano. Posteriormente, <span class="citation">Diday (<a href="referencias.html#ref-diday1971nouvelle">1971</a>)</span>, <span class="citation">Diday (<a href="referencias.html#ref-diday1973dynamic">1973</a>)</span>, <span class="citation">Anderberg (<a href="referencias.html#ref-anderberg1973">1973</a>)</span>, <span class="citation">Bock (<a href="referencias.html#ref-bock1974">1974</a>)</span> y <span class="citation">Späth (<a href="referencias.html#ref-spath1975cluster">1975</a>)</span> desarrollaron una variante del método de Forgy que solo se diferencia de él en que los centroides se recalculan después de asignar cada elemento (con la técnica de Forgy primero se llevan a cabo todas las asignaciones y posteriormente se recalculan los centroides). Diday la llamó método de las nubes dinámicas o clústeres dinámicos, Anderberg se refirió a ella como el criterio de inclusión en el grupo del centroide más cercano, Bock la denominó particionamiento iterativo basado en la mínima distancia y Späth la llamó HMEANS, una versión por lotes del procedimiento de los autores anteriores. Sin embargo, fue <span class="citation">MacQueen (<a href="referencias.html#ref-macqueen1967classification">1967</a>)</span> quien previamente acuñó la denominación de “<span class="math inline">\(k\)</span>-medias” que se usa hasta la fecha.</p>
<p></p>
<p><span class="math inline">\(K\)</span>-medias<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Recuérdese que el centro de un conglomerado viene dado por el centroide, vector de medias.&lt;/p&gt;"><sup>210</sup></a> requiere la especificación previa del número de grupos, <span class="math inline">\(k\)</span>, en los que se va a dividir el conjunto de elementos. El algoritmo:</p>
<ol style="list-style-type: decimal">
<li><p>Selecciona <span class="math inline">\(k\)</span> elementos por algún procedimiento;</p></li>
<li><p>asigna los restantes elementos al elemento más cercano de los previamente seleccionados;</p></li>
<li><p>sustituye los elementos seleccionados en (1) por los centroides de los grupos que se han formado;</p></li>
<li><p>asigna el conjunto de elementos al centroide más cercano del punto (3);</p></li>
<li><p>repite iterativamente los dos últimos pasos hasta que la asignación de elementos a los centroides no cambia. Los grupos entonces formados maximizan la distancia intergrupos y minimizan la distancia intragrupos.</p></li>
</ol>
<p>Recuérdese que con el método de Forgy la etapa (3) no comienza hasta que no se hayan asignado todos los elementos a un clúster en la etapa (2), mientras que en “<span class="math inline">\(k\)</span>-medias” los centroides se recomputan cada vez que un elemento es asignado a un grupo.</p>
<p>La partición que se obtiene es un óptimo local (pequeños cambios en la reasignación de elementos no lo mejoran), pero no se puede asegurar que sea el global, pues se trata de un método heurístico. Sí se puede asegurar que la partición es de calidad.</p>
<p><span class="math inline">\(K\)</span>-medias es eficiente y sencillo de implementar, pero tiene algunas desventajas: <span class="math inline">\((i)\)</span> necesita conocer <em>a priori</em> el número de grupos; <span class="math inline">\((ii)\)</span> la agrupación resultante puede depender de la asignación inicial (normalmente aleatoria) de los centroides, pudiendo converger a mínimos locales, por lo que se recomienda repetir la clusterización 25-50 veces y seleccionar la que tenga menor varianza intragrupos; <span class="math inline">\((iii)\)</span> no es robusto a valores extremos; y <span class="math inline">\((iv)\)</span> no trabaja con datos nominales. </p>
<p>En el ejemplo TIC se ha usado el algoritmo AS 136 de <span class="citation">J. A. Hartigan and Wong (<a href="referencias.html#ref-hartigan1979">1979</a>)</span>, una versión eficiente del de <span class="citation">John A. Hartigan (<a href="referencias.html#ref-hartigan1975clustering">1975</a>)</span> que no busca óptimos locales (varianza intragrupos mínima en cada grupo), sino soluciones tales que ninguna reasignación de elementos reduzca la varianza (global) intragrupos (véase Fig. <a href="no-jerarquico.html#fig:kmeans">31.1</a>).</p>
<div class="sourceCode" id="cb443"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">kmeans_tic</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/eclust.html">eclust</a></span><span class="op">(</span><span class="va">tic</span>, <span class="st">"kmeans"</span>, k <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:kmeans"></span>
<img src="160027-29_cluster_files/figure-html/kmeans-1.png" alt="Clusterización no jerárquica con $k$-medias." width="50%"><p class="caption">
Figura 31.1: Clusterización no jerárquica con <span class="math inline">\(k\)</span>-medias.
</p>
</div>
<p>Algunas versiones del <span class="math inline">\(k\)</span>-medias como el <span class="math inline">\(k\)</span>-medias difuso, el <span class="math inline">\(k\)</span>-medias recortadas, el <span class="math inline">\(k\)</span>-medias armónicas, el <span class="math inline">\(k\)</span>-medias <em>sparse</em> y el <span class="math inline">\(k\)</span>-medias <em>sparse</em> robusto pueden verse en <span class="citation">Carrasco-Oberto (<a href="referencias.html#ref-carrasco2020">2020</a>)</span>.</p>
<p>
</p>
<!-- **Nubes Dinámicas** -->
<!-- Fue introducido por Diday (1972), generalizando el metodo de K-Medias de Forgy. Se basa en que cada cluster debe tener una representación llamada núcleo o centroide de cluster para, posteriormente, hacer una búsqueda iterada de centroides y de cluster, por lo que cada clase estará repsentada por un núcleo, que será un elemento representativo de todos los que intefran la misma. -->
<!-- Es decir, que en lugar de elegir como referencia de clase un solo punto que constituye su centro y reasignar los puntos por proximidad a ese centro, se eligen varios puntos “h” para representar cada clase, siendo estos puntos el “núcleo” de la clase. -->
</div>
<div id="técnicas-basadas-en-medoides" class="section level3" number="31.2.2">
<h3>
<span class="header-section-number">31.2.2</span> Técnicas basadas en medoides<a class="anchor" aria-label="anchor" href="#t%C3%A9cnicas-basadas-en-medoides"><i class="fas fa-link"></i></a>
</h3>
<div id="k-medoides-pam" class="section level4" number="31.2.2.1">
<h4>
<span class="header-section-number">31.2.2.1</span> <span class="math inline">\(K\)</span>-medoides (PAM)<a class="anchor" aria-label="anchor" href="#k-medoides-pam"><i class="fas fa-link"></i></a>
</h4>
<p>Es un método de clusterización similar al <span class="math inline">\(k\)</span>-medias que también requiere la especificación <em>a priori</em> del número de grupos. La diferencia es que en <span class="math inline">\(k\)</span>-medoides cada grupo está representado por uno de sus elementos, denominados medoides (o centrotipos).<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;El medoide de un conglomerado es el elemento del conglomerado con la menor disimilitud promedio entre él y todos los demás miembros del grupo. Es el elemento más céntrico del grupo.&lt;/p&gt;"><sup>211</sup></a> En el <span class="math inline">\(k\)</span>-medias están representados por sus centroides, que no tienen por qué coincidir con ninguno de los elementos a agrupar. Se trata, pues, de formar grupos <strong>particionando el conjunto de elementos alrededor de los medoides</strong> (PAM). Un ejemplo se encuentra en la Fig. <a href="no-jerarquico.html#fig:pam">31.2</a>.</p>
<p>El algoritmo <span class="math inline">\(k\)</span>-medoides es más robusto al ruido y a valores grandes de los datos (de hecho es invariante a los <em>outliers</em>) que el <span class="math inline">\(k\)</span>-medias, ya que minimiza la suma de diferencias por
parejas (utiliza la distancia Manhattan) en lugar de la suma de los cuadrados de las distancias euclídeas.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Las técnicas basadas en la minimización de promedios de distancias o de residuos en valor absoluto son más robustas
que las basadas en sumas de cuadrados.&lt;/p&gt;"><sup>212</sup></a>
Además, sus agrupaciones no dependen del orden en que han sido introducidos los elementos, cosa que puede ocurrir con otras técnicas no jerárquicas, y, como se avanzó anteriormente, propone como centro del clúster un elemento del mismo.</p>
<p>PAM funciona muy bien con conjuntos de datos pequeños (por ejemplo, 100 elementos en 5 grupos) y permite un análisis detallado de la partición realizada, puesto que proporciona las características del agrupamiento y un gráfico de silueta, así como un índice de validez propio para determinar el número óptimo de clústeres. El algoritmo PAM puede verse al completo en <span class="citation">Kaufman and Rousseeuw (<a href="referencias.html#ref-kauf_1990">1990</a>)</span>; para un muy buen resumen véase <span class="citation">Amat Rodrigo (<a href="referencias.html#ref-amat2017">2017</a>)</span>.
</p>
<p></p>
<div class="sourceCode" id="cb444"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">pam_tic</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/eclust.html">eclust</a></span><span class="op">(</span><span class="va">tic</span>, <span class="st">"pam"</span>, k <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:pam"></span>
<img src="160027-29_cluster_files/figure-html/pam-1.png" alt="Clusterización no jerárquica con PAM." width="50%"><p class="caption">
Figura 31.2: Clusterización no jerárquica con PAM.
</p>
</div>
</div>
<div id="clara" class="section level4" number="31.2.2.2">
<h4>
<span class="header-section-number">31.2.2.2</span> CLARA<a class="anchor" aria-label="anchor" href="#clara"><i class="fas fa-link"></i></a>
</h4>
<p>La ineficiencia de PAM para bases de datos grandes, junto con su complejidad computacional, llevó al desarrollo de CLARA (<em>clustering large applications</em>).<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;No obstante, hay una interesante modificación de PAM cambiando el orden de anidamiento de los bucles. La idea es encontrar el mejor intercambio de elementos para cada medoide y ejecutar tantos como sea posible en cada iteración, lo que, de acuerdo con &lt;span class="citation"&gt;Schubert and Rousseeuw (&lt;a href="referencias.html#ref-shubert2021"&gt;2021&lt;/a&gt;)&lt;/span&gt;, reduce el número de iteraciones necesarias para la convergencia sin pérdida de calidad. También se puede aplicar a los algoritmos CLARA y CLARANS (este último se presenta en la siguiente subsección).&lt;/p&gt;'><sup>213</sup></a></p>
<p>La diferencia entre PAM y CLARA es que el segundo se basa en muestreos. Solo una pequeña porción de los datos totales es seleccionada como representativa de los datos y
los medoides son escogidos (en la muestra) usando PAM.
CLARA, pues, combina la idea de <span class="math inline">\(k\)</span>-medoides con el remuestreo para que pueda aplicarse a grandes volúmenes de datos. De acuerdo con <span class="citation">Amat Rodrigo (<a href="referencias.html#ref-amat2017">2017</a>)</span>, CLARA selecciona una muestra aleatoria y le aplica el algoritmo de PAM para encontrar los clústeres óptimos dada esa muestra.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Una descripción completa puede verse en &lt;span class="citation"&gt;Kaufman and Rousseeuw (&lt;a href="referencias.html#ref-kauf_1990"&gt;1990&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;'><sup>214</sup></a> Alrededor de esos medoides se agrupan los elementos de todo el conjunto de datos. La calidad de los medoides resultantes se cuantifica con la suma total de distancias intragrupos. CLARA repite este proceso un número predeterminado de veces con el objetivo de reducir el sesgo de muestreo. Por último, se seleccionan como clústeres finales los obtenidos con los medoides que minimizaron la suma total de distancias intragrupo. Se presenta un ejemplo en la Fig. <a href="no-jerarquico.html#fig:clara">31.3</a>. </p>
<div class="sourceCode" id="cb445"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">clara_tic</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/eclust.html">eclust</a></span><span class="op">(</span><span class="va">tic</span>, <span class="st">"clara"</span>, k <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:clara"></span>
<img src="160027-29_cluster_files/figure-html/clara-1.png" alt="Clusterización no jerárquica con CLARA." width="50%"><p class="caption">
Figura 31.3: Clusterización no jerárquica con CLARA.
</p>
</div>
<p></p>
</div>
<div id="clarans" class="section level4" number="31.2.2.3">
<h4>
<span class="header-section-number">31.2.2.3</span> CLARANS<a class="anchor" aria-label="anchor" href="#clarans"><i class="fas fa-link"></i></a>
</h4>
<p></p>
<p>CLARANS (del inglés <em>clustering large applications based upon randomized search</em>) es una mezcla de PAM y CLARA.
Como CLARA puede dar lugar a una mala clusterización si uno de los medoides de la muestra está lejos de los mejores medoides, CLARANS trata de superar esta limitación. El algoritmo puede verse en <span class="citation">Ng and Han (<a href="referencias.html#ref-ng2002clarans">2002</a>)</span>.</p>
</div>
</div>
<div id="técnicas-basadas-en-medianas-k-medianas" class="section level3" number="31.2.3">
<h3>
<span class="header-section-number">31.2.3</span> Técnicas basadas en medianas: <span class="math inline">\(k\)</span>-medianas<a class="anchor" aria-label="anchor" href="#t%C3%A9cnicas-basadas-en-medianas-k-medianas"><i class="fas fa-link"></i></a>
</h3>
<p></p>
<p>Igual que el <span class="math inline">\(k\)</span>-medoides, es una variante del <span class="math inline">\(k\)</span>-medias que utiliza como centros las medianas para que no le afecten ni el ruido ni los valores atípicos. La diferencia con el <span class="math inline">\(k\)</span>-medoides es que la mediana de un grupo no tiene por qué ser una de las observaciones. <span class="math inline">\(K\)</span>-medianas utiliza la distancia Manhattan. </p>
<p>Volviendo al ejemplo TIC, como se ha podido comprobar, la clusterización de los países de la UE-27 en función del uso del las TIC es prácticamente la misma con técnicas jerárquicas aglomerativas como el vecino más lejano, el método de Ward, el del centroide, o algoritmos divisivos como DIANA, que con las técnicas no jerárquicas con preselección de 3 grupos (<span class="math inline">\(k\)</span>-medias, PAM y CLARA).</p>
</div>
</div>
<div id="métodos-basados-en-la-densidad-de-elementos" class="section level2" number="31.3">
<h2>
<span class="header-section-number">31.3</span> Métodos basados en la densidad de elementos<a class="anchor" aria-label="anchor" href="#m%C3%A9todos-basados-en-la-densidad-de-elementos"><i class="fas fa-link"></i></a>
</h2>
<p>Utilizan indicadores de frecuencia, construyendo grupos mediante la detección de aquellas zonas del espacio de las variables (que caracterizan a los elementos) densamente pobladas (clústeres naturales) y de aquellas otras con un escasa densidad de elementos. Los elementos que no forman parte de un conglomerado se consideran ruido. Emulan, pues, el funcionamiento del cerebro humano.</p>
<p>La identificación de los grupos (y los parámetros que los caracterizan, cuando se manejan modelos probabilísticos) se lleva a cabo haciéndolos crecer hasta que la densidad del grupo más próximo sobrepase un cierto umbral. Por tanto, imponen reglas para evitar el problema de obtener un solo grupo cuando existen puntos intermedios. Se suele suponer que la densidad de elementos en los grupos es Gaussiana si las variables son cuantitativas, y multinomial si son cualitativas.</p>
<p>Se suelen clasificar en: </p>
<ul>
<li><p>Los que tienen un <strong>enfoque tipológico</strong>: los grupos se construyen buscando las zonas con mayor concentración de elementos. Pertenecen a este tipo el <em>análisis modal de Wishart</em>, que supone clústeres esféricos y dada la complejidad de su algoritmo no tuvo mucho éxito, el <em>método TaxMap</em>, que introduce un valor de corte en caso de que los grupos no estén claramente aislados (ello lleva a que los resultados tengan un cierto grado de subjetividad), y el <em>método de Fortin</em>, también con muy escasa difusión en la literatura.</p></li>
<li><p>Los que tienen un <strong>enfoque probabilístico</strong>: las variables que caracterizan los elementos siguen una distribución de probabilidad cuyos parámetros cambian de un grupo a otro. Se trata, pues, de agrupar los elementos que pertenecen a la misma distribución. Un ejemplo es el <em>método de las combinaciones de Wolf</em>. </p></li>
</ul>
<p>No obstante, estos algoritmos y otros como, por ejemplo, los de Gitman y Levine, y Catel y Coulter, aunque muy citados en la literatura en español, tuvieron poco éxito.</p>
<p>Mayor éxito han tenido otros algoritmos como <em>expectation-maximization</em> (EM), <em>model based clustering</em> (MCLUST), <em>density-based spatial clustering of applications with noise</em> (DBSCAN), <em>ordering points to identify the clustering structure</em> (OPTICS), que es una generalización de DBSCAN, <em>wavelet-based cluster</em> (WAVECLUSTER) y <em>density-based clustering</em> (DENCLUE), entre otros.</p>
<p> </p>
<p>DBSCAN<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;La densidad se refiere al número de elementos en una misma zona. Sin embargo, es un concepto subjetivo, porque &lt;span class="math inline"&gt;\((i)\)&lt;/span&gt; ¿cuántos puntos son necesarios para considerar a una zona como densa? y &lt;span class="math inline"&gt;\((ii)\)&lt;/span&gt; ¿cómo de distantes pueden estar dichos elementos entre sí? Por ello, ambos (número de puntos y distancia) son los dos hiperparámetros del modelo. &lt;/p&gt;'><sup>215</sup></a> es, quizás, el más popular. Incluso ha recibido premios por sus numerosísimas aplicaciones a lo largo del tiempo. Soluciona los problemas de los métodos de reasignación, que son buenos para clústeres con forma esférica o convexa que no tengan demasiados <em>outliers</em> o ruido, pero que fallan cuando los clústeres tienen formas arbitrarias. De acuerdo con <span class="citation">Amat Rodrigo (<a href="referencias.html#ref-amat2017">2017</a>)</span>, DBSCAN evita este problema siguiendo la idea de que <span class="math inline">\((i)\)</span> para que una observación forme parte de un clúster, tiene que haber un mínimo de observaciones vecinas dentro de un radio de proximidad y <span class="math inline">\((ii)\)</span> que los clústeres están separados por regiones vacías o con pocas observaciones. Consecuentemente, DBSCAN necesita dos parámetros: el radio (<span class="math inline">\(\epsilon\)</span>) que define la región vecina a una observación (<span class="math inline">\(\epsilon\)</span>-<em>neighborhood</em>); y el número mínimo de puntos (minPts) u observaciones en ella.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Véase &lt;span class="citation"&gt;Amat Rodrigo (&lt;a href="referencias.html#ref-amat2017"&gt;2017&lt;/a&gt;)&lt;/span&gt; para una discusión sobre el valor de ambos hiperparámetros.&lt;/p&gt;'><sup>216</sup></a> </p>
<p>Los elementos objeto de agrupación se pueden clasificar, en función de su <span class="math inline">\(\epsilon\)</span>-<em>neighborhood</em> y minPts, como:</p>
<ol style="list-style-type: decimal">
<li><p><strong>elementos centrales</strong>, si el número de elementos en su <span class="math inline">\(\epsilon\)</span>-<em>neighborhood</em> es igual o mayor que minPts;</p></li>
<li><p><strong>elementos frontera</strong>, si no son elementos centrales pero pertenecen al <span class="math inline">\(\epsilon\)</span>-<em>neighborhood</em> de otro elemento que sí es central; y</p></li>
<li><p><strong>elementos atípicos o de ruido</strong>, si no verifican ni (1) ni (2).</p></li>
</ol>
<p>A partir de la clasificación anterior, y para <span class="math inline">\(\epsilon\)</span>-<em>neighborhood</em> y minPts dados, se origina otra:</p>
<ol style="list-style-type: decimal">
<li><p>un elemento <span class="math inline">\(Q\)</span> es <strong>denso-alcanzable directamente</strong> desde el elemento <span class="math inline">\(P\)</span> si <span class="math inline">\(Q\)</span> está en el <span class="math inline">\(\epsilon\)</span>-<em>neighborhood</em> de <span class="math inline">\(P\)</span> y <span class="math inline">\(P\)</span> es un elemento central;</p></li>
<li><p><span class="math inline">\(Q\)</span> es <strong>denso-alcanzable</strong> desde <span class="math inline">\(P\)</span> si existe una cadena de objetos <span class="math inline">\(\{Q_{1}=P, Q_2, Q_3,..., Q_{n}\}\)</span> tal que <span class="math inline">\(Q_{i+1}\)</span> es denso-alcanzable directamente desde <span class="math inline">\(Q_{i}\)</span>, <span class="math inline">\(\forall 1 \leq i \leq n\)</span>;</p></li>
<li><p><span class="math inline">\(Q\)</span> está <strong>denso-conectado</strong> con <span class="math inline">\(P\)</span> si hay un elemento <span class="math inline">\(R\)</span> desde el cual <span class="math inline">\(P\)</span> y <span class="math inline">\(Q\)</span> son denso-alcanzables.</p></li>
</ol>
<p> </p>
<p>Los pasos del algoritmo DBSCAN son los siguientes <span class="citation">(<a href="referencias.html#ref-amat2017">Amat Rodrigo 2017</a>)</span>: </p>
<ul>
<li><p>Para cada elemento u observación <span class="math inline">\(x_i\)</span> calcúlese su distancia con el resto de observaciones. Márquese como central si lo es y como visitado si no lo es.</p></li>
<li><p>Para cada observación marcada como elemento central, si aún no ha sido asignada a ningún grupo, créese un grupo nuevo y asígnesele a él. Búsquense, recursivamente, todas las observaciones denso-conectadas con ella y asígnense al mismo grupo.</p></li>
<li><p>Itérese el mismo proceso para todas todas las observaciones no visitadas.</p></li>
<li><p>Aquellas observaciones que tras haber sido visitadas no pertenecen a ningún clúster se marcan como <em>outliers</em>.</p></li>
</ul>
<p>Como resultado del algoritmo DBSCAN se generan clústeres que verifican: <span class="math inline">\((i)\)</span> todos los elementos que forman parte de un mismo clúster están denso-conectados entre ellos; y <span class="math inline">\((ii)\)</span> si un elemento <span class="math inline">\(P\)</span> es denso-alcanzable desde cualquier otro de un clúster, entonces <span class="math inline">\(P\)</span> también pertenece al clúster.</p>
<p>El éxito de DBSCAN se debe a sus importantes ventajas. De nuevo siguiendo a <span class="citation">Amat Rodrigo (<a href="referencias.html#ref-amat2017">2017</a>)</span>, no requiere la especificación previa del número de clústeres; no require esfericidad (ni ninguna forma determinada) en los grupos; y puede identificar valores atípicos, por lo que la clusterización resultante no vendrá influenciada por ellos. También tiene algunas desventajas, como que no es un método totalmente determinista puesto que <span class="math inline">\((i)\)</span> los puntos frontera que son denso-alcanzables desde más de un clúster pueden asignarse a uno u otro dependiendo del orden en el que se procesen los datos; y <span class="math inline">\((ii)\)</span> no genera buenos resultados cuando la densidad de los grupos es muy distinta, ya que no es posible encontrar un <span class="math inline">\(\epsilon\)</span>-<em>neighborhood</em> y un minPts válidos para todos a la vez.</p>
<p>Dado el escaso número de datos de la base de datos <code>TIC2021</code> del paquete <code>CDR</code> no se puede utilizar DBSCAN. Sin embargo, para ilustrar su utilización, la Fig. <a href="no-jerarquico.html#fig:density-kmeans">31.4</a> muestra la agrupación en 5 clústeres de la base de datos <code>multishapes</code> de la librería <code>factoextra</code> mediante DBSCAN (función <code>dbscan()</code>) y <span class="math inline">\(k\)</span>-medias. Se trata de una base de datos que contiene observaciones pertenecientes a 5 grupos distintos y con cierto ruido (<em>outliers</em>); en consecuencia, los grupos no deberían ser esféricos y DBSCAN sería un algoritmo de agrupación adecuado.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:density-kmeans"></span>
<img src="160027-29_cluster_files/figure-html/density-kmeans-1.png" alt="Comparación entre los algoritmos $k$-means y DBSCAN para el conjunto de datos simulado $multishapes$." width="60%"><p class="caption">
Figura 31.4: Comparación entre los algoritmos <span class="math inline">\(k\)</span>-means y DBSCAN para el conjunto de datos simulado <span class="math inline">\(multishapes\)</span>.
</p>
</div>
<p>
</p>
</div>
<div id="otros-métodos" class="section level2" number="31.4">
<h2>
<span class="header-section-number">31.4</span> Otros métodos<a class="anchor" aria-label="anchor" href="#otros-m%C3%A9todos"><i class="fas fa-link"></i></a>
</h2>
<p>Por último, en el cajón de sastre de otras técnicas de clusterización no jerárquicas, merece la pena siquiera mencionar los métodos directos, los de reducción de la dimensionalidad, los de <em>clustering</em> difuso y la clusterización basada en modelos.</p>
<p>Los <strong>métodos directos</strong> agrupan simultáneamente los elementos y las variables. El más conocido es el clúster por bloques (<span class="math inline">\(block\)</span>-; <span class="math inline">\(bi\)</span>-; <span class="math inline">\(co\)</span>-; o <em>two-mode clustering</em>). El paquete <code>biclust</code> proporciona varios algoritmos para encontrar clústeres en dos dimensiones. Además, es muy recomendable para el preprocesamiento de los datos y para la visualización y validación de los resultados.</p>
<p><strong>Las técnicas de reducción de la dimensionalidad</strong> buscan factores en el espacio de los elementos (modelo <span class="math inline">\(Q\)</span>-factorial) o de las variables (modelo <span class="math inline">\(R\)</span>-factorial) haciendo corresponder un clúster a cada factor. Centrándonos en el modelo <span class="math inline">\(Q\)</span>-factorial, el método parte de la matriz de correlaciones entre los elementos y rota ortogonalmente los factores encontrados. Dado que los elementos pueden pertenecer a varios clústeres y, por tanto, los clústeres pueden solaparse, su interpretación se hace muy difícil.</p>
<p><strong>Las técnicas de</strong> <strong><em>clustering</em></strong> <strong>difuso</strong> precisamente permiten la pertenencia de un elemento a varios clústeres, estableciendo un grado de pertenencia a cada uno de ellos. El algoritmo de <em>clustering</em> difuso más popular es <em>fuzzy</em> <em>c</em>-medias, muy similar al <em>k</em>-medias, pero que calcula los centroides como una media ponderada (la ponderación es la probabilidad de pertenencia) y, lógicamente, proporciona la probabilidad de pertenencia a cada grupo.</p>
<p>
</p>
<p>La <strong>clusterización basada en modelos</strong> tiene un enfoque estadístico y consiste en la utilización de una mixtura finita de modelos estocásticos para la construcción de los grupos.</p>
<p>
Un vector aleatorio <span class="math inline">\(\bf X\)</span> procede de una mixtura finita de distribuciones paramétricas si <span class="math inline">\(\forall \bf x \subset \bf X\)</span> su función de densidad conjunta se puede escribir como
<span class="math inline">\(f{(\bf {x} | \bf {\psi})}=\sum_{g=1}^{G} {\pi_g} f_g {(\bf {x}|\bf {\theta_g})},\)</span> donde <span class="math inline">\(\pi_g\)</span> son las proporciones asignadas a cada grupo en la mixtura, tal que <span class="math inline">\(\sum_{g=1}^{G} \pi_g =1\)</span>; <span class="math inline">\(f_g(\bf x|\theta_g)\)</span> es la función de densidad correspondiente al <span class="math inline">\(g\)</span>-ésimo grupo y <span class="math inline">\(\bf \psi=(\pi_1, \pi_2,..., \pi_G, \bf \theta_1,\bf \theta_2,...\bf \theta_G\)</span>). Las funciones de densidad <span class="math inline">\(f_g {(\bf {x}|\bf {\theta_g})}\)</span> suelen ser idénticas para todos los grupos.</p>
<p>En términos menos formales, el <em>clustering</em> basado en modelos considera que los datos observados (multivariantes) han sido generados a partir de una combinación finita de modelos componentes (distribuciones de probabilidad, normalmente paramétricas). A modo de ejemplo, en un modelo resultante de una mixtura de normales multivariantes (el caso habitual), cada componente (clúster) es una normal multivariante y el componente responsable de la generación de una observación específica determina el grupo al que pertenece dicha observación. Para la estimación de la media y matriz de covarianzas se suele recurrir al algoritmo <em>expectation-maximization</em>, una extensión del <span class="math inline">\(k\)</span>-medias.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Esta es la razón para incluir este tipo de clusterización en entre las técnicas no jerárquicas.&lt;/p&gt;"><sup>217</sup></a> El paquete <code>mclust</code> utiliza la estimación máximo verosímil para estimar dichos modelos con distintos número de clústeres, utilizando el <em>Bayesian information criterion</em> (BIC) para la selección del mejor.</p>
<p>Sus limitaciones fundamentales son (<span class="math inline">\(i\)</span>) considerar que las características de los elementos son independientes y (<span class="math inline">\(ii\)</span>) que no es recomendable para grandes bases de datos o distribuciones de probabilidad que impliquen un elevado coste computacional.</p>
<p>Una revisión de la evolución de la clusterización basada en modelos desde sus orígenes en 1965 puede verse en <span class="citation">McNicholas (<a href="referencias.html#ref-mcnicholas2016model">2016</a>)</span>. Para una idea intuitiva, véase <span class="citation">Amat Rodrigo (<a href="referencias.html#ref-amat2017">2017</a>)</span>.</p>
<p></p>
</div>
<div id="nota-final" class="section level2" number="31.5">
<h2>
<span class="header-section-number">31.5</span> Nota final<a class="anchor" aria-label="anchor" href="#nota-final"><i class="fas fa-link"></i></a>
</h2>
<p>La elección de qué técnica de clusterización utilizar, jerárquica o no, es una decisión del investigador, y dependerá de cómo quiera realizar la agrupación, la métrica de las variables y la distancia o medida de similaridad elegida. No obstante, ambos tipos de técnicas tienen sus ventajas y desventajas, y deberán ser tenidas en cuenta a la hora de decidir. Las jerárquicas adolecen de cierta inestabilidad, lo que plantea dudas sobre la fiabilidad de sus resultados. Además, a veces es difícil decidir cuántos grupos deben seleccionarse. Suelen recomendarse en caso de conjuntos de datos pequeños. En caso de grandes conjuntos de datos, la literatura suele recomendar las no jerárquicas; además, tienen una gran fiabilidad, ya que al permitir la reasignación de los elementos, una incorrecta asignación puede ser corregida posteriomente.</p>
<div id="resumen-30" class="section level3 unnumbered infobox_resume">
<h3>Resumen<a class="anchor" aria-label="anchor" href="#resumen-30"><i class="fas fa-link"></i></a>
</h3>
<p>En este capítulo se pasa revista a las principales técnicas y algoritmos de agrupación no jerárquicas.</p>
<ul>
<li><p>Se abordan los principales métodos de reasignación, y en particular los basados en centroides (método de Forgy y <span class="math inline">\(k\)</span>-medias), medoides (<span class="math inline">\(k\)</span>-medoides, PAM, CLARA, CLARANS) y medianas (<span class="math inline">\(k\)</span>-medianas).</p></li>
<li><p>Se exponen las técnicas basadas en la densidad de puntos desde las perspectivas tipológica (análisis modal, métodos TaxMap, de Fortin, de Gitman y Levine, y de Catel y Coulter) y probabilística (método de Wolf), así como se estudia el DBSCAN.</p></li>
<li><p>Se muestran otras técnicas de agrupación no jerárquicas como los métodos directos (<span class="math inline">\(block\)</span>-; <span class="math inline">\(bi\)</span>-; <span class="math inline">\(co\)</span>-; <em>two-mode clustering</em>), los de reducción de la dimensionalidad (modelos <span class="math inline">\(Q\)</span>- y <span class="math inline">\(R\)</span>-factorial), el <em>clustering</em> difuso y los métodos basados en mixturas de modelos.</p></li>
</ul>
</div>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="cap-cluster.html"><span class="header-section-number">30</span> Análisis clúster: clusterización jerárquica</a></div>
<div class="next"><a href="acp.html"><span class="header-section-number">32</span> Análisis de componentes principales</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="Índice del capítulo"><h2>Índice del capítulo</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#no-jerarquico"><span class="header-section-number">31</span> Análisis clúster: clusterización no jerárquica</a></li>
<li><a class="nav-link" href="#introducci%C3%B3n-13"><span class="header-section-number">31.1</span> Introducción</a></li>
<li>
<a class="nav-link" href="#m%C3%A9todos-de-reasignaci%C3%B3n"><span class="header-section-number">31.2</span> Métodos de reasignación</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#t%C3%A9cnicas-basadas-en-centroides-m%C3%A9todos-de-forgy-y-k-medias"><span class="header-section-number">31.2.1</span> Técnicas basadas en centroides: métodos de Forgy y \(k\)-medias</a></li>
<li><a class="nav-link" href="#t%C3%A9cnicas-basadas-en-medoides"><span class="header-section-number">31.2.2</span> Técnicas basadas en medoides</a></li>
<li><a class="nav-link" href="#t%C3%A9cnicas-basadas-en-medianas-k-medianas"><span class="header-section-number">31.2.3</span> Técnicas basadas en medianas: \(k\)-medianas</a></li>
</ul>
</li>
<li><a class="nav-link" href="#m%C3%A9todos-basados-en-la-densidad-de-elementos"><span class="header-section-number">31.3</span> Métodos basados en la densidad de elementos</a></li>
<li><a class="nav-link" href="#otros-m%C3%A9todos"><span class="header-section-number">31.4</span> Otros métodos</a></li>
<li>
<a class="nav-link" href="#nota-final"><span class="header-section-number">31.5</span> Nota final</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#resumen-30">Resumen</a></li></ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Fundamentos de ciencia de datos con <strong>R</strong></strong>" coordinado por <a href="https://blog.uclm.es/gemafaviles/" class="text-light">Gema Fernández-Avilés y José-María Montero</a>. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>Este libro ha sido generado con el paquete de R <a class="text-light" href="https://bookdown.org">bookdown</a>.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
