<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Capítulo 24 Árboles de clasificación y regresión | Fundamentos de ciencia de datos con R</title>
<meta name="author" content="Gema Fernández-Avilés y José-María Montero">
<meta name="description" content="Ramón A. Carrasco\(^{a}\), Itzcóatl Bueno\(^{a,\hspace{0,05cm} b}\) y José-María Montero\(^{c}\) \(^{a}\)Universidad Complutense de Madrid\(^{b}\)Instituto Nacional de...">
<meta name="generator" content="bookdown 0.37 with bs4_book()">
<meta property="og:title" content="Capítulo 24 Árboles de clasificación y regresión | Fundamentos de ciencia de datos con R">
<meta property="og:type" content="book">
<meta property="og:image" content="/img/cover.png">
<meta property="og:description" content="Ramón A. Carrasco\(^{a}\), Itzcóatl Bueno\(^{a,\hspace{0,05cm} b}\) y José-María Montero\(^{c}\) \(^{a}\)Universidad Complutense de Madrid\(^{b}\)Instituto Nacional de...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Capítulo 24 Árboles de clasificación y regresión | Fundamentos de ciencia de datos con R">
<meta name="twitter:description" content="Ramón A. Carrasco\(^{a}\), Itzcóatl Bueno\(^{a,\hspace{0,05cm} b}\) y José-María Montero\(^{c}\) \(^{a}\)Universidad Complutense de Madrid\(^{b}\)Instituto Nacional de...">
<meta name="twitter:image" content="/img/cover.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><link href="libs/tabwid-1.1.3/tabwid.css" rel="stylesheet">
<script src="libs/tabwid-1.1.3/tabwid.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css">
<link rel="stylesheet" href="bs4_book.css">
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Fundamentos de ciencia de datos con <strong>R</strong></a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Buscar" aria-label="Buscar">
</form>

      <nav aria-label="Contenido"><h2>Contenido</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Prefacio</a></li>
<li><a class="" href="pr%C3%B3logo-by-julia-silge.html">Prólogo (by Julia Silge)</a></li>
<li><a class="" href="pr%C3%B3logo-por-yanina-bellini.html">Prólogo (por Yanina Bellini)</a></li>
<li class="book-part">Ciencia, datos, software… y científicos</li>
<li><a class="" href="ciencia-datos.html"><span class="header-section-number">1</span> ¿Es la ciencia de datos una ciencia?</a></li>
<li><a class="" href="metodologia.html"><span class="header-section-number">2</span> Metodología en ciencia de datos</a></li>
<li><a class="" href="ch-110003.html"><span class="header-section-number">3</span> R para ciencia de datos</a></li>
<li><a class="" href="cap-etica.html"><span class="header-section-number">4</span> Ética en la ciencia de datos</a></li>
<li class="book-part">Bienvenidos a la jungla de datos</li>
<li><a class="" href="datos-sql.html"><span class="header-section-number">5</span> Gestión de bases de datos relacionales</a></li>
<li><a class="" href="cap-nosql.html"><span class="header-section-number">6</span> Gestión de bases de datos NoSQL</a></li>
<li><a class="" href="DGDQM.html"><span class="header-section-number">7</span> Gobierno, gestión y calidad del dato</a></li>
<li><a class="" href="cap-130009.html"><span class="header-section-number">8</span> Integración y limpieza de datos</a></li>
<li><a class="" href="chap-feature.html"><span class="header-section-number">9</span> Selección y transformación de variables</a></li>
<li><a class="" href="chap-herramientas.html"><span class="header-section-number">10</span> Herramientas para el análisis en ciencia de datos</a></li>
<li><a class="" href="cap-120006-aed.html"><span class="header-section-number">11</span> Análisis exploratorio de datos</a></li>
<li class="book-part">Fundamentos de estadística</li>
<li><a class="" href="Funda-probab.html"><span class="header-section-number">12</span> Probabilidad</a></li>
<li><a class="" href="Fundainfer.html"><span class="header-section-number">13</span> Inferencia estadística</a></li>
<li><a class="" href="muestreo.html"><span class="header-section-number">14</span> Muestreo y remuestreo</a></li>
<li class="book-part">Modelización estadística</li>
<li><a class="" href="cap-lm.html"><span class="header-section-number">15</span> Modelización lineal</a></li>
<li><a class="" href="cap-glm.html"><span class="header-section-number">16</span> Modelos lineales generalizados</a></li>
<li><a class="" href="cap-gam.html"><span class="header-section-number">17</span> Modelos aditivos generalizados</a></li>
<li><a class="" href="cap-mxm.html"><span class="header-section-number">18</span> Modelos mixtos</a></li>
<li><a class="" href="cap-sparse.html"><span class="header-section-number">19</span> Modelos \(\textit{sparse}\) y métodos penalizados de regresión</a></li>
<li><a class="" href="cap-series-temp.html"><span class="header-section-number">20</span> Modelización de series temporales</a></li>
<li><a class="" href="cap-discriminante.html"><span class="header-section-number">21</span> Análisis discriminante</a></li>
<li><a class="" href="cap-conjunto.html"><span class="header-section-number">22</span> Análisis conjunto</a></li>
<li><a class="" href="tablas-contingencia.html"><span class="header-section-number">23</span> Análisis de tablas de contingencia</a></li>
<li class="book-part">Machine learning supervisado</li>
<li><a class="active" href="cap-arboles.html"><span class="header-section-number">24</span> Árboles de clasificación y regresión</a></li>
<li><a class="" href="cap-svm.html"><span class="header-section-number">25</span> Máquinas de vector soporte</a></li>
<li><a class="" href="cap-knn.html"><span class="header-section-number">26</span> Clasificador \(k\)-vecinos más próximos</a></li>
<li><a class="" href="cap-naive-bayes.html"><span class="header-section-number">27</span> Naive Bayes</a></li>
<li><a class="" href="cap-bagg-rf.html"><span class="header-section-number">28</span> Métodos ensamblados: \(\bf \textit {bagging}\) y \(\bf \textit{random}\) \(\bf \textit{forest}\)</a></li>
<li><a class="" href="cap-boosting-xgboost.html"><span class="header-section-number">29</span> \(\bf \textit{Boosting}\) y el algoritmo XGBoost</a></li>
<li class="book-part">Machine learning no supervisado</li>
<li><a class="" href="cap-cluster.html"><span class="header-section-number">30</span> Análisis clúster: clusterización jerárquica</a></li>
<li><a class="" href="no-jerarquico.html"><span class="header-section-number">31</span> Análisis clúster: clusterización no jerárquica</a></li>
<li><a class="" href="acp.html"><span class="header-section-number">32</span> Análisis de componentes principales</a></li>
<li><a class="" href="af.html"><span class="header-section-number">33</span> Análisis factorial</a></li>
<li><a class="" href="mds.html"><span class="header-section-number">34</span> Escalamiento multidimensional</a></li>
<li><a class="" href="correspondencias.html"><span class="header-section-number">35</span> Análisis de correspondencias</a></li>
<li class="book-part">Deep learning</li>
<li><a class="" href="capNN.html"><span class="header-section-number">36</span> Redes neuronales artificiales</a></li>
<li><a class="" href="cap-redes-convol.html"><span class="header-section-number">37</span> Redes neuronales convolucionales</a></li>
<li class="book-part">Ciencia de datos de texto y redes</li>
<li><a class="" href="mineria-textos.html"><span class="header-section-number">38</span> Minería de textos</a></li>
<li><a class="" href="grafos.html"><span class="header-section-number">39</span> Análisis de grafos y redes sociales</a></li>
<li class="book-part">Ciencia de datos espaciales</li>
<li><a class="" href="datos-espaciales.html"><span class="header-section-number">40</span> Trabajando con datos espaciales</a></li>
<li><a class="" href="geo.html"><span class="header-section-number">41</span> Geoestadística</a></li>
<li><a class="" href="cap-econom-esp.html"><span class="header-section-number">42</span> Modelos econométricos espaciales</a></li>
<li><a class="" href="cap-pp.html"><span class="header-section-number">43</span> Procesos de puntos</a></li>
<li class="book-part">Comunica y colabora</li>
<li><a class="" href="cap-120007-informes.html"><span class="header-section-number">44</span> Informes reproducibles con R Markdown y Quarto</a></li>
<li><a class="" href="shiny.html"><span class="header-section-number">45</span> Creación de aplicaciones web interactivas con Shiny</a></li>
<li><a class="" href="github.html"><span class="header-section-number">46</span> Git y GitHub R</a></li>
<li><a class="" href="geoproces.html"><span class="header-section-number">47</span> Geoprocesamiento en nube</a></li>
<li class="book-part">Casos de estudio en ciencia de datos</li>
<li><a class="" href="cap-crimen.html"><span class="header-section-number">48</span> Análisis de una red criminal</a></li>
<li><a class="" href="cap-publicidad.html"><span class="header-section-number">49</span> Optimización de inversiones publicitarias</a></li>
<li><a class="" href="cap-twitter.html"><span class="header-section-number">50</span> ¿Cómo tuitea Elon Musk?</a></li>
<li><a class="" href="cap-periodismo.html"><span class="header-section-number">51</span> Análisis electoral: de RStudio a su periódico favorito</a></li>
<li><a class="" href="paro-clm.html"><span class="header-section-number">52</span> El impacto de las crisis financiera y de la COVID-19 en el paro de CLM</a></li>
<li><a class="" href="cap-rfm.html"><span class="header-section-number">53</span> Segmentación de clientes en el comercio minorista</a></li>
<li><a class="" href="cap-medicina.html"><span class="header-section-number">54</span> Análisis de datos en medicina</a></li>
<li><a class="" href="cap-futbol.html"><span class="header-section-number">55</span> Messi y Ronaldo: dos ídolos desde la perspectiva de los datos</a></li>
<li><a class="" href="cambioclimatico.html"><span class="header-section-number">56</span> Una nota sobre el cambio climático</a></li>
<li><a class="" href="cap-sist-exp.html"><span class="header-section-number">57</span> Implementación de un sistema experto en el ámbito pediátrico</a></li>
<li><a class="" href="cap-ree.html"><span class="header-section-number">58</span> Predicción de consumo eléctrico con redes neuronales artificiales</a></li>
<li><a class="" href="nlp-textil.html"><span class="header-section-number">59</span> El procesamiento del lenguaje natural para tendencias de moda en textil</a></li>
<li><a class="" href="cap-fraude.html"><span class="header-section-number">60</span> Detección de fraude de tarjetas de crédito</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="info-session.html"><span class="header-section-number">A</span> Información de la sesión</a></li>
<li><a class="" href="referencias.html">Referencias</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="cap-arboles" class="section level1" number="24">
<h1>
<span class="header-section-number">Capítulo 24</span> Árboles de clasificación y regresión<a class="anchor" aria-label="anchor" href="#cap-arboles"><i class="fas fa-link"></i></a>
</h1>
<p><em>Ramón A. Carrasco</em><span class="math inline">\(^{a}\)</span>, <em>Itzcóatl Bueno</em><span class="math inline">\(^{a,\hspace{0,05cm} b}\)</span> y <em>José-María Montero</em><span class="math inline">\(^{c}\)</span></p>
<p><span class="math inline">\(^{a}\)</span>Universidad Complutense de Madrid<br><span class="math inline">\(^{b}\)</span>Instituto Nacional de Estadística<br><span class="math inline">\(^{c}\)</span>Universidad de Castilla-La Mancha</p>
<p></p>
<div id="intro-dectree" class="section level2" number="24.1">
<h2>
<span class="header-section-number">24.1</span> Introducción<a class="anchor" aria-label="anchor" href="#intro-dectree"><i class="fas fa-link"></i></a>
</h2>
<p>Los árboles de decisión son modelos o algoritmos no paraméticos que se utilizan principalmente para
la resolución de problemas de clasificación, en los que hay que predecir
las distintas categorías de la variable objetivo o dependiente, aunque
también son aplicables a la predicción de valores numéricos de dicha
variable objetivo, esto es, como modelos de regresión. De ahí que sean
conocidos como árboles de clasificación y regresión (CART,
<em>classification and regression trees</em>).</p>
<p>Específicamente, lo que hacen los árboles de decisión es utilizar una serie de reglas de decisión para dividir el espacio de características predictoras en un número menor de regiones disjuntas en cada una de las cuales los valores de la variable respuesta son similares.</p>
<p>Un árbol de decisión parte del conjunto de datos de entrenamiento, correspondiente a un <strong>nodo raíz</strong>, y lo va dividiendo recursivamente en subconjuntos de datos homogéneos, dando lugar a nuevos <strong>nodos</strong>. La manera de formar los subgrupos es mediante la formulación de preguntas con respuesta binaria (si la variable respuesta es “jugar al tenis” se formula la pregunta ¿Sí o No juega al tenis?; si es “pesa más o menos de 75 kg.” la pregunta es ¿el peso es <span class="math inline">\(\leq 75\)</span> o <span class="math inline">\(&gt;75\)</span>?). El proceso de partición se lleva a cabo hasta que se alcanza algún criterio de parada previamente establecido. Es resultado que produce dicho proceso es el valor medio de la variable respuesta en las observaciones incluidas en cada subgrupo (en árboles de regresión) o la clase de la variable respuesta con presencia mayoritaria en ellos (cuando se trata de árboles de clasificación). En este último caso también proporciona, en cada subgrupo, una estimación de la probabilidad de pertenencia a cada clase.</p>
<p>Algunos ejemplos de árboles de decisión son:</p>
<ul>
<li>
<strong>Clasificación</strong>: en la medida que
la variable objetivo debe ser categórica, se podrían usar, por ejemplo,
para tomar la decisión de qué empleados deberían promocionar
(variable con dos categorías: sí promocionar o no promocionar) en
base a sus méritos, capacidades, edad, etc. Otro ejemplo podría ser
su uso para decidir si se juega o no un partido de tenis en base a
la climatología prevista. Este ejemplo se muestra gráficamente en la
Fig. <a href="cap-arboles.html#fig:dectree-plot">24.1</a>. En este último caso, el algoritmo que
se utilice indicará la decisión a tomar en base a los registros
climatológicos de los partidos que ya se hayan jugado. Así, si un
determinado día se quiere jugar al tenis, se deberán tomar como
datos de entrada las previsiones de las variables (en este caso atributos) <em>Tipo de día</em> (soleado, nublado o lluvioso), <em>Fuerza del Viento</em> y <em>Humedad</em>. En caso de
ser un día nublado, el algoritmo sugerirá que se juegue. En caso de
ser soleado, comprobará el nivel de humedad y, si no es muy elevada,
recomendará que se juegue el partido. Lo mismo pasará si la
previsión es de lluvia pero la fuerza del viento prevista no es lo
suficientemente elevada como para impedir el normal desarrollo del
partido.</li>
</ul>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:dectree-plot"></span>
<img src="img/dectree_tenis.png" alt="Ejemplo de árbol de decisión." width="60%"><p class="caption">
Figura 24.1: Ejemplo de árbol de decisión.
</p>
</div>
<ul>
<li>
<strong>Regresión</strong>: siguiendo con el ejemplo
del partido de tenis, también se puede utilizar un árbol de decisión
para determinar cuántas horas jugar de acuerdo a las condiciones
climatológicas. En la Fig. <a href="cap-arboles.html#fig:dectree-plot">24.1</a> se sustituirían la predicciones
dicotómicas SÍ/NO por valores numéricos, como se muestra en la Fig. <a href="cap-arboles.html#fig:regtree-plot">24.2</a>. Por ejemplo, el algoritmo puede sugerir jugar 5 horas si el día está soleado pero la humedad
es del 30% de vapor de agua por <span class="math inline">\(\text{m}^3\)</span>; y 3,5 horas si está soleado
pero la humedad es del 80%. También puede decidir que si el día está
nublado se jueguen 4 horas. O en caso de lluvia, podría decidir que
el partido dure 0,75 horas si la fuerza del viento es de 62 km/h y
1,15 horas si es de 27 km/h.</li>
</ul>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:regtree-plot"></span>
<img src="img/tenis-tree-reg.png" alt="Ejemplo de árbol de regresión." width="60%"><p class="caption">
Figura 24.2: Ejemplo de árbol de regresión.
</p>
</div>
<p> Como se ha comentado anteriormente, CART es un término
genérico para describir este tipo de algoritmos de árbol y también un
nombre específico para el algoritmo original de
<span class="citation">Breiman et al. (<a href="referencias.html#ref-breiman1984classification">1984</a>)</span> de construcción de árboles de clasificación
y regresión. Sin embargo, existen otros como el ID3 (<em>induction decision
trees</em>), o el C4.5, que está basado en el ID3. En la Tabla
<a href="cap-arboles.html#tab:alg-dectree">24.1</a> se muestra una pequeña comparativa de estos tres
algoritmos:</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:alg-dectree">Tabla 24.1: </span> Características de los principales algoritmos de árboles de decisión</caption>
<colgroup>
<col width="12%">
<col width="24%">
<col width="29%">
<col width="33%">
</colgroup>
<thead><tr class="header">
<th align="right">Algoritmo</th>
<th align="left">Criterio de división</th>
<th>Tipo de variables <em>input</em>
</th>
<th align="center">Estrategia de poda</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">ID3</td>
<td align="left">Ganancia de información</td>
<td>Solo categóricas</td>
<td align="center">No poda</td>
</tr>
<tr class="even">
<td align="right">CART</td>
<td align="left">Índice de Gini</td>
<td>Categóricas y numéricas</td>
<td align="center">Poda basada en el coste de complejidad</td>
</tr>
<tr class="odd">
<td align="right">C4.5</td>
<td align="left">Ratio de ganancia</td>
<td>Categóricas y numéricas</td>
<td align="center">Poda basada en el error</td>
</tr>
</tbody>
</table></div>
<p>Los árboles de decisión tienen múltiples ventajas. Entre ellas destacan:</p>
<ul>
<li>Son fáciles de entender e interpretar. Su visualización es muy clara y
permite interpretar fácilmente la salida del modelo, así como entender su proceso como
un conjunto de condicionantes.</li>
<li>El mismo algoritmo incorporado en <strong>R</strong> (<code>CART</code>) es válido tanto para
problemas de clasificación como de regresión y, por tanto, la
variable objetivo puede ser continua o categórica. Respecto a las variables de entrada, las independientes pueden ser
tanto categóricas como numéricas. Al contrario de lo que ocurre con otros
algoritmos, este último tipo de variables no necesitan ser estandarizadas, puesto que los árboles de decisión se basan en reglas y no en el cálculo de
distancias entre observaciones.</li>
<li>Tratan mejor que otros algoritmos el problema de la no linealidad.</li>
<li>Respecto a los datos, llevan a cabo un tratamiento automático de valores
ausentes (en la mayoría de los árboles de clasificación) y no se ven
afectados por las observaciones atípicas.</li>
</ul>
<p>Sin embargo, también tienen ciertas desventajas:</p>
<ul>
<li>Son inestables, ya que la inclusión de una nueva observación en la
fase de entrenamiento obliga a reconstruirlo, pudiendo modificar la
estructura del árbol final.</li>
<li>No son recomendables en caso de grandes conjuntos de datos, puesto
que el modelo entrenado puede estar sobreajustado. Este sobreajuste
es el principal problema de los árboles de decisión, ya que modelos
demasiados complejos pueden ajustar muy bien los datos observados,
pero también pueden cometer muchos errores en la fase de predicción.
Cuando se da esta circunstancia, el modelo ha aprendido los datos de
entrenamiento pero no la generalidad del problema, que es lo que
normalmente se pretende. El sobreajuste da lugar también a una
varianza elevada.</li>
<li>Tienen una capacidad predictiva inferior a la de otros algoritmos más complejos, como las redes neuronales artificiales y los <em>splines</em> de regresión adaptativos multivariantes (MARS), aunque es cierto que si se combina adecuadamente un número elevado de árboles de decisión, mediante las técnicas de <em>bagging</em> y <em>boosting</em> (véanse Caps. <a href="cap-bagg-rf.html#cap-bagg-rf">28</a> y <a href="cap-boosting-xgboost.html#cap-boosting-xgboost">29</a>, respectivamente), la potencia predictiva mejora sustancialmente.</li>
</ul>
</div>
<div id="procedimiento-con-r-la-función-rpart" class="section level2" number="24.2">
<h2>
<span class="header-section-number">24.2</span> Procedimiento con <strong>R</strong>: la función <code>rpart()</code><a class="anchor" aria-label="anchor" href="#procedimiento-con-r-la-funci%C3%B3n-rpart"><i class="fas fa-link"></i></a>
</h2>
<p>En el paquete <code>rpart</code> de <strong>R</strong> se encuentra la función <code><a href="https://rdrr.io/pkg/rpart/man/rpart.html">rpart()</a></code> que se
utiliza para entrenar un árbol de decisión:</p>
<div class="sourceCode" id="cb343"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">rpart</span><span class="op">(</span><span class="va">formula</span>, <span class="va">data</span>, <span class="va">...</span><span class="op">)</span></span></code></pre></div>
<ul>
<li>
<code>formula</code>: refleja la relación entre la variable dependiente <span class="math inline">\(Y\)</span> y
los predictores tal que <span class="math inline">\(Y \sim X_1 + ... + X_p\)</span>.</li>
<li>
<code>data</code>: conjunto de datos con el que entrenar el árbol de acuerdo a
la fórmula indicada.</li>
</ul>
</div>
<div id="árboles-de-clasificación" class="section level2" number="24.3">
<h2>
<span class="header-section-number">24.3</span> Árboles de clasificación<a class="anchor" aria-label="anchor" href="#%C3%A1rboles-de-clasificaci%C3%B3n"><i class="fas fa-link"></i></a>
</h2>
<p></p>
<p>Como se avanzó en la sección anterior, formalmente, un árbol de decisión es un grafo acíclico (un grafo sin ciclos, siendo un ciclo un circuito completo) que se inicia en un <strong>nodo
raíz</strong>, el cual se divide en <strong>ramas</strong>,
también conocidas como <strong>aristas</strong>. De las ramas salen las
<strong>hojas</strong>, también denominadas <strong>nodos</strong>. Estos
nodos pueden ser <strong>nodos finales</strong>, o <strong>puntos de
decisión</strong>, (si de ellos no salen nuevas ramas
con nuevos nodos) o no ( si de ellos salen nuevas ramas con nuevas hojas o
nodos) y así hasta que todos los nodos sean puntos de decisión. En el
ejemplo de la Fig. <a href="cap-arboles.html#fig:dectree-plot">24.1</a> el nodo raíz es la caja <em>Tipo
de día</em>. Las ramas o aristas son sus tres niveles o categorías:
<em>Soleado</em>, <em>Nublado</em> o <em>Lluvia</em>. Cada una de estas ramas conecta con una
nueva hoja o nodo: <em>Humedad</em> o <em>Viento</em> en los casos de soleado o
lluvia, respectivamente. Sin embargo, en ese ejemplo, <em>Nublado</em>
representa un nodo terminal, puesto que, llegados a ese punto, la salida
que proporcionaría el árbol es <em>Jugar al tenis</em>. Este proceso se
repite utilizando el conjunto de datos disponible en cada hoja,
generándose una clasificación final cuando una hoja no tenga ramas
nuevas, en cuyo caso recibe la denominación de nodo final. El objetivo
es que el árbol sea lo más general y pequeño posible. Esto se consigue
seleccionando, en cada paso, la variable que optimice la división de los
datos en subconjuntos homogéneos, de tal forma que se prediga mejor la
clase objetivo.</p>
<p>La Fig. <a href="cap-arboles.html#fig:form-arbol">24.3</a> ilustra la nomenclatura relativa a los elementos del árbol mencionados anteriormente.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:form-arbol"></span>
<img src="img/formacion-arbol.png" alt="Ejemplo de formación de árbol de decisión." width="90%"><p class="caption">
Figura 24.3: Ejemplo de formación de árbol de decisión.
</p>
</div>
<div id="cómo-se-va-formando-el-árbol-de-clasificación" class="section level3" number="24.3.1">
<h3>
<span class="header-section-number">24.3.1</span> ¿Cómo se va formando el árbol de clasificación?<a class="anchor" aria-label="anchor" href="#c%C3%B3mo-se-va-formando-el-%C3%A1rbol-de-clasificaci%C3%B3n"><i class="fas fa-link"></i></a>
</h3>
<p></p>
<p>Como ya se ha mencionado, la construcción de un árbol de decisión se basa en la división recursiva en nuevas ramas, es decir, cada división
está condicionada por las anteriores. El objetivo en cada nodo es
encontrar la variable (o atributo, como en este caso) más adecuada para dividir los datos de ese nodo en dos nuevos subconjuntos, de tal forma que el error global entre la clase
observada y la predicha por el árbol se minimice. Para la construcción
de árboles de clasificación, el algoritmo CART utiliza la medida de
impureza de Gini para generar las particiones, mientras que los
algoritmos ID3 y C4.5 están basados en medidas de entropía.</p>
<div id="impureza-de-gini" class="section level4" number="24.3.1.1">
<h4>
<span class="header-section-number">24.3.1.1</span> Impureza de Gini<a class="anchor" aria-label="anchor" href="#impureza-de-gini"><i class="fas fa-link"></i></a>
</h4>
<p>La <strong>impureza de Gini</strong> es una medida
de la frecuencia con la que una observación elegida aleatoriamente de un
conjunto de observaciones (en este caso los conjuntos de observaciones correspondientes a los nodos) se asignaría a la clase errónea si se etiqueta al azar en una
de las clases que se consideran. Formalmente, sea <span class="math inline">\(\bf{X}\)</span> un conjunto de
datos con <span class="math inline">\(\kappa\)</span> clases, y sea <span class="math inline">\(p_i\)</span> la probabilidad de que una
observación pertenezca a la clase <span class="math inline">\(i=1,...,\kappa\)</span>. La impureza de Gini para <span class="math inline">\(\bf{X}\)</span> se
define como:</p>
<span class="math display">\[\begin{equation}
Gini({\bf{X}}) = 1 - \sum^{\kappa}_{i=1}{p^{2}_{i}},
\end{equation}\]</span>
<p>donde, en este caso, al ser la respuesta binaria, <span class="math inline">\({\bf{\kappa}}=2\)</span>.</p>
<p>A la hora de construir el árbol se selecciona el atributo con menor impureza ponderada de Gini para dividir en dos el conjunto de datos <span class="math inline">\(\bf{X}\)</span> correspondiente al nodo del que se trate: el subconjunto con el primer valor de la variable respuesta, <span class="math inline">\({\bf{X}}_1\)</span> y el subconjunto con el segundo valor de la variable respuesta, <span class="math inline">\({\bf{X}}_2\)</span>.</p>
<p>Si en un nodo el subconjunto de datos se divide en dos subconjuntos <span class="math inline">\(\bf{\bf {X}}_1\)</span> y <span class="math inline">\({\bf{X}}_2\)</span>, con tamaños <span class="math inline">\(n_1\)</span> y <span class="math inline">\(n_2\)</span>, respectivamente, la impureza ponderada de Gini se define como:</p>
<span class="math display">\[\begin{equation}
Gini_{\varphi}({\bf{X}}) = \frac{n_1}{n}{Gini \left(\bf{X}_{1}\right) } +  \frac{n_2}{n}{Gini \left({\bf{X}}_{2}\right)},
\end{equation}\]</span>
<p>donde las probabilidades se computan a partir de las correspondientes frecuencias relativas.</p>
<p>En el ejemplo de la Fig. <a href="cap-arboles.html#fig:dectree-plot">24.1</a> considérese la
situación reflejada en la Tabla <a href="cap-arboles.html#tab:data-imp-gini">24.2</a>:</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:data-imp-gini">Tabla 24.2: </span> Datos para decidir si se juega el partido</caption>
<thead><tr class="header">
<th>Día</th>
<th align="center">Tipo de día</th>
<th align="center">Humedad</th>
<th align="center">Viento</th>
<th align="center">Decisión</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>1</td>
<td align="center">Soleado</td>
<td align="center">Fuerte</td>
<td align="center">Débil</td>
<td align="center">NO</td>
</tr>
<tr class="even">
<td>2</td>
<td align="center">Soleado</td>
<td align="center">Fuerte</td>
<td align="center">Fuerte</td>
<td align="center">NO</td>
</tr>
<tr class="odd">
<td>3</td>
<td align="center">Lluvia</td>
<td align="center">Fuerte</td>
<td align="center">Débil</td>
<td align="center">SÍ</td>
</tr>
<tr class="even">
<td>4</td>
<td align="center">Nublado</td>
<td align="center">Fuerte</td>
<td align="center">Débil</td>
<td align="center">SÍ</td>
</tr>
<tr class="odd">
<td>5</td>
<td align="center">Lluvia</td>
<td align="center">Débil</td>
<td align="center">Débil</td>
<td align="center">SÍ</td>
</tr>
<tr class="even">
<td>6</td>
<td align="center">Lluvia</td>
<td align="center">Débil</td>
<td align="center">Fuerte</td>
<td align="center">NO</td>
</tr>
<tr class="odd">
<td>7</td>
<td align="center">Soleado</td>
<td align="center">Fuerte</td>
<td align="center">Débil</td>
<td align="center">NO</td>
</tr>
<tr class="even">
<td>8</td>
<td align="center">Nublado</td>
<td align="center">Débil</td>
<td align="center">Fuerte</td>
<td align="center">SÍ</td>
</tr>
<tr class="odd">
<td>9</td>
<td align="center">Soleado</td>
<td align="center">Débil</td>
<td align="center">Débil</td>
<td align="center">SÍ</td>
</tr>
<tr class="even">
<td>10</td>
<td align="center">Lluvia</td>
<td align="center">Débil</td>
<td align="center">Débil</td>
<td align="center">SÍ</td>
</tr>
<tr class="odd">
<td>11</td>
<td align="center">Soleado</td>
<td align="center">Débil</td>
<td align="center">Fuerte</td>
<td align="center">SÍ</td>
</tr>
<tr class="even">
<td>12</td>
<td align="center">Nublado</td>
<td align="center">Fuerte</td>
<td align="center">Fuerte</td>
<td align="center">SÍ</td>
</tr>
<tr class="odd">
<td>13</td>
<td align="center">Nublado</td>
<td align="center">Débil</td>
<td align="center">Débil</td>
<td align="center">SÍ</td>
</tr>
<tr class="even">
<td>14</td>
<td align="center">Lluvia</td>
<td align="center">Fuerte</td>
<td align="center">Fuerte</td>
<td align="center">SÍ</td>
</tr>
<tr class="odd">
<td>15</td>
<td align="center">Soleado</td>
<td align="center">Fuerte</td>
<td align="center">Fuerte</td>
<td align="center">NO</td>
</tr>
</tbody>
</table></div>
<p>Lo primero que se ha de decidir es cuál es el nodo raíz del árbol (el punto de partida del proceso recursivo de partición). Hay tres candidaturas: la de <em>Tipo de día</em>, la de <em>Humedad</em> y la de <em>Fuerza del viento</em>. Se elegirá aquella con menor impureza ponderada de Gini.</p>
<p>Comenzando, por ejemplo, con el <em>Tipo de día</em>, el conjunto de datos se divide en tres subconjuntos (de 6, 4 y 5 observaciones, respectivamente), tal y como muestra la Tabla
<a href="cap-arboles.html#tab:data-td-imp-gini">24.3</a>:</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:data-td-imp-gini">Tabla 24.3: </span> Días que se juega o no de acuerdo al <em>Tipo de
día</em>
</caption>
<thead><tr class="header">
<th>Tipo de día</th>
<th align="center">SÍ</th>
<th align="center">NO</th>
<th align="center"># observaciones</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Soleado</td>
<td align="center">2</td>
<td align="center">4</td>
<td align="center">6</td>
</tr>
<tr class="even">
<td>Nublado</td>
<td align="center">4</td>
<td align="center">0</td>
<td align="center">4</td>
</tr>
<tr class="odd">
<td>Lluvia</td>
<td align="center">4</td>
<td align="center">1</td>
<td align="center">5</td>
</tr>
</tbody>
</table></div>
<p>La impureza de Gini para cada una de las tres categorías es:
<span class="math display">\[\begin{equation*}
Gini(Soleado) = 1 - \Bigl(\frac{2}{6}\Bigr)^{2} - \Bigl(\frac{4}{6}\Bigr)^{2} = \text {0,45},
\end{equation*}\]</span>
<span class="math display">\[\begin{equation*}
Gini(Nublado) = 1 - \Bigl(\frac{4}{4}\Bigr)^{2} = 0,
\end{equation*}\]</span>
<span class="math display">\[\begin{equation*}
Gini(Lluvia) = 1 - \Bigl(\frac{4}{5}\Bigr)^{2} - \Bigl(\frac{1}{5}\Bigr)^{2} = \text {0,32}.
\end{equation*}\]</span></p>
<p>siendo la pregunta que se formula: ¿Se juega al tenis?, con respuestas SÍ y NO.</p>
<p>La suma ponderada de las anteriores impurezas proporciona la impureza ponderada (de Gini) de
variable <em>Tipo de día</em>:</p>
<span class="math display">\[\begin{equation*}
Gini(Tipo \hspace{0,1cm}de \hspace{0,1cm} día) = \text {0,45}\cdot\Bigl(\frac{6}{15}\Bigr) + 0\cdot\Bigl(\frac{4}{15}\Bigr) + \text {0,32}\cdot\Bigl(\frac{5}{15}\Bigr) = \text {0,29}.
\end{equation*}\]</span>
<p>Del mismo modo, se puede calcular la impureza ponderada de Gini de <em>Humedad</em> y <em>Viento</em>, los dos nodos candidatos a ser nodo raíz. La Tabla <a href="cap-arboles.html#tab:hum-imp-gini">24.4</a> y la Tabla
<a href="cap-arboles.html#tab:wind-imp-gini">24.5</a> presentan los subconjuntos de datos que se forman para cada una de sus categorías, respectivamente. A partir de dichas impurezas, debajo de cada tabla, se calcula la impureza ponderada de cada uno de estos dos nodos candidatos.</p>
<div class="inline-table"><table style="width:100%;" class="table table-sm">
<caption>
<span id="tab:hum-imp-gini">Tabla 24.4: </span> Impureza de Gini para las categorías de <em>Humedad</em>
</caption>
<colgroup>
<col width="12%">
<col width="6%">
<col width="6%">
<col width="24%">
<col width="13%">
<col width="13%">
<col width="24%">
</colgroup>
<thead><tr class="header">
<th>Humedad</th>
<th align="center">SÍ</th>
<th align="center">NO</th>
<th align="center"># observaciones</th>
<th align="center"><span class="math inline">\(p_{SÍ}\)</span></th>
<th align="center"><span class="math inline">\(p_{NO}\)</span></th>
<th align="center">Impureza de Gini</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Fuerte</td>
<td align="center">4</td>
<td align="center">4</td>
<td align="center">8</td>
<td align="center">0,50</td>
<td align="center">0,50</td>
<td align="center">0,50</td>
</tr>
<tr class="even">
<td>Débil</td>
<td align="center">6</td>
<td align="center">1</td>
<td align="center">7</td>
<td align="center">0,86</td>
<td align="center">0,14</td>
<td align="center">0,76</td>
</tr>
</tbody>
</table></div>
<span class="math display">\[\begin{equation*}
Gini(Humedad) = \text {0,5}\cdot\Bigl(\frac{8}{15}\Bigr) + \text {0,76}\cdot\Bigl(\frac{7}{15}\Bigr) = \text {0,62}.
\end{equation*}\]</span>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:wind-imp-gini">Tabla 24.5: </span> Impureza de Gini para las categorías de <em>Viento</em>
</caption>
<colgroup>
<col width="10%">
<col width="6%">
<col width="6%">
<col width="24%">
<col width="13%">
<col width="13%">
<col width="24%">
</colgroup>
<thead><tr class="header">
<th>Viento</th>
<th align="center">SÍ</th>
<th align="center">NO</th>
<th align="center"># observaciones</th>
<th align="center"><span class="math inline">\(p_{SÍ}\)</span></th>
<th align="center"><span class="math inline">\(p_{NO}\)</span></th>
<th align="center">Impureza de Gini</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Fuerte</td>
<td align="center">4</td>
<td align="center">3</td>
<td align="center">7</td>
<td align="center">0,57</td>
<td align="center">0,43</td>
<td align="center">0,49</td>
</tr>
<tr class="even">
<td>Débil</td>
<td align="center">6</td>
<td align="center">2</td>
<td align="center">8</td>
<td align="center">0,75</td>
<td align="center">0,25</td>
<td align="center">0,38</td>
</tr>
</tbody>
</table></div>
<span class="math display">\[\begin{equation*}
Gini(Viento) = \text {0,49}\cdot\Bigl(\frac{7}{15}\Bigr) + \text{0,38}\cdot\Bigl(\frac{8}{15}\Bigr) = \text{0,43}.
\end{equation*}\]</span>
<p>En la Tabla <a href="cap-arboles.html#tab:features-imp-gini">24.6</a> se muestran las impurezas ponderadas de cada una de las variables candidatas a ser nodo raíz. Se puede comprobar que la menor impureza ponderada de Gini corresponde a <em>Tipo de día</em>; por consiguiente, se elige como nodo raíz del árbol de clasificación.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:features-imp-gini">Tabla 24.6: </span> Impureza de Gini para las variables de
entrada</caption>
<thead><tr class="header">
<th>Variable</th>
<th align="center">Impureza ponderada de Gini</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Tipo de día</td>
<td align="center">0,29</td>
</tr>
<tr class="even">
<td>Humedad</td>
<td align="center">0,62</td>
</tr>
<tr class="odd">
<td>Viento</td>
<td align="center">0,43</td>
</tr>
</tbody>
</table></div>
<p>La siguiente decisión a tomar es: en cada categoría de <em>Tipo de día</em>, ¿qué variable (<em>Humedad</em> o <em>Viento</em>) se elige para llevar a cabo una nueva partición del subconjunto de datos correspondiente cada una de ellas? La respuesta a esta pregunta la proporciona la ganancia de información, <span class="math inline">\(\Delta Gini()\)</span>, correspondiente a cada una de estas dos variables:</p>
<span class="math display">\[\begin{equation}
\Delta Gini(\varphi) =  Gini({\bf{X}}) - Gini_{\varphi}({\bf{X}}),
\end{equation}\]</span>
<p>si bien, como puede comprobarse, la mayor ganancia de información corresponde a la menor impureza ponderada de Gini, puesto que <span class="math inline">\(Gini({\bf{X}})\)</span> permanece constante.</p>
<p>Por ejemplo, para obtener la ganancia de información para <em>Humedad</em> en cada una de las categorías de <em>Tipo de día</em>,<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Aunque se utilice el término “variable”, por simplicidad, es válido tanto para variables propiamente dichas como para atributos.&lt;/p&gt;"><sup>185</sup></a> la impureza
ponderada de <em>Humedad</em> se resta de la impureza de cada una de las tres categorías de <em>Tipo de día</em>. Lo mismo se haría con <em>Viento</em>.</p>
<p>Dicho lo anterior, para la categoría <em>Tipo de día: soleado</em>, la Tabla <a href="cap-arboles.html#tab:sunfeat-imp-gini">24.7</a> muestra el valor de las impurezas ponderadas de las dos variables candidatas a ser las nuevas “particionadoras”:</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:sunfeat-imp-gini">Tabla 24.7: </span> Impureza de Gini para las variables en días
soleados</caption>
<thead><tr class="header">
<th>Variable</th>
<th align="center">Impureza ponderada de Gini</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Humedad</td>
<td align="center">0,00</td>
</tr>
<tr class="even">
<td>Viento</td>
<td align="center">0,44</td>
</tr>
</tbody>
</table></div>
<p>Entonces, la ganancia de Gini para cada variable es:</p>
<p>Puede observarse que, en la categoría <em>Tipo de día: soleado</em>, la ganancia de información que se obtiene al particionar el subconjunto de datos correspondiente a dicha categoría según <em>Humedad</em> es mayor (en realidad mucho mayor) que al hacerlo según <em>Viento</em>, por lo que <em>Humedad</em> es la variable elegida para realizar la partición del subconjunto de datos de la categoría <em>Tipo de día: soleado</em>, como se observa en la Fig. <a href="cap-arboles.html#fig:dectree-plot">24.1</a>.</p>
<p>En el caso de la categoría <em>Tipo de día: nublado</em>, el subconjunto de observaciones es de tamaño 4 y en todas ellas la categoría de la variable respuesta es SÍ (se juega al tenis); su impureza es nula. Por tanto, no tiene sentido realizar partición alguna del subconjunto de datos, ni según <em>Humedad</em> ni según <em>Viento</em>. Se trata de un punto de decisión final.</p>
<p>Finalmente, para la categoría <em>Tipo de día: con lluvia</em>, con un subconjunto de 5 observaciones y una impureza de Gini de 0,32, la impureza ponderada correspondiente a <em>Humedad</em> es de 0,267, mientras que la correspondiente a <em>Viento</em> es 0,20, por lo que la ganancia de información de tomar como variable divisora <em>Humedad</em> es 0,053 mientras que se eleva a 0,12 cuando la variable divisora es <em>Viento</em> (se deja al lector la labor de comprobar etos cálculos). Es por eso que, como puede verse en Fig. <a href="cap-arboles.html#fig:dectree-plot">24.1</a>, se elige esta última para realizar una nueva partición en el subconjunto de datos <em>Tipo de día: con lluvia</em>.</p>
<p>En este punto del proceso de partición (o crecimiento del árbol) ya se tiene primer nivel de nodos, que surgen del nodo raíz. A partir de este primer nivel, siguiendo el mismo proceso llevado a cabo más arriba, se obtendría un segundo nivel de nodos, y a partir de estos, un tercero, y así sucesivamente hasta que el criterio de parada preestablecido indique que el proceso de finalización ha terminado. En el ejemplo propuesto, al ser tan sencillo, pues el conjunto de datos solo cuenta con tres variables predictoras, solo se va a decidir cuál es el nodo raíz y cuáles los que componen el primer nivel de nodos. Con tres variables predictoras no se pueden obtener más niveles de nodos.</p>
</div>
<div id="entropía" class="section level4" number="24.3.1.2">
<h4>
<span class="header-section-number">24.3.1.2</span> Entropía <a class="anchor" aria-label="anchor" href="#entrop%C3%ADa"><i class="fas fa-link"></i></a>
</h4>
<p>La entropía es un concepto matemático que mide el grado de incertidumbre que se tiene en relación con un conjunto de datos, es decir, la varianza en los datos entre
diferentes clases. Dada una variable predictora, para los conjuntos de datos de cada una de sus categorías, la entropía se calcula
como:</p>
<span class="math display" id="eq:entropy">\[\begin{equation}
E = -p_1\log_2 (p_1) - p_2\log_2 (p_2),
\tag{24.1}
\end{equation}\]</span>
<p>donde <span class="math inline">\(p_1\)</span> y <span class="math inline">\(p_2\)</span> representan la probabilidad de pertenecer a cada una
de las clases de la variable respuesta (en este caso dos) en ese nodo, y se estiman mediante sus correspondientes frecuencias relativas. En teoría de la información, la base logarítmica varía dependiendo de la aplicación, y con ella varía la
unidad de medida.</p>
<p>Se denomina entropía ponderada de la variable a la suma ponderada de las entropías correspondientes a sus categorías, siendo las ponderaciones el número de observaciones en cada categoría dividido por el número de observaciones correspondientes a la variable en el nodo del que se trate.</p>
<p>Finalmente, se define ganancia de información, <span class="math inline">\(IG\)</span>, como:</p>
<span class="math display">\[\begin{equation}
IG = E_{\varkappa} - E_{\varkappa + 1},
\end{equation}\]</span>
<p>donde <span class="math inline">\(E_\varkappa\)</span> representa la entropía en el nodo “padre” (en el conjunto de datos que se está particionando), mientras
que <span class="math inline">\(E_{\varkappa+1}\)</span> representa la entropía en cada categoría de dicho nodo padre (en el conjunto de datos de cada categoría de la variable del nodo padre).</p>
<p>Lógicamente, se elige como variable “particionadora” aquella en la que que se obtiene mayor ganancia de información con la partición.</p>
<p>Volviendo de nuevo al ejemplo basado en los datos de la
Tabla <a href="cap-arboles.html#tab:data-td-imp-gini">24.3</a>, para generar el árbol de clasificación se siguen los mismos pasos que en la subsección anterior, pero sustituyendo la medida de impureza por la medida de entropía.</p>
<p>Se parte de la entropía existente en el conjunto total de datos:</p>
<span class="math display">\[\begin{equation*}
E = -\frac{10}{15}\log_2 \Bigl(\frac{10}{15}\Bigr) - \frac{5}{15}\log_2 \Bigl(\frac{5}{15}\Bigr) = \text {0,9183}.
\end{equation*}\]</span>
<p>y, en cada nodo, se va dividiendo el árbol en función de la variable que más reduzca la entropía existente en dicho nodo.</p>
<p>La primera decisión es la relativa a cuál de las tres variables predictoras ocupará el nodo raíz. Para ello, se procede como sigue:<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Aunque nos estamos centrando en la primera división que se lleva a cabo, la del conjunto total de datos, los pasos son los mismos para dividir, posteriormente, el conjunto de datos correspondiente a cualquier nodo del árbol.&lt;/p&gt;"><sup>186</sup></a></p>
<ol style="list-style-type: decimal">
<li>Para cada una de las variables predictoras se calcula la entropía de cada una de sus categorías o niveles.</li>
<li>A partir de dichas entropías, ponderándolas adecuadamente, se calcula la entropía de cada una de las variables predictoras.</li>
<li>Se calcula la ganancia de información que se obtiene al dividir el conjunto de datos según dicha variable restando de la entropía del conjunto de datos total la entropía ponderada de cada variable.</li>
<li>El conjunto de datos se divide en función de la variable con la que se tenga más ganancia de información.</li>
</ol>
<p>Comenzando por la variable <em>Tipo de día</em> se calcula:</p>
<p><span class="math display">\[\begin{equation*}
E_{Soleado} = -\frac{2}{6}\log_2 \Bigl(\frac{2}{6}\Bigr) - \frac{4}{6}\log_2 \Bigl(\frac{4}{6}\Bigr) = \text{0,9183},
\end{equation*}\]</span>
<span class="math display">\[\begin{equation*}
E_{Nublado} = -\frac{4}{4}\log_2 \Bigl(\frac{4}{4}\Bigr) - \frac{0}{4}\log_2 \Bigl(\frac{0}{4}\Bigr) = 0,
\end{equation*}\]</span>
<span class="math display">\[\begin{equation*}
E_{Lluvia} = -\frac{4}{5}\log_2 \Bigl(\frac{4}{5}\Bigr) - \frac{1}{5}\log_2 \Bigl(\frac{1}{5}\Bigr) = \text {0,7219}.
\end{equation*}\]</span></p>
<p>Con lo que se tiene que:</p>
<span class="math display">\[\begin{equation*}
E_{\text{Tipo de día}} = \frac{6}{15}\cdot \text {0,9183} + \frac{4}{15}\cdot 0 + \frac{5}{15}\cdot \text {0,7219} = \text {0,608},
\end{equation*}\]</span>
<p>y que:</p>
<p><span class="math display">\[\begin{equation*}
IG_{\text{Tipo de día}} = E - E_{\text{Tipo de día}} = \text {0,918} - \text {0,608} = \text {0,310}.
\end{equation*}\]</span></p>
<p>Repitiendo el mismo procedimiento con las variables <em>Viento</em> y <em>Humedad</em>
se puede comprobar que <span class="math inline">\(E(Viento) = \text {0,893}\)</span> y <span class="math inline">\(E(Humedad) = \text{0,809}\)</span>, con lo que:</p>
<p><span class="math display">\[\begin{equation*}
IG_{Viento} = E - E_{Viento} = \text {0,918 - 0,893 = 0,025},
\end{equation*}\]</span>
<span class="math display">\[\begin{equation*}
IG_{Humedad} = E - E_{Humedad} = \text {0,918 - 0,809 = 0,109},
\end{equation*}\]</span></p>
<p>pudiéndose comprobar que la disminución de la aleatoriedad, o la ganancia
de información, es mayor para la variable <em>Tipo de día</em> y, por tanto, se
elige para ser el nodo raíz. Repitiendo este proceso, igual que se hizo en la subsección anterior, se va construyendo
el árbol hasta alcanzar los nodos terminales.</p>
</div>
</div>
<div id="sobreajuste" class="section level3" number="24.3.2">
<h3>
<span class="header-section-number">24.3.2</span> Sobreajuste <a class="anchor" aria-label="anchor" href="#sobreajuste"><i class="fas fa-link"></i></a>
</h3>
<p>Ya se ha comentado en la Sec. <a href="cap-arboles.html#intro-dectree">24.1</a> que una de las
principales desventajas de los árboles de decisión es su propensión a
sobreajustar el modelo al conjunto de datos de entrenamiento y, por
tanto, hay que prestar especial atención a la complejidad del modelo.
Basándose en las observaciones utilizadas en la fase de entrenamiento,
un árbol de decisión puede extraer los patrones presentes en el conjunto
de observaciones de entrenamiento y ser muy preciso en el ajuste de
dichas observaciones. Sin embargo, puede ocurrir que el árbol resultante
no sea capaz de clasificar correctamente
ni el conjunto de validación aun con nuevas observaciones.
Esta circunstancia puede ocurrir porque haya
patrones no observados en los datos de entrenamiento que el modelo no es
capaz de detectar, o porque la división de los datos entre entrenamiento
y validación no se realizó correctamente siendo los datos de
entrenamiento no representativos del conjunto de datos completo.
Al intentar que el árbol entrenado tenga la capacidad de aprender
patrones muy complejos, se puede producir este sobreajuste, que se materializa
en árboles muy profundos. La forma de evitar el sobreajuste es
controlar el crecimiento del árbol para evitar que se vuelva
excesivamente complejo.</p>
</div>
<div id="cuánto-debe-crecer-un-árbol-de-clasificación" class="section level3" number="24.3.3">
<h3>
<span class="header-section-number">24.3.3</span> ¿Cuánto debe crecer un árbol de clasificación?<a class="anchor" aria-label="anchor" href="#cu%C3%A1nto-debe-crecer-un-%C3%A1rbol-de-clasificaci%C3%B3n"><i class="fas fa-link"></i></a>
</h3>
<p></p>
<p>En cada paso de construcción del árbol se determina la variable óptima
para realizar la división de las observaciones de un nodo padre en sus
nodos hijos. La pregunta es: ¿cuándo se detiene?, ¿cuál es el criterio
de parada? Por ejemplo, se puede utilizar como criterio de parada que el
árbol alcance un tamaño o profundidad determinado, para que no sea
excesivamente complejo y así no tengan lugar las consecuencias derivadas
del sobreajuste.</p>
<p>Por consiguiente, se debe llegar a un equilibrio entre la profundidad y
complejidad del árbol para optimizar la predicción de
observaciones futuras. Este equilibrio se puede lograr siguiendo alguno
de los siguientes enfoques: la parada temprana o la poda.</p>
<div id="la-parada-temprana" class="section level4" number="24.3.3.1">
<h4>
<span class="header-section-number">24.3.3.1</span> La parada temprana<a class="anchor" aria-label="anchor" href="#la-parada-temprana"><i class="fas fa-link"></i></a>
</h4>
<p></p>
<p>La parada temprana restringe el crecimiento del árbol, tanto de
clasificación como de regresión, de forma explícita. Existen distintas
maneras de imponer al árbol esta restricción, pero dos de las
técnicas más populares son: <span class="math inline">\((i)\)</span> restringir la profundidad a un cierto
nivel y <span class="math inline">\((ii)\)</span> establecer un número mínimo de observaciones permitidas en
un nodo terminal. En el primer caso, el árbol deja de dividirse al
llegar a cierta profundidad. Así, cuanto menos profundo sea el árbol,
menos variación habrá en las predicciones que proporcione. Sin embargo,
existe el riesgo de introducir mucho sesgo al modelo al no ser capaz de
captar interacciones y patrones complejos en los datos. El segundo
enfoque lo que provoca es que no se dividan nodos intermedios con pocas
observaciones. Llevando el razonamiento al extremo, si se permite que un nodo terminal
tenga solo una observación, esta actuaría como predicción, si bien su variabilidad sería muy elevada. Si, por el contrario, se exige un gran número de
observaciones en los nodos terminales, se reduce el número de divisiones y,
por lo tanto, se reduce la varianza, pero puede haber patrones interesantes en los datos que el árbol no es capaz de detectar y aprender.</p>
</div>
<div id="la-poda" class="section level4" number="24.3.3.2">
<h4>
<span class="header-section-number">24.3.3.2</span> La poda<a class="anchor" aria-label="anchor" href="#la-poda"><i class="fas fa-link"></i></a>
</h4>
<p></p>
<p>El otro enfoque es el de la poda, que consiste en construir un árbol muy
profundo y complejo y después podarlo para encontrar el subárbol óptimo.
En este proceso, se utiliza un hiperparámetro de complejidad
<span class="math inline">\((\zeta)\)</span> que penaliza la función objetivo de la partición por el número
de nodos terminales del árbol <span class="math inline">\((\tau)\)</span>. El subárbol óptimo es aquel que minimiza:</p>
<span class="math display" id="eq:poda">\[\begin{equation}
R_{\zeta}(\tau) = R(\tau) + \zeta|\tau|,
\tag{24.2}
\end{equation}\]</span>
<p>donde <span class="math inline">\(R(\tau)\)</span> es la tasa total de clasificación errónea en los nodos terminales,
<span class="math inline">\(|\tau|\)</span> es el número total de nodos y <span class="math inline">\(\zeta\)</span> es el hiperparámetro de
complejidad. A medida que <span class="math inline">\(\zeta\)</span> aumenta, más ramas del árbol son
podadas, mientras que para valores más bajos los modelos resultantes son más complejos y, en consecuencia, más grandes. En conclusión, a medida que
el árbol crece, el error de entrenamiento (la tasa total de clasificación errónea en los nodos terminales) debe tener una reducción mayor
que el término de penalización por complejidad.</p>
</div>
</div>
<div id="ejemplo-árbol-de-clasificación-para-determinar-la-intención-de-compra" class="section level3" number="24.3.4">
<h3>
<span class="header-section-number">24.3.4</span> Ejemplo: árbol de clasificación para determinar la intención de compra<a class="anchor" aria-label="anchor" href="#ejemplo-%C3%A1rbol-de-clasificaci%C3%B3n-para-determinar-la-intenci%C3%B3n-de-compra"><i class="fas fa-link"></i></a>
</h3>
<p>A continuación se describe el caso que se va a resolver mediante modelos
de clasificación tanto en este como en los siguientes capítulos.</p>
<p>Existen
diversas aserciones para definir Comercio Electrónico (CE). Entre ellas,
la Organización para la Cooperación y el Desarrollo Económico (OCDE) lo
define como el proceso de compra, venta o intercambio de bienes,
servicios e información a través de redes de comunicación, comúnmente
Internet. La clasificación más básica del CE se hace en base al tipo de
entes que se relacionan: empresas (<em>businesses</em>, B), consumidores
(<em>consumers</em>, C) y entes públicos (<em>governments</em>, G). De esta forma, una
empresa de CE convencional suele ser B2B si vende a otras empresas, B2G
si su relación comercial es con administraciones o B2C si vende a consumidores finales.</p>
<p>En este caso, se puede considerar que la empresa Beauty eSheep lleva a
cabo un CE de tipo B2C. Su producto estrella es una crema hidratante
unisex, denominada internamente como “Crema Luxury”, con mucho éxito
entre su clientela. A partir de este producto inicial, la empresa ha ido
ofreciendo un catálogo de productos tanto de belleza como de
bienestar y salud.</p>
<p>Hace tiempo la empresa instauró una estrategia relacional, centrada en
los clientes, de tal manera que han ido recabando diversos datos sobre ellos, incluidas las distintas compras que han realizado.</p>
<p>Basándose en los datos recopilados para cada cliente, la empresa quiere
realizar una campaña para impulsar la venta de tensiómetros digitales.
La empresa tiene acceso a un <em>stock</em> de estos productos, muy flexible en cuanto a fechas de envío, y el precio de los tensiómetros es muy bueno, por lo que
se espera una buena rentabilidad en su venta.</p>
<p>Por tanto, en este proyecto hay que identificar el público objetivo
susceptible de comprar dicho producto para ofrecérselo a través de la
plataforma de CE de la compañía, SMS y/o webmail durante el periodo que
dura la campaña.</p>
<p>La tabla con los datos integrados a nivel de cliente, incluyendo el
consumo de los distintos productos de la empresa, es <code>dp_ENTR</code>,
incluida en el paquete <code>CDR</code>, y se resume en la Tabla
<a href="cap-arboles.html#tab:dpentr">24.8</a>. Este ejemplo se va a replicar en el resto de
capítulos de <em>machine learning</em> supervisado para clasificación.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:dpentr">Tabla 24.8: </span> Descripción de las variables del conjunto de datos <code>dp_entr</code>
</caption>
<colgroup>
<col width="18%">
<col width="9%">
<col width="72%">
</colgroup>
<thead><tr class="header">
<th>VARIABLE</th>
<th>TIPO<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Doble: puede tomar todos los valores en la recta numérica real; es decir, son variables numéricas que pueden tener decimales. Entero: variables numéricas que pueden tomar valores negativos y positivos pero que no tienen decimales.&lt;/p&gt;"><sup>187</sup></a>
</th>
<th>DESCRIPCIÓN</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><code>CLS_PRO_pro13</code></td>
<td>Factor</td>
<td>Clase objetivo, es un indicador de si el cliente es consumidor de ese producto “tensiómetro digital”. Sí (‘S’) o no (‘N’)</td>
</tr>
<tr class="even">
<td><code>ind_pro11</code></td>
<td>Factor</td>
<td>Indicador de si el cliente es consumidor del producto “fragancia luxury”. Sí (‘S’) o no (‘N’)</td>
</tr>
<tr class="odd">
<td><code>ind_pro12</code></td>
<td>Factor</td>
<td>Indicador de si el cliente es consumidor del producto “depiladora eléctrica”. Sí (‘S’) o no (‘N’)</td>
</tr>
<tr class="even">
<td><code>ind_pro14</code></td>
<td>Factor</td>
<td>Indicador de si el cliente es consumidor del producto “crema luxury”. Sí (‘S’) o no (‘N’)</td>
</tr>
<tr class="odd">
<td><code>ind_pro15</code></td>
<td>Factor</td>
<td>Indicador de si el cliente es consumidor del producto “smartwatch fitness”. Sí (‘S’) o no (‘N’)</td>
</tr>
<tr class="even">
<td><code>ind_pro16</code></td>
<td>Factor</td>
<td>Indicador de si el cliente es consumidor del producto “kit pesas inteligentes”. Sí (‘S’) o no (‘N’)</td>
</tr>
<tr class="odd">
<td><code>ind_pro17</code></td>
<td>Factor</td>
<td>Indicador de si el cliente es consumidor del producto “estimulador muscular”. Sí (‘S’) o no (‘N’)</td>
</tr>
<tr class="even">
<td><code>importe_pro11</code></td>
<td>Doble</td>
<td>Importe neto global gastado por el cliente en ese producto, en euros</td>
</tr>
<tr class="odd">
<td><code>importe_pro12</code></td>
<td>Doble</td>
<td>Importe neto global gastado por el cliente en ese producto, en euros</td>
</tr>
<tr class="even">
<td><code>importe_pro14</code></td>
<td>Doble</td>
<td>Importe neto global gastado por el cliente en ese producto, en euros</td>
</tr>
<tr class="odd">
<td><code>importe_pro15</code></td>
<td>Doble</td>
<td>Importe neto global gastado por el cliente en ese producto, en euros</td>
</tr>
<tr class="even">
<td><code>importe_pro16</code></td>
<td>Doble</td>
<td>Importe neto global gastado por el cliente en ese producto, en euros</td>
</tr>
<tr class="odd">
<td><code>importe_pro17</code></td>
<td>Doble</td>
<td>Importe neto global gastado por el cliente en ese producto, en euros</td>
</tr>
<tr class="even">
<td><code>edad</code></td>
<td>Entero</td>
<td>Edad del cliente</td>
</tr>
<tr class="odd">
<td><code>tamano_fam</code></td>
<td>Entero</td>
<td>Número de miembros de la unidad familiar a la que pertenece el cliente, él incluido</td>
</tr>
<tr class="even">
<td><code>anos_exp</code></td>
<td>Entero</td>
<td>Años de trabajo del cliente</td>
</tr>
<tr class="odd">
<td><code>ingresos_ano</code></td>
<td>Doble</td>
<td>Ingresos anuales del cliente, en euros</td>
</tr>
<tr class="even">
<td><code>des_nivel_edu</code></td>
<td>Factor</td>
<td>Descripción del nivel de educación del cliente</td>
</tr>
</tbody>
</table></div>
<p>
A partir del conjunto de entrenamiento, se construye un árbol de clasificación, tal y como se ha expuesto anteriormente, sin
transformar (en su escala original) mediante el algoritmo CART
implementado en el paquete <code>rpart</code> con árboles de regresión y partición
recursiva (<em>recursive partitioning and regression trees</em>, RPART), que se
puede usar tanto para regresión como para clasificación.</p>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb344-1"><a href="cap-arboles.html#cb344-1" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"CDR"</span>)</span>
<span id="cb344-2"><a href="cap-arboles.html#cb344-2" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"reshape"</span>)</span>
<span id="cb344-3"><a href="cap-arboles.html#cb344-3" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"caret"</span>)</span>
<span id="cb344-4"><a href="cap-arboles.html#cb344-4" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"rpart"</span>)</span>
<span id="cb344-5"><a href="cap-arboles.html#cb344-5" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"rpart.plot"</span>)</span>
<span id="cb344-6"><a href="cap-arboles.html#cb344-6" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"ggplot2"</span>)</span>
<span id="cb344-7"><a href="cap-arboles.html#cb344-7" tabindex="-1"></a></span>
<span id="cb344-8"><a href="cap-arboles.html#cb344-8" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"dp_entr"</span>)</span>
<span id="cb344-9"><a href="cap-arboles.html#cb344-9" tabindex="-1"></a><span class="fu">head</span>(dp_entr)</span>
<span id="cb344-10"><a href="cap-arboles.html#cb344-10" tabindex="-1"></a>    ind_pro11 ind_pro12 ind_pro14 ind_pro15 ind_pro16 ind_pro17 importe_pro11</span>
<span id="cb344-11"><a href="cap-arboles.html#cb344-11" tabindex="-1"></a><span class="dv">1</span>           S         N         S         S         S         N           <span class="dv">157</span></span>
<span id="cb344-12"><a href="cap-arboles.html#cb344-12" tabindex="-1"></a><span class="dv">497</span>         N         N         S         N         S         N             <span class="dv">0</span></span>
<span id="cb344-13"><a href="cap-arboles.html#cb344-13" tabindex="-1"></a><span class="dv">265</span>         N         N         S         S         S         S             <span class="dv">0</span></span>
<span id="cb344-14"><a href="cap-arboles.html#cb344-14" tabindex="-1"></a><span class="dv">534</span>         N         S         S         N         N         N             <span class="dv">0</span></span>
<span id="cb344-15"><a href="cap-arboles.html#cb344-15" tabindex="-1"></a><span class="dv">415</span>         N         S         S         N         S         N             <span class="dv">0</span></span>
<span id="cb344-16"><a href="cap-arboles.html#cb344-16" tabindex="-1"></a><span class="dv">298</span>         S         N         S         N         N         N           <span class="dv">115</span></span>
<span id="cb344-17"><a href="cap-arboles.html#cb344-17" tabindex="-1"></a>    importe_pro12 importe_pro14 importe_pro15 importe_pro16 importe_pro17 edad</span>
<span id="cb344-18"><a href="cap-arboles.html#cb344-18" tabindex="-1"></a><span class="dv">1</span>               <span class="dv">0</span>            <span class="dv">40</span>           <span class="dv">200</span>           <span class="dv">180</span>             <span class="dv">0</span>   <span class="dv">49</span></span>
<span id="cb344-19"><a href="cap-arboles.html#cb344-19" tabindex="-1"></a><span class="dv">497</span>             <span class="dv">0</span>           <span class="dv">240</span>             <span class="dv">0</span>           <span class="dv">180</span>             <span class="dv">0</span>   <span class="dv">38</span></span>
<span id="cb344-20"><a href="cap-arboles.html#cb344-20" tabindex="-1"></a><span class="dv">265</span>             <span class="dv">0</span>           <span class="dv">425</span>           <span class="dv">200</span>           <span class="dv">180</span>           <span class="dv">300</span>   <span class="dv">61</span></span>
<span id="cb344-21"><a href="cap-arboles.html#cb344-21" tabindex="-1"></a><span class="dv">534</span>           <span class="dv">120</span>            <span class="dv">60</span>             <span class="dv">0</span>             <span class="dv">0</span>             <span class="dv">0</span>   <span class="dv">47</span></span>
<span id="cb344-22"><a href="cap-arboles.html#cb344-22" tabindex="-1"></a><span class="dv">415</span>           <span class="dv">120</span>           <span class="dv">133</span>             <span class="dv">0</span>           <span class="dv">180</span>             <span class="dv">0</span>   <span class="dv">34</span></span>
<span id="cb344-23"><a href="cap-arboles.html#cb344-23" tabindex="-1"></a><span class="dv">298</span>             <span class="dv">0</span>           <span class="dv">220</span>             <span class="dv">0</span>             <span class="dv">0</span>             <span class="dv">0</span>   <span class="dv">43</span></span>
<span id="cb344-24"><a href="cap-arboles.html#cb344-24" tabindex="-1"></a>    tamano_fam anos_exp ingresos_ano des_nivel_edu CLS_PRO_pro13</span>
<span id="cb344-25"><a href="cap-arboles.html#cb344-25" tabindex="-1"></a><span class="dv">1</span>            <span class="dv">4</span>       <span class="dv">24</span>        <span class="dv">30000</span>         MEDIO             S</span>
<span id="cb344-26"><a href="cap-arboles.html#cb344-26" tabindex="-1"></a><span class="dv">497</span>          <span class="dv">2</span>       <span class="dv">12</span>        <span class="dv">53000</span>         MEDIO             N</span>
<span id="cb344-27"><a href="cap-arboles.html#cb344-27" tabindex="-1"></a><span class="dv">265</span>          <span class="dv">4</span>       <span class="dv">37</span>       <span class="dv">172000</span>        BASICO             S</span>
<span id="cb344-28"><a href="cap-arboles.html#cb344-28" tabindex="-1"></a><span class="dv">534</span>          <span class="dv">3</span>       <span class="dv">21</span>        <span class="dv">38000</span>         MEDIO             N</span>
<span id="cb344-29"><a href="cap-arboles.html#cb344-29" tabindex="-1"></a><span class="dv">415</span>          <span class="dv">1</span>       <span class="dv">10</span>        <span class="dv">38000</span>        BASICO             N</span>
<span id="cb344-30"><a href="cap-arboles.html#cb344-30" tabindex="-1"></a><span class="dv">298</span>          <span class="dv">2</span>       <span class="dv">18</span>        <span class="dv">60000</span>          ALTO             N</span></code></pre></div>
<div class="sourceCode" id="cb345"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">trControl</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/trainControl.html">trainControl</a></span><span class="op">(</span></span>
<span>  method <span class="op">=</span> <span class="st">"cv"</span>,</span>
<span>  number <span class="op">=</span> <span class="fl">10</span>,</span>
<span>  classProbs <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  summaryFunction <span class="op">=</span> <span class="va">twoClassSummary</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>En primer lugar, se carga la librería necesaria para entrenar el modelo,
así como los datos de compras de los clientes. En este caso se usa el
método de remuestreo de validación cruzada con 10 grupos (10 <em>folds</em>), visto en el
Cap. <a href="chap-herramientas.html#chap-herramientas">10</a>. A continuación se determina la semilla
aleatoria, para que los resultados sean replicables, y se entrena el
modelo.</p>
<div class="sourceCode" id="cb346"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># se fija una semilla aleatoria</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">101</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># se entrena el modelo</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/train.html">train</a></span><span class="op">(</span><span class="va">CLS_PRO_pro13</span> <span class="op">~</span> <span class="va">.</span>,  <span class="co"># . equivale a incluir todas las variables</span></span>
<span>             data<span class="op">=</span><span class="va">dp_entr</span>,</span>
<span>             method<span class="op">=</span><span class="st">"rpart"</span>,</span>
<span>             metric<span class="op">=</span><span class="st">"ROC"</span>,</span>
<span>             trControl<span class="op">=</span><span class="va">trControl</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb347-1"><a href="cap-arboles.html#cb347-1" tabindex="-1"></a>model</span>
<span id="cb347-2"><a href="cap-arboles.html#cb347-2" tabindex="-1"></a></span>
<span id="cb347-3"><a href="cap-arboles.html#cb347-3" tabindex="-1"></a>CART</span>
<span id="cb347-4"><a href="cap-arboles.html#cb347-4" tabindex="-1"></a></span>
<span id="cb347-5"><a href="cap-arboles.html#cb347-5" tabindex="-1"></a><span class="dv">558</span> samples</span>
<span id="cb347-6"><a href="cap-arboles.html#cb347-6" tabindex="-1"></a> <span class="dv">17</span> predictor</span>
<span id="cb347-7"><a href="cap-arboles.html#cb347-7" tabindex="-1"></a>  <span class="dv">2</span> classes<span class="sc">:</span> <span class="st">'S'</span>, <span class="st">'N'</span></span>
<span id="cb347-8"><a href="cap-arboles.html#cb347-8" tabindex="-1"></a></span>
<span id="cb347-9"><a href="cap-arboles.html#cb347-9" tabindex="-1"></a>No pre<span class="sc">-</span>processing</span>
<span id="cb347-10"><a href="cap-arboles.html#cb347-10" tabindex="-1"></a>Resampling<span class="sc">:</span> Cross<span class="sc">-</span><span class="fu">Validated</span> (<span class="dv">10</span> fold)</span>
<span id="cb347-11"><a href="cap-arboles.html#cb347-11" tabindex="-1"></a>Summary of sample sizes<span class="sc">:</span> <span class="dv">502</span>, <span class="dv">502</span>, <span class="dv">502</span>, <span class="dv">503</span>, <span class="dv">503</span>, <span class="dv">502</span>, ...</span>
<span id="cb347-12"><a href="cap-arboles.html#cb347-12" tabindex="-1"></a>Resampling results across tuning parameters<span class="sc">:</span></span>
<span id="cb347-13"><a href="cap-arboles.html#cb347-13" tabindex="-1"></a></span>
<span id="cb347-14"><a href="cap-arboles.html#cb347-14" tabindex="-1"></a>  cp          ROC        Sens       Spec</span>
<span id="cb347-15"><a href="cap-arboles.html#cb347-15" tabindex="-1"></a>  <span class="fl">0.05017921</span>  <span class="fl">0.8172123</span>  <span class="fl">0.9214286</span>  <span class="fl">0.7026455</span></span>
<span id="cb347-16"><a href="cap-arboles.html#cb347-16" tabindex="-1"></a>  <span class="fl">0.10394265</span>  <span class="fl">0.7559406</span>  <span class="fl">0.8386243</span>  <span class="fl">0.6914021</span></span>
<span id="cb347-17"><a href="cap-arboles.html#cb347-17" tabindex="-1"></a>  <span class="fl">0.51971326</span>  <span class="fl">0.6347222</span>  <span class="fl">0.8564815</span>  <span class="fl">0.4129630</span></span>
<span id="cb347-18"><a href="cap-arboles.html#cb347-18" tabindex="-1"></a></span>
<span id="cb347-19"><a href="cap-arboles.html#cb347-19" tabindex="-1"></a>ROC was used to select the optimal model using the largest value.</span>
<span id="cb347-20"><a href="cap-arboles.html#cb347-20" tabindex="-1"></a>The final value used <span class="cf">for</span> the model was cp <span class="ot">=</span> <span class="fl">0.05017921</span>, where cp is the complexity paramenter.</span></code></pre></div>
<div class="sourceCode" id="cb348"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu">melt</span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">resample</span><span class="op">[</span>,<span class="op">-</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">variable</span>, y <span class="op">=</span> <span class="va">value</span>, fill<span class="op">=</span><span class="va">variable</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_boxplot.html">geom_boxplot</a></span><span class="op">(</span>show.legend<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:006-002-001RPARTRESULTS"></span>
<img src="img/notune_rpart_boxplot.png" alt="Resultados del modelo durante la validación cruzada." width="40%"><p class="caption">
Figura 24.4: Resultados del modelo durante la validación cruzada.
</p>
</div>
<p>Los resultados de validación cruzada quedan recogidos en los <em>box plots</em> de la Fig. <a href="cap-arboles.html#fig:006-002-001RPARTRESULTS">24.4</a>, por lo que se pueden ver los valores entre los que oscilan las principales medidas de bondad de clasificación en los 10 grupos del proceso de validación. Estas
medidas (ROC, sensibilidad y especificidad) se definieron en el Cap.
<a href="chap-herramientas.html#chap-herramientas">10</a>, y en el caso de árboles de clasificación se
utilizan para evaluar el modelo. El árbol generado se muestra en la Fig. <a href="cap-arboles.html#fig:006-002-001RPARTRESULTS2">24.5</a>. Se puede observar que este árbol es muy sencillo (téngase en cuenta que se seleccionan las mejores particiones y que las que no aparecen carecen de interés a efectos predictivos) y,
por tanto, fácil de interpretar. En primer lugar decide si
un cliente que compra el <em>smartchwatch fitness</em> comprará el nuevo
producto. En caso de no comprar el <em>smartchwatch fitness</em> (No a
<code>ind_pro15S=1</code>), pero sí la <em>depiladora eléctrica</em> (Yes a
<code>ind_pro12S=1</code>), comprará el <em>tensiómetro digital</em>. Si no compra ninguno
de esos dos productos no comprará el nuevo producto.</p>
<p>Se seleccionan las variables que generan las mejores particiones. Aquellas que no aparecen no generan particiones interesantes a nivel predictivo. Esto también se entiende con la importancia de variables explicada más abajo.</p>
<div class="sourceCode" id="cb349"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Gráfico del árbol obtenido</span></span>
<span><span class="fu">rpart.plot</span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">finalModel</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:006-002-001RPARTRESULTS2"></span>
<img src="img/notune_rpart_plot.png" alt="Árbol de clasificación sin ajuste automático de hiperparámetros." width="60%"><p class="caption">
Figura 24.5: Árbol de clasificación sin ajuste automático de hiperparámetros.
</p>
</div>
<p>Este modelo se puede mejorar ajustando automáticamente
el hiperparámetro<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Como se expuso en el Cap. &lt;a href="chap-herramientas.html#chap-herramientas"&gt;10&lt;/a&gt;, los hiperparámetros de un modelo son los valores de las configuraciones utilizadas durante el proceso de entrenamiento. A diferencia de los parámetros, son valores que no se obtienen a partir de los datos, sino que los propone el científico de datos. Podría decirse que son conjeturas (buenas conjeturas) realizadas sin utilizar las observaciones disponibles. Los hiperparámetros, a diferencia de los parámetros, se fijan antes del entrenamiento. Siendo más específicos, al entrenar un modelo de aprendizaje automático se fijan los valores de los hiperparámetros para que con estos se estimen los parámetros. Podría decirse que son los ajustes del modelo para que este pueda resolver de manera óptima el problema de aprendizaje automático. En conclusión, hiperparámetros y parámetros son conceptos bien diferentes.&lt;/p&gt;'><sup>188</sup></a> <a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Cuando no se realiza el ajuste automático del hiperparámetro, el paquete &lt;code&gt;rpart&lt;/code&gt; le asigna tres posibles valores de forma aleatoria: uno bajo, uno medio y uno alto.&lt;/p&gt;"><sup>189</sup></a>
incluido en <code>rpart</code> para el entrenamiento de árboles de decisión. Por consiguiente, primero
es necesario conocer el hiperparámetro a optimizar en el algoritmo
implementado en <strong>R</strong> que se esté utilizando. Esto se consigue mediante la
siguiente instrucción, incluida en el paquete <code>caret</code>:</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb350-1"><a href="cap-arboles.html#cb350-1" tabindex="-1"></a><span class="fu">modelLookup</span>(<span class="st">"rpart"</span>)</span>
<span id="cb350-2"><a href="cap-arboles.html#cb350-2" tabindex="-1"></a></span>
<span id="cb350-3"><a href="cap-arboles.html#cb350-3" tabindex="-1"></a>  model parameter                label forReg forClass probModel</span>
<span id="cb350-4"><a href="cap-arboles.html#cb350-4" tabindex="-1"></a><span class="dv">1</span> rpart        cp Complexity Parameter   <span class="cn">TRUE</span>     <span class="cn">TRUE</span>      <span class="cn">TRUE</span></span></code></pre></div>
<p>El hiperparámetro a optimizar es la complejidad
del árbol, <span class="math inline">\(\zeta\)</span> (o <code>cp</code>), y se aplica en la fase de
parada durante la construcción del árbol. Como se avanzó anteriormente, esta
fase tiene como función principal evitar divisiones que no
valgan la pena. El hiperparámetro de complejidad <span class="math inline">\(\zeta\)</span> (o <code>cp</code> en los resultados) puede entenderse como un umbral que permite divisiones (<em>splits</em>) en cada nodo del modelo siempre y cuando haya una mejora mínima en las métricas. Es necesario definir los valores de dicho hiperparámetro que se
quieren evaluar con el objetivo de obtener su valor óptimo.</p>
<div class="sourceCode" id="cb351"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Se especifica un rango de valores típicos para el hiperparámetro</span></span>
<span><span class="va">tuneGrid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/expand.grid.html">expand.grid</a></span><span class="op">(</span>cp <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0.01</span>,<span class="fl">0.05</span>,<span class="fl">0.01</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb352"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># se entrena el modelo</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">101</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/train.html">train</a></span><span class="op">(</span><span class="va">CLS_PRO_pro13</span> <span class="op">~</span> <span class="va">.</span>,</span>
<span>             data<span class="op">=</span><span class="va">dp_entr</span>,</span>
<span>             method<span class="op">=</span><span class="st">"rpart"</span>,</span>
<span>             metric<span class="op">=</span><span class="st">"ROC"</span>,</span>
<span>             trControl<span class="op">=</span><span class="va">trControl</span>,</span>
<span>             tuneGrid<span class="op">=</span><span class="va">tuneGrid</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb353-1"><a href="cap-arboles.html#cb353-1" tabindex="-1"></a><span class="co"># se muestra la salida del modelo</span></span>
<span id="cb353-2"><a href="cap-arboles.html#cb353-2" tabindex="-1"></a>model</span>
<span id="cb353-3"><a href="cap-arboles.html#cb353-3" tabindex="-1"></a></span>
<span id="cb353-4"><a href="cap-arboles.html#cb353-4" tabindex="-1"></a>CART</span>
<span id="cb353-5"><a href="cap-arboles.html#cb353-5" tabindex="-1"></a></span>
<span id="cb353-6"><a href="cap-arboles.html#cb353-6" tabindex="-1"></a><span class="dv">558</span> samples</span>
<span id="cb353-7"><a href="cap-arboles.html#cb353-7" tabindex="-1"></a> <span class="dv">17</span> predictor</span>
<span id="cb353-8"><a href="cap-arboles.html#cb353-8" tabindex="-1"></a>  <span class="dv">2</span> classes<span class="sc">:</span> <span class="st">'S'</span>, <span class="st">'N'</span></span>
<span id="cb353-9"><a href="cap-arboles.html#cb353-9" tabindex="-1"></a></span>
<span id="cb353-10"><a href="cap-arboles.html#cb353-10" tabindex="-1"></a>No pre<span class="sc">-</span>processing</span>
<span id="cb353-11"><a href="cap-arboles.html#cb353-11" tabindex="-1"></a>Resampling<span class="sc">:</span> Cross<span class="sc">-</span><span class="fu">Validated</span> (<span class="dv">10</span> fold)</span>
<span id="cb353-12"><a href="cap-arboles.html#cb353-12" tabindex="-1"></a>Summary of sample sizes<span class="sc">:</span> <span class="dv">502</span>, <span class="dv">502</span>, <span class="dv">502</span>, <span class="dv">503</span>, <span class="dv">503</span>, <span class="dv">502</span>, ...</span>
<span id="cb353-13"><a href="cap-arboles.html#cb353-13" tabindex="-1"></a>Resampling results across tuning parameters<span class="sc">:</span></span>
<span id="cb353-14"><a href="cap-arboles.html#cb353-14" tabindex="-1"></a></span>
<span id="cb353-15"><a href="cap-arboles.html#cb353-15" tabindex="-1"></a>  cp    ROC        Sens       Spec</span>
<span id="cb353-16"><a href="cap-arboles.html#cb353-16" tabindex="-1"></a>  <span class="fl">0.01</span>  <span class="fl">0.8962254</span>  <span class="fl">0.8678571</span>  <span class="fl">0.8167989</span></span>
<span id="cb353-17"><a href="cap-arboles.html#cb353-17" tabindex="-1"></a>  <span class="fl">0.02</span>  <span class="fl">0.8663454</span>  <span class="fl">0.9000000</span>  <span class="fl">0.7667989</span></span>
<span id="cb353-18"><a href="cap-arboles.html#cb353-18" tabindex="-1"></a>  <span class="fl">0.03</span>  <span class="fl">0.8458097</span>  <span class="fl">0.9392857</span>  <span class="fl">0.7310847</span></span>
<span id="cb353-19"><a href="cap-arboles.html#cb353-19" tabindex="-1"></a>  <span class="fl">0.04</span>  <span class="fl">0.8449381</span>  <span class="fl">0.9214286</span>  <span class="fl">0.7383598</span></span>
<span id="cb353-20"><a href="cap-arboles.html#cb353-20" tabindex="-1"></a>  <span class="fl">0.05</span>  <span class="fl">0.8172123</span>  <span class="fl">0.9214286</span>  <span class="fl">0.7026455</span></span>
<span id="cb353-21"><a href="cap-arboles.html#cb353-21" tabindex="-1"></a></span>
<span id="cb353-22"><a href="cap-arboles.html#cb353-22" tabindex="-1"></a>ROC was used to select the optimal model using the largest value.</span>
<span id="cb353-23"><a href="cap-arboles.html#cb353-23" tabindex="-1"></a>The final value used <span class="cf">for</span> the model was cp <span class="ot">=</span> <span class="dv">0</span>.<span class="fl">01.</span></span></code></pre></div>
<p>De forma automática se construyen diversos árboles para cada uno de los
valores explicitados del parámetro de complejdad, <span class="math inline">\(\zeta\)</span> (denominado <code>cp</code>en los resultados). Para cada uno de esos árboles se obtienen las siguientes correspondientes métricas: el área bajo la
curva ROC (denotada así por las siglas en inglés de <em>receiver operating
characteristic</em>), sensibilidad (Sens) y especificidad (Spec), todas ellas
definidas en el Cap. <a href="chap-herramientas.html#chap-herramientas">10</a>. El valor ROC<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;La curva ROC es la que se obtiene al graficar la sensibilidad (tasa de verdaderos positivos) frente a la tasa de falsos positivos (también denominada 1-especificidad). Cuanto más grande sea el área bajo la curva ROC, mayor será la precisión obtenida.&lt;/p&gt;"><sup>190</sup></a> es el utilizado
para la elección del valor óptimo del hiperparámetro de complejidad, por lo que se determina que
finalmente el óptimo es <span class="math inline">\(cp= \text {0,01}\)</span>, puesto que en ese caso el área bajo la curva ROC alcanza el valor máximo: 89,6%. Por tanto, ajustando el hiperparámetro se ha aumentado la
precisión del modelo en casi 8 puntos porcentuales respecto al 81,7% que tenía el modelo
sin ajuste automático de <code>cp</code>.</p>
<p>En la Fig. <a href="cap-arboles.html#fig:006-002-003RPARTRESULTS1">24.6</a> se puede ver el
rendimiento de cada una de las métricas del árbol entrenado utilizando
validación cruzada. Dicha figura se obtiene con la siguiente instrucción:</p>
<div class="sourceCode" id="cb354"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu">melt</span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">resample</span><span class="op">[</span>,<span class="op">-</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">variable</span>, y <span class="op">=</span> <span class="va">value</span>, fill<span class="op">=</span><span class="va">variable</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>   <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_violin.html">geom_violin</a></span><span class="op">(</span>show.legend<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span></span>
<span>   <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span> <span class="op">+</span></span>
<span>   <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:006-002-003RPARTRESULTS1"></span>
<img src="img/tune_rpart_boxplot.png" alt="Resultados del modelo con ajuste automático durante la validación cruzada." width="40%"><p class="caption">
Figura 24.6: Resultados del modelo con ajuste automático durante la validación cruzada.
</p>
</div>
<p>En la Fig. <a href="cap-arboles.html#fig:006002003RPARTPLOT2">24.7</a> se muestra el árbol generado.
La visualización del árbol se obtiene con el siguiente código:</p>
<div class="sourceCode" id="cb355"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Gráfico del árbol obtenido</span></span>
<span><span class="fu">rpart.plot</span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">finalModel</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:006002003RPARTPLOT2"></span>
<img src="img/classbigtree.png" alt="Árbol de clasificación con ajuste automático." width="100%"><p class="caption">
Figura 24.7: Árbol de clasificación con ajuste automático.
</p>
</div>
<p>Con el objetivo de aumentar la generalidad del árbol y facilitar su
interpretación, se procede a reducir su tamaño podándolo. Para ello se
establece el criterio de que un nodo terminal tiene que tener, como mínimo, 50 observaciones. El árbol resultante se muestra en la Fig. <a href="cap-arboles.html#fig:PLOTCLASSPRUNEDTREE">24.8</a>.</p>
<div class="sourceCode" id="cb356"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">101</span><span class="op">)</span></span>
<span><span class="va">prunedtree</span> <span class="op">&lt;-</span> <span class="fu">rpart</span><span class="op">(</span><span class="va">CLS_PRO_pro13</span> <span class="op">~</span> <span class="va">.</span>, data<span class="op">=</span><span class="va">dp_entr</span>,</span>
<span>                    cp<span class="op">=</span> <span class="fl">0.01</span>, control <span class="op">=</span> <span class="fu">rpart.control</span><span class="op">(</span>minbucket <span class="op">=</span> <span class="fl">50</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">rpart.plot</span><span class="op">(</span><span class="va">prunedtree</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:PLOTCLASSPRUNEDTREE"></span>
<img src="img/pruned_class_tree.png" alt="Árbol de clasificación con ajuste automático y podado." width="60%"><p class="caption">
Figura 24.8: Árbol de clasificación con ajuste automático y podado.
</p>
</div>
<p>Como puede observarse, el número de nodos terminales del árbol se ha reducido considerablemente (de 12 a 5). En tres de ellos el árbol predice que un cliente comprará el nuevo producto si:</p>
<ol style="list-style-type: decimal">
<li><p>Compra el <em>smartwatch fitness</em> (<code>ind_pro15</code> = S - Yes) y la
<em>depiladora eléctrica</em> (<code>ind_pro12</code> = S - Yes).</p></li>
<li><p>Compra el <em>smartwatch fitness</em> (<code>ind_pro15</code> = S - Yes) y el
<em>estimulador muscular</em> (<code>ind_pro17</code> = S - Yes), pero no la <em>depiladora eléctrica</em> ( <code>ind_pro12</code> = S - No).</p></li>
<li><p>No compra el <em>smartwatch fitness</em> (<code>ind_pro15</code> = S - No), pero sí la
<em>depiladora eléctrica</em> (<code>ind_pro12</code> = S - Yes).</p></li>
</ol>
<p>Sin embargo, dos nodos terminales predicen que el cliente no comprará el
nuevo producto si:</p>
<ol style="list-style-type: decimal">
<li><p>Compra el <em>smartwatch fitness</em> (<code>ind_pro15</code> = S - Yes), pero no la
<em>depiladora eléctrica</em> (<code>ind_pro12</code> = S - No) ni el <em>estimulador muscular</em> (<code>ind_pro17</code> = S - No).</p></li>
<li><p>No compra el <em>smartwatch fitness</em> (<code>ind_pro15</code> = S - No) ni la
<em>depiladora eléctrica</em> (<code>ind_pro12</code> = S - No).</p></li>
</ol>
</div>
</div>
<div id="árboles-de-regresión" class="section level2" number="24.4">
<h2>
<span class="header-section-number">24.4</span> Árboles de regresión<a class="anchor" aria-label="anchor" href="#%C3%A1rboles-de-regresi%C3%B3n"><i class="fas fa-link"></i></a>
</h2>
<p></p>
<p>Los árboles de decisión también pueden usarse para resolver problemas de regresión. En este caso, la idea es que la
predicción sea un valor numérico en lugar de una categoría. En la Tabla <a href="cap-arboles.html#tab:dataregtree">24.9</a> se muestran los
datos para un problema de regresión equivalente al presentado en las
secciones anteriores para clasificación. La variable objetivo (<em>Horas jugadas</em>) ahora es continua en lugar de
categórica, como ocurría en el ejemplo de clasificación con la variable
<em>Decisión</em>.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:dataregtree">Tabla 24.9: </span> Datos de Horas jugadas dada la climatología del día</caption>
<thead><tr class="header">
<th>Día</th>
<th align="center">Tipo de día</th>
<th align="center">Humedad</th>
<th align="center">Viento</th>
<th align="center">Horas jugadas</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>1</td>
<td align="center">Soleado</td>
<td align="center">Fuerte</td>
<td align="center">Débil</td>
<td align="center">2,3</td>
</tr>
<tr class="even">
<td>2</td>
<td align="center">Soleado</td>
<td align="center">Fuerte</td>
<td align="center">Fuerte</td>
<td align="center">1,5</td>
</tr>
<tr class="odd">
<td>3</td>
<td align="center">Lluvia</td>
<td align="center">Fuerte</td>
<td align="center">Débil</td>
<td align="center">1,3</td>
</tr>
<tr class="even">
<td>4</td>
<td align="center">Nublado</td>
<td align="center">Fuerte</td>
<td align="center">Débil</td>
<td align="center">2,4</td>
</tr>
<tr class="odd">
<td>5</td>
<td align="center">Lluvia</td>
<td align="center">Débil</td>
<td align="center">Débil</td>
<td align="center">1,9</td>
</tr>
<tr class="even">
<td>6</td>
<td align="center">Lluvia</td>
<td align="center">Débil</td>
<td align="center">Fuerte</td>
<td align="center">2,4</td>
</tr>
<tr class="odd">
<td>7</td>
<td align="center">Soleado</td>
<td align="center">Fuerte</td>
<td align="center">Débil</td>
<td align="center">2,3</td>
</tr>
<tr class="even">
<td>8</td>
<td align="center">Nublado</td>
<td align="center">Débil</td>
<td align="center">Fuerte</td>
<td align="center">2,2</td>
</tr>
<tr class="odd">
<td>9</td>
<td align="center">Soleado</td>
<td align="center">Débil</td>
<td align="center">Débil</td>
<td align="center">1,3</td>
</tr>
<tr class="even">
<td>10</td>
<td align="center">Lluvia</td>
<td align="center">Débil</td>
<td align="center">Débil</td>
<td align="center">1,8</td>
</tr>
<tr class="odd">
<td>11</td>
<td align="center">Soleado</td>
<td align="center">Débil</td>
<td align="center">Fuerte</td>
<td align="center">1,2</td>
</tr>
<tr class="even">
<td>12</td>
<td align="center">Nublado</td>
<td align="center">Fuerte</td>
<td align="center">Fuerte</td>
<td align="center">2,9</td>
</tr>
<tr class="odd">
<td>13</td>
<td align="center">Nublado</td>
<td align="center">Débil</td>
<td align="center">Débil</td>
<td align="center">2,2</td>
</tr>
<tr class="even">
<td>14</td>
<td align="center">Lluvia</td>
<td align="center">Fuerte</td>
<td align="center">Fuerte</td>
<td align="center">1,5</td>
</tr>
<tr class="odd">
<td>15</td>
<td align="center">Soleado</td>
<td align="center">Fuerte</td>
<td align="center">Fuerte</td>
<td align="center">1,5</td>
</tr>
</tbody>
</table></div>
<p>Las principales medidas descriptivas de la variable respuesta (media, varianza, desviación típica y coeficiente de variación) son:</p>
<span class="math display" id="eq:mean-horas">\[\begin{equation}
\bar{x}_{\text{Horas jugadas}} = \frac{1}{n}\sum{x} = \text {1,91},
\tag{24.3}
\end{equation}\]</span>
<span class="math display" id="eq:varhoras">\[\begin{equation}
\sigma^{2}_{\text{Horas jugadas}} = \frac{\sum{(x-\bar{x}\Bigr)^{2}}}{n} = \text {0,25},
\tag{24.4}
\end{equation}\]</span>
<span class="math display" id="eq:sdhoras">\[\begin{equation}
\sigma_{\text{Horas jugadas}} = \sqrt{\sigma^{2}} = \text {0,50},
\tag{24.5}
\end{equation}\]</span>
<span class="math display" id="eq:cvhoras">\[\begin{equation}
CV_{\text{Horas jugadas}} = \frac{\sigma}{\bar{x}} = \text {0,26}.
\tag{24.6}
\end{equation}\]</span>
<p>De ellas, es de especial interés la desviación típica del conjunto de datos, pues en los árboles de regresión las divisiones que se harán en dicho conjunto conducirán a subconjuntos que, cada vez, tienen menor desviación típica.</p>
<div id="cómo-se-va-formando-el-árbol-de-regresión" class="section level3" number="24.4.1">
<h3>
<span class="header-section-number">24.4.1</span> ¿Cómo se va formando el árbol de regresión?<a class="anchor" aria-label="anchor" href="#c%C3%B3mo-se-va-formando-el-%C3%A1rbol-de-regresi%C3%B3n"><i class="fas fa-link"></i></a>
</h3>
<p></p>
<p>Mientras que en los árboles de clasificación se utiliza la entropía o
la impureza de Gini para medir la homogeneidad de un nodo, en los
árboles de regresión se utiliza como métrica la desviación típica
<span class="math inline">\((\sigma)\)</span> de la variable respuesta. Por tanto, cuando se selecciona una variable para hacer la
división, se calcula la desviación típica para cada una de las ramas, y
se obtiene una media ponderada en función del número de elementos de
cada una de ellas:</p>
<span class="math display" id="eq:sigmavar">\[\begin{equation}
\sigma_{X} = \sum_{r\in X}{P(r)\cdot\sigma_{r}},
\tag{24.7}
\end{equation}\]</span>
<p>donde <span class="math inline">\(X\)</span> es la variable de la cual se quiere obtener la desviación típica (también se podría hablar en términos de conjunto de datos), <span class="math inline">\(r\)</span> son las ramas y <span class="math inline">\(P(r)\)</span> es la probabilidad de seleccionar cada una de esas ramas, que se puede estimar mediante el cociente entre el número de observaciones de la rama y el total de las que se consideran en el nodo.</p>
<p>Siguiendo un proceso idéntico al que se ha mostrado en el caso de los árboles de clasificación, para los datos mostrados en la Tabla
<a href="cap-arboles.html#tab:dataregtree">24.9</a>), la desviación típica es 0,50 horas jugadas (véase ecuación <a href="cap-arboles.html#eq:sdhoras">(24.5)</a>, y entre las variables predictoras, <em>Tipo de día</em>, <em>Humedad</em> y <em>Viento</em>, se seleccionará como nodo raíz aquella que genere una partición del conjunto inicial más homogénea (con menos desviación típica ponderada).</p>
<p>Para cada una de las tres variables candidatas a nodo raíz, las desviaciones típicas en los subconjuntos correspondientes a cada una de sus ramas o categorías son las que se indican en las Tablas <a href="cap-arboles.html#tab:sd-tipodia">24.10</a>, <a href="cap-arboles.html#tab:sd-Humedad">24.11</a> y <a href="cap-arboles.html#tab:sd-Viento">24.12</a>:</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:sd-tipodia">Tabla 24.10: </span> Desviación típica en las ramas de la variable <em>Tipo
de día</em>
</caption>
<thead><tr class="header">
<th>Tipo de día</th>
<th align="center"># observaciones</th>
<th align="center"><span class="math inline">\(\sigma_{\text{Horas jugadas}}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Soleado</td>
<td align="center">6</td>
<td align="center">0,45</td>
</tr>
<tr class="even">
<td>Nublado</td>
<td align="center">4</td>
<td align="center">0,29</td>
</tr>
<tr class="odd">
<td>Lluvia</td>
<td align="center">5</td>
<td align="center">0,38</td>
</tr>
</tbody>
</table></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:sd-Humedad">Tabla 24.11: </span> Desviación típica en las ramas de la variable
<em>Humedad</em>
</caption>
<thead><tr class="header">
<th>Humedad</th>
<th align="center"># observaciones</th>
<th align="center"><span class="math inline">\(\sigma_{\text{Horas jugadas}}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Fuerte</td>
<td align="center">8</td>
<td align="center">0,55</td>
</tr>
<tr class="even">
<td>Débil</td>
<td align="center">7</td>
<td align="center">0,43</td>
</tr>
</tbody>
</table></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:sd-Viento">Tabla 24.12: </span> Desviación típica en las ramas de la variable
<em>Viento</em>
</caption>
<thead><tr class="header">
<th>Viento</th>
<th align="center"># observaciones</th>
<th align="center"><span class="math inline">\(\sigma_{\text{Horas jugadas}}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Fuerte</td>
<td align="center">7</td>
<td align="center">0,57</td>
</tr>
<tr class="even">
<td>Débil</td>
<td align="center">8</td>
<td align="center">0,42</td>
</tr>
</tbody>
</table></div>
<p>A partir de las desviaciones típicas en las ramas de cada variable, se obtiene la desviación típica (ponderada) de cada variable de acuerdo a la
ecuación <a href="cap-arboles.html#eq:sigmavar">(24.7)</a>.</p>
<p>La reducción que se opera en la desviación típica de los valores de la variable respuesta del conjunto inicial (diferencia entre la desviación de la variable
respuesta y la que se obtiene cuando se divide el conjunto de datos en base a
cada una de las variables) puede verse en la Tabla <a href="cap-arboles.html#tab:sdvars">24.13</a>.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:sdvars">Tabla 24.13: </span> Desviación típica y reducción de de la desviación para cada
variable</caption>
<thead><tr class="header">
<th>Variable</th>
<th align="center"><span class="math inline">\(\sigma_{\text{Horas jugadas}}\)</span></th>
<th align="center">Reducción</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Tipo de día</td>
<td align="center">0,38</td>
<td align="center">0,12</td>
</tr>
<tr class="even">
<td>Humedad</td>
<td align="center">0,49</td>
<td align="center">0,01</td>
</tr>
<tr class="odd">
<td>Viento</td>
<td align="center">0,49</td>
<td align="center">0,01</td>
</tr>
</tbody>
</table></div>
<p>Dado que la partición del conjunto de datos según la variable <em>Tipo de día</em> es la que produce una mayor reducción
en la desviación típica, dicha variable resulta elegida como nodo raíz.</p>
<p>A continuación, se procede a la partición de los subconjuntos de observaciones correspondientes a las ramas de la variable <em>Tipo de día</em>: <em>soleado</em>, <em>nublado</em> y <em>con lluvia</em> en función de <em>Humedad</em> o <em>Viento</em>. Comenzando con <em>Tipo de día: soleado</em>, se tiene que la partición del subconjunto de datos correspondientes a días soleados según la <em>Humedad</em> da lugar a los resultados que se muestran en la Tabla <a href="cap-arboles.html#tab:sd-sol-Humedad">24.14</a>:</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:sd-sol-Humedad">Tabla 24.14: </span> Desviación típica en las ramas de la variable
<em>Humedad</em> en días soleados</caption>
<thead><tr class="header">
<th>Humedad</th>
<th align="center"># observaciones</th>
<th align="center"><span class="math inline">\(\sigma_{\text{Horas jugadas}}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Fuerte</td>
<td align="center">4</td>
<td align="center">0,4</td>
</tr>
<tr class="even">
<td>Débil</td>
<td align="center">2</td>
<td align="center">0,05</td>
</tr>
</tbody>
</table></div>
<p>Si la variable de partición fuese la fuerza del <em>Viento</em>, entonces se tendrían los resultados que figuran en la Tabla <a href="cap-arboles.html#tab:sd-sol-Viento">24.15</a>:</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:sd-sol-Viento">Tabla 24.15: </span> Desviación típica en las ramas de la variable
<em>Viento</em> en días soleados</caption>
<thead><tr class="header">
<th>Viento</th>
<th align="center"># observaciones</th>
<th align="center"><span class="math inline">\(\sigma_{\text{Horas jugadas}}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Fuerte</td>
<td align="center">3</td>
<td align="center">0,14</td>
</tr>
<tr class="even">
<td>Débil</td>
<td align="center">3</td>
<td align="center">0,47</td>
</tr>
</tbody>
</table></div>
<p>En la Tabla <a href="cap-arboles.html#tab:sdvarssol">24.16</a> se muestra la desviación típica para
cada variable (obtenida como media ponderada de las desviaciones típicas correspondientes a sus ramas), así como la reducción de desviación que produce. Como la mayor reducción se produce en <em>Humedad</em>, la siguiente división se realizaría en función de las categorías de esta variable.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:sdvarssol">Tabla 24.16: </span> Desviación típica y decremento de desviación de <em>Humedad</em> y <em>Viento</em> en la rama <em>Soleado</em>
</caption>
<thead><tr class="header">
<th>Variable</th>
<th align="center"><span class="math inline">\(\sigma_{\text{Horas jugadas}}\)</span></th>
<th align="center">Decremento</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Humedad</td>
<td align="center">0,28</td>
<td align="center">0,17</td>
</tr>
<tr class="even">
<td>Viento</td>
<td align="center">0,31</td>
<td align="center">0,14</td>
</tr>
</tbody>
</table></div>
<p>De igual manera se procedería en el caso de días nublados o lluviosos. En el caso de días nublados la desviación típica (ponderada) del número de horas en los subconjuntos de datos que genera la partición por <em>Humedad</em> es 0,2208, mientras la de los subconjuntos que genera la partición por fuerza del <em>Viento</em> es 0,2775 (se dejan al lector estos cálculos), por lo cual la reducción en desviación típica (o el aumento de homogenidad en los valores de la variable respuesta) es mayor cuando se particiona por <em>Humedad</em>. En el caso de los días lluviosos, la partición que da lugar a mayor homogeneidad en los subconjuntos que genera también es la que se lleva a cabo mediante la variable <em>Humedad</em> (la desviación típica ponderada en los subconjuntos de las ramas de <em>Humedad</em> es 0,1975, mientras que la de los subconjuntos generados al particionar por la fuerza del <em>Viento</em> es 0,3375; también se dejan al lector estos cálculos.</p>
</div>
<div id="cuánto-debe-crecer-el-árbol-de-regresión" class="section level3" number="24.4.2">
<h3>
<span class="header-section-number">24.4.2</span> ¿Cuánto debe crecer el árbol de regresión?<a class="anchor" aria-label="anchor" href="#cu%C3%A1nto-debe-crecer-el-%C3%A1rbol-de-regresi%C3%B3n"><i class="fas fa-link"></i></a>
</h3>
<p></p>
<p>Como en el caso de los árboles de clasificación, es necesario establecer
reglas que pongan fin al proceso de crecimiento del árbol. Además de los
criterios de parada que se utilizan en árboles de clasificación (número
de elementos mínimos en un nodo y número de niveles máximos en la estructura del árbol), en los árboles de regresión se detiene su crecimiento estableciendo un <em>threshold</em> (umbral
de decisión) sobre el coeficiente de variación del nodo. En el ejemplo
expuesto en la Tabla <a href="cap-arboles.html#tab:cv-nodos">24.17</a> sobre <em>Horas jugadas</em>, se puede ver qué nodos podrían seguir
creciendo si se establece que el árbol continúe creciendo en nodos con
un coeficiente de variación de un 15% o más y con al menos 5
observaciones.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:cv-nodos">Tabla 24.17: </span> Medidas para decidir si el árbol sigue creciendo</caption>
<thead><tr class="header">
<th>Nodo padre</th>
<th align="center">Rama</th>
<th align="center">CV en nodo hijo</th>
<th align="center"># observaciones</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Tipo de día</td>
<td align="center">Nublado</td>
<td align="center">11,80%</td>
<td align="center">4</td>
</tr>
<tr class="even">
<td>Tipo de día</td>
<td align="center">Lluvia</td>
<td align="center">21,14%</td>
<td align="center">5</td>
</tr>
<tr class="odd">
<td>Humedad</td>
<td align="center">Fuerte</td>
<td align="center">21,04%</td>
<td align="center">4</td>
</tr>
<tr class="even">
<td>Humedad</td>
<td align="center">Débil</td>
<td align="center">4,04%</td>
<td align="center">2</td>
</tr>
</tbody>
</table></div>
<p>En este ejemplo, el árbol seguiría creciendo por la rama <em>Lluvia</em>, donde
habría que seleccionar la siguiente variable de división. En el resto de
ramas no se supera el número mínimo establecido de observaciones en el
nodo y en <em>Nublado</em> y <em>Débil</em> tampoco se alcanza el coeficiente de variación
mínimo. Por otra parte, en los árboles de regresión la poda se lleva a
cabo del mismo modo que para árboles de clasificación. Igual que en la ecuación <a href="cap-arboles.html#eq:poda">(24.2)</a>, el error de entrenamiento se mide a través de la suma
de los cuadrados de los errores (en inglés <em>sum of squared (estimated)
errors</em>, SSE), es decir:</p>
<span class="math display" id="eq:regpoda">\[\begin{equation}
SSE_{\zeta}(\tau) = SSE(\tau) + \zeta|\tau|.
\tag{24.8}
\end{equation}\]</span>
</div>
<div id="árbol-de-regresión-para-estimar-el-número-de-días-de-hospitalización" class="section level3" number="24.4.3">
<h3>
<span class="header-section-number">24.4.3</span> Árbol de regresión para estimar el número de días de hospitalización<a class="anchor" aria-label="anchor" href="#%C3%A1rbol-de-regresi%C3%B3n-para-estimar-el-n%C3%BAmero-de-d%C3%ADas-de-hospitalizaci%C3%B3n"><i class="fas fa-link"></i></a>
</h3>
<p>En este ejemplo se utilizan los datos <code>cleveland</code>, incluidos en el
paquete <code>CDR</code>, y que han sido utilizados en el Cap. <a href="cap-glm.html#cap-glm">16</a> para
estimar la variable <em>Días de hospitalización</em>, <code>dhosp</code>. El conjunto de datos contiene información sobre pacientes que llegan a un hospital con dolor de pecho y de los
cuales se han recogido distintas características. Se pretende predecir
el número de días de hospitalización que necesitará un paciente en base
al resto de características observadas: si
el paciente está diagnosticado de accidente coronario o no, su edad, su sexo,
el tipo de dolor que padece y la depresión en el segmento ST inducida
por ejercicio en relación con el reposo.</p>
<div class="sourceCode" id="cb357"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># se cargan los datos</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"cleveland"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># se entrena el modelo</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">101</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">rpart</span><span class="op">(</span><span class="va">dhosp</span> <span class="op">~</span> <span class="va">diag</span> <span class="op">+</span> <span class="va">edad</span> <span class="op">+</span> <span class="va">sexo</span> <span class="op">+</span> <span class="va">tdolor</span> <span class="op">+</span> <span class="va">dep</span>,</span>
<span>               data<span class="op">=</span><span class="va">cleveland</span>, method<span class="op">=</span><span class="st">"anova"</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb358-1"><a href="cap-arboles.html#cb358-1" tabindex="-1"></a>model<span class="sc">$</span>cptable</span>
<span id="cb358-2"><a href="cap-arboles.html#cb358-2" tabindex="-1"></a></span>
<span id="cb358-3"><a href="cap-arboles.html#cb358-3" tabindex="-1"></a>          CP nsplit rel error    xerror       xstd</span>
<span id="cb358-4"><a href="cap-arboles.html#cb358-4" tabindex="-1"></a><span class="dv">1</span> <span class="fl">0.37275022</span>      <span class="dv">0</span> <span class="fl">1.0000000</span> <span class="fl">1.0128283</span> <span class="fl">0.09213359</span></span>
<span id="cb358-5"><a href="cap-arboles.html#cb358-5" tabindex="-1"></a><span class="dv">2</span> <span class="fl">0.01674747</span>      <span class="dv">1</span> <span class="fl">0.6272498</span> <span class="fl">0.6427926</span> <span class="fl">0.06048143</span></span>
<span id="cb358-6"><a href="cap-arboles.html#cb358-6" tabindex="-1"></a><span class="dv">3</span> <span class="fl">0.01132433</span>      <span class="dv">4</span> <span class="fl">0.5770074</span> <span class="fl">0.6788431</span> <span class="fl">0.06681871</span></span>
<span id="cb358-7"><a href="cap-arboles.html#cb358-7" tabindex="-1"></a><span class="dv">4</span> <span class="fl">0.01007684</span>      <span class="dv">6</span> <span class="fl">0.5543587</span> <span class="fl">0.6825792</span> <span class="fl">0.06505426</span></span>
<span id="cb358-8"><a href="cap-arboles.html#cb358-8" tabindex="-1"></a><span class="dv">5</span> <span class="fl">0.01000000</span>      <span class="dv">7</span> <span class="fl">0.5442819</span> <span class="fl">0.6843192</span> <span class="fl">0.06514439</span></span></code></pre></div>
<p>En la salida anterior <code>xerror</code> es el error de validación cruzada, <code>rel error</code> es el <code>xerror</code> reescalado para que el valor máximo sea 1 y <code>xstd</code> es la desviación típica del error de la validación cruzada.</p>
<p>Se observa que para valores muy altos del hiperparámetro de complejidad (<code>cp</code>en los resultados),
SSE es muy elevado. Esto es, produce modelos muy sencillos pero con
nula potencia predictiva. En el otro extremo, para <span class="math inline">\(\zeta=\text {0,01}\)</span>, SSE
se reduce hasta llegar a SSE = 0,54 por lo que el árbol se poda de
acuerdo a la ecuación <a href="cap-arboles.html#eq:regpoda">(24.8)</a> con dicho valor de <span class="math inline">\(\zeta\)</span>. El
resultado del modelo se muestra en el árbol de la Fig.
<a href="cap-arboles.html#fig:dhosp-plot">24.9</a>. La interpretación de este árbol sería:</p>
<ol style="list-style-type: decimal">
<li><p>Si el paciente no tiene diagnóstico de accidente coronario, solo
necesitará un día de hospitalización.</p></li>
<li><p>En el caso de tener este diagnóstico y una depresión mayor o igual
a dos en el segmento ST inducida por ejercicio en relación al
reposo, necesitará 2,8 días de hospitalización.</p></li>
<li><p>En un último ejemplo, si la depresión en el segmento ST inducida por
ejercicio en relación con el reposo está entre 0,35 y 2, entonces el
paciente necesitará 3,8 días de hospitalización. Si, por el
contrario, la depresión en el segmento ST inducida por ejercicio en
relación con el reposo es menor a 0,35, el número de días de
hospitalización depende del sexo del paciente: los hombres
necesitarán 3,2 días y las mujeres tan solo 1,9 días.</p></li>
</ol>
<div class="sourceCode" id="cb359"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># se pinta el árbol obtenido</span></span>
<span><span class="fu">rpart.plot</span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:dhosp-plot"></span>
<img src="img/regtree_dhosp.png" alt="Árbol de regresión para predecir el número de días de hospitalización." width="55%"><p class="caption">
Figura 24.9: Árbol de regresión para predecir el número de días de hospitalización.
</p>
</div>
</div>
<div id="árbol-de-regresión-para-la-predicción-del-precio-unitario-de-la-vivienda-en-madrid" class="section level3" number="24.4.4">
<h3>
<span class="header-section-number">24.4.4</span> Árbol de regresión para la predicción del precio unitario de la vivienda en Madrid<a class="anchor" aria-label="anchor" href="#%C3%A1rbol-de-regresi%C3%B3n-para-la-predicci%C3%B3n-del-precio-unitario-de-la-vivienda-en-madrid"><i class="fas fa-link"></i></a>
</h3>
<p>En este ejemplo se entrena un árbol de regresión para predecir el
precio unitario de la vivienda en Madrid. Para ello se utilizan
los datos de viviendas a la venta en Madrid publicados en Idealista
durante el año 2018. Estos datos están incluidos en el paquete
<code>idealista18</code>. Para facilitar la interpretación del modelo, solo se van
a utilizar 8 de las variables incluidas en el conjunto de datos:
superficie construida, número de dormitorios, número de baños, si tiene
terraza, si tiene ascensor, si el precio incluye el parking, distancia
al centro de Madrid y distancia a una parada de metro.</p>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb360-1"><a href="cap-arboles.html#cb360-1" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"idealista18"</span>)</span>
<span id="cb360-2"><a href="cap-arboles.html#cb360-2" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"Madrid_Sale"</span>)</span>
<span id="cb360-3"><a href="cap-arboles.html#cb360-3" tabindex="-1"></a></span>
<span id="cb360-4"><a href="cap-arboles.html#cb360-4" tabindex="-1"></a>Madrid_Sale <span class="ot">&lt;-</span> Madrid_Sale <span class="sc">|&gt;</span></span>
<span id="cb360-5"><a href="cap-arboles.html#cb360-5" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(UNITPRICE, CONSTRUCTEDAREA, ROOMNUMBER, BATHNUMBER,</span>
<span id="cb360-6"><a href="cap-arboles.html#cb360-6" tabindex="-1"></a>         HASTERRACE,HASLIFT, ISPARKINGSPACEINCLUDEDINPRICE,</span>
<span id="cb360-7"><a href="cap-arboles.html#cb360-7" tabindex="-1"></a>         DISTANCE_TO_CITY_CENTER, DISTANCE_TO_METRO)</span>
<span id="cb360-8"><a href="cap-arboles.html#cb360-8" tabindex="-1"></a></span>
<span id="cb360-9"><a href="cap-arboles.html#cb360-9" tabindex="-1"></a><span class="fu">head</span>(Madrid_Sale)</span>
<span id="cb360-10"><a href="cap-arboles.html#cb360-10" tabindex="-1"></a></span>
<span id="cb360-11"><a href="cap-arboles.html#cb360-11" tabindex="-1"></a>  UNITPRICE CONSTRUCTEDAREA ROOMNUMBER BATHNUMBER HASTERRACE HASLIFT</span>
<span id="cb360-12"><a href="cap-arboles.html#cb360-12" tabindex="-1"></a><span class="dv">1</span>  <span class="fl">2680.851</span>              <span class="dv">47</span>          <span class="dv">1</span>          <span class="dv">1</span>          <span class="dv">0</span>       <span class="dv">1</span></span>
<span id="cb360-13"><a href="cap-arboles.html#cb360-13" tabindex="-1"></a><span class="dv">2</span>  <span class="fl">4351.852</span>              <span class="dv">54</span>          <span class="dv">1</span>          <span class="dv">1</span>          <span class="dv">0</span>       <span class="dv">0</span></span>
<span id="cb360-14"><a href="cap-arboles.html#cb360-14" tabindex="-1"></a><span class="dv">3</span>  <span class="fl">4973.333</span>              <span class="dv">75</span>          <span class="dv">2</span>          <span class="dv">1</span>          <span class="dv">0</span>       <span class="dv">0</span></span>
<span id="cb360-15"><a href="cap-arboles.html#cb360-15" tabindex="-1"></a><span class="dv">4</span>  <span class="fl">5916.667</span>              <span class="dv">48</span>          <span class="dv">1</span>          <span class="dv">1</span>          <span class="dv">0</span>       <span class="dv">1</span></span>
<span id="cb360-16"><a href="cap-arboles.html#cb360-16" tabindex="-1"></a><span class="dv">5</span>  <span class="fl">4560.000</span>              <span class="dv">50</span>          <span class="dv">0</span>          <span class="dv">1</span>          <span class="dv">0</span>       <span class="dv">0</span></span>
<span id="cb360-17"><a href="cap-arboles.html#cb360-17" tabindex="-1"></a><span class="dv">6</span>  <span class="fl">3921.260</span>             <span class="dv">127</span>          <span class="dv">3</span>          <span class="dv">2</span>          <span class="dv">0</span>       <span class="dv">1</span></span>
<span id="cb360-18"><a href="cap-arboles.html#cb360-18" tabindex="-1"></a>  ISPARKINGSPACEINCLUDEDINPRICE <span class="fu">DISTANCE_TO_CITY_CENTER</span> (km) <span class="fu">DISTANCE_TO_METRO</span> (Km)</span>
<span id="cb360-19"><a href="cap-arboles.html#cb360-19" tabindex="-1"></a><span class="dv">1</span>                             <span class="dv">0</span>               <span class="fl">8.0584293</span>         <span class="fl">0.8720746</span></span>
<span id="cb360-20"><a href="cap-arboles.html#cb360-20" tabindex="-1"></a><span class="dv">2</span>                             <span class="dv">0</span>               <span class="fl">0.8763693</span>         <span class="fl">0.1163821</span></span>
<span id="cb360-21"><a href="cap-arboles.html#cb360-21" tabindex="-1"></a><span class="dv">3</span>                             <span class="dv">0</span>               <span class="fl">0.9074793</span>         <span class="fl">0.1391088</span></span>
<span id="cb360-22"><a href="cap-arboles.html#cb360-22" tabindex="-1"></a><span class="dv">4</span>                             <span class="dv">0</span>               <span class="fl">0.8454622</span>         <span class="fl">0.1442990</span></span>
<span id="cb360-23"><a href="cap-arboles.html#cb360-23" tabindex="-1"></a><span class="dv">5</span>                             <span class="dv">0</span>               <span class="fl">1.2502313</span>         <span class="fl">0.3370982</span></span>
<span id="cb360-24"><a href="cap-arboles.html#cb360-24" tabindex="-1"></a><span class="dv">6</span>                             <span class="dv">0</span>               <span class="fl">0.5417727</span>         <span class="fl">0.1614363</span></span></code></pre></div>
<div class="sourceCode" id="cb361"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Se entrena el modelo</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/bethatkinson/rpart">"rpart"</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">101</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/rpart.html">rpart</a></span><span class="op">(</span><span class="va">UNITPRICE</span> <span class="op">~</span> <span class="va">.</span>, <span class="va">Madrid_Sale</span>, method <span class="op">=</span> <span class="st">"anova"</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb362-1"><a href="cap-arboles.html#cb362-1" tabindex="-1"></a><span class="co"># se muestra la salida del modelo</span></span>
<span id="cb362-2"><a href="cap-arboles.html#cb362-2" tabindex="-1"></a>model</span>
<span id="cb362-3"><a href="cap-arboles.html#cb362-3" tabindex="-1"></a></span>
<span id="cb362-4"><a href="cap-arboles.html#cb362-4" tabindex="-1"></a>n<span class="ot">=</span> <span class="dv">94815</span> </span>
<span id="cb362-5"><a href="cap-arboles.html#cb362-5" tabindex="-1"></a>node, split, n, deviance, yval</span>
<span id="cb362-6"><a href="cap-arboles.html#cb362-6" tabindex="-1"></a>      <span class="sc">*</span> denotes terminal node</span>
<span id="cb362-7"><a href="cap-arboles.html#cb362-7" tabindex="-1"></a></span>
<span id="cb362-8"><a href="cap-arboles.html#cb362-8" tabindex="-1"></a> <span class="dv">1</span><span class="er">)</span> root <span class="dv">94815</span> <span class="dv">274173500000</span> <span class="fl">3661.052</span>  </span>
<span id="cb362-9"><a href="cap-arboles.html#cb362-9" tabindex="-1"></a>   <span class="dv">2</span><span class="er">)</span> DISTANCE_TO_CITY_CENTER<span class="sc">&gt;=</span><span class="fl">3.201972</span> <span class="dv">59706</span> <span class="dv">102376700000</span> <span class="fl">2932.605</span>  </span>
<span id="cb362-10"><a href="cap-arboles.html#cb362-10" tabindex="-1"></a>     <span class="dv">4</span><span class="er">)</span> HASLIFT<span class="sc">&lt;</span> <span class="fl">0.5</span> <span class="dv">20515</span>  <span class="dv">16728050000</span> <span class="fl">2160.029</span> <span class="sc">*</span></span>
<span id="cb362-11"><a href="cap-arboles.html#cb362-11" tabindex="-1"></a>     <span class="dv">5</span><span class="er">)</span> HASLIFT<span class="sc">&gt;=</span><span class="fl">0.5</span> <span class="dv">39191</span>  <span class="dv">66994050000</span> <span class="fl">3337.019</span>  </span>
<span id="cb362-12"><a href="cap-arboles.html#cb362-12" tabindex="-1"></a>      <span class="dv">10</span><span class="er">)</span> BATHNUMBER<span class="sc">&lt;</span> <span class="fl">2.5</span> <span class="dv">34662</span>  <span class="dv">55189210000</span> <span class="fl">3212.614</span>  </span>
<span id="cb362-13"><a href="cap-arboles.html#cb362-13" tabindex="-1"></a>        <span class="dv">20</span><span class="er">)</span> ROOMNUMBER<span class="sc">&gt;=</span><span class="fl">1.5</span> <span class="dv">29068</span>  <span class="dv">40422630000</span> <span class="fl">3080.170</span> <span class="sc">*</span></span>
<span id="cb362-14"><a href="cap-arboles.html#cb362-14" tabindex="-1"></a>        <span class="dv">21</span><span class="er">)</span> ROOMNUMBER<span class="sc">&lt;</span> <span class="fl">1.5</span> <span class="dv">5594</span>  <span class="dv">11607120000</span> <span class="fl">3900.833</span> <span class="sc">*</span></span>
<span id="cb362-15"><a href="cap-arboles.html#cb362-15" tabindex="-1"></a>      <span class="dv">11</span><span class="er">)</span> BATHNUMBER<span class="sc">&gt;=</span><span class="fl">2.5</span> <span class="dv">4529</span>   <span class="dv">7162750000</span> <span class="fl">4289.133</span> <span class="sc">*</span></span>
<span id="cb362-16"><a href="cap-arboles.html#cb362-16" tabindex="-1"></a>   <span class="dv">3</span><span class="er">)</span> DISTANCE_TO_CITY_CENTER<span class="sc">&lt;</span> <span class="fl">3.201972</span> <span class="dv">35109</span>  <span class="dv">86236590000</span> <span class="fl">4899.840</span>  </span>
<span id="cb362-17"><a href="cap-arboles.html#cb362-17" tabindex="-1"></a>     <span class="dv">6</span><span class="er">)</span> DISTANCE_TO_METRO<span class="sc">&gt;=</span><span class="fl">0.4584401</span> <span class="dv">4749</span>   <span class="dv">9978473000</span> <span class="fl">3873.427</span> <span class="sc">*</span></span>
<span id="cb362-18"><a href="cap-arboles.html#cb362-18" tabindex="-1"></a>     <span class="dv">7</span><span class="er">)</span> DISTANCE_TO_METRO<span class="sc">&lt;</span> <span class="fl">0.4584401</span> <span class="dv">30360</span>  <span class="dv">70472330000</span> <span class="fl">5060.394</span>  </span>
<span id="cb362-19"><a href="cap-arboles.html#cb362-19" tabindex="-1"></a>      <span class="dv">14</span><span class="er">)</span> HASLIFT<span class="sc">&lt;</span> <span class="fl">0.5</span> <span class="dv">7293</span>  <span class="dv">12520500000</span> <span class="fl">4465.560</span> <span class="sc">*</span></span>
<span id="cb362-20"><a href="cap-arboles.html#cb362-20" tabindex="-1"></a>      <span class="dv">15</span><span class="er">)</span> HASLIFT<span class="sc">&gt;=</span><span class="fl">0.5</span> <span class="dv">23067</span>  <span class="dv">54555510000</span> <span class="fl">5248.461</span> <span class="sc">*</span></span></code></pre></div>
<p>Como en el ejemplo anterior, para <span class="math inline">\(\zeta=\text {0,01}\)</span> SSE se reduce hasta SSE <span class="math inline">\(=\text {0,56}\)</span>, por lo que el árbol se poda de acuerdo a la
ecuación <a href="cap-arboles.html#eq:regpoda">(24.8)</a> con dicho valor de <span class="math inline">\(\zeta\)</span>. El resultado del
modelo se muestra en el árbol de la Fig. <a href="cap-arboles.html#fig:idealista-treeplot">24.10</a>. La interpretación de este árbol sería:</p>
<ol style="list-style-type: decimal">
<li><p>Si una vivienda con ascensor se encuentra a menos de 3,2 km del
centro de Madrid y a menos de 0,46 km de una estación de metro, el
precio por metro cuadrado predicho para esa vivienda será de 5.248 €.</p></li>
<li><p>Si una vivienda se encuentra a más de 3,2 km del centro de Madrid y
no tiene ascensor, el precio predicho será de
2.160 € por metro cuadrado.</p></li>
<li><p>Si una vivienda se encuentra a menos de 3,2 km del centro de Madrid y
a más de 0,46 km de una estación de metro, el precio unitario
predicho para esa vivienda será de 3.873 €/<span class="math inline">\(\text {m}^{2}\)</span>.</p></li>
<li><p>Se deja al lector la interpretación de los casos donde, además, se tiene en cuenta el número de baños y de habitaciones.</p></li>
</ol>
<div class="sourceCode" id="cb363"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># se pinta el árbol obtenido</span></span>
<span><span class="fu">rpart.plot</span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:idealista-treeplot"></span>
<img src="img/regtree_idealista.png" alt="Árbol de regresión para predecir el precio por metro cuadrado de las viviendas en Madrid." width="60%"><p class="caption">
Figura 24.10: Árbol de regresión para predecir el precio por metro cuadrado de las viviendas en Madrid.
</p>
</div>
</div>
<div id="resumen-23" class="section level3 unnumbered infobox_resume">
<h3>Resumen<a class="anchor" aria-label="anchor" href="#resumen-23"><i class="fas fa-link"></i></a>
</h3>
<p>En este capítulo se introduce al lector en los árboles de decisión para
abordar problemas de clasificación y regresión. En particular:</p>
<ul>
<li><p>Se muestra la lógica de la construcción de árboles de decisión,
ya sean de regresión o clasificación.</p></li>
<li><p>Se contemplan diferentes medidas con las que el árbol decide avanzar
hacia un nuevo punto de decisión.</p></li>
<li><p>Se abordan los conceptos de sobreajuste y complejidad del árbol,
así como la forma de controlarlos.</p></li>
<li><p>Se muestra el uso de <strong>R</strong> para la clasificación en clases binarias y
para la predicción de variables respuesta numéricas en casos
aplicados.</p></li>
</ul>
</div>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="tablas-contingencia.html"><span class="header-section-number">23</span> Análisis de tablas de contingencia</a></div>
<div class="next"><a href="cap-svm.html"><span class="header-section-number">25</span> Máquinas de vector soporte</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="Índice del capítulo"><h2>Índice del capítulo</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#cap-arboles"><span class="header-section-number">24</span> Árboles de clasificación y regresión</a></li>
<li><a class="nav-link" href="#intro-dectree"><span class="header-section-number">24.1</span> Introducción</a></li>
<li><a class="nav-link" href="#procedimiento-con-r-la-funci%C3%B3n-rpart"><span class="header-section-number">24.2</span> Procedimiento con R: la función rpart()</a></li>
<li>
<a class="nav-link" href="#%C3%A1rboles-de-clasificaci%C3%B3n"><span class="header-section-number">24.3</span> Árboles de clasificación</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#c%C3%B3mo-se-va-formando-el-%C3%A1rbol-de-clasificaci%C3%B3n"><span class="header-section-number">24.3.1</span> ¿Cómo se va formando el árbol de clasificación?</a></li>
<li><a class="nav-link" href="#sobreajuste"><span class="header-section-number">24.3.2</span> Sobreajuste </a></li>
<li><a class="nav-link" href="#cu%C3%A1nto-debe-crecer-un-%C3%A1rbol-de-clasificaci%C3%B3n"><span class="header-section-number">24.3.3</span> ¿Cuánto debe crecer un árbol de clasificación?</a></li>
<li><a class="nav-link" href="#ejemplo-%C3%A1rbol-de-clasificaci%C3%B3n-para-determinar-la-intenci%C3%B3n-de-compra"><span class="header-section-number">24.3.4</span> Ejemplo: árbol de clasificación para determinar la intención de compra</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#%C3%A1rboles-de-regresi%C3%B3n"><span class="header-section-number">24.4</span> Árboles de regresión</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#c%C3%B3mo-se-va-formando-el-%C3%A1rbol-de-regresi%C3%B3n"><span class="header-section-number">24.4.1</span> ¿Cómo se va formando el árbol de regresión?</a></li>
<li><a class="nav-link" href="#cu%C3%A1nto-debe-crecer-el-%C3%A1rbol-de-regresi%C3%B3n"><span class="header-section-number">24.4.2</span> ¿Cuánto debe crecer el árbol de regresión?</a></li>
<li><a class="nav-link" href="#%C3%A1rbol-de-regresi%C3%B3n-para-estimar-el-n%C3%BAmero-de-d%C3%ADas-de-hospitalizaci%C3%B3n"><span class="header-section-number">24.4.3</span> Árbol de regresión para estimar el número de días de hospitalización</a></li>
<li><a class="nav-link" href="#%C3%A1rbol-de-regresi%C3%B3n-para-la-predicci%C3%B3n-del-precio-unitario-de-la-vivienda-en-madrid"><span class="header-section-number">24.4.4</span> Árbol de regresión para la predicción del precio unitario de la vivienda en Madrid</a></li>
<li><a class="nav-link" href="#resumen-23">Resumen</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Fundamentos de ciencia de datos con <strong>R</strong></strong>" coordinado por <a href="https://blog.uclm.es/gemafaviles/" class="text-light">Gema Fernández-Avilés y José-María Montero</a>. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>Este libro ha sido generado con el paquete de R <a class="text-light" href="https://bookdown.org">bookdown</a>.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
