<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Capítulo 19 Modelos sparse y métodos penalizados de regresión | Fundamentos de ciencia de datos con R</title>
<meta name="author" content="Gema Fernández-Avilés y José-María Montero">
<meta name="description" content="María Durbán Universidad Carlos III de Madrid  19.1 Introducción El modelo de regresión lineal múltiple: \(y=\beta_0+\beta_1 X_1+\ldots + \beta_pX_p+\varepsilon,\) visto en el Cap. 15, a pesar de...">
<meta name="generator" content="bookdown 0.28 with bs4_book()">
<meta property="og:title" content="Capítulo 19 Modelos sparse y métodos penalizados de regresión | Fundamentos de ciencia de datos con R">
<meta property="og:type" content="book">
<meta property="og:image" content="/img/cover.png">
<meta property="og:description" content="María Durbán Universidad Carlos III de Madrid  19.1 Introducción El modelo de regresión lineal múltiple: \(y=\beta_0+\beta_1 X_1+\ldots + \beta_pX_p+\varepsilon,\) visto en el Cap. 15, a pesar de...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Capítulo 19 Modelos sparse y métodos penalizados de regresión | Fundamentos de ciencia de datos con R">
<meta name="twitter:description" content="María Durbán Universidad Carlos III de Madrid  19.1 Introducción El modelo de regresión lineal múltiple: \(y=\beta_0+\beta_1 X_1+\ldots + \beta_pX_p+\varepsilon,\) visto en el Cap. 15, a pesar de...">
<meta name="twitter:image" content="/img/cover.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.2/transition.js"></script><script src="libs/bs3compat-0.4.2/tabs.js"></script><script src="libs/bs3compat-0.4.2/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><link href="libs/tabwid-1.1.0/tabwid.css" rel="stylesheet">
<link href="libs/tabwid-1.1.0/scrool.css" rel="stylesheet">
<script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css">
<link rel="stylesheet" href="bs4_book.css">
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Fundamentos de ciencia de datos con R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Buscar" aria-label="Buscar">
</form>

      <nav aria-label="Contenido"><h2>Contenido</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Prefacio</a></li>
<li class="book-part">Ciencia, datos, software… y científicos</li>
<li><a class="" href="ciencia-datos.html"><span class="header-section-number">1</span> ¿Es la ciencia de datos una ciencia?</a></li>
<li><a class="" href="metodologia.html"><span class="header-section-number">2</span> Metodología en ciencia de datos</a></li>
<li><a class="" href="ch-110003.html"><span class="header-section-number">3</span> R para ciencia de datos</a></li>
<li><a class="" href="cap-etica.html"><span class="header-section-number">4</span> Ética en la ciencia de datos</a></li>
<li class="book-part">Bienvenidos a la jungla de datos</li>
<li><a class="" href="datos-sql.html"><span class="header-section-number">5</span> Gestión de bases de datos relacionales</a></li>
<li><a class="" href="cap-nosql.html"><span class="header-section-number">6</span> Gestión de bases de datos NoSQL</a></li>
<li><a class="" href="DGDQM.html"><span class="header-section-number">7</span> Gobierno, gestión y calidad del dato</a></li>
<li><a class="" href="id_130009.html"><span class="header-section-number">8</span> Integración y limpieza de datos</a></li>
<li><a class="" href="chap-feature.html"><span class="header-section-number">9</span> Selección y transformación de variables</a></li>
<li><a class="" href="chap-herramientas.html"><span class="header-section-number">10</span> Herramientas para el análisis en ciencia de datos</a></li>
<li><a class="" href="id_120006-aed.html"><span class="header-section-number">11</span> Análisis exploratorio de datos</a></li>
<li class="book-part">Fundamentos de estadística</li>
<li><a class="" href="Funda-probab.html"><span class="header-section-number">12</span> Probabilidad</a></li>
<li><a class="" href="Fundainfer.html"><span class="header-section-number">13</span> Inferencia estadística</a></li>
<li><a class="" href="muestreo.html"><span class="header-section-number">14</span> Muestreo y remuestreo</a></li>
<li class="book-part">Modelización estadística</li>
<li><a class="" href="cap-lm.html"><span class="header-section-number">15</span> Modelización lineal</a></li>
<li><a class="" href="cap-glm.html"><span class="header-section-number">16</span> Modelos lineales generalizados</a></li>
<li><a class="" href="cap-gam.html"><span class="header-section-number">17</span> Modelos aditivos generalizados</a></li>
<li><a class="" href="cap-mxm.html"><span class="header-section-number">18</span> Modelos mixtos</a></li>
<li><a class="active" href="cap-sparse.html"><span class="header-section-number">19</span> Modelos sparse y métodos penalizados de regresión</a></li>
<li><a class="" href="cap-series-temp.html"><span class="header-section-number">20</span> Modelización de series temporales</a></li>
<li><a class="" href="cap-discriminante.html"><span class="header-section-number">21</span> Análisis discriminante</a></li>
<li><a class="" href="cap-conjunto.html"><span class="header-section-number">22</span> Análisis conjunto</a></li>
<li><a class="" href="tablas-contingencia.html"><span class="header-section-number">23</span> Análisis de tablas de contingencia</a></li>
<li class="book-part">Machine learning supervisado</li>
<li><a class="" href="cap-arboles.html"><span class="header-section-number">24</span> Árboles de clasificación y regresión</a></li>
<li><a class="" href="cap-svm.html"><span class="header-section-number">25</span> Máquinas de vector soporte</a></li>
<li><a class="" href="cap-knn.html"><span class="header-section-number">26</span> Clasificador k-vecinos más próximos</a></li>
<li><a class="" href="cap-naive-bayes.html"><span class="header-section-number">27</span> Naive Bayes</a></li>
<li><a class="" href="cap-bagg-rf.html"><span class="header-section-number">28</span> Métodos ensamblados: bagging y random forest</a></li>
<li><a class="" href="cap-boosting-xgboost.html"><span class="header-section-number">29</span> Boosting y el algoritmo XGBoost</a></li>
<li class="book-part">Machine learning no supervisado</li>
<li><a class="" href="jerarquico.html"><span class="header-section-number">30</span> Análisis cluster: clusterización jerárquica</a></li>
<li><a class="" href="no-jerarquico.html"><span class="header-section-number">31</span> Análisis cluster: clusterización no jerárquica</a></li>
<li><a class="" href="acp.html"><span class="header-section-number">32</span> Análisis de componentes principales</a></li>
<li><a class="" href="an%C3%A1lisis-factorial.html"><span class="header-section-number">33</span> Análisis factorial</a></li>
<li><a class="" href="escalamiento-multidimensional.html"><span class="header-section-number">34</span> Escalamiento multidimensional</a></li>
<li><a class="" href="correspondencias.html"><span class="header-section-number">35</span> Análisis de correspondencias</a></li>
<li class="book-part">Deep learning</li>
<li><a class="" href="capNN.html"><span class="header-section-number">36</span> Redes neuronales artificiales</a></li>
<li><a class="" href="cap-redes-convol.html"><span class="header-section-number">37</span> Redes neuronales convolucionales</a></li>
<li class="book-part">Ciencia de datos de texto y redes</li>
<li><a class="" href="mineria-textos.html"><span class="header-section-number">38</span> Minería de textos</a></li>
<li><a class="" href="grafos.html"><span class="header-section-number">39</span> Análisis de grafos y redes sociales</a></li>
<li class="book-part">Ciencia de datos espaciales</li>
<li><a class="" href="datos-espaciales.html"><span class="header-section-number">40</span> Trabajando con datos espaciales</a></li>
<li><a class="" href="geo.html"><span class="header-section-number">41</span> Geoestadística</a></li>
<li><a class="" href="cap-econom-esp.html"><span class="header-section-number">42</span> Modelos econométricos espaciales</a></li>
<li><a class="" href="cap-pp.html"><span class="header-section-number">43</span> Procesos de puntos</a></li>
<li class="book-part">Comunica y colabora</li>
<li><a class="" href="id_120007-informes.html"><span class="header-section-number">44</span> Informes reproducibles con R Markdown y Quarto</a></li>
<li><a class="" href="shiny.html"><span class="header-section-number">45</span> Creación de aplicaciones web interactivas con Shiny</a></li>
<li><a class="" href="github.html"><span class="header-section-number">46</span> Git y GitHub R</a></li>
<li><a class="" href="geoproces.html"><span class="header-section-number">47</span> Geoprocesamiento en nube</a></li>
<li class="book-part">Casos de estudio en ciencia de datos</li>
<li><a class="" href="cap-crimen.html"><span class="header-section-number">48</span> Análisis de una red criminal</a></li>
<li><a class="" href="cap-publicidad.html"><span class="header-section-number">49</span> Optimización de inversiones publicitarias</a></li>
<li><a class="" href="cap-twitter.html"><span class="header-section-number">50</span> ¿Cómo twitea Elon Musk?</a></li>
<li><a class="" href="cap-periodismo.html"><span class="header-section-number">51</span> Análisis electoral: de Rstudio a su periódico</a></li>
<li><a class="" href="paro-clm.html"><span class="header-section-number">52</span> Crisis: impacto en el paro de Castilla-La Mancha</a></li>
<li><a class="" href="cap-rfm.html"><span class="header-section-number">53</span> Segmentación de clientes en el comerico minorista</a></li>
<li><a class="" href="cap-medicina.html"><span class="header-section-number">54</span> Análisis de datos en medicina</a></li>
<li><a class="" href="cap-futbol.html"><span class="header-section-number">55</span> Messi y Ronaldo: dos ídolos desde la perspectiva de los datos</a></li>
<li><a class="" href="cambioclimatico.html"><span class="header-section-number">56</span> Un dato sobre el cambio climático</a></li>
<li><a class="" href="cap-ree.html"><span class="header-section-number">57</span> Predicción de consumo eléctrico con redes neuronales</a></li>
<li><a class="" href="cap-sist-exp.html"><span class="header-section-number">58</span> Implementación de un sistema experto en el ámbito pediátrico</a></li>
<li><a class="" href="nlp-textil.html"><span class="header-section-number">59</span> El procesamiento del lenguaje natural para tendencias de moda en textil</a></li>
<li><a class="" href="cap-fraude.html"><span class="header-section-number">60</span> Detección de fraude de tarjetas de crédito</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="info-session.html"><span class="header-section-number">A</span> Información de la sesión</a></li>
<li><a class="" href="referncias.html">Referncias</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="cap-sparse" class="section level1" number="19">
<h1>
<span class="header-section-number">Capítulo 19</span> Modelos <em>sparse</em> y métodos penalizados de regresión<a class="anchor" aria-label="anchor" href="#cap-sparse"><i class="fas fa-link"></i></a>
</h1>
<p><em>María Durbán</em></p>
<p>Universidad Carlos III de Madrid</p>
<div id="introducción-9" class="section level2" number="19.1">
<h2>
<span class="header-section-number">19.1</span> Introducción<a class="anchor" aria-label="anchor" href="#introducci%C3%B3n-9"><i class="fas fa-link"></i></a>
</h2>
<p>El modelo de regresión lineal múltiple: <span class="math inline">\(y=\beta_0+\beta_1 X_1+\ldots + \beta_pX_p+\varepsilon,\)</span> visto en el Cap. <a href="cap-lm.html#cap-lm">15</a>, a pesar de su simplicidad, tiene importantes ventajas como la <strong>interpretabilidad</strong> y su buen poder <strong>predictivo</strong> en muchas situaciones.</p>
<p>En este capítulo enseña cómo se puede hacer el modelo aún más interpretable y mejor predictor, y para conseguirlo se reemplaza el método de estimación de mínimos cuadrados por un método alternativo. Más concretamente, el objetivo de este capítulo es presentar técnicas para mejorar la:</p>
<ul>
<li>
<strong>Precisión de la predicción:</strong> en particular, cuando el número de variables es mayor que el número de observaciones: <span class="math inline">\(p&gt;n\)</span> (algo que ocurre con mucha frecuencia hoy en día). En este caso no se pueden utilizar mínimos cuadrados ya que la matriz de diseño no es de rango completo y, por lo tanto, no se puede encontrar una solución única al problema de minimización. Por ello, se necesita reducir el número de variables, que además, evitará que se sobreajusten los datos.</li>
<li>
<strong>Interpretabilidad del modelo:</strong> al eliminar las variables irrelevantes (es decir, haciendo cero los correspondientes coeficientes) se obtendrá un modelo más fácil de interpretar.</li>
</ul>
<p>En base a lo anterior,a continuación se presentan varios métodos para llevar a cabo de forma automática la reducción de variables en el modelo, actividad también denominada <strong>selección de variables</strong>. Tales métodos son:</p>
<ul>
<li>
<strong>Selección del mejor subconjunto:</strong> su objetivo es identificar el subconjunto de <span class="math inline">\(k&lt;p\)</span> predictores que contenga sólo los que mejor expliquen el comportamiento de la variable respuesta.</li>
<li>
<strong><em>Shrinkage:</em></strong> en este caso no se quieren seleccionar variables explícitamente, sino que se añade una penalización que penaliza el número de coeficientes o su tamaño.</li>
<li>
<strong>Reducción de la dimensión:</strong> el objetivo es proyectar los <span class="math inline">\(p\)</span>-predictores en un subespacio de dimensión más pequeña (mediante el uso de combinaciones lineales de las variables predictoras, las cuales se usarán como “nuevos” predictores). Dichas combinaciones lineales se llaman <strong>componentes principales</strong> y a su análisis se dedica el Cap. <a href="acp.html#acp">32</a>.</li>
</ul>
<p>En este Capítulo se ven los dos primeros métodos. Para el tercero, se remite al lector al Cap. <a href="acp.html#acp">32</a>.</p>
</div>
<div id="selección-del-mejor-subconjunto" class="section level2" number="19.2">
<h2>
<span class="header-section-number">19.2</span> Selección del mejor subconjunto<a class="anchor" aria-label="anchor" href="#selecci%C3%B3n-del-mejor-subconjunto"><i class="fas fa-link"></i></a>
</h2>
<p>Supóngase que se tiene acceso a <span class="math inline">\(p\)</span> variables predictoras, pero se quiere un modelo más simple que involucre sólo a un subconjunto de esos <span class="math inline">\(p\)</span> predictores. La forma lógica de conseguirlo es considerar todos los posibles subconjuntos de los <span class="math inline">\(p\)</span> predictores y elegir el mejor modelo de entre todos los modelos construidos con cada uno de los subconjuntos de variables. Los pasos a seguir serían:</p>
<ol style="list-style-type: decimal">
<li><p>Se crea el modelo <strong>nulo</strong>, <span class="math inline">\(M_0\)</span>, que es aquel que únicamente contiene la ordenada en el origen y ningún predictor. Este modelo simplemente predice la media muestral para cada observación.</p></li>
<li><p>Para cada valor de <span class="math inline">\(k=1,2,\ldots , p\)</span>, se calculan los <span class="math inline">\(\binom{p}{k}\)</span> modelos que contienen <span class="math inline">\(k\)</span> predictores. Es decir, los <span class="math inline">\(p\)</span> modelos que contienen 1 predictor, los <span class="math inline">\(p\times (p-1)/2\)</span> modelos que contienen 2 predictores, etc.</p></li>
<li><p>Para cada valor de <span class="math inline">\(k\)</span>, se elige el mejor entre los <span class="math inline">\(\binom{p}{k}=\frac{p!}{(p-k)!k!}\)</span> posibles modelos y se denota por <span class="math inline">\(M_k\)</span>. Es decir, <span class="math inline">\(M_1\)</span> sería el mejor modelo entre los <span class="math inline">\(p\)</span> modelos con una única variable, <span class="math inline">\(M_2\)</span> sería el mejor modelo entre los modelos con dos variables, etc. En este caso, el <strong>mejor</strong> modelo sería aquel cuyo <span class="math inline">\(RSS\)</span> (suma de residuos al cuadrado) sea menor, o equivalentemente, aquel cuyo <span class="math inline">\(R^2\)</span> sea mayor.</p></li>
<li><p>Finalmente, entre los modelos: <span class="math inline">\(M_1,\ldots ,M_p\)</span> ase elige el mejor utilizando un criterio como AIC (criterio de información de Akaike), BIC (criterio de información bayesiano) o <span class="math inline">\(R^2\)</span> ajustado.</p></li>
</ol>
<p>Este método se puede usar también en el caso de GLMs, si bien, en este caso, se usa la <em>deviance</em> en vez de <span class="math inline">\(RSS\)</span>.</p>
<div id="procedimiento-con-r-la-función-regsubset" class="section level3" number="19.2.1">
<h3>
<span class="header-section-number">19.2.1</span> Procedimiento con <strong>R</strong>: la función <code>regsubset()</code><a class="anchor" aria-label="anchor" href="#procedimiento-con-r-la-funci%C3%B3n-regsubset"><i class="fas fa-link"></i></a>
</h3>
<p>En esta subsección se aplica el método descrito al conjunto de datos <code>Hitters</code> del paquete <code>ISRL2</code>. El objetivo es predecir el sueldo, <code>Salary</code>, de jugadores de béisbol a partir de varias variables asociadas con su rendimiento el año anterior.</p>
<p>La variable <code>Salary</code> no está disponible para algunos de los jugadores. Éstos se pueden identificar con la función <code><a href="https://rdrr.io/r/base/NA.html">is.na()</a></code>. La función <code><a href="https://rdrr.io/r/base/sum.html">sum()</a></code> permite ver cuántos hay. Se utiliza <code><a href="https://rdrr.io/r/stats/na.fail.html">na.omit()</a></code> para eliminarlos.</p>
<div class="sourceCode" id="cb259"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://www.statlearning.com">"ISLR2"</a></span><span class="op">)</span></span>
<span><span class="va">Hitters</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/na.fail.html">na.omit</a></span><span class="op">(</span><span class="va">Hitters</span><span class="op">)</span></span></code></pre></div>
<p>La función <code><a href="https://rdrr.io/pkg/leaps/man/regsubsets.html">regsubsets()</a></code> del paquete <code>leaps</code> lleva a cabo la selección del mejor subconjunto de variables predictoras, identificando el mejor modelo que contiene un número dado de ellas (1,2,3, etc.) atendiendo a <span class="math inline">\(RSS\)</span>. La sintaxis usada es similar a la de la función <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code>.</p>
<div class="sourceCode" id="cb260"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"leaps"</span><span class="op">)</span></span>
<span><span class="va">regfit_full</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/leaps/man/regsubsets.html">regsubsets</a></span><span class="op">(</span><span class="va">Salary</span> <span class="op">~</span> <span class="va">.</span>, <span class="va">Hitters</span><span class="op">)</span></span></code></pre></div>
<p>Los resultados se pueden ver usando <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code>, donde se muestra el mejor modelo para cada número específico de variables. Las variables incluidas en cada modelo se indican con un asterisco. Por ejemplo, el mejor modelo con dos variables incluye <code>Hits</code> y <code>CRBI</code>.
Por defecto, <code><a href="https://rdrr.io/pkg/leaps/man/regsubsets.html">regsubsets()</a></code> solo muestra los resultados de los modelos que contienen hasta ocho variables. La opción <code>nvmax</code> se puede usar para incrementar esta cantidad, por ejemplo hasta 19 variables (que es el número de variables predictoras en el conjunto de datos):</p>
<div class="sourceCode" id="cb261"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">regfit_full</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/leaps/man/regsubsets.html">regsubsets</a></span><span class="op">(</span><span class="va">Salary</span> <span class="op">~</span> <span class="va">.</span>,</span>
<span>  data <span class="op">=</span> <span class="va">Hitters</span>,</span>
<span>  nvmax <span class="op">=</span> <span class="fl">19</span></span>
<span><span class="op">)</span></span>
<span><span class="va">reg_summary</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">regfit_full</span><span class="op">)</span></span></code></pre></div>
<p>La función <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> devuelve diferentes medias de bondad de ajuste: <span class="math inline">\(R^2\)</span>, <span class="math inline">\(RSS\)</span>, <span class="math inline">\(R^2\)</span> ajustado, <span class="math inline">\(C_p\)</span> y <span class="math inline">\(BIC\)</span> que se utilizan para elegir el <em>mejor</em> de entre todos los modelos.</p>
<div class="sourceCode" id="cb262"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">reg_summary</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "which"  "rsq"    "rss"    "adjr2"  "cp"     "bic"    "outmat" "obj"</span></span>
<span><span class="va">reg_summary</span><span class="op">$</span><span class="va">adjr2</span></span>
<span><span class="co">#&gt;  [1] 0.3188503 0.4208024 0.4450753 0.4672734 0.4808971 0.4972001 0.5007849</span></span>
<span><span class="co">#&gt;  [8] 0.5137083 0.5180572 0.5222606 0.5225706 0.5217245 0.5206736 0.5195431</span></span>
<span><span class="co">#&gt; [15] 0.5178661 0.5162219 0.5144464 0.5126097 0.5106270</span></span></code></pre></div>
<p>En el ejemplo, el <span class="math inline">\(R^2\)</span> ajustado mayor corresponde al modelo con 11 variables.</p>
<p>Los resultados también se se pueden mostrar y dibujar simultáneamente; por ejemplo, los valores de <span class="math inline">\(RSS\)</span> y <span class="math inline">\(R^2\)</span> ajustado de todos los modelos se muestran en la Fig. <a href="cap-sparse.html#fig:chunk7">19.1</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:chunk7"></span>
<img src="150020_sparse_prm_files/figure-html/chunk7-1.png" alt="Valores de $R^2$ y $R^2$ ajustados corerspondientes a modelos con distinto número de variables" width="60%"><p class="caption">
Figura 19.1: Valores de <span class="math inline">\(R^2\)</span> y <span class="math inline">\(R^2\)</span> ajustados corerspondientes a modelos con distinto número de variables
</p>
</div>
<p>Otra manera de visualizar los resultados es:</p>
<div class="sourceCode" id="cb263"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">regfit_full</span>, scale <span class="op">=</span> <span class="st">"adjr2"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:chunk10"></span>
<img src="150020_sparse_prm_files/figure-html/chunk10-1.png" alt="Variables seleccionadas en cada uno de los modelos y su correspondiente valor de $R^2$ ajustado." width="60%"><p class="caption">
Figura 19.2: Variables seleccionadas en cada uno de los modelos y su correspondiente valor de <span class="math inline">\(R^2\)</span> ajustado.
</p>
</div>
<p>La primera fila tiene un cuadrado negro en cada una de las variables explicativas del modelo con mayor <span class="math inline">\(R^2\)</span> ajustado (en este caso, sería similar para los otros criterios).</p>
<p>Varios modelos tienen un valor de <span class="math inline">\(R^2\)</span> ajustado próximo a <span class="math inline">\(0.52\)</span>, pero es el modelo con 11 variables el que alcanza el mayor valor. La función <code><a href="https://rdrr.io/r/stats/coef.html">coef()</a></code> permite ver los coeficientes estimados de este modelo.</p>
<div class="sourceCode" id="cb264"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">regfit_full</span>, <span class="fl">11</span><span class="op">)</span></span>
<span><span class="co">#&gt;  (Intercept)        AtBat         Hits        Walks       CAtBat        CRuns </span></span>
<span><span class="co">#&gt;  135.7512195   -2.1277482    6.9236994    5.6202755   -0.1389914    1.4553310 </span></span>
<span><span class="co">#&gt;         CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists </span></span>
<span><span class="co">#&gt;    0.7852528   -0.8228559   43.1116152 -111.1460252    0.2894087    0.2688277</span></span></code></pre></div>
</div>
<div id="selección-stepwise" class="section level3" number="19.2.2">
<h3>
<span class="header-section-number">19.2.2</span> Selección <em>stepwise</em><a class="anchor" aria-label="anchor" href="#selecci%C3%B3n-stepwise"><i class="fas fa-link"></i></a>
</h3>
<p>Cuando el número de variables predictoras, <span class="math inline">\(p\)</span>, es grande, el método anterior es computacionalmente muy costoso ya que el número de posibles combinaciones de variables crece de una manera alarmante. En general, la función <code>regsubset()</code> puede lidiar con hasta 30-40 variables predictoras. Además, otro problema es el sobreajuste. Si se tienen 40 variables, se estarían ajustando millones de modelos, y puede que el modelo elegido funcione muy bien en los datos utilizados para su construcción, pero no tan bien en un nuevo conjunto de datos. Una alternativa es el método <strong>stepwise</strong>.
La idea detrás de este método es similar a la anterior, pero se busca el mejor modelo entre un conjunto mucho más pequeño de modelos.</p>
<p>Hay dos posibilidades de hacer stepwise: <strong><em>forward</em></strong> y <strong><em>backward</em></strong>. Ambas son bastante parecidas; la principal diferencia es el modelo del que se parte: del modelo sin ninguna variable predictora (<em>forward</em>) o del modelo con todas ellas (<em>backward</em>).</p>
<div id="forward-stepwise" class="section level4" number="19.2.2.1">
<h4>
<span class="header-section-number">19.2.2.1</span> <em>Forward stepwise</em><a class="anchor" aria-label="anchor" href="#forward-stepwise"><i class="fas fa-link"></i></a>
</h4>
<p>En este caso se comienza con el <strong>modelo nulo</strong>, <span class="math inline">\(M_0\)</span>, y se van añadiendo variables secuencialmente. En particular, en cada paso (<em>step</em>) la variable que proporciona la mayor mejora al ajuste es la que se añade al modelo. Los pasos a seguir serían:</p>
<ol style="list-style-type: decimal">
<li><p>Se crea el modelo nulo, <span class="math inline">\(M_0\)</span>.</p></li>
<li>
<p>Para cada valor de <span class="math inline">\(k=0,1,2,\ldots , p\)</span>:</p>
<p><em>a</em>) Se consideran todos los <span class="math inline">\(p-k\)</span> modelos que surgen de aumentar el modelo <span class="math inline">\(M_k\)</span> con un predictor.</p>
<p><em>b</em>) Se elige el <strong>mejor</strong> de esos <span class="math inline">\(p-k\)</span> modelos, que se denota <span class="math inline">\(M_{k+1}\)</span>. El término <strong>mejor</strong> significa tener el <span class="math inline">\(RSS\)</span> más bajo o el <span class="math inline">\(R^2\)</span> más alto.</p>
</li>
<li><p>Se elige el mejor de los modelos <span class="math inline">\(M_0,\ldots ,M_p\)</span> en función de un criterio que tenga en cuenta la complejidad del modelo, como AIC, BIC o <span class="math inline">\(R^2\)</span> ajustado.</p></li>
</ol>
<p>Este enfoque tiene ventajas computacionales claras, ya que el número de modelos ajustados es mucho menor, pero no garantiza que el modelo elegido sea el mejor modelo posible, especialmente si existe correlación entre las variables predictoras.</p>
</div>
<div id="backward-stepwise" class="section level4" number="19.2.2.2">
<h4>
<span class="header-section-number">19.2.2.2</span> Backward stepwise<a class="anchor" aria-label="anchor" href="#backward-stepwise"><i class="fas fa-link"></i></a>
</h4>
<p>En este caso, se comienza con el modelo que incluye las <span class="math inline">\(p\)</span> las variables predictoras y se van eliminando de forma iterativa hasta llegar al modelo nulo (<span class="math inline">\(M_0\)</span>). Los pasos serían:</p>
<ol style="list-style-type: decimal">
<li>Se ajusta el modelo <span class="math inline">\(M_p\)</span>, que contiene todas (<span class="math inline">\(p\)</span>) las variable predictoras.</li>
<li>Para cada valor de <span class="math inline">\(k=p,p-1,\ldots , 1\)</span>:
<em>a</em>) Se consideran todos los <span class="math inline">\(k\)</span> modelos que surgen de reducir en el modelo <span class="math inline">\(M_k\)</span> un predictor, es decir, modelos con <span class="math inline">\(k-1\)</span> variables predictoras.
<em>b</em>) Se elige el <strong>mejor</strong> de esos <span class="math inline">\(k\)</span> modelos, que se denota <span class="math inline">\(M_{k-1}\)</span>. Dicho modelo será el que tenga el <span class="math inline">\(RSS\)</span> más bajo o el <span class="math inline">\(R^2\)</span> más alto.</li>
<li>Entre los modelos <span class="math inline">\(M_0,\ldots ,M_p\)</span> se elige el mejor en función de un criterio como AIC, BIC o <span class="math inline">\(R^2\)</span> ajustado.</li>
</ol>
<p>Tanto en el caso <em>forward</em> como en el caso <em>backward</em>, se busca el mejor modelo “sólo” entre <span class="math inline">\(1+p(p+1)/2\)</span> modelos, lo que los hace recomendables frente a la selección del mejor subconjunto de variables cuando <span class="math inline">\(p\)</span> es demasiado grande..</p>
<p>El método <em>backward stewise</em> necesita que el número de observaciones <span class="math inline">\(n\)</span> sea mayor que el de variables predictoras <span class="math inline">\(p\)</span> (ya que necesita ajustar el modelo con todas las variables). Por el contrario, el método <em>forward stepwise</em> se puede usar incluso cuando <span class="math inline">\(n&lt;p\)</span>.</p>
</div>
<div id="procedimiento-con-r-la-función-regsubset-1" class="section level4" number="19.2.2.3">
<h4>
<span class="header-section-number">19.2.2.3</span> Procedimiento con <strong>R</strong>: la función <code>regsubset()</code><a class="anchor" aria-label="anchor" href="#procedimiento-con-r-la-funci%C3%B3n-regsubset-1"><i class="fas fa-link"></i></a>
</h4>
<p>La función <code>regsubset()</code> permite utilizar los métodos <em>forward</em> y <em>backward</em>, usando los argumentos <code>method = "forward"</code> o <code>method = "backward"</code>:</p>
<div class="sourceCode" id="cb265"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">regfit_fwd</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/leaps/man/regsubsets.html">regsubsets</a></span><span class="op">(</span><span class="va">Salary</span> <span class="op">~</span> <span class="va">.</span>,</span>
<span>  data <span class="op">=</span> <span class="va">Hitters</span>,</span>
<span>  nvmax <span class="op">=</span> <span class="fl">19</span>, method <span class="op">=</span> <span class="st">"forward"</span></span>
<span><span class="op">)</span></span>
<span><span class="va">regfit_bwd</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/leaps/man/regsubsets.html">regsubsets</a></span><span class="op">(</span><span class="va">Salary</span> <span class="op">~</span> <span class="va">.</span>,</span>
<span>  data <span class="op">=</span> <span class="va">Hitters</span>,</span>
<span>  nvmax <span class="op">=</span> <span class="fl">19</span>, method <span class="op">=</span> <span class="st">"backward"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Los métodos mejor subconjunto de variables, <em>forward stepwise</em> y <em>backward stepwise</em> no tienen por qué seleccionar el mismo (mejor) modelo. Ni siquiera tienen por qué seleccionar el mismo modelo para cada número de predictores en la fase previa a la selección final. Así ocurre, por ejemplo, cuando el número de variables predictoras es <span class="math inline">\(k=2\)</span>:</p>
<div class="sourceCode" id="cb266"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">regfit_full</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; (Intercept)        Hits        CRBI </span></span>
<span><span class="co">#&gt; -47.9559022   3.3008446   0.6898994</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">regfit_fwd</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; (Intercept)        Hits        CRBI </span></span>
<span><span class="co">#&gt; -47.9559022   3.3008446   0.6898994</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">regfit_bwd</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; (Intercept)        Hits       CRuns </span></span>
<span><span class="co">#&gt; -50.8174029   3.2257212   0.6614168</span></span></code></pre></div>
<p>En la etapa final de selección, el modelo seleccionado no tiene por qué ser el mismo en función de los distintos criterios de selección (aunque normalmente lo es). Lo habitual es decidir un criterio para elegir el mejor modelo (<span class="math inline">\(R^2\)</span> ajustado, <span class="math inline">\(BIC\)</span>, etc.) y seleccionarlo en función de él. En el ejemplo, seleccionando el criterio del <span class="math inline">\(R^2\)</span> ajustado, el mejor modelo es el que tiene 11 variables, tanto con el criterio <em>forward</em> como con el <em>backward</em><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Recuérdese que con el método del mejor subconjunto de variables predictoras el criterio del &lt;span class="math inline"&gt;\(R^2\)&lt;/span&gt; ajustado también seleccionó el modelo con 11 variables.&lt;/p&gt;'><sup>143</sup></a>:</p>
<div class="sourceCode" id="cb267"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/which.min.html">which.max</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">regfit_fwd</span><span class="op">)</span><span class="op">$</span><span class="va">adjr2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 11</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/which.min.html">which.max</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">regfit_bwd</span><span class="op">)</span><span class="op">$</span><span class="va">adjr2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 11</span></span></code></pre></div>
<p>Con el criterio <span class="math inline">\(BIC\)</span> también se selecciona el modelo con 11 variables predictoras:</p>
<div class="sourceCode" id="cb268"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/which.min.html">which.min</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">regfit_fwd</span><span class="op">)</span><span class="op">$</span><span class="va">bic</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 6</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/which.min.html">which.min</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">regfit_bwd</span><span class="op">)</span><span class="op">$</span><span class="va">bic</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 8</span></span></code></pre></div>
<p>Otra posibilidad es utilizar como criterio de selección el error de predicción, y para ello se puede echar mano de algún esquema de validación cruzada. A continuación se ilustra el caso en el que se divide la muestra en dos subconjuntos: <em>training</em> y <em>testing</em>, pero se puede utilizar cualquier otro método (validación cruzada k-grupos*, etc. Véase Sec. <a href="chap-herramientas.html#enfoque-validacion">10.4</a>.</p>
<div class="sourceCode" id="cb269"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">entreno</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="cn">TRUE</span>, <span class="cn">FALSE</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">Hitters</span><span class="op">)</span>, replace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">test</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="op">!</span><span class="va">entreno</span><span class="op">)</span></span></code></pre></div>
<p>Se utiliza <code><a href="https://rdrr.io/pkg/leaps/man/regsubsets.html">regsubsets()</a></code> en la muestra de entrenamiento para obtener los modelos con distinto número de variables predictoras:</p>
<div class="sourceCode" id="cb270"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">regfit_best</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/leaps/man/regsubsets.html">regsubsets</a></span><span class="op">(</span><span class="va">Salary</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">Hitters</span><span class="op">[</span><span class="va">entreno</span>, <span class="op">]</span>, nvmax <span class="op">=</span> <span class="fl">19</span><span class="op">)</span></span></code></pre></div>
<p>Para calcular el error de predicción, dado que la función <code>regsubset()</code> no tiene asociada una función <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code>, se han de calcular “manualmente” los valores predichos para la muestra de test. Para eso se necesita la matriz de diseño del modelo.</p>
<div class="sourceCode" id="cb271"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">test.mat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">Salary</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">Hitters</span><span class="op">[</span><span class="va">test</span>, <span class="op">]</span><span class="op">)</span></span></code></pre></div>
<p>Ahora, para cada modelo de tamaño <span class="math inline">\(k\)</span>, se extraen los coeficientes de
<code>regfit_best</code> para el mejor modelo de ese tamaño, se multiplica el vector de coeficientes por la matriz de diseño y se obtienen las predicciones; a continuación se calcula el error cuadrático medio (<span class="math inline">\(MSE\)</span>).</p>
<div class="sourceCode" id="cb272"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">val_errors</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="cn">NA</span>, <span class="fl">19</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">19</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">coefi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">regfit_best</span>, id <span class="op">=</span> <span class="va">i</span><span class="op">)</span></span>
<span>  <span class="va">pred</span> <span class="op">&lt;-</span> <span class="va">test.mat</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">coefi</span><span class="op">)</span><span class="op">]</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">coefi</span></span>
<span>  <span class="va">val_errors</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">Hitters</span><span class="op">$</span><span class="va">Salary</span><span class="op">[</span><span class="va">test</span><span class="op">]</span> <span class="op">-</span> <span class="va">pred</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Con este criterio, el mejor modelo es el que contiene 7 variables:</p>
<div class="sourceCode" id="cb273"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">val_errors</span></span>
<span><span class="co">#&gt;  [1] 164377.3 144405.5 152175.7 145198.4 137902.1 139175.7 126849.0 136191.4</span></span>
<span><span class="co">#&gt;  [9] 132889.6 135434.9 136963.3 140694.9 140690.9 141951.2 141508.2 142164.4</span></span>
<span><span class="co">#&gt; [17] 141767.4 142339.6 142238.2</span></span></code></pre></div>
</div>
</div>
</div>
<div id="métodos-shrinkage" class="section level2" number="19.3">
<h2>
<span class="header-section-number">19.3</span> Métodos <em>shrinkage</em><a class="anchor" aria-label="anchor" href="#m%C3%A9todos-shrinkage"><i class="fas fa-link"></i></a>
</h2>
<p>Los métodos anteriores se basan en el ajuste de modelos mediante mínimos cuadrados ordinarios. Los métodos <strong><em>shrinkage</em></strong>, sin embargo, se basan en una modificación del procedimiento de mínimos cuadrados ordinarios que consiste en añadir una penalización que <em>encoje</em> los coeficientes del modelo (normalmente hacia <span class="math inline">\(0\)</span>). Una de las ventajas de este tipo de métodos es que reduce la varianza de los coeficientes estimados.</p>
<p>Recuérdese que en el ajuste por mínimos cuadrados las estimaciones de <span class="math inline">\(\beta_0, \beta_1, \ldots , \beta_p\)</span> son los valores que minimizan:
<span class="math display">\[RSS=\sum_{i=1}^n \left ( y_i-\beta_0-\sum_{j=1}^p \beta_jx_{ij}\right )^2.\]</span></p>
<div id="regresión-ridge" class="section level3" number="19.3.1">
<h3>
<span class="header-section-number">19.3.1</span> Regresión <em>ridge</em><a class="anchor" aria-label="anchor" href="#regresi%C3%B3n-ridge"><i class="fas fa-link"></i></a>
</h3>
<p>La <strong>regresión</strong> <strong><em>ridge</em></strong><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;La traducción en español sería regresión contraída o regresión alomada.&lt;/p&gt;"><sup>144</sup></a> añade un término de penalización controlado por un parámetro (que habrá que elegir) que penalizará la magnitud de los coeficientes. Cuanto más grande es el coeficiente mayor es la penalización. En consecuencia, en la regresión <em>ridge</em> la expresión que se minimiza para obtener las estimaciones de los parámetros del modelo es:
<span class="math display" id="eq:sparse1">\[\begin{equation}
\sum_{i=1}^n \left ( y_i-\beta_0-\sum_{j=1}^p \beta_jx_{ij}\right )^2+\lambda \sum_{j=1}^p \beta_j^2=RSS+\lambda \sum_{j=1}^p \beta_j^2.
\tag{19.1}
\end{equation}\]</span>
En realidad, lo que se está haciendo es hacer pagar al modelo un precio (en términos de ajuste) por el hecho de que los coeficientes no sean cero, y el precio será tanto mayor cuanto más grande sea la magnitud del coeficiente. A esta penalización se le llama <strong>penalización</strong> <strong><em>shrinkage</em></strong> porque “anima” a los coeficientes a que se <em>contraigan</em> hacia <span class="math inline">\(0\)</span> (así es como este método favorece la simplicidad de los modelos). La magnitud de dicha contracción está gobernada por lambda, el parametro de afinado o regulación (también conocido en la jerga como “de tuneado”). Si <span class="math inline">\(\lambda=0\)</span>, se está en el caso de mínimos cuadrados ordinarios, y cuanto mayor sea <span class="math inline">\(\lambda\)</span>, mayor será el precio a pagar para que esos coeficientes sean distintos de <span class="math inline">\(0\)</span>. Si <span class="math inline">\(\lambda\)</span> es extremadamente grande, los coeficientes estarán muy próximos a <span class="math inline">\(0\)</span>, para que el segundo término pequeño (recuérdese que se minimiza <span class="math inline">\(RSS\)</span> más la penalización). Aunque valores más grandes de los coeficientes proporcionasen un mejor ajuste (y por lo tanto un menor <span class="math inline">\(RSS\)</span>), el término de penalización aumentaría se hará grande y no se alcanzaría el mínimo. Por lo tanto <span class="math inline">\(\lambda\)</span> gobierna el equilibrio entre un buen ajuste del modelo y el tamaño de los coeficientes (y por lo tanto el número de coeficientes distintos de cero).</p>
<p>La elección del valor de <span class="math inline">\(\lambda\)</span> es un punto crucial de este tipo de regresión. Para su determinación se suelen utilizar procedimientos de validación cruzada.</p>
<div id="escalado-de-variables-predictoras" class="section level4" number="19.3.1.1">
<h4>
<span class="header-section-number">19.3.1.1</span> Escalado de variables predictoras<a class="anchor" aria-label="anchor" href="#escalado-de-variables-predictoras"><i class="fas fa-link"></i></a>
</h4>
<p>Un punto importante en regresión ridge es si las variables predictoras están escaladas o no.</p>
<p>El método de mínimos cuadrados ordinarios es <em>invariante a la escala</em> (<em>scale-invariant</em>), es decir, que si se multiplica una variable predictora <span class="math inline">\(X_j\)</span> por una constante <span class="math inline">\(c\)</span>, el coeficiente estimado se multiplicada por <span class="math inline">\(1/c\)</span>, pero <span class="math inline">\(X_j\hat \beta_j\)</span> no cambia. Sin embargo, en el caso de la regresión <em>ridge</em> los coeficientes estimados pueden cambiar sustancialmente ante un cambio de escala (es decir, si se multiplica una variable predictora por una constante), ya que todos los coeficientes forman parte del término de penalización. Por lo tanto, antes de utilizar la regresión <em>ridge</em> (o cualquier método de regularización) es importante <strong>estandarizar las variables predictoras</strong>, dividiendo cada variable por su desviación estándar, de forma que todas tengan desviación estándar igual a <span class="math inline">\(1\)</span>:
<span class="math display">\[\tilde x_{ij}= \frac{x_{ij}}{\sqrt{\frac{1}{N}\sum_{i=1}^N (x_{ij}-\overline{x}_{ij})^2}}.\]</span>
Con esto se consigue que los coeficientes estén en “igualdad de condiciones”.</p>
<p>En muchas ocasiones la regresión <em>ridge</em> da lugar a un menor <span class="math inline">\(MSE\)</span> que el obtenido con mínimos cuadrados ordinarios. Sin embargo, por muy grande que sea <span class="math inline">\(\lambda\)</span> los coeficientes no serán <span class="math inline">\(0\)</span>, sino que estarán próximos a cero, por lo que <strong>este método no es realmente un método de selección de variables</strong>.</p>
<p>Sin embargo, la regresión ridge puede ser muy útil cuando hay variables predictoras altamente correlacionadas pero se desea mantener todas en el modelo. En estos casos, la regresión ridge soluciona los problemas de multicolinealidad.</p>
</div>
<div id="procedimiento-con-r-la-función-glmnet" class="section level4" number="19.3.1.2">
<h4>
<span class="header-section-number">19.3.1.2</span> Procedimiento con R: la función glmnet()<a class="anchor" aria-label="anchor" href="#procedimiento-con-r-la-funci%C3%B3n-glmnet"><i class="fas fa-link"></i></a>
</h4>
<p>Para llevar a cabo la regresión <em>ridge</em> (y para otros métodos de regresión <em>shrinkage</em>) se usa el paquete <code>glmnet</code>.
La función principal en este paquete se llama también <code><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet()</a></code>. Esta función tiene una sintaxis un poco diferente a las funciones usuales para el ajuste de distintos modelos en <strong>R</strong>. Es necesario pasarle la matriz <span class="math inline">\(\boldsymbol X\)</span> de variables predictoras (sin la columna correspondiente a la ordenada en el origen) y el vector <span class="math inline">\(\boldsymbol y\)</span> con la variable respuesta. Para ilustrar su uso se utilizan los datos anteriores sobre béisbol.</p>
<div class="sourceCode" id="cb274"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix</a></span><span class="op">(</span><span class="va">Salary</span> <span class="op">~</span> <span class="va">.</span>, <span class="va">Hitters</span><span class="op">)</span><span class="op">[</span>, <span class="op">-</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="va">Hitters</span><span class="op">$</span><span class="va">Salary</span></span></code></pre></div>
<p>La función <code><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet()</a></code> tiene un argumento, <code>alpha</code>, que determina el tipo de penalización que se añade en el modelo. En el caso de regresión <em>ridge</em>, <code>alpha=0</code>.</p>
<p>Por defecto, la función <code><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet()</a></code> elige de forma automática el rango de valores de <span class="math inline">\(\lambda\)</span>. Sin embargo, a modo ilustrativo, se va a elegir la rejilla de valores que van desde <span class="math inline">\(\lambda=10^{10}\)</span> hasta <span class="math inline">\(\lambda=10^{-2}\)</span>, cubriendo de esta forma una gran gama de escenarios, desde el modelo nulo (solo la ordenada en el origen) hasta el caso de mínimos cuadrados ordinarios. Más adelante se verá que se puede llevar a cabo el ajuste del modelo para un valor determinado de <span class="math inline">\(\lambda\)</span> que no esté entre los de la rejilla inicial.</p>
<div class="sourceCode" id="cb275"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://glmnet.stanford.edu">"glmnet"</a></span><span class="op">)</span></span>
<span><span class="va">grid</span> <span class="op">&lt;-</span> <span class="fl">10</span><span class="op">^</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">10</span>, <span class="op">-</span><span class="fl">2</span>, length <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span><span class="va">ridge_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, alpha <span class="op">=</span> <span class="fl">0</span>, lambda <span class="op">=</span> <span class="va">grid</span><span class="op">)</span></span></code></pre></div>
<p>Por defecto, la función <code><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet()</a></code> estandariza las variables predictoras para que estén en la misma escala. Si por alguna razón no se quisiera hacer, se usaría <code>standardize = FALSE</code>.</p>
<p>Asociado con cada valor de <span class="math inline">\(\lambda\)</span> hay un vector de coeficientes estimados mediante regresión ridge almacenados en un matriz accesible utilizando <code><a href="https://rdrr.io/r/stats/coef.html">coef()</a></code>. En este caso, el tamaño de la matriz es <span class="math inline">\(20 \times 100\)</span>, donde las <span class="math inline">\(20\)</span> filas corresponden a cada uno de los predictores más la ordenada en el origen y las 100 columnas a cada valor de <span class="math inline">\(\lambda\)</span>. Lo esperable es que los coeficientes estimados sean más pequeños cuanto mayor sea el valor de <span class="math inline">\(\lambda\)</span>. A continuación, se muestra el valor de los coeficientes cuando <span class="math inline">\(\lambda = 11.498\)</span>, así como la suma de sus cuadrados, <span class="math inline">\(\sum_{j=1}^p\beta_j^2\)</span>:</p>
<div class="sourceCode" id="cb276"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ridge_mod</span><span class="op">$</span><span class="va">lambda</span><span class="op">[</span><span class="fl">50</span><span class="op">]</span></span>
<span><span class="co">#&gt; [1] 11497.57</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">ridge_mod</span><span class="op">)</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">50</span><span class="op">]</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 40.45739</span></span></code></pre></div>
<p>Por el contrario, si <span class="math inline">\(\lambda\)</span> es más pequeño, <span class="math inline">\(705\)</span>, el valor de su suma de cuadrados es mucho mayor.</p>
<div class="sourceCode" id="cb277"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ridge_mod</span><span class="op">$</span><span class="va">lambda</span><span class="op">[</span><span class="fl">60</span><span class="op">]</span></span>
<span><span class="co">#&gt; [1] 705.4802</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">ridge_mod</span><span class="op">)</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">60</span><span class="op">]</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 3261.554</span></span></code></pre></div>
<p>La Fig. <a href="cap-sparse.html#fig:chunk292">19.3</a> muestra el efecto de <span class="math inline">\(\lambda\)</span> en los coeficientes del modfelo:</p>
<div class="sourceCode" id="cb278"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">ridge_mod</span>, xvar <span class="op">=</span> <span class="st">"lambda"</span>, label <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:chunk292"></span>
<img src="150020_sparse_prm_files/figure-html/chunk292-1.png" alt="Coeficientes estimados para distintos valores del parámetro de penalización (en la escala logarítmica)" width="60%"><p class="caption">
Figura 19.3: Coeficientes estimados para distintos valores del parámetro de penalización (en la escala logarítmica)
</p>
</div>
<p>El lado izquierdo de la Fig <a href="cap-sparse.html#fig:chunk292">19.3</a> corresponde a valores de <span class="math inline">\(\lambda\)</span> muy pequeños, y por lo tanto no existen restricciones sobre los coeficientes. Conforme aumenta el valor de <span class="math inline">\(\lambda\)</span> los coeficientes se aproximan rápidamente a cero. Pero no todos se aproximan a cero de la misma manera: hay un conjunto de variables cuyo coeficiente es prácticamente cero para cualquier valor de <span class="math inline">\(\lambda\)</span>, mientras que para un valor de <span class="math inline">\(log(\lambda)=3\)</span> parece que hay sólo <span class="math inline">\(4\)</span> coeficientes distintos de <span class="math inline">\(0\)</span>.</p>
<p>La función <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> se puede utilizar con diferentes propósitos. Por ejemplo, se pueden obtener los coeficientes de la regresión <em>ridge</em> para un valor específico de <span class="math inline">\(\lambda\)</span>, por ejemplo <span class="math inline">\(\lambda=50\)</span>:</p>
<div class="sourceCode" id="cb279"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">ridge_mod</span>, s <span class="op">=</span> <span class="fl">50</span>, type <span class="op">=</span> <span class="st">"coefficients"</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">20</span>, <span class="op">]</span></span></code></pre></div>
<p>En lo que sigue, se ilustra el ajuste de una regresión ridge y se computa el <span class="math inline">\(MSE\)</span> de predicción para distintos valores de <span class="math inline">\(\lambda\)</span>. Primeramente, se divide el conjunto de datos en un subconjunto de entrenamiento y otro de test:</p>
<div class="sourceCode" id="cb280"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">entreno</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">/</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">test</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="op">-</span><span class="va">entreno</span><span class="op">)</span></span>
<span><span class="va">y_test</span> <span class="op">&lt;-</span> <span class="va">y</span><span class="op">[</span><span class="va">test</span><span class="op">]</span></span></code></pre></div>
<p>A continuación se ajusta la regresión ridge a los datos del subconjunto de entrenamiento usando un valor específico de <span class="math inline">\(\lambda\)</span> (por ejemplo <span class="math inline">\(\lambda=4\)</span>). Posteriormente, se evalúa su <span class="math inline">\(MSE\)</span> con los datos del subconjunto de test. Para ello se usa la función <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code>. En este caso, para obtener las predicciones para la muestra de test, se reemplaza <code>type =</code> <code>"coefficients"</code> por el argumento <code>newx</code>.</p>
<div class="sourceCode" id="cb281"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ridge_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="va">entreno</span>, <span class="op">]</span>, <span class="va">y</span><span class="op">[</span><span class="va">entreno</span><span class="op">]</span>, alpha <span class="op">=</span> <span class="fl">0</span>, lambda <span class="op">=</span> <span class="va">grid</span><span class="op">)</span></span>
<span><span class="va">ridge_pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">ridge_mod</span>, s <span class="op">=</span> <span class="fl">4</span>, newx <span class="op">=</span> <span class="va">x</span><span class="op">[</span><span class="va">test</span>, <span class="op">]</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">ridge_pred</span> <span class="op">-</span> <span class="va">y_test</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 142226.5</span></span></code></pre></div>
<p>El <span class="math inline">\(MSE\)</span> es <span class="math inline">\(142{,}199\)</span>. Si se usa un valor muy alto de <span class="math inline">\(\lambda\)</span>, por ejemplo <span class="math inline">\(10^{10}\)</span> (esto sería equivalente a ajustar un modelo solo con la ordenada en el origen), el resultado es muy distinto:</p>
<div class="sourceCode" id="cb282"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ridge_pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">ridge_mod</span>, s <span class="op">=</span> <span class="fl">1e10</span>, newx <span class="op">=</span> <span class="va">x</span><span class="op">[</span><span class="va">test</span>, <span class="op">]</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">ridge_pred</span> <span class="op">-</span> <span class="va">y_test</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 224669.8</span></span></code></pre></div>
<p>Por lo tanto, en este caso, ajustar un modelo de regresión ridge con <span class="math inline">\(\lambda=4\)</span> da un <span class="math inline">\(MSE\)</span> mucho menor que el obtenido cuando el modelo sólo contiene la ordenada en el origen.</p>
<p>A continuación se compara el resultado para <span class="math inline">\(\lambda=4\)</span> con el obtenido utilizando mínimos cuadrados ordinarios (<span class="math inline">\(\lambda=0\)</span>).</p>
<div class="sourceCode" id="cb283"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ridge_pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">ridge_mod</span>, s <span class="op">=</span> <span class="fl">0</span>, newx <span class="op">=</span> <span class="va">x</span><span class="op">[</span><span class="va">test</span>, <span class="op">]</span>, exact <span class="op">=</span> <span class="cn">T</span>, </span>
<span>                      x <span class="op">=</span> <span class="va">x</span><span class="op">[</span><span class="va">entreno</span>, <span class="op">]</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">[</span><span class="va">entreno</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">ridge_pred</span> <span class="op">-</span> <span class="va">y_test</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 167018.2</span></span></code></pre></div>
<p>Se observa que el <span class="math inline">\(MSE\)</span> es menor cuando se usa regresión <em>ridge</em> (con <span class="math inline">\(\lambda=4\)</span>) que cuando se usan mínimos cuadrados ordinarios.</p>
<p>Hasta ahora se ha elegido el valor <span class="math inline">\(\lambda=4\)</span> de forma arbitraria. En la siguiente sección se aborda la cuestión de cómo seleccionar el valor de dicho parámetro de una forma automática.</p>
</div>
</div>
<div id="selección-del-parámetro-de-penalización" class="section level3" number="19.3.2">
<h3>
<span class="header-section-number">19.3.2</span> Selección del parámetro de penalización<a class="anchor" aria-label="anchor" href="#selecci%C3%B3n-del-par%C3%A1metro-de-penalizaci%C3%B3n"><i class="fas fa-link"></i></a>
</h3>
<p>En la subsección anterior se ha visto que el valor de <span class="math inline">\(\lambda\)</span> tiene un gran impacto en los resultados obtenidos cuando se utiliza un modelo con penalización.</p>
<p>Una buena manera de elegir <span class="math inline">\(\lambda\)</span> es usar validación cruzada (<em>cross-validation</em>). Por ejemplo, se puede usar validación cruzada con 10 grupos (<em>k-fold cross-validation</em>) :</p>
<ul>
<li>Se dividen los datos en <span class="math inline">\(k\)</span> grupos, se ajusta el modelo ridge a <span class="math inline">\(k-1\)</span> de esos grupos (para una rejilla de valores de <span class="math inline">\(\lambda\)</span>) y se calcula el error de predicción para el otro grupo.</li>
<li>La acción anterior se repite tomando como muestra de test cada uno de los <span class="math inline">\(k\)</span> grupos y se suman los errores de predicción.</li>
<li>Al final se dispondrá de una curva con los errores para cada valor de <span class="math inline">\(\lambda\)</span> y se elegirá el que dé el mínimo error.</li>
</ul>
<p>En la práctica, el procedimiento anterior se puede hacer con la función <code><a href="https://glmnet.stanford.edu/reference/cv.glmnet.html">cv.glmnet()</a></code>. Por defecto, esta función usa un <span class="math inline">\(10\)</span>-<em>fold cross-validation</em>, pero el número de grupos se puede cambiar usando el argumento <code>nfolds</code>.</p>
<p>En el ejemplo del béisbol:</p>
<div class="sourceCode" id="cb284"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">cv_out</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/cv.glmnet.html">cv.glmnet</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="va">entreno</span>, <span class="op">]</span>, <span class="va">y</span><span class="op">[</span><span class="va">entreno</span><span class="op">]</span>, alpha <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">cv_out</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:sparse2"></span>
<img src="150020_sparse_prm_files/figure-html/sparse2-1.png" alt="Valor del error cudrático medio y su intervalo de confianza (calculado sobre los 10 grupos) para distintos valores del parámetro de penalización" width="60%"><p class="caption">
Figura 19.4: Valor del error cudrático medio y su intervalo de confianza (calculado sobre los 10 grupos) para distintos valores del parámetro de penalización
</p>
</div>
<div class="sourceCode" id="cb285"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mejorlam</span> <span class="op">&lt;-</span> <span class="va">cv_out</span><span class="op">$</span><span class="va">lambda.min</span></span>
<span><span class="va">mejorlam</span></span>
<span><span class="co">#&gt; [1] 326.0828</span></span></code></pre></div>
<p>En la Fig. <a href="cap-sparse.html#fig:sparse2">19.4</a>, los puntos rojos corresponden a la media del <span class="math inline">\(MSE\)</span> para los 10 grupos y las barras superior e inferior corresponden a esa cantidad más/menos una desviación estándar (el ancho será tanto menor cuanto mayor sea el número de grupos). La primera línea vertical corresponde al valor de <span class="math inline">\(\lambda\)</span> que hace mínimo el <span class="math inline">\(MSE\)</span> y la segunda es el valor que está a una distancia de una desviación típica del <span class="math inline">\(\lambda\)</span> mínimo (usar este último valor podría ser una buena opción para evitar el sobre-ajuste, es decir dejar demasiadas variables en el modelo).</p>
<p>El valor mínimo del <span class="math inline">\(MSE\)</span> se calcula como sigue:</p>
<div class="sourceCode" id="cb286"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ridge_pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">ridge_mod</span>, s <span class="op">=</span> <span class="va">mejorlam</span>, newx <span class="op">=</span> <span class="va">x</span><span class="op">[</span><span class="va">test</span>, <span class="op">]</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">ridge_pred</span> <span class="op">-</span> <span class="va">y_test</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 139833.6</span></span></code></pre></div>
<p>Como se puede apreciar, hay una apreciable mejora en el error de predicción que se había obtenido cuando el parámetro de penalización se había fijado en <span class="math inline">\(\lambda=4\)</span>.</p>
</div>
<div id="regresión-lasso" class="section level3" number="19.3.3">
<h3>
<span class="header-section-number">19.3.3</span> Regresión <em>lasso</em><a class="anchor" aria-label="anchor" href="#regresi%C3%B3n-lasso"><i class="fas fa-link"></i></a>
</h3>
<p>Uno de los puntos débiles de la regresión <em>ridge</em> es que no hace selección de variables (los coeficientes pueden estar próximos a cero pero no ser exactamente cero). En el modelo final se incluyen todos los coeficientes y, por lo tanto, <strong>la regresión <em>ridge</em> sólo es útil cuando la mayoría de las variables predictoras tienen un impacto significativo en la respuesta</strong>.</p>
<p>La regresión <em>lasso</em> (least absolute srinkage and selection operator, por sus siglas en inglés) , introducida por <span class="citation">R. Tibshirani (<a href="referncias.html#ref-Tibshirani96">1996</a>)</span>, es una alternativa a la regresión <em>ridge</em> cuyo objetivo es precisamente corregir la limitación anteriormente mencionada de la regresión <em>ridge</em>, y es útil cuando la mayoría de las variables predictoras no son relevantes en el modelo. Los coeficientes <em>lasso</em>, <span class="math inline">\(\hat \beta^L\)</span>, minimizan la siguiente cantidad:
<span class="math display">\[\sum_{i=1}^n \left ( y_i-\beta_0-\sum_{j=1}^p \beta_jx_{ij}\right )^2+\lambda \sum_{j=1}^p |\beta_j|=RSS+\lambda \sum_{j=1}^p |\beta_j|.\]</span>
Ahora los coeficientes se <em>contraen</em> hacia cero utilizando la suma de los coeficientes en valor absoluto en vez de la suma de los cuadrados de dichos coeficientes. A esta norma se le llama <span class="math inline">\(l_1\)</span>, <span class="math inline">\(\|\beta\|_1=\sum_{j=1}^p|\beta_j|\)</span>. El cambio que supone es sutil pero importante. En ambos casos los coeficientes se contraen hacia <span class="math inline">\(0\)</span>, pero en el caso de la regresión <em>lasso</em> cuando <span class="math inline">\(\lambda\)</span> es suficientemente grande los coeficientes serán <span class="math inline">\(0\)</span>, de modo que se está haciendo una selección de variables. Por consiguiente, la regresión <em>lasso</em> anulará los coeficientes de las variables que no son importantes a la hora de explicar el comportamiento de la variable respuesta mediante un valor de <span class="math inline">\(\lambda\)</span> es suficientemente grande. En este sentido el modelo de regresión <span class="math inline">\(lasso\)</span> es lo que se llama un <strong>modelo sparse</strong> (un modelo con un número <em>sparse</em>, o escaso, de parámetros).</p>
<p><strong>¿Por qué <em>lasso</em> hace que los coeficiente se contraigan exactamente hacia cero?</strong>
Para entenderlo se va a ver una formulación equivalente a la de los mínimos cuadrados penalizados en el caso de la regresión <em>lasso</em>:
<span class="math display">\[\sum_{i=1}^n \left ( y_i-\beta_0-\sum_{j=1}^p \beta_jx_{ij}\right )^2\quad \text{sujeto a} \quad \sum_{j=1}^p |\beta_j|&lt;s.\]</span>
Dicha formulación equivalente corresponde a mínimos cuadrados con una restricción, o lo que es lo mismo, con un <em>presupuesto</em> en la norma <span class="math inline">\(l_1\)</span> sobre los coeficientes. Las dos formulaciones son equivalentes en el sentido de que si se tiene un <em>presupuesto</em> <span class="math inline">\(s\)</span>, habrá un <span class="math inline">\(\lambda\)</span> en la primera formulación que corresponda al presupuesto <span class="math inline">\(s\)</span> en la segunda, y viceversa.
Supóngase que se hacen mínimos cuadrados y se obtienen las estimaciones de los parámetros (coeficientes) tal que la suma de sus valores absolutos es 10,
pero alguien dice que nuestro <em>presupuesto</em> es <span class="math inline">\(5\)</span> (la suma de los valores absolutos de los coeficientes no puede ser mayor que esa cantidad). Entonces, hay que resolver el problema de mínimos cuadrados pero los coeficientes no pueden tomar cualquier valor, ya que se tiene una restricción sobre los mismos. Cuanto más pequeño sea el <em>presupuesto</em>, más próximos a cero serán los coeficientes. Si el <em>presupuesto</em> es <span class="math inline">\(0\)</span>, todos los coeficientes serán también <span class="math inline">\(0\)</span>. Si el presupuesto es muy alto, hay libertad para que los coeficientes tomen el valor que quieran, y se estaría en el caso de mínimos cuadrados. El <em>presupuesto</em> impone que haya un equilibrio entre el ajuste a los datos y el tamaño de los coeficientes.</p>
<p>La Fig. <a href="cap-sparse.html#fig:lassoridge">19.5</a> (tomada de <span class="citation">G. James et al. (<a href="referncias.html#ref-james2013introduction">2013</a>)</span>) muestra por qué el modelo de regresión lasso es <em>sparse</em>. El gráfico corresponde a un modelo de regresión con dos variables predictoras. El punto donde está el vector de coeficientes, <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, es donde se alcanza el valor mínimo de la suma los cuadrados de los residuos del modelo (<span class="math inline">\(RSS\)</span>) y los contornos son combinaciones de valores de <span class="math inline">\(\beta_1\)</span> y <span class="math inline">\(\beta_2\)</span> que dan lugar al mismo valor de <span class="math inline">\(RSS\)</span>, pero que ya no sería el mínimo. Las regiones de restricción son <span class="math inline">\(|\beta_1|+|\beta_2|&lt;s\)</span> (<em>lasso</em>) y <span class="math inline">\(\beta_1^2 +\beta_2^2&lt;s\)</span> (<em>ridge</em>). En el caso de la regresión <em>ridge</em>, el <em>presupuesto</em> es el radio del círculo y la regresión <em>ridge</em> busca el primer lugar en el que el contorno toca a la región de restricción, pero, al ser un círculo, difícilmente uno u otro coeficiente va a ser <span class="math inline">\(0\)</span>. En el caso de la regresión <em>lasso</em>, la región de restricción tiene forma de diamante y, por lo tanto, tiene vértices. Como puede apreciarse, en la Fig. <a href="cap-sparse.html#fig:lassoridge">19.5</a> el contorno toca a la región de restricción en el caso en que <span class="math inline">\(\beta_1=0\)</span>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:lassoridge"></span>
<img src="img/lasso_ridge.png" alt="Contornos (rojo) de $RSS$ y regiones de restricción (en azul) para la regresión lasso (izquierda) y ridge (derecha)" width="60%"><p class="caption">
Figura 19.5: Contornos (rojo) de <span class="math inline">\(RSS\)</span> y regiones de restricción (en azul) para la regresión lasso (izquierda) y ridge (derecha)
</p>
</div>
<p>Se vuelve al ejemplo del béisbol para mostrar la regresión lasso; en este caso el argumento <span class="math inline">\(\alpha\)</span> toma valor <span class="math inline">\(1\)</span> (<span class="math inline">\(0\)</span> en el caso de la regresión <em>ridge</em>.</p>
<div class="sourceCode" id="cb287"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">lasso_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="va">entreno</span>, <span class="op">]</span>, <span class="va">y</span><span class="op">[</span><span class="va">entreno</span><span class="op">]</span>, alpha <span class="op">=</span> <span class="fl">1</span>, lambda <span class="op">=</span> <span class="va">grid</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">lasso_mod</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:sparse3"></span>
<img src="150020_sparse_prm_files/figure-html/sparse3-1.png" alt="Valor de los parámetros estimados para distintos valores de la penalización (que depende del parámetros de penalización)" width="60%"><p class="caption">
Figura 19.6: Valor de los parámetros estimados para distintos valores de la penalización (que depende del parámetros de penalización)
</p>
</div>
<p>En la Fig. <a href="cap-sparse.html#fig:sparse3">19.6</a> se puede ver que, dependiendo del valor del parámetro de penalización, algunos de los coeficientes se hacen exactamente <span class="math inline">\(0\)</span>. Para elegir el valor de dicho parámetro y calcular el <span class="math inline">\(MSE\)</span> resultante en el conjunto de test se procede como sigue:</p>
<div class="sourceCode" id="cb288"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">cv_out</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/cv.glmnet.html">cv.glmnet</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="va">entreno</span>, <span class="op">]</span>, <span class="va">y</span><span class="op">[</span><span class="va">entreno</span><span class="op">]</span>, alpha <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">cv_out</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:sparse4"></span>
<img src="150020_sparse_prm_files/figure-html/sparse4-1.png" alt="Valor del error cuadrático medio y su intervalo de confianza para distintos valores del parámetro de penalización" width="60%"><p class="caption">
Figura 19.7: Valor del error cuadrático medio y su intervalo de confianza para distintos valores del parámetro de penalización
</p>
</div>
<div class="sourceCode" id="cb289"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mejorlab</span> <span class="op">&lt;-</span> <span class="va">cv_out</span><span class="op">$</span><span class="va">lambda.min</span></span>
<span><span class="va">lasso.pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">lasso_mod</span>, s <span class="op">=</span> <span class="va">mejorlab</span>, newx <span class="op">=</span> <span class="va">x</span><span class="op">[</span><span class="va">test</span>, <span class="op">]</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">lasso.pred</span> <span class="op">-</span> <span class="va">y_test</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 143673.6</span></span></code></pre></div>
<p>Este valor es bastante más bajo que <span class="math inline">\(MSE\)</span> en la muestra de test en el caso de mínimos cuadrados ordinarios <span class="math inline">\((224.666,8\)</span>) y bastante parecido al obtenido con la regresión <em>ridge</em> cuando el parámetro de penalización se elige mediante validación cruzada: <span class="math inline">\(139,856,6\)</span>). Sin embargo, la regresión <em>lasso</em> tiene una ventaja importante con respecto a la regresión <em>ridge</em> ya que los coeficientes estimados son <em>sparse</em>. En los resultados que se muestran a continuación, se puede observar que 10 de los 20 coeficientes estimados son <span class="math inline">\(0\)</span>. Por lo tanto, el modelo <em>lasso</em> con <span class="math inline">\(\lambda\)</span> elegido mediante validación cruzada contiene sólo nueve variables predictoras.</p>
<div class="sourceCode" id="cb290"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">out</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, alpha <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">lasso_coef</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">out</span>, type <span class="op">=</span> <span class="st">"coefficients"</span>, s <span class="op">=</span> <span class="va">mejorlab</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">20</span>, <span class="op">]</span></span>
<span><span class="va">lasso_coef</span><span class="op">[</span><span class="va">lasso_coef</span> <span class="op">!=</span> <span class="fl">0</span><span class="op">]</span></span>
<span><span class="co">#&gt;   (Intercept)          Hits         Walks        CHmRun         CRuns </span></span>
<span><span class="co">#&gt;   -3.04787656    2.02551572    2.26853781    0.01647106    0.21177390 </span></span>
<span><span class="co">#&gt;          CRBI       LeagueN     DivisionW       PutOuts        Errors </span></span>
<span><span class="co">#&gt;    0.41944632   20.48456551 -116.59062083    0.23718459   -0.94739923</span></span></code></pre></div>
</div>
<div id="elastic-net" class="section level3" number="19.3.4">
<h3>
<span class="header-section-number">19.3.4</span> <em>Elastic net</em> <a class="anchor" aria-label="anchor" href="#elastic-net"><i class="fas fa-link"></i></a>
</h3>
<p>Uno de los problemas de la regresión <em>lasso</em> es cuando hay variables predictoras correladas entre sí, pues elegirá una de ellas (y los coeficientes de las demás los hará cero) sin un criterio objetivo. Además, supóngase que se está en una situación en la que el número de variables <span class="math inline">\(p\)</span> es mayor que el número de observaciones <span class="math inline">\(n\)</span>; en este caso la regresión <em>lasso</em> elegiría como mucho <span class="math inline">\(n\)</span> variables; mientras que la regresión <em>ridge</em> las utilizaría todas, aumentando la complejidad del modelo (esto en algunos casos puede ser lo deseable o no). <em>Elastic net</em> <span class="citation">(<a href="referncias.html#ref-Zou2005">Zou and Hastie 2005</a>)</span> es una generalización de los métodos anteriores que combina las penalizaciones de las regresiones <em>ridge</em> y <em>lasso</em>:
<span class="math display">\[\sum_{i=1}^n \left ( y_i-\beta_0-\sum_{j=1}^p \beta_jx_{ij}\right )^2+\lambda_1 \sum_{j=1}^p \beta_j^2+\lambda_2 \sum_{j=1}^p |\beta_j|.\]</span>
También aparece en muchas ocasiones de esta otra forma:
<span class="math display">\[\sum_{i=1}^n \left ( y_i-\beta_0-\sum_{j=1}^p \beta_jx_{ij}\right )^2+\lambda \left [ \frac{1}{2} (1-\alpha)\sum_{j=1}^p \beta_j^2+\alpha \sum_{j=1}^p |\beta_j|\right ],\]</span>
donde <span class="math inline">\(\alpha\in [0,1]\)</span>. El parámetro <span class="math inline">\(\alpha\)</span> es que gobierna la combinación de las dos penalizaciones, mientras que <span class="math inline">\(\lambda\)</span> es el que controla la cantidad de penalización. Si <span class="math inline">\(\alpha=0\)</span> se está en el caso de la regresión <em>ridge</em>; <span class="math inline">\(\alpha=1\)</span> lleva a la regresión <em>lasso</em>.</p>
<p>La función <code><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet()</a></code> también sirve para ajustar <em>elastic net</em>, pero el parámetro <span class="math inline">\(\alpha\)</span> hay que elegirlo a priori. Otra opción es utilizar el paquete <code>caret</code> para hacer validación cruzada sobre <span class="math inline">\(\alpha\)</span> y <span class="math inline">\(\lambda\)</span> simultáneamente:</p>
<div class="sourceCode" id="cb291"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/topepo/caret/">"caret"</a></span><span class="op">)</span></span>
<span><span class="va">cv_glmnet</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/train.html">train</a></span><span class="op">(</span></span>
<span>  x <span class="op">=</span> <span class="va">x</span><span class="op">[</span><span class="va">entreno</span>, <span class="op">]</span>,</span>
<span>  y <span class="op">=</span> <span class="va">y</span><span class="op">[</span><span class="va">entreno</span><span class="op">]</span>,</span>
<span>  method <span class="op">=</span> <span class="st">"glmnet"</span>,</span>
<span>  trControl <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/trainControl.html">trainControl</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"cv"</span>, number <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>,</span>
<span>  tuneLength <span class="op">=</span> <span class="fl">10</span></span>
<span><span class="op">)</span></span>
<span><span class="co"># modelo con el MSE más pequeño</span></span>
<span><span class="va">cv_glmnet</span><span class="op">$</span><span class="va">bestTune</span></span>
<span><span class="co">#&gt;   alpha   lambda</span></span>
<span><span class="co">#&gt; 9   0.1 99.12337</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">cv_glmnet</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:sparse5"></span>
<img src="150020_sparse_prm_files/figure-html/sparse5-1.png" alt="Valor de la raíz cuadrada del error cudrático medio para distintas combinaciones de $\alpha$ y $\lambda$" width="60%"><p class="caption">
Figura 19.8: Valor de la raíz cuadrada del error cudrático medio para distintas combinaciones de <span class="math inline">\(\alpha\)</span> y <span class="math inline">\(\lambda\)</span>
</p>
</div>
<p>La Fig. <a href="cap-sparse.html#fig:sparse5">19.8</a> muestra como la combinación de <span class="math inline">\(\alpha\)</span> y <span class="math inline">\(\lambda\)</span> da lugar a diferentes <span class="math inline">\(MSE\)</span> (en la figura aparece el <span class="math inline">\(RMSE\)</span>, o sea, su raíz cuadrada). Cada línea corresponde a un valor de <span class="math inline">\(\lambda\)</span> distinto, y en el eje <span class="math inline">\(x\)</span> se representan los valores de <span class="math inline">\(\alpha\)</span>.</p>
<p>A continuación se calcula el valor del <span class="math inline">\(MSE\)</span> en el conjunto de test para el modelo <em>elastic net</em> con <span class="math inline">\(\alpha=0,1\)</span> y <span class="math inline">\(\lambda=99,337\)</span>, que son los valores de <span class="math inline">\(\alpha\)</span> y <span class="math inline">\(\lambda\)</span> que hacen mínimo dicho <span class="math inline">\(MSE\)</span>:</p>
<div class="sourceCode" id="cb292"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">elastic_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/glmnet.html">glmnet</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="va">entreno</span>, <span class="op">]</span>, <span class="va">y</span><span class="op">[</span><span class="va">entreno</span><span class="op">]</span>, alpha <span class="op">=</span> <span class="va">cv_glmnet</span><span class="op">$</span><span class="va">bestTune</span><span class="op">$</span><span class="va">alpha</span><span class="op">)</span></span>
<span><span class="va">elastic_pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">elastic_mod</span>, newx <span class="op">=</span> <span class="va">x</span><span class="op">[</span><span class="va">test</span>, <span class="op">]</span>, s <span class="op">=</span> <span class="va">cv_glmnet</span><span class="op">$</span><span class="va">bestTune</span><span class="op">$</span><span class="va">lambda</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">elastic_pred</span> <span class="op">-</span> <span class="va">y_test</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 141626.1</span></span></code></pre></div>
<p>Como puede comprobarse, es peor que el de la regresión <em>ridge</em> pero mejor que el de <em>lasso</em>.</p>
<p>Por tanto, si no se quiere hacer ningún tipo de selección debe estimar una regresión <em>ridge</em> y si se si se quiere reducir al máximo el número de variables predictoras se debe estimar una regresión <em>lasso</em> (a costa de que aumente el <span class="math inline">\(MSE\)</span> en el conjunto de test). El el equilibrio viene de la mano del modelo <strong>elastic-net</strong>, que hace selección de variables pero no aumenta el <span class="math inline">\(MSE\)</span> de predicción.</p>
<p>Existen otros métodos de penalización que se derivan de estos, como el <em>group lasso</em>, el <em>sparse group-lasso</em>, etc. Se puede encontrar información sobre ellos en <span class="citation">(<a href="referncias.html#ref-hastiebook2015">Hastie and Tibshirani 2015</a>)</span>.</p>
</div>
<div id="resumen-17" class="section level3 unnumbered infobox_resume">
<h3>Resumen<a class="anchor" aria-label="anchor" href="#resumen-17"><i class="fas fa-link"></i></a>
</h3>
<p>En este capítulo se introducen una serie de técnicas para mejorar la predicción y la interpretabilidad de los modelos de regresión. En particular:</p>
<ul>
<li><p>Se muestra el uso de la técnica de selección del mejor subconjunto de variables en el modelo, así como los métodos <em>stepwise</em>.</p></li>
<li><p>Se presentan 3 métodos tipo <em>shrinkage</em>: regresión <em>ridge</em>, <em>lasso</em> y <em>elastic net</em>, bien para la selección de variables, o para solventar problemas de multicolinealidad en el modelo.</p></li>
<li><p>Se muestra cómo seleccionar el parámetro de penalización (o de conbinación de penalizaciones en el caso de la regresión <em>elastic net</em>) que controla la regresión penalizada.</p></li>
<li><p>Se ilustra el uso de todas las metodologías propuestas en el capítulo mediante el análisis de un caso práctico.</p></li>
</ul>
</div>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="cap-mxm.html"><span class="header-section-number">18</span> Modelos mixtos</a></div>
<div class="next"><a href="cap-series-temp.html"><span class="header-section-number">20</span> Modelización de series temporales</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="Índice capítulo"><h2>Índice capítulo</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#cap-sparse"><span class="header-section-number">19</span> Modelos sparse y métodos penalizados de regresión</a></li>
<li><a class="nav-link" href="#introducci%C3%B3n-9"><span class="header-section-number">19.1</span> Introducción</a></li>
<li>
<a class="nav-link" href="#selecci%C3%B3n-del-mejor-subconjunto"><span class="header-section-number">19.2</span> Selección del mejor subconjunto</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#procedimiento-con-r-la-funci%C3%B3n-regsubset"><span class="header-section-number">19.2.1</span> Procedimiento con R: la función regsubset()</a></li>
<li><a class="nav-link" href="#selecci%C3%B3n-stepwise"><span class="header-section-number">19.2.2</span> Selección stepwise</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#m%C3%A9todos-shrinkage"><span class="header-section-number">19.3</span> Métodos shrinkage</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#regresi%C3%B3n-ridge"><span class="header-section-number">19.3.1</span> Regresión ridge</a></li>
<li><a class="nav-link" href="#selecci%C3%B3n-del-par%C3%A1metro-de-penalizaci%C3%B3n"><span class="header-section-number">19.3.2</span> Selección del parámetro de penalización</a></li>
<li><a class="nav-link" href="#regresi%C3%B3n-lasso"><span class="header-section-number">19.3.3</span> Regresión lasso</a></li>
<li><a class="nav-link" href="#elastic-net"><span class="header-section-number">19.3.4</span> Elastic net </a></li>
<li><a class="nav-link" href="#resumen-17">Resumen</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Fundamentos de ciencia de datos con R</strong>" coordinado por <a href="https://blog.uclm.es/gemafaviles/" class="text-light">Gema Fernández-Avilés y José-María Montero</a>. Generado por última vez el día 2023-06-16.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>Este libro ha sido generado con el paquete de R <a class="text-light" href="https://bookdown.org">bookdown</a>.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
