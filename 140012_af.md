---
#   bibliography: bib/bookJMML-JLAN.bib
---

# Análisis factorial

*José-María Montero*$^{a}$ y *José Luis Alfaro Navarro*$^{a}$

$^{a}$Universidad de Castilla-La Mancha

## Introducción {#introaf}

Según el trabajo pionero de @harman1976, el objeto del **Análisis Factorial**\index{análisis!factorial} (AF) es la representación de una variable $X_j$ en términos de varios factores subyacentes \index{factores!subyacentes} no observables[^af-0]. En el marco lineal, y considerando $p$ variables[^af-1], hay varias alternativas dependiendo del objetivo que se pretenda:


[^af-0]: Se asumirá la representación lineal, por sencillez, pero puede ser cualquier otra.
[^af-1]: Por las mismas razones que en el análisis de componentes principales (ACP), se trabaja con las \index{variables estandarizadas} variables estandarizadas; véase Cap. \@ref(acp).

- La captura de la mayor cantidad posible de la varianza de dichas variables (o "explicación" de su varianza). 
- La mejor reproducción (o "explicación") de sus correlaciones observadas.



A modo introductorio, supónganse dos variables politómicas que surgen de las respuestas de $N$ futbolistas profesionales a dos preguntas: (1) ¿Está usted a gusto en el club? y (2) ¿Se quedaría en el club la siguiente temporada? Las posibles respuestas son: 1, 2, 3, 4, 5 (1 "en total desacuerdo" y 5 "totalmente de acuerdo").

Cada variable tiene su varianza (nula si todos los futbolistas opinasen igual y máxima si la mitad marcase el 1 y la otra mitad el 5). Esta varianza puede ser *común* o *compartida* por las dos variables, o no. Lo normal es que cuanto más a gusto estén los futbolistas en su club mayor sea su deseo de permanecer en él la siguiente temporada, por lo que gran parte de la variabilidad \index{variabilidad} de cada una de las variables sería compartida (ya que la relación -lineal- entre ellas es positiva). El resto de la variabilidad sería *específica* de cada variable (puede que un futbolista esté muy bien en el club, pero quiera ir a otro más prestigioso; o que esté mal, pero a su familia le encante la ciudad) o *residual* (normalmente debida a factores de medida). El porcentaje de *varianza compartida* se mide a través del coeficiente de determinación lineal, $r^2$. El resto, hasta la varianza unidad, o el 100%, es *varianza única* de cada variable, que incluye tanto la específica como la residual.

\index{varianza!común}
\index{varianza!compartida}
\index{varianza!específica}
\index{varianza!residual}
\index{varianza!única}

De acuerdo con @santiagodelafuente2011, en el AF caben dos enfoques:[^af-2]

-   El análisis de toda la varianza (común y no común).
-   El análisis, únicamente, de la varianza común. \index{varianza!común}

Ambos caben bajo el paraguas genérico del AF; ambos se basan en las relaciones entre las variables para identificar grupos de ellas asociadas entre sí. Sin embargo, del primero se ocupa el ACP (Cap. \@ref(acp)) y, si se parte de la matriz de correlaciones (cuyas entradas fuera de la diagonal principal, al cuadrado, indican la proporción de varianza compartida por las variables que se cruzan en dicha entrada),  ésta lleva unos en la diagonal principal. Al segundo se le aplica la denominación de AF y en la matriz de correlaciones \index{matriz!de correlaciones} se sustituyen  los unos de la diagonal principal por la varianza que cada variable comparte con las demás (su **comunalidad**). Por eso se dice que el objetivo del AF es la explicación de la varianza compartida \index{varianza!compartida} o común de las variables en estudio mediante una serie de **factores comunes** latentes.[^af-3]

\index{comunalidad}
\index{factores!comunes}

[^af-2]: No son los únicos.
[^af-3]: Ambos enfoques dan resultados similares cuando hay más de 30 variables y las comunalidades (véase Sec. \@ref(modelobasicoaf)) son superiores a 0,70, y se interpretan de manera casi idéntica.

El AF puede ser exploratorio o confirmatorio. En el primero no se establecen consideraciones a priori sobre el número de factores comunes a extraer, sino que éste se determina a lo largo del análisis. Por el contrario, en el segundo se trata de contrastar hipótesis relativas al número de factores comunes, así como sobre qué variables serán agrupadas o tendrán más peso en cada factor. Una práctica habitual es validar mediante el *análisis factorial confirmatorio*\index{análisis!factorial} los modelos teóricos basados en los resultados del *análisis factorial exploratorio*. Sin embargo, @rodriguez2000validez alertan de los peligros de esta práctica. Este capítulo considera la versión exploratoria del AF.

\index{análisis!factorial exploratorio}
\index{análisis!factorial confirmatorio}
A efectos prácticos, se utilizará la base de datos `TIC2021` del paquete `CDR`, ya trabajada en Cap. \@ref(acp) para el ACP, relativa al uso (por empresas e individuos) y equipación (de los hogares) de las TIC en los países de la UE-27, así como la librería `psych` [@Revelle2022] de **R**.


```r
library("psych")
library("CDR")
data("TIC2021")
```

## Elementos teóricos del análisis factorial

### Modelo básico y terminología{#modelobasicoaf}

Considérense $p$ variables $\{X_1, X_2,..., X_p\}$ y $N$ elementos, objetos o individuos, siendo las matrices de datos, $\bf X$, y datos estandarizados, $\bf Z$, las siguientes:

$$\bf X=\left(\begin{matrix} x_{11} & x_{12} & \cdots &x_{1N}\\
x_{21}&x_{22}&\cdots&x_{2N}\\
\vdots&\vdots&\ddots     &\vdots\\
x_{p1}&x_{p2}&\cdots&x_{pN}\\
\end {matrix}\right),\quad
\bf Z=\left(\begin{matrix} z_{11} & z_{12} & \cdots &z_{1N}\\
z_{21}&z_{22}&\cdots&z_{2N}\\
\vdots&\vdots&\ddots     &\vdots\\
z_{p1}&z_{p2}&\cdots&z_{pN}\\
\end {matrix}\right),$$

donde el primer subíndice indica la variable y el segundo el elemento.

Mientras que el enfoque de componentes principales está representado por: 
\begin{equation} Z_{j}=a_{j1}F_1+a_{j2}F_2+ \cdots +a_{jp}F_p, \quad j=1,2,\cdots,p,
(\#eq:eqaf1)
\end{equation}
en el enfoque AF clásico el modelo teórico es: 
\begin{equation}
Z_{j}=a_{j1}F_1+a_{j2}F_2+ \cdots +a_{jk}F_k + b_jSP_j+c_jE_j, \quad j=1,2,\cdots,p,
(\#eq:eqaf2)
\end{equation} 
donde $Z_{j}, \hspace{0,1cm} j=1,2,\cdots, p$, se modeliza, linealmente, en términos de $(i)$ $k\ll p$ **factores comunes**, $F_m,\hspace{0,1cm} m=1,2,\cdots,k$, que dan cuenta de la correlaciones entre las variables $Z_{j}, \hspace{0,1cm} j=1,2,\cdots, p$, y $(ii)$ un **factor específico**, $SP_j, \hspace{0,1cm} j=1,2,\cdots,p$, y  un término de error, $E_j, \hspace{0,1cm} j=1,2,\cdots,p,$ que dan cuenta de la **varianza no compartida** (específica y residual, respectivamente). Los coeficientes $a_{jm}$ se denominan **cargas factoriales** y, aunque su notación es igual que  en el modelo de componentes principales, no tienen por qué coincidir; el problema básico del AF es precisamente la estimación de dichas cargas. En lo que sigue, se aunarán el factor específico y el término de error de $Z_{j}$ en un **factor único**, $U_{j}$, con lo que: 

\index{componentes principales}
\index{cargas factoriales}
\index{factores!específicos}
\index{varianza!no compartida}
\index{factores!únicos}

\begin{equation}
Z_{j}=a_{j1}F_1+a_{j2}F_2+ \cdots +a_{jk}F_k + d_jU_j, \quad j=1,2,\cdots,p,
(\#eq:eqaf3)
\end{equation} 

Los supuestos del modelo \@ref(eq:eqaf3) son los siguientes: 

- Como en la práctica los factores comunes y únicos son desconocidos, sin pérdida de generalidad pueden suponerse con media cero y varianza unitaria;
- Los factores únicos se suponen independientes entre sí y de los factores comunes; \index{factores!comunes}
- Y dado que los factores involucrados en el modelo se consideran variables aleatorias, si se asume normalidad, e independencia de los factores comunes, $\{F_{1},F_{2},\cdots, F_k\}$ sigue una distribución normal multivariante y $Z_{j},\hspace{0,1cm} j=1,2,\cdots,p,$ una distribución normal.

En términos de valores observados, el modelo AF \@ref(eq:eqaf3) viene dado por:[^af-4]
\begin{equation}
z_{ji}=a_{j1}f_{1i}+a_{j2}f_{2i}+ \cdots +a_{jk}f_{ki} + d_ju_{ji}, \quad i=1,2,\cdots,N; \hspace{0.1cm} j=1,2,\cdots,p,
(\#eq:eqaf4)
\end{equation}
El modelo AF es muy parecido al de regresión lineal: una variable se describe como una combinación lineal de otro conjunto de variables más un residuo. Sin embargo, en el análisis de regresión las variables son observables, mientras que en el AF son construcciones hipotéticas que sólo pueden estimarse a partir de los datos observados. Los propios factores se estiman en una etapa posterior del análisis.

[^af-4]: Aunque en el modelo figuran explícitamente los valores de los factores, en la práctica hay que estimarlos. En otras versiones del AF estos valores se estiman conjuntamente con los parámetros.

En términos matriciales, y considerando:
$$\bf z=\left(\begin{matrix}Z{1}\\
Z_{2}\\
\vdots\\
Z_{p}\\
\end{matrix}\right),\quad \bf f=\left(\begin{matrix} 
F_{1}\\
F_{2}\\
\vdots\\
F_{k}\\
\end{matrix}\right),\quad \bf u=\left(\begin{matrix} U_{1}\\
U_{2}\\
\vdots\\
U_{p}\\
\end{matrix}\right),$$
$$\bf A=\left(\begin{matrix} a_{11} & a_{12} & \cdots &a_{1k}\\
a_{21}&a_{22}&\cdots&a_{2k}\\
\vdots&\vdots&\ddots     &\vdots\\
a_{p1}&a_{p2}&\cdots&a_{pk}\\
\end {matrix}\right),\quad \bf D=\left(\begin{matrix} d_{1} & 0 & \cdots &0\\
0&d_{2}&\cdots&0\\
\vdots&\vdots&\ddots     &\vdots\\
0&0&\cdots&d_{p}\\
\end {matrix}\right),$$
el modelo \@ref(eq:eqaf3) puede expresarse como $\bf z=\bf A \bf f +\bf D \bf u$. 


Centrándonos en el modelo \@ref(eq:eqaf3), la varianza de $Z_j$ viene dada por:
\begin{equation}
V(Z_j)=1= \sum_{m=1}^{k} a_{jm}^2+
2\sum_{m< q }^{k} a_{jm} a_{jq} r_{(F_{mi},F_{qi})}  +d_j^2,
(\#eq:eqaf4b)
\end{equation}
y si los factores comunes están incorrelados, $V(Z_j)=1= \sum_{m=1}^{k} a_{jm}^2+ d_j^2$. 

De la expresión \@ref(eq:eqaf4b) surgen las siguientes definiciones: 

- $a_{jm}^2$ es la contribución de $F_m$ a la varianza de $Z_j$.
- $V_m=\sum_{j=1}^{p}a_{jm}^2$ es la contribución de $F_m$ a suma de las varianzas de todas las variables $Z_j,\hspace{0,1cm} j=1,2,\cdots,p$.
- $V=\sum_{m=1}^{k}V_m$ es la contribución de todos los factores comunes a la varianza de todas las variables $Z_j,\hspace{0,1cm} j=1,2,\cdots,p$.
- $\frac{V} {p}$ es un indicador de la *completitud* del análisis factorial. \index{análisis!factorial}
- $h_j^2=a_{j1}^2+a_{j2}^2+\cdots+a_{jk}^2$ es la comunalidad \index{comunalidad} de $Z_j,\hspace{0,1cm} j=1,2,\cdots,p$, es decir la contribución de los factores comunes \index{factores!comunes} a la variabilidad \index{variabilidad} de $Z_j$. 
- $d_j^2$ es la *unicidad* (o varianza única) de $Z_j,\hspace{0,1cm} j=1,2,\cdots,p$, o contribución de $U_j$ a la varianza de $Z_j$. Es un indicador de la medida en la que los factores comunes fracasan a la hora de representar la varianza (unitaria) de $Z_j$. \index{unicidad} \index{varianza!única}
- Cuando se descompone el factor único \index{factores!únicos} en sus dos componentes (modelo \@ref(eq:eqaf2)), $b_j^2$ se denomina *especificidad* (o varianza específica) de $Z_j$ y es la varianza de $Z_j$ debida a la particular selección de las variables en el estudio, mientras que $c_j^2$ es la que se debe al error (normalmente de medida), que mide la "falta de fiabilidad".
\index{unicidad}
\index{especificidad}
\index{completitud}

### Patrón y estructura factoriales
Se denomina **patrón factorial** \index{patrón factorial} a la siguiente expansión del modelo \@ref(eq:eqaf3), 
\begin{equation}
\begin{split}
Z_{1}= a_{11}F_{1}+ a_{12}F_{2}+ \dotsb + a_{1k}F_{k}+ d_1U_1\\
Z_{2}= a_{21}F_{1} + a_{22}F_{2}+ \dotsb + a_{2k}F_{k}+d_2U_2 \\
\ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \ddots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \vdots \ \ \ \\
Z_{p}= a_{p1}F_{1}+ a_{p2}F_{2}+ \dotsb + a_{pk}F_{k}+ d_pU_p\\
\end{split}
(\#eq:eqaf5)
\end{equation}
o simplemente a la tabla, o matriz, con los coeficientes $a_{jm}$ y $d_j$ (o únicamente, caso habitual, a la *matriz de cargas* \index{matriz!de cargas} $\bf A$). $k$ determina la "complejidad del modelo".

Se denomina **estructura factorial** \index{estructura factorial} al siguiente conglomerado de $k+1$ conjuntos de $p$ ecuaciones lineales, en $\{a_{jm}\}$, los $k$ primeros, y en $\{d_{j}\}$, el último, $\hspace{0,1cm} j=1,2,\cdots, p; \hspace{0,1cm} m=1,2,\cdots, k$:[^af-5]

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- r_{Z_jF_1} & = a_{j1}+ a_{j2}r_{F_1F_2}+ \dotsb +a_{jm}r_{F_1F_m}+\dotsb + a_{jk}r_{F_1F_k}\\ -->
<!-- \ \ \ & \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \ddots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \\ -->
<!-- r_{Z_jF_m} & = a_{j1}r_{F_mF_1}+ a_{j2}r_{F_mF_2}+ \dotsb +a_{jm}+\dotsb + a_{jk}r_{F_mF_k}\\ -->
<!-- \ \ \ &  \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \ddots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \\ -->
<!-- r_{Z_jF_k} & = a_{j1}r_{F_kF_1}+ a_{j2}r_{F_kF_2}+ \dotsb +a_{jm}r_{F_kF_m}+\dotsb + a_{jk}\\ \\ -->
<!--  \ \ \ &  \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \ddots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \\ \\ -->
<!--  r_{Z_jU_j} & = d_j\\ -->
<!-- \end{split} -->
<!-- (\#eq:eqaf6) -->
<!-- \end{equation} -->

\begin{equation}
\begin{split}
r_{Z_jF_1} & = a_{j1}r_{F_1F_1}+ a_{j2}r_{F_1F_2}+ \dotsb +a_{jm}r_{F_1F_m}+\dotsb + a_{jk}r_{F_1F_k}\\
\ \ \ & \vdots \ \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ddots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ddots\ \ \ \ \ \ \ \ \vdots \ \ \ \\
r_{Z_jF_m} & = a_{j1}r_{F_mF_1}+ a_{j2}r_{F_mF_2}+ \dotsb +a_{jm}r_{F_mF_m}+\dotsb + a_{jk}r_{F_mF_k}\\
\ \ \ &  \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ddots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ddots\ \ \ \ \ \ \ \vdots \ \ \ \\
r_{Z_jF_k} & = a_{j1}r_{F_kF_1}+ a_{j2}r_{F_kF_2}+ \dotsb +a_{jm}r_{F_kF_m}+\dotsb + a_{jk}r_{F_kF_k}\\ \\
\ \ \ &  \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ddots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ddots \ \ \ \ \ \ \ \vdots \ \ \ \\
r_{Z_jU_j} & = d_j\\
\ \ \ \ \ \ \ \ \ \ \ \ & \vdots
\end{split}
(\#eq:eqaf6)
\end{equation}
En la práctica, viene dada por una tabla, o matriz, $\bf \Gamma$, con los coeficientes $\{r_{jm}\}$. Cuando todos los factores están incorrelados el patrón y la estructura coinciden.

[^af-5]: En caso de variables dicotómicas se utiliza el coeficiente $\phi$ (véase \@ref(medidas)) como medida de correlación momento-producto.


El conjunto patrón factorial más estructura factorial \index{estructura factorial} se denomina **solución factorial completa**. \index{solución factorial!completa} El patrón factorial muestra la relación lineal de las variables en términos de los factores, como si de una regresión lineal se tratase, y puede usarse para reproducir la correlación entre las variables (y, por tanto, para determinar la bondad de la solución). La estructura factorial es útil para la identificación de los factores \index{identificación de los factores} y la posterior estimación de las **puntuaciones factoriales**.  \index{puntuaciones factoriales}

En términos matriciales, denominando
$$\bf F=\left(\begin{matrix} f_{11} & f_{12} & \cdots &f_{1N}\\
f_{21}&f_{22}&\cdots&f_{2N}\\
\vdots&\vdots&\ddots     &\vdots\\
f_{k1}&f_{k2}&\cdots&f_{kN}\\
\end {matrix}\right),\quad \bf \Gamma=\left(\begin{matrix} r_{Z_1F_1} & r_{Z_1F_2} & \cdots &r_{Z_1F_k}\\
r_{Z_2F_1}&r_{Z_2F_2}&\cdots&r_{Z_2F_k}\\
\vdots&\vdots&\ddots     &\vdots\\
r_{Z_pF_1}&r_{X_p^*Z_pF_2}&\cdots&r_{Z_pF_k}\\
\end {matrix}\right),$$
el patrón factorial \index{patrón factorial} viene dado por $\bf Z=\bf A \bf F + \bf D \bf U$. Multiplicando por $\bf F^{\prime}$ y realizando simples operaciones se tiene que $\bf \Gamma = \bf A\bf \Phi$, donde $\bf \Phi$ es la matriz de correlaciones \index{matriz!de correlaciones} entre los factores comunes. Si los factores comunes están incorrelados, $\bf \Gamma=\bf A$. \index{factores!comunes}

Por último, resaltar que el AF es indeterminado, es decir, dado un conjunto de correlaciones, los coeficientes del patrón factorial \index{patrón factorial} no son únicos (dado $\bf R$, se pueden encontrar infinitos sistemas de factores incorrelados u ortogonales)[^af-6] consistentes con ella. Por ello, normalmente, tras obtener una solución que ajuste bien los datos originales, se lleva a cabo una rotación de la misma (que ajusta igual de bien dichos datos) que facilite la *interpretación de los factores*.[^af-7]  \index{interpretación de los factores}

[^af-6]: La historia es más larga: el AF es indeterminado porque dada una matriz ${\bf {C}}_{kxk}$ no singular, si se define otro vector de factores comunes $\bf f^*=\bf C^{-1}\bf f$ y otra matriz $\bf A^*=\bf A\bf C$, entonces $\bf Z=\bf A \bf f+\bf D \bf u=\bf A^*\bf C^{-1}\bf C\bf f^*+\bf D \bf u=\bf A^*\bf f\bf^*+ D \bf u$ y ambos son equivalentes. Una solución es exigir la incorrelación de los factores comunes ($\bf \Phi=\bf I$), con lo que la indeterminación se reduciría sólo a cuando $\bf C$ sea ortogonal. En este caso, la solución será única salvo rotaciones ortogonales. \index{rotaciones!ortogonales}


[^af-7]: Una solución determina el espacio $k$-dimensional que contiene los $k$ factores comunes, pero no su posición exacta.



## El análisis factorial en la práctica
\index{análisis!factorial}

### Pre-análisis factorial

#### ¿Procede la realización de un análisis factorial?

Antes de comenzar con el AF, conviene determinar si procede o no; es decir, si las variables se encuentran fuertemente intercorrelacionadas o no. En caso negativo, el AF no tendría sentido. Para ello, se pueden utilizar procedimientos sencillos como observar si el determinante de $\bf R$ es bajo (correlaciones altas) o elevado (correlaciones bajas); o calcular la *matriz de correlaciones anti-imagen*, cuyos elementos son los coeficientes de correlación parcial cambiados de signo. En la diagonal muestra la *medida de adecuación muestral*  \index{medida de adecuación muestral} para esa variable, $MSA_j$. Para que se den las condiciones de realización del AF, la mayoría de los elementos no diagonales deben ser pequeños y los diagonales deben estar próximos a la unidad. 

 \index{matriz!de correlaciones}
 \index{matriz!de correlaciones anti-imagen}


Otras alternativas más sofisticadas incluyen las dos siguientes: 

**Contraste de esfericidad de Bartlett**\index{contraste!de esfericidad de Bartlett} 

Exige normalidad multivariante. Contrasta la incorrelación de las variables, es decir, $H_0:\bf R=\bf I$ frente a $H_1:\bf R\neq \bf I$ (o $H_{0}:|\bf{\bf R}|=1$ frente a $H_{1}:|\bf{\bf R}|=1$). El estadístico de contraste es $d_{\bf R}= - \left( N-1-\frac{1}{6} (2p+5)\right) ln|\mathbf{R}|$ y, bajo $H_0$, sigue una $\chi^2_{\frac{p(p-1)}{2}}$, siendo nulo en caso de incorrelación.

```r
n <- dim(TIC2021)[1] 
cortest.bartlett(cor(TIC2021),n)$chisq
#> [1] 149.7113
cortest.bartlett(cor(TIC2021),n)$p.value
#> [1] 1.992514e-21
```

**Medida de adecuación muestral de Kaiser, Meyer y Olkin (KMO)** \index{indice@índice!KMO} 
\index{medida de adecuación muestral}

Se basa en la idea de que, entre cada par de variables, el coeficiente de correlación parcial (que mide la correlación existente entre cada par de ellas eliminando el efecto que el resto de variables tiene sobre las dos consideradas), debe ser cercano a cero, puesto que es una estimación de la correlación entre sus factores específicos, que se suponen incorrelados. Por tanto, si el número de coeficientes de correlación parcial no nulos es elevado, la solución factorial no es compatible con los datos.

\index{coeficiente de correlación parcial}

En otros términos, cuando las variables incluidas en el análisis comparten gran cantidad de información debido a la presencia de factores comunes, la correlación parcial entre cualquier par de variables debe ser reducida. Por el contrario, cuando dos variables comparten gran cantidad de información entre ellas, pero no la comparten con las restantes variables (ni, consecuentemente, con los factores comunes), la correlación parcial entre ellas será elevada, lo cual es un mal síntoma de cara a la idoneidad del AF. \index{factores!comunes}

El índice KMO \index{indice@índice!KMO} se define como $KMO=\frac{\displaystyle\sum_{j}\displaystyle\sum_{i \neq j} r_{ji}^2}{\displaystyle\sum_{j}\displaystyle\sum_{i \neq j} r_{ji}^2+\displaystyle\sum_{j}\displaystyle\sum_{i \neq j}r_{ji}^{*2}}$, donde $r_{ji}^{*}$ es el coeficiente de correlación parcial \index{coeficiente de correlación parcial} entre las variables $Z_j$ y $Z_i$. Se considera que valores por encima 0,9 implican elevadísismas correlaciones en $\bf R$; entre 0,5 y 0,9 permiten el AF; y por debajo de 0,5 resultan inaceptables para el AF. 

Las $MSA_j$ mencionadas anteriormente, son la versión del índice KMO limitado a cada variable: $MSA_{j}= \frac{\displaystyle\sum_{i \neq j} r_{ji}^2}{\displaystyle\sum_{i \neq j} r_{ji}^2+\displaystyle\sum_{i \neq j} r_{ji}^{*2}}.$

La interpretación es similar a la de KMO, pero mide la adecuación de cada variable para realizar un AF, lo que permite no considerar aquellas variables con menor MSA de cara a mejorar la KMO. \index{indice@índice!KMO} No obstante, para eliminar una variable del estudio es aconsejable tener en cuenta también las comunalidades \index{comunalidad} de cada variable, los residuos del modelo e interpretar los factores obtenidos. 

<!-- $r_{ij}$ es la correlación observada entre las variables originales $X_{i}$ y $X_{j}$ y $a_{ij}$ es la correlación parcial. Si la muestra se adecua a un modelo de AF, los coeficientes de correlación parcial $(a_{ij})$ serán muy pequeños o nulos según lo dicho anteriormente y el índice KMO tenderá a 1. @Kaiser_1974 y @KaiserRice_1974 consideran valores por encima 0,9 muy buenos, entre 0,5 y 0,9 mediocres y por debajo de 0,5, inaceptables. Por lo tanto, valores por encima de 0,5 se van a considerar validos [@Hair_et_al1999]. Esta misma medida para cada variable de forma individual se denomina medida de adecuación a la muestra correspondiente a la variable i-ésima (MSA)\index{Medida de adecuación a la muestra de la variable i-ésima (MSA)}: \begin{equation} -->


```r
round(KMO(TIC2021)$MSA,3)
#> [1] 0.83
round(KMO(TIC2021)$MSAi,3)
#>    ebroad    esales esocmedia      eweb    hbroad     hiacc      iuse 
#>     0.850     0.671     0.934     0.856     0.808     0.764     0.875
```
Como puede apreciarse, en nuestro ejemplo TIC, tanto el test de Barlett como el índice $KMO$ \index{indice@índice!KMO} y las $MSA_j$ indican que el AF se puede llevar a cabo con garantías.

<!-- El valor de 0,83 de la KMO indica la adeucación de desarrollar un AF en el ejemplo analizado siendo el valor de la MSA correcto para todas las variables alcanzando el menor valor para la variable *esales* y el mayor para la variable *esocmedia*. -->


#### El problema de la comunalidad y/o del número de factores comunes
\index{comunalidad}
\index{factores!comunes}

El objetivo del AF es encontrar una *matriz de correlaciones reproducida*  \index{matriz!de correlaciones reproducida} a partir de los resultados obtenidos, ${\bf R}^{rep}$, con menor rango que la original, $\bf R$, tal que su diferencia, la *matriz de correlaciones de los residuos*  \index{matriz!de correlaciones de los residuos}, ${\bf R}^{res}$, se atribuya únicamente a errores muestrales. $\bf R$ es una matriz gramiana: simétrica de números reales y de diagonal principal dominante, con lo cual es semidefinida positiva y sus autovalores \index{autovalor} son nulos o positivos. Por tanto, el número de factores comunes será igual al de autovalores positivos ($k\leq p$). Si el punto de partida en el análisis es $\bf R$, rara vez se obtienen menos factores comunes que variables originales, con lo cual el AF realmente es un ACP. Ahora bien, como el número de factores comunes coincide con el rango de ${\bf R}^{rep}$, y éste se ve afectado por los valores de la diagonal principal, al sustituir los unos por las estimaciones de las comunalidades (en este caso se está realizando un AF), ${\bf R}^{rep}$ no será, en general, gramiana y $k< p$. En conclusión: como la solución factorial ($k< p$) pasa por el conocimiento del rango de $\bf R$ o de las comunalidades, o se hipotetiza sobre dicho rango o se hipotetizan o estiman las comunalidades. Normalmente se sigue uno de estos dos caminos:

- (1) Se parte de un $k$ prefijado, se lleva a cabo el AF y se contrasta la hipótesis $H_0$: número\hspace{0,1cm} de factores\hspace{0,1cm} comunes = $k$.
- (2) Se estiman las comunalidades y se obtienen los factores comunes \index{factores!comunes}.

En cuanto a prefijar un número $k$ de factores, se pueden seguir los criterios expuestos en el Cap. \@ref(acp) para determinar el número de componentes principales \index{componentes principales} a retener (criterio de Kaiser \index{criterio!de Kaiser}, gráfico de sedimentación, porcentaje mínimo de varianza explicada, ...). \index{gráfico de sedimentación}

En cuanto a la estimación de las comunalidades, de las múltiples posibilidades existentes, las siguientes son interesantes por su sencillez y buenos resultados: \index{comunalidad}

- Una de las más sencillas, si el número de variables es grande, es aproximar la comunalidad de una variable por su correlación más alta con las demás variables: $\hat {h}_j^2=max(r_{j1},r_{j2},\cdots,r_{j(j-1)},r_{j(j+1)},\cdots, r_{jp})$. 
- Otra posibilidad es $\hat{h}_j^2=\frac{r_{js}r_{jt}}{r_{ts}}$, donde $Z_{s}$ y $Z_{t}$ son, por este orden, las dos variables más correlacionadas con $Z_{j}$. Este procedimiento modera el efecto que tendría en el anterior una correlación excepcionalmente elevada. 
- En  la misma línea, otra posibilidad es el promedio de los coeficientes de correlación entre la variable en cuestión y las restantes: $\hat{h}_j^2=\frac{\sum_{j \neq s}r_{js}}{p-1}$.
- Otra alternativa es realizar un ACP y tomar como comunalidad de cada variable la varianza explicada por los factores retenidos con el criterio de autovalor \index{autovalor} mayor que la unidad.
- También se puede utilizar el coeficiente de determinación lineal múltiple de cada variable con las demás como estimación de la cota inferior de sus comunalidades: $\hat {h}_j^2 \geq r^2_{{Z_j};(Z_{j1},\cdots,Z_{2},\cdots Z_{j-1},Z_{j+1},\cdots, Z_{p})}=1-\frac{1}{r^{jj}}$, donde $r^{jj}$ es el $j$-ésimo elemento de la diagonal de ${\bf R}^{-1}$.

Un valor alto de la comunalidad, próximo a $V(X_j)$, significa que dicha variable está bien representada en el espacio de factores.

<!-- ## Determinación del número de factores. -->

<!-- Una vez analizada la adecuación para realizar un AF, una decisión clave es determinar el número de factores comunes a retener. A menudo un conjunto pequeño de factores comunes contiene gran cantidad de la información de las variables originales por lo que no es necesario usar un número de factores superior. Existen diversos criterios para decidir el número de factores que se deben considerar: -->

<!-- - *Determinación “a priori”*. Es necesario un conocimiento previo de la situación por parte del investigador con una elección correcta de los datos y variables a usar lo que le permite establecer previamente el número de factores que hay y cuáles son. -->

<!-- - *Regla de Kaiser*\index{Regla de Kaiser}. Se calculan los valores propios de la matriz de correlaciones R y se toman como número de factores el número de valores propios superiores a la unidad. Este criterio es una alusión del Análisis de Componentes Principales y se ha verificado en simulaciones que, generalmente, tiende a infraestimar el número de factores. Para ello, en primer lugar se desarrolla un análisis de componentes principales con la matriz de correlaciones. -->

<!-- ```{r valorsuperiormedia.ch12,echo=TRUE,eval=TRUE} -->

<!-- acp <- princomp(TIC2021,cor=T) -->

<!-- acp$sd^2 -->

<!-- ``` -->

<!-- - *Criterio del porcentaje de la varianza*. Consiste en tomar como número de factores el número mínimo necesario para que el porcentaje acumulado de la varianza explicado alcance un nivel satisfactorio, normalmente se usan valores del 75% u 80%. Tiene la ventaja de que se puede aplicar también cuando la matriz analizada es la de varianzas y covarianzas, pero no tiene ninguna justificación teórica o práctica. -->

<!-- ```{r porcentajevarianza.ch12,echo=TRUE,eval=TRUE} -->

<!-- round((acp$sdev)^2/sum((acp$sdev)^2),digits=4)*100 -->

<!-- ``` -->

<!-- - *Gráfico de Sedimentación*\index{Gráfico de sedimentación}. Se trata de la representación gráfica donde los factores están en el eje de abscisas y los valores propios en el de ordenadas. Los factores con varianzas altas suelen diferenciarse de los factores con varianzas bajas. Se pueden conservar los factores situados antes de este punto de inflexión. En simulaciones el criterio ha funcionado bien, tiene el inconveniente de que es muy subjetivo. -->

<!-- ```{r grafsediment.ch12,echo=TRUE,eval=TRUE} -->

<!-- screeplot(acp,type="l") -->

<!-- ``` -->

<!-- Los criterios analizados llevan a seleccionar tres factores comunes, número de factores que va a ser considerados a la hora de mostrar algunas salidas de R y que constituye uno de los argumentos necesarios en las funciones usadas para desarrollar el AF. -->


### Análisis factorial
\index{análisis!factorial}

#### Métodos de extracción de los factores {#metodosdeextraccion}

**Método de componentes principales**  \index{método!de componentes principales}

Su objetivo es el análisis de toda la varianza, común \index{varianza!común} y no común, (modelo \@ref(eq:eqaf1)). Por consiguiente, las entradas de la diagonal de $\bf R$[^af-8] son unitarias y no se requiere la estimación a priori de las comunalidades \index{comunalidad}; tampoco se requiere la estimación a priori del numero de factores comunes, que se determinan a posteriori. Para la exposición del método, así como para su ejemplificación con la base de datos `TIC2021` del paquete `CDR`, se remite al lector al Cap. \@ref(acp). Aunque en el Cap. \@ref(acp) se utilizó la función `PCA` de la librería `FactoMineR`, también se puede utilizar la función `principal` de la librería `psych`. \index{factores!comunes}

[^af-8]: Cuando se utiliza este método, se debe utilizar $\bf R$, porque, de otra forma, los factores comunes no tendrían media cero y varianza unitaria.


Este método tiene la ventaja de que siempre proporciona una solución. Sin embargo, al no estar basado en el modelo \@ref(eq:eqaf3), puede dar estimaciones de las cargas factoriales \index{cargas factoriales} muy sesgadas, sobre todo cuando hay variables con comunalidades \index{comunalidad} pequeñas.

<!-- Una vez analizada la adecuación para realizar un AF, una decisión clave es determinar el número de factores comunes a retener que contengan la mayor cantidad de la información de las variables originales. Los métodos para determinar el número de factores comunes a retener son los mismos que los usados para fijar el número de componentes principales visto en la sección XXX del capítulo XXX (**PONER REFERENCIA CRUZADA**). En el ejemplo práctico analizado recordar que los resultados llevaban a seleccionar dos componentes principales y, por lo tanto, en AF dos factores comunes. -->

<!-- Algunos de los métodos más usuales para la estimación de los parámetros del modelo (pesos de los factores comunes ($l_{ij}$) y varianzas específicas ($\psi_{i}$) son: -->

<!-- 1.- **Método de Componentes Principales** \index{Método de Componentes Principales} -->

<!-- Se trata de un método de carácter exploratorio. El modelo de componentes principales se especificó de la siguiente forma: \begin{equation} -->
<!-- \begin{split} -->
<!-- Y_{1}= a_{11}X_{1}+ a_{21}X_{2}+ \dotsb+ a_{p1}X_{p}\\ -->
<!-- Y_{2}= a_{12}X_{1}+ a_{22}X_{2}+ \dotsb+ a_{p2}X_{p}\\ -->
<!-- \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \vdots \ \ \ \\ -->
<!-- Y_{p}= a_{1p}X_{1}+ a_{2p}X_{2}+ \dotsb+ a_{pp}X_{p} -->
<!-- \end{split} -->
<!-- (\#eq:ecuacion6) -->
<!-- \end{equation} -->

<!-- El modelo anterior se transforma para llegar a la especificación del modelo de AF: \begin{equation} -->
<!-- \begin{split} -->
<!-- X_{1}= l_{11}F_{1}+ l_{12}F_{2}+ \dotsb + l_{1k}F_{k}+ \varepsilon_{1}\\ -->
<!-- X_{2}= l_{21}F_{1} + l_{22}F_{2}+ \dotsb + l_{2k}F_{k}+ \varepsilon_{2}\\ -->
<!-- \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \vdots \ \ \ \\ -->
<!-- X_{p}= l_{p1}F_{1}+ l_{p2}F_{2}+ \dotsb + l_{pk}F_{k}+ \varepsilon_{p}\\ -->
<!-- \end{split} -->
<!-- (\#eq:ecuacion7) -->
<!-- \end{equation} -->

<!-- Mientras que en el primer sistema las variables no observadas se especifican en función de las observadas, en el segundo es al contrario. Teniendo en cuenta que el número de variables no observadas es p, mientras que en el sistema objetivo (AF) es k+1, la solución consiste en englobar los últimos p-k términos de cada ecuación en uno, que llamaremos $\varepsilon$. Además, en este último sistema los factores comunes no tienen varianza unitaria, para salvar este problema, se tipifican las componentes principales dividiendo por la raíz del correspondiente autovalor (multiplicando los coeficientes por la misma cantidad para que no cambie nada).  -->

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- X_{1}= a_{11}\sqrt{\lambda_{1}} \frac{Y_{1}}{\sqrt{\lambda_{1}}}+ a_{12}\sqrt{\lambda_{2}} \frac{Y_{2}}{\sqrt{\lambda_{2}}}+ \dotsb+ a_{1k}\sqrt{\lambda_{k}} \frac{Y_{k}}{\sqrt{\lambda_{k}}}+ \varepsilon_{1}\\ -->
<!-- X_{2}= a_{21}\sqrt{\lambda_{1}} \frac{Y_{1}}{\sqrt{\lambda_{1}}}+ a_{22}\sqrt{\lambda_{2}} \frac{Y_{2}}{\sqrt{\lambda_{2}}}+ \dotsb+ a_{2k}\sqrt{\lambda_{k}} \frac{Y_{k}}{\sqrt{\lambda_{k}}}+ \varepsilon_{2}\\ -->
<!-- \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots\ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \vdots \ \ \ \\ -->
<!-- X_{p}= a_{p1}\sqrt{\lambda_{1}} \frac{Y_{1}}{\sqrt{\lambda_{1}}}+ a_{p2}\sqrt{\lambda_{2}} \frac{Y_{2}}{\sqrt{\lambda_{2}}}+\dotsb+ a_{pk}\sqrt{\lambda_{k}} \frac{Y_{k}}{\sqrt{\lambda_{k}}}+ \varepsilon_{p}\\ -->
<!-- \end{split} -->
<!-- (\#eq:ecuacion8) -->
<!-- \end{equation} -->

<!-- Los factores comunes $F_{j}$ serían $\frac{Y_{j}}{\sqrt{\lambda_{j}}}$ y los pesos $l_{ij}= a_{ij} \sqrt{\lambda_{j}}$. Sin embargo, hay que señalar que todos los factores específicos son combinaciones lineales de las mismas variables no observadas, luego no serían realmente específicos de cada variable original, este es el inconveniente fundamental de este método. Además, este método lleva a veces a confundir el análisis de componentes principales y el AF. 
<!-- En R este método se aplica usando la función "principal" de la librería **psych** . --> 



<!-- ```{r factorialCP-ch12,echo=TRUE,eval=TRUE, , fig.cap="Gráfica de variables sobre el primer plano de componentes"} -->
<!-- afcp <- principal(TIC2021, nfactors=2, rotate="none") -->
<!-- afcp$loadings -->
<!-- factor.plot(afcp$loadings[,1:2], pch=20, xlim=c(-0.5,1), ylim=c(-1,1), xlab="Factor 1", ylab="Factor 2") -->
<!-- abline(h=0,lty=3) -->
<!-- abline(v=0,lty=3) -->
<!-- text(afcp$loadings[,1:2],pos=1,labels=names(TIC2021),cex=0.8) -->
<!-- ``` -->


**Método de los factores principales**\index{método!de los factores principales} 

Es la aplicación del método de componentes principales \index{método!de componentes principales} a la *matriz de correlaciones reducida*, ${\bf R}^*$, es decir, con comunalidades en la diagonal en vez de unos. Exige, por tanto, la estimación previa de las comunalidades y su objetivo es el análisis de la varianza  compartida por todas las variables (modelo \@ref(eq:eqaf3)).  Se trata de un procedimiento iterativo que consta de las siguientes etapas:

\index{matriz!de correlaciones reducida}

<!-- Es un método iterativo que parte de estimar la comunalidad a través de los coeficientes de determinación obtenidos en la regresión lineal de cada variable sobre el resto. Los pasos son: -->

1.- Cálculo de la matriz de correlaciones muestrales.

2.- Estimación inicial de las comunalidades \index{comunalidad} utilizando el coeficiente de determinación lineal múltiple de cada variable  con las demás.[^af-10] 

3.- Cálculo de la matriz de correlaciones reducida:

$$\mathbf{R}^*
=\begin{pmatrix}
\hat{h}_{1(0)}^2 & r_{12} & \dotsb & r_{1p}\\
r_{21} & \hat{h}_{2(0)}^2 & \dotsb & r_{2p}\\
\vdots & \vdots & \dotsb & \vdots \\
r_{p1} & r_{p2} & \dotsb & \hat{h}_{p(0)}^2\\
\end{pmatrix}.$$
 
[^af-10]: Una estimación inicial de las comunalidades equivale a una estimación inicial de las varianzas únicas: $\hat{d}_{j(0)}^2=\hat{\sigma}_{j}^2-\hat{h}_{j(0)}^2$; y si las variables originales están tipificadas: $\hat{d}_{j(0)}^2=1-\hat{h}_{j(0)}^2$.



4.- Cálculo de los autovalores \index{autovalor} y autovectores \index{autovector} asociados a $\mathbf{R}^*$ (matriz no necesariamente semidefinida positiva) y, a partir de ellos, obtención de las estimaciones de la matriz de cargas \index{matriz!de cargas} factoriales $\bf{A}_{(0)}$. En este paso hay que determinar el número de factores utilizando los criterios del ACP. 

5.- A partir de la estimación de $\bf{A}_{(0)}$, obtención de una nueva estimación de las comunalidades: \index{comunalidad} $\hat{h}_{j(1)}^2= \hat{a}_{j1(1)}^2+\hat{a}_{j2(1)}^2+ \dotsb +\hat{a}_{jk(1)}^2$ y, por tanto, de una nueva estimación de la varianza única (o unicidad) $\hat{d}_{j(1)}^2 =1 - \hat{h}_{j(1)}^2$.
\index{unicidad}
\index{varianza!única}

6.- Comparación de $\hat{h}_{j(1)}^2$ con $\hat{h}_{j(0)}^2$, $j=1,2,\cdots,p$. Si hay diferencia significativa se vuelve al paso 3, y si la discrepancia no supera una cantidad prefijada se aceptan como válidas las últimas estimaciones disponibles.

En el software **R**, el método de los factores principales  \index{método!de los factores principales} se implementa con la función `fa` de la librería `psych`, que parte de ${\bf R}^*$ (véase @harman1976 para el procedimiento iterativo y @Revelle2022 para los detalles sobre la manera cómo `fa` parte de ${\bf R}^*$ y lleva a cabo la extracción de los factores).


```r
af_facprin <- fa(cor(TIC2021), nfactors=2, rotate="none", fm='pa')
```



```r
print(af_facprin, digits=3)
#> Factor Analysis using method = pa
#> Call: fa(r = cor(TIC2021), nfactors = 2, rotate = "none", fm = "pa")
#> Standardized loadings (pattern matrix) based upon correlation matrix
#>              PA1     PA2    h2    u2   com
#> ebroad     0.678   0.189 0.495 0.5050 1.16
#> esales     0.503   0.547 0.553 0.4474 1.99
#> esocmedia  0.796   0.212 0.678 0.3218 1.14
#> eweb       0.872   0.239 0.818 0.1822 1.15
#> hbroad     0.816  -0.452 0.869 0.1306 1.56
#> hiacc      0.888  -0.439 0.982 0.0181 1.46
#> iuse       0.935  -0.023 0.875 0.1248 1.00
#>
#>                  PA1   PA2
#> SS loadings    4.435 0.835
#> Proportion Var 0.634 0.119
#> Cumulative Var 0.634 0.753
```

En la salida anterior, $SS$ $loadings$ son \index{loadings} los autovalores \index{autovalor} de ${\bf R}^*$, que coinciden con la suma de los cuadrados de las cargas de las variables en cada factor (suma de las cargas al cuadrado por columnas). $h2$ son las comunalidades (suma de las cargas \index{cargas factoriales} al cuadrado por filas; sólo se muestran las cargas para los dos primeros factores puesto que entre ambos ya acumulan una varianza explicada de más del 75%). $u2$ son las varianzas de los factores únicos; finalmente, $com_j$ (en la salida $com$) es el número de factores comunes \index{factores!comunes} necesarios para explicar la variable $Z_j$: $com_j=\frac{\left( \sum_{j=1}^{p} a_{jm}^{2} \right)^2}{\sum_{j=1}^{p} a_{jm}^4}$. Cuanto mayor es $com_j$ mejor es la calidad de la variable para participar en la extracción factorial. El promedio de los  $com_j$ se denomina *índice de complejidad de Hoffmann*. Una solución de estructura simple \index{estructura simple} perfecta tiene una complejidad de uno (cada variable carga solo en un factor); una solución con elementos distribuidos uniformemente tiene una complejidad mayor que 1. Interesa que la estructura no sea simple y perfecta porque entonces no tendría sentido la reducción dimensional. Por tanto, el índice de Hofmann deberá ser superior a la unidad. \index{indice@índice!de complejidad de Hoffmann} 


Las comunalidades y unicidades son:
\index{comunalidad}
\index{unicidad}


```r
round(af_facprin2$communality,3)
#> ebroad esales esocmedia    eweb hbroad hiacc iuse 
#>  0.495 0.553      0.678  0.818   0.869 0.982 0.875 
round(af_facprin2$uniquenesses,3)
#> ebroad esales esocmedia    eweb hbroad hiacc iuse 
#>  0.505 0.447      0.322   0.182 0.131  0.018 0.125 
```

\textcolor{red}{**Ojo, $af_facprin2$ se crea más abajo, este objeto es $af_facprin$ o que ha psado aquí**}

Nótese que con el método de los factores principales, al aplicar ACP sobre ${\bf R}^*$, los factores obtenidos están incorrelados y la estructura coincide con el patrón.
\index{método!de los factores principales}

Los resultados son, en signo, aunque no tanto en valor, similares a los obtenidos por el método de componentes principales. Además, como era de esperar, no permiten una interpretación clara de los factores comunes. Para facilitar dicha interpretación, dichos factores deberán ser rotados (véase Sec. \@ref(rotaciones)). \index{método!de componentes principales} \index{factores!comunes}


**Método de máxima verosimilitud** \index{método!de máxima verosimilitud}

<!-- Si se supone que la población se distribuye según una ley normal, se pueden obtener las estimaciones máximo verosímiles de los pesos de los factores, así como de las varianzas específicas. -->

Exige normalidad multivariante y la determinación a priori del número de factores comunes, pero no la estimación de las comunalidades. \index{comunalidad}Obedece al modelo \@ref(eq:eqaf3) y consiste en obtener las estimaciones máximo verosímiles de $\bf A$ y $\bf D$. Dado que cualquier transformación ortogonal de $\bf{A}$ puede ser una solución, se impone la condición de que $\bf A^{\prime}(DD^{\prime})^{-1}\bf A$ sea diagonal. La log-verosimilitud viene dada por $l=-\frac{N}{2}\left( log|2\pi{\bf {\Sigma}}|+ tr{\bf {\Sigma}}^{-1}\bf S \right)$, donde $\bf\Sigma=\bf A \bf A^{\prime}+\bf D \bf D^{\prime}$ y $\bf S$ son las matrices de covarianzas poblacional y muestral, respectivamente, de las variables $X_j$, $j= 1,2,\cdots,p$. $\hspace{0,1cm} \bf D \bf D^{\prime}$ es la matriz de covarianzas (diagonal) \index{matriz!de covarianzas} de los factores únicos, donde los elementos de la diagonal representan la parte de la varianza única \index{varianza!única} de cada variable, y en la literatura sobre AF es conocida como $\bf \Psi$ (por ello, $d_j^2=\psi_{jj})$.

La decisión sobre el número de factores comunes \index{factores!comunes}, $k$, que en este método debe hacerse al principio, es muy importante, pues dos soluciones, una con $k$ factores y otra con $k+1$, pueden tener matrices de cargas factoriales \index{cargas factoriales} muy diferentes, al contrario que en el método de componentes principales, \index{método!de componentes principales} donde los primeros $k$ componentes son siempre iguales. Pues bien, una ventaja del método de máxima verosimilitud \index{método!de máxima verosimilitud} es que lleva asociado un test estadístico secuencial para determinar el número de factores (véase Sec. \@ref(postanalisis)). Otra ventaja es que las estimaciones máximo verosímiles son invariantes ante cambios de escala; en consecuencia, las matrices de covarianzas teórica y muestral de la log-verosimilitud pueden ser sustituidas por sus homónimas de correlación sin variación alguna en los resultados. Una desventaja es que puede haber un problema de grados de libertad; o, en otros términos, el número de factores $k$ debe ser compatible con un número de grados de libertad positivo. 


<!-- Antes de entrar en el método, se debe tener en cuenta el posible problema de los grados de libertad que se da cuando el número de elementos informativos es menor que el número de parámetros a estimar. En este caso, la matriz de varianzas y covarianza muestral tiene $p(p + 1)/2$ elementos informativos ya que es una matriz simétrica, mientras que los elementos a estimar son $pxk$ elementos en la matriz de pesos de los factores comunes y $p$ elementos en la matriz de varianzas de los factores específicos, por tanto, $p (k+1)$. Aparece un problema de grados de libertad cuando $p(p+1)/2<p(k+1)$. -->


El método máximo verosímil se puede implementar en **R** con la librería `pysch` y la función `fa` (con `fm=ml`). Otra posibilidad es utilizar la función `factanal`. En ambos casos hay comprobar el cumplimiento de la hipótesis de normalidad. En el ejemplo `TIC` no procede su implementación al no cumplirse tal hipótesis.




<!-- ```{r factorialMV-ch12,echo=TRUE,eval=TRUE} -->
<!-- afmv <- factanal(TIC2021, factors=3, rotation="none") -->
<!-- afmv$loadings -->
<!-- ``` -->

<!-- \textcolor{red}{**LO EJECUTAMOS O NO (NO HAY NORMALIDAD), SOLAMENTE DECIMOS LA FORMA DE HACERLO TAL Y COMO VIENE**} -->


**Otros métodos**

Razones de espacio impiden comentar otros procedimientos de extracción de los factores. No obstante, hay que señalar que la función `fa` de la librería `psych` también permite implementar los métodos $(i)$ minres (mínimo residuo), que estima las cargas factoriales \index{cargas factoriales} minimizando (sin ponderaciones) los cuadrados de los residuos no  diagonales de ${\bf R}^{res}$; parte de una estimación de $k$ y, como el método máximo verosímil, no precisa estimar las comunalidades, que se obtienen como subproducto tras la estimación de las cargas; y $(ii)$ alpha, que maximiza el alpha de Cronbach \index{alpha de Cronbach} para los factores. Aunque `fa` también proporciona  el método del centroide \index{método!del centroide} o la descomposición triangular (que exigen la estimación de las comunalidades, o el análisis imagen \index{análisis!imagen}(que requiere el número de factores), en la actualidad están en desuso. \index{método!alpha} \index{método!del centroide} \index{comunalidad} \index{método!de la descomposición triangular} \index{método!del análisis imagen} \index{método!minres}

Otros métodos de extracción de los factores son los métodos de mínimos cuadrados no ponderados \index{método!de mínimos cuadrados no ponderados} y mínimos cuadrados generalizados, \index{método!de mínimos cuadrados generalizados} que minimizan la suma de las diferencias cuadráticas entre las matrices de correlación observada y reproducida, en el último caso ponderando los coeficientes de correlación inversamente a la unicidad \index{unicidad} de las variables (alta unicidad supone baja comunalidad). Ambos son proporcionados por `fa` y `FAiR`, que también es una librería muy recomendable.

#### Rotaciones en el modelo de análisis factorial{#rotaciones}
\index{análisis!factorial}

La interpretación de los factores \index{interpretación de los factores} se lleva a cabo a través de la estructura factorial \index{estructura factorial}, que,  si los factores comunes están incorrelados, coincide con el patrón factorial. \index{patrón factorial} Sin embargo, aunque el modelo obtenido sea representativo de la realidad, en ocasiones la interpretación de los factores es harto dificultosa, porque presentan correlaciones similares con un gran número de variables. Como la solución AF no es única (si $\bf{A}$ es una solución factorial, también lo es cualquier transformación ortogonal), con la rotación se trata de que cada variable tenga una correlación próxima a 1 con un factor y a 0 con el resto, facilitando la interpretación de los factores.

Geométricamente, la  $j$-ésima fila de la matriz de cargas \index{matriz!de cargas} contiene las coordenadas de un punto (elemento, observación) en el espacio de las cargas \index{cargas factoriales} correspondientes a $X_j$. Al realizar la rotación, se obtienen las coordenadas respecto a unos nuevos ejes, siendo el objetivo situarlos lo más cerca posible del mayor número de puntos. Esto asociaría cada grupo de variables con un sólo factor, haciendo la interpretación más objetiva y sencilla.

Sea $\bf T$ una matriz ortogonal ($\bf T^{\prime} \bf T=\bf T\bf T^{\prime}=\bf I$), denominada *matriz de transformación*. \index{matriz!de transformación} Entonces, el modelo \@ref(eq:eqaf3) puede escribirse como $\bf Z=\bf A\bf T\bf T^{\prime}\bf F+ \bf U=\bf B \bf T^{\prime}\bf F+ \bf U$. Se trata de llegar a una *estructura simple*, \index{estructura simple} que se caracteriza porque en $\bf B$:

- Cada fila tiene al menos un cero.
- Cada columna tiene, al menos, tantos ceros como factores comunes \index{factores!comunes} ($k$).
- Cada par de columnas debe ser tal que, para varias variables, una tenga cargas despreciables y la otra no.
- Si $k\geq 4$, cada par de columnas debe tener un número elevado de variables cuyas cargas sean nulas en ambas variables.
- Para cada par de columnas, el número de variables con cargas no nulas en ambas columnas debe ser muy pequeño.

Como se avanzó, se trata de que las variables se aglomeren lo más posible en
torno a los factores comunes, y de la manera más discriminatoria posible. Así mejora la interpretación de éstos y, por lo general, aumenta su significado teórico. \index{factores!comunes}

Las rotaciones pueden ser ortogonales \index{rotaciones!ortogonales} u oblicuas, \index{rotaciones!oblicuas} dependiendo de si los nuevos factores siguen estando incorrelacionados (ejes perpendiculares) o no (ejes oblicuos).

##### Rotaciones ortogonales 

\hspace{0px} Preservan la perpendicularidad de los ejes y no varían las comunalidades \index{comunalidad}, pues ${\bf{B}}{\bf{B}}^{\prime}= {\bf{A}} \bf{T} \bf{T}^{\prime} {\bf{A}}^{\prime}= \bf{A} \bf{A}^{\prime}$. Tampoco modifican los cuadrados de las comunalidades ni, por tanto, la suma de sus cuadrados (para todas las variables): $\sum_{j=1}^p\sum_{m=1}^k b_{jm}^4+2\sum_{m<r=1}^k\sum_{m=1}^k b_{jm}^2b_{jr}^2$. Y como esta expresión se mantiene invariante, minimizar el segundo término implica maximizar el primero. 

Las rotaciones ortogonales \index{rotaciones!ortogonales} más usadas son: 

**Rotación VARIMAX**\index{rotación!VARIMAX} 

Se define *simplicidad* del factor $m$-ésimo como la varianza de los cuadrados de las cargas factoriales \index{cargas factoriales} (rotadas) $b_{ji}$, $j=1,2,\cdots,p$: 
${SMPL}_{m} = \frac{\sum_{j=1}^{p} {b}_{jm}^4}{p}- \left(\frac {\sum_{j=1}^{p}b_{jm}^2}{p}\right)^2$. Cuanto mayor es la simplicidad de los factores, más sencilla es su interpretación. Por ello, el objetivo es que $\bf{T}$ sea tal que se maximice la varianza del cuadrado de las cargas en cada columna del patrón factorial, \index{patrón factorial} es decir, en cada factor. 


<!-- \begin{equation} -->
<!--     SIMP_{i}^2 = \frac{\displaystyle\sum_{j=1}^{p} (\hat{a}_{ji}^2)^2}{p}- \left( \frac {1}{p}\displaystyle\sum_{j=1}^{p} \hat{a}_{ji}^2 \right)^2 -->
<!--     (\#eq:ecuacion9) -->
<!--     \end{equation} -->

Dicho lo anterior, la rotación VARIMAX \index{rotación!VARIMAX} consiste en la obtención de una $\bf{T}$ que maximice la suma de las simplicidades de todos los factores, $V=\sum_{m=1}^{k}{SMPL}_{m}$.[^af-11]

[^af-11]: Este criterio es compatible con el hecho de que, en cada fila, uno de los elementos esté próximo a cero y los demás a uno, porque la suma de los cuadrados de los elementos de una fila, es la comunalidad fija de la variable correspondiente.

Sin embargo, las variables con mayor comunalidad, y por tanto con mayores cargas factoriales, tendrán mayor influencia en la solución final, porque la comunalidad no se ve afectada por la rotación ortogonal. Para evitar esto, Kaiser propuso la rotación VARIMAX normalizada[^af-12], donde las cargas se dividen entre la raíz cuadrada de la comunalidad de la variable correspondiente. Los valores obtenidos son los elementos de $\bf B$. 

El procedimiento de cálculo de las cargas \index{cargas factoriales} de los factores rotados es iterativo, rotándose los factores por parejas hasta que se consigue maximizar la suma de simplicidades normalizadas.

\index{rotación!VARIMAX normalizada}

[^af-12]: La normalización Kaiser se aplica también en los demás tipos de rotación.



La rotación VARIMAX \index{rotación!VARIMAX} es muy popular por la robustez de sus resultados, si bien se recomienda para un número no muy elevado de factores comunes \index{factores!comunes}.

**Rotación QUARTIMAX**\index{rotación!QUARTIMAX} 

Su objetivo es maximizar la varianza de los cuadrados de todas las cargas factoriales, es decir, maximizar $Q=\frac{\sum_{j=1}^{p}\sum_{m=1}^{k}{b}_{jm}^4}{pk}-\left( \frac {\sum_{j=1}^{p}\sum_{m=1}^{k}{b}_{jm}^2}{pk} \right)^2$. \index{cargas factoriales}

Nótese que, como la rotación ortogonal no modifica las comunalidades \index{comunalidad}, ${h}_{j}^2=\sum_{m=1}^{k} b_{jm}^2$, el segundo término de la expresión anterior no se verá modificado, por lo que el criterio anterior equivale a maximizar $\frac{\sum_{j=1}^{p}\sum_{m=1}^{k}{b}_{jm}^4}{pk}$.

QUARTIMAX \index{rotación!QUARTIMAX} es recomendable cuando el número de factores es elevado. Tiende a generar un factor general, el primero, sobre el que la mayor parte de las variables tienen cargas elevadas, lo cual contradice los objetivos que persigue la rotación.

**Rotación ORTOMAX**\index{rotación!ORTOMAX}

Es una clase general de los métodos de rotación ortogonal que se construye como una composición ponderada de las dos rotaciones anteriores: $\alpha Q+ \beta V$, donde $V$ se multiplica por $p$ por conveniencia, ya que una constante multiplicativa no afecta a la solución. Por tanto, su objetivo es maximizar la expresión: $ORT=\sum_{m=1}^{k} \left({\sum_{j=1}^{p} {b}_{ji}^4 - \left( \frac{\theta}{p}\sum_{j=1}^{p} b_{ji}^2 \right)^2}\right),\hspace{0,1cm} 0< \theta=\frac{\alpha}{\alpha+\beta}< 1$.

Si $\theta=1$, se tiene el criterio VARIMAX; si $\theta=0$, se tiene el criterio QUARTIMAX; si $\theta=0,5$, se tiene un criterio igualmente ponderado denominado BIQUARTIMAX; y si $\theta=\frac{k}{2}$, se tiene el criterio EQUAMAX, recomendado por parte de la literatura. \index{rotación!BIQUARTIMAX} \index{rotación!EQUAMAX}

Nótese que QUARTIMAX \index{rotación!QUARTIMAX} pone el énfasis en la simplificación de la descripción por filas (variables) de la matriz factorial, mientras que VARIMAX \index{rotación!VARIMAX} lo pone en la simplificación por columnas (factores), para satisfacer los requisitos de *estructura simple*; \index{estructura simple} así, aunque se pueda conseguir la simplicidad de cada variable y que, a la vez, las cargas respecto del mismo factor sean grandes, tal factor queda excluido por la restricción impuesta por la simplificación sobre cada factor [@harman1976]. 

##### Rotaciones oblicuas 
\index{rotaciones!oblicuas}

Superan la incorrelación u ortogonalidad de los factores y se suelen aplicar cuando: $(i)$ se sospecha que, en la población, los factores tienen una fuerte correlación y/o $(ii)$ cierta correlación entre los factores permite una gran ganancia en la interpretación de los mismos. Podrían aplicarse siempre, como norma general, puesto que, en realidad, la ortogonalidad es un caso particular de la oblicuidad.

Los procedimientos que proporcionan soluciones con estructura simple oblicua emanan de los mismos criterios objetivos que los que  proporcionan soluciones con estructura simple ortogonal. De hecho, si se relajan las condiciones de ortogonalidad, algunos procedimientos de rotación ortogonal pueden adaptarse al caso oblicuo (tal es el caso, por ejemplo, del método OBLIMAX,\index{rotación!OBLIMAX} a partir del criterio QUARTIMAX). Por otra parte, los métodos de rotación oblicua no solo son directos, sino que también pueden introducir los principios de estructura simple \index{estructura simple} que se requieren para la solución factorial primaria de forma indirecta (métodos indirectos). Las rotaciones oblicuas \index{rotaciones!oblicuas} exigen nuevos conceptos y nueva nomenclatura: \index{rotaciones!directas} \index{rotaciones!indirectas} \index{estructura simple!ortogonal} \index{estructura simple!oblicua}

- *Factores de referencia*, ${G}_m$, $m=1, 2,\cdots, k$: para cada factor rotado se puede encontrar un nuevo factor incorrelado con los rotados. A esos nuevos factores de les llama factores de referencia. En caso de rotación ortogonal, los factores de referencia coinciden con los primeros. \index{factores!de referencia}
- *Estructura factorial de referencia*: hasta ahora, se denominaba estructura factorial \index{estructura factorial} a la matriz de correlaciones entre las variables $Z_j$, ${j=1,2, \cdots, p}$ y los factores rotados, que en el caso ortogonal coincide con la matriz de cargas factoriales \index{cargas factoriales} rotadas. Pues bien, se denomina estructura factorial de referencia a la matriz de correlaciones entre las variables $Z_j$ y los factores de referencia. Si la rotación es ortogonal, coincide con la estructura factorial. \index{estructura factorial!de referencia}

- *Matriz de transformación*: \index{matriz!de transformación} en el caso oblicuo se representa por $\bf\Lambda$.

- *Estructura factorial oblicua*: $\bf V$, tal que $\bf V= \bf A \bf \Lambda$; sus elementos son $v_{jm}$. \index{estructura factorial!oblícua}
- *Cargas*: en el caso oblicuo el término "carga" \index{cargas factoriales} se utiliza para denotar la correlación de la variable con el eje de referencia: $v_{jm}=r_{Z_j;\Lambda_m}$.

Mientras las rotaciones ortogonales \index{rotaciones!ortogonales} intentan encontrar la estructura factorial más simple, las oblicuas \index{rotaciones!oblicuas} hacen lo mismo pero con la estructura de referencia. 

El método (directo) OBLIMAX maximiza la expresión $K=\frac{\sum_{j=1}^{p}\sum_{m=1}^{k}v_{jm}^4}{\left(\sum_{j=1}^{p}\sum_{m=1}^{k}v_{jm}^{2}\right)^2}$. Nótese que se trata del criterio QUARTIMAX ortogonal, pero incorporando el denominador, puesto que en la rotación oblicua ya no es constante.  \index{rotación!OBLIMAX}

El QUARTIMIN directo, también derivado del QUARTIMAX ortogonal, minimiza el criterio $N=\sum_{j=1}^{p}\sum_{m\leq q=1}^{k}v_{jm}^2v_{jq}^2$, y recibe este nombre por minimizar términos de cuarto grado. \index{rotación!QUARTIMIN}

La generalización del criterio "minimizar $H=\sum_{j=1}^{p}\sum _{m<q=1}^k b_{jm}^2b_{jq}^2$" para factores oblicuos se denomina OBLIMIN \index{rotación!OBLIMIN} \index{rotación!COVARIMIN}, y da lugar a métodos indirectos. Entre ellos, destaca el COVARIMIN, que se obtiene relajando la condición de ortogonalidad en el VARIMAX \index{rotación!VARIMAX}, minimizando las covarianzas de los cuadrados de los elementos de $\bf V$: $C^*=\sum_{m\leq q=1}^{k}\left(p\sum_{j=1}^{p} v_{jm}^2v_{jq}^2-\sum_{j=1}^ {p}v_{jm}^2\sum_{j=1}^ {p}v_{jq}^2\right)$. La versión COVARIMIN normalizada minimiza $C=\sum_{m\leq q=1}^{k}\left(p\sum_{j=1}^{p} \frac{v_{jm}^2}{h_j^2}\frac{v_{jq}^2}{h_j^2}-\sum_{j=1}^ {p}\frac {v_{jm}^2}{h_j^2}\sum_{j=1}^ {p}\frac{v_{jq}^2}{h_j^2}\right)$.

Se ha comprobado empíricamente que QUARTIMIN tiende a ser demasiado oblicuo y COVARIMIN demasiado ortogonal. Una solución intermedia es la rotación BIQUARTIMIN, que consiste en minimizar $B^*=H+\frac{C^*}{p}$, donde $\frac{C^*}{p}$ es la expresión completa del COVARIMIN. Una generalización de la rotación BIQUARTIMIN es $B^*=\alpha H+\beta \frac{C^*}{p}$. Sencillas operaciones aritméticas llevan a $B^*=\sum_{m< q=1}^{k}\left(p \sum_{j=1}^{p} v_{jm}^2 v_{jq}^2-\gamma \sum _{j=1}^{p}v_{jm}^2 \sum_{j=1}^{p}v_{jq}^2\right)$, con $\gamma= \frac {\beta}{\alpha + \beta}$. La rotación QUARTIMIN se obtiene con $\gamma=0$, la BIQUARTIMIN con $\gamma=0,5$ y la COVARIMIN con $\gamma=1$. También se pueden obtener versiones normalizadas sin más que normalizar las cargas (dividirlas por $h_{jm}^2$). \index{rotación!BIQUARTIMIN} \index{rotación!BINORMALMIN} \index{rotación!COVARIMIN} 

El criterio BINORMALMIN (normalizado) es una alternativa al BIQUARTIMIN para corregir el sesgo de oblicuidad del criterio COVARIMIN. Minimiza $D=\sum_{m< q=1}^{k}\left( \frac{\sum_{j=1}^{p} \frac {v_{jm}^2}{h_j^2} \frac {v_{jq}^2}{h_j^2}} {\sum _{j=1}^{p} \frac{v_{jm}^2}{h_j^2}\sum _{j=1}^{p} \frac{v_{jq}^2}{h_j^2}}\right)$. BINORMALMIN suele ser mejor con datos muy simples o muy complejos; BIQUARTIMIN es más recomendable con datos moderadamente complejos.

El método de rotación OBLIMIN \index{rotación!OBLIMIN} directo, en vez de proceder como $B^*$, que depende de los valores de la estructura, minimiza directamente una función de la matriz del patrón factorial \index{patrón factorial} primario: $F{(\bf{A})}= \sum_{m< q=1}^{k}\left(\sum_{j=1}^p a_{jm}^2 a_{jq}^2-\frac{\delta}{p}\sum_{j=1}^pa_{jm}^2\sum_{j=1}^pa_{jq}^2\right)$. Cuando $\delta=0$, se tiene el QUARTIMIN directo.

Hay otros tipos de transformaciones oblicuas, pero únicamente se mencionarán $(i)$ la ORTOBLICUA, \index{rotación!ORTOBLICUA} que llega a la solución oblicua mediante una serie de transformaciones ortogonales intermedias; y $(ii)$ el la PROMAX, \index{rotación!PROMAX} muy popular, que actúa alterando los resultados de una rotación ortogonal (concretamente elevando las cargas \index{cargas factoriales} de la rotación ortogonal a una potencia entre 2 y 4) hasta crear una solución con cargas factoriales \index{cargas factoriales} lo más próximas a la estructura ideal. Cuanto mayor es esta potencia más oblicua es la solución obtenida. 


##### ¿Rotaciones ortogonales u oblicuas?
\index{rotaciones!oblicuas}
\index{rotaciones!ortogonales}

\hspace{0px}La selección del método de rotación, ortogonal u oblicua, depende del objetivo perseguido. Si se pretende reducir el número de variables originales a un conjunto mucho menor de variables incorrelacionadas para su uso posterior en otra técnica, por ejemplo regresión, la rotación debe ser ortogonal. Si el objetivo es obtener unos factores teóricos significativos, puede resultar apropiada la aplicación de una rotación oblicua.

En **R** es muy sencillo implementar una rotación ortogonal u oblicua. Basta, por ejemplo, con utilizar la librería `GPArotation` [@Coen2005] e indicarlo en el argumento `rotate` de la función `fa`. A modo de ejemplo, extrayendo los factores por el método de los factores principales \index{método!de los factores principales} y utilizando una rotación VARIMAX normalizada, \index{rotación!VARIMAX normalizada} sería: 



\textcolor{red}{**AQUÍ SE CREA $af_facprin2$, ARRIBA YA SE LLAMABA**}

```r
library("GPArotation")
af_facprin2 <- fa(cor(TIC2021), nfactors=2, rotate="varimax", fm="pa", digits=3)
```


```r
af_facprin2 # el objeto contiene información adicional no relevante en estos momentos
#> Factor Analysis using method = pa
#> Standardized loadings (pattern matrix) based upon correlation matrix 
#>              PA1 PA2   h2    u2 com
#> ebroad     0.38 0.59 0.50 0.505 1.7
#> esales     0.02 0.74 0.55 0.447 1.0
#> esocmedia  0.46 0.68 0.68 0.322 1.7
#> eweb       0.50 0.75 0.82 0.182 1.7
#> hbroad     0.91 0.20 0.87 0.131 1.1
#> hiacc      0.96 0.26 0.98 0.018 1.1
#> iuse       0.72 0.60 0.88 0.125 1.9
#>
#>                 PA1  PA2
#> SS loadings    2.87 2.40
#> Proportion Var 0.41 0.34
#> Cumulative Var 0.41 0.75
```

Nótese que la salida por defecto es la normalizada. También se puede utilizar la libería `stats` indicando `T` o `F` en el argumento `normalize`, dependiendo de que se quiera o no, respectivamente, una rotación VARIMAX (u otra) normalizada \index{rotación!VARIMAX normalizada} .


```r
library(stats)
varimax(loadings(af_facprin),normalize=T)
```

En el ejemplo del uso las TIC en los países de la UE-27, la rotación VARIMAX \index{rotación!VARIMAX} ha conseguido facilitar la interpretación de los factores \index{interpretación de los factores} comunes \index{factores!comunes}, ya que, tras la rotación, las variables relacionadas con el uso de las TIC a escala individual y de hogar cargan en el primer factor, mientras que las relacionadas con el uso de las TIC a nivel empresarial cargan en el segundo. Por tanto, ambos factores pueden considerarse indicadores de la dotación y uso de las TIC en los ámbitos familiar y empresarial, respectivamente. El lector puede probar (y comparar) con otras rotaciones sin más que incluirlas en el argumento `rotate`.

### Post-análisis factorial{#postanalisis}

Realizado el AF, los siguientes procedimientos permiten comprobar la bondad del modelo obtenido:

**Análisis de las correlaciones residuales**  

Se entiende por bondad de la solución factorial la medida del grado en que los factores del modelo explican las correlaciones entre las variables. Por ello, parece natural que tal medida se base en la comparación entre las correlaciones observadas y las que se derivan del modelo factorial (reproducidas) o, en términos matriciales, en la magnitud de las entradas de la matriz de correlaciones residuales ${\bf R}^{res}={\bf R} - {\bf R}^{rep}$, donde ${\bf R}=\frac{1}{N} \bf Z \bf Z^{\prime}$ y ${\bf R}^{rep}=\bf A\bf \Phi\bf A^{\prime}=\bf \Gamma \bf A^{\prime}$ (relación fundamental entre el patrón y la estructura factorial; en caso de incorrelación entre los factores, $\bf \Phi=\bf I$ y ${\bf R}^{rep}=\bf A {\bf A}^{\prime}$. La matriz ${\bf R}^{rep}$ se obtiene sin más que sustituir $\bf Z$ por $\bf A \bf F$ en la expresión de $\bf R$. \index{estructura factorial}


Ahora bien, ¿cuál es el criterio apropiado para concluir si una solución factorial es aceptable o no? Para que sea aceptable, los elementos (los residuos) de  ${\bf R}^{res}$ deben ser cercanos a cero, y como todos los factores comunes \index{factores!comunes} han sido considerados, se supone que no existen más vínculos entre las variables y que la distribución de dichos residuos debe ser como la de correlación cero en una muestra del mismo tamaño. Por tanto, como $\sigma_{r=0}=\frac{1}{\sqrt{N-1}}$, entonces $S_{r_{res}}\leq\frac{1}{\sqrt{N-1}}:$[^af-14]

[^af-14]: Este criterio tiene como ventaja la simplicidad. Sin embargo, sería conveniente que tuviese en cuenta, al menos, el número de variables.

- Si $S_{r_{res}}\gg\frac{1}{\sqrt{N-1}}$, es razonable pensar que existen relaciones adicionales significativas entre las variables y hay que modificar la solución factorial. \index{solución factorial}
- Si $S_{r_{res}}\ll\frac{1}{\sqrt{N-1}}$, es razonable pensar que la solución factorial incluye relaciones que no están justificadas.
- Si $S_{r_{res}}\leq pero \hspace{0,07cm} no\ll \frac{1}{\sqrt{N-1}}$, la solución es aceptable.

Otra posibilidad, también muy sencilla, propuesta por @Revelle2022, es utilizar $fit= 1-\frac{\sum \left (\bf R-{\bf FF}^{\prime}\right)^2}{\sum (\bf R)^2}$, que indica la reducción proporcional en la matriz de correlación debida al modelo factorial. Nótese que esta medida es sensible al tamaño de las correlaciones originales. Es decir, si los residuos son pequeños, pero las correlaciones son pequeñas, el ajuste es malo. Las medidas clásicas como el RMSE (raíz cuadrada del error cuadrático medio),  o similares, también son susceptibles de uso.

En el ejemplo TIC seguido en este capítulo el ajuste realizado es muy bueno:


```r
round(af_facprin2$residual, 3)
#>           ebroad esales esocmedia   eweb hbroad  hiacc   iuse
#> ebroad     0.505 -0.068     0.008  0.068 -0.045  0.002  0.003
#> esales    -0.068  0.447     0.026  0.015  0.004 -0.012  0.021
#> esocmedia  0.008  0.026     0.322 -0.047  0.014 -0.005  0.017
#> eweb       0.068  0.015    -0.047  0.182  0.012  0.015 -0.042
#> hbroad    -0.045  0.004     0.014  0.012  0.131 -0.005  0.010
#> hiacc      0.002 -0.012    -0.005  0.015 -0.005  0.018  0.002
#> iuse       0.003  0.021     0.017 -0.042  0.010  0.002  0.125
af_facprin2$rms
#> [1] 0.02907475
af_facprin2$fit
#> [1] 0.9715865
```

::: {.infobox data-latex=""}
**NOTA IMPORTANTE**

Como se avanzó en la introducción, el AF está enfocado al ajuste de las correlaciones entre las variables observadas mediante el patrón factorial \index{patrón factorial} correspondiente al modelo \@ref(eq:eqaf2) (con los factores comunes \index{factores!comunes} y el factor único). Pues bien, si en el proceso reproductivo se utiliza el modelo sólo con los factores comunes, la matriz de correlaciones \index{matriz!de correlaciones} que se reproduce es $\bf R$, lo que implica el modelo ACP (modelo \@ref(eq:eqaf1)). Si se incluye también el factor específico, la matriz de correlaciones que se reproduce es ${\bf R}^*$ (modelo AF). Si en dicha reproducción se utilizasen los factores comunes y el término de error, se reproduciría $\bf R$ con una diagonal principal cuyas entradas serían la unidad menos las estimaciones de las  comunalidades. \index{factores!únicos} \index{factores!específicos} \index{comunalidad}

:::

**Test de bondad de ajuste** 

Se trata de un contraste de razón de verosimilitudes que se puede llevar a cabo cuando se extraigan los factores por el método de máxima verosimilitud. \index{método!de máxima verosimilitud} La hipótesis nula es la suficiencia de $k$ factores comunes para explicación de las correlaciones entre las variables originales y de la varianza que comparten. \index{contraste!de razón de verosimilitudes}

El estadístico del contraste es $-2ln\lambda=np(\hat{a} - ln \hat{g} -1]$, donde $\hat{a}$ y $\hat{g}$ son las medias aritmética y geométrica, respectivamente, de los autovalores \index{autovalor} de la matriz $\hat{\boldsymbol{\Sigma}}_{H_{0}}^{-1} \mathbf{S}$. Bajo $H_0$, se distribuye asintóticamente como una $\chi_{df}^2$, con $df= \left( p+\frac{p(p+1)}{2}\right) - \left( p+pk+p-\frac{k(k-1)}{2}\right)= \frac{1}{2} (p-k)^2- \frac{1}{2}(p+k)$.[^af-15]

[^af-15]: $df$ indica la medida en que el modelo factorial ofrece una interpretación más simple que $\bf \Sigma$.


Este test se aplica de manera secuencial: se formula como hipótesis nula $k=0$. Si no se rechaza, no hay factores comunes \index{factores!comunes} subyacentes. Si se rechaza, se sigue con $k=1$. Si no se rechaza $k=1$, se concluye que el modelo con un factor es una adecuada representación de la realidad; si se rechaza, se formula la hipótesis nula de que $k=2$, y el proceso continúa hasta que no se rechace la hipótesis nula, siempre que el valor de $k$ sea compatible con un número de grados de libertad positivo.

### Puntuaciones factoriales \index{puntuaciones factoriales}

Las puntuaciones factoriales son  las estimaciones de los valores de los factores aleatorios no observados, es decir, de los elementos de ${\bf F}_{mxm}$. Así, $\hat{f}_{im}$ será la estimación del valor del $m$-ésimo factor para la $i$-ésima observación (elemento, individuo, objeto...). Cuando se extraen los factores por componentes principales las puntuaciones son exactas.

Estas estimaciones pueden ser usadas como inputs para posteriores análisis (regresión, cluster, etc.) en los que se trabaje con los mismos elementos o individuos, sustituyendo las variables originales por los nuevos factores obtenidos. La cuestión es: ¿cómo calcular estas puntuaciones?, porque tanto los factores como los errores no son observables sino aleatorios. 

Los métodos más populares para obtener la estimación de las puntuaciones factoriales \index{puntuaciones factoriales} son:

- El de regresión por mínimos cuadrados ordinarios (MCO), donde $\hat{\bf F}=\left (\bf A^{\prime} \bf A \right)^{-1}\bf A^{\prime}\bf Z$.

- El de Bartlett, basado en el método de estimación por mínimos cuadrados generalizados (MCG), \index{método!de Barlett} con $\hat{\bf F}=\left (\bf A^{\prime} \bf \Psi ^{-1}\bf A \right)^{-1}\bf A^{\prime}\Psi ^{-1}\bf Z$. El mismo estimador se puede obtener por máxima verosimilitud asumiendo normalidad multivariante.

- El de Thompson (con un enfoque bayesiano), donde $\hat{\bf F}=\left (\bf I+\bf A^{\prime} \bf \Psi ^{-1}\bf A \right)^{-1}\bf A^{\prime}\Psi ^{-1}\bf Z$. \index{método!de Thompson}

- El de Anderson-Rubin (que obtiene estimaciones MCG imponiendo la condición $\bf F^{\prime}F =I$ ($\hat{\bf F}=\left (\bf A^{\prime} \bf \Psi ^{-1}\bf R \bf \Psi ^{-1}\bf A \right)^{-1}\bf A^{\prime}\Psi ^{-1}\bf Z$). \index{método!de Anderson-Rubin}

Las ventajas y desventajas de cada uno de ellos pueden verse en @mardiaetal1979 y @santiagodelafuente2011.  



<!-- El objetivo es obtener para el factor $m$-ésimo: \begin{equation} -->
<!-- \hat{F}_{m} =c_{1m} Z_{1}+c_{2m} Z_{2}+ \dotsb + c_{p}m Z_{p} = \mathbf{c}_{j}^{\prime} \mathbf{Z} = \mathbf{Z}^{\prime} \mathbf{c}_{j} -->
<!-- (\#eq:ecuacion13) -->
<!-- \end{equation} -->

<!-- de forma que la observación $i$-ésima correspondiente al factor $m$-ésimo es: \begin{equation} -->
<!-- \hat{f}_{im} =c_{1m} Z_{1i}+c_{2m} Z_{2i}+ \dotsb + c_{pm} Z_{pm} -->
<!-- (\#eq:ecuacion14) -->
<!-- \end{equation} -->

<!-- siendo $c_{j}$ el vector de coeficientes que minimiza la esperanza de los errores al cuadrado,  quedando la estimación de las puntuaciones del factor j-ésimo como: \begin{equation} -->
<!-- \hat{F}_{j} = \mathbf{c}_{j}^T \mathbf{Z} = \mathbf{l}_{j}^T \boldsymbol{\Sigma}^{-1}\mathbf{X} -->
<!-- (\#eq:ecuacion15) -->
<!-- \end{equation} -->
<!-- \textcolor{red}{LAS PUNTUACIONES SON LOS SCORES NO?} -->
<!-- Estas puntuaciones se obtienen a través de los scores, que por defecto son calculados usando el método de regresión, y pueden usarse para análisis futuros en lugar de las variables originales. -->

En el ejemplo de las TIC, las puntuaciones de los dos factores extraídos con el método de los factores principales \index{método!de los factores principales} y rotados con \index{rotación!VARIMAX} VARIMAX (la rotación no afecta a las puntuaciones), calculadas por el método de regresión, para los países de la UE-27 (se muestran los de Bélgica, Bulgaria y la República Checa), se obtienen en **R** como sigue:


```r
af_facprin3 <- fa(cor(TIC2021), nfactors=2, rotate="VARIMAX", fm="pa", scores="regression") 
factor.scores(TIC2021, af_facprin3)$scores[1:3,]
#>           PA1         PA2
#> BE  0.6256359  1.01289866
#> BG -2.1820404 -0.03439974
#> CZ -0.2189723  1.08635525
```


## Relaciones y diferencias entre el AF y el ACP

ACF y AF son aparentemente muy similares, pero en realidad son muy diferentes. 
Tanto ACP como AF son técnicas de reducción de la dimensionalidad que aparecen juntas en los paquetes estadísticos y persiguen objetivos muy similares, lo cual, en determinadas ocasiones, lleva al lector a pensar que son intercambiables entre sí, cuando ello no es cierto. Por ello, este capítulo finaliza con un breve comentario sobre las diferencias más relevantes entre ambos enfoques.

La primera es que ACP es una mera transformación de los datos en la que no se hace ningún supuesto sobre la matriz de covarianzas \index{matriz!de covarianzas} o de correlaciones. \index{matriz!de correlaciones} Sin embargo, AF asume que los datos proceden de un modelo bien definido, el modelo \@ref(eq:eqaf3), en el que los factores subyacentes \index{factores!subyacentes} satisfacen unos supuestos bien definidos.

En segundo lugar, en ACP el énfasis se pone en el paso desde las variables observadas a las componentes principales, \index{componentes principales} mientras que en AF se pone en el paso desde los factores latentes a las variables observadas. Es cierto que en ACP se pueden retener $k$ componentes y a partir de ellas aproximar (reproducir) las variables observadas; sin embargo, esta manera de proceder parece menos natural que la aproximación de las variables observadas en términos de los factores comunes \index{factores!comunes} y, además, al no tener en cuenta la unicidad \index{unicidad} de las variables, sobrestima las cargas factoriales \index{cargas factoriales} y la dimensionalidad del conjunto de variables originales.

Una tercera diferencia es que, mientras que ACP obtiene componentes en función de las variables originales (los valores de las variables pueden ser estimados a posteriori en función de dichas componentes o factores), en AF las variables son, ellas mismas, combinaciones lineales de factores desconocidos. Es decir, mientras que en ACP la solución viene de la mano de la descomposición en valores singulares, en AF requiere procedimientos de estimación, normalmente iterativos. 

La cuarta es que ACF es un procedimiento cerrado mientras que AF es abierto, en el sentido de que explica la varianza común \index{varianza!común} y no toda la varianza.

Finalmente, como pudo verse en \@ref(metodosdeextraccion), cuando las varianzas de los factores únicos son prácticamente nulas, el método de los factores principales \index{método!de los factores principales} es equivalente a ACP, y cuando son pequeñas ambos dan resultados similares. Sin embargo, cuando son grandes, en ACP las componentes principales \index{componentes principales} (tanto las retenidas como las que no se retienen) las absorben, mientras que el AF las considera y les da su lugar.  
 
::: {.infobox_resume data-latex=""}

**RESUMEN**

El Análisis Factorial\index{análisis!factorial} es una técnica de reducción de la dimensionalidad que trata de dar una explicación de la varianza compartida,  \index{varianza!compartida} o común, de las variables objeto de estudio (no de toda la varianza, como hace el análisis de componentes principales) mediante un número mucho menor de factores comunes \index{factores!comunes} latentes. Por consiguiente, solo tiene sentido implementarlo si dichas variables se encuentran fuertemente correlacionadas. Tras introducir al lector en los principales elementos teóricos del Análisis Factorial (el modelo básico y la solución factorial completa), \index{solución factorial!completa} se abordan las distintas etapas del procedimiento en su vertiente práctica: $(i)$ el pre-análisis factorial, que responde a la pregunta de si procede o no llevarlo a cabo; $(ii)$ el análisis factorial propiamente dicho, prestando especial atención a los métodos de extracción de los factores y a las rotaciones de los mismos para facilitar su interpretación; y $(iii)$ el post-análisis factorial, que incluye una serie de procedimientos para determinar si la solución factorial obtenida es o no aceptable. Posteriormente, se aborda la cuestión de cómo estimar los valores de los factores obtenidos para cada elemento o individuo involucrado en el análisis, pues estas estimaciones pueden usarse como inputs en análisis posteriores (regresión, cluster, etc.) sustituyendo las variables originales por los factores obtenidos. El capítulo finaliza con algunos comentarios sobre las diferencias entre el análisis factorial y el de componentes principales, \index{componentes principales} aparentemente muy similares, pero en realidad muy diferentes.
:::



