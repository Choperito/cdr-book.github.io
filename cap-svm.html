<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Capítulo 25 Máquinas de vector soporte | Fundamentos de ciencia de datos con R</title>
<meta name="author" content="Gema Fernández-Avilés y José-María Montero">
<meta name="description" content="Ramón A. Carrasco\(^{a}\) e Itzcóatl Bueno\(^{b,a}\) \(^{a}\)Universidad Complutense de Madrid \(^{b}\)Instituto Nacional de Estadística  25.1 Introducción Aunque las máquinas de vector soporte se...">
<meta name="generator" content="bookdown 0.28 with bs4_book()">
<meta property="og:title" content="Capítulo 25 Máquinas de vector soporte | Fundamentos de ciencia de datos con R">
<meta property="og:type" content="book">
<meta property="og:image" content="/img/cover.png">
<meta property="og:description" content="Ramón A. Carrasco\(^{a}\) e Itzcóatl Bueno\(^{b,a}\) \(^{a}\)Universidad Complutense de Madrid \(^{b}\)Instituto Nacional de Estadística  25.1 Introducción Aunque las máquinas de vector soporte se...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Capítulo 25 Máquinas de vector soporte | Fundamentos de ciencia de datos con R">
<meta name="twitter:description" content="Ramón A. Carrasco\(^{a}\) e Itzcóatl Bueno\(^{b,a}\) \(^{a}\)Universidad Complutense de Madrid \(^{b}\)Instituto Nacional de Estadística  25.1 Introducción Aunque las máquinas de vector soporte se...">
<meta name="twitter:image" content="/img/cover.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.2/transition.js"></script><script src="libs/bs3compat-0.4.2/tabs.js"></script><script src="libs/bs3compat-0.4.2/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><link href="libs/tabwid-1.1.0/tabwid.css" rel="stylesheet">
<link href="libs/tabwid-1.1.0/scrool.css" rel="stylesheet">
<script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css">
<link rel="stylesheet" href="bs4_book.css">
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Fundamentos de ciencia de datos con R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Buscar" aria-label="Buscar">
</form>

      <nav aria-label="Contenido"><h2>Contenido</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Prefacio</a></li>
<li class="book-part">Ciencia, datos, software… y científicos</li>
<li><a class="" href="ciencia-datos.html"><span class="header-section-number">1</span> ¿Es la ciencia de datos una ciencia?</a></li>
<li><a class="" href="metodologia.html"><span class="header-section-number">2</span> Metodología en ciencia de datos</a></li>
<li><a class="" href="ch-110003.html"><span class="header-section-number">3</span> R para ciencia de datos</a></li>
<li><a class="" href="cap-etica.html"><span class="header-section-number">4</span> Ética en la ciencia de datos</a></li>
<li class="book-part">Bienvenidos a la jungla de datos</li>
<li><a class="" href="datos-sql.html"><span class="header-section-number">5</span> Gestión de bases de datos relacionales</a></li>
<li><a class="" href="cap-nosql.html"><span class="header-section-number">6</span> Gestión de bases de datos NoSQL</a></li>
<li><a class="" href="DGDQM.html"><span class="header-section-number">7</span> Gobierno, gestión y calidad del dato</a></li>
<li><a class="" href="id_130009.html"><span class="header-section-number">8</span> Integración y limpieza de datos</a></li>
<li><a class="" href="chap-feature.html"><span class="header-section-number">9</span> Selección y transformación de variables</a></li>
<li><a class="" href="chap-herramientas.html"><span class="header-section-number">10</span> Herramientas para el análisis en ciencia de datos</a></li>
<li><a class="" href="id_120006-aed.html"><span class="header-section-number">11</span> Análisis exploratorio de datos</a></li>
<li class="book-part">Fundamentos de estadística</li>
<li><a class="" href="Funda-probab.html"><span class="header-section-number">12</span> Probabilidad</a></li>
<li><a class="" href="Fundainfer.html"><span class="header-section-number">13</span> Inferencia estadística</a></li>
<li><a class="" href="muestreo.html"><span class="header-section-number">14</span> Muestreo y remuestreo</a></li>
<li class="book-part">Modelización estadística</li>
<li><a class="" href="cap-lm.html"><span class="header-section-number">15</span> Modelización lineal</a></li>
<li><a class="" href="cap-glm.html"><span class="header-section-number">16</span> Modelos lineales generalizados</a></li>
<li><a class="" href="cap-gam.html"><span class="header-section-number">17</span> Modelos aditivos generalizados</a></li>
<li><a class="" href="cap-mxm.html"><span class="header-section-number">18</span> Modelos mixtos</a></li>
<li><a class="" href="cap-sparse.html"><span class="header-section-number">19</span> Modelos sparse y métodos penalizados de regresión</a></li>
<li><a class="" href="cap-series-temp.html"><span class="header-section-number">20</span> Modelización de series temporales</a></li>
<li><a class="" href="cap-discriminante.html"><span class="header-section-number">21</span> Análisis discriminante</a></li>
<li><a class="" href="cap-conjunto.html"><span class="header-section-number">22</span> Análisis conjunto</a></li>
<li><a class="" href="tablas-contingencia.html"><span class="header-section-number">23</span> Análisis de tablas de contingencia</a></li>
<li class="book-part">Machine learning supervisado</li>
<li><a class="" href="cap-arboles.html"><span class="header-section-number">24</span> Árboles de clasificación y regresión</a></li>
<li><a class="active" href="cap-svm.html"><span class="header-section-number">25</span> Máquinas de vector soporte</a></li>
<li><a class="" href="cap-knn.html"><span class="header-section-number">26</span> Clasificador k-vecinos más próximos</a></li>
<li><a class="" href="cap-naive-bayes.html"><span class="header-section-number">27</span> Naive Bayes</a></li>
<li><a class="" href="cap-bagg-rf.html"><span class="header-section-number">28</span> Métodos ensamblados: bagging y random forest</a></li>
<li><a class="" href="cap-boosting-xgboost.html"><span class="header-section-number">29</span> Boosting y el algoritmo XGBoost</a></li>
<li class="book-part">Machine learning no supervisado</li>
<li><a class="" href="jerarquico.html"><span class="header-section-number">30</span> Análisis cluster: clusterización jerárquica</a></li>
<li><a class="" href="no-jerarquico.html"><span class="header-section-number">31</span> Análisis cluster: clusterización no jerárquica</a></li>
<li><a class="" href="acp.html"><span class="header-section-number">32</span> Análisis de componentes principales</a></li>
<li><a class="" href="an%C3%A1lisis-factorial.html"><span class="header-section-number">33</span> Análisis factorial</a></li>
<li><a class="" href="escalamiento-multidimensional.html"><span class="header-section-number">34</span> Escalamiento multidimensional</a></li>
<li><a class="" href="correspondencias.html"><span class="header-section-number">35</span> Análisis de correspondencias</a></li>
<li class="book-part">Deep learning</li>
<li><a class="" href="capNN.html"><span class="header-section-number">36</span> Redes neuronales artificiales</a></li>
<li><a class="" href="cap-redes-convol.html"><span class="header-section-number">37</span> Redes neuronales convolucionales</a></li>
<li class="book-part">Ciencia de datos de texto y redes</li>
<li><a class="" href="mineria-textos.html"><span class="header-section-number">38</span> Minería de textos</a></li>
<li><a class="" href="grafos.html"><span class="header-section-number">39</span> Análisis de grafos y redes sociales</a></li>
<li class="book-part">Ciencia de datos espaciales</li>
<li><a class="" href="datos-espaciales.html"><span class="header-section-number">40</span> Trabajando con datos espaciales</a></li>
<li><a class="" href="geo.html"><span class="header-section-number">41</span> Geoestadística</a></li>
<li><a class="" href="cap-econom-esp.html"><span class="header-section-number">42</span> Modelos econométricos espaciales</a></li>
<li><a class="" href="cap-pp.html"><span class="header-section-number">43</span> Procesos de puntos</a></li>
<li class="book-part">Comunica y colabora</li>
<li><a class="" href="id_120007-informes.html"><span class="header-section-number">44</span> Informes reproducibles con R Markdown y Quarto</a></li>
<li><a class="" href="shiny.html"><span class="header-section-number">45</span> Creación de aplicaciones web interactivas con Shiny</a></li>
<li><a class="" href="github.html"><span class="header-section-number">46</span> Git y GitHub R</a></li>
<li><a class="" href="geoproces.html"><span class="header-section-number">47</span> Geoprocesamiento en nube</a></li>
<li class="book-part">Casos de estudio en ciencia de datos</li>
<li><a class="" href="cap-crimen.html"><span class="header-section-number">48</span> Análisis de una red criminal</a></li>
<li><a class="" href="cap-publicidad.html"><span class="header-section-number">49</span> Optimización de inversiones publicitarias</a></li>
<li><a class="" href="cap-twitter.html"><span class="header-section-number">50</span> ¿Cómo twitea Elon Musk?</a></li>
<li><a class="" href="cap-periodismo.html"><span class="header-section-number">51</span> Análisis electoral: de Rstudio a su periódico</a></li>
<li><a class="" href="paro-clm.html"><span class="header-section-number">52</span> Crisis: impacto en el paro de Castilla-La Mancha</a></li>
<li><a class="" href="cap-rfm.html"><span class="header-section-number">53</span> Segmentación de clientes en el comerico minorista</a></li>
<li><a class="" href="cap-medicina.html"><span class="header-section-number">54</span> Análisis de datos en medicina</a></li>
<li><a class="" href="cap-futbol.html"><span class="header-section-number">55</span> Messi y Ronaldo: dos ídolos desde la perspectiva de los datos</a></li>
<li><a class="" href="cambioclimatico.html"><span class="header-section-number">56</span> Un dato sobre el cambio climático</a></li>
<li><a class="" href="cap-ree.html"><span class="header-section-number">57</span> Predicción de consumo eléctrico con redes neuronales</a></li>
<li><a class="" href="cap-sist-exp.html"><span class="header-section-number">58</span> Implementación de un sistema experto en el ámbito pediátrico</a></li>
<li><a class="" href="nlp-textil.html"><span class="header-section-number">59</span> El procesamiento del lenguaje natural para tendencias de moda en textil</a></li>
<li><a class="" href="cap-fraude.html"><span class="header-section-number">60</span> Detección de fraude de tarjetas de crédito</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="info-session.html"><span class="header-section-number">A</span> Información de la sesión</a></li>
<li><a class="" href="referncias.html">Referncias</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="cap-svm" class="section level1" number="25">
<h1>
<span class="header-section-number">Capítulo 25</span> Máquinas de vector soporte<a class="anchor" aria-label="anchor" href="#cap-svm"><i class="fas fa-link"></i></a>
</h1>
<p><em>Ramón A. Carrasco</em><span class="math inline">\(^{a}\)</span> e <em>Itzcóatl Bueno</em><span class="math inline">\(^{b,a}\)</span></p>
<p><span class="math inline">\(^{a}\)</span>Universidad Complutense de Madrid
<span class="math inline">\(^{b}\)</span>Instituto Nacional de Estadística</p>
<div id="introducción-11" class="section level2" number="25.1">
<h2>
<span class="header-section-number">25.1</span> Introducción<a class="anchor" aria-label="anchor" href="#introducci%C3%B3n-11"><i class="fas fa-link"></i></a>
</h2>
<p>Aunque las máquinas de vector soporte se desarrollaron en los años 90 dentro de la comunidad informática (<span class="citation">(<a href="referncias.html#ref-boser1992training">Boser, Guyon, and Vapnik 1992</a>)</span>, <span class="citation">(<a href="referncias.html#ref-cortes1995support">Cortes and Vapnik 1995</a>)</span>) como un método de clasificación binaria, su aplicación se ha extendido a problemas de clasificación múltiple y regresión. Como técnica de clasificación, las máquinas de vector soporte (SVM por sus siglas inglés <em>Support Vector Machines</em>) son similares a la regresión logística pero la SVM enfatiza en un margen de error aceptable en torno a la frontera de decisión. </p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:svm-log"></span>
<img src="img/svm_vs_log.png" alt="SVM vs Regresión logística." width="60%"><p class="caption">
Figura 25.1: SVM vs Regresión logística.
</p>
</div>
<p>En la Fig. <a href="cap-svm.html#fig:svm-log">25.1</a> se muestra que la regresión logística divide las observaciones en dos clases de tal forma que se minimice la distancia entre los puntos y la <strong>frontera de decisión</strong> (A). Por otro lado, la frontera de decisión (B) de la SVM separa los datos en dos clases, pero maximizando la distancia entre esta y los puntos de ambas clases. El <strong>margen</strong> es la distancia entre la frontera de decisión y los puntos más cercanos. El margen es una parte clave del SVM, puesto que evita clasificaciones erróneas de casos futuros como podría pasar en el caso de la regresión logística y como se ilustra en la Fig. <a href="cap-svm.html#fig:new-svm-obs">25.2</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:new-svm-obs"></span>
<img src="img/new_svm_obs.png" alt="Nueva observación clasificada en SVM vs Regresión logística." width="60%"><p class="caption">
Figura 25.2: Nueva observación clasificada en SVM vs Regresión logística.
</p>
</div>
<p>En resumen, los nuevos datos pueden ser clasificados dentro del margen. Cuanto mayor sea este margen, mayor será la capacidad para clasificar correctamente estos puntos. Por tanto, para obtener una clasificación errónea en la SVM es necesario que una observación se clasifique aún más allá del margen que en cualquier otro discriminante lineal. En problemas reales, es difícil que los discriminantes lineales, vistos en el Cap. <a href="cap-discriminante.html#cap-discriminante">21</a>, logren una línea que divida perfectamente las categorías a clasificar. Sin embargo, en la SVM, se incluye en la función objetivo (que mide la calidad del ajuste de los datos de entrenamiento) una penalización a los puntos que queden del lado equivocado del límite de decisión. En caso de que los datos puedan ser divididos linealmente, no se cometerá ninguna penalización y se maximizará el margen. Mientras que si los datos no son linealmente separables, el mejor ajuste vendrá dado por el equilibrio entre una penalización del error total bajo y un margen de decisión grande. La penalización a una observación mal clasificada es proporcional a la distancia desde la frontera de decisión.</p>
<p>Sin embargo, la SVM también tiene desventajas reseñables. En primer lugar, la SVM no es adecuada en conjuntos de datos grandes porque la complejidad de entrenamiento es elevada. Además, la SVM no funciona bien cuando los datos tienen mucho ruido, es decir, cuando las clases se superponen. Finalmente, si el conjunto de datos de entrenamiento tiene más variables que observaciones, el rendimiento del modelo disminuirá.</p>
</div>
<div id="algoritmo-svm-para-clasificación-binaria" class="section level2" number="25.2">
<h2>
<span class="header-section-number">25.2</span> Algoritmo SVM para clasificación binaria<a class="anchor" aria-label="anchor" href="#algoritmo-svm-para-clasificaci%C3%B3n-binaria"><i class="fas fa-link"></i></a>
</h2>
<p>
El algoritmo por el que se obtiene un modelo SVM <span class="citation">(<a href="referncias.html#ref-vapnik1997support">Vapnik 1997</a>)</span> se basa en la ecuación del hiperplano compuesta por dos hiperparámetros: un vector de números reales <span class="math inline">\(\omega\)</span> de la misma dimensión que el vector de variables de entrada <span class="math inline">\(x\)</span>, y un número real <span class="math inline">\(b\)</span> tal que:</p>
<span class="math display">\[\begin{equation}
  \omega x- b=0
\end{equation}\]</span>
<p>Donde <span class="math inline">\(\omega x\)</span> es <span class="math inline">\(\omega^{(1)}x^{(1)} + \omega^{(2)}x^{(2)} + \dots + \omega^{(p)}x^{(p)}\)</span> siendo <span class="math inline">\(p\)</span> el número de variables incluidas en <span class="math inline">\(x\)</span>. De este modo, la predicción para una instancia de <span class="math inline">\(x\)</span> viene dada por:</p>
<span class="math display">\[\begin{equation}
  y=sign(\omega x-b)
\end{equation}\]</span>
<p>Siendo <span class="math inline">\(sign\)</span> el operador que devuelve +1 para cualquier valor positivo y -1 para los valores negativos. Por tanto, el objetivo es ajustar los valores óptimos de <span class="math inline">\(\omega\)</span> y <span class="math inline">\(b\)</span> para el algoritmo. Estos hiperparámetros se obtienen resolviendo un problema de optimización sujeto a las siguientes restricciones:</p>
<p><span class="math display">\[\begin{eqnarray}
\omega x_i - b\geq 1 \textrm{ si } y_i &amp;= +1 \textrm{ y } \\
\omega x_i - b\leq 1 \textrm{ si } y_i &amp;= -1
\end{eqnarray}\]</span></p>
<p>Además, el objetivo del problema de optimización es maximizar el margen en torno a la frontera de decisión. Para conseguir esto es necesario minimizar la norma euclídea, y, por tanto, el problema a resolver es:</p>
</div>
<div id="y-si-tengo-más-de-dos-clases" class="section level2" number="25.3">
<h2>
<span class="header-section-number">25.3</span> ¿Y si tengo más de dos clases?<a class="anchor" aria-label="anchor" href="#y-si-tengo-m%C3%A1s-de-dos-clases"><i class="fas fa-link"></i></a>
</h2>
<p></p>
<p>Hasta ahora se ha presentado la SVM como un algoritmo solo aplicable a la clasificación de dos clases pero ¿y si se tienen más de dos clases? En general, hay dos enfoques para resolver esto: <strong>uno contra todos</strong> (OVA, por <em>One Vs All</em>) y <strong>uno contra uno</strong> (OVO, por <em>One Vs One</em>). En el enfoque OVA, se ajusta una SVM para cada clase, es decir una clase contra las demás y se clasifica a la clase para la cual el margen es mayor. En cambio, en el enfoque OVO se ajustan todas las SVM por pares y se clasifica a la clase que gane las competiciones por pares.</p>
</div>
<div id="truco-del-kernel-tratando-con-la-no-linealidad" class="section level2" number="25.4">
<h2>
<span class="header-section-number">25.4</span> Truco del <em>kernel</em>: tratando con la no linealidad<a class="anchor" aria-label="anchor" href="#truco-del-kernel-tratando-con-la-no-linealidad"><i class="fas fa-link"></i></a>
</h2>
<p>Las SVM funcionan muy bien si la separación entre clases es lineal. Sin embargo, si la separación es más compleja se intenta transformar el espacio en otro de mayor dimensionalidad donde las clases sí sean separables linealmente. Para ello, el modelo SVM se extiende incluyendo la función de pérdida (<span class="math inline">\(\ell\)</span>)
“hinge” (<span class="citation">(<a href="referncias.html#ref-gentile1998linear">Gentile and Warmuth 1998</a>)</span>,<span class="citation">(<a href="referncias.html#ref-lee2013study">Lee and Lin 2013</a>)</span>) definida como:</p>
<span class="math display">\[\begin{equation}
\ell(y_i) = \max(0,1-y_i(\omega x_i-b))
\end{equation}\]</span>
<p>En machine learning, esta función de pérdida se utiliza para entrenar clasificadores, más concretamente para la clasificación por el margen máximo (métodos de clasificación binaria que se utiliza cuando hay una frontera lineal que separa perfectamente los datos de entrenamiento de una categoría de los de la otra), sobre todo para las SVM. La función de pérdida es cero cuando se cumplen las restricciones, es decir, si <span class="math inline">\(\omega x_i\)</span> es clasificado en el lado correcto de la frontera de decisión. Por otro lado, si un dato es mal clasificados, el valor obtenido con la función de pérdida es proporcional a la distancia hasta la frontera de decisión. Por tanto, el objetivo es minimizar la función de coste:</p>
<span class="math display" id="eq:svmloss">\[\begin{equation}
C||\omega||^{2}+\frac{1}{N}\sum_{i=1}^{N}{\max(0,1-y_i(\omega x_i-b))}
\tag{25.1}
\end{equation}\]</span>
<p>Donde <span class="math inline">\(C\)</span> es un hiperparámetro que controla la compensación entre incrementar el tamaño de la frontera de decisión y asegurar que cada <span class="math inline">\(x_i\)</span> sea clasificado en el lado correcto de la frontera de decisión.</p>
<p>Un modelo SVM que optimiza la función de pérdida se denomina SVM <em>soft-margin</em> mientras que el modelo original es conocido como SVM <em>hard-margin</em>. La ecuación <a href="cap-svm.html#eq:svmloss">(25.1)</a> muestra que para valores grandes de <span class="math inline">\(C\)</span> el segundo término es despreciable, por lo que el algoritmo ignorará por completo la clasificación errónea y tratará de obtener el mayor margen posible. Si se reduce el valor de <span class="math inline">\(C\)</span>, se penaliza más cada error de clasificación, por lo que se cometerán menos errores sacrificando amplitud del margen.</p>
<p>A veces no es posible separar los datos por un hiperplano en su espacio original. Sin embargo, el <strong>truco del kernel</strong> utiliza una función que implícitamente transforma el espacio original a un espacio de mayor dimensión durante la optimización de la función de coste, como se muestra en la Fig. <a href="cap-svm.html#fig:kernel-trick">25.3</a>. Así, es posible transformar un espacio de datos bidimensional no separable linealmente en un espacio de datos tridimensional linealmente separable usando un mapeo específico definido por <span class="math inline">\(\phi:x\rightarrow\phi(x)\)</span> donde <span class="math inline">\(\phi(x)\)</span> es un vector de mayor dimensión que <span class="math inline">\(x\)</span>. Sin embargo, no se conoce la función de mapeo que funcionará en los datos. Si se prueban todas las transformaciones posibles, podría ser ineficiente y no llegar a la resolución del problema de clasificación planteado.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:kernel-trick"></span>
<img src="img/kernel-trick.png" alt="Izquierda: Las dos clases en el espacio original (2-D). Derecha: Las dos clases en el espacio de sobredimensionado (3-D)." width="100%"><p class="caption">
Figura 25.3: Izquierda: Las dos clases en el espacio original (2-D). Derecha: Las dos clases en el espacio de sobredimensionado (3-D).
</p>
</div>
<p>Se puede trabajar eficientemente en espacios de mayor dimensión sin necesidad de hacer las transformaciones explícitamente. Utilizando el truco del <em>kernel</em> se puede evitar este proceso costoso de transformación de tal manera que se evita calcular el producto escalar reemplazándolo por una operación más simple con las variables originales que proporciona el mismo resultado. A continuación se explican algunos de estos operadores especiales, llamados <em>kernels</em>, que permiten llevar a cabo dicha transformación.</p>
<div id="algunos-kernels-populares" class="section level3" number="25.4.1">
<h3>
<span class="header-section-number">25.4.1</span> Algunos <em>kernels</em> populares<a class="anchor" aria-label="anchor" href="#algunos-kernels-populares"><i class="fas fa-link"></i></a>
</h3>
<p>Los <em>kernels</em> (<span class="citation">(<a href="referncias.html#ref-scholkopf1997prior">Schölkopf et al. 1997</a>)</span>, <span class="citation">(<a href="referncias.html#ref-scholkopf1997comparing">Scholkopf et al. 1997</a>)</span>) más populares en el entrenamiento de SVM están incluidos dentro de la función <code><a href="https://rdrr.io/pkg/e1071/man/svm.html">svm()</a></code> del paquete <code>e1071</code> donde se puede se especifican en el hiperparámetro <code>kernel</code>. Estos kernel son:</p>
<ul>
<li>lineal: <span class="math inline">\(K(u,v) = \langle u,v\rangle\)</span>
</li>
<li>polinomial de grado <span class="math inline">\(\delta\)</span>: <span class="math inline">\(K(u,v) = \gamma(k_1+\langle u,v\rangle)^\delta\)</span>
</li>
<li>base radial: <span class="math inline">\(K(u,v) = e^{\gamma||u-v||^2}\)</span>
</li>
<li>sigmoidal: <span class="math inline">\(K(u,v) = \tanh(\gamma\langle u,v\rangle+k_1)\)</span>
</li>
</ul>
<p>Donde <span class="math inline">\(\langle u,v\rangle = \sum_{i=1}^{n}{u_iv_i}\)</span> es el producto escalar. Cada uno de estos kernels tiene sus propios hiperparámetros, como <span class="math inline">\(\delta\)</span> o <span class="math inline">\(\gamma\)</span>, que es necesario tunear para optimizar el rendimiento de la SVM. En machine learning, el término <strong>tunear</strong>, hace referencia al hecho de ajustar automáticamente tratando de optimizar los hiperparámetros del algoritmo. A la hora de ajustar en <strong>R</strong> un modelo SVM se puede conocer los hiperparámetros a ajustar utilizando la función <code>modelLookup</code> de <code>caret</code>. Por ejemplo, para una SVM con kernel de base lineal se usaría así:</p>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb360-1"><a href="cap-svm.html#cb360-1" tabindex="-1"></a><span class="fu">modelLookup</span>(<span class="st">"svmLinear"</span>)</span>
<span id="cb360-2"><a href="cap-svm.html#cb360-2" tabindex="-1"></a>      model parameter label forReg forClass probModel</span>
<span id="cb360-3"><a href="cap-svm.html#cb360-3" tabindex="-1"></a><span class="dv">1</span> svmLinear         C  Cost   <span class="cn">TRUE</span>     <span class="cn">TRUE</span>      <span class="cn">TRUE</span></span></code></pre></div>
<p>El hiperparámetro que se puede ajustar en un modelo SVM con kernel de base lineal en <strong>R</strong> es el coste (C), el cual representa a la constante <span class="math inline">\(C\)</span> en la ecuación <a href="cap-svm.html#eq:svmloss">(25.1)</a>.</p>
</div>
</div>
<div id="procedimiento-con-r-la-función-svm" class="section level2" number="25.5">
<h2>
<span class="header-section-number">25.5</span> Procedimiento con R: la función <code>svm()</code><a class="anchor" aria-label="anchor" href="#procedimiento-con-r-la-funci%C3%B3n-svm"><i class="fas fa-link"></i></a>
</h2>
<p>En el paquete <code>e1071</code> de R se encuentra la función <code><a href="https://rdrr.io/pkg/e1071/man/svm.html">svm()</a></code> que se utiliza para entrenar un modelo máquinas vector soporte:</p>
<div class="sourceCode" id="cb361"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">svm</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, scale <span class="op">=</span> <span class="cn">TRUE</span>, type <span class="op">=</span> <span class="cn">NULL</span>, kernel <span class="op">=</span> <span class="va">...</span>, <span class="va">...</span><span class="op">)</span></span></code></pre></div>
<ul>
<li>
<code>x</code>: conjunto de datos de entrenamiento que contiene los predictores</li>
<li>
<code>y</code>: vector respuesta con las clases o valores de la variable respuesta.</li>
<li>
<code>scale</code>: booleano que indica si es necesario escalar las variables.</li>
<li>
<code>type</code>: indica si se pretende resolver un problema de clasificación o de regresión.</li>
<li>
<code>kernel</code>: <em>kernel</em> utilizado durante el entrenamiento y la predicción.</li>
</ul>
</div>
<div id="aplicación-de-un-modelo-svm-radial-con-ajuste-automático-en-r" class="section level2" number="25.6">
<h2>
<span class="header-section-number">25.6</span> Aplicación de un modelo SVM Radial con ajuste automático en R<a class="anchor" aria-label="anchor" href="#aplicaci%C3%B3n-de-un-modelo-svm-radial-con-ajuste-autom%C3%A1tico-en-r"><i class="fas fa-link"></i></a>
</h2>
<p>Los datos utilizados para entrenar el modelo SVM en este capítulo se cargan desde la librería <code>CDR</code>. Además, para su entrenamiento se requieren las librerías <code>caret</code> y <code>e1071</code>.</p>
<div class="sourceCode" id="cb362"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"CDR"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/topepo/caret/">"caret"</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"e1071"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="http://had.co.nz/reshape">"reshape"</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://ggplot2.tidyverse.org">"ggplot2"</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">dp_entr_NUM</span><span class="op">)</span></span></code></pre></div>
<p>Se entrena un modelo SVM con kernel radial utilizando el conjunto de entrenamiento con todas las variables numéricas. Previamente, se aplica una normalización z-score, presentadas en el Cap. <a href="chap-feature.html#chap-feature">9</a>, al conjunto de entrenamiento. De este modo, las variables que inicialmente tenían distintas escalas de medida, ahora todas se miden en la misma escala. Además, se ajustan automáticamente los hiperparámetros de dicho algoritmo durante el proceso de entrenamiento.</p>
<div class="sourceCode" id="cb363"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">trControl</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/trainControl.html">trainControl</a></span><span class="op">(</span></span>
<span>  method <span class="op">=</span> <span class="st">"cv"</span>, </span>
<span>  number <span class="op">=</span> <span class="fl">10</span>, </span>
<span>  classProbs <span class="op">=</span> <span class="cn">TRUE</span>,   </span>
<span>  preProcOptions <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="st">"center"</span><span class="op">)</span>,</span>
<span>  summaryFunction <span class="op">=</span> <span class="va">twoClassSummary</span>  </span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Se especifica un rango de valores para los hiperparámetros</span></span>
<span><span class="va">tuneGrid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/expand.grid.html">expand.grid</a></span><span class="op">(</span>sigma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span>from<span class="op">=</span><span class="fl">0.1</span>, to<span class="op">=</span><span class="fl">0.2</span>, by<span class="op">=</span><span class="fl">0.05</span><span class="op">)</span>,</span>
<span>                        C <span class="op">=</span> <span class="fl">10</span><span class="op">**</span><span class="op">(</span><span class="op">-</span><span class="fl">2</span><span class="op">:</span><span class="fl">4</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Se define como procedimiento de muestreo una validación cruzada, como la presentada en el Cap. <a href="chap-feature.html#chap-feature">9</a>, de 10 folds. Además, se le indica al modelo que debe calcular las probabilidades de clase en cada remuestreo en caso de estar entrenando un modelo de clasificación. Con el argumento <code>summaryFunction = twoClassSummary</code> se le indica al modelo que para resumir los resultados se calculen la sensibilidad, especificidad y el área bajo la curva ROC. Como se ha comentado, conviene estandarizar los datos, esto se le indica a la función a través del argumento <code>preProcOptions</code> con la opción <code>center</code>. A su vez, se define una red de hiperparametros a optimizar. A través de la función <code><a href="https://rdrr.io/pkg/caret/man/train.html">train()</a></code> se ajusta automáticamente el modelo con los hiperparametros óptimos.</p>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb364-1"><a href="cap-svm.html#cb364-1" tabindex="-1"></a><span class="co"># Se fija la semilla aleatoria</span></span>
<span id="cb364-2"><a href="cap-svm.html#cb364-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb364-3"><a href="cap-svm.html#cb364-3" tabindex="-1"></a></span>
<span id="cb364-4"><a href="cap-svm.html#cb364-4" tabindex="-1"></a><span class="co"># Se entrena el modelo</span></span>
<span id="cb364-5"><a href="cap-svm.html#cb364-5" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">train</span>(CLS_PRO_pro13 <span class="sc">~</span> ., </span>
<span id="cb364-6"><a href="cap-svm.html#cb364-6" tabindex="-1"></a>             <span class="at">data=</span>dp_entr_NUM, </span>
<span id="cb364-7"><a href="cap-svm.html#cb364-7" tabindex="-1"></a>             <span class="at">method=</span><span class="st">"svmRadial"</span>, </span>
<span id="cb364-8"><a href="cap-svm.html#cb364-8" tabindex="-1"></a>             <span class="at">metric=</span><span class="st">"ROC"</span>,</span>
<span id="cb364-9"><a href="cap-svm.html#cb364-9" tabindex="-1"></a>             <span class="at">trControl=</span>trControl,</span>
<span id="cb364-10"><a href="cap-svm.html#cb364-10" tabindex="-1"></a>             <span class="at">tuneGrid=</span>tuneGrid)</span>
<span id="cb364-11"><a href="cap-svm.html#cb364-11" tabindex="-1"></a></span>
<span id="cb364-12"><a href="cap-svm.html#cb364-12" tabindex="-1"></a>model</span>
<span id="cb364-13"><a href="cap-svm.html#cb364-13" tabindex="-1"></a></span>
<span id="cb364-14"><a href="cap-svm.html#cb364-14" tabindex="-1"></a></span>
<span id="cb364-15"><a href="cap-svm.html#cb364-15" tabindex="-1"></a>Support Vector Machines with Radial Basis Function Kernel </span>
<span id="cb364-16"><a href="cap-svm.html#cb364-16" tabindex="-1"></a></span>
<span id="cb364-17"><a href="cap-svm.html#cb364-17" tabindex="-1"></a><span class="dv">558</span> samples</span>
<span id="cb364-18"><a href="cap-svm.html#cb364-18" tabindex="-1"></a> <span class="dv">19</span> predictor</span>
<span id="cb364-19"><a href="cap-svm.html#cb364-19" tabindex="-1"></a>  <span class="dv">2</span> classes<span class="sc">:</span> <span class="st">'S'</span>, <span class="st">'N'</span> </span>
<span id="cb364-20"><a href="cap-svm.html#cb364-20" tabindex="-1"></a></span>
<span id="cb364-21"><a href="cap-svm.html#cb364-21" tabindex="-1"></a>No pre<span class="sc">-</span>processing</span>
<span id="cb364-22"><a href="cap-svm.html#cb364-22" tabindex="-1"></a>Resampling<span class="sc">:</span> Cross<span class="sc">-</span><span class="fu">Validated</span> (<span class="dv">10</span> fold) </span>
<span id="cb364-23"><a href="cap-svm.html#cb364-23" tabindex="-1"></a>Summary of sample sizes<span class="sc">:</span> <span class="dv">502</span>, <span class="dv">502</span>, <span class="dv">502</span>, <span class="dv">503</span>, <span class="dv">503</span>, <span class="dv">502</span>, ... </span>
<span id="cb364-24"><a href="cap-svm.html#cb364-24" tabindex="-1"></a>Resampling results across tuning parameters<span class="sc">:</span></span>
<span id="cb364-25"><a href="cap-svm.html#cb364-25" tabindex="-1"></a></span>
<span id="cb364-26"><a href="cap-svm.html#cb364-26" tabindex="-1"></a>  sigma  C      ROC        Sens       Spec     </span>
<span id="cb364-27"><a href="cap-svm.html#cb364-27" tabindex="-1"></a>  <span class="fl">0.10</span>   <span class="fl">1e-02</span>  <span class="fl">0.9553241</span>  <span class="fl">0.8785714</span>  <span class="fl">0.7071429</span></span>
<span id="cb364-28"><a href="cap-svm.html#cb364-28" tabindex="-1"></a>  <span class="fl">0.10</span>   <span class="fl">1e-01</span>  <span class="fl">0.9566327</span>  <span class="fl">0.8924603</span>  <span class="fl">0.8247354</span></span>
<span id="cb364-29"><a href="cap-svm.html#cb364-29" tabindex="-1"></a>  <span class="fl">0.10</span>   <span class="fl">1e+00</span>  <span class="fl">0.9434902</span>  <span class="fl">0.8604497</span>  <span class="fl">0.8496032</span></span>
<span id="cb364-30"><a href="cap-svm.html#cb364-30" tabindex="-1"></a>  <span class="fl">0.10</span>   <span class="fl">1e+01</span>  <span class="fl">0.9227230</span>  <span class="fl">0.8460317</span>  <span class="fl">0.8423280</span></span>
<span id="cb364-31"><a href="cap-svm.html#cb364-31" tabindex="-1"></a>  <span class="fl">0.10</span>   <span class="fl">1e+02</span>  <span class="fl">0.8804894</span>  <span class="fl">0.8567460</span>  <span class="fl">0.8279101</span></span>
<span id="cb364-32"><a href="cap-svm.html#cb364-32" tabindex="-1"></a>  <span class="fl">0.10</span>   <span class="fl">1e+03</span>  <span class="fl">0.8645692</span>  <span class="fl">0.8674603</span>  <span class="fl">0.8206349</span></span>
<span id="cb364-33"><a href="cap-svm.html#cb364-33" tabindex="-1"></a>  <span class="fl">0.10</span>   <span class="fl">1e+04</span>  <span class="fl">0.8548469</span>  <span class="fl">0.8423280</span>  <span class="fl">0.8242063</span></span>
<span id="cb364-34"><a href="cap-svm.html#cb364-34" tabindex="-1"></a>  <span class="fl">0.15</span>   <span class="fl">1e-02</span>  <span class="fl">0.9527636</span>  <span class="fl">0.8535714</span>  <span class="fl">0.6642857</span></span>
<span id="cb364-35"><a href="cap-svm.html#cb364-35" tabindex="-1"></a>  <span class="fl">0.15</span>   <span class="fl">1e-01</span>  <span class="fl">0.9513653</span>  <span class="fl">0.9105820</span>  <span class="fl">0.8105820</span></span>
<span id="cb364-36"><a href="cap-svm.html#cb364-36" tabindex="-1"></a>  <span class="fl">0.15</span>   <span class="fl">1e+00</span>  <span class="fl">0.9310091</span>  <span class="fl">0.8783069</span>  <span class="fl">0.8494709</span></span>
<span id="cb364-37"><a href="cap-svm.html#cb364-37" tabindex="-1"></a>  <span class="fl">0.15</span>   <span class="fl">1e+01</span>  <span class="fl">0.8941421</span>  <span class="fl">0.8531746</span>  <span class="fl">0.8387566</span></span>
<span id="cb364-38"><a href="cap-svm.html#cb364-38" tabindex="-1"></a>  <span class="fl">0.15</span>   <span class="fl">1e+02</span>  <span class="fl">0.8602088</span>  <span class="fl">0.8781746</span>  <span class="fl">0.8242063</span></span>
<span id="cb364-39"><a href="cap-svm.html#cb364-39" tabindex="-1"></a>  <span class="fl">0.15</span>   <span class="fl">1e+03</span>  <span class="fl">0.8369331</span>  <span class="fl">0.8458995</span>  <span class="fl">0.8134921</span></span>
<span id="cb364-40"><a href="cap-svm.html#cb364-40" tabindex="-1"></a>  <span class="fl">0.15</span>   <span class="fl">1e+04</span>  <span class="fl">0.8369284</span>  <span class="fl">0.8637566</span>  <span class="fl">0.8064815</span></span>
<span id="cb364-41"><a href="cap-svm.html#cb364-41" tabindex="-1"></a>  <span class="fl">0.20</span>   <span class="fl">1e-02</span>  <span class="fl">0.9443925</span>  <span class="fl">0.8535714</span>  <span class="fl">0.6321429</span></span>
<span id="cb364-42"><a href="cap-svm.html#cb364-42" tabindex="-1"></a>  <span class="fl">0.20</span>   <span class="fl">1e-01</span>  <span class="fl">0.9440098</span>  <span class="fl">0.9250000</span>  <span class="fl">0.7384921</span></span>
<span id="cb364-43"><a href="cap-svm.html#cb364-43" tabindex="-1"></a>  <span class="fl">0.20</span>   <span class="fl">1e+00</span>  <span class="fl">0.9199310</span>  <span class="fl">0.8818783</span>  <span class="fl">0.8387566</span></span>
<span id="cb364-44"><a href="cap-svm.html#cb364-44" tabindex="-1"></a>  <span class="fl">0.20</span>   <span class="fl">1e+01</span>  <span class="fl">0.8752031</span>  <span class="fl">0.8674603</span>  <span class="fl">0.8207672</span></span>
<span id="cb364-45"><a href="cap-svm.html#cb364-45" tabindex="-1"></a>  <span class="fl">0.20</span>   <span class="fl">1e+02</span>  <span class="fl">0.8477324</span>  <span class="fl">0.8674603</span>  <span class="fl">0.8063492</span></span>
<span id="cb364-46"><a href="cap-svm.html#cb364-46" tabindex="-1"></a>  <span class="fl">0.20</span>   <span class="fl">1e+03</span>  <span class="fl">0.8308296</span>  <span class="fl">0.8638889</span>  <span class="fl">0.8134921</span></span>
<span id="cb364-47"><a href="cap-svm.html#cb364-47" tabindex="-1"></a>  <span class="fl">0.20</span>   <span class="fl">1e+04</span>  <span class="fl">0.8308296</span>  <span class="fl">0.8638889</span>  <span class="fl">0.8099206</span></span>
<span id="cb364-48"><a href="cap-svm.html#cb364-48" tabindex="-1"></a></span>
<span id="cb364-49"><a href="cap-svm.html#cb364-49" tabindex="-1"></a>ROC was used to select the optimal model using the largest value.</span>
<span id="cb364-50"><a href="cap-svm.html#cb364-50" tabindex="-1"></a>The final values used <span class="cf">for</span> the model were sigma <span class="ot">=</span> <span class="fl">0.1</span> and C <span class="ot">=</span> <span class="dv">0</span>.<span class="fl">1.</span></span></code></pre></div>
<p>Los argumentos que requiere la función son la <code>formula</code>, es decir, indicar la variable respuesta y qué predictores intervienen en el modelo. Los datos que se van a utilizar, así como el algoritmo a entrenar, en este caso la SVM con kernel de base radial. Además, se indica una métrica para el rendimiento del modelo, en caso de no indicarlo <strong>R</strong> asigna la más acorde de acuerdo a la variable respuesta. Finalmente, se incluyen las opciones de entrenamiento y la red de hiperparámetros a probar para determinar la combinación óptima. Los hiperparámetros del modelo entrenado son <span class="math inline">\(\sigma=0.1\)</span> y <span class="math inline">\(C=0.1\)</span>. Este resultado queda definido en la salida del modelo, pero también es representable como en la Fig. <a href="cap-svm.html#fig:006-002-202SVMTUNEPLOT">25.4</a>. En este gráfico el eje y mide el rendimiento del modelo para ciertos valores de los hiperparámetros. Cada línea representa un valor para el hiperparámetro sigma, y se mide su rendimiento variando distintos niveles del parámetro coste (C), que queda representado en el eje x. Así, se observa que la línea roja (sigma=0,1) alcanza el mayor nivel de precisión en el valor C=0,1.</p>
<div class="sourceCode" id="cb365"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:006-002-202SVMTUNEPLOT"></span>
<img src="img/svm-tune-process.png" alt="Optimización de los parámetros C y sigma de una SVM." width="60%"><p class="caption">
Figura 25.4: Optimización de los parámetros C y sigma de una SVM.
</p>
</div>
<p>Los boxplot de la Fig. <a href="cap-svm.html#fig:006-002-202SVMRESULTS3">25.5</a> muestran un resumen del rendimiento del modelo en las distintas repeticiones del proceso de validación cruzada. De esta manera, se observa como la sensibilidad es superior al 75% y la especificidad supera valores del 70%, esto indica que el modelo entrenado es capaz de predecir correctamente tanto a los clientes que van a comprar el nuevo producto como los que no lo van a hacer.</p>
<div class="sourceCode" id="cb366"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/reshape/man/melt-24.html">melt</a></span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">resample</span><span class="op">[</span>,<span class="op">-</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">variable</span>, y <span class="op">=</span> <span class="va">value</span>, fill<span class="op">=</span><span class="va">variable</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_boxplot.html">geom_boxplot</a></span><span class="op">(</span>show.legend<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:006-002-202SVMRESULTS3"></span>
<img src="img/svm-boxplot.png" alt="Resultados del modelo obtenidos durante la validación cruzada." width="60%"><p class="caption">
Figura 25.5: Resultados del modelo obtenidos durante la validación cruzada.
</p>
</div>
<div id="importancia-de-las-variables" class="section level3" number="25.6.1">
<h3>
<span class="header-section-number">25.6.1</span> Importancia de las variables<a class="anchor" aria-label="anchor" href="#importancia-de-las-variables"><i class="fas fa-link"></i></a>
</h3>
<p>En machine learning, muchos de los algoritmos de caja negra (definidos en el Cap. <a href="cap-etica.html#cap-etica">4</a>), no proporcionan información sobre la importancia que tiene cada variable en el modelo. Este es el caso de las máquinas de vector soporte. Tanto para la SVM como para otros algoritmos, es posible cuantificar la importancia de cada variable utilizando paquetes de <strong>R</strong> como <code>DALEX</code>, <code>iml</code> o <code>vip</code>.</p>
<p>Este último paquete incluye una función con el mismo nombre <code><a href="https://rdrr.io/pkg/vip/man/vip.html">vip()</a></code>. Para medir la importancia, se indica qué métrica se utilizó en el proceso de entrenamiento del modelo, en el caso de la SVM se indicará que fue el área bajo la curva (<code>metric="auc"</code>). En el argumento <code>pred_wrapper</code> se indica una función de medida que contenga tanto los valores observados como los valores predichos. Dado que la SVM entrenada utiliza AUC para medir el rendimiento del modelo ajustado, la función de medida indicada en <code>pred_wrapper</code> deberá devolver la probabilidad de que el modelo asigne una observación a la clase de referencia. En este ejemplo, la clase de referencia es “SI”, puesto que interesa saber si un cliente comprará el nuevo producto. Entonces, la función de predicción se define como:</p>
<div class="sourceCode" id="cb367"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">prob_si</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">object</span>, <span class="va">newdata</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">object</span>, newdata <span class="op">=</span> <span class="va">newdata</span>, type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span><span class="op">[</span>, <span class="st">"S"</span><span class="op">]</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Ejecutando la función <code><a href="https://rdrr.io/pkg/vip/man/vip.html">vip()</a></code> con los argumentos mencionados se genera la Fig. <a href="cap-svm.html#fig:vip-svm">25.6</a>. Este gráfico indica la importancia de cada variable en el modelo de más a menos importante. En este caso, la variable más importante es el importe gastado en el <em>smartchwatch fitness</em>, seguida muy de cerca por la variable que indica si el cliente compra o no el <em>smartchwatch fitness</em>. En el otro extremo, se observa que las variables que indican si el cliente tiene un nivel de educación básico o no, no son muy relevantes en la SVM entrenada.</p>
<div class="sourceCode" id="cb368"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/koalaverse/vip/">"vip"</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">101</span><span class="op">)</span>  </span>
<span><span class="fu"><a href="https://rdrr.io/pkg/vip/man/vip.html">vip</a></span><span class="op">(</span><span class="va">model</span>, train <span class="op">=</span> <span class="va">dp_entr_NUM</span>, target <span class="op">=</span> <span class="st">"CLS_PRO_pro13"</span>, metric <span class="op">=</span> <span class="st">"auc"</span>,</span>
<span>    reference_class <span class="op">=</span> <span class="st">"S"</span>, pred_wrapper <span class="op">=</span> <span class="va">prob_si</span>, method<span class="op">=</span><span class="st">"permute"</span>,</span>
<span>    aesthetics <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>color <span class="op">=</span> <span class="st">"steelblue2"</span>, fill <span class="op">=</span> <span class="st">"steelblue2"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:vip-svm"></span>
<img src="img/vip-svm.png" alt="Importancia de las variables incluidas en la SVM." width="60%"><p class="caption">
Figura 25.6: Importancia de las variables incluidas en la SVM.
</p>
</div>
<p>A partir de la Fig. <a href="cap-svm.html#fig:vip-svm">25.6</a> se puede concluir que para predecir si un cliente comprará o no el <em>tensiómetro digital</em> las variables que más importancia tienen: son el importe que gastó en el <em>smartchwatch fitness</em>, si compró o no el <em>smartchwatch fitness</em>, el importe que gastó en la <em>depiladora eléctrica</em> y si compró o no la <em>depiladora eléctrica</em>.</p>
<p>De forma similar se podrían probar el resto de <em>kernels</em> disponibles para el algoritmo SVM.</p>
</div>
<div id="resumen-22" class="section level3 unnumbered infobox_resume">
<h3>Resumen<a class="anchor" aria-label="anchor" href="#resumen-22"><i class="fas fa-link"></i></a>
</h3>
<p>En este capítulo se introduce al lector en el algoritmo de máquinas vector soporte, en particular:</p>
<ul>
<li>Se presenta el concepto de margen de decisión, y las ventajas de la SVM respecto a otras técnicas de clasificación.</li>
<li>Se explica el truco del kernel cuando los datos no son separables por un hiperplano en su espacio original</li>
<li>Se da un repaso a los kernels más utilizados.</li>
<li>Se presenta la aplicación de una SVM con kernel radial en <strong>R</strong> para la clasificación de datos con respuesta binaria, en el que se ajustan automáticamente los hiperparámetros.</li>
<li>Se obtiene la importancia de las variables del modelo final.</li>
</ul>
</div>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="cap-arboles.html"><span class="header-section-number">24</span> Árboles de clasificación y regresión</a></div>
<div class="next"><a href="cap-knn.html"><span class="header-section-number">26</span> Clasificador k-vecinos más próximos</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="Índice capítulo"><h2>Índice capítulo</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#cap-svm"><span class="header-section-number">25</span> Máquinas de vector soporte</a></li>
<li><a class="nav-link" href="#introducci%C3%B3n-11"><span class="header-section-number">25.1</span> Introducción</a></li>
<li><a class="nav-link" href="#algoritmo-svm-para-clasificaci%C3%B3n-binaria"><span class="header-section-number">25.2</span> Algoritmo SVM para clasificación binaria</a></li>
<li><a class="nav-link" href="#y-si-tengo-m%C3%A1s-de-dos-clases"><span class="header-section-number">25.3</span> ¿Y si tengo más de dos clases?</a></li>
<li>
<a class="nav-link" href="#truco-del-kernel-tratando-con-la-no-linealidad"><span class="header-section-number">25.4</span> Truco del kernel: tratando con la no linealidad</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#algunos-kernels-populares"><span class="header-section-number">25.4.1</span> Algunos kernels populares</a></li></ul>
</li>
<li><a class="nav-link" href="#procedimiento-con-r-la-funci%C3%B3n-svm"><span class="header-section-number">25.5</span> Procedimiento con R: la función svm()</a></li>
<li>
<a class="nav-link" href="#aplicaci%C3%B3n-de-un-modelo-svm-radial-con-ajuste-autom%C3%A1tico-en-r"><span class="header-section-number">25.6</span> Aplicación de un modelo SVM Radial con ajuste automático en R</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#importancia-de-las-variables"><span class="header-section-number">25.6.1</span> Importancia de las variables</a></li>
<li><a class="nav-link" href="#resumen-22">Resumen</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Fundamentos de ciencia de datos con R</strong>" coordinado por <a href="https://blog.uclm.es/gemafaviles/" class="text-light">Gema Fernández-Avilés y José-María Montero</a>. Generado por última vez el día 2023-06-16.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>Este libro ha sido generado con el paquete de R <a class="text-light" href="https://bookdown.org">bookdown</a>.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
