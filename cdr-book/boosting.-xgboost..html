<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 30 Boosting. XGBoost. | Fundamentos de ciencia de datos con R</title>
  <meta name="description" content="Falta hacer" />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 30 Boosting. XGBoost. | Fundamentos de ciencia de datos con R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Falta hacer" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 30 Boosting. XGBoost. | Fundamentos de ciencia de datos con R" />
  
  <meta name="twitter:description" content="Falta hacer" />
  

<meta name="author" content="Gema Fernández-Avilés y José-María Montero" />


<meta name="date" content="2022-12-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bagging.-random-forest.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-33KQ1S5ZCJ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-33KQ1S5ZCJ');
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"> Ciencia de datos con <strong>R</strong></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Fundamentos y ‘tool-kit’ de la ciencia de datos</b></span></li>
<li class="chapter" data-level="1" data-path="ciencia-datos.html"><a href="ciencia-datos.html"><i class="fa fa-check"></i><b>1</b> ¿Es la Ciencia de datos una Ciencia?</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ciencia-datos.html"><a href="ciencia-datos.html#ciencia"><i class="fa fa-check"></i><b>1.1</b> ¿Qué se entiende por Ciencia?</a></li>
<li class="chapter" data-level="1.2" data-path="ciencia-datos.html"><a href="ciencia-datos.html#qué-es-la-ciencia-de-datos"><i class="fa fa-check"></i><b>1.2</b> ¿Qué es la Ciencia de Datos?</a></li>
<li class="chapter" data-level="1.3" data-path="ciencia-datos.html"><a href="ciencia-datos.html#lo-científico-de-la-ciencia-de-datos"><i class="fa fa-check"></i><b>1.3</b> Lo científico de la Ciencia de datos</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="metodología.html"><a href="metodología.html"><i class="fa fa-check"></i><b>2</b> Metodología para la Ciencia de datos</a>
<ul>
<li class="chapter" data-level="2.1" data-path="metodología.html"><a href="metodología.html#preliminares"><i class="fa fa-check"></i><b>2.1</b> Preliminares</a></li>
<li class="chapter" data-level="2.2" data-path="metodología.html"><a href="metodología.html#principales-metodologías-en-ciencia-de-datos"><i class="fa fa-check"></i><b>2.2</b> Principales metodologías en Ciencia de datos</a></li>
<li class="chapter" data-level="2.3" data-path="metodología.html"><a href="metodología.html#met-crisp-dm"><i class="fa fa-check"></i><b>2.3</b> CRISP-DM para Ciencia de datos</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-110003.html"><a href="ch-110003.html"><i class="fa fa-check"></i><b>3</b> R para ciencia de datos</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-110003.html"><a href="ch-110003.html#introducción"><i class="fa fa-check"></i><b>3.1</b> Introducción</a></li>
<li class="chapter" data-level="3.2" data-path="ch-110003.html"><a href="ch-110003.html#id_110003-bases"><i class="fa fa-check"></i><b>3.2</b> La sesión de R</a></li>
<li class="chapter" data-level="3.3" data-path="ch-110003.html"><a href="ch-110003.html#instalación-de-r"><i class="fa fa-check"></i><b>3.3</b> Instalación de R</a></li>
<li class="chapter" data-level="3.4" data-path="ch-110003.html"><a href="ch-110003.html#id_110003-proyectos"><i class="fa fa-check"></i><b>3.4</b> Trabajar con proyectos de RStudio</a></li>
<li class="chapter" data-level="3.5" data-path="ch-110003.html"><a href="ch-110003.html#manipulación-de-datos-con-r"><i class="fa fa-check"></i><b>3.5</b> Manipulación de datos con R</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="ch-110003.html"><a href="ch-110003.html#id_110003-estructuras"><i class="fa fa-check"></i><b>3.5.1</b> Estructuras y tipos de datos</a></li>
<li class="chapter" data-level="3.5.2" data-path="ch-110003.html"><a href="ch-110003.html#id_110003-importacion"><i class="fa fa-check"></i><b>3.5.2</b> Importación de datos</a></li>
<li class="chapter" data-level="3.5.3" data-path="ch-110003.html"><a href="ch-110003.html#exportación-y-archivos-de-datos-de-r"><i class="fa fa-check"></i><b>3.5.3</b> Exportación y archivos de datos de R</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="ch-110003.html"><a href="ch-110003.html#id_110003-tidyverse"><i class="fa fa-check"></i><b>3.6</b> Organización de datos con el <em>tidyverse</em></a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="ch-110003.html"><a href="ch-110003.html#el-tidyverse-y-su-flujo-de-trabajo"><i class="fa fa-check"></i><b>3.6.1</b> El <em>tidyverse</em> y su flujo de trabajo</a></li>
<li class="chapter" data-level="3.6.2" data-path="ch-110003.html"><a href="ch-110003.html#transformación-de-datos-con-dplyr"><i class="fa fa-check"></i><b>3.6.2</b> Transformación de datos con <code>dplyr</code></a></li>
<li class="chapter" data-level="3.6.3" data-path="ch-110003.html"><a href="ch-110003.html#combinación-de-datos"><i class="fa fa-check"></i><b>3.6.3</b> Combinación de datos</a></li>
<li class="chapter" data-level="3.6.4" data-path="ch-110003.html"><a href="ch-110003.html#reorganización-de-datos"><i class="fa fa-check"></i><b>3.6.4</b> Reorganización de datos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><i class="fa fa-check"></i><b>4</b> Gestión y operación de datos con bases de datos relacionales</a>
<ul>
<li class="chapter" data-level="4.1" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#introducción-1"><i class="fa fa-check"></i><b>4.1</b> Introducción</a></li>
<li class="chapter" data-level="4.2" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#concepto-de-base-de-datos"><i class="fa fa-check"></i><b>4.2</b> Concepto de Base de datos</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#gestión-de-los-datos-en-una-base-o-repositorio-de-datos"><i class="fa fa-check"></i><b>4.2.1</b> Gestión de los datos en una base o repositorio de datos</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#el-lenguaje-estructurado-de-consulta-sql"><i class="fa fa-check"></i><b>4.3</b> El Lenguaje Estructurado de Consulta (SQL)</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#sql-como-lenguaje-de-definición-de-datos-ldd"><i class="fa fa-check"></i><b>4.3.1</b> SQL como Lenguaje de Definición de Datos (LDD)</a></li>
<li class="chapter" data-level="4.3.2" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#sql-como-lenguaje-de-manipulación-de-datos-lmd"><i class="fa fa-check"></i><b>4.3.2</b> SQL como Lenguaje de Manipulación de Datos (LMD)</a></li>
<li class="chapter" data-level="4.3.3" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#sql-como-lenguaje-de-administración-de-datos-lad"><i class="fa fa-check"></i><b>4.3.3</b> SQL como Lenguaje de Administración de Datos (LAD)</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#usando-bases-de-datos-desde-r"><i class="fa fa-check"></i><b>4.4</b> Usando bases de datos desde R</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#conexión-a-una-base-de-datos"><i class="fa fa-check"></i><b>4.4.1</b> Conexión a una base de datos</a></li>
<li class="chapter" data-level="4.4.2" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#operaciones-de-lectura-selección-read-de-datos"><i class="fa fa-check"></i><b>4.4.2</b> Operaciones de lectura / selección (<em>read</em>) de datos</a></li>
<li class="chapter" data-level="4.4.3" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#operaciones-de-inserción-create-y-actualización-update-de-datos"><i class="fa fa-check"></i><b>4.4.3</b> Operaciones de inserción (<em>create</em>) y actualización (<em>update</em>) de datos</a></li>
<li class="chapter" data-level="4.4.4" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#operaciones-de-borrado-de-datos-delete"><i class="fa fa-check"></i><b>4.4.4</b> Operaciones de Borrado de datos (<em>delete</em>)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="datos-no-sql.html"><a href="datos-no-sql.html"><i class="fa fa-check"></i><b>5</b> Gestión y operación de datos masivos (BigData) con bases de datos NoSQL</a>
<ul>
<li class="chapter" data-level="5.1" data-path="datos-no-sql.html"><a href="datos-no-sql.html#introducción-al-big-data"><i class="fa fa-check"></i><b>5.1</b> Introducción al Big Data</a></li>
<li class="chapter" data-level="5.2" data-path="datos-no-sql.html"><a href="datos-no-sql.html#VsBigData"><i class="fa fa-check"></i><b>5.2</b> Las V’s del Big Data</a></li>
<li class="chapter" data-level="5.3" data-path="datos-no-sql.html"><a href="datos-no-sql.html#fuentes-de-datos-en-entornos-big-data"><i class="fa fa-check"></i><b>5.3</b> Fuentes de Datos en entornos Big Data</a></li>
<li class="chapter" data-level="5.4" data-path="datos-no-sql.html"><a href="datos-no-sql.html#bases-de-datos-relacionales-vs.-nosql"><i class="fa fa-check"></i><b>5.4</b> Bases de datos Relacionales vs. NoSQL</a></li>
<li class="chapter" data-level="5.5" data-path="datos-no-sql.html"><a href="datos-no-sql.html#bases-de-datos-nosql"><i class="fa fa-check"></i><b>5.5</b> Bases de datos NoSQL</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="datos-no-sql.html"><a href="datos-no-sql.html#definición-de-bases-de-datos-nosql"><i class="fa fa-check"></i><b>5.5.1</b> Definición de bases de datos NoSQL</a></li>
<li class="chapter" data-level="5.5.2" data-path="datos-no-sql.html"><a href="datos-no-sql.html#necesidades-no-cubiertas-por-las-bases-de-datos-relacionales"><i class="fa fa-check"></i><b>5.5.2</b> Necesidades no cubiertas por las bases de datos relacionales</a></li>
<li class="chapter" data-level="5.5.3" data-path="datos-no-sql.html"><a href="datos-no-sql.html#tipos-de-almacenamiento-en-bases-de-datos-nosql"><i class="fa fa-check"></i><b>5.5.3</b> Tipos de almacenamiento en bases de datos NoSQL</a></li>
<li class="chapter" data-level="5.5.4" data-path="datos-no-sql.html"><a href="datos-no-sql.html#limitaciones-de-las-bases-de-datos-nosql"><i class="fa fa-check"></i><b>5.5.4</b> Limitaciones de las bases de datos NoSQL</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="datos-no-sql.html"><a href="datos-no-sql.html#ejemplo-de-integración-de-una-base-de-datos-nosql-y-análisis-de-datos-en-r"><i class="fa fa-check"></i><b>5.6</b> Ejemplo de integración de una base de datos NoSQL y análisis de datos en R</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="datos-no-sql.html"><a href="datos-no-sql.html#introMongo"><i class="fa fa-check"></i><b>5.6.1</b> Introducción a MongoDB</a></li>
<li class="chapter" data-level="5.6.2" data-path="datos-no-sql.html"><a href="datos-no-sql.html#paquetesCaso"><i class="fa fa-check"></i><b>5.6.2</b> Plataforma tecnológica para el caso práctico</a></li>
<li class="chapter" data-level="5.6.3" data-path="datos-no-sql.html"><a href="datos-no-sql.html#conexionMongo"><i class="fa fa-check"></i><b>5.6.3</b> Conexión y acceso a MongoDB desde R</a></li>
<li class="chapter" data-level="5.6.4" data-path="datos-no-sql.html"><a href="datos-no-sql.html#consultaViajes"><i class="fa fa-check"></i><b>5.6.4</b> Obtención de datos en R desde MongoDB</a></li>
<li class="chapter" data-level="5.6.5" data-path="datos-no-sql.html"><a href="datos-no-sql.html#analisisViajes"><i class="fa fa-check"></i><b>5.6.5</b> Analizando datos de MongoDB en R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="id_120007-informes.html"><a href="id_120007-informes.html"><i class="fa fa-check"></i><b>6</b> Informes reproducibles con R-markdown -&gt; Quarto</a>
<ul>
<li class="chapter" data-level="6.1" data-path="id_120007-informes.html"><a href="id_120007-informes.html#introducción-2"><i class="fa fa-check"></i><b>6.1</b> Introducción</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="id_120007-informes.html"><a href="id_120007-informes.html#por-qué-informes-reproducibles"><i class="fa fa-check"></i><b>6.1.1</b> ¿Por qué informes reproducibles?</a></li>
<li class="chapter" data-level="6.1.2" data-path="id_120007-informes.html"><a href="id_120007-informes.html#markdown-r-markdown-y-rstudio"><i class="fa fa-check"></i><b>6.1.2</b> Markdown, R Markdown y RStudio</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="id_120007-informes.html"><a href="id_120007-informes.html#documentos-r-markdown"><i class="fa fa-check"></i><b>6.2</b> Documentos R Markdown</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="id_120007-informes.html"><a href="id_120007-informes.html#encabezado-yaml-y-configuración"><i class="fa fa-check"></i><b>6.2.1</b> Encabezado YAML y configuración</a></li>
<li class="chapter" data-level="6.2.2" data-path="id_120007-informes.html"><a href="id_120007-informes.html#formateado-de-texto"><i class="fa fa-check"></i><b>6.2.2</b> Formateado de texto</a></li>
<li class="chapter" data-level="6.2.3" data-path="id_120007-informes.html"><a href="id_120007-informes.html#inclusión-de-código"><i class="fa fa-check"></i><b>6.2.3</b> Inclusión de código</a></li>
<li class="chapter" data-level="6.2.4" data-path="id_120007-informes.html"><a href="id_120007-informes.html#opciones-de-los-bloques-de-código-chunks"><i class="fa fa-check"></i><b>6.2.4</b> Opciones de los bloques de código (<em>chunks</em>)</a></li>
<li class="chapter" data-level="6.2.5" data-path="id_120007-informes.html"><a href="id_120007-informes.html#editor-visual"><i class="fa fa-check"></i><b>6.2.5</b> Editor visual</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="id_120007-informes.html"><a href="id_120007-informes.html#otros-formatos"><i class="fa fa-check"></i><b>6.3</b> Otros formatos</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="github.html"><a href="github.html"><i class="fa fa-check"></i><b>7</b> Git y GitHub en R</a>
<ul>
<li class="chapter" data-level="7.1" data-path="github.html"><a href="github.html#qué-es-git"><i class="fa fa-check"></i><b>7.1</b> ¿Qué es Git?</a></li>
<li class="chapter" data-level="7.2" data-path="github.html"><a href="github.html#qué-es-github"><i class="fa fa-check"></i><b>7.2</b> ¿Qué es GitHub?</a></li>
<li class="chapter" data-level="7.3" data-path="github.html"><a href="github.html#por-qué-usar-git-y-github"><i class="fa fa-check"></i><b>7.3</b> ¿Por qué usar Git y GitHub?</a></li>
<li class="chapter" data-level="7.4" data-path="github.html"><a href="github.html#configuración"><i class="fa fa-check"></i><b>7.4</b> Configuración</a></li>
<li class="chapter" data-level="7.5" data-path="github.html"><a href="github.html#configurar-git"><i class="fa fa-check"></i><b>7.5</b> Configurar git</a></li>
<li class="chapter" data-level="7.6" data-path="github.html"><a href="github.html#workflow"><i class="fa fa-check"></i><b>7.6</b> Workflow</a></li>
</ul></li>
<li class="part"><span><b>II Manipulación de datos con R. Técnicas y herramientas</b></span></li>
<li class="chapter" data-level="8" data-path="id_120006-aed.html"><a href="id_120006-aed.html"><i class="fa fa-check"></i><b>8</b> Análisis exploratorio de datos</a>
<ul>
<li class="chapter" data-level="8.1" data-path="id_120006-aed.html"><a href="id_120006-aed.html#introducción-3"><i class="fa fa-check"></i><b>8.1</b> Introducción</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="id_120006-aed.html"><a href="id_120006-aed.html#el-cuarterto-de-anscombe"><i class="fa fa-check"></i><b>8.1.1</b> El cuarterto de Anscombe</a></li>
<li class="chapter" data-level="8.1.2" data-path="id_120006-aed.html"><a href="id_120006-aed.html#conceptos-generales"><i class="fa fa-check"></i><b>8.1.2</b> Conceptos generales</a></li>
<li class="chapter" data-level="8.1.3" data-path="id_120006-aed.html"><a href="id_120006-aed.html#componentes-de-un-gráfico-y-su-representación-en-r"><i class="fa fa-check"></i><b>8.1.3</b> Componentes de un gráfico y su representación en R</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="id_120006-aed.html"><a href="id_120006-aed.html#id_120006-aeduni"><i class="fa fa-check"></i><b>8.2</b> Análisis exploratorio de una característica</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="id_120006-aed.html"><a href="id_120006-aed.html#variables-cualitativas"><i class="fa fa-check"></i><b>8.2.1</b> Variables cualitativas</a></li>
<li class="chapter" data-level="8.2.2" data-path="id_120006-aed.html"><a href="id_120006-aed.html#variables-cuantitativas"><i class="fa fa-check"></i><b>8.2.2</b> Variables cuantitativas</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="id_120006-aed.html"><a href="id_120006-aed.html#id_120006-aedmulti"><i class="fa fa-check"></i><b>8.3</b> Análisis exploratorio de varias características</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="id_120006-aed.html"><a href="id_120006-aed.html#variables-cualitativas-1"><i class="fa fa-check"></i><b>8.3.1</b> Variables cualitativas</a></li>
<li class="chapter" data-level="8.3.2" data-path="id_120006-aed.html"><a href="id_120006-aed.html#variables-cuantitativas-1"><i class="fa fa-check"></i><b>8.3.2</b> Variables cuantitativas</a></li>
<li class="chapter" data-level="8.3.3" data-path="id_120006-aed.html"><a href="id_120006-aed.html#variables-cualitativas-y-cuantitativas"><i class="fa fa-check"></i><b>8.3.3</b> Variables cualitativas y cuantitativas</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Preparación de datos: evaluación de la calidad de los datos. Integración, limpieza y transformación</b></span></li>
<li class="chapter" data-level="9" data-path="DGDQM.html"><a href="DGDQM.html"><i class="fa fa-check"></i><b>9</b> Gobierno y gestión de calidad de Datos</a>
<ul>
<li class="chapter" data-level="9.1" data-path="DGDQM.html"><a href="DGDQM.html#introducción-4"><i class="fa fa-check"></i><b>9.1</b> Introducción</a></li>
<li class="chapter" data-level="9.2" data-path="DGDQM.html"><a href="DGDQM.html#concepto-de-gobierno-de-datos"><i class="fa fa-check"></i><b>9.2</b> Concepto de Gobierno de datos</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="DGDQM.html"><a href="DGDQM.html#beneficiosDG"><i class="fa fa-check"></i><b>9.2.1</b> Beneficios del Gobierno de Datos</a></li>
<li class="chapter" data-level="9.2.2" data-path="DGDQM.html"><a href="DGDQM.html#artefactosDG"><i class="fa fa-check"></i><b>9.2.2</b> Artefactos de un sistema de Gobierno de Datos</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="DGDQM.html"><a href="DGDQM.html#marcos-y-metodologías-existentes-de-gobierno-de-datos"><i class="fa fa-check"></i><b>9.3</b> Marcos y metodologías existentes de Gobierno de Datos</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="DGDQM.html"><a href="DGDQM.html#modelo-alarcos-de-mejora-de-datos-mamd"><i class="fa fa-check"></i><b>9.3.1</b> Modelo Alarcos de Mejora de Datos (MAMD)</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="DGDQM.html"><a href="DGDQM.html#gestión-de-calidad-de-datos"><i class="fa fa-check"></i><b>9.4</b> Gestión de calidad de datos</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="DGDQM.html"><a href="DGDQM.html#medición-de-calidad-de-datos-vs-perfilado-de-datos"><i class="fa fa-check"></i><b>9.4.1</b> Medición de calidad de datos vs perfilado de datos</a></li>
<li class="chapter" data-level="9.4.2" data-path="DGDQM.html"><a href="DGDQM.html#mejora-de-datos"><i class="fa fa-check"></i><b>9.4.2</b> Mejora de datos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="id_130009.html"><a href="id_130009.html"><i class="fa fa-check"></i><b>10</b> Integración y limpieza de datos</a>
<ul>
<li class="chapter" data-level="10.1" data-path="id_130009.html"><a href="id_130009.html#introducción-5"><i class="fa fa-check"></i><b>10.1</b> Introducción</a></li>
<li class="chapter" data-level="10.2" data-path="id_130009.html"><a href="id_130009.html#problemas-de-calidad-de-datos"><i class="fa fa-check"></i><b>10.2</b> Problemas de calidad de datos</a></li>
<li class="chapter" data-level="10.3" data-path="id_130009.html"><a href="id_130009.html#niveles-inadecuados-de-completitud-valores-missing"><i class="fa fa-check"></i><b>10.3</b> Niveles inadecuados de completitud: Valores <em>missing</em></a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="id_130009.html"><a href="id_130009.html#visualización"><i class="fa fa-check"></i><b>10.3.1</b> Visualización</a></li>
<li class="chapter" data-level="10.3.2" data-path="id_130009.html"><a href="id_130009.html#imputacion"><i class="fa fa-check"></i><b>10.3.2</b> Técnicas de Imputación</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="id_130009.html"><a href="id_130009.html#mejorando-la-exactitud-y-la-precisión-eliminación-del-ruido-estadístico"><i class="fa fa-check"></i><b>10.4</b> Mejorando la exactitud y la precisión: eliminación del ruido estadístico</a></li>
<li class="chapter" data-level="10.5" data-path="id_130009.html"><a href="id_130009.html#integración-de-datos"><i class="fa fa-check"></i><b>10.5</b> Integración de datos</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="id_130010.html"><a href="id_130010.html"><i class="fa fa-check"></i><b>11</b> Feature Selection and Engineering</a>
<ul>
<li class="chapter" data-level="11.1" data-path="id_130010.html"><a href="id_130010.html#introducción-6"><i class="fa fa-check"></i><b>11.1</b> Introducción</a></li>
<li class="chapter" data-level="11.2" data-path="id_130010.html"><a href="id_130010.html#feature-selection-selección-de-variables"><i class="fa fa-check"></i><b>11.2</b> <em>Feature Selection</em> (Selección de variables)</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="id_130010.html"><a href="id_130010.html#métodos-de-selección-tipo-filtro"><i class="fa fa-check"></i><b>11.2.1</b> Métodos de selección tipo Filtro</a></li>
<li class="chapter" data-level="11.2.2" data-path="id_130010.html"><a href="id_130010.html#métodos-de-selección-de-variables-tipo-wrapper"><i class="fa fa-check"></i><b>11.2.2</b> Métodos de selección de variables tipo <em>wrapper</em></a></li>
<li class="chapter" data-level="11.2.3" data-path="id_130010.html"><a href="id_130010.html#métodos-de-selección-tipo-embedded"><i class="fa fa-check"></i><b>11.2.3</b> Métodos de selección tipo Embedded</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="id_130010.html"><a href="id_130010.html#transformaciones-de-escala-y-de-la-distribución-de-la-variable-objetivo"><i class="fa fa-check"></i><b>11.3</b> Transformaciones de escala y de la distribución de la variable objetivo</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="id_130010.html"><a href="id_130010.html#id_31"><i class="fa fa-check"></i><b>11.3.1</b> Transformaciones de la variable objetivo</a></li>
<li class="chapter" data-level="11.3.2" data-path="id_130010.html"><a href="id_130010.html#escalado-de-datos"><i class="fa fa-check"></i><b>11.3.2</b> Escalado de datos</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="id_130010.html"><a href="id_130010.html#feature-engineering"><i class="fa fa-check"></i><b>11.4</b> <em>Feature engineering</em></a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="id_130010.html"><a href="id_130010.html#binning"><i class="fa fa-check"></i><b>11.4.1</b> <em>Binning</em></a></li>
<li class="chapter" data-level="11.4.2" data-path="id_130010.html"><a href="id_130010.html#codificación"><i class="fa fa-check"></i><b>11.4.2</b> Codificación</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="id_130010.html"><a href="id_130010.html#reducción-de-dimensionalidad"><i class="fa fa-check"></i><b>11.5</b> Reducción de dimensionalidad</a></li>
<li class="chapter" data-level="11.6" data-path="id_130010.html"><a href="id_130010.html#otras-transformaciones"><i class="fa fa-check"></i><b>11.6</b> Otras transformaciones</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="id_130010.html"><a href="id_130010.html#particionado-de-datos"><i class="fa fa-check"></i><b>11.6.1</b> Particionado de datos</a></li>
<li class="chapter" data-level="11.6.2" data-path="id_130010.html"><a href="id_130010.html#técnicas-para-manejar-datos-no-balanceados"><i class="fa fa-check"></i><b>11.6.2</b> Técnicas para manejar datos no balanceados</a></li>
<li class="chapter" data-level="11.6.3" data-path="id_130010.html"><a href="id_130010.html#métodos-de-remuestreo"><i class="fa fa-check"></i><b>11.6.3</b> Métodos de remuestreo</a></li>
<li class="chapter" data-level="11.6.4" data-path="id_130010.html"><a href="id_130010.html#ajuste-de-hiperparámetros"><i class="fa fa-check"></i><b>11.6.4</b> Ajuste de hiperparámetros</a></li>
<li class="chapter" data-level="11.6.5" data-path="id_130010.html"><a href="id_130010.html#evaluación-de-modelos"><i class="fa fa-check"></i><b>11.6.5</b> Evaluación de modelos</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Técnicas de modelización estadísticas avanzadas</b></span></li>
<li class="chapter" data-level="12" data-path="Funda-probab.html"><a href="Funda-probab.html"><i class="fa fa-check"></i><b>12</b> Fundamentos de probabilidad</a>
<ul>
<li class="chapter" data-level="12.1" data-path="Funda-probab.html"><a href="Funda-probab.html#introducción-a-la-probabilidad"><i class="fa fa-check"></i><b>12.1</b> Introducción a la probabilidad</a></li>
<li class="chapter" data-level="12.2" data-path="Funda-probab.html"><a href="Funda-probab.html#probabilidad-elementos-básicos-definición-y-teoremas"><i class="fa fa-check"></i><b>12.2</b> Probabilidad: elementos básicos, definición y teoremas</a></li>
<li class="chapter" data-level="12.3" data-path="Funda-probab.html"><a href="Funda-probab.html#variable-aleatoria-y-su-distribución-tipos-de-variables-aleatorias"><i class="fa fa-check"></i><b>12.3</b> Variable aleatoria y su distribución: tipos de variables aleatorias</a></li>
<li class="chapter" data-level="12.4" data-path="Funda-probab.html"><a href="Funda-probab.html#modelos-de-distribución-de-probabilidad"><i class="fa fa-check"></i><b>12.4</b> Modelos de distribución de probabilidad</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="Funda-probab.html"><a href="Funda-probab.html#modelos-discretos"><i class="fa fa-check"></i><b>12.4.1</b> Modelos discretos</a></li>
<li class="chapter" data-level="12.4.2" data-path="Funda-probab.html"><a href="Funda-probab.html#modelos-continuos"><i class="fa fa-check"></i><b>12.4.2</b> Modelos continuos</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="Funda-probab.html"><a href="Funda-probab.html#tcl"><i class="fa fa-check"></i><b>12.5</b> Teorema central del límite (TCL)</a></li>
<li class="chapter" data-level="12.6" data-path="Funda-probab.html"><a href="Funda-probab.html#ejemplo-de-distribuciones-usando-r"><i class="fa fa-check"></i><b>12.6</b> Ejemplo de distribuciones usando <strong>R</strong></a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Fundainfer.html"><a href="Fundainfer.html"><i class="fa fa-check"></i><b>13</b> Fundamentos de Inferencia Estadística</a>
<ul>
<li class="chapter" data-level="13.1" data-path="Fundainfer.html"><a href="Fundainfer.html#introinfer"><i class="fa fa-check"></i><b>13.1</b> Introducción a la Inferencia Estadística</a></li>
<li class="chapter" data-level="13.2" data-path="Fundainfer.html"><a href="Fundainfer.html#mas"><i class="fa fa-check"></i><b>13.2</b> Muestreo aleatorio simple</a></li>
<li class="chapter" data-level="13.3" data-path="Fundainfer.html"><a href="Fundainfer.html#estimpuntual"><i class="fa fa-check"></i><b>13.3</b> Estimación puntual</a></li>
<li class="chapter" data-level="13.4" data-path="Fundainfer.html"><a href="Fundainfer.html#estimintervalos"><i class="fa fa-check"></i><b>13.4</b> Estimación por intervalos</a></li>
<li class="chapter" data-level="13.5" data-path="Fundainfer.html"><a href="Fundainfer.html#contrhip"><i class="fa fa-check"></i><b>13.5</b> Contrastes de hipótesis</a></li>
<li class="chapter" data-level="13.6" data-path="Fundainfer.html"><a href="Fundainfer.html#pobnormales"><i class="fa fa-check"></i><b>13.6</b> Inferencia estadística paramétrica sobre poblaciones normales</a></li>
<li class="chapter" data-level="13.7" data-path="Fundainfer.html"><a href="Fundainfer.html#ejemplopobnorm"><i class="fa fa-check"></i><b>13.7</b> Inferencia sobre poblaciones normales con <strong>R</strong></a></li>
<li class="chapter" data-level="13.8" data-path="Fundainfer.html"><a href="Fundainfer.html#contrnormalidad"><i class="fa fa-check"></i><b>13.8</b> Inferencia estadística no paramétrica: contrastes de normalidad</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="muestreo.html"><a href="muestreo.html"><i class="fa fa-check"></i><b>14</b> Métodos de muestreo y remuestreo</a>
<ul>
<li class="chapter" data-level="14.1" data-path="muestreo.html"><a href="muestreo.html#introducción-al-muestreo"><i class="fa fa-check"></i><b>14.1</b> Introducción al muestreo</a></li>
<li class="chapter" data-level="14.2" data-path="muestreo.html"><a href="muestreo.html#muestreo-aleatorio-simple-1"><i class="fa fa-check"></i><b>14.2</b> Muestreo aleatorio simple</a></li>
<li class="chapter" data-level="14.3" data-path="muestreo.html"><a href="muestreo.html#muestestra"><i class="fa fa-check"></i><b>14.3</b> Muestreo estratificado</a></li>
<li class="chapter" data-level="14.4" data-path="muestreo.html"><a href="muestreo.html#otros-tipos-de-muestreo-probabilístico"><i class="fa fa-check"></i><b>14.4</b> Otros tipos de muestreo probabilístico</a></li>
<li class="chapter" data-level="14.5" data-path="muestreo.html"><a href="muestreo.html#técnicas-de-remuestreo-bootstrap."><i class="fa fa-check"></i><b>14.5</b> Técnicas de remuestreo: Bootstrap.</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="modelización-lineal.html"><a href="modelización-lineal.html"><i class="fa fa-check"></i><b>15</b> Modelización lineal</a>
<ul>
<li class="chapter" data-level="15.1" data-path="modelización-lineal.html"><a href="modelización-lineal.html#modelización"><i class="fa fa-check"></i><b>15.1</b> Modelización</a></li>
<li class="chapter" data-level="15.2" data-path="modelización-lineal.html"><a href="modelización-lineal.html#procedimiento-de-modelización"><i class="fa fa-check"></i><b>15.2</b> Procedimiento de modelización</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="modelización-lineal.html"><a href="modelización-lineal.html#Bondad"><i class="fa fa-check"></i><b>15.2.1</b> Estimación del modelo</a></li>
<li class="chapter" data-level="15.2.2" data-path="modelización-lineal.html"><a href="modelización-lineal.html#validación-del-modelo"><i class="fa fa-check"></i><b>15.2.2</b> Validación del modelo</a></li>
<li class="chapter" data-level="15.2.3" data-path="modelización-lineal.html"><a href="modelización-lineal.html#interpretación-de-los-coeficientes"><i class="fa fa-check"></i><b>15.2.3</b> Interpretación de los coeficientes</a></li>
<li class="chapter" data-level="15.2.4" data-path="modelización-lineal.html"><a href="modelización-lineal.html#predicción"><i class="fa fa-check"></i><b>15.2.4</b> Predicción</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="modelización-lineal.html"><a href="modelización-lineal.html#procedimiento-con-r-la-función-lm"><i class="fa fa-check"></i><b>15.3</b> Procedimiento con R: la función <code>lm</code></a></li>
<li class="chapter" data-level="15.4" data-path="modelización-lineal.html"><a href="modelización-lineal.html#Casos"><i class="fa fa-check"></i><b>15.4</b> Casos prácticos</a>
<ul>
<li class="chapter" data-level="15.4.1" data-path="modelización-lineal.html"><a href="modelización-lineal.html#estimación-de-los-coeficientes"><i class="fa fa-check"></i><b>15.4.1</b> Estimación de los coeficientes</a></li>
<li class="chapter" data-level="15.4.2" data-path="modelización-lineal.html"><a href="modelización-lineal.html#validación"><i class="fa fa-check"></i><b>15.4.2</b> Validación</a></li>
<li class="chapter" data-level="15.4.3" data-path="modelización-lineal.html"><a href="modelización-lineal.html#interpretación-de-los-coeficientes-1"><i class="fa fa-check"></i><b>15.4.3</b> Interpretación de los coeficientes</a></li>
<li class="chapter" data-level="15.4.4" data-path="modelización-lineal.html"><a href="modelización-lineal.html#predicción-1"><i class="fa fa-check"></i><b>15.4.4</b> Predicción</a></li>
<li class="chapter" data-level="15.4.5" data-path="modelización-lineal.html"><a href="modelización-lineal.html#nuevo-ajuste-con-logozone"><i class="fa fa-check"></i><b>15.4.5</b> Nuevo ajuste con <code>log(Ozone)</code></a></li>
<li class="chapter" data-level="15.4.6" data-path="modelización-lineal.html"><a href="modelización-lineal.html#coeficientes-de-variables-categóricas"><i class="fa fa-check"></i><b>15.4.6</b> Coeficientes de variables categóricas</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="modelización-lineal.html"><a href="modelización-lineal.html#comentarios-finales"><i class="fa fa-check"></i><b>15.5</b> Comentarios finales</a>
<ul>
<li class="chapter" data-level="" data-path="modelización-lineal.html"><a href="modelización-lineal.html#resumen"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html"><i class="fa fa-check"></i><b>16</b> Modelos lineales generalizados</a>
<ul>
<li class="chapter" data-level="16.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#motivación"><i class="fa fa-check"></i><b>16.1</b> Motivación</a></li>
<li class="chapter" data-level="16.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#modelo-y-sus-componentes"><i class="fa fa-check"></i><b>16.2</b> Modelo y sus componentes</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#función-enlace"><i class="fa fa-check"></i><b>16.2.1</b> Función enlace </a></li>
<li class="chapter" data-level="16.2.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glms-en-r"><i class="fa fa-check"></i><b>16.2.2</b> GLMs en <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-logística"><i class="fa fa-check"></i><b>16.3</b> Regresión logística</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#procedimiento-de-ajuste"><i class="fa fa-check"></i><b>16.3.1</b> Procedimiento de ajuste</a></li>
<li class="chapter" data-level="16.3.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#adecuación-del-modelo"><i class="fa fa-check"></i><b>16.3.2</b> Adecuación del modelo</a></li>
<li class="chapter" data-level="16.3.3" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#SECCinterp"><i class="fa fa-check"></i><b>16.3.3</b> Interpretación de resultados</a></li>
<li class="chapter" data-level="16.3.4" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#predicción.-curva-roc-y-auc"><i class="fa fa-check"></i><b>16.3.4</b> Predicción. Curva ROC y AUC</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-de-poisson"><i class="fa fa-check"></i><b>16.4</b> Regresión de Poisson</a></li>
<li class="chapter" data-level="16.5" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#casos-prácticos"><i class="fa fa-check"></i><b>16.5</b> Casos prácticos</a>
<ul>
<li class="chapter" data-level="16.5.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejemplos-de-regresión-logística"><i class="fa fa-check"></i><b>16.5.1</b> Ejemplos de regresión logística</a></li>
<li class="chapter" data-level="16.5.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejemplo-de-regresión-de-poisson"><i class="fa fa-check"></i><b>16.5.2</b> Ejemplo de regresión de Poisson</a></li>
<li class="chapter" data-level="" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#resumen-1"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html"><i class="fa fa-check"></i><b>17</b> Modelos aditivos generalizados</a>
<ul>
<li class="chapter" data-level="17.1" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#introducción-7"><i class="fa fa-check"></i><b>17.1</b> Introducción</a></li>
<li class="chapter" data-level="17.2" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#splines-con-penalizaciones"><i class="fa fa-check"></i><b>17.2</b> Splines con penalizaciones</a></li>
<li class="chapter" data-level="17.3" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#aspectos-metodológicos"><i class="fa fa-check"></i><b>17.3</b> Aspectos metodológicos</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#estimación-de-los-paraámetros-del-modelo"><i class="fa fa-check"></i><b>17.3.1</b> Estimación de los paraámetros del modelo</a></li>
<li class="chapter" data-level="17.3.2" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#inferencia-sobre-las-funciones-suaves"><i class="fa fa-check"></i><b>17.3.2</b> Inferencia sobre las funciones suaves</a></li>
<li class="chapter" data-level="17.3.3" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#suavizado-mutidimensional-y-para-datos-no-gaussianos"><i class="fa fa-check"></i><b>17.3.3</b> Suavizado mutidimensional y para datos no Gaussianos</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#la-función-gam-del-paquete-mgcv"><i class="fa fa-check"></i><b>17.4</b> La función <code>gam</code> del paquete <code>mgcv</code></a></li>
<li class="chapter" data-level="17.5" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#casos-prácticos-1"><i class="fa fa-check"></i><b>17.5</b> Casos prácticos</a>
<ul>
<li class="chapter" data-level="17.5.1" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#modelo-unidimensional-con-fossil"><i class="fa fa-check"></i><b>17.5.1</b> Modelo unidimensional con <code>fossil</code></a></li>
<li class="chapter" data-level="17.5.2" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#modelo-aditivo-con-airquality"><i class="fa fa-check"></i><b>17.5.2</b> Modelo aditivo con <code>airquality</code></a></li>
<li class="chapter" data-level="17.5.3" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#modelo-semiparamétrico-con-onions"><i class="fa fa-check"></i><b>17.5.3</b> Modelo semiparamétrico con <code>onions</code></a></li>
<li class="chapter" data-level="17.5.4" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#modelo-aditivo-generalizado-y-multidimensional-con-smacker"><i class="fa fa-check"></i><b>17.5.4</b> Modelo aditivo generalizado y multidimensional, con <code>smacker</code></a></li>
<li class="chapter" data-level="" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#resumen-2"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html"><i class="fa fa-check"></i><b>18</b> Modelos mixtos</a>
<ul>
<li class="chapter" data-level="18.1" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#conceptos-básicos"><i class="fa fa-check"></i><b>18.1</b> Conceptos básicos</a>
<ul>
<li class="chapter" data-level="18.1.1" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#tipo-y-estructura-de-los-datos"><i class="fa fa-check"></i><b>18.1.1</b> Tipo y estructura de los datos</a></li>
<li class="chapter" data-level="18.1.2" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#efectos-fijos-o-aleatorios"><i class="fa fa-check"></i><b>18.1.2</b> ¿Efectos fijos o aleatorios?</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#formulación-del-modelo-con-efectos-aleatorios-o-modelos-mixtos"><i class="fa fa-check"></i><b>18.2</b> Formulación del modelo con efectos aleatorios o modelos mixtos</a>
<ul>
<li class="chapter" data-level="18.2.1" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#formulación-general"><i class="fa fa-check"></i><b>18.2.1</b> Formulación general</a></li>
<li class="chapter" data-level="18.2.2" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#inferencia-y-selección-del-modelo"><i class="fa fa-check"></i><b>18.2.2</b> Inferencia y selección del modelo</a></li>
<li class="chapter" data-level="18.2.3" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#diagnosis-del-modelo"><i class="fa fa-check"></i><b>18.2.3</b> Diagnosis del modelo</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#funciones-de-r-para-ajustar-modelos-mixtos"><i class="fa fa-check"></i><b>18.3</b> Funciones de <code>R</code> para ajustar modelos mixtos</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#la-función-lmer"><i class="fa fa-check"></i><b>18.3.1</b> La función <code>lmer()</code></a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#caso-práctico"><i class="fa fa-check"></i><b>18.4</b> Caso práctico</a>
<ul>
<li class="chapter" data-level="18.4.1" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#modelo-con-ordenada-en-el-origen-aleatoria"><i class="fa fa-check"></i><b>18.4.1</b> Modelo con ordenada en el origen aleatoria</a></li>
<li class="chapter" data-level="18.4.2" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#modelo-con-pendiente-aleatoria"><i class="fa fa-check"></i><b>18.4.2</b> Modelo con pendiente aleatoria</a></li>
<li class="chapter" data-level="18.4.3" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#cómo-construir-el-modelo-en-la-práctica"><i class="fa fa-check"></i><b>18.4.3</b> ¿Cómo construir el modelo en la práctica?</a></li>
<li class="chapter" data-level="" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#resumen-3"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html"><i class="fa fa-check"></i><b>19</b> Modelos sparse y métodos penalizados de regresión</a>
<ul>
<li class="chapter" data-level="19.1" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#introducción-8"><i class="fa fa-check"></i><b>19.1</b> Introducción</a></li>
<li class="chapter" data-level="19.2" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#selección-del-mejor-subconjunto"><i class="fa fa-check"></i><b>19.2</b> Selección del mejor subconjunto</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#ejemplo-sueldo-de-jugadores-de-béisbol"><i class="fa fa-check"></i><b>19.2.1</b> Ejemplo: Sueldo de jugadores de béisbol</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#selección-stepwise"><i class="fa fa-check"></i><b>19.3</b> Selección <em>Stepwise</em></a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#forward-stepwise"><i class="fa fa-check"></i><b>19.3.1</b> Forward stepwise</a></li>
<li class="chapter" data-level="19.3.2" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#backward-stepwise"><i class="fa fa-check"></i><b>19.3.2</b> Backward stepwise</a></li>
<li class="chapter" data-level="19.3.3" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#ejemplo-sueldo-de-jugadores-de-béisbol-1"><i class="fa fa-check"></i><b>19.3.3</b> Ejemplo: Sueldo de jugadores de béisbol</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#métodos-shrinkage"><i class="fa fa-check"></i><b>19.4</b> Métodos Shrinkage</a>
<ul>
<li class="chapter" data-level="19.4.1" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#regresión-ridge"><i class="fa fa-check"></i><b>19.4.1</b> Regresión ridge</a></li>
<li class="chapter" data-level="19.4.2" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#selección-del-parámetro-de-tuneado"><i class="fa fa-check"></i><b>19.4.2</b> Selección del parámetro de tuneado</a></li>
<li class="chapter" data-level="19.4.3" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#regresión-lasso"><i class="fa fa-check"></i><b>19.4.3</b> Regresión Lasso</a></li>
<li class="chapter" data-level="19.4.4" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#elastic-net"><i class="fa fa-check"></i><b>19.4.4</b> Elastic net </a></li>
<li class="chapter" data-level="" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#resumen-4"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="modelización-de-series-temporales.html"><a href="modelización-de-series-temporales.html"><i class="fa fa-check"></i><b>20</b> Modelización de series temporales</a>
<ul>
<li class="chapter" data-level="20.1" data-path="modelización-de-series-temporales.html"><a href="modelización-de-series-temporales.html#conceptos-básicos-1"><i class="fa fa-check"></i><b>20.1</b> Conceptos básicos</a></li>
<li class="chapter" data-level="20.2" data-path="modelización-de-series-temporales.html"><a href="modelización-de-series-temporales.html#modelos-arima"><i class="fa fa-check"></i><b>20.2</b> Modelos ARIMA</a></li>
<li class="chapter" data-level="20.3" data-path="modelización-de-series-temporales.html"><a href="modelización-de-series-temporales.html#análisis-de-series-temporales-con-r"><i class="fa fa-check"></i><b>20.3</b> Análisis de series temporales con R</a>
<ul>
<li class="chapter" data-level="20.3.1" data-path="modelización-de-series-temporales.html"><a href="modelización-de-series-temporales.html#identificación-o-especificación-del-modelo"><i class="fa fa-check"></i><b>20.3.1</b> Identificación o especificación del modelo</a></li>
<li class="chapter" data-level="20.3.2" data-path="modelización-de-series-temporales.html"><a href="modelización-de-series-temporales.html#estimación-del-modelo"><i class="fa fa-check"></i><b>20.3.2</b> Estimación del modelo</a></li>
<li class="chapter" data-level="20.3.3" data-path="modelización-de-series-temporales.html"><a href="modelización-de-series-temporales.html#diagnosis-validación-y-contrastación"><i class="fa fa-check"></i><b>20.3.3</b> Diagnosis, validación y contrastación</a></li>
<li class="chapter" data-level="20.3.4" data-path="modelización-de-series-temporales.html"><a href="modelización-de-series-temporales.html#predicción-2"><i class="fa fa-check"></i><b>20.3.4</b> Predicción</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliografía.html"><a href="bibliografía.html"><i class="fa fa-check"></i>Bibliografía</a></li>
<li class="chapter" data-level="21" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html"><i class="fa fa-check"></i><b>21</b> Análisis de tablas de contingencia</a>
<ul>
<li class="chapter" data-level="21.1" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#introducción-9"><i class="fa fa-check"></i><b>21.1</b> Introducción</a>
<ul>
<li class="chapter" data-level="21.1.1" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#motiv"><i class="fa fa-check"></i><b>21.1.1</b> Motivación</a></li>
<li class="chapter" data-level="21.1.2" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#notac"><i class="fa fa-check"></i><b>21.1.2</b> Notación</a></li>
<li class="chapter" data-level="21.1.3" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#diseños-experimentales-o-procedimientos-de-muestreo-que-dan-lugar-a-una-tabla-de-contingencia"><i class="fa fa-check"></i><b>21.1.3</b> Diseños experimentales o procedimientos de muestreo que dan lugar a una tabla de contingencia</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#contraste-de-independencia-en-tablas-2times-2"><i class="fa fa-check"></i><b>21.2</b> Contraste de independencia en tablas <span class="math inline">\((2\times 2)\)</span></a>
<ul>
<li class="chapter" data-level="21.2.1" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#introducción-10"><i class="fa fa-check"></i><b>21.2.1</b> Introducción</a></li>
<li class="chapter" data-level="21.2.2" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#plantgen"><i class="fa fa-check"></i><b>21.2.2</b> Planteamiento general del contraste exacto de independencia</a></li>
<li class="chapter" data-level="21.2.3" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#algoritmo"><i class="fa fa-check"></i><b>21.2.3</b> Algoritmo para la realización del contraste exacto de independencia</a></li>
<li class="chapter" data-level="21.2.4" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#contraste-de-independencia-diseño-tipo-1"><i class="fa fa-check"></i><b>21.2.4</b> Contraste de independencia: Diseño Tipo 1</a></li>
<li class="chapter" data-level="21.2.5" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#dise"><i class="fa fa-check"></i><b>21.2.5</b> Contraste de independencia: Diseños Tipo 2 y Tipo 3</a></li>
</ul></li>
<li class="chapter" data-level="21.3" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#contraste-de-independencia-en-tablas-rtimes-c"><i class="fa fa-check"></i><b>21.3</b> Contraste de independencia en tablas <span class="math inline">\(R\times C\)</span></a>
<ul>
<li class="chapter" data-level="21.3.1" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#IntroRxC"><i class="fa fa-check"></i><b>21.3.1</b> Introducción</a></li>
<li class="chapter" data-level="21.3.2" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#contaprox"><i class="fa fa-check"></i><b>21.3.2</b> Contrastes aproximados</a></li>
<li class="chapter" data-level="21.3.3" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#contraste-aproximado-con-corrección-de-continuidad-1"><i class="fa fa-check"></i><b>21.3.3</b> Contraste aproximado con corrección de continuidad</a></li>
</ul></li>
<li class="chapter" data-level="21.4" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#medidas"><i class="fa fa-check"></i><b>21.4</b> Medidas de asociación en tablas <span class="math inline">\(2\times 2\)</span></a>
<ul>
<li class="chapter" data-level="21.4.1" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#introducción-11"><i class="fa fa-check"></i><b>21.4.1</b> Introducción</a></li>
<li class="chapter" data-level="21.4.2" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#la-hatq-de-yule"><i class="fa fa-check"></i><b>21.4.2</b> La <span class="math inline">\(\hat{Q}\)</span> de Yule</a></li>
<li class="chapter" data-level="21.4.3" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#otras-medidas-de-asociación-para-tablas-2times-2"><i class="fa fa-check"></i><b>21.4.3</b> Otras medidas de asociación para tablas <span class="math inline">\(2\times 2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="21.5" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#medidas-de-asociación-en-tablas-rtimes-c"><i class="fa fa-check"></i><b>21.5</b> Medidas de asociación en tablas <span class="math inline">\(R\times C\)</span></a>
<ul>
<li class="chapter" data-level="21.5.1" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#introducción-12"><i class="fa fa-check"></i><b>21.5.1</b> Introducción</a></li>
<li class="chapter" data-level="21.5.2" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#medidas-derivadas-del-estadístico-chi-cuadrado"><i class="fa fa-check"></i><b>21.5.2</b> Medidas derivadas del estadístico Chi-cuadrado</a></li>
<li class="chapter" data-level="21.5.3" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#medidas-basadas-en-la-reducción-proporcional-del-error-lambda-de-goodman-y-kruskal"><i class="fa fa-check"></i><b>21.5.3</b> Medidas basadas en la reducción proporcional del error: <span class="math inline">\(\lambda\)</span> de Goodman y Kruskal</a></li>
<li class="chapter" data-level="21.5.4" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#determinación-de-las-fuentes-de-asociación"><i class="fa fa-check"></i><b>21.5.4</b> Determinación de las fuentes de asociación</a></li>
</ul></li>
<li class="chapter" data-level="21.6" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#contrastes-de-independencia-en-tablas-multidimensionales"><i class="fa fa-check"></i><b>21.6</b> Contrastes de independencia en tablas multidimensionales</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="correspondencias.html"><a href="correspondencias.html"><i class="fa fa-check"></i><b>22</b> Análisis de correspondencias</a>
<ul>
<li class="chapter" data-level="22.1" data-path="correspondencias.html"><a href="correspondencias.html#introducción-13"><i class="fa fa-check"></i><b>22.1</b> Introducción</a></li>
<li class="chapter" data-level="22.2" data-path="correspondencias.html"><a href="correspondencias.html#metodología-del-análisis-de-correspondencias"><i class="fa fa-check"></i><b>22.2</b> Metodología del análisis de correspondencias</a>
<ul>
<li class="chapter" data-level="22.2.1" data-path="correspondencias.html"><a href="correspondencias.html#proyecciones-fila-columna-y-simétrica"><i class="fa fa-check"></i><b>22.2.1</b> Proyecciones fila, columna y simétrica</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="correspondencias.html"><a href="correspondencias.html#ejemplos-de-análisis-de-correspondencias-con-r"><i class="fa fa-check"></i><b>22.3</b> Ejemplos de análisis de correspondencias con <strong>R</strong></a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="análisis-conjunto.html"><a href="análisis-conjunto.html"><i class="fa fa-check"></i><b>23</b> Análisis conjunto</a>
<ul>
<li class="chapter" data-level="23.1" data-path="análisis-conjunto.html"><a href="análisis-conjunto.html#introducción-conceptos-clave-y-tipos-de-análisis"><i class="fa fa-check"></i><b>23.1</b> Introducción, conceptos clave y tipos de análisis</a></li>
<li class="chapter" data-level="23.2" data-path="análisis-conjunto.html"><a href="análisis-conjunto.html#aplicación-del-análisis-conjunto-etapas"><i class="fa fa-check"></i><b>23.2</b> Aplicación del Análisis Conjunto (etapas):</a></li>
<li class="chapter" data-level="23.3" data-path="análisis-conjunto.html"><a href="análisis-conjunto.html#ejemplo-utilizando-r"><i class="fa fa-check"></i><b>23.3</b> Ejemplo utilizando R:</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="análisis-discriminante.html"><a href="análisis-discriminante.html"><i class="fa fa-check"></i><b>24</b> Análisis discriminante</a>
<ul>
<li class="chapter" data-level="24.1" data-path="análisis-discriminante.html"><a href="análisis-discriminante.html#introducción-14"><i class="fa fa-check"></i><b>24.1</b> Introducción</a></li>
<li class="chapter" data-level="24.2" data-path="análisis-discriminante.html"><a href="análisis-discriminante.html#tipos-de-análisis-discriminantes"><i class="fa fa-check"></i><b>24.2</b> Tipos de análisis discriminantes:</a></li>
<li class="chapter" data-level="24.3" data-path="análisis-discriminante.html"><a href="análisis-discriminante.html#ejemplos"><i class="fa fa-check"></i><b>24.3</b> Ejemplos:</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html"><i class="fa fa-check"></i><b>25</b> Árboles de clasificación y regresión </a>
<ul>
<li class="chapter" data-level="25.1" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#introducción-15"><i class="fa fa-check"></i><b>25.1</b> Introducción </a></li>
<li class="chapter" data-level="25.2" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#aprendizaje-con-árboles-de-decisión"><i class="fa fa-check"></i><b>25.2</b> Aprendizaje con árboles de decisión</a></li>
<li class="chapter" data-level="25.3" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#cómo-se-va-dividiendo-el-árbol"><i class="fa fa-check"></i><b>25.3</b> ¿Cómo se va dividiendo el árbol? </a>
<ul>
<li class="chapter" data-level="25.3.1" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#impureza-de-gini"><i class="fa fa-check"></i><b>25.3.1</b> Impureza de Gini</a></li>
<li class="chapter" data-level="25.3.2" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#entropía"><i class="fa fa-check"></i><b>25.3.2</b> Entropía </a></li>
<li class="chapter" data-level="25.3.3" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#ganancia-de-información"><i class="fa fa-check"></i><b>25.3.3</b> Ganancia de información</a></li>
<li class="chapter" data-level="25.3.4" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#suma-residual-de-cuadrados-mínima"><i class="fa fa-check"></i><b>25.3.4</b> Suma residual de cuadrados mínima</a></li>
</ul></li>
<li class="chapter" data-level="25.4" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#sobreajuste"><i class="fa fa-check"></i><b>25.4</b> Sobreajuste </a></li>
<li class="chapter" data-level="25.5" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#cuánto-debe-crecer-un-árbol"><i class="fa fa-check"></i><b>25.5</b> ¿Cuánto debe crecer un árbol? </a>
<ul>
<li class="chapter" data-level="25.5.1" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#la-parada-temprana"><i class="fa fa-check"></i><b>25.5.1</b> La parada temprana </a></li>
<li class="chapter" data-level="25.5.2" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#la-poda"><i class="fa fa-check"></i><b>25.5.2</b> La poda </a></li>
</ul></li>
<li class="chapter" data-level="25.6" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#el-algoritmo-id3-para-la-construcción-de-un-árbol-de-decisión"><i class="fa fa-check"></i><b>25.6</b> El algoritmo ID3 para la construcción de un árbol de decisión</a></li>
<li class="chapter" data-level="25.7" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#procedimiento-con-r-la-funcion-rpart"><i class="fa fa-check"></i><b>25.7</b> Procedimiento con R: la funcion <code>rpart</code></a></li>
<li class="chapter" data-level="25.8" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#aplicaciones-de-los-árboles-de-decisión"><i class="fa fa-check"></i><b>25.8</b> Aplicaciones de los árboles de decisión</a>
<ul>
<li class="chapter" data-level="25.8.1" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#el-caso-de-negocio"><i class="fa fa-check"></i><b>25.8.1</b> El caso de negocio</a></li>
<li class="chapter" data-level="25.8.2" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#árbol-de-clasificación-para-determinar-la-intención-de-compra"><i class="fa fa-check"></i><b>25.8.2</b> Árbol de clasificación para determinar la intención de compra</a></li>
<li class="chapter" data-level="25.8.3" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#árbol-de-regresión-para-estimar-el-número-de-días-hospitalizado"><i class="fa fa-check"></i><b>25.8.3</b> Árbol de regresión para estimar el número de días hospitalizado</a></li>
<li class="chapter" data-level="" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#resumen-5"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="26" data-path="máquinas-de-vector-soporte.html"><a href="máquinas-de-vector-soporte.html"><i class="fa fa-check"></i><b>26</b> Máquinas de Vector Soporte</a>
<ul>
<li class="chapter" data-level="26.1" data-path="máquinas-de-vector-soporte.html"><a href="máquinas-de-vector-soporte.html#introducción-16"><i class="fa fa-check"></i><b>26.1</b> Introducción</a></li>
<li class="chapter" data-level="26.2" data-path="máquinas-de-vector-soporte.html"><a href="máquinas-de-vector-soporte.html#algoritmo-svm-para-clasificación-binaria"><i class="fa fa-check"></i><b>26.2</b> Algoritmo SVM para clasificación binaria</a></li>
<li class="chapter" data-level="26.3" data-path="máquinas-de-vector-soporte.html"><a href="máquinas-de-vector-soporte.html#y-si-tengo-más-de-dos-clases"><i class="fa fa-check"></i><b>26.3</b> ¿Y si tengo más de dos clases?</a></li>
<li class="chapter" data-level="26.4" data-path="máquinas-de-vector-soporte.html"><a href="máquinas-de-vector-soporte.html#truco-del-kernel-tratando-con-la-no-linearidad"><i class="fa fa-check"></i><b>26.4</b> Truco del Kernel: Tratando con la no linearidad</a>
<ul>
<li class="chapter" data-level="26.4.1" data-path="máquinas-de-vector-soporte.html"><a href="máquinas-de-vector-soporte.html#algunos-kernels-populares"><i class="fa fa-check"></i><b>26.4.1</b> Algunos kernels populares</a></li>
</ul></li>
<li class="chapter" data-level="26.5" data-path="máquinas-de-vector-soporte.html"><a href="máquinas-de-vector-soporte.html#procedimiento-con-r-la-funcion-svm"><i class="fa fa-check"></i><b>26.5</b> Procedimiento con R: la funcion <code>svm</code></a></li>
<li class="chapter" data-level="26.6" data-path="máquinas-de-vector-soporte.html"><a href="máquinas-de-vector-soporte.html#aplicación-de-un-modelo-svm-radial-con-ajuste-automático-en-r"><i class="fa fa-check"></i><b>26.6</b> Aplicación de un modelo SVM Radial con ajuste automático en R</a>
<ul>
<li class="chapter" data-level="26.6.1" data-path="máquinas-de-vector-soporte.html"><a href="máquinas-de-vector-soporte.html#importancia-de-las-variables"><i class="fa fa-check"></i><b>26.6.1</b> Importancia de las variables</a></li>
<li class="chapter" data-level="" data-path="máquinas-de-vector-soporte.html"><a href="máquinas-de-vector-soporte.html#resumen-6"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="27" data-path="clasificador-k-vecinos-más-próximos.html"><a href="clasificador-k-vecinos-más-próximos.html"><i class="fa fa-check"></i><b>27</b> Clasificador k-vecinos más próximos</a>
<ul>
<li class="chapter" data-level="27.1" data-path="clasificador-k-vecinos-más-próximos.html"><a href="clasificador-k-vecinos-más-próximos.html#introducción-17"><i class="fa fa-check"></i><b>27.1</b> Introducción</a></li>
<li class="chapter" data-level="27.2" data-path="clasificador-k-vecinos-más-próximos.html"><a href="clasificador-k-vecinos-más-próximos.html#decisiones-a-tener-en-cuenta"><i class="fa fa-check"></i><b>27.2</b> Decisiones a tener en cuenta</a>
<ul>
<li class="chapter" data-level="27.2.1" data-path="clasificador-k-vecinos-más-próximos.html"><a href="clasificador-k-vecinos-más-próximos.html#función-de-distancia-a-utilizar"><i class="fa fa-check"></i><b>27.2.1</b> Función de distancia a utilizar</a></li>
<li class="chapter" data-level="27.2.2" data-path="clasificador-k-vecinos-más-próximos.html"><a href="clasificador-k-vecinos-más-próximos.html#número-de-vecinos-k-seleccionados"><i class="fa fa-check"></i><b>27.2.2</b> Número de vecinos (k) seleccionados</a></li>
</ul></li>
<li class="chapter" data-level="27.3" data-path="clasificador-k-vecinos-más-próximos.html"><a href="clasificador-k-vecinos-más-próximos.html#procedimiento-con-r-la-funcion-knn"><i class="fa fa-check"></i><b>27.3</b> Procedimiento con R: la funcion knn</a></li>
<li class="chapter" data-level="27.4" data-path="clasificador-k-vecinos-más-próximos.html"><a href="clasificador-k-vecinos-más-próximos.html#aplicación-del-modelo-knn-en-r"><i class="fa fa-check"></i><b>27.4</b> Aplicación del modelo KNN en R</a>
<ul>
<li class="chapter" data-level="" data-path="clasificador-k-vecinos-más-próximos.html"><a href="clasificador-k-vecinos-más-próximos.html#resumen-7"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="28" data-path="naive-bayes.html"><a href="naive-bayes.html"><i class="fa fa-check"></i><b>28</b> Naive Bayes</a>
<ul>
<li class="chapter" data-level="28.1" data-path="naive-bayes.html"><a href="naive-bayes.html#introducción-18"><i class="fa fa-check"></i><b>28.1</b> Introducción</a></li>
<li class="chapter" data-level="28.2" data-path="naive-bayes.html"><a href="naive-bayes.html#teorema-de-bayes"><i class="fa fa-check"></i><b>28.2</b> Teorema de Bayes</a></li>
<li class="chapter" data-level="28.3" data-path="naive-bayes.html"><a href="naive-bayes.html#el-algoritmo-naive-bayes"><i class="fa fa-check"></i><b>28.3</b> El algoritmo Naive Bayes</a></li>
<li class="chapter" data-level="28.4" data-path="naive-bayes.html"><a href="naive-bayes.html#procedimiento-con-r-la-funcion-naive_bayes"><i class="fa fa-check"></i><b>28.4</b> Procedimiento con R: la funcion <code>naive_bayes</code></a></li>
<li class="chapter" data-level="28.5" data-path="naive-bayes.html"><a href="naive-bayes.html#aplicación-del-modelo-naive-bayes"><i class="fa fa-check"></i><b>28.5</b> Aplicación del modelo Naive Bayes</a>
<ul>
<li class="chapter" data-level="" data-path="naive-bayes.html"><a href="naive-bayes.html#resumen-8"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="29" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html"><i class="fa fa-check"></i><b>29</b> Bagging. Random Forest </a>
<ul>
<li class="chapter" data-level="29.1" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#introducción-metodos-de-ensamble"><i class="fa fa-check"></i><b>29.1</b> Introducción: Metodos de Ensamble</a></li>
<li class="chapter" data-level="29.2" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#bagging"><i class="fa fa-check"></i><b>29.2</b> Bagging</a></li>
<li class="chapter" data-level="29.3" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#procedimiento-con-r-la-función-bagging"><i class="fa fa-check"></i><b>29.3</b> Procedimiento con R: la función <code>bagging</code> </a></li>
<li class="chapter" data-level="29.4" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#implementando-bagging-en-r"><i class="fa fa-check"></i><b>29.4</b> Implementando bagging en R</a>
<ul>
<li class="chapter" data-level="29.4.1" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#interpretación-de-variables-en-el-bagging"><i class="fa fa-check"></i><b>29.4.1</b> Interpretación de variables en el bagging</a></li>
</ul></li>
<li class="chapter" data-level="29.5" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#random-forest"><i class="fa fa-check"></i><b>29.5</b> Random Forest</a>
<ul>
<li class="chapter" data-level="29.5.1" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#número-de-árboles-k"><i class="fa fa-check"></i><b>29.5.1</b> Número de árboles (<span class="math inline">\(K\)</span>)</a></li>
<li class="chapter" data-level="29.5.2" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#número-de-variables-a-considerar-mtry"><i class="fa fa-check"></i><b>29.5.2</b> Número de variables a considerar (<span class="math inline">\(mtry\)</span>)</a></li>
<li class="chapter" data-level="29.5.3" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#complejidad-de-los-árboles"><i class="fa fa-check"></i><b>29.5.3</b> Complejidad de los árboles</a></li>
<li class="chapter" data-level="29.5.4" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#esquema-de-muestreo"><i class="fa fa-check"></i><b>29.5.4</b> Esquema de muestreo</a></li>
<li class="chapter" data-level="29.5.5" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#regla-de-división"><i class="fa fa-check"></i><b>29.5.5</b> Regla de división</a></li>
</ul></li>
<li class="chapter" data-level="29.6" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#procedimiento-con-r-la-función-randomforest"><i class="fa fa-check"></i><b>29.6</b> Procedimiento con R: la función <code>randomForest</code></a></li>
<li class="chapter" data-level="29.7" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#aplicación-del-modelo-random-forest-en-r"><i class="fa fa-check"></i><b>29.7</b> Aplicación del modelo Random Forest en R</a>
<ul>
<li class="chapter" data-level="29.7.1" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#aplicación-del-random-forest"><i class="fa fa-check"></i><b>29.7.1</b> Aplicación del Random Forest</a></li>
<li class="chapter" data-level="" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#resumen-9"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="30" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html"><i class="fa fa-check"></i><b>30</b> Boosting. XGBoost.</a>
<ul>
<li class="chapter" data-level="30.1" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#introducción.-boosting."><i class="fa fa-check"></i><b>30.1</b> Introducción. Boosting.</a></li>
<li class="chapter" data-level="30.2" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#gradient-boosting"><i class="fa fa-check"></i><b>30.2</b> Gradient Boosting</a>
<ul>
<li class="chapter" data-level="30.2.1" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#hiperparámetros-del-modelo-gradient-boosting"><i class="fa fa-check"></i><b>30.2.1</b> Hiperparámetros del modelo gradient boosting</a></li>
<li class="chapter" data-level="30.2.2" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#estrategia-de-ajuste-de-hiperparametros"><i class="fa fa-check"></i><b>30.2.2</b> Estrategia de ajuste de hiperparametros</a></li>
</ul></li>
<li class="chapter" data-level="30.3" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#procedimiento-con-r-la-funcion-gbm"><i class="fa fa-check"></i><b>30.3</b> Procedimiento con R: la funcion <code>gbm</code></a></li>
<li class="chapter" data-level="30.4" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#aplicación-del-modelo-gbm-en-r"><i class="fa fa-check"></i><b>30.4</b> Aplicación del modelo GBM en R</a>
<ul>
<li class="chapter" data-level="30.4.1" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#gbm-con-ajuste-automático"><i class="fa fa-check"></i><b>30.4.1</b> GBM con ajuste automático</a></li>
</ul></li>
<li class="chapter" data-level="30.5" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#extreme-gradient-boosting-xgb"><i class="fa fa-check"></i><b>30.5</b> eXtreme Gradient Boosting (XGB)</a>
<ul>
<li class="chapter" data-level="30.5.1" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#hiperparametros-del-modelo-xgboost"><i class="fa fa-check"></i><b>30.5.1</b> Hiperparametros del modelo XGBoost</a></li>
</ul></li>
<li class="chapter" data-level="30.6" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#procedimiento-con-r-la-funcion-xgboost"><i class="fa fa-check"></i><b>30.6</b> Procedimiento con R: la funcion <code>xgboost</code></a></li>
<li class="chapter" data-level="30.7" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#aplicación-del-módelo-xgboost-en-r"><i class="fa fa-check"></i><b>30.7</b> Aplicación del módelo XGBoost en R</a>
<ul>
<li class="chapter" data-level="30.7.1" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#xgboost-y-ajuste-automático"><i class="fa fa-check"></i><b>30.7.1</b> XGBoost y ajuste automático</a></li>
<li class="chapter" data-level="" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#resumen-10"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://blog.uclm.es/tp-mbsba/"> Ciencia de datos con R </a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Fundamentos de ciencia de datos con R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="boosting.-xgboost." class="section level1 hasAnchor" number="30">
<h1><span class="header-section-number">Capítulo 30</span> Boosting. XGBoost.<a href="boosting.-xgboost..html#boosting.-xgboost." class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Ramón A. Carrasco e Itzcóatl Bueno<a href="#fn73" class="footnote-ref" id="fnref73"><sup>73</sup></a></em></p>
<div id="introducción.-boosting." class="section level2 hasAnchor" number="30.1">
<h2><span class="header-section-number">30.1</span> Introducción. Boosting.<a href="boosting.-xgboost..html#introducción.-boosting." class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>El boosting es el otro de los paradigmas de aprendizaje ensamblado presentado en el capítulo <span class="math inline">\(\ref{cap-bagg-rf}\)</span>. De nuevo, el boosting agrega multiples árboles de decisión. La lógica de los algoritmos boosting es combinar modelos débiles para conseguir un modelo fuerte. En este caso, el término “débil” hace referencia a un modelo con poco poder predictivo pero mejor que una predicción aleatoria. Mientras que “fuerte” es considerado un buen predictor para los datos. Para llegar a un algoritmo fuerte a partir de varios modelos debiles es necesario introducir ponderaciones a los árboles basadas en las clasificaciones erroneas del árbol anterior.</p>
<p>El boosting reduce el problema del sobreajuste utilizando menos árboles que un modelo random forest. Mientras que agregar más árboles al random forest ayuda a compensar el sobreajuste, también puede llevar a un aumento del mismo; y por ello hay que ser cauteloso a la hora de agregar nuevos árboles. El enfoque del boosting a apreder reiteradamente de los errores anteriores puede llevarle a generar sobreajuste. Aunque ese enfoque produce predicciones más precisas, muchas veces mejores a la mayoría de algoritmos, puede llevar a ajustar las observaciones atípicas. Por ello, el random forest es una técnica más recomendada para conjuntos de datos muy complejos con un gran número de observaciones atípicas.</p>
<p>Otra de las grandes desventajas del boosting es que tiene unos elevados tiempos de procesamiento dado que su entrenamiento sigue una lógica secuencial. Puesto que un árbol debe esperar al anterior para ser entrenado, se limita la escalabilidad del modelo. Sobre todo cuando se agregan más árboles. Mientras tanto, un random forest entrena los árboles en paralelo, lo que hace que sea más rápido de procesar. El inconveniente que es aplicable tanto a los algoritmos de boosting como a los de bagging y al random forest, es la perdidad de la simplicidad visual y la dificultad de interpretación que tienen respecto a un simple árbol de decisión.</p>
</div>
<div id="gradient-boosting" class="section level2 hasAnchor" number="30.2">
<h2><span class="header-section-number">30.2</span> Gradient Boosting<a href="boosting.-xgboost..html#gradient-boosting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Uno de los algoritmos de boosting más famosos es el gradient boosting. Mientras que el random forest seleccionaba combinaciones aleatorias de variables, el gradient boosting selecciona variables que mejoren la precisión con cada nuevo árbol. Por lo tanto, la construcción del modelo secuencial puesto que cada nuevo árbol se crea utilizando información derivada del árbol anterior, y en consecuencia no son independientes. En cada iteración se registran los errores cometidos en los datos de entrenamiento y se aplican a la siguiente ronda de datos de entrenamiento. Además, se agregan pesos a los datos basandose en los resultados de la iteración anterior. Las ponderaciones más altas se aplicarán a los datos que fueron errorneamente clasificados, y no se aplicará tanta atención a los bien clasificados. Este proceso se repite hasta que se llega a un nivel bajo de error. El resultado final se obtiene a través de la media ponderada de las predicciones de los árboles de decisión.</p>
<div class="figure">
<img src="img/boosting.png" alt="" />
<p class="caption">Ejemplo de Boosting</p>
</div>
<p>Matemáticamente, un algoritmo gradient boosting para clasificación sigue los pasos que a continuación se detallan. Sea un problema de clasificación binaria, y asumiendo que se tienen <span class="math inline">\(K\)</span> árboles de decisión de regresión. Equivalentemente a la regresión logística, la predicción del ensamblado de los árboles de decisión es modelado utilizando una función sigmoidal tal que:</p>
<p><span class="math display">\[\begin{equation}
P(y=1|x,f)=\frac{1}{1+e^{-f(x)}}
\end{equation}\]</span></p>
<p>donde <span class="math inline">\(f(x)=\sum_{\kappa=1}^{K}{f_{\kappa}(x)}\)</span> y <span class="math inline">\(f_m\)</span> es un árbol de regresión. De nuevo, como en la regresión logística, se aplica el principio de máxima verosimilitud tratando de hayar una <span class="math inline">\(f\)</span> que maximice <span class="math inline">\(\mathcal{L}_f = \sum_{i=1}^{N}{\ln(P(y_i=1|x_i,f))}\)</span></p>
<p>El algorítmo en inicio es un modelo constante de la forma <span class="math inline">\(f=f_0=\frac{p}{1-p}\)</span> donde <span class="math inline">\(p=\frac{1}{N}\sum^{N}_{i=1}\)</span>. Tras cada iteración se añade un nuevo arbol <span class="math inline">\(f_\kappa\)</span> al modelo. Para encontrar el mejor árbol <span class="math inline">\(f_\kappa\)</span>, la primera derivada parcial <span class="math inline">\(g_i\)</span> del modelo actual se obtiene para <span class="math inline">\(i=1,\dots,N\)</span>:</p>
<p><span class="math display">\[\begin{equation}
g_i = \frac{\delta\mathcal{L}_f}{\delta f}
\end{equation}\]</span></p>
<p>donde <span class="math inline">\(f\)</span> es el modelo de clasificación ensamblado construido en la iteración previa. Se necesita obtener las derivadas de <span class="math inline">\(\ln(P(y_i=1|x_i,f))\)</span> con respecto a <span class="math inline">\(f\)</span> para todo <span class="math inline">\(i\)</span> para poder calcular <span class="math inline">\(g_i\)</span>. Notese que:</p>
<p><span class="math display">\[\begin{equation}
\ln(P(y_i=1|x_i,f))=\ln(\frac{1}{1+e^{-f(x_i)}})
\end{equation}\]</span></p>
<p>y por tanto, la derivada respecto a <span class="math inline">\(f\)</span> es igual a:</p>
<p><span class="math display">\[\begin{equation}
\frac{\delta \ln(P(y_i=1|x_i,f))}{\delta f} = \frac{1}{e^{f(x_i)}+1}
\end{equation}\]</span></p>
<p>Después, se reemplaza en el conjunto de entrenamiento la categoría original <span class="math inline">\(y_i\)</span> por su correspondiente derivada parcial <span class="math inline">\(g_i\)</span> y se construye un nuevo modelo <span class="math inline">\(f_\kappa\)</span> utilizando el conjunto de entrenamiento transformado. Tras esto, se obtiene el paso de actualización óptimo <span class="math inline">\(\rho_\kappa\)</span> como:</p>
<p><span class="math display">\[\begin{equation}
\rho_\kappa = \arg \max\limits_{\rho}{\mathcal{L}_{f+\rho f_\kappa}}
\end{equation}\]</span></p>
<p>Al terminar la iteración <span class="math inline">\(\kappa\)</span>, se actualiza el modelo ensamblado <span class="math inline">\(f\)</span> añadiendo el nuevo arbol <span class="math inline">\(f_\kappa\)</span>:</p>
<p><span class="math display">\[\begin{equation}
f\leftarrow f+\alpha\rho_\kappa f_\kappa
\end{equation}\]</span></p>
<p>Se itera hasta que <span class="math inline">\(\kappa=K\)</span>, entonces el proceso se detiene y se obtiene el modelo ensamblado final <span class="math inline">\(f\)</span>.</p>
<div id="hiperparámetros-del-modelo-gradient-boosting" class="section level3 hasAnchor" number="30.2.1">
<h3><span class="header-section-number">30.2.1</span> Hiperparámetros del modelo gradient boosting<a href="boosting.-xgboost..html#hiperparámetros-del-modelo-gradient-boosting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
Un modelo de gradient boosting tiene dos tipos de hiperparámetros:
<div id="hiperparámetros-de-boosting" class="section level4 hasAnchor" number="30.2.1.1">
<h4><span class="header-section-number">30.2.1.1</span> Hiperparámetros de boosting<a href="boosting.-xgboost..html#hiperparámetros-de-boosting" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Los hiperparametos de boosting son principalmente dos: el número de árboles y la tasa de aprendizaje. El primero indica la cantidad de árboles que crecen de forma secuencial, y que es importante optimizar para evitar el sobreajuste del modelo. A diferencia de los modelos random forest o bagging, en el boosting los árboles crecen en secuencia para que cada árbol corrija los errores del anterior. Además, el número de árboles necesarios por el GBM puede incrementarse debido a los valores que tomen otros hiperparámetros.</p>
<p>La tasa de aprendizaje es el hiperparámetro con el que se determina la contribución de cada árbol en el resultado final y controla la rapidez con la que el algoritmo avanza por el descenso del gradiente, es decir, la velocidad a la que aprende. Este hiperparámetro toma valores entre 0 y 1, aunque los valores habituales oscilan entre 0,001 y 0,3. El modelo es más robusto a las características especificas de cada árbol, permitiendo una buena generalización, cuando la tasa de aprendizaje toma valores bajos. Estos valores también facilitan la parada temprana antes del sobreajuste del modelo. Sin embargo, utilizar estos valores vuelve al modelo más exigente computacionalmente hablando y dificulta alcanzar el óptimo con un número fijo de árboles. En resumen, cuanto menor sea este valor, más preciso puede ser el modelo, pero también requerirá más árboles en la secuencia.</p>
</div>
<div id="hiperparámetros-de-árbol" class="section level4 hasAnchor" number="30.2.1.2">
<h4><span class="header-section-number">30.2.1.2</span> Hiperparámetros de árbol<a href="boosting.-xgboost..html#hiperparámetros-de-árbol" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Los principales hiperparametos de árbol son: la profundidad del árbol y el número mínimo de observaciones en nodos terminales. El primer hiperparámetro controla la profundidad de los árboles individuales. Los valores habituales de profundidad oscilan entre 3 y 8. Los árboles de menor profundidad son eficientes computacionalmente hablando, pero necesitan más árboles. Sin embargo, los árboles de mayor profundidad permiten que el algoritmo capture interacciones únicas, aunque a su vez aumentan el riesgo de sobreajuste. El segundo hiperparámetro, además de controlar el número mínimo de observaciones en nodos terminales, controla la complejidad de cada árbol. Los valores típicos de este hiperparámetro suelen estar entre 5 y 15. Los valores más altos ayudan a evitar que un modelo aprenda relaciones que pueden ser muy específicas de la muestra particular seleccionada para un árbol, evitando así el sobreajuste. Sin embargo, los valores más pequeños pueden ayudar con clases objetivo desbalanceadas en problemas de clasificación.</p>
</div>
</div>
<div id="estrategia-de-ajuste-de-hiperparametros" class="section level3 hasAnchor" number="30.2.2">
<h3><span class="header-section-number">30.2.2</span> Estrategia de ajuste de hiperparametros<a href="boosting.-xgboost..html#estrategia-de-ajuste-de-hiperparametros" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A diferencia del random forest, los modelos gradient boosting pueden variar mucho en su precisión de acuerdo a su configuración de hiperparametros. Por ello, el ajuste puede requerir seguir una estrategia. Un buen enfoque para esto es:</p>
</div>
</div>
<div id="procedimiento-con-r-la-funcion-gbm" class="section level2 hasAnchor" number="30.3">
<h2><span class="header-section-number">30.3</span> Procedimiento con R: la funcion <code>gbm</code><a href="boosting.-xgboost..html#procedimiento-con-r-la-funcion-gbm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>En el paquete <code>gbm</code> de <strong>R</strong> se encuentra la función <code>gbm</code> que se utiliza para entrenar un modelo gradient boosting:</p>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb368-1"><a href="boosting.-xgboost..html#cb368-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">gbm</span>(formula, <span class="at">data =</span> ..., ...)</span></code></pre></div>
</div>
<div id="aplicación-del-modelo-gbm-en-r" class="section level2 hasAnchor" number="30.4">
<h2><span class="header-section-number">30.4</span> Aplicación del modelo GBM en R<a href="boosting.-xgboost..html#aplicación-del-modelo-gbm-en-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A través de los datos de compras <code>dp_entr</code> incluidos en el paquete <code>CDR</code> se va a aplicar el modelo gradient boosting para clasificar qué clientes van a comprar un nuevo producto (<em>tensiómetro digital</em>) y cuales no. Se contruye el modelo usando el conjunto de entrenamiento sin transformar (en su escala orginal). Así, en lugar de tener las variables categóricas transformadas a one-hot-encoding se guardan en su variable original, como ocurre con el caso de la variable que mide el nivel educativo.</p>
<div class="sourceCode" id="cb369"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb369-1"><a href="boosting.-xgboost..html#cb369-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">library</span>(CDR)</span>
<span id="cb369-2"><a href="boosting.-xgboost..html#cb369-2" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">data</span>(dp_entr)</span></code></pre></div>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb370-1"><a href="boosting.-xgboost..html#cb370-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># se fija una semilla común a todos los modelos</span></span>
<span id="cb370-2"><a href="boosting.-xgboost..html#cb370-2" aria-hidden="true" tabindex="-1"></a><span class="er">&gt;</span> <span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb370-3"><a href="boosting.-xgboost..html#cb370-3" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> </span>
<span id="cb370-4"><a href="boosting.-xgboost..html#cb370-4" aria-hidden="true" tabindex="-1"></a><span class="er">&gt;</span> <span class="co"># se entrena el modelo</span></span>
<span id="cb370-5"><a href="boosting.-xgboost..html#cb370-5" aria-hidden="true" tabindex="-1"></a><span class="er">&gt;</span> model <span class="ot">&lt;-</span> <span class="fu">train</span>(CLS_PRO_pro13 <span class="sc">~</span> .,</span>
<span id="cb370-6"><a href="boosting.-xgboost..html#cb370-6" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   <span class="at">data =</span> dp_entr,</span>
<span id="cb370-7"><a href="boosting.-xgboost..html#cb370-7" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   <span class="at">method =</span> <span class="st">&quot;gbm&quot;</span>,</span>
<span id="cb370-8"><a href="boosting.-xgboost..html#cb370-8" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   <span class="at">metric =</span> <span class="st">&quot;Accuracy&quot;</span>,</span>
<span id="cb370-9"><a href="boosting.-xgboost..html#cb370-9" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   <span class="at">trControl =</span> <span class="fu">trainControl</span>(</span>
<span id="cb370-10"><a href="boosting.-xgboost..html#cb370-10" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>     <span class="at">classProbs =</span> <span class="cn">TRUE</span>,</span>
<span id="cb370-11"><a href="boosting.-xgboost..html#cb370-11" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>     <span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span></span>
<span id="cb370-12"><a href="boosting.-xgboost..html#cb370-12" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   )</span>
<span id="cb370-13"><a href="boosting.-xgboost..html#cb370-13" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> )</span></code></pre></div>
<pre><code>Stochastic Gradient Boosting 

558 samples
 17 predictor
  2 classes: &#39;S&#39;, &#39;N&#39; 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 502, 502, 502, 503, 503, 502, ... 
Resampling results across tuning parameters:

  interaction.depth  n.trees  Accuracy   Kappa    
  1                   50      0.8564610  0.7130031
  1                  100      0.8690909  0.7383556
  1                  150      0.8762338  0.7526413
  2                   50      0.8690909  0.7382344
  2                  100      0.8762338  0.7526413
  2                  150      0.8799026  0.7599227
  3                   50      0.8763636  0.7528004
  3                  100      0.8781494  0.7563575
  3                  150      0.8835390  0.7671499

Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1

Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10
Accuracy was used to select the optimal model using the largest value.
The final values used for the model were n.trees = 150, interaction.depth =
 3, shrinkage = 0.1 and n.minobsinnode = 10.</code></pre>
<p>El modelo resultante del proceso de entrenamiento es un gradient boosting con 100 árboles, una profundidad de 3, ambos parametros han sido ajustados. Además, se habia dejado constantes tanto el número mínimo de observaciones en nodos igual a 10 y una tasa de aprendizaje de 0,1. Los resultados en el proceso de validación cruzada se muestra en la figura <span class="math inline">\(\ref{GBMBOXPLOT}\)</span>, en el que se observa como la precisión oscila entre el 84% y el 93% en las iteraciones.</p>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb372-1"><a href="boosting.-xgboost..html#cb372-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">ggplot</span>(<span class="fu">melt</span>(model<span class="sc">$</span>resample[, <span class="sc">-</span><span class="dv">4</span>]), <span class="fu">aes</span>(<span class="at">x =</span> variable, <span class="at">y =</span> value, <span class="at">fill =</span> variable)) <span class="sc">+</span></span>
<span id="cb372-2"><a href="boosting.-xgboost..html#cb372-2" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">geom_boxplot</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:GBMBOXPLOT"></span>
<img src="Ciencia_de_datos_con_r_files/figure-html/GBMBOXPLOT-1.svg" alt="Resultados del modelo GB obtenidos durante el proceso de validación cruzada." width="60%" />
<p class="caption">
Figura 30.1: Resultados del modelo GB obtenidos durante el proceso de validación cruzada.
</p>
</div>
<div id="gbm-con-ajuste-automático" class="section level3 hasAnchor" number="30.4.1">
<h3><span class="header-section-number">30.4.1</span> GBM con ajuste automático<a href="boosting.-xgboost..html#gbm-con-ajuste-automático" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Se repite el procedimiento para el ejemplo anterior. Sin embargo, en este ejemplo se va a proceder a ajustar de forma automática los hiperparámetros más relevantes de dicho algoritmo para mejorar los resultados respecto al modelo anterior. Se observa como los parámetros a ajustar para el método <code>gbm</code> son: el número de árboles, la profundidad, la tasa de aprendizaje y el número de observaciones en un nodo.</p>
<div class="sourceCode" id="cb373"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb373-1"><a href="boosting.-xgboost..html#cb373-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">modelLookup</span>(<span class="st">&quot;gbm&quot;</span>)</span>
<span id="cb373-2"><a href="boosting.-xgboost..html#cb373-2" aria-hidden="true" tabindex="-1"></a>  model         parameter                   label forReg forClass probModel</span>
<span id="cb373-3"><a href="boosting.-xgboost..html#cb373-3" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span>   gbm           n.trees   <span class="co"># Boosting Iterations   TRUE     TRUE      TRUE</span></span>
<span id="cb373-4"><a href="boosting.-xgboost..html#cb373-4" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span>   gbm interaction.depth          Max Tree Depth   <span class="cn">TRUE</span>     <span class="cn">TRUE</span>      <span class="cn">TRUE</span></span>
<span id="cb373-5"><a href="boosting.-xgboost..html#cb373-5" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span>   gbm         shrinkage               Shrinkage   <span class="cn">TRUE</span>     <span class="cn">TRUE</span>      <span class="cn">TRUE</span></span>
<span id="cb373-6"><a href="boosting.-xgboost..html#cb373-6" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span>   gbm    n.minobsinnode Min. Terminal Node Size   <span class="cn">TRUE</span>     <span class="cn">TRUE</span>      <span class="cn">TRUE</span></span></code></pre></div>
<p>Siguiendo la estrategia descrita definimos rangos de valores para los principales hiperparámetros a optimizar.</p>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb374-1"><a href="boosting.-xgboost..html#cb374-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Se especifica un rango de valores típicos para los hiperparámetros</span></span>
<span id="cb374-2"><a href="boosting.-xgboost..html#cb374-2" aria-hidden="true" tabindex="-1"></a><span class="er">&gt;</span> tuneGrid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(</span>
<span id="cb374-3"><a href="boosting.-xgboost..html#cb374-3" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>   <span class="at">interaction.depth =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">8</span>),</span>
<span id="cb374-4"><a href="boosting.-xgboost..html#cb374-4" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>   <span class="at">n.trees =</span> <span class="fu">c</span>(<span class="dv">10</span> <span class="sc">*</span> <span class="fu">ncol</span>(dp_entr), <span class="dv">300</span>, <span class="dv">500</span>),</span>
<span id="cb374-5"><a href="boosting.-xgboost..html#cb374-5" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>   <span class="at">shrinkage =</span> <span class="fu">c</span>(<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>),</span>
<span id="cb374-6"><a href="boosting.-xgboost..html#cb374-6" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>   <span class="at">n.minobsinnode =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>)</span>
<span id="cb374-7"><a href="boosting.-xgboost..html#cb374-7" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span> )</span></code></pre></div>
<p>Esta red de posibles valores para los parámetros del modelo se incorporan a la función de entrenamiento. Cuanto más exhaustiva sea la busqueda de estos valores, mayor será el tiempo de ajuste del modelo. La red presentada está formada por 81 combinaciones de los posibles cuatro hiperparametros.</p>
<div class="sourceCode" id="cb375"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb375-1"><a href="boosting.-xgboost..html#cb375-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># se fija una semilla común a todos los modelos</span></span>
<span id="cb375-2"><a href="boosting.-xgboost..html#cb375-2" aria-hidden="true" tabindex="-1"></a><span class="er">&gt;</span> <span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb375-3"><a href="boosting.-xgboost..html#cb375-3" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> </span>
<span id="cb375-4"><a href="boosting.-xgboost..html#cb375-4" aria-hidden="true" tabindex="-1"></a><span class="er">&gt;</span> </span>
<span id="cb375-5"><a href="boosting.-xgboost..html#cb375-5" aria-hidden="true" tabindex="-1"></a><span class="er">&gt;</span> <span class="co"># se entrena el modelo</span></span>
<span id="cb375-6"><a href="boosting.-xgboost..html#cb375-6" aria-hidden="true" tabindex="-1"></a><span class="er">&gt;</span> model <span class="ot">&lt;-</span> <span class="fu">train</span>(CLS_PRO_pro13 <span class="sc">~</span> .,</span>
<span id="cb375-7"><a href="boosting.-xgboost..html#cb375-7" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   <span class="at">data =</span> dp_entr,</span>
<span id="cb375-8"><a href="boosting.-xgboost..html#cb375-8" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   <span class="at">method =</span> <span class="st">&quot;gbm&quot;</span>,</span>
<span id="cb375-9"><a href="boosting.-xgboost..html#cb375-9" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   <span class="at">metric =</span> <span class="st">&quot;Accuracy&quot;</span>,</span>
<span id="cb375-10"><a href="boosting.-xgboost..html#cb375-10" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   <span class="at">trControl =</span> <span class="fu">trainControl</span>(</span>
<span id="cb375-11"><a href="boosting.-xgboost..html#cb375-11" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>     <span class="at">classProbs =</span> <span class="cn">TRUE</span>,</span>
<span id="cb375-12"><a href="boosting.-xgboost..html#cb375-12" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>     <span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span></span>
<span id="cb375-13"><a href="boosting.-xgboost..html#cb375-13" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   ),</span>
<span id="cb375-14"><a href="boosting.-xgboost..html#cb375-14" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   <span class="at">tuneGrid =</span> tuneGrid</span>
<span id="cb375-15"><a href="boosting.-xgboost..html#cb375-15" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> )</span></code></pre></div>
<p>El modelo que mejores resultados proporciona es aquel que ajusta los hiperparametros a los siguientes valores: está compuesto de 180 árboles, con una profundidad igual a 6, una tasa de aprendizaje de 0.05 y un tamaño mínimo de los nodos de 10 observaciones.</p>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb376-1"><a href="boosting.-xgboost..html#cb376-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> model<span class="sc">$</span>bestTune</span>
<span id="cb376-2"><a href="boosting.-xgboost..html#cb376-2" aria-hidden="true" tabindex="-1"></a>   n.trees interaction.depth shrinkage n.minobsinnode</span>
<span id="cb376-3"><a href="boosting.-xgboost..html#cb376-3" aria-hidden="true" tabindex="-1"></a><span class="dv">13</span>     <span class="dv">180</span>                 <span class="dv">6</span>      <span class="fl">0.05</span>             <span class="dv">10</span></span></code></pre></div>
<p>En el la figura <span class="math inline">\(\ref{modelgbmBOXPLOT}\)</span> se ven los resultados obtenidos durante el proceso de validación cruzada. Se puede ver que los resultados son similares a los del modelo anterior, aunque hay puntos diferenciables importantes. En primer lugar, se alcanza un valor máximo de precisión mayor al anterior, pues en este caso la precisión oscila entre el 84% y el 95%. En segundo lugar, vemos que el valor mediano de la precisión ha subido del 87.5% del modelo anterior hasta el 90% de este modelo. Por último, que el rendimiento haya variado tan poco desde el modelo por defecto a un modelo en el que se ha intentado ajustar los hiperparámetros, confirma lo ya expuesto sobre el buen rendimiento de un modelo de gradient boosting con los parámetros por defecto.</p>
<div class="sourceCode" id="cb377"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb377-1"><a href="boosting.-xgboost..html#cb377-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">ggplot</span>(<span class="fu">melt</span>(model<span class="sc">$</span>resample[, <span class="sc">-</span><span class="dv">4</span>]), <span class="fu">aes</span>(<span class="at">x =</span> variable, <span class="at">y =</span> value, <span class="at">fill =</span> variable)) <span class="sc">+</span></span>
<span id="cb377-2"><a href="boosting.-xgboost..html#cb377-2" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">geom_boxplot</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:modelgbmBOXPLOT"></span>
<img src="Ciencia_de_datos_con_r_files/figure-html/modelgbmBOXPLOT-1.svg" alt="Resultados del modelo GB con ajuste autómatico obtenidos durante el proceso de validación cruzada." width="60%" />
<p class="caption">
Figura 30.2: Resultados del modelo GB con ajuste autómatico obtenidos durante el proceso de validación cruzada.
</p>
</div>
</div>
</div>
<div id="extreme-gradient-boosting-xgb" class="section level2 hasAnchor" number="30.5">
<h2><span class="header-section-number">30.5</span> eXtreme Gradient Boosting (XGB)<a href="boosting.-xgboost..html#extreme-gradient-boosting-xgb" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>El eXtreme Gradient Boosting es una implementación eficiente y escalable del modelo gradient boosting. Este modelo, abreviado como XGBoost, es un paquete de código abierto en C++, Java, Python (), R, Julia, Perl y Scala. En R el modelo se incluye dentro del paquete <code>xgboost</code> (). El paquete incluye un solucionador eficiente de modelos lineales y un algoritmo de aprendizaje de árboles.</p>
<p>El paquete es compatible con funciones objetivo de regresión, clasificación y ranking. El paquete tiene varias características:</p>
<ol style="list-style-type: decimal">
<li><p>Velocidad: xgboost puede realizar automáticamente cálculos paralelos. Por lo general, es más de 10 veces más rápido que el modelo Gradient Boosting.</p></li>
<li><p>Tipo de entrada: xgboost toma varios tipos de datos de entrada:</p></li>
</ol>
<ul>
<li>Matriz densa (matrix)</li>
<li>Matriz dispersa (Matrix::dgCMatrix)</li>
<li>Archivo de datos locales</li>
<li>Un tipo de datos propio del paquete: xgb.DMatrix</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li><p>Dispersión: xgboost acepta datos de entrada dispersos para los modelos incluidos.</p></li>
<li><p>Personalización: xgboost admite tanto funcion de objetivo y función de evaluación personalizadas.</p></li>
<li><p>Rendimiento: xgboost tiene un mejor rendimiento en varios conjuntos de datos.</p></li>
</ol>
<div id="hiperparametros-del-modelo-xgboost" class="section level3 hasAnchor" number="30.5.1">
<h3><span class="header-section-number">30.5.1</span> Hiperparametros del modelo XGBoost<a href="boosting.-xgboost..html#hiperparametros-del-modelo-xgboost" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>El modelo XGBoost proporciona los hiperparámetros que ya incluía el modelo gradient boosting referentes tanto al boosting como a los árboles. Sin embargo, xgboost también proporciona hiperparámetros adicionales que pueden ayudar a reducir las posibilidades de sobreajuste, lo que lleva a una menor variabilidad de predicción y, por lo tanto, a una mayor precisión. Estos parametros son: la regularización y el dropout.</p>
<p>Los parámetros de regularización se incluyen para ayudar a evitar el sobreajuste y reducir la complejidad del modelo. Existen tres hiperparámetros que tienen esta funcionalidad: gamma (<span class="math inline">\(\gamma\)</span>), alpha (<span class="math inline">\(\alpha\)</span>) y lambda (<span class="math inline">\(\lambda\)</span>). Gamma es un hiperparámetro de pseudo-regularización conocido como multiplicador Lagrangiano y controla la complejidad de un árbol dado. Este hiperparámetro establece que para hacer una partición adicional en un nodo es necesaria una reducción de pérdida mínima especificada por gamma. Al especificarlo, el modelo XGBoost hace crecer los árboles hasta una profundidad máxima establecida, pero en un paso de poda eliminará las divisiones que no cumplan con la regularización <span class="math inline">\(\gamma\)</span>. Este hiperparámetro toma valores entre 0 e infinito (<span class="math inline">\(\infty\)</span>), siguiendo la regla de que a mayor valor mayor será la regularización. Los otros hiperparámetros de regularización, <span class="math inline">\(\alpha\)</span> y <span class="math inline">\(\lambda\)</span>, son más clásicos. Mientras que <span class="math inline">\(\alpha\)</span> proprociona una regularización <span class="math inline">\(L_1\)</span>, <span class="math inline">\(\lambda\)</span> proprociona una regularización <span class="math inline">\(L_2\)</span>. Estos parámetros de regularización establecen un límite a cómo de extremos pueden llegar a ser los pesos de los nodos en un árbol. Sus valores se encuentran, al igual que los de <span class="math inline">\(\gamma\)</span>, entre 0 y <span class="math inline">\(\infty\)</span>.</p>
<p>El dropout es un enfoque alternativo para reducir el sobreajuste. Cuando se entrena un modelo de gradient boosting, los primeros árboles tienden a dominar el rendimiento del modelo mientras que los que se agregan después suelen mejorar la predicción solo para un pequeño grupo de variables. Esto puede llevar a que se incremente el riesgo de sobreajuste. Con el dropout, se descartan árboles aleatoriamente en el proceso de entrenamiento.</p>
<p>En su implementación en R, el modelo XGBoost incluye principalmente los siguientes parámetros para ser optimizados: número de iteraciones, profundidad máxima de los árboles, tasa de aprendizaje y la regularización <span class="math inline">\(\gamma\)</span>.</p>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb378-1"><a href="boosting.-xgboost..html#cb378-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">head</span>(<span class="fu">modelLookup</span>(<span class="st">&quot;xgbTree&quot;</span>), <span class="dv">4</span>)</span>
<span id="cb378-2"><a href="boosting.-xgboost..html#cb378-2" aria-hidden="true" tabindex="-1"></a>    model parameter                  label forReg forClass probModel</span>
<span id="cb378-3"><a href="boosting.-xgboost..html#cb378-3" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> xgbTree   nrounds  <span class="co"># Boosting Iterations   TRUE     TRUE      TRUE</span></span>
<span id="cb378-4"><a href="boosting.-xgboost..html#cb378-4" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span> xgbTree max_depth         Max Tree Depth   <span class="cn">TRUE</span>     <span class="cn">TRUE</span>      <span class="cn">TRUE</span></span>
<span id="cb378-5"><a href="boosting.-xgboost..html#cb378-5" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span> xgbTree       eta              Shrinkage   <span class="cn">TRUE</span>     <span class="cn">TRUE</span>      <span class="cn">TRUE</span></span>
<span id="cb378-6"><a href="boosting.-xgboost..html#cb378-6" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span> xgbTree     gamma Minimum Loss Reduction   <span class="cn">TRUE</span>     <span class="cn">TRUE</span>      <span class="cn">TRUE</span></span></code></pre></div>
</div>
</div>
<div id="procedimiento-con-r-la-funcion-xgboost" class="section level2 hasAnchor" number="30.6">
<h2><span class="header-section-number">30.6</span> Procedimiento con R: la funcion <code>xgboost</code><a href="boosting.-xgboost..html#procedimiento-con-r-la-funcion-xgboost" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>En el paquete <code>xgboost</code> de <strong>R</strong> se encuentra la función <code>xgboost</code> que se utiliza para entrenar un modelo extreme gradient boosting:</p>
<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb379-1"><a href="boosting.-xgboost..html#cb379-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">xgboost</span>(<span class="at">data =</span> ..., <span class="at">label =</span> ..., ...)</span></code></pre></div>
</div>
<div id="aplicación-del-módelo-xgboost-en-r" class="section level2 hasAnchor" number="30.7">
<h2><span class="header-section-number">30.7</span> Aplicación del módelo XGBoost en R<a href="boosting.-xgboost..html#aplicación-del-módelo-xgboost-en-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Este algoritmo vuelve a estar basado en árboles, así se contruye el modelo usando el conjunto de entrenamiento sin transformar (en su escala orginal). Se continua así el ejemplo expuesto durante la aplicación del modelo gradient boosting sin y con ajuste automático de sus hiperparámetros. Se repite el procedimiento de entrenar el modelo para los parámetros por defecto que proporciona R.</p>
<div class="sourceCode" id="cb380"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb380-1"><a href="boosting.-xgboost..html#cb380-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># se fija una semilla común a todos los modelos</span></span>
<span id="cb380-2"><a href="boosting.-xgboost..html#cb380-2" aria-hidden="true" tabindex="-1"></a><span class="er">&gt;</span> <span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb380-3"><a href="boosting.-xgboost..html#cb380-3" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> </span>
<span id="cb380-4"><a href="boosting.-xgboost..html#cb380-4" aria-hidden="true" tabindex="-1"></a><span class="er">&gt;</span> <span class="co"># se entrena el modelo</span></span>
<span id="cb380-5"><a href="boosting.-xgboost..html#cb380-5" aria-hidden="true" tabindex="-1"></a><span class="er">&gt;</span> model <span class="ot">&lt;-</span> <span class="fu">train</span>(CLS_PRO_pro13 <span class="sc">~</span> .,</span>
<span id="cb380-6"><a href="boosting.-xgboost..html#cb380-6" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   <span class="at">data =</span> dp_entr,</span>
<span id="cb380-7"><a href="boosting.-xgboost..html#cb380-7" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   <span class="at">method =</span> <span class="st">&quot;xgbTree&quot;</span>,</span>
<span id="cb380-8"><a href="boosting.-xgboost..html#cb380-8" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   <span class="at">metric =</span> <span class="st">&quot;Accuracy&quot;</span>,</span>
<span id="cb380-9"><a href="boosting.-xgboost..html#cb380-9" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   <span class="at">trControl =</span> <span class="fu">trainControl</span>(</span>
<span id="cb380-10"><a href="boosting.-xgboost..html#cb380-10" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>     <span class="at">classProbs =</span> <span class="cn">TRUE</span>,</span>
<span id="cb380-11"><a href="boosting.-xgboost..html#cb380-11" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>     <span class="at">method =</span> <span class="st">&quot;cv&quot;</span>,</span>
<span id="cb380-12"><a href="boosting.-xgboost..html#cb380-12" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>     <span class="at">number =</span> <span class="dv">10</span></span>
<span id="cb380-13"><a href="boosting.-xgboost..html#cb380-13" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   )</span>
<span id="cb380-14"><a href="boosting.-xgboost..html#cb380-14" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> )</span></code></pre></div>
<p>Por defecto, el entrenamiento establece valores constantes para la regularización <span class="math inline">\(\gamma\)</span> (igual a 0) y para el tamaño mínimo del nodo (igual a 1). En cambio, ajusta los parametros del modelo dentro de los valores por defecto de la función. Así, el modelo XGBoost resultante ha sido uno con 50 iteraciónes, una profundidad máxima igual a 2 y una tasa de aprendizaje de 0,3. Los resultados de la validación cruzada muestran que la precisión obtenida oscila entre el 85% y el 95%, resultado similar al del gradient boosting con parametros ajustados. Sin embargo, el valor mediano de la precisión es del 88%, ligeramente inferior a la observada en el modelo gradient boosting con ajuste automático.</p>
<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb381-1"><a href="boosting.-xgboost..html#cb381-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># resultados del modelo por cada iteración del cv</span></span>
<span id="cb381-2"><a href="boosting.-xgboost..html#cb381-2" aria-hidden="true" tabindex="-1"></a><span class="er">&gt;</span> <span class="fu">ggplot</span>(<span class="fu">melt</span>(model<span class="sc">$</span>resample[, <span class="sc">-</span><span class="dv">4</span>]), <span class="fu">aes</span>(<span class="at">x =</span> variable, <span class="at">y =</span> value, <span class="at">fill =</span> variable)) <span class="sc">+</span></span>
<span id="cb381-3"><a href="boosting.-xgboost..html#cb381-3" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">geom_boxplot</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:XGBRESULTS"></span>
<img src="Ciencia_de_datos_con_r_files/figure-html/XGBRESULTS-1.svg" alt="Resultados del modelo XGBoost obtenidos durante el proceso de validación cruzada." width="60%" />
<p class="caption">
Figura 30.3: Resultados del modelo XGBoost obtenidos durante el proceso de validación cruzada.
</p>
</div>
<div id="xgboost-y-ajuste-automático" class="section level3 hasAnchor" number="30.7.1">
<h3><span class="header-section-number">30.7.1</span> XGBoost y ajuste automático<a href="boosting.-xgboost..html#xgboost-y-ajuste-automático" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Se continua el ejemplo de aplicar los datos sobre compra de un nuevo producto por parte de los clientes utilizando un modelo extreme gradient boosting en R. Sin embargo, se quieren mejorar los resultados obtenidos, y por ello se va a proceder a ajustar de forma automática los parámetros más relevantes de dicho algoritmo. Se genera una red de posibles valores para dichos parametros. Por motivos computacionales, esta no se hace muy exhaustiva para evitar largos tiempos de entrenamiento, pero es aconsejable tratar de estudiar más valores para los parámetros que interese optimizar.</p>
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb382-1"><a href="boosting.-xgboost..html#cb382-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Se especifica un rango de valores típicos para los hiperparámetros</span></span>
<span id="cb382-2"><a href="boosting.-xgboost..html#cb382-2" aria-hidden="true" tabindex="-1"></a><span class="er">&gt;</span> tuneGrid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(</span>
<span id="cb382-3"><a href="boosting.-xgboost..html#cb382-3" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>   <span class="at">nrounds =</span> <span class="fu">c</span>(<span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>),</span>
<span id="cb382-4"><a href="boosting.-xgboost..html#cb382-4" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>   <span class="at">max_depth =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">8</span>),</span>
<span id="cb382-5"><a href="boosting.-xgboost..html#cb382-5" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>   <span class="at">eta =</span> <span class="fu">c</span>(<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>),</span>
<span id="cb382-6"><a href="boosting.-xgboost..html#cb382-6" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>   <span class="at">gamma =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="dv">5</span>),</span>
<span id="cb382-7"><a href="boosting.-xgboost..html#cb382-7" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>   <span class="at">colsample_bytree =</span> <span class="fu">c</span>(<span class="fl">0.8</span>),</span>
<span id="cb382-8"><a href="boosting.-xgboost..html#cb382-8" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>   <span class="at">min_child_weight =</span> <span class="fu">c</span>(<span class="dv">5</span>),</span>
<span id="cb382-9"><a href="boosting.-xgboost..html#cb382-9" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>   <span class="at">subsample =</span> <span class="fu">c</span>(<span class="fl">0.5</span>)</span>
<span id="cb382-10"><a href="boosting.-xgboost..html#cb382-10" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span> )</span></code></pre></div>
<div class="sourceCode" id="cb383"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb383-1"><a href="boosting.-xgboost..html#cb383-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># se fija una semilla común a todos los modelos</span></span>
<span id="cb383-2"><a href="boosting.-xgboost..html#cb383-2" aria-hidden="true" tabindex="-1"></a><span class="er">&gt;</span> <span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb383-3"><a href="boosting.-xgboost..html#cb383-3" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> </span>
<span id="cb383-4"><a href="boosting.-xgboost..html#cb383-4" aria-hidden="true" tabindex="-1"></a><span class="er">&gt;</span> </span>
<span id="cb383-5"><a href="boosting.-xgboost..html#cb383-5" aria-hidden="true" tabindex="-1"></a><span class="er">&gt;</span> <span class="co"># se entrena el modelo</span></span>
<span id="cb383-6"><a href="boosting.-xgboost..html#cb383-6" aria-hidden="true" tabindex="-1"></a><span class="er">&gt;</span> model <span class="ot">&lt;-</span> <span class="fu">train</span>(CLS_PRO_pro13 <span class="sc">~</span> .,</span>
<span id="cb383-7"><a href="boosting.-xgboost..html#cb383-7" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   <span class="at">data =</span> dp_entr,</span>
<span id="cb383-8"><a href="boosting.-xgboost..html#cb383-8" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   <span class="at">method =</span> <span class="st">&quot;xgbTree&quot;</span>,</span>
<span id="cb383-9"><a href="boosting.-xgboost..html#cb383-9" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   <span class="at">metric =</span> <span class="st">&quot;Accuracy&quot;</span>,</span>
<span id="cb383-10"><a href="boosting.-xgboost..html#cb383-10" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   <span class="at">trControl =</span> <span class="fu">trainControl</span>(</span>
<span id="cb383-11"><a href="boosting.-xgboost..html#cb383-11" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>     <span class="at">classProbs =</span> <span class="cn">TRUE</span>,</span>
<span id="cb383-12"><a href="boosting.-xgboost..html#cb383-12" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>     <span class="at">method =</span> <span class="st">&quot;cv&quot;</span>,</span>
<span id="cb383-13"><a href="boosting.-xgboost..html#cb383-13" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>     <span class="at">number =</span> <span class="dv">10</span></span>
<span id="cb383-14"><a href="boosting.-xgboost..html#cb383-14" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   ),</span>
<span id="cb383-15"><a href="boosting.-xgboost..html#cb383-15" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span>   <span class="at">tuneGrid =</span> tuneGrid</span>
<span id="cb383-16"><a href="boosting.-xgboost..html#cb383-16" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> )</span></code></pre></div>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb384-1"><a href="boosting.-xgboost..html#cb384-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> model<span class="sc">$</span>bestTune[, <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>]</span>
<span id="cb384-2"><a href="boosting.-xgboost..html#cb384-2" aria-hidden="true" tabindex="-1"></a>   nrounds max_depth eta gamma</span>
<span id="cb384-3"><a href="boosting.-xgboost..html#cb384-3" aria-hidden="true" tabindex="-1"></a><span class="dv">71</span>     <span class="dv">100</span>         <span class="dv">4</span> <span class="fl">0.2</span>     <span class="dv">5</span></span></code></pre></div>
<p>El modelo resultante establece que se utilicen 100 iteraciones, que los árboles tengan una profundidad máxima de 4, que la tasa de aprendizaje sea del 0,2 y que la regularización <span class="math inline">\(\gamma\)</span> tome el valor 5.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:xgb-tuned-RESULTS"></span>
<img src="Ciencia_de_datos_con_r_files/figure-html/xgb-tuned-RESULTS-1.svg" alt="Resultados del modelo XGBoost con ajuste autómatico obtenidos durante el proceso de validación cruzada." width="60%" />
<p class="caption">
Figura 30.4: Resultados del modelo XGBoost con ajuste autómatico obtenidos durante el proceso de validación cruzada.
</p>
</div>
<p>Los resultados guardados durante la validación cruzada muestran que la precisión es muy similar a la del modelo por defecto, al encontrarse entre el 85% y el 95%. Sin embargo, se observa en el valor mediano de la precisión una ligera mejoría, al aumentar hasta el 90%.</p>
</div>
<div id="resumen-10" class="section level3 unnumbered hasAnchor infobox_resume">
<h3>Resumen<a href="boosting.-xgboost..html#resumen-10" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>En este capítulo se ha introducido al lector en el algoritmo de aprendizaje supervisado conocido como gradient boosting, en concreto:</p>
<ul>
<li>Se ha presentado el otro paradigma principal de aprendizaje ensamblado: el boosting.</li>
<li>Se explica el modelo basado en este paradigma, el gradient boosting, así como sus diferencias con el random forest (basado en bagging).</li>
<li>Se han expuesto los hiperparámetros más relevantes a la hora de optimizar un modelo de gradient boosting.</li>
<li>Se ha presentado el extreme gradient boosting, una implementación eficiente y escalable del modelo gradient boosting. Así como los hiperparámetros de regularización y otros parametros importantes en esta implementación.</li>
<li>Se han aplicado ambos algoritmos en <code>R</code> en un caso práctico para la clasificación binaria de datos.</li>
</ul>
</div>

<div id="refs" class="references csl-bib-body hanging-indent">
<div class="csl-entry">
Almudevar, Anthony. 2021. <em>Theory of Statistical Inference</em>. Texts in Statistical Science. Chapman &amp; Hall/CRC.
</div>
<div class="csl-entry">
Arnab, Raghunath. 2017. <em>Survey Sampling Theory and Applications</em>. Elsevier.
</div>
<div class="csl-entry">
Balakrishnan, N., Markos V. Koutras, and Konstadinos G. Politis. 2019. <em>Introduction to Probability: Models and Applications</em>. Wiley Series in Probability and Statistics. John Wiley &amp; Sons.
</div>
<div class="csl-entry">
Beh, Eric J., and Rosaria Lombardo. 2014. <em>Correspondence Analysis: Theory, Practice and New Strategies</em>. Wiley Series in Probability and Statistics. Wiley.
</div>
<div class="csl-entry">
Blais, Brian. 2020. <em>Statistical Inference for Everyone</em>. Save the broccoli publishing.
</div>
<div class="csl-entry">
Casella, George, and Roger L. Berger. 2007. <em>Statistical Inference, 2nd Ed.</em> Cengage Learning.
</div>
<div class="csl-entry">
Chaudhuri, Arijit, and Horst Stenger. 2005. <em>Survey Sampling. Theory and Methods, 2nd Ed.</em> STATISTICS: A Series of TEXTBOOKS and MONOGRAPHS. Chapman &amp; Hall/CRC.
</div>
<div class="csl-entry">
Finetti, Bruno de. 2017. <em>Theory of Probability: A Critical Introductory Treatment</em>. Wiley Series in Probability and Statistics. Wiley.
</div>
<div class="csl-entry">
Greenacre, Michael. 2008. <em>La Pr<span>á</span>ctica Del an<span>á</span>lisis de Correspondencias</em>. Fundaci<span>ó</span>n BBVA.
</div>
<div class="csl-entry">
Hajek, Alan, and Christopher Hitchcock. 2016. <em>The Oxford Handbook of Probability and Philosophy</em>. Oxford University Press.
</div>
<div class="csl-entry">
Johnson, Norman L., Adrienne W. Kemp, and Samuel Kotz. 2008. <em>Univariate Discrete Distributions, 3e Set</em>. Wiley Series in Probability and Statistics. Wi.
</div>
<div class="csl-entry">
Morin, David J. 2016. <em>Probability: For the Enthusiastic Beginner</em>. CreateSpace Independent Publishing Platform.
</div>
<div class="csl-entry">
Ross, Sheldon. 2012. <em>A First Course in Probability 9th Ed.</em> Upper Saddle River, NJ 07458: Prentice Hall.
</div>
<div class="csl-entry">
Wu, Changbao, and Mary E. Thompson. 2020. <em>Sampling Theory and Practice</em>. Edited by Jiahua Chen and Ding-Geng Chen. ICSA Book Series in Statistics. Springer.
</div>
</div>
</div>
</div>
































































































<div class="footnotes">
<hr />
<ol start="73">
<li id="fn73"><p>Universidad Complutense de Madrid<a href="boosting.-xgboost..html#fnref73" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bagging.-random-forest.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Ciencia_de_datos_con_r.pdf", "Ciencia_de_datos_con_r.epub"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
