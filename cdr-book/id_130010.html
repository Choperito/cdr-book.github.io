<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 11 Feature Selection and Engineering | Fundamentos de ciencia de datos con R</title>
  <meta name="description" content="Falta hacer" />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 11 Feature Selection and Engineering | Fundamentos de ciencia de datos con R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Falta hacer" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 11 Feature Selection and Engineering | Fundamentos de ciencia de datos con R" />
  
  <meta name="twitter:description" content="Falta hacer" />
  

<meta name="author" content="Gema Fernández-Avilés y José-María Montero" />


<meta name="date" content="2022-12-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="id_130009.html"/>
<link rel="next" href="Funda-probab.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-33KQ1S5ZCJ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-33KQ1S5ZCJ');
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"> Ciencia de datos con <strong>R</strong></a></li>

<li class="divider"></li>
<li class="part"><span><b>I Fundamentos y ‘tool-kit’ de la ciencia de datos</b></span></li>
<li class="chapter" data-level="1" data-path="ciencia-datos.html"><a href="ciencia-datos.html"><i class="fa fa-check"></i><b>1</b> ¿Es la Ciencia de datos una Ciencia?</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ciencia-datos.html"><a href="ciencia-datos.html#ciencia"><i class="fa fa-check"></i><b>1.1</b> ¿Qué se entiende por Ciencia?</a></li>
<li class="chapter" data-level="1.2" data-path="ciencia-datos.html"><a href="ciencia-datos.html#qué-es-la-ciencia-de-datos"><i class="fa fa-check"></i><b>1.2</b> ¿Qué es la Ciencia de Datos?</a></li>
<li class="chapter" data-level="1.3" data-path="ciencia-datos.html"><a href="ciencia-datos.html#lo-científico-de-la-ciencia-de-datos"><i class="fa fa-check"></i><b>1.3</b> Lo científico de la Ciencia de datos</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="metodología.html"><a href="metodología.html"><i class="fa fa-check"></i><b>2</b> Metodología para la Ciencia de datos</a>
<ul>
<li class="chapter" data-level="2.1" data-path="metodología.html"><a href="metodología.html#preliminares"><i class="fa fa-check"></i><b>2.1</b> Preliminares</a></li>
<li class="chapter" data-level="2.2" data-path="metodología.html"><a href="metodología.html#principales-metodologías-en-ciencia-de-datos"><i class="fa fa-check"></i><b>2.2</b> Principales metodologías en Ciencia de datos</a></li>
<li class="chapter" data-level="2.3" data-path="metodología.html"><a href="metodología.html#met-crisp-dm"><i class="fa fa-check"></i><b>2.3</b> CRISP-DM para Ciencia de datos</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-110003.html"><a href="ch-110003.html"><i class="fa fa-check"></i><b>3</b> R para ciencia de datos</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch-110003.html"><a href="ch-110003.html#introducción"><i class="fa fa-check"></i><b>3.1</b> Introducción</a></li>
<li class="chapter" data-level="3.2" data-path="ch-110003.html"><a href="ch-110003.html#id_110003-bases"><i class="fa fa-check"></i><b>3.2</b> La sesión de R</a></li>
<li class="chapter" data-level="3.3" data-path="ch-110003.html"><a href="ch-110003.html#instalación-de-r"><i class="fa fa-check"></i><b>3.3</b> Instalación de R</a></li>
<li class="chapter" data-level="3.4" data-path="ch-110003.html"><a href="ch-110003.html#id_110003-proyectos"><i class="fa fa-check"></i><b>3.4</b> Trabajar con proyectos de RStudio</a></li>
<li class="chapter" data-level="3.5" data-path="ch-110003.html"><a href="ch-110003.html#manipulación-de-datos-con-r"><i class="fa fa-check"></i><b>3.5</b> Manipulación de datos con R</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="ch-110003.html"><a href="ch-110003.html#id_110003-estructuras"><i class="fa fa-check"></i><b>3.5.1</b> Estructuras y tipos de datos</a></li>
<li class="chapter" data-level="3.5.2" data-path="ch-110003.html"><a href="ch-110003.html#id_110003-importacion"><i class="fa fa-check"></i><b>3.5.2</b> Importación de datos</a></li>
<li class="chapter" data-level="3.5.3" data-path="ch-110003.html"><a href="ch-110003.html#exportación-y-archivos-de-datos-de-r"><i class="fa fa-check"></i><b>3.5.3</b> Exportación y archivos de datos de R</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="ch-110003.html"><a href="ch-110003.html#id_110003-tidyverse"><i class="fa fa-check"></i><b>3.6</b> Organización de datos con el <em>tidyverse</em></a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="ch-110003.html"><a href="ch-110003.html#el-tidyverse-y-su-flujo-de-trabajo"><i class="fa fa-check"></i><b>3.6.1</b> El <em>tidyverse</em> y su flujo de trabajo</a></li>
<li class="chapter" data-level="3.6.2" data-path="ch-110003.html"><a href="ch-110003.html#transformación-de-datos-con-dplyr"><i class="fa fa-check"></i><b>3.6.2</b> Transformación de datos con <code>dplyr</code></a></li>
<li class="chapter" data-level="3.6.3" data-path="ch-110003.html"><a href="ch-110003.html#combinación-de-datos"><i class="fa fa-check"></i><b>3.6.3</b> Combinación de datos</a></li>
<li class="chapter" data-level="3.6.4" data-path="ch-110003.html"><a href="ch-110003.html#reorganización-de-datos"><i class="fa fa-check"></i><b>3.6.4</b> Reorganización de datos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><i class="fa fa-check"></i><b>4</b> Gestión y operación de datos con bases de datos relacionales</a>
<ul>
<li class="chapter" data-level="4.1" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#introducción-1"><i class="fa fa-check"></i><b>4.1</b> Introducción</a></li>
<li class="chapter" data-level="4.2" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#concepto-de-base-de-datos"><i class="fa fa-check"></i><b>4.2</b> Concepto de Base de datos</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#gestión-de-los-datos-en-una-base-o-repositorio-de-datos"><i class="fa fa-check"></i><b>4.2.1</b> Gestión de los datos en una base o repositorio de datos</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#el-lenguaje-estructurado-de-consulta-sql"><i class="fa fa-check"></i><b>4.3</b> El Lenguaje Estructurado de Consulta (SQL)</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#sql-como-lenguaje-de-definición-de-datos-ldd"><i class="fa fa-check"></i><b>4.3.1</b> SQL como Lenguaje de Definición de Datos (LDD)</a></li>
<li class="chapter" data-level="4.3.2" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#sql-como-lenguaje-de-manipulación-de-datos-lmd"><i class="fa fa-check"></i><b>4.3.2</b> SQL como Lenguaje de Manipulación de Datos (LMD)</a></li>
<li class="chapter" data-level="4.3.3" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#sql-como-lenguaje-de-administración-de-datos-lad"><i class="fa fa-check"></i><b>4.3.3</b> SQL como Lenguaje de Administración de Datos (LAD)</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#usando-bases-de-datos-desde-r"><i class="fa fa-check"></i><b>4.4</b> Usando bases de datos desde R</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#conexión-a-una-base-de-datos"><i class="fa fa-check"></i><b>4.4.1</b> Conexión a una base de datos</a></li>
<li class="chapter" data-level="4.4.2" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#operaciones-de-lectura-selección-read-de-datos"><i class="fa fa-check"></i><b>4.4.2</b> Operaciones de lectura / selección (<em>read</em>) de datos</a></li>
<li class="chapter" data-level="4.4.3" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#operaciones-de-inserción-create-y-actualización-update-de-datos"><i class="fa fa-check"></i><b>4.4.3</b> Operaciones de inserción (<em>create</em>) y actualización (<em>update</em>) de datos</a></li>
<li class="chapter" data-level="4.4.4" data-path="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html"><a href="gestión-y-operación-de-datos-con-bases-de-datos-relacionales.html#operaciones-de-borrado-de-datos-delete"><i class="fa fa-check"></i><b>4.4.4</b> Operaciones de Borrado de datos (<em>delete</em>)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="datos-no-sql.html"><a href="datos-no-sql.html"><i class="fa fa-check"></i><b>5</b> Gestión y operación de datos masivos (BigData) con bases de datos NoSQL</a>
<ul>
<li class="chapter" data-level="5.1" data-path="datos-no-sql.html"><a href="datos-no-sql.html#introducción-al-big-data"><i class="fa fa-check"></i><b>5.1</b> Introducción al Big Data</a></li>
<li class="chapter" data-level="5.2" data-path="datos-no-sql.html"><a href="datos-no-sql.html#VsBigData"><i class="fa fa-check"></i><b>5.2</b> Las V’s del Big Data</a></li>
<li class="chapter" data-level="5.3" data-path="datos-no-sql.html"><a href="datos-no-sql.html#fuentes-de-datos-en-entornos-big-data"><i class="fa fa-check"></i><b>5.3</b> Fuentes de Datos en entornos Big Data</a></li>
<li class="chapter" data-level="5.4" data-path="datos-no-sql.html"><a href="datos-no-sql.html#bases-de-datos-relacionales-vs.-nosql"><i class="fa fa-check"></i><b>5.4</b> Bases de datos Relacionales vs. NoSQL</a></li>
<li class="chapter" data-level="5.5" data-path="datos-no-sql.html"><a href="datos-no-sql.html#bases-de-datos-nosql"><i class="fa fa-check"></i><b>5.5</b> Bases de datos NoSQL</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="datos-no-sql.html"><a href="datos-no-sql.html#definición-de-bases-de-datos-nosql"><i class="fa fa-check"></i><b>5.5.1</b> Definición de bases de datos NoSQL</a></li>
<li class="chapter" data-level="5.5.2" data-path="datos-no-sql.html"><a href="datos-no-sql.html#necesidades-no-cubiertas-por-las-bases-de-datos-relacionales"><i class="fa fa-check"></i><b>5.5.2</b> Necesidades no cubiertas por las bases de datos relacionales</a></li>
<li class="chapter" data-level="5.5.3" data-path="datos-no-sql.html"><a href="datos-no-sql.html#tipos-de-almacenamiento-en-bases-de-datos-nosql"><i class="fa fa-check"></i><b>5.5.3</b> Tipos de almacenamiento en bases de datos NoSQL</a></li>
<li class="chapter" data-level="5.5.4" data-path="datos-no-sql.html"><a href="datos-no-sql.html#limitaciones-de-las-bases-de-datos-nosql"><i class="fa fa-check"></i><b>5.5.4</b> Limitaciones de las bases de datos NoSQL</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="datos-no-sql.html"><a href="datos-no-sql.html#ejemplo-de-integración-de-una-base-de-datos-nosql-y-análisis-de-datos-en-r"><i class="fa fa-check"></i><b>5.6</b> Ejemplo de integración de una base de datos NoSQL y análisis de datos en R</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="datos-no-sql.html"><a href="datos-no-sql.html#introMongo"><i class="fa fa-check"></i><b>5.6.1</b> Introducción a MongoDB</a></li>
<li class="chapter" data-level="5.6.2" data-path="datos-no-sql.html"><a href="datos-no-sql.html#paquetesCaso"><i class="fa fa-check"></i><b>5.6.2</b> Plataforma tecnológica para el caso práctico</a></li>
<li class="chapter" data-level="5.6.3" data-path="datos-no-sql.html"><a href="datos-no-sql.html#conexionMongo"><i class="fa fa-check"></i><b>5.6.3</b> Conexión y acceso a MongoDB desde R</a></li>
<li class="chapter" data-level="5.6.4" data-path="datos-no-sql.html"><a href="datos-no-sql.html#consultaViajes"><i class="fa fa-check"></i><b>5.6.4</b> Obtención de datos en R desde MongoDB</a></li>
<li class="chapter" data-level="5.6.5" data-path="datos-no-sql.html"><a href="datos-no-sql.html#analisisViajes"><i class="fa fa-check"></i><b>5.6.5</b> Analizando datos de MongoDB en R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="id_120007-informes.html"><a href="id_120007-informes.html"><i class="fa fa-check"></i><b>6</b> Informes reproducibles con R-markdown -&gt; Quarto</a>
<ul>
<li class="chapter" data-level="6.1" data-path="id_120007-informes.html"><a href="id_120007-informes.html#introducción-2"><i class="fa fa-check"></i><b>6.1</b> Introducción</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="id_120007-informes.html"><a href="id_120007-informes.html#por-qué-informes-reproducibles"><i class="fa fa-check"></i><b>6.1.1</b> ¿Por qué informes reproducibles?</a></li>
<li class="chapter" data-level="6.1.2" data-path="id_120007-informes.html"><a href="id_120007-informes.html#markdown-r-markdown-y-rstudio"><i class="fa fa-check"></i><b>6.1.2</b> Markdown, R Markdown y RStudio</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="id_120007-informes.html"><a href="id_120007-informes.html#documentos-r-markdown"><i class="fa fa-check"></i><b>6.2</b> Documentos R Markdown</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="id_120007-informes.html"><a href="id_120007-informes.html#encabezado-yaml-y-configuración"><i class="fa fa-check"></i><b>6.2.1</b> Encabezado YAML y configuración</a></li>
<li class="chapter" data-level="6.2.2" data-path="id_120007-informes.html"><a href="id_120007-informes.html#formateado-de-texto"><i class="fa fa-check"></i><b>6.2.2</b> Formateado de texto</a></li>
<li class="chapter" data-level="6.2.3" data-path="id_120007-informes.html"><a href="id_120007-informes.html#inclusión-de-código"><i class="fa fa-check"></i><b>6.2.3</b> Inclusión de código</a></li>
<li class="chapter" data-level="6.2.4" data-path="id_120007-informes.html"><a href="id_120007-informes.html#opciones-de-los-bloques-de-código-chunks"><i class="fa fa-check"></i><b>6.2.4</b> Opciones de los bloques de código (<em>chunks</em>)</a></li>
<li class="chapter" data-level="6.2.5" data-path="id_120007-informes.html"><a href="id_120007-informes.html#editor-visual"><i class="fa fa-check"></i><b>6.2.5</b> Editor visual</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="id_120007-informes.html"><a href="id_120007-informes.html#otros-formatos"><i class="fa fa-check"></i><b>6.3</b> Otros formatos</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="github.html"><a href="github.html"><i class="fa fa-check"></i><b>7</b> Git y GitHub en R</a>
<ul>
<li class="chapter" data-level="7.1" data-path="github.html"><a href="github.html#qué-es-git"><i class="fa fa-check"></i><b>7.1</b> ¿Qué es Git?</a></li>
<li class="chapter" data-level="7.2" data-path="github.html"><a href="github.html#qué-es-github"><i class="fa fa-check"></i><b>7.2</b> ¿Qué es GitHub?</a></li>
<li class="chapter" data-level="7.3" data-path="github.html"><a href="github.html#por-qué-usar-git-y-github"><i class="fa fa-check"></i><b>7.3</b> ¿Por qué usar Git y GitHub?</a></li>
<li class="chapter" data-level="7.4" data-path="github.html"><a href="github.html#configuración"><i class="fa fa-check"></i><b>7.4</b> Configuración</a></li>
<li class="chapter" data-level="7.5" data-path="github.html"><a href="github.html#configurar-git"><i class="fa fa-check"></i><b>7.5</b> Configurar git</a></li>
<li class="chapter" data-level="7.6" data-path="github.html"><a href="github.html#workflow"><i class="fa fa-check"></i><b>7.6</b> Workflow</a></li>
</ul></li>
<li class="part"><span><b>II Manipulación de datos con R. Técnicas y herramientas</b></span></li>
<li class="chapter" data-level="8" data-path="id_120006-aed.html"><a href="id_120006-aed.html"><i class="fa fa-check"></i><b>8</b> Análisis exploratorio de datos</a>
<ul>
<li class="chapter" data-level="8.1" data-path="id_120006-aed.html"><a href="id_120006-aed.html#introducción-3"><i class="fa fa-check"></i><b>8.1</b> Introducción</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="id_120006-aed.html"><a href="id_120006-aed.html#el-cuarterto-de-anscombe"><i class="fa fa-check"></i><b>8.1.1</b> El cuarterto de Anscombe</a></li>
<li class="chapter" data-level="8.1.2" data-path="id_120006-aed.html"><a href="id_120006-aed.html#conceptos-generales"><i class="fa fa-check"></i><b>8.1.2</b> Conceptos generales</a></li>
<li class="chapter" data-level="8.1.3" data-path="id_120006-aed.html"><a href="id_120006-aed.html#componentes-de-un-gráfico-y-su-representación-en-r"><i class="fa fa-check"></i><b>8.1.3</b> Componentes de un gráfico y su representación en R</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="id_120006-aed.html"><a href="id_120006-aed.html#id_120006-aeduni"><i class="fa fa-check"></i><b>8.2</b> Análisis exploratorio de una característica</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="id_120006-aed.html"><a href="id_120006-aed.html#variables-cualitativas"><i class="fa fa-check"></i><b>8.2.1</b> Variables cualitativas</a></li>
<li class="chapter" data-level="8.2.2" data-path="id_120006-aed.html"><a href="id_120006-aed.html#variables-cuantitativas"><i class="fa fa-check"></i><b>8.2.2</b> Variables cuantitativas</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="id_120006-aed.html"><a href="id_120006-aed.html#id_120006-aedmulti"><i class="fa fa-check"></i><b>8.3</b> Análisis exploratorio de varias características</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="id_120006-aed.html"><a href="id_120006-aed.html#variables-cualitativas-1"><i class="fa fa-check"></i><b>8.3.1</b> Variables cualitativas</a></li>
<li class="chapter" data-level="8.3.2" data-path="id_120006-aed.html"><a href="id_120006-aed.html#variables-cuantitativas-1"><i class="fa fa-check"></i><b>8.3.2</b> Variables cuantitativas</a></li>
<li class="chapter" data-level="8.3.3" data-path="id_120006-aed.html"><a href="id_120006-aed.html#variables-cualitativas-y-cuantitativas"><i class="fa fa-check"></i><b>8.3.3</b> Variables cualitativas y cuantitativas</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Preparación de datos: evaluación de la calidad de los datos. Integración, limpieza y transformación</b></span></li>
<li class="chapter" data-level="9" data-path="DGDQM.html"><a href="DGDQM.html"><i class="fa fa-check"></i><b>9</b> Gobierno y gestión de calidad de Datos</a>
<ul>
<li class="chapter" data-level="9.1" data-path="DGDQM.html"><a href="DGDQM.html#introducción-4"><i class="fa fa-check"></i><b>9.1</b> Introducción</a></li>
<li class="chapter" data-level="9.2" data-path="DGDQM.html"><a href="DGDQM.html#concepto-de-gobierno-de-datos"><i class="fa fa-check"></i><b>9.2</b> Concepto de Gobierno de datos</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="DGDQM.html"><a href="DGDQM.html#beneficiosDG"><i class="fa fa-check"></i><b>9.2.1</b> Beneficios del Gobierno de Datos</a></li>
<li class="chapter" data-level="9.2.2" data-path="DGDQM.html"><a href="DGDQM.html#artefactosDG"><i class="fa fa-check"></i><b>9.2.2</b> Artefactos de un sistema de Gobierno de Datos</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="DGDQM.html"><a href="DGDQM.html#marcos-y-metodologías-existentes-de-gobierno-de-datos"><i class="fa fa-check"></i><b>9.3</b> Marcos y metodologías existentes de Gobierno de Datos</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="DGDQM.html"><a href="DGDQM.html#modelo-alarcos-de-mejora-de-datos-mamd"><i class="fa fa-check"></i><b>9.3.1</b> Modelo Alarcos de Mejora de Datos (MAMD)</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="DGDQM.html"><a href="DGDQM.html#gestión-de-calidad-de-datos"><i class="fa fa-check"></i><b>9.4</b> Gestión de calidad de datos</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="DGDQM.html"><a href="DGDQM.html#medición-de-calidad-de-datos-vs-perfilado-de-datos"><i class="fa fa-check"></i><b>9.4.1</b> Medición de calidad de datos vs perfilado de datos</a></li>
<li class="chapter" data-level="9.4.2" data-path="DGDQM.html"><a href="DGDQM.html#mejora-de-datos"><i class="fa fa-check"></i><b>9.4.2</b> Mejora de datos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="id_130009.html"><a href="id_130009.html"><i class="fa fa-check"></i><b>10</b> Integración y limpieza de datos</a>
<ul>
<li class="chapter" data-level="10.1" data-path="id_130009.html"><a href="id_130009.html#introducción-5"><i class="fa fa-check"></i><b>10.1</b> Introducción</a></li>
<li class="chapter" data-level="10.2" data-path="id_130009.html"><a href="id_130009.html#problemas-de-calidad-de-datos"><i class="fa fa-check"></i><b>10.2</b> Problemas de calidad de datos</a></li>
<li class="chapter" data-level="10.3" data-path="id_130009.html"><a href="id_130009.html#niveles-inadecuados-de-completitud-valores-missing"><i class="fa fa-check"></i><b>10.3</b> Niveles inadecuados de completitud: Valores <em>missing</em></a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="id_130009.html"><a href="id_130009.html#visualización"><i class="fa fa-check"></i><b>10.3.1</b> Visualización</a></li>
<li class="chapter" data-level="10.3.2" data-path="id_130009.html"><a href="id_130009.html#imputacion"><i class="fa fa-check"></i><b>10.3.2</b> Técnicas de Imputación</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="id_130009.html"><a href="id_130009.html#mejorando-la-exactitud-y-la-precisión-eliminación-del-ruido-estadístico"><i class="fa fa-check"></i><b>10.4</b> Mejorando la exactitud y la precisión: eliminación del ruido estadístico</a></li>
<li class="chapter" data-level="10.5" data-path="id_130009.html"><a href="id_130009.html#integración-de-datos"><i class="fa fa-check"></i><b>10.5</b> Integración de datos</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="id_130010.html"><a href="id_130010.html"><i class="fa fa-check"></i><b>11</b> Feature Selection and Engineering</a>
<ul>
<li class="chapter" data-level="11.1" data-path="id_130010.html"><a href="id_130010.html#introducción-6"><i class="fa fa-check"></i><b>11.1</b> Introducción</a></li>
<li class="chapter" data-level="11.2" data-path="id_130010.html"><a href="id_130010.html#feature-selection-selección-de-variables"><i class="fa fa-check"></i><b>11.2</b> <em>Feature Selection</em> (Selección de variables)</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="id_130010.html"><a href="id_130010.html#métodos-de-selección-tipo-filtro"><i class="fa fa-check"></i><b>11.2.1</b> Métodos de selección tipo Filtro</a></li>
<li class="chapter" data-level="11.2.2" data-path="id_130010.html"><a href="id_130010.html#métodos-de-selección-de-variables-tipo-wrapper"><i class="fa fa-check"></i><b>11.2.2</b> Métodos de selección de variables tipo <em>wrapper</em></a></li>
<li class="chapter" data-level="11.2.3" data-path="id_130010.html"><a href="id_130010.html#métodos-de-selección-tipo-embedded"><i class="fa fa-check"></i><b>11.2.3</b> Métodos de selección tipo Embedded</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="id_130010.html"><a href="id_130010.html#transformaciones-de-escala-y-de-la-distribución-de-la-variable-objetivo"><i class="fa fa-check"></i><b>11.3</b> Transformaciones de escala y de la distribución de la variable objetivo</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="id_130010.html"><a href="id_130010.html#id_31"><i class="fa fa-check"></i><b>11.3.1</b> Transformaciones de la variable objetivo</a></li>
<li class="chapter" data-level="11.3.2" data-path="id_130010.html"><a href="id_130010.html#escalado-de-datos"><i class="fa fa-check"></i><b>11.3.2</b> Escalado de datos</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="id_130010.html"><a href="id_130010.html#feature-engineering"><i class="fa fa-check"></i><b>11.4</b> <em>Feature engineering</em></a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="id_130010.html"><a href="id_130010.html#binning"><i class="fa fa-check"></i><b>11.4.1</b> <em>Binning</em></a></li>
<li class="chapter" data-level="11.4.2" data-path="id_130010.html"><a href="id_130010.html#codificación"><i class="fa fa-check"></i><b>11.4.2</b> Codificación</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="id_130010.html"><a href="id_130010.html#reducción-de-dimensionalidad"><i class="fa fa-check"></i><b>11.5</b> Reducción de dimensionalidad</a></li>
<li class="chapter" data-level="11.6" data-path="id_130010.html"><a href="id_130010.html#otras-transformaciones"><i class="fa fa-check"></i><b>11.6</b> Otras transformaciones</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="id_130010.html"><a href="id_130010.html#particionado-de-datos"><i class="fa fa-check"></i><b>11.6.1</b> Particionado de datos</a></li>
<li class="chapter" data-level="11.6.2" data-path="id_130010.html"><a href="id_130010.html#técnicas-para-manejar-datos-no-balanceados"><i class="fa fa-check"></i><b>11.6.2</b> Técnicas para manejar datos no balanceados</a></li>
<li class="chapter" data-level="11.6.3" data-path="id_130010.html"><a href="id_130010.html#métodos-de-remuestreo"><i class="fa fa-check"></i><b>11.6.3</b> Métodos de remuestreo</a></li>
<li class="chapter" data-level="11.6.4" data-path="id_130010.html"><a href="id_130010.html#ajuste-de-hiperparámetros"><i class="fa fa-check"></i><b>11.6.4</b> Ajuste de hiperparámetros</a></li>
<li class="chapter" data-level="11.6.5" data-path="id_130010.html"><a href="id_130010.html#evaluación-de-modelos"><i class="fa fa-check"></i><b>11.6.5</b> Evaluación de modelos</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Técnicas de modelización estadísticas avanzadas</b></span></li>
<li class="chapter" data-level="12" data-path="Funda-probab.html"><a href="Funda-probab.html"><i class="fa fa-check"></i><b>12</b> Fundamentos de probabilidad</a>
<ul>
<li class="chapter" data-level="12.1" data-path="Funda-probab.html"><a href="Funda-probab.html#introducción-a-la-probabilidad"><i class="fa fa-check"></i><b>12.1</b> Introducción a la probabilidad</a></li>
<li class="chapter" data-level="12.2" data-path="Funda-probab.html"><a href="Funda-probab.html#probabilidad-elementos-básicos-definición-y-teoremas"><i class="fa fa-check"></i><b>12.2</b> Probabilidad: elementos básicos, definición y teoremas</a></li>
<li class="chapter" data-level="12.3" data-path="Funda-probab.html"><a href="Funda-probab.html#variable-aleatoria-y-su-distribución-tipos-de-variables-aleatorias"><i class="fa fa-check"></i><b>12.3</b> Variable aleatoria y su distribución: tipos de variables aleatorias</a></li>
<li class="chapter" data-level="12.4" data-path="Funda-probab.html"><a href="Funda-probab.html#modelos-de-distribución-de-probabilidad"><i class="fa fa-check"></i><b>12.4</b> Modelos de distribución de probabilidad</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="Funda-probab.html"><a href="Funda-probab.html#modelos-discretos"><i class="fa fa-check"></i><b>12.4.1</b> Modelos discretos</a></li>
<li class="chapter" data-level="12.4.2" data-path="Funda-probab.html"><a href="Funda-probab.html#modelos-continuos"><i class="fa fa-check"></i><b>12.4.2</b> Modelos continuos</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="Funda-probab.html"><a href="Funda-probab.html#tcl"><i class="fa fa-check"></i><b>12.5</b> Teorema central del límite (TCL)</a></li>
<li class="chapter" data-level="12.6" data-path="Funda-probab.html"><a href="Funda-probab.html#ejemplo-de-distribuciones-usando-r"><i class="fa fa-check"></i><b>12.6</b> Ejemplo de distribuciones usando <strong>R</strong></a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="Fundainfer.html"><a href="Fundainfer.html"><i class="fa fa-check"></i><b>13</b> Fundamentos de Inferencia Estadística</a>
<ul>
<li class="chapter" data-level="13.1" data-path="Fundainfer.html"><a href="Fundainfer.html#introinfer"><i class="fa fa-check"></i><b>13.1</b> Introducción a la Inferencia Estadística</a></li>
<li class="chapter" data-level="13.2" data-path="Fundainfer.html"><a href="Fundainfer.html#mas"><i class="fa fa-check"></i><b>13.2</b> Muestreo aleatorio simple</a></li>
<li class="chapter" data-level="13.3" data-path="Fundainfer.html"><a href="Fundainfer.html#estimpuntual"><i class="fa fa-check"></i><b>13.3</b> Estimación puntual</a></li>
<li class="chapter" data-level="13.4" data-path="Fundainfer.html"><a href="Fundainfer.html#estimintervalos"><i class="fa fa-check"></i><b>13.4</b> Estimación por intervalos</a></li>
<li class="chapter" data-level="13.5" data-path="Fundainfer.html"><a href="Fundainfer.html#contrhip"><i class="fa fa-check"></i><b>13.5</b> Contrastes de hipótesis</a></li>
<li class="chapter" data-level="13.6" data-path="Fundainfer.html"><a href="Fundainfer.html#pobnormales"><i class="fa fa-check"></i><b>13.6</b> Inferencia estadística paramétrica sobre poblaciones normales</a></li>
<li class="chapter" data-level="13.7" data-path="Fundainfer.html"><a href="Fundainfer.html#ejemplopobnorm"><i class="fa fa-check"></i><b>13.7</b> Inferencia sobre poblaciones normales con <strong>R</strong></a></li>
<li class="chapter" data-level="13.8" data-path="Fundainfer.html"><a href="Fundainfer.html#contrnormalidad"><i class="fa fa-check"></i><b>13.8</b> Inferencia estadística no paramétrica: contrastes de normalidad</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="muestreo.html"><a href="muestreo.html"><i class="fa fa-check"></i><b>14</b> Métodos de muestreo y remuestreo</a>
<ul>
<li class="chapter" data-level="14.1" data-path="muestreo.html"><a href="muestreo.html#introducción-al-muestreo"><i class="fa fa-check"></i><b>14.1</b> Introducción al muestreo</a></li>
<li class="chapter" data-level="14.2" data-path="muestreo.html"><a href="muestreo.html#muestreo-aleatorio-simple-1"><i class="fa fa-check"></i><b>14.2</b> Muestreo aleatorio simple</a></li>
<li class="chapter" data-level="14.3" data-path="muestreo.html"><a href="muestreo.html#muestestra"><i class="fa fa-check"></i><b>14.3</b> Muestreo estratificado</a></li>
<li class="chapter" data-level="14.4" data-path="muestreo.html"><a href="muestreo.html#otros-tipos-de-muestreo-probabilístico"><i class="fa fa-check"></i><b>14.4</b> Otros tipos de muestreo probabilístico</a></li>
<li class="chapter" data-level="14.5" data-path="muestreo.html"><a href="muestreo.html#técnicas-de-remuestreo-bootstrap."><i class="fa fa-check"></i><b>14.5</b> Técnicas de remuestreo: Bootstrap.</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="modelización-lineal.html"><a href="modelización-lineal.html"><i class="fa fa-check"></i><b>15</b> Modelización lineal</a>
<ul>
<li class="chapter" data-level="15.1" data-path="modelización-lineal.html"><a href="modelización-lineal.html#modelización"><i class="fa fa-check"></i><b>15.1</b> Modelización</a></li>
<li class="chapter" data-level="15.2" data-path="modelización-lineal.html"><a href="modelización-lineal.html#procedimiento-de-modelización"><i class="fa fa-check"></i><b>15.2</b> Procedimiento de modelización</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="modelización-lineal.html"><a href="modelización-lineal.html#Bondad"><i class="fa fa-check"></i><b>15.2.1</b> Estimación del modelo</a></li>
<li class="chapter" data-level="15.2.2" data-path="modelización-lineal.html"><a href="modelización-lineal.html#validación-del-modelo"><i class="fa fa-check"></i><b>15.2.2</b> Validación del modelo</a></li>
<li class="chapter" data-level="15.2.3" data-path="modelización-lineal.html"><a href="modelización-lineal.html#interpretación-de-los-coeficientes"><i class="fa fa-check"></i><b>15.2.3</b> Interpretación de los coeficientes</a></li>
<li class="chapter" data-level="15.2.4" data-path="modelización-lineal.html"><a href="modelización-lineal.html#predicción"><i class="fa fa-check"></i><b>15.2.4</b> Predicción</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="modelización-lineal.html"><a href="modelización-lineal.html#procedimiento-con-r-la-función-lm"><i class="fa fa-check"></i><b>15.3</b> Procedimiento con R: la función <code>lm</code></a></li>
<li class="chapter" data-level="15.4" data-path="modelización-lineal.html"><a href="modelización-lineal.html#Casos"><i class="fa fa-check"></i><b>15.4</b> Casos prácticos</a>
<ul>
<li class="chapter" data-level="15.4.1" data-path="modelización-lineal.html"><a href="modelización-lineal.html#estimación-de-los-coeficientes"><i class="fa fa-check"></i><b>15.4.1</b> Estimación de los coeficientes</a></li>
<li class="chapter" data-level="15.4.2" data-path="modelización-lineal.html"><a href="modelización-lineal.html#validación"><i class="fa fa-check"></i><b>15.4.2</b> Validación</a></li>
<li class="chapter" data-level="15.4.3" data-path="modelización-lineal.html"><a href="modelización-lineal.html#interpretación-de-los-coeficientes-1"><i class="fa fa-check"></i><b>15.4.3</b> Interpretación de los coeficientes</a></li>
<li class="chapter" data-level="15.4.4" data-path="modelización-lineal.html"><a href="modelización-lineal.html#predicción-1"><i class="fa fa-check"></i><b>15.4.4</b> Predicción</a></li>
<li class="chapter" data-level="15.4.5" data-path="modelización-lineal.html"><a href="modelización-lineal.html#nuevo-ajuste-con-logozone"><i class="fa fa-check"></i><b>15.4.5</b> Nuevo ajuste con <code>log(Ozone)</code></a></li>
<li class="chapter" data-level="15.4.6" data-path="modelización-lineal.html"><a href="modelización-lineal.html#coeficientes-de-variables-categóricas"><i class="fa fa-check"></i><b>15.4.6</b> Coeficientes de variables categóricas</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="modelización-lineal.html"><a href="modelización-lineal.html#comentarios-finales"><i class="fa fa-check"></i><b>15.5</b> Comentarios finales</a>
<ul>
<li class="chapter" data-level="" data-path="modelización-lineal.html"><a href="modelización-lineal.html#resumen"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html"><i class="fa fa-check"></i><b>16</b> Modelos lineales generalizados</a>
<ul>
<li class="chapter" data-level="16.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#motivación"><i class="fa fa-check"></i><b>16.1</b> Motivación</a></li>
<li class="chapter" data-level="16.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#modelo-y-sus-componentes"><i class="fa fa-check"></i><b>16.2</b> Modelo y sus componentes</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#función-enlace"><i class="fa fa-check"></i><b>16.2.1</b> Función enlace </a></li>
<li class="chapter" data-level="16.2.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#glms-en-r"><i class="fa fa-check"></i><b>16.2.2</b> GLMs en <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-logística"><i class="fa fa-check"></i><b>16.3</b> Regresión logística</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#procedimiento-de-ajuste"><i class="fa fa-check"></i><b>16.3.1</b> Procedimiento de ajuste</a></li>
<li class="chapter" data-level="16.3.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#adecuación-del-modelo"><i class="fa fa-check"></i><b>16.3.2</b> Adecuación del modelo</a></li>
<li class="chapter" data-level="16.3.3" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#SECCinterp"><i class="fa fa-check"></i><b>16.3.3</b> Interpretación de resultados</a></li>
<li class="chapter" data-level="16.3.4" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#predicción.-curva-roc-y-auc"><i class="fa fa-check"></i><b>16.3.4</b> Predicción. Curva ROC y AUC</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#regresión-de-poisson"><i class="fa fa-check"></i><b>16.4</b> Regresión de Poisson</a></li>
<li class="chapter" data-level="16.5" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#casos-prácticos"><i class="fa fa-check"></i><b>16.5</b> Casos prácticos</a>
<ul>
<li class="chapter" data-level="16.5.1" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejemplos-de-regresión-logística"><i class="fa fa-check"></i><b>16.5.1</b> Ejemplos de regresión logística</a></li>
<li class="chapter" data-level="16.5.2" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#ejemplo-de-regresión-de-poisson"><i class="fa fa-check"></i><b>16.5.2</b> Ejemplo de regresión de Poisson</a></li>
<li class="chapter" data-level="" data-path="modelos-lineales-generalizados.html"><a href="modelos-lineales-generalizados.html#resumen-1"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html"><i class="fa fa-check"></i><b>17</b> Modelos aditivos generalizados</a>
<ul>
<li class="chapter" data-level="17.1" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#introducción-7"><i class="fa fa-check"></i><b>17.1</b> Introducción</a></li>
<li class="chapter" data-level="17.2" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#splines-con-penalizaciones"><i class="fa fa-check"></i><b>17.2</b> Splines con penalizaciones</a></li>
<li class="chapter" data-level="17.3" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#aspectos-metodológicos"><i class="fa fa-check"></i><b>17.3</b> Aspectos metodológicos</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#estimación-de-los-paraámetros-del-modelo"><i class="fa fa-check"></i><b>17.3.1</b> Estimación de los paraámetros del modelo</a></li>
<li class="chapter" data-level="17.3.2" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#inferencia-sobre-las-funciones-suaves"><i class="fa fa-check"></i><b>17.3.2</b> Inferencia sobre las funciones suaves</a></li>
<li class="chapter" data-level="17.3.3" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#suavizado-mutidimensional-y-para-datos-no-gaussianos"><i class="fa fa-check"></i><b>17.3.3</b> Suavizado mutidimensional y para datos no Gaussianos</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#la-función-gam-del-paquete-mgcv"><i class="fa fa-check"></i><b>17.4</b> La función <code>gam</code> del paquete <code>mgcv</code></a></li>
<li class="chapter" data-level="17.5" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#casos-prácticos-1"><i class="fa fa-check"></i><b>17.5</b> Casos prácticos</a>
<ul>
<li class="chapter" data-level="17.5.1" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#modelo-unidimensional-con-fossil"><i class="fa fa-check"></i><b>17.5.1</b> Modelo unidimensional con <code>fossil</code></a></li>
<li class="chapter" data-level="17.5.2" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#modelo-aditivo-con-airquality"><i class="fa fa-check"></i><b>17.5.2</b> Modelo aditivo con <code>airquality</code></a></li>
<li class="chapter" data-level="17.5.3" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#modelo-semiparamétrico-con-onions"><i class="fa fa-check"></i><b>17.5.3</b> Modelo semiparamétrico con <code>onions</code></a></li>
<li class="chapter" data-level="17.5.4" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#modelo-aditivo-generalizado-y-multidimensional-con-smacker"><i class="fa fa-check"></i><b>17.5.4</b> Modelo aditivo generalizado y multidimensional, con <code>smacker</code></a></li>
<li class="chapter" data-level="" data-path="modelos-aditivos-generalizados.html"><a href="modelos-aditivos-generalizados.html#resumen-2"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html"><i class="fa fa-check"></i><b>18</b> Modelos mixtos</a>
<ul>
<li class="chapter" data-level="18.1" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#conceptos-básicos"><i class="fa fa-check"></i><b>18.1</b> Conceptos básicos</a>
<ul>
<li class="chapter" data-level="18.1.1" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#tipo-y-estructura-de-los-datos"><i class="fa fa-check"></i><b>18.1.1</b> Tipo y estructura de los datos</a></li>
<li class="chapter" data-level="18.1.2" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#efectos-fijos-o-aleatorios"><i class="fa fa-check"></i><b>18.1.2</b> ¿Efectos fijos o aleatorios?</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#formulación-del-modelo-con-efectos-aleatorios-o-modelos-mixtos"><i class="fa fa-check"></i><b>18.2</b> Formulación del modelo con efectos aleatorios o modelos mixtos</a>
<ul>
<li class="chapter" data-level="18.2.1" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#formulación-general"><i class="fa fa-check"></i><b>18.2.1</b> Formulación general</a></li>
<li class="chapter" data-level="18.2.2" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#inferencia-y-selección-del-modelo"><i class="fa fa-check"></i><b>18.2.2</b> Inferencia y selección del modelo</a></li>
<li class="chapter" data-level="18.2.3" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#diagnosis-del-modelo"><i class="fa fa-check"></i><b>18.2.3</b> Diagnosis del modelo</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#funciones-de-r-para-ajustar-modelos-mixtos"><i class="fa fa-check"></i><b>18.3</b> Funciones de <code>R</code> para ajustar modelos mixtos</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#la-función-lmer"><i class="fa fa-check"></i><b>18.3.1</b> La función <code>lmer()</code></a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#caso-práctico"><i class="fa fa-check"></i><b>18.4</b> Caso práctico</a>
<ul>
<li class="chapter" data-level="18.4.1" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#modelo-con-ordenada-en-el-origen-aleatoria"><i class="fa fa-check"></i><b>18.4.1</b> Modelo con ordenada en el origen aleatoria</a></li>
<li class="chapter" data-level="18.4.2" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#modelo-con-pendiente-aleatoria"><i class="fa fa-check"></i><b>18.4.2</b> Modelo con pendiente aleatoria</a></li>
<li class="chapter" data-level="18.4.3" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#cómo-construir-el-modelo-en-la-práctica"><i class="fa fa-check"></i><b>18.4.3</b> ¿Cómo construir el modelo en la práctica?</a></li>
<li class="chapter" data-level="" data-path="modelos-mixtos.html"><a href="modelos-mixtos.html#resumen-3"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html"><i class="fa fa-check"></i><b>19</b> Modelos sparse y métodos penalizados de regresión</a>
<ul>
<li class="chapter" data-level="19.1" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#introducción-8"><i class="fa fa-check"></i><b>19.1</b> Introducción</a></li>
<li class="chapter" data-level="19.2" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#selección-del-mejor-subconjunto"><i class="fa fa-check"></i><b>19.2</b> Selección del mejor subconjunto</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#ejemplo-sueldo-de-jugadores-de-béisbol"><i class="fa fa-check"></i><b>19.2.1</b> Ejemplo: Sueldo de jugadores de béisbol</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#selección-stepwise"><i class="fa fa-check"></i><b>19.3</b> Selección <em>Stepwise</em></a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#forward-stepwise"><i class="fa fa-check"></i><b>19.3.1</b> Forward stepwise</a></li>
<li class="chapter" data-level="19.3.2" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#backward-stepwise"><i class="fa fa-check"></i><b>19.3.2</b> Backward stepwise</a></li>
<li class="chapter" data-level="19.3.3" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#ejemplo-sueldo-de-jugadores-de-béisbol-1"><i class="fa fa-check"></i><b>19.3.3</b> Ejemplo: Sueldo de jugadores de béisbol</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#métodos-shrinkage"><i class="fa fa-check"></i><b>19.4</b> Métodos Shrinkage</a>
<ul>
<li class="chapter" data-level="19.4.1" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#regresión-ridge"><i class="fa fa-check"></i><b>19.4.1</b> Regresión ridge</a></li>
<li class="chapter" data-level="19.4.2" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#selección-del-parámetro-de-tuneado"><i class="fa fa-check"></i><b>19.4.2</b> Selección del parámetro de tuneado</a></li>
<li class="chapter" data-level="19.4.3" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#regresión-lasso"><i class="fa fa-check"></i><b>19.4.3</b> Regresión Lasso</a></li>
<li class="chapter" data-level="19.4.4" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#elastic-net"><i class="fa fa-check"></i><b>19.4.4</b> Elastic net </a></li>
<li class="chapter" data-level="" data-path="modelos-sparse-y-métodos-penalizados-de-regresión.html"><a href="modelos-sparse-y-métodos-penalizados-de-regresión.html#resumen-4"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="modelización-de-series-temporales.html"><a href="modelización-de-series-temporales.html"><i class="fa fa-check"></i><b>20</b> Modelización de series temporales</a>
<ul>
<li class="chapter" data-level="20.1" data-path="modelización-de-series-temporales.html"><a href="modelización-de-series-temporales.html#conceptos-básicos-1"><i class="fa fa-check"></i><b>20.1</b> Conceptos básicos</a></li>
<li class="chapter" data-level="20.2" data-path="modelización-de-series-temporales.html"><a href="modelización-de-series-temporales.html#modelos-arima"><i class="fa fa-check"></i><b>20.2</b> Modelos ARIMA</a></li>
<li class="chapter" data-level="20.3" data-path="modelización-de-series-temporales.html"><a href="modelización-de-series-temporales.html#análisis-de-series-temporales-con-r"><i class="fa fa-check"></i><b>20.3</b> Análisis de series temporales con R</a>
<ul>
<li class="chapter" data-level="20.3.1" data-path="modelización-de-series-temporales.html"><a href="modelización-de-series-temporales.html#identificación-o-especificación-del-modelo"><i class="fa fa-check"></i><b>20.3.1</b> Identificación o especificación del modelo</a></li>
<li class="chapter" data-level="20.3.2" data-path="modelización-de-series-temporales.html"><a href="modelización-de-series-temporales.html#estimación-del-modelo"><i class="fa fa-check"></i><b>20.3.2</b> Estimación del modelo</a></li>
<li class="chapter" data-level="20.3.3" data-path="modelización-de-series-temporales.html"><a href="modelización-de-series-temporales.html#diagnosis-validación-y-contrastación"><i class="fa fa-check"></i><b>20.3.3</b> Diagnosis, validación y contrastación</a></li>
<li class="chapter" data-level="20.3.4" data-path="modelización-de-series-temporales.html"><a href="modelización-de-series-temporales.html#predicción-2"><i class="fa fa-check"></i><b>20.3.4</b> Predicción</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliografía.html"><a href="bibliografía.html"><i class="fa fa-check"></i>Bibliografía</a></li>
<li class="chapter" data-level="21" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html"><i class="fa fa-check"></i><b>21</b> Análisis de tablas de contingencia</a>
<ul>
<li class="chapter" data-level="21.1" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#introducción-9"><i class="fa fa-check"></i><b>21.1</b> Introducción</a>
<ul>
<li class="chapter" data-level="21.1.1" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#motiv"><i class="fa fa-check"></i><b>21.1.1</b> Motivación</a></li>
<li class="chapter" data-level="21.1.2" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#notac"><i class="fa fa-check"></i><b>21.1.2</b> Notación</a></li>
<li class="chapter" data-level="21.1.3" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#diseños-experimentales-o-procedimientos-de-muestreo-que-dan-lugar-a-una-tabla-de-contingencia"><i class="fa fa-check"></i><b>21.1.3</b> Diseños experimentales o procedimientos de muestreo que dan lugar a una tabla de contingencia</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#contraste-de-independencia-en-tablas-2times-2"><i class="fa fa-check"></i><b>21.2</b> Contraste de independencia en tablas <span class="math inline">\((2\times 2)\)</span></a>
<ul>
<li class="chapter" data-level="21.2.1" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#introducción-10"><i class="fa fa-check"></i><b>21.2.1</b> Introducción</a></li>
<li class="chapter" data-level="21.2.2" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#plantgen"><i class="fa fa-check"></i><b>21.2.2</b> Planteamiento general del contraste exacto de independencia</a></li>
<li class="chapter" data-level="21.2.3" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#algoritmo"><i class="fa fa-check"></i><b>21.2.3</b> Algoritmo para la realización del contraste exacto de independencia</a></li>
<li class="chapter" data-level="21.2.4" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#contraste-de-independencia-diseño-tipo-1"><i class="fa fa-check"></i><b>21.2.4</b> Contraste de independencia: Diseño Tipo 1</a></li>
<li class="chapter" data-level="21.2.5" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#dise"><i class="fa fa-check"></i><b>21.2.5</b> Contraste de independencia: Diseños Tipo 2 y Tipo 3</a></li>
</ul></li>
<li class="chapter" data-level="21.3" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#contraste-de-independencia-en-tablas-rtimes-c"><i class="fa fa-check"></i><b>21.3</b> Contraste de independencia en tablas <span class="math inline">\(R\times C\)</span></a>
<ul>
<li class="chapter" data-level="21.3.1" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#IntroRxC"><i class="fa fa-check"></i><b>21.3.1</b> Introducción</a></li>
<li class="chapter" data-level="21.3.2" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#contaprox"><i class="fa fa-check"></i><b>21.3.2</b> Contrastes aproximados</a></li>
<li class="chapter" data-level="21.3.3" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#contraste-aproximado-con-corrección-de-continuidad-1"><i class="fa fa-check"></i><b>21.3.3</b> Contraste aproximado con corrección de continuidad</a></li>
</ul></li>
<li class="chapter" data-level="21.4" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#medidas"><i class="fa fa-check"></i><b>21.4</b> Medidas de asociación en tablas <span class="math inline">\(2\times 2\)</span></a>
<ul>
<li class="chapter" data-level="21.4.1" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#introducción-11"><i class="fa fa-check"></i><b>21.4.1</b> Introducción</a></li>
<li class="chapter" data-level="21.4.2" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#la-hatq-de-yule"><i class="fa fa-check"></i><b>21.4.2</b> La <span class="math inline">\(\hat{Q}\)</span> de Yule</a></li>
<li class="chapter" data-level="21.4.3" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#otras-medidas-de-asociación-para-tablas-2times-2"><i class="fa fa-check"></i><b>21.4.3</b> Otras medidas de asociación para tablas <span class="math inline">\(2\times 2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="21.5" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#medidas-de-asociación-en-tablas-rtimes-c"><i class="fa fa-check"></i><b>21.5</b> Medidas de asociación en tablas <span class="math inline">\(R\times C\)</span></a>
<ul>
<li class="chapter" data-level="21.5.1" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#introducción-12"><i class="fa fa-check"></i><b>21.5.1</b> Introducción</a></li>
<li class="chapter" data-level="21.5.2" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#medidas-derivadas-del-estadístico-chi-cuadrado"><i class="fa fa-check"></i><b>21.5.2</b> Medidas derivadas del estadístico Chi-cuadrado</a></li>
<li class="chapter" data-level="21.5.3" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#medidas-basadas-en-la-reducción-proporcional-del-error-lambda-de-goodman-y-kruskal"><i class="fa fa-check"></i><b>21.5.3</b> Medidas basadas en la reducción proporcional del error: <span class="math inline">\(\lambda\)</span> de Goodman y Kruskal</a></li>
<li class="chapter" data-level="21.5.4" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#determinación-de-las-fuentes-de-asociación"><i class="fa fa-check"></i><b>21.5.4</b> Determinación de las fuentes de asociación</a></li>
</ul></li>
<li class="chapter" data-level="21.6" data-path="tablas-contingencia.html"><a href="tablas-contingencia.html#contrastes-de-independencia-en-tablas-multidimensionales"><i class="fa fa-check"></i><b>21.6</b> Contrastes de independencia en tablas multidimensionales</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="correspondencias.html"><a href="correspondencias.html"><i class="fa fa-check"></i><b>22</b> Análisis de correspondencias</a>
<ul>
<li class="chapter" data-level="22.1" data-path="correspondencias.html"><a href="correspondencias.html#introducción-13"><i class="fa fa-check"></i><b>22.1</b> Introducción</a></li>
<li class="chapter" data-level="22.2" data-path="correspondencias.html"><a href="correspondencias.html#metodología-del-análisis-de-correspondencias"><i class="fa fa-check"></i><b>22.2</b> Metodología del análisis de correspondencias</a>
<ul>
<li class="chapter" data-level="22.2.1" data-path="correspondencias.html"><a href="correspondencias.html#proyecciones-fila-columna-y-simétrica"><i class="fa fa-check"></i><b>22.2.1</b> Proyecciones fila, columna y simétrica</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="correspondencias.html"><a href="correspondencias.html#ejemplos-de-análisis-de-correspondencias-con-r"><i class="fa fa-check"></i><b>22.3</b> Ejemplos de análisis de correspondencias con <strong>R</strong></a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="análisis-conjunto.html"><a href="análisis-conjunto.html"><i class="fa fa-check"></i><b>23</b> Análisis conjunto</a>
<ul>
<li class="chapter" data-level="23.1" data-path="análisis-conjunto.html"><a href="análisis-conjunto.html#introducción-conceptos-clave-y-tipos-de-análisis"><i class="fa fa-check"></i><b>23.1</b> Introducción, conceptos clave y tipos de análisis</a></li>
<li class="chapter" data-level="23.2" data-path="análisis-conjunto.html"><a href="análisis-conjunto.html#aplicación-del-análisis-conjunto-etapas"><i class="fa fa-check"></i><b>23.2</b> Aplicación del Análisis Conjunto (etapas):</a></li>
<li class="chapter" data-level="23.3" data-path="análisis-conjunto.html"><a href="análisis-conjunto.html#ejemplo-utilizando-r"><i class="fa fa-check"></i><b>23.3</b> Ejemplo utilizando R:</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="análisis-discriminante.html"><a href="análisis-discriminante.html"><i class="fa fa-check"></i><b>24</b> Análisis discriminante</a>
<ul>
<li class="chapter" data-level="24.1" data-path="análisis-discriminante.html"><a href="análisis-discriminante.html#introducción-14"><i class="fa fa-check"></i><b>24.1</b> Introducción</a></li>
<li class="chapter" data-level="24.2" data-path="análisis-discriminante.html"><a href="análisis-discriminante.html#tipos-de-análisis-discriminantes"><i class="fa fa-check"></i><b>24.2</b> Tipos de análisis discriminantes:</a></li>
<li class="chapter" data-level="24.3" data-path="análisis-discriminante.html"><a href="análisis-discriminante.html#ejemplos"><i class="fa fa-check"></i><b>24.3</b> Ejemplos:</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html"><i class="fa fa-check"></i><b>25</b> Árboles de clasificación y regresión </a>
<ul>
<li class="chapter" data-level="25.1" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#introducción-15"><i class="fa fa-check"></i><b>25.1</b> Introducción </a></li>
<li class="chapter" data-level="25.2" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#aprendizaje-con-árboles-de-decisión"><i class="fa fa-check"></i><b>25.2</b> Aprendizaje con árboles de decisión</a></li>
<li class="chapter" data-level="25.3" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#cómo-se-va-dividiendo-el-árbol"><i class="fa fa-check"></i><b>25.3</b> ¿Cómo se va dividiendo el árbol? </a>
<ul>
<li class="chapter" data-level="25.3.1" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#impureza-de-gini"><i class="fa fa-check"></i><b>25.3.1</b> Impureza de Gini</a></li>
<li class="chapter" data-level="25.3.2" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#entropía"><i class="fa fa-check"></i><b>25.3.2</b> Entropía </a></li>
<li class="chapter" data-level="25.3.3" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#ganancia-de-información"><i class="fa fa-check"></i><b>25.3.3</b> Ganancia de información</a></li>
<li class="chapter" data-level="25.3.4" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#suma-residual-de-cuadrados-mínima"><i class="fa fa-check"></i><b>25.3.4</b> Suma residual de cuadrados mínima</a></li>
</ul></li>
<li class="chapter" data-level="25.4" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#sobreajuste"><i class="fa fa-check"></i><b>25.4</b> Sobreajuste </a></li>
<li class="chapter" data-level="25.5" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#cuánto-debe-crecer-un-árbol"><i class="fa fa-check"></i><b>25.5</b> ¿Cuánto debe crecer un árbol? </a>
<ul>
<li class="chapter" data-level="25.5.1" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#la-parada-temprana"><i class="fa fa-check"></i><b>25.5.1</b> La parada temprana </a></li>
<li class="chapter" data-level="25.5.2" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#la-poda"><i class="fa fa-check"></i><b>25.5.2</b> La poda </a></li>
</ul></li>
<li class="chapter" data-level="25.6" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#el-algoritmo-id3-para-la-construcción-de-un-árbol-de-decisión"><i class="fa fa-check"></i><b>25.6</b> El algoritmo ID3 para la construcción de un árbol de decisión</a></li>
<li class="chapter" data-level="25.7" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#procedimiento-con-r-la-funcion-rpart"><i class="fa fa-check"></i><b>25.7</b> Procedimiento con R: la funcion <code>rpart</code></a></li>
<li class="chapter" data-level="25.8" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#aplicaciones-de-los-árboles-de-decisión"><i class="fa fa-check"></i><b>25.8</b> Aplicaciones de los árboles de decisión</a>
<ul>
<li class="chapter" data-level="25.8.1" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#el-caso-de-negocio"><i class="fa fa-check"></i><b>25.8.1</b> El caso de negocio</a></li>
<li class="chapter" data-level="25.8.2" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#árbol-de-clasificación-para-determinar-la-intención-de-compra"><i class="fa fa-check"></i><b>25.8.2</b> Árbol de clasificación para determinar la intención de compra</a></li>
<li class="chapter" data-level="25.8.3" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#árbol-de-regresión-para-estimar-el-número-de-días-hospitalizado"><i class="fa fa-check"></i><b>25.8.3</b> Árbol de regresión para estimar el número de días hospitalizado</a></li>
<li class="chapter" data-level="" data-path="árboles-de-clasificación-y-regresión.html"><a href="árboles-de-clasificación-y-regresión.html#resumen-5"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="26" data-path="máquinas-de-vector-soporte.html"><a href="máquinas-de-vector-soporte.html"><i class="fa fa-check"></i><b>26</b> Máquinas de Vector Soporte</a>
<ul>
<li class="chapter" data-level="26.1" data-path="máquinas-de-vector-soporte.html"><a href="máquinas-de-vector-soporte.html#introducción-16"><i class="fa fa-check"></i><b>26.1</b> Introducción</a></li>
<li class="chapter" data-level="26.2" data-path="máquinas-de-vector-soporte.html"><a href="máquinas-de-vector-soporte.html#algoritmo-svm-para-clasificación-binaria"><i class="fa fa-check"></i><b>26.2</b> Algoritmo SVM para clasificación binaria</a></li>
<li class="chapter" data-level="26.3" data-path="máquinas-de-vector-soporte.html"><a href="máquinas-de-vector-soporte.html#y-si-tengo-más-de-dos-clases"><i class="fa fa-check"></i><b>26.3</b> ¿Y si tengo más de dos clases?</a></li>
<li class="chapter" data-level="26.4" data-path="máquinas-de-vector-soporte.html"><a href="máquinas-de-vector-soporte.html#truco-del-kernel-tratando-con-la-no-linearidad"><i class="fa fa-check"></i><b>26.4</b> Truco del Kernel: Tratando con la no linearidad</a>
<ul>
<li class="chapter" data-level="26.4.1" data-path="máquinas-de-vector-soporte.html"><a href="máquinas-de-vector-soporte.html#algunos-kernels-populares"><i class="fa fa-check"></i><b>26.4.1</b> Algunos kernels populares</a></li>
</ul></li>
<li class="chapter" data-level="26.5" data-path="máquinas-de-vector-soporte.html"><a href="máquinas-de-vector-soporte.html#procedimiento-con-r-la-funcion-svm"><i class="fa fa-check"></i><b>26.5</b> Procedimiento con R: la funcion <code>svm</code></a></li>
<li class="chapter" data-level="26.6" data-path="máquinas-de-vector-soporte.html"><a href="máquinas-de-vector-soporte.html#aplicación-de-un-modelo-svm-radial-con-ajuste-automático-en-r"><i class="fa fa-check"></i><b>26.6</b> Aplicación de un modelo SVM Radial con ajuste automático en R</a>
<ul>
<li class="chapter" data-level="26.6.1" data-path="máquinas-de-vector-soporte.html"><a href="máquinas-de-vector-soporte.html#importancia-de-las-variables"><i class="fa fa-check"></i><b>26.6.1</b> Importancia de las variables</a></li>
<li class="chapter" data-level="" data-path="máquinas-de-vector-soporte.html"><a href="máquinas-de-vector-soporte.html#resumen-6"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="27" data-path="clasificador-k-vecinos-más-próximos.html"><a href="clasificador-k-vecinos-más-próximos.html"><i class="fa fa-check"></i><b>27</b> Clasificador k-vecinos más próximos</a>
<ul>
<li class="chapter" data-level="27.1" data-path="clasificador-k-vecinos-más-próximos.html"><a href="clasificador-k-vecinos-más-próximos.html#introducción-17"><i class="fa fa-check"></i><b>27.1</b> Introducción</a></li>
<li class="chapter" data-level="27.2" data-path="clasificador-k-vecinos-más-próximos.html"><a href="clasificador-k-vecinos-más-próximos.html#decisiones-a-tener-en-cuenta"><i class="fa fa-check"></i><b>27.2</b> Decisiones a tener en cuenta</a>
<ul>
<li class="chapter" data-level="27.2.1" data-path="clasificador-k-vecinos-más-próximos.html"><a href="clasificador-k-vecinos-más-próximos.html#función-de-distancia-a-utilizar"><i class="fa fa-check"></i><b>27.2.1</b> Función de distancia a utilizar</a></li>
<li class="chapter" data-level="27.2.2" data-path="clasificador-k-vecinos-más-próximos.html"><a href="clasificador-k-vecinos-más-próximos.html#número-de-vecinos-k-seleccionados"><i class="fa fa-check"></i><b>27.2.2</b> Número de vecinos (k) seleccionados</a></li>
</ul></li>
<li class="chapter" data-level="27.3" data-path="clasificador-k-vecinos-más-próximos.html"><a href="clasificador-k-vecinos-más-próximos.html#procedimiento-con-r-la-funcion-knn"><i class="fa fa-check"></i><b>27.3</b> Procedimiento con R: la funcion knn</a></li>
<li class="chapter" data-level="27.4" data-path="clasificador-k-vecinos-más-próximos.html"><a href="clasificador-k-vecinos-más-próximos.html#aplicación-del-modelo-knn-en-r"><i class="fa fa-check"></i><b>27.4</b> Aplicación del modelo KNN en R</a>
<ul>
<li class="chapter" data-level="" data-path="clasificador-k-vecinos-más-próximos.html"><a href="clasificador-k-vecinos-más-próximos.html#resumen-7"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="28" data-path="naive-bayes.html"><a href="naive-bayes.html"><i class="fa fa-check"></i><b>28</b> Naive Bayes</a>
<ul>
<li class="chapter" data-level="28.1" data-path="naive-bayes.html"><a href="naive-bayes.html#introducción-18"><i class="fa fa-check"></i><b>28.1</b> Introducción</a></li>
<li class="chapter" data-level="28.2" data-path="naive-bayes.html"><a href="naive-bayes.html#teorema-de-bayes"><i class="fa fa-check"></i><b>28.2</b> Teorema de Bayes</a></li>
<li class="chapter" data-level="28.3" data-path="naive-bayes.html"><a href="naive-bayes.html#el-algoritmo-naive-bayes"><i class="fa fa-check"></i><b>28.3</b> El algoritmo Naive Bayes</a></li>
<li class="chapter" data-level="28.4" data-path="naive-bayes.html"><a href="naive-bayes.html#procedimiento-con-r-la-funcion-naive_bayes"><i class="fa fa-check"></i><b>28.4</b> Procedimiento con R: la funcion <code>naive_bayes</code></a></li>
<li class="chapter" data-level="28.5" data-path="naive-bayes.html"><a href="naive-bayes.html#aplicación-del-modelo-naive-bayes"><i class="fa fa-check"></i><b>28.5</b> Aplicación del modelo Naive Bayes</a>
<ul>
<li class="chapter" data-level="" data-path="naive-bayes.html"><a href="naive-bayes.html#resumen-8"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="29" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html"><i class="fa fa-check"></i><b>29</b> Bagging. Random Forest </a>
<ul>
<li class="chapter" data-level="29.1" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#introducción-metodos-de-ensamble"><i class="fa fa-check"></i><b>29.1</b> Introducción: Metodos de Ensamble</a></li>
<li class="chapter" data-level="29.2" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#bagging"><i class="fa fa-check"></i><b>29.2</b> Bagging</a></li>
<li class="chapter" data-level="29.3" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#procedimiento-con-r-la-función-bagging"><i class="fa fa-check"></i><b>29.3</b> Procedimiento con R: la función <code>bagging</code> </a></li>
<li class="chapter" data-level="29.4" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#implementando-bagging-en-r"><i class="fa fa-check"></i><b>29.4</b> Implementando bagging en R</a>
<ul>
<li class="chapter" data-level="29.4.1" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#interpretación-de-variables-en-el-bagging"><i class="fa fa-check"></i><b>29.4.1</b> Interpretación de variables en el bagging</a></li>
</ul></li>
<li class="chapter" data-level="29.5" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#random-forest"><i class="fa fa-check"></i><b>29.5</b> Random Forest</a>
<ul>
<li class="chapter" data-level="29.5.1" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#número-de-árboles-k"><i class="fa fa-check"></i><b>29.5.1</b> Número de árboles (<span class="math inline">\(K\)</span>)</a></li>
<li class="chapter" data-level="29.5.2" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#número-de-variables-a-considerar-mtry"><i class="fa fa-check"></i><b>29.5.2</b> Número de variables a considerar (<span class="math inline">\(mtry\)</span>)</a></li>
<li class="chapter" data-level="29.5.3" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#complejidad-de-los-árboles"><i class="fa fa-check"></i><b>29.5.3</b> Complejidad de los árboles</a></li>
<li class="chapter" data-level="29.5.4" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#esquema-de-muestreo"><i class="fa fa-check"></i><b>29.5.4</b> Esquema de muestreo</a></li>
<li class="chapter" data-level="29.5.5" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#regla-de-división"><i class="fa fa-check"></i><b>29.5.5</b> Regla de división</a></li>
</ul></li>
<li class="chapter" data-level="29.6" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#procedimiento-con-r-la-función-randomforest"><i class="fa fa-check"></i><b>29.6</b> Procedimiento con R: la función <code>randomForest</code></a></li>
<li class="chapter" data-level="29.7" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#aplicación-del-modelo-random-forest-en-r"><i class="fa fa-check"></i><b>29.7</b> Aplicación del modelo Random Forest en R</a>
<ul>
<li class="chapter" data-level="29.7.1" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#aplicación-del-random-forest"><i class="fa fa-check"></i><b>29.7.1</b> Aplicación del Random Forest</a></li>
<li class="chapter" data-level="" data-path="bagging.-random-forest.html"><a href="bagging.-random-forest.html#resumen-9"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="30" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html"><i class="fa fa-check"></i><b>30</b> Boosting. XGBoost.</a>
<ul>
<li class="chapter" data-level="30.1" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#introducción.-boosting."><i class="fa fa-check"></i><b>30.1</b> Introducción. Boosting.</a></li>
<li class="chapter" data-level="30.2" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#gradient-boosting"><i class="fa fa-check"></i><b>30.2</b> Gradient Boosting</a>
<ul>
<li class="chapter" data-level="30.2.1" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#hiperparámetros-del-modelo-gradient-boosting"><i class="fa fa-check"></i><b>30.2.1</b> Hiperparámetros del modelo gradient boosting</a></li>
<li class="chapter" data-level="30.2.2" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#estrategia-de-ajuste-de-hiperparametros"><i class="fa fa-check"></i><b>30.2.2</b> Estrategia de ajuste de hiperparametros</a></li>
</ul></li>
<li class="chapter" data-level="30.3" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#procedimiento-con-r-la-funcion-gbm"><i class="fa fa-check"></i><b>30.3</b> Procedimiento con R: la funcion <code>gbm</code></a></li>
<li class="chapter" data-level="30.4" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#aplicación-del-modelo-gbm-en-r"><i class="fa fa-check"></i><b>30.4</b> Aplicación del modelo GBM en R</a>
<ul>
<li class="chapter" data-level="30.4.1" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#gbm-con-ajuste-automático"><i class="fa fa-check"></i><b>30.4.1</b> GBM con ajuste automático</a></li>
</ul></li>
<li class="chapter" data-level="30.5" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#extreme-gradient-boosting-xgb"><i class="fa fa-check"></i><b>30.5</b> eXtreme Gradient Boosting (XGB)</a>
<ul>
<li class="chapter" data-level="30.5.1" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#hiperparametros-del-modelo-xgboost"><i class="fa fa-check"></i><b>30.5.1</b> Hiperparametros del modelo XGBoost</a></li>
</ul></li>
<li class="chapter" data-level="30.6" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#procedimiento-con-r-la-funcion-xgboost"><i class="fa fa-check"></i><b>30.6</b> Procedimiento con R: la funcion <code>xgboost</code></a></li>
<li class="chapter" data-level="30.7" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#aplicación-del-módelo-xgboost-en-r"><i class="fa fa-check"></i><b>30.7</b> Aplicación del módelo XGBoost en R</a>
<ul>
<li class="chapter" data-level="30.7.1" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#xgboost-y-ajuste-automático"><i class="fa fa-check"></i><b>30.7.1</b> XGBoost y ajuste automático</a></li>
<li class="chapter" data-level="" data-path="boosting.-xgboost..html"><a href="boosting.-xgboost..html#resumen-10"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://blog.uclm.es/tp-mbsba/"> Ciencia de datos con R </a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Fundamentos de ciencia de datos con R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="id_130010" class="section level1 hasAnchor" number="11">
<h1><span class="header-section-number">Capítulo 11</span> Feature Selection and Engineering<a href="id_130010.html#id_130010" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><strong>Jorge Velasco López</strong></p>
<div id="introducción-6" class="section level2 hasAnchor" number="11.1">
<h2><span class="header-section-number">11.1</span> Introducción<a href="id_130010.html#introducción-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Como se indicaba en el capítulo anterior, la preparación de datos en un contexto de un poryecto de modelado predictivo, consiste en la transformación de datos sin procesar en una forma más adecuada para el modelado. Esta preparación puede ser un proceso minuciosamente laborioso e incluye tareas como la integración y limpieza de datos, como se vio en el capítulo anterior. En este capítulo se verán el resto de tareas de preprocesamiento:</p>
<ul>
<li>Selección de variables (<em>feature selection</em>): identificar aquellas variables de entrada que sean más relevantes para las fases posteriores del proceso de modelado.</li>
<li>Transformaciones de la escala o distribución de la variable objetivo.</li>
<li>Transformación de variables (<em>feature engineering</em>): derivación de nuevas variables a partir de los datos disponibles.</li>
<li>Reducción de dimensionalidad: Creación de proyecciones compactas de los datos.</li>
<li>Otro tipo de tratamientos, como el <em>split</em> de datos o el manejo de datos no balanceados.</li>
</ul>
<p>La creación de variables predictoras a partir de los datos en bruto tiene una componente creativa. Requiere además de herramientas adecuadas y de experiencia para encontrar las mejores representaciones, apoyándose en lo posible en el conocimiento que se tenga de los datos durante la exploración de los mismos.
Para seleccionar y configurar la preparación de datos, pueden usarse <strong>estadísticas descriptivas</strong>, para determinar si las operaciones de escalado pueden ser apropiadas. Las pruebas de hipótesis estadísticas se pueden utilizar para determinar si una variable coincide con una distribución de probabilidad dada. Se pueden usar diagramas para determinar si las variables están relacionadas y si lo están, en qué medida, proporcionando información sobre si una o más variables son redundantes o irrelevantes a la variable objetivo. Tambiés es posible apoyarse en visualizaciones, como <strong>gráficas de los datos</strong> para identificar si una variable tiene valores atípicos y proporcionar información sobre la distribución de probabilidad que subyace a los datos y así poder decidir si una transformación de la distribución de probabilidad de una variable sería apropiada.</p>
<p>La fase posterior de modelización, también puede informar sobre la selección y configuración de los métodos de preparación de datos. Por ejemplo, la elección de <strong>algoritmos</strong> puede imponer requisitos y expectativas sobre el tipo y forma de variables de entrada en los datos <span class="citation">(<a href="#ref-boehmke2019hands" role="doc-biblioref"><strong>boehmke2019hands?</strong></a>)</span>. Así, podría ser necesario que las variables tengan una distribución de probabilidad específica, la eliminación de variables de entrada correlacionadas y/o la eliminación de variables que no estén fuertemente relacionadas con la variable objetivo.</p>
<p>Incluso preparando los datos para cumplir con las expectativas del modelo, es posible que no se obtenga todo su potencial, dado que hay representaciones de datos que son mejores que otras. El <em>feature engineering</em> ayudará a aumentar la eficacia de un modelo.
Nótese que, para un conjunto determinado de datos y un problema concreto de modelado predictivo de clasificación o regresión, puede ser la primera vez que se realice una modelización y por tanto, sea necesario trabajar los datos de manera innovadora. Tanto es así, que se suele decir que se invierte hasta el 80% del tiempo de análisis de datos en el proceso de preparación de datos.</p>
<p>El paquete <strong><code>caret</code></strong> <span class="citation">(<a href="#ref-kuhn2008building" role="doc-biblioref"><strong>kuhn2008building?</strong></a>)</span> proporciona una interfaz unificada que simplifica el proceso de modelado empleando la mayoría de los métodos de aprendizaje estadístico implementados en <strong>R</strong>. Se puede decir que caret es un <em>meta-engine</em> (agregador) que permite aplicar casi cualquier algoritmo y se ha elegido como herramienta principal para la parte de preprocesamiento por su amplia difusión y por coherencia en la parte de <em>machine learning</em> del presente libro, que también lo utiliza. No obstante, se podrían usar otros, como el paquete <code>recipes</code> incluido en <code>tidymodels</code>. Los <em>meta-engines</em> y <code>caret</code> en particular brindan más coherencia en la forma en que especifican las entradas y se extraen las salidas, pero pueden ser menos flexibles que los algoritmos directos. En relación al preprocesamiento de los datos, además de una serie de funciones auxiliares, como <code>dummyVars()</code> y <code>rfe()</code>, incluye la función principal <code>preProcess()</code>, que ya se introdujo en el capítulo anterior y para la que el parámetro <code>method</code> permite establecer una lista de procesamientos:</p>
<ul>
<li><strong>Imputación</strong>: <code>knnImpute</code>, <code>bagImpute</code> o <code>medianImpute</code>, para imputar valores missing como se vio en el capítulo anterior.</li>
<li><strong>Creación y transformación de variables explicativas</strong>: <code>center</code> (resta la media de los valores), <code>scale</code> (divide los valores por la desviación estándar), <code>range</code> (normaliza los valores), <code>BoxCox</code> (aplica una transformada de Box-Cox siendo los valores positivos), <code>YeoJohnson</code> (aplica una transformada Yeo-Johnson), <code>expoTrans</code> (aplica una transformación de potencia), <code>spatialSign</code> (transforma los datos a un círculo unitario de <code>p</code> dimensiones).</li>
<li><strong>Selección de predictores y extracción de componentes</strong>: <code>corr</code> (correlación), <code>nzv</code> (eliminar atributos con una varianza cercana a cero), <code>zv</code> (eliminar atributos con varianza cero), <code>pca</code> (transformar datos a los componentes principales), <code>ica</code> (transformar datos a los componentes independientes).</li>
</ul>
<p>Es más probable que estas transformaciones sean útiles para algoritmos como el de regresión, métodos basados en instancias (también llamado <em>memory-based learning</em> como <code>K</code> vecinos más cercanos-KNN- y aprendizaje de cuantificación vectorial-LVQ-), máquinas de vectores de soporte-SVM- y redes neuronales-NN- y menos probable que sean útiles para métodos basados en árboles y reglas.
En <code>caret</code>, estas transformaciones pueden ser utilizadas de dos maneras: 1) <strong>Independiente</strong>: las transformaciones se pueden modelar a partir de datos de entrenamiento y aplicarse a múltiples conjuntos de datos. El modelo de la transformación se prepara utilizando la función <code>preProcess()</code> y se aplica a un conjunto de datos utilizando la función <code>predict()</code>. 2) <strong>Entrenamiento</strong>: las transformaciones se pueden preparar y aplicar automáticamente durante la evaluación del modelo. Las transformaciones aplicadas durante el entrenamiento se preparan usando <code>preProcess()</code> y se pasan a la función <code>train()</code> a través del argumento <code>preProcess</code>. En esta sección se presentan varios ejemplos de preprocesamiento de datos usando ambos métodos.</p>
<p>Se usará el dataset de <code>Madrid_Sale</code> (disponibles en el paquete de <strong>R</strong> <code>Idealista18</code>), con datos inmobiliaros del año 2018 y los datos de la tienda de comercio electrónico “Beauty eSheep” que se usa en los capítulos de <em>machine learning</em> y que se introdujo en el capítulo anterior.
Las librerías que se usan, además de las menciondas de <code>tidyverse</code>, <code>idealista18</code> y <code>caret</code>, son <code>FSelector</code> (para la selección de atributos mediante métodos <em>embedded</em>), <code>rsample</code> (para realizar muestreo), <code>gridExtra</code> (para visualizar gráficos) y <code>corrplot</code> (para visualizar correlaciones).</p>
</div>
<div id="feature-selection-selección-de-variables" class="section level2 hasAnchor" number="11.2">
<h2><span class="header-section-number">11.2</span> <em>Feature Selection</em> (Selección de variables)<a href="id_130010.html#feature-selection-selección-de-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La selección de variables es el conjunto de técnicas para seleccionar el subconjunto de variables de entrada que sea más relevante para la modelización de la predicción de variable objetivo. Esto es importante porque: 1) variables redundantes de entrada pueden distraer o engañar a los algoritmos de aprendizaje, lo que posiblemente signifique un menor rendimiento predictivo; 2) es deseable desarrollar modelos utilizando únicamente los datos que se requieren para hacer una predicción, tanto por el coste computacional como por la interpretabilidad del modelo utilizado.</p>
<p>Las técnicas de selección generalmente pueden agruparse en las que usan la variable de destino (supervisados) y los que no (no supervisados). Debido a la complejidad de la cuestión, se va a revisar únicamente las técnias supervisadas más relevantes, que se pueden dividir en: 1) los de tipo filtro, que puntúan cada variable de entrada y permiten seleccionar un subconjunto <span class="citation">(<a href="#ref-brownlee2020data" role="doc-biblioref"><strong>brownlee2020data?</strong></a>)</span>; 2) los métodos <em>wrapper</em>, que eligen las variables que dan como resultado el modelo de mejor rendimiento y 3) los modelos intrínsecos o <em>embedded</em>, que seleccionan variables automáticamente como parte del ajuste del modelo durante el entrenamiento (como algunos modelos de regresión penalizados como <code>Lasso</code> y árboles de decisión y <em>random forests</em>).</p>
<p>La selección de variables también está relacionada con las técnicas de reducción de dimensionalidad, ya que en ambos métodos se busca reducir el número de variables de entrada para un modelo predictivo. La diferencia es que la reducción de la dimensionalidad crea una proyección de los datos que dan como resultado variables de entrada completamente nuevas. Así, la reducción de la dimensionalidad es una alternativa a la selección de variables.</p>
<div id="métodos-de-selección-tipo-filtro" class="section level3 hasAnchor" number="11.2.1">
<h3><span class="header-section-number">11.2.1</span> Métodos de selección tipo Filtro<a href="id_130010.html#métodos-de-selección-tipo-filtro" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Los <strong>métodos de selección de variables de filtro</strong> usan técnicas estadísticas para evaluar la relación entre cada variable de entrada (también llamadas predictoras) y la variable de destino (también llamada objetivo o de salida). Las puntuaciones obtenidas se utilizan como base para clasificar y elegir las variables de entrada que se utilizarán en el modelo.</p>
<p>La elección de las técnicas estadísticas depende de los tipos de datos de las variables. Las medidas estadísticas utilizadas en la selección de variables basadas en filtros generalmente calculan una variable de entrada a la vez con la variable de destino. Como tales, se les conoce como medidas estadísticas univariantes.</p>
<p>En función de si la entrada/salida es numérica/categórica, se estaría tratando de un problema de modelado predictivo o de clasificación y las técnicas de selección de filtro serían diferentes. Por ejemplo, si las variables de entrada y salida fueran numéricas, sería un problema de modelado predictivo, para las que se suelen usar técnicas de coeficientes de correlación, como el de <em>Pearson</em> para una correlación lineal o métodos basados en rangos para una correlación no lineal (por ejemplo con el Coeficiente de rango de <em>Spearman</em>) y el método de Información Mutua. Si ambas fueran categóricas podría usarse la medida de correlación de chi-cuadrado.
Sin embargo, no es habitual tener un conjunto de datos con solo un tipo de variable de entrada. Un enfoque para manejar diferentes tipos de datos de variables de entrada es seleccionar por separado variables de entrada numéricas y variables de entrada categóricas usando las métricas apropiadas.</p>
<p>Uno de los aspectos fundamentales en la selección de variables es comprobar si su varianza es cero o cercana a cero. Si la varianza es cercana a cero, significa que casi todas las observaciones tienen valores similares y por tanto, esas variables podrían ser descartadas, puesto que es muy probable que solo añadan ruido al modelo. Comprobar con <code>caret</code> si las observaciones tienen <strong>varianza</strong> cero se hace con la función <code>nearZeroVar()</code>. A continuación se muestra su uso con un ejemplo del dataset <code>Madrid_Sale</code>, incluido en el paquete de <strong>R</strong> <code>Idealista18</code>.</p>
<p>En el dataset se tienen varias variables numéricas (como las correspondientes al número de baños, metros cuadrados, precio, latitud, longitud, si tiene terraza, etc.). Para comprobar si tienen o no varianza cero se puede ejecutar el siguiente código, que devuelve entre otras la variable <code>nzv=FALSE</code> (<em>near zero variance</em>) para casi todas las variables, excepto <code>PARKINGSPACEPRICE, ISDUPLEX, ISSTUDIO, ISINTOPFLOOR</code>y<code>BUILTTYPEID_1</code> que podrían descartarse para el modelo:</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="id_130010.html#cb117-1" aria-hidden="true" tabindex="-1"></a>numeric_cols <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Madrid_Sale, is.numeric)</span>
<span id="cb117-2"><a href="id_130010.html#cb117-2" aria-hidden="true" tabindex="-1"></a>variance <span class="ot">&lt;-</span> <span class="fu">nearZeroVar</span>(Madrid_Sale[numeric_cols], <span class="at">saveMetrics =</span> T)</span>
<span id="cb117-3"><a href="id_130010.html#cb117-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Con el argumento saveMetrics, se guardan los valores que se han utilizado para los cálculos.</span></span>
<span id="cb117-4"><a href="id_130010.html#cb117-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Se muestran los primeros resultados</span></span>
<span id="cb117-5"><a href="id_130010.html#cb117-5" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(variance, <span class="at">n =</span> <span class="dv">2</span>)</span>
<span id="cb117-6"><a href="id_130010.html#cb117-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;       freqRatio  percentUnique   zeroVar   nzv</span></span>
<span id="cb117-7"><a href="id_130010.html#cb117-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; PERIOD  2.019617   0.004218742   FALSE   FALSE</span></span>
<span id="cb117-8"><a href="id_130010.html#cb117-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; PRICE   1.076923   2.911986500   FALSE   FALSE</span></span></code></pre></div>
<p>Otra de las cuestiones importantes, es la <strong>correlación entre variables</strong>. Existen varios modelos, como la regresión lineal y la regresión logística para las que una de las bases del modelo es la no colinealidad o multicolinealidad, por lo que no deberían haber variables correlacionadas.</p>
<p>Para comprobarla, se puede usar la función <code>findCorrelation()</code> de <code>caret</code>a la que se le pasa una matriz de correlaciones, con lo que se obtiene qué variables habría que eliminar en caso necesario. Con el paquete de <code>corrplot</code> se genera la visualización de la Fig. <a href="id_130010.html#fig:corr">11.1</a>:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:corr"></span>
<img src="img/corridealista.png" alt="Correlación" width="60%" />
<p class="caption">
Figura 11.1: Correlación
</p>
</div>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="id_130010.html#cb118-1" aria-hidden="true" tabindex="-1"></a>madrid_cor <span class="ot">&lt;-</span> <span class="fu">cor</span>(Madrid_Sale[numeric_cols])</span>
<span id="cb118-2"><a href="id_130010.html#cb118-2" aria-hidden="true" tabindex="-1"></a><span class="fu">findCorrelation</span>(madrid_cor)</span>
<span id="cb118-3"><a href="id_130010.html#cb118-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Se visualiza</span></span>
<span id="cb118-4"><a href="id_130010.html#cb118-4" aria-hidden="true" tabindex="-1"></a>correlationMatrix <span class="ot">&lt;-</span> <span class="fu">cor</span>(madrid_cor[, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>])</span>
<span id="cb118-5"><a href="id_130010.html#cb118-5" aria-hidden="true" tabindex="-1"></a><span class="fu">corrplot</span>(correlationMatrix, <span class="at">method =</span> <span class="st">&quot;circle&quot;</span>)</span></code></pre></div>
<p>En este caso, no habría variables altamente correlacionadas, aunque sí habría que analizar principalmente la relación entre <code>CONSTRUCTEDAREA</code> y <code>BATHNUMBER.</code> Si las hubiera, habría que considerar eliminar determinadas variables. De manera similar, habría que controlar que no haya combinaciones lineales, para lo que se puede usar la función <code>findLinearCombos()</code>.</p>
<p>La colinealidad ocurre cuando los predictores del modelo están relacionados de tal manera que constituyen una combinación lineal entre sí. Aunque no empeoraría el poder predictivo del modelo, sí dificultaría la evaluación de la importancia de los predictores individuales.</p>
<p>Las variables perfectamente correlacionadas son redundantes y es posible que no agreguen valor al modelo y, si se eliminan, serían más rápidas de entrenar. Sin embargo, dos variables que no están fuertemente correlacionadas aún podrían ser importantes para el modelo. En caso de duda, lo más seguro sería entrenar el modelo y observar su rendimiento.</p>
</div>
<div id="métodos-de-selección-de-variables-tipo-wrapper" class="section level3 hasAnchor" number="11.2.2">
<h3><span class="header-section-number">11.2.2</span> Métodos de selección de variables tipo <em>wrapper</em><a href="id_130010.html#métodos-de-selección-de-variables-tipo-wrapper" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Otro enfoque consiste en utilizar un método contenedor o <em>wrapper</em> <span class="citation">(<a href="#ref-saeys2007review" role="doc-biblioref"><strong>saeys2007review?</strong></a>)</span>, que realiza una búsqueda a través de diferentes combinaciones o subconjuntos de variables de entrada para comprobar el efecto que tienen en la precisión del modelo.</p>
<p>Hay varias alternativas:</p>
<ul>
<li>Evaluar las variables individualmente y seleccionar las <span class="math inline">\(n\)</span> principales variables que obtienen una buena precisión. Sin embargo, al probar el modelo repetidamente con solo una de las variables a la vez, se pierde la información de las dependencias entre variables.</li>
<li>Observar la precisión del modelo para todas las combinaciones de variables posibles. En este sentido, se puede utilizar un algoritmo de búsqueda global estocástica, como los algoritmos genéticos. Aunque efectivos, estos enfoques pueden ser computacionalmente muy costosos, especialmente para grandes conjuntos de datos de entrenamiento y modelos más sofisticados.</li>
<li>La selección <em>forward</em> es un algoritmo simple en el que comienza con una sola variable y agrega secuencialmente la siguiente variable, actualizando el modelo y observando su rendimiento.</li>
<li>La selección <em>backward</em> es el enfoque inverso de la selección <em>forward</em>. Se comienza con todas las variables y luego se eliminan iterativamente la variable de menor rendimiento, mientras se actualiza el modelo. Esto ayuda a comprender si la última variable clasificada era realmente útil.</li>
<li>Un método automático popular para la selección de variables proporcionado por el paquete <code>caret</code>se llama “Eliminación de variables recursivas” o RFE.</li>
</ul>
<p>El siguiente ejemplo ilustra la última de las alternativas, el <strong>método RFE</strong>, en el conjunto de las variables numéricas del <code>data.frame</code> de <code>Madrid_Sale</code>. Se utiliza un algoritmo <em>Random Forest</em> en cada iteración para evaluar el modelo. El algoritmo está configurado para explorar todos los subconjuntos posibles de los atributos seleccionados. En la Fig. <a href="id_130010.html#fig:rfe">11.2</a>, se puede comprobar que en 3 atributos se estabiliza el RMSE.</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="id_130010.html#cb119-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Se toma una muestra con el paquete rsample y se seleccionan las variables de interés</span></span>
<span id="cb119-2"><a href="id_130010.html#cb119-2" aria-hidden="true" tabindex="-1"></a>Madrid_Sale_num <span class="ot">&lt;-</span> Madrid_Sale[, <span class="dv">3</span><span class="sc">:</span><span class="dv">9</span>] <span class="sc">|&gt;</span> <span class="fu">select</span>(<span class="sc">-</span>UNITPRICE)</span>
<span id="cb119-3"><a href="id_130010.html#cb119-3" aria-hidden="true" tabindex="-1"></a>Madrid_Sale_num_sample <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(Madrid_Sale_num), <span class="at">size =</span> <span class="dv">5000</span>, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb119-4"><a href="id_130010.html#cb119-4" aria-hidden="true" tabindex="-1"></a>Madrid_Sale_num_sample <span class="ot">&lt;-</span> Madrid_Sale_num[Madrid_Sale_num_sample, ]</span>
<span id="cb119-5"><a href="id_130010.html#cb119-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Para asegurar que los resultados sean repetibles</span></span>
<span id="cb119-6"><a href="id_130010.html#cb119-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">7</span>)</span>
<span id="cb119-7"><a href="id_130010.html#cb119-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Definir el control usando una función de selección de random forest</span></span>
<span id="cb119-8"><a href="id_130010.html#cb119-8" aria-hidden="true" tabindex="-1"></a>control <span class="ot">&lt;-</span> <span class="fu">rfeControl</span>(<span class="at">functions =</span> rfFuncs, <span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>)</span>
<span id="cb119-9"><a href="id_130010.html#cb119-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Ejecutar el algoritmo RFE sobre el dataset de Sacramento de solo variables numéricas</span></span>
<span id="cb119-10"><a href="id_130010.html#cb119-10" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">rfe</span>(Madrid_Sale_num_sample[, <span class="dv">2</span><span class="sc">:</span><span class="dv">6</span>], Madrid_Sale_num_sample[, <span class="dv">1</span>], <span class="at">sizes =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>), <span class="at">rfeControl =</span> control)</span>
<span id="cb119-11"><a href="id_130010.html#cb119-11" aria-hidden="true" tabindex="-1"></a><span class="co"># sumarizar los resultados</span></span>
<span id="cb119-12"><a href="id_130010.html#cb119-12" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(results)</span>
<span id="cb119-13"><a href="id_130010.html#cb119-13" aria-hidden="true" tabindex="-1"></a><span class="co"># listar las variables resultantes</span></span>
<span id="cb119-14"><a href="id_130010.html#cb119-14" aria-hidden="true" tabindex="-1"></a><span class="fu">predictors</span>(results)</span>
<span id="cb119-15"><a href="id_130010.html#cb119-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Graficar los resultados</span></span>
<span id="cb119-16"><a href="id_130010.html#cb119-16" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(results, <span class="at">type =</span> <span class="fu">c</span>(<span class="st">&quot;g&quot;</span>, <span class="st">&quot;o&quot;</span>))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rfe"></span>
<img src="img/rfeidealista.png" alt="Selección RFE" width="60%" />
<p class="caption">
Figura 11.2: Selección RFE
</p>
</div>
</div>
<div id="métodos-de-selección-tipo-embedded" class="section level3 hasAnchor" number="11.2.3">
<h3><span class="header-section-number">11.2.3</span> Métodos de selección tipo Embedded<a href="id_130010.html#métodos-de-selección-tipo-embedded" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Finalmente, hay algunos algoritmos de aprendizaje automático que realizan la selección automática de funciones como parte del aprendizaje del modelo. Se podría aludir a estas técnicas como <strong>métodos de selección de variables intrínsecas (o <em>embedded</em>)</strong>. Esto incluye algoritmos como modelos de regresión penalizados como Lasso y árboles de decisión, incluidos los <em>random forests</em>.</p>
<p>El siguiente ejemplo carga el conjunto de datos de <code>Madrid_Sale</code> con una selección de variables numéricas (<code>PRICE, CONSTRUCTEDAREA, ROOMNUMBER, BATHNUMBER, HASTERRACE, HASLIFT</code>) y construye un modelo de aprendizaje de cuantificación vectorial o LVQ, <span class="citation">(<a href="#ref-kohonen1995learning" role="doc-biblioref"><strong>kohonen1995learning?</strong></a>)</span>. Luego, se usa la función <code>varImp()</code> para estimar la importancia de la variable, que se muestra en consola y se representa gráficamente. En la Fig. <a href="id_130010.html#fig:embeddedidealista">11.3</a> se muestra que entre los atributos analizados, los de <code>CONSTRUCTEDAREA, BATHNUMBER y ROOMNUMBER</code> son los 3 más importantes del conjunto de datos para todas las categorías y <code>HASTERRACE</code> es el menos importante, para la variable objetivo <code>PRICE</code>, transformada como se explicará en los siguientes apartados mediante <em>binning</em>.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="id_130010.html#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Para asegurar que los resultados sean repetibles</span></span>
<span id="cb120-2"><a href="id_130010.html#cb120-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">7</span>)</span>
<span id="cb120-3"><a href="id_130010.html#cb120-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Preparar el esquema de entrenamiento en este caso &quot;repeated CV&quot;</span></span>
<span id="cb120-4"><a href="id_130010.html#cb120-4" aria-hidden="true" tabindex="-1"></a>control <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>, <span class="at">repeats =</span> <span class="dv">3</span>)</span>
<span id="cb120-5"><a href="id_130010.html#cb120-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Realizar el *Binning* con un número determinado de bins</span></span>
<span id="cb120-6"><a href="id_130010.html#cb120-6" aria-hidden="true" tabindex="-1"></a>df3 <span class="ot">&lt;-</span> Madrid_Sale_num_sample <span class="sc">|&gt;</span></span>
<span id="cb120-7"><a href="id_130010.html#cb120-7" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">mutate</span>(<span class="at">price_bin =</span> <span class="fu">cut</span>(PRICE, <span class="at">breaks =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">250000</span>, <span class="dv">500000</span>, <span class="dv">750000</span>, <span class="dv">10000000</span>), <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;0k_250k&quot;</span>, <span class="st">&quot;250k_500k&quot;</span>, <span class="st">&quot;500k_750k&quot;</span>, <span class="st">&quot;&gt;750k&quot;</span>), <span class="at">include.lowest =</span> <span class="cn">TRUE</span>)) <span class="sc">|&gt;</span></span>
<span id="cb120-8"><a href="id_130010.html#cb120-8" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>PRICE)</span>
<span id="cb120-9"><a href="id_130010.html#cb120-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Entrenar el modelo:</span></span>
<span id="cb120-10"><a href="id_130010.html#cb120-10" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">train</span>(price_bin <span class="sc">~</span> ., <span class="at">data =</span> df3, <span class="at">method =</span> <span class="st">&quot;lvq&quot;</span>, <span class="at">preProcess =</span> <span class="st">&quot;scale&quot;</span>, <span class="at">trControl =</span> control)</span>
<span id="cb120-11"><a href="id_130010.html#cb120-11" aria-hidden="true" tabindex="-1"></a>importancia <span class="ot">&lt;-</span> <span class="fu">varImp</span>(model, <span class="at">scale =</span> <span class="cn">FALSE</span>) <span class="co"># Estimar la importancia de la variable</span></span>
<span id="cb120-12"><a href="id_130010.html#cb120-12" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(importancia) <span class="co"># Graficar la importancia</span></span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:embeddedidealista"></span>
<img src="img/embeddedidealista.png" alt="Método Embedded" width="60%" />
<p class="caption">
Figura 11.3: Método Embedded
</p>
</div>
</div>
</div>
<div id="transformaciones-de-escala-y-de-la-distribución-de-la-variable-objetivo" class="section level2 hasAnchor" number="11.3">
<h2><span class="header-section-number">11.3</span> Transformaciones de escala y de la distribución de la variable objetivo<a href="id_130010.html#transformaciones-de-escala-y-de-la-distribución-de-la-variable-objetivo" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="id_31" class="section level3 hasAnchor" number="11.3.1">
<h3><span class="header-section-number">11.3.1</span> Transformaciones de la variable objetivo<a href="id_130010.html#id_31" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Aunque no siempre es necesario, la transformación de la variable objetivo puede llevar a una mejora predictiva, especialmente con modelos paramétricos. Por ejemplo, los modelos de regresión lineal ordinarios asumen que los errores de predicción (y por lo tanto la respuesta) se distribuyen normalmente. Pero puede ocurrir que la variable objetivo tenga valores atípicos y la suposición de normalidad no se cumpla.</p>
<p>Para minimizar la asimetría de la variable respuesta, se puede usar una transformación log (u otra). Esta sería una alternativa al uso de la función de pérdida del “error logarítmico cuadrático medio” (RMSLE) como medida de evaluación del modelo.</p>
<p>Existen dos enfoques principales para ayudar a corregir las variables objetivo con sesgo positivo:</p>
<ul>
<li>Normalizar con una transformación log. Esto transformará la mayoría de las distribuciones sesgadas a la derecha para que sean aproximadamente normales. Sin embargo, hay que considerar el preprocesamiento como la creación de un flujo de trabajo que pueda revisarse y volver a usarse. Para esto, se puede usar el paquete <code>recipes</code> o la función <code>caret::preProcess()</code> <a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a>. En la Fig. <a href="id_130010.html#fig:log">11.4</a>, se puede ver que tomar logaritmos da buen resultado para normalizar esta distribución.</li>
</ul>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="id_130010.html#cb121-1" aria-hidden="true" tabindex="-1"></a>respuesta_log <span class="ot">&lt;-</span> <span class="fu">log</span>(Madrid_Sale<span class="sc">$</span>PRICE)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:log"></span>
<img src="img/logidealista.png" alt="Variable respuesta log" width="60%" />
<p class="caption">
Figura 11.4: Variable respuesta log
</p>
</div>
<ul>
<li>Como segunda opción se puede usar una transformación de Box-Cox, que es más flexible que la transformación logarítmica y se puede encontrar la función adecuada a partir de una familia de transformadas de potencia, que transformarán la variable a lo mas parecido a una distribución normal <span class="citation">(<a href="#ref-sakia1992box" role="doc-biblioref"><strong>sakia1992box?</strong></a>)</span> <a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a>.</li>
</ul>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="id_130010.html#cb122-1" aria-hidden="true" tabindex="-1"></a>respuesta_boxcox <span class="ot">&lt;-</span> <span class="fu">preProcess</span>(Madrid_Sale_num_sample, <span class="at">method =</span> <span class="st">&quot;BoxCox&quot;</span>)</span>
<span id="cb122-2"><a href="id_130010.html#cb122-2" aria-hidden="true" tabindex="-1"></a>trainBC <span class="ot">&lt;-</span> <span class="fu">predict</span>(respuesta_boxcox, Madrid_Sale_num_sample)</span>
<span id="cb122-3"><a href="id_130010.html#cb122-3" aria-hidden="true" tabindex="-1"></a>respuesta_boxcox</span>
<span id="cb122-4"><a href="id_130010.html#cb122-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Created from 5000 samples and 2 variables</span></span>
<span id="cb122-5"><a href="id_130010.html#cb122-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;</span></span>
<span id="cb122-6"><a href="id_130010.html#cb122-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Pre-processing:</span></span>
<span id="cb122-7"><a href="id_130010.html#cb122-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; - Box-Cox transformation (2)</span></span>
<span id="cb122-8"><a href="id_130010.html#cb122-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; - ignored (0)</span></span>
<span id="cb122-9"><a href="id_130010.html#cb122-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;</span></span>
<span id="cb122-10"><a href="id_130010.html#cb122-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Lambda estimates for Box-Cox transformation:</span></span>
<span id="cb122-11"><a href="id_130010.html#cb122-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; -0.3, -0.3</span></span></code></pre></div>
<p>Hay que tener en cuenta que cuando se modela con una variable objetivo transformada, las predicciones también estarán en la escala transformada. Es posible que haya que deshacer (o volver a transformar) los valores pronosticados a su escala original para que los responsables de la toma de decisiones puedan interpretar los resultados más fácilmente.</p>
<div class="infobox">
<p><strong>NOTA</strong></p>
<p>El paquete <code>recipes</code>, incluido en <code>tidymodels</code>, permite desarrollar el modelo de transformación de variables de forma secuencial. La idea detrás del paquete es similar a <code>caret::preProcess()</code>, donde se quiere crear el modelo de preprocesamiento pero aplicarlo más tarde y dentro de cada remuestreo. Hay tres pasos principales para crear y aplicar la transformación de variables con recipes, <em>recipe</em> (donde se definen los pasos para crear el plan), <em>prep</em> (donde se estiman los parámetros en función de los datos de entrenamiento) y <em>bake</em> (donde se aplica el modelo a nuevos datos). Sin embargo, a diferencia de <code>caret</code>, no manejan automáticamente las variables categóricas y se requiere crear variables ficticias manualmente.</p>
</div>
</div>
<div id="escalado-de-datos" class="section level3 hasAnchor" number="11.3.2">
<h3><span class="header-section-number">11.3.2</span> Escalado de datos<a href="id_130010.html#escalado-de-datos" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>También hay que considerar la escala en la que se miden las variables individuales.
Los modelos que incorporan funciones lineales en las variables de entrada, son sensibles a la escala de esas variables. Muchos algoritmos usan estas funciones, algunas de manera más obvia (como Modelos Lineales Generalizados-GLM- y regresión regularizada) que otras (como redes neuronales-NN-, SVM y análisis de componentes principales). Otros ejemplos incluyen algoritmos que utilizan medidas de distancia, como la distancia euclidia (p. ej., KNN, agrupación de k-medias y agrupación jerárquica).
Para estos modelos, a menudo es aconsejable estandarizar las variables de entrada. Las funciones de estandarización incluyen el centrado y la escala para que las variables numéricas tengan una media cero y una varianza unitaria, lo que proporciona una unidad de medida comparable común en todas las variables.</p>
<p>La función <code>preProcess()</code> de <code>caret</code> acepta dos tipos principales: <code>center</code>, que resta el promedio a los valores (todos tendrán promedio 0); <code>scale</code>, que divide los valores entre la desviación estándar (todos tendrán desviación típica 1) y <code>range</code>, que normaliza los datos, haciendo que éstos tengan un rango de 0 a 1.</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="id_130010.html#cb123-1" aria-hidden="true" tabindex="-1"></a>preprocess <span class="ot">&lt;-</span> <span class="fu">preProcess</span>(Madrid_Sale_num, <span class="at">method =</span> <span class="st">&quot;center&quot;</span>)</span>
<span id="cb123-2"><a href="id_130010.html#cb123-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">predict</span>(preprocess, Madrid_Sale_num)[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>], <span class="at">n =</span> <span class="dv">3</span>)</span>
<span id="cb123-3"><a href="id_130010.html#cb123-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;       PRICE    CONSTRUCTEDAREA     ROOMNUMBER</span></span>
<span id="cb123-4"><a href="id_130010.html#cb123-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 -270110.11       -54.39716       -1.5808996</span></span>
<span id="cb123-5"><a href="id_130010.html#cb123-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2 -161110.11       -47.39716       -1.5808996</span></span>
<span id="cb123-6"><a href="id_130010.html#cb123-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 3  -23110.11       -26.39716       -0.5808996</span></span></code></pre></div>
<p>Con esta función y para este caso, <code>caret</code> centra los datos de 6 variables, correspondientes a las variables numéricas seleccionadas. La función <code>preProcess()</code> no está pensada para transformar los datos en el momento, sino para hacer la transformación en el proceso de entrenamiento o de predicción. Para ello, en este caso se pasa el preprocesamiento y los datos a la función <code>predict()</code> devolviendo caret todos los datos con el procesamiento ya aplicado (en este caso, habiendo restado la media).
La combinación de las transformaciones de scale y center puede servir para estandarizar los datos. Los atributos tendrán un valor medio de 0 y una desviación estándar de 1 denominándose estandarización <strong>z-score</strong>. Para ello, se usaría la sentencia <code>method=c("center", "scale")</code>. Otra estandarización ampliamente usada es la <strong>min-max</strong>.</p>
<p>Cuando se estandariza, hay que hacerlo tanto en los datos de entrenamiento (<em>train</em>) como en los de prueba (<em>test</em>), para que se basen en la misma media y varianza. En todo caso hay que
tener en cuenta que nunca deben de usarse los datos que se han apartado para el conjunto de <em>test</em> en el proceso de estandarización.</p>
</div>
</div>
<div id="feature-engineering" class="section level2 hasAnchor" number="11.4">
<h2><span class="header-section-number">11.4</span> <em>Feature engineering</em><a href="id_130010.html#feature-engineering" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La mayoría de los modelos requieren que los predictores tomen forma numérica, con alguna excepción. Por ejemplo, los modelos basados en árboles manejan de manera natural variables numéricas o categóricas. Pero incluso estos modelos pueden mejorarse con el uso del preprocesamiento de variables categóricas. Las siguientes secciones se centran en algunos de los enfoques más comunes para diseñar variables categóricas.</p>
<p>Estos métodos generan nuevas variables, lo que aumenta la precisión del modelo y las predicciones generales. Son de dos tipos, de Agrupamiento o <em>Binning</em>, donde se crean agrupaciones (o <em>bins</em>) para variables continuas y de Codificación, donde las variables numéricas se forman a partir de variables categóricas.</p>
<div id="binning" class="section level3 hasAnchor" number="11.4.1">
<h3><span class="header-section-number">11.4.1</span> <em>Binning</em><a href="id_130010.html#binning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><em>Binning</em> es el agrupamiento que se realiza para crear agrupamientos de variables continuas convirtiéndolas en categóricas. Hay dos tipos de agrupamiento, 1) no supervisado (agrupamiento automático o manual) y 2) supervisado, que implica la creación de <em>bins</em> para la variable continua mientras se tiene en cuenta también la variable objetivo.
Sin embargo, el <em>Binning</em> debe usarse con moderación, ya que puede haber una pérdida en el rendimiento del modelo <span class="citation">(<a href="#ref-kuhn2013applied" role="doc-biblioref"><strong>kuhn2013applied?</strong></a>)</span>. En el ejemplo que se mostraba en apartados anteriores, se hacía <em>Binning</em> sobre la variable de <code>PRICE</code> para emplear un método de selección de variables tipo <em>embedded</em>. A continuación, se muestra una forma alternativa de hacer el <em>bin</em>.</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="id_130010.html#cb124-1" aria-hidden="true" tabindex="-1"></a>df2 <span class="ot">&lt;-</span> Madrid_Sale_num_sample <span class="sc">|&gt;</span></span>
<span id="cb124-2"><a href="id_130010.html#cb124-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">price_bin =</span> <span class="fu">cut</span>(PRICE, <span class="at">breaks =</span> <span class="dv">4</span>)) <span class="sc">|&gt;</span></span>
<span id="cb124-3"><a href="id_130010.html#cb124-3" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>PRICE)</span>
<span id="cb124-4"><a href="id_130010.html#cb124-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(df2, <span class="at">n =</span> <span class="dv">2</span>)</span>
<span id="cb124-5"><a href="id_130010.html#cb124-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;       CONSTRUCTEDAREA ROOMNUMBER BATHNUMBER HASTERRACE HASLIFT</span></span>
<span id="cb124-6"><a href="id_130010.html#cb124-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 32694              43          2          1          0       0</span></span>
<span id="cb124-7"><a href="id_130010.html#cb124-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 31262             149          4          3          0       1</span></span>
<span id="cb124-8"><a href="id_130010.html#cb124-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                 price_bin</span></span>
<span id="cb124-9"><a href="id_130010.html#cb124-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 32694 (1.71e+04,1.76e+06]</span></span>
<span id="cb124-10"><a href="id_130010.html#cb124-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 31262 (1.71e+04,1.76e+06]</span></span></code></pre></div>
</div>
<div id="codificación" class="section level3 hasAnchor" number="11.4.2">
<h3><span class="header-section-number">11.4.2</span> Codificación<a href="id_130010.html#codificación" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Muchos modelos requieren que todas las variables predictoras sean numéricas. En consecuencia, se puede necesitar transformar las variables categóricas en representaciones numéricas para que estos algoritmos puedan procesarse. La codificación es el proceso en el que se crean variables numéricas a partir de variables categóricas. Es de dos tipos, 1) <strong>Codificación de etiquetas</strong> (implica asignar a cada etiqueta un entero o valor único según el orden alfabético, que es la codificación más popular y ampliamente utilizada) y 2) <strong>Codificación <em>One-hot</em></strong> (implica la creación de variables adicionales o <em>dummies</em> sobre la base de valores únicos para cada categoría de la variable categórica). Esta última se suele usar cuando se agrupan categorías que ocurren con poca frecuencia y en general, muchos algoritmos trabajan sobre ellas de manera eficiente.
Para crear variables <em>dummies</em> con <code>caret</code>, se puede usar la función <code>dummyVars()</code> y aplicar un <code>predict()</code> para obtener los datos resultantes. La estrategia que implementa es crear una columna para cada valor distinto que exista en la variable que se está codificando y, para cada registro, marca con un <span class="math inline">\(1\)</span> la columna a la que pertenezca dicho registro y deja las demás con <span class="math inline">\(0\)</span>.
Para el dataset de la tienda de comercio electrónico “Beauty eSheep”, se haría lo siguiente.</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="id_130010.html#cb125-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Se obtiene el modelo de *one hot encoding*</span></span>
<span id="cb125-2"><a href="id_130010.html#cb125-2" aria-hidden="true" tabindex="-1"></a><span class="co"># se aplica el modelo para obtener un data.frame con las variables ya modificadas</span></span>
<span id="cb125-3"><a href="id_130010.html#cb125-3" aria-hidden="true" tabindex="-1"></a><span class="co"># se debe hacer lo mismo con los de test.</span></span>
<span id="cb125-4"><a href="id_130010.html#cb125-4" aria-hidden="true" tabindex="-1"></a>dp_dummies <span class="ot">&lt;-</span> <span class="fu">dummyVars</span>(<span class="st">&quot; ~ .&quot;</span>, <span class="at">data =</span> df_pre_dummies)</span>
<span id="cb125-5"><a href="id_130010.html#cb125-5" aria-hidden="true" tabindex="-1"></a>df_pos_dummies <span class="ot">&lt;-</span> <span class="fu">predict</span>(dp_dummies, <span class="at">newdata =</span> dp_entr)</span></code></pre></div>
<p>Adicionalmente, en este mismo ejemplo, se crea un <code>data.frame</code> uniendo estas variables <code>df_pos_dummies</code> con el resto de variables del modelo:</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="id_130010.html#cb126-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Se obtiene la tabla con el resto de variables que no se han transformado: las variables que son numéricas y la clase objetivo</span></span>
<span id="cb126-2"><a href="id_130010.html#cb126-2" aria-hidden="true" tabindex="-1"></a>df_pos_resto <span class="ot">&lt;-</span> dp_entr <span class="sc">|&gt;</span></span>
<span id="cb126-3"><a href="id_130010.html#cb126-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span><span class="fu">starts_with</span>(<span class="fu">c</span>(<span class="st">&quot;des_&quot;</span>, <span class="st">&quot;ind_&quot;</span>)))</span>
<span id="cb126-4"><a href="id_130010.html#cb126-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Se obtiene así el data.frame todas las variables numéricas: las columnas de indicadores, las columnas dummies y el resto de columnas no transformadas (la clase objetivo queda al final siendo un factor):</span></span>
<span id="cb126-5"><a href="id_130010.html#cb126-5" aria-hidden="true" tabindex="-1"></a>dp_entr_num <span class="ot">&lt;-</span> df_pos_ind <span class="sc">|&gt;</span></span>
<span id="cb126-6"><a href="id_130010.html#cb126-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(df_pos_dummies, df_pos_resto)</span></code></pre></div>
</div>
</div>
<div id="reducción-de-dimensionalidad" class="section level2 hasAnchor" number="11.5">
<h2><span class="header-section-number">11.5</span> Reducción de dimensionalidad<a href="id_130010.html#reducción-de-dimensionalidad" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>La reducción de dimensionalidad es un enfoque alternativo para filtrar las variables no informativas sin eliminarlas mediante el <em>feature selection</em>, que generalmente se usa para variables numéricas. Como se explica en el capítulo de <strong><em>ref(ACP)</em></strong>, el espacio de un conjunto de variables podría reducirse proyectándolo a un subespacio de variables de menor dimensión utilizando componentes principales.</p>
<p>Para aplicar un análisis de componentes principales (PCA por sus siglas en inglés) en <strong>R</strong> con <code>caret</code>, se indica el valor <code>pca</code> al parámetro method de la función <code>preProcess()</code>. Asimismo, con el parámetro <code>thresh</code> se puede indicar el porcentaje de la variabilidad deseado. Igualmente, se podrían combinar métodos, por ejemplo haciendo <code>method=c("center", "scale", "pca")</code>.</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="id_130010.html#cb127-1" aria-hidden="true" tabindex="-1"></a>pre_pca <span class="ot">&lt;-</span> <span class="fu">preProcess</span>(Madrid_Sale_num, <span class="at">method =</span> <span class="st">&quot;pca&quot;</span>, <span class="at">thresh =</span> <span class="fl">0.8</span>)</span>
<span id="cb127-2"><a href="id_130010.html#cb127-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(pre_pca, Madrid_Sale_num)</span></code></pre></div>
</div>
<div id="otras-transformaciones" class="section level2 hasAnchor" number="11.6">
<h2><span class="header-section-number">11.6</span> Otras transformaciones<a href="id_130010.html#otras-transformaciones" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="particionado-de-datos" class="section level3 hasAnchor" number="11.6.1">
<h3><span class="header-section-number">11.6.1</span> Particionado de datos<a href="id_130010.html#particionado-de-datos" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Un objetivo principal del proceso de aprendizaje automático es encontrar el algoritmo que prediga con mayor precisión los valores futuros en función de un conjunto de variables. En otras palabras, un algoritmo que, no solo se ajuste bien a nuestros datos pasados sino, lo que es más importante, que prediga un resultado futuro con precisión. Esto se llama la generalización del algoritmo.</p>
<p>Para proporcionar una comprensión precisa de la generalización del modelo óptimo final, se pueden dividir los datos en conjuntos de datos de entrenamiento y prueba:</p>
<ul>
<li><strong>Conjunto de entrenamiento (<em>train</em>)</strong>: estos datos se utilizan para desarrollar conjuntos de funciones, entrenar los algoritmos, ajustar hiperparámetros, comparar modelos y todas las demás actividades necesarias para elegir un modelo final.</li>
<li><strong>Conjunto de prueba (<em>test</em>)</strong>: habiendo elegido un modelo final, estos datos se utilizan para estimar una evaluación imparcial del rendimiento del modelo, que se puede denominar” error de generalización”.</li>
</ul>
<p>Dada una cantidad fija de datos, las recomendaciones típicas para dividir los datos en divisiones de prueba de entrenamiento incluyen 60 % (entrenamiento)–40 % (<code>test</code>), 70 %–30 % u 80 %–20 %. Las dos formas más comunes de dividir los datos incluyen el muestreo aleatorio simple y el muestreo estratificado, que explican en detalle en capítulos dedicados de este libro.</p>
<div id="muestreo-aleatorio-simple" class="section level4 hasAnchor" number="11.6.1.1">
<h4><span class="header-section-number">11.6.1.1</span> Muestreo aleatorio simple<a href="id_130010.html#muestreo-aleatorio-simple" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>La forma más sencilla de dividir los datos en conjuntos de entrenamiento y <code>test</code> es tomar una muestra aleatoria simple. Esto no controla ningún atributo de datos, como la distribución de su variable objetivo (<span class="math inline">\(Y\)</span>). Hay varias formas de dividir nuestros datos en <strong>R</strong>. Por ejemplo, con <code>caret</code> se haría lo siguiente para producir una división de 70 a 30 en los datos:</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="id_130010.html#cb128-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Usando el paquete caret</span></span>
<span id="cb128-2"><a href="id_130010.html#cb128-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co"># para permitir reproductirlo</span></span>
<span id="cb128-3"><a href="id_130010.html#cb128-3" aria-hidden="true" tabindex="-1"></a>index <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(Madrid_Sale_num<span class="sc">$</span>PRICE, <span class="at">p =</span> <span class="fl">0.7</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)</span>
<span id="cb128-4"><a href="id_130010.html#cb128-4" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> Madrid_Sale_num[index, ]</span>
<span id="cb128-5"><a href="id_130010.html#cb128-5" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> Madrid_Sale_num[<span class="sc">-</span>index, ]</span>
<span id="cb128-6"><a href="id_130010.html#cb128-6" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(Madrid_Sale_num) <span class="co"># 94815</span></span>
<span id="cb128-7"><a href="id_130010.html#cb128-7" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(train) <span class="co"># 66373</span></span>
<span id="cb128-8"><a href="id_130010.html#cb128-8" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(test) <span class="co"># 28442</span></span></code></pre></div>
</div>
<div id="muestreo-estratificado" class="section level4 hasAnchor" number="11.6.1.2">
<h4><span class="header-section-number">11.6.1.2</span> Muestreo estratificado<a href="id_130010.html#muestreo-estratificado" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Si se desea controlar el muestreo para que los conjuntos de entrenamiento y prueba tengan similares distribuciones, se puede usar muestreo estratificado. Esto es habitual en problemas de clasificación donde la variable objetivo puede estar notablemente desbalanceada (por ejemplo, 90% de las observaciones con respuesta “Sí” y 10% con respuesta “No”). Sin embargo, también se puede aplicar el muestreo estratificado a problemas de regresión para conjuntos de datos que tienen un tamaño de muestra pequeño y donde la variable objetivo se desvía mucho de la normalidad.</p>
<p>La forma más sencilla de realizar un muestreo estratificado en una variable objetivo es usar el paquete <code>rsample</code>, donde se especifica la variable objetivo para estratificar. Lo siguiente ilustra que en nuestros datos originales la variable del precio con <em>binning</em> (<code>price_bin</code>) tiene una respuesta desbalanceada. Al aplicar el muestreo estratificado, tanto los conjuntos de entrenamiento como de prueba tienen distribuciones de respuesta aproximadamente iguales.</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="id_130010.html#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(df3<span class="sc">$</span>price_bin) <span class="sc">|&gt;</span> <span class="fu">prop.table</span>()</span>
<span id="cb129-2"><a href="id_130010.html#cb129-2" aria-hidden="true" tabindex="-1"></a><span class="co">#         0k_250k 250k_500k 500k_750k     &gt;750k</span></span>
<span id="cb129-3"><a href="id_130010.html#cb129-3" aria-hidden="true" tabindex="-1"></a><span class="co">#         0.4776    0.2914    0.1132    0.1178</span></span>
<span id="cb129-4"><a href="id_130010.html#cb129-4" aria-hidden="true" tabindex="-1"></a>split_strat <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(df3, <span class="at">prop =</span> <span class="fl">0.7</span>, <span class="at">strata =</span> <span class="st">&quot;price_bin&quot;</span>)</span>
<span id="cb129-5"><a href="id_130010.html#cb129-5" aria-hidden="true" tabindex="-1"></a>train_strat <span class="ot">&lt;-</span> <span class="fu">training</span>(split_strat)</span>
<span id="cb129-6"><a href="id_130010.html#cb129-6" aria-hidden="true" tabindex="-1"></a>test_strat <span class="ot">&lt;-</span> <span class="fu">testing</span>(split_strat)</span>
<span id="cb129-7"><a href="id_130010.html#cb129-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Tasa de respuesta consistente entre los datos train y test</span></span>
<span id="cb129-8"><a href="id_130010.html#cb129-8" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(train_strat<span class="sc">$</span>price_bin) <span class="sc">|&gt;</span> <span class="fu">prop.table</span>()</span>
<span id="cb129-9"><a href="id_130010.html#cb129-9" aria-hidden="true" tabindex="-1"></a><span class="co">#   0k_250k 250k_500k 500k_750k     &gt;750k</span></span>
<span id="cb129-10"><a href="id_130010.html#cb129-10" aria-hidden="true" tabindex="-1"></a><span class="co">#   0.4777015 0.2913093 0.1132075 0.1177816</span></span>
<span id="cb129-11"><a href="id_130010.html#cb129-11" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(test_strat<span class="sc">$</span>price_bin) <span class="sc">|&gt;</span> <span class="fu">prop.table</span>()</span>
<span id="cb129-12"><a href="id_130010.html#cb129-12" aria-hidden="true" tabindex="-1"></a><span class="co">#   0k_250k 250k_500k 500k_750k     &gt;750k</span></span>
<span id="cb129-13"><a href="id_130010.html#cb129-13" aria-hidden="true" tabindex="-1"></a><span class="co">#   0.4773635 0.2916112 0.1131824 0.1178429</span></span></code></pre></div>
<p>Para el ejemplo que se presenta en este libro para el <em>machine learning</em> de la tienda de comercio electrónico “eSheep” se obtenía, después de un proceso de integración descrito en el capítulo anterior, el <code>data.frame</code> <code>dp_fac_cliente_venta</code>, a partir del cual se obtenía <code>dp_entr</code> (datos de entrenamiento de los modelos en su escala original) y <code>dp_test</code> (datos para prueba de los modelos en su escala original):</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="id_130010.html#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Se parte en entrenamiento y test usando muestreo estratificado por la variable objetivo</span></span>
<span id="cb130-2"><a href="id_130010.html#cb130-2" aria-hidden="true" tabindex="-1"></a>trainIndex <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(dp_fac_cliente_venta_pro13[, dep], <span class="at">p =</span> .<span class="dv">8</span>, <span class="at">list =</span> F, <span class="at">times =</span> <span class="dv">1</span>)</span>
<span id="cb130-3"><a href="id_130010.html#cb130-3" aria-hidden="true" tabindex="-1"></a>dp_entr <span class="ot">&lt;-</span> dp_fac_cliente_venta_pro13[trainIndex, ]</span>
<span id="cb130-4"><a href="id_130010.html#cb130-4" aria-hidden="true" tabindex="-1"></a>dp_test <span class="ot">&lt;-</span> dp_fac_cliente_venta_pro13[<span class="sc">-</span>trainIndex, ]</span>
<span id="cb130-5"><a href="id_130010.html#cb130-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Una selección de variables numéricas da como resultado los data.frames dp_entr_num y dp_test_num, para los conjuntos de entrenamiento y test, respectivamente.</span></span></code></pre></div>
</div>
</div>
<div id="técnicas-para-manejar-datos-no-balanceados" class="section level3 hasAnchor" number="11.6.2">
<h3><span class="header-section-number">11.6.2</span> Técnicas para manejar datos no balanceados<a href="id_130010.html#técnicas-para-manejar-datos-no-balanceados" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Los datos utilizados en distintas áreas a menudo tienen menos del 1% de eventos raros pero “interesantes” (por ejemplo, estafadores que usan tarjetas de crédito, usuarios que hacen clic en anuncios o servidores dañados que escanean su red). Sin embargo, la mayoría de los algoritmos de aprendizaje automático no funcionan bien con conjuntos de datos desbalanceados <span class="citation">(<a href="#ref-kuhn2013applied" role="doc-biblioref"><strong>kuhn2013applied?</strong></a>)</span>. Hay varias técnicas para lidiar con esto de las que a continuación se introducen algunas:</p>
<ul>
<li><strong><em>Downsampling</em></strong> equilibra el conjunto de datos al reducir el tamaño de las clases abundantes para que coincidan con las frecuencias en la clase menos prevalente. Este método se utiliza cuando la cantidad de datos es suficiente. Manteniendo todas las muestras en la clase rara y seleccionando aleatoriamente un número igual de muestras en la clase abundante, se puede recuperar un nuevo conjunto de datos equilibrado para modelado adicional.</li>
<li><strong><em>Upsampling</em></strong> se usa cuando la cantidad de datos es insuficiente. Trata de equilibrar el conjunto de datos aumentando el tamaño de las muestras más raras. En lugar de deshacerse de las muestras abundantes, se generan nuevas muestras raras mediante la repetición o el <em>bootstrapping</em>.</li>
<li><strong>Creación de datos sintéticos</strong> <span class="citation">(<a href="#ref-chawla2002smote" role="doc-biblioref"><strong>chawla2002smote?</strong></a>)</span>: Esta técnica consiste en balancear el conjunto de entrenamiento generando nuevos registros sintéticos, esto es, inventados de la clase minoritaria. Existen diversos algoritmos que realizan esta tarea siendo uno de los más conocidos la técnica de SMOTE (<em>Synthetic Minority Oversampling Technique</em>).</li>
<li><strong>Otras técnicas</strong>, como que el algoritmo implemente mecanismos para dar mayor peso a los casos de la clase minoritaria, etc</li>
</ul>
<p>Por ejemplo, en capítulos posteriores se modelará por ejemplo el algoritmo <strong>RPART</strong> (algoritmos de árboles de regresión y clasificación) con <em>downsampling</em>, para obtener mejora en el rendimiento:</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="id_130010.html#cb131-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Se especifica que el modelo se entrene con downsampling</span></span>
<span id="cb131-2"><a href="id_130010.html#cb131-2" aria-hidden="true" tabindex="-1"></a>md_conf_model<span class="sc">$</span>fitControl<span class="sc">$</span>sampling <span class="ot">&lt;-</span> <span class="st">&quot;down&quot;</span></span>
<span id="cb131-3"><a href="id_130010.html#cb131-3" aria-hidden="true" tabindex="-1"></a><span class="co">#  Se le indica que haga un centrado a los datos</span></span>
<span id="cb131-4"><a href="id_130010.html#cb131-4" aria-hidden="true" tabindex="-1"></a>md_conf_model<span class="sc">$</span>fitControl<span class="sc">$</span>preProcess <span class="ot">&lt;-</span> <span class="st">&quot;center&quot;</span></span></code></pre></div>
<p>Para todos los modelos de <em>machine learning</em> de capítulos posteriores, se hace previamente una parametrización común en esta fase de preparación de los datos y se guarda en la lista <code>md_conf_model</code>. Esta lista incluye los elementos <code>dep</code>, <code>form</code>, <code>fitControl</code>, <code>positive_class</code>, <code>metric</code> y <code>seed</code>, que se conforman así:</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="id_130010.html#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="co"># nombre de la columna de la clase objetivo</span></span>
<span id="cb132-2"><a href="id_130010.html#cb132-2" aria-hidden="true" tabindex="-1"></a>dep <span class="ot">&lt;-</span> <span class="fu">colnames</span>(dp_entr[<span class="fu">ncol</span>(dp_ENTR)])</span>
<span id="cb132-3"><a href="id_130010.html#cb132-3" aria-hidden="true" tabindex="-1"></a><span class="co"># metrica de validacion</span></span>
<span id="cb132-4"><a href="id_130010.html#cb132-4" aria-hidden="true" tabindex="-1"></a>metric <span class="ot">&lt;-</span> <span class="st">&quot;ROC&quot;</span></span>
<span id="cb132-5"><a href="id_130010.html#cb132-5" aria-hidden="true" tabindex="-1"></a><span class="co"># preparar cv</span></span>
<span id="cb132-6"><a href="id_130010.html#cb132-6" aria-hidden="true" tabindex="-1"></a>fitControl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>, <span class="at">savePredictions =</span> T, <span class="at">summaryFunction =</span> twoClassSummary, <span class="at">classProbs =</span> T)</span>
<span id="cb132-7"><a href="id_130010.html#cb132-7" aria-hidden="true" tabindex="-1"></a><span class="co"># clase positiva el primer nivel del factor</span></span>
<span id="cb132-8"><a href="id_130010.html#cb132-8" aria-hidden="true" tabindex="-1"></a>positive_class <span class="ot">&lt;-</span> <span class="fu">levels</span>(dp_entr[, dep])[<span class="dv">1</span>]</span>
<span id="cb132-9"><a href="id_130010.html#cb132-9" aria-hidden="true" tabindex="-1"></a><span class="co"># fórmula con la clase objetivo en función del resto</span></span>
<span id="cb132-10"><a href="id_130010.html#cb132-10" aria-hidden="true" tabindex="-1"></a>form <span class="ot">&lt;-</span> <span class="fu">formula</span>(<span class="fu">paste0</span>(dep, <span class="st">&quot;~.&quot;</span>))</span></code></pre></div>
<p>Hay que considerar, que no existe una ventaja absoluta de un método de muestreo sobre otro. La aplicación de estos métodos depende del caso de uso al que se aplica y del conjunto de datos en sí. La función de <code>caret</code> para implementar estas técnicas está en <code>?caret::trainControl())</code>, como se ve en el ejemplo anterior.</p>
</div>
<div id="métodos-de-remuestreo" class="section level3 hasAnchor" number="11.6.3">
<h3><span class="header-section-number">11.6.3</span> Métodos de remuestreo<a href="id_130010.html#métodos-de-remuestreo" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>En la sección anterior se indicaba que los datos deben dividirse en conjuntos de entrenamiento y prueba y que no debía usarse el conjunto de prueba para evaluar el rendimiento del modelo durante la fase de entrenamiento. Para evaluar el rendimiento del modelo, una opción sería evaluar una métrica de error basada en los datos de entrenamiento. Desafortunadamente, esto conduce a resultados sesgados, ya que algunos modelos pueden funcionar muy bien con los datos de entrenamiento pero no generalizarse bien a un nuevo conjunto de datos.</p>
<p>Un segundo método es utilizar un enfoque de validación, que implica dividir aún más el conjunto de entrenamiento para crear dos partes: un conjunto de entrenamiento y un conjunto de <strong>validación.</strong> Entonces se puede entrenar el modelo en el nuevo conjunto de entrenamiento y estimar el rendimiento en el conjunto de validación. Lamentablemente, la validación con un solo conjunto de reserva puede ser muy variable y poco confiable a menos que esté trabajando con conjuntos de datos muy grandes <span class="citation">(<a href="#ref-molinaro2005prediction" role="doc-biblioref"><strong>molinaro2005prediction?</strong></a>)</span>.</p>
<p>En el capítulo de “Técnicas de Modelización estadística avanzadas” se explicaban los métodos de remuestreo, que brindan un enfoque alternativo al permitir ajustar repetidamente un modelo a partir de los datos de entrenamiento y probar su rendimiento en otras partes. Los dos métodos de remuestreo más utilizados incluyen la validación cruzada de kfold y el <em>bootstrapping</em>.</p>
<div id="kfold-cross-validation-o-validación-cruzada-kfold" class="section level4 hasAnchor" number="11.6.3.1">
<h4><span class="header-section-number">11.6.3.1</span> <em>kfold cross validation</em> (o validación cruzada kfold)<a href="id_130010.html#kfold-cross-validation-o-validación-cruzada-kfold" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>La validación cruzada kfold (kfold CV) es un método de remuestreo que divide aleatoriamente los datos de entrenamiento en k grupos (<em>folds</em>) de aproximadamente el mismo tamaño. El modelo se ajusta en k menos 1 grupos y el último se usa para calcular el rendimiento del modelo. Este procedimiento se repite k veces; cada vez, un grupo diferente se trata como el conjunto de validación. Este proceso da como resultado k estimaciones del error de generalización. Por lo tanto, la estimación del kfold CV se calcula promediando los errores de prueba k, lo que nos proporciona una aproximación del error de generalización que podríamos esperar en datos nuevos.</p>
En consecuencia, con kfold CV, cada observación en los datos de entrenamiento se mantendrá una vez para ser incluida en el conjunto de prueba, como se ilustra en <a href="id_130010.html#fig:26ch10">11.5</a>. En la práctica, normalmente se usa k=5 o k=10. No existe una regla formal en cuanto al tamaño de k; sin embargo, a medida que k aumenta, la diferencia entre el rendimiento estimado y el rendimiento real que se verá en el conjunto de prueba disminuirá. Por otro lado, el uso de k demasiado grande puede introducir cargas computacionales. Además, en <span class="citation">(<a href="#ref-molinaro2005prediction" role="doc-biblioref"><strong>molinaro2005prediction?</strong></a>)</span> encontraron que k=10 funcionaba de manera similar a la validación cruzada dejando uno fuera (LOOCV), que es el enfoque más extremo (es decir, establecer k=n).
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:26ch10"></span>
<img src="img/kfold.png" alt="Remuestro k fold cv" width="60%" />
<p class="caption">
Figura 11.5: Remuestro k fold cv
</p>
</div>
<p>Aunque el uso de <span class="math inline">\(k\geq 10\)</span> ayuda a minimizar la variabilidad en el rendimiento estimado, el kfold CV tiende a tener una mayor variabilidad que el <em>bootstrapping</em> (que se analiza a continuación). <span class="citation">(<a href="#ref-kim2009estimating" role="doc-biblioref"><strong>kim2009estimating?</strong></a>)</span> demostró que repetir kfold CV puede ayudar a aumentar la precisión del error de generalización estimado.</p>
<p>Para el caso de “Beauty eSheep”, cuando se entrena el algoritmo se le pasa el parámetro de <code>trControl</code> entre los que está <code>fitControl</code>, que como se veía anteriormente está parametrizado para realizar un kfold cv de 10 <em>folds</em>.</p>
</div>
<div id="bootstrapping" class="section level4 hasAnchor" number="11.6.3.2">
<h4><span class="header-section-number">11.6.3.2</span> <em>Bootstrapping</em><a href="id_130010.html#bootstrapping" class="anchor-section" aria-label="Anchor link to header"></a></h4>
Es una muestra aleatoria de los datos tomados con reemplazo <span class="citation">(<a href="#ref-efron1986bootstrap" role="doc-biblioref"><strong>efron1986bootstrap?</strong></a>)</span>. Esto significa que, después de seleccionar un dato para incluirlo en el subconjunto, aún está disponible para una selección posterior. Una muestra bootstrap tiene el mismo tamaño que el conjunto de datos original a partir del cual se construyó. La Fig. <a href="id_130010.html#fig:28ch10">11.6</a> proporciona un esquema de muestreo bootstrap donde cada muestra contiene 12 observaciones al igual que en el conjunto de datos original. Además, el muestreo bootstrap contendrá aproximadamente la misma distribución de valores (representados por colores) que el conjunto de datos original.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:28ch10"></span>
<img src="img/bootstrap.png" alt="Remuestro bootstrap" width="60%" />
<p class="caption">
Figura 11.6: Remuestro bootstrap
</p>
</div>
<p>Dado que las observaciones se replican en <em>bootstrapping</em>, tiende a haber menor variabilidad en la medida del error en comparación con kfold CV. Sin embargo, también puede aumentar el sesgo de su estimación de error. Esto puede ser un problema con conjuntos de datos más pequeños. Sin embargo, para la mayoría de los conjuntos de datos de tipo medio o grande (por ejemplo, <span class="math inline">\(n \geq 1000\)</span>), no ocasiona problemas. Se pueden crear muestras <em>bootstrap</em> fácilmente con <code>rsample::bootstraps()</code>, como se ilustra en el fragmento de código a continuación.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="id_130010.html#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bootstraps</span>(df3, <span class="at">times =</span> <span class="dv">10</span>)</span></code></pre></div>
<p>Para series temporales, para incorporar el origen móvil y otros procedimientos de remuestreo de series temporales <span class="citation">(<a href="#ref-hyndman2018forecasting" role="doc-biblioref"><strong>hyndman2018forecasting?</strong></a>)</span> es el recurso más usado, centrado en <strong>R</strong>.</p>
</div>
</div>
<div id="ajuste-de-hiperparámetros" class="section level3 hasAnchor" number="11.6.4">
<h3><span class="header-section-number">11.6.4</span> Ajuste de hiperparámetros<a href="id_130010.html#ajuste-de-hiperparámetros" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Aunque no se trata de transformación de preprocesamiento, sino ya de la fase de modelización, se introduce aquí el concepto de hiperparámetros (también conocidos como parámetros de ajuste), que sirven para para controlar la complejidad de los algoritmos de aprendizaje automático y, por tanto, la compensación entre sesgo y varianza. No todos los algoritmos tienen hiperparámetros (por ejemplo, mínimos cuadrados ordinarios-MCO-). Sin embargo, la mayoría tiene al menos uno. La configuración adecuada de estos hiperparámetros a menudo depende de los datos y el problema en cuestión y no siempre se puede ajustar solo con los datos de entrenamiento. En consecuencia, se requiere un método para identificar la configuración óptima.</p>
<p>Supongamos que los valores de <span class="math inline">\(k\)</span> más pequeños (por ejemplo 2, 5 o 10) conducen a una varianza alta (pero un sesgo más bajo) y los valores más grandes (por ejemplo, 150) conducen a un sesgo alto (pero una varianza más baja). Entonces, el valor óptimo de k podría ser entre 20 y 50, pero hay que preguntarse por el valor óptimo. Una forma de realizar el ajuste de hiperparámetros es probar los hiperparámetros manualmente hasta que se encuentre la mejor combinación de valores de hiperparámetros que resulten en la mayor precisión predictiva (medida usando k fold CV, por ejemplo), aunque puede ser un trabajo muy tedioso dependiendo de la cantidad de hiperparámetros. Un enfoque alternativo es realizar una búsqueda en cuadrícula, que es un enfoque automatizado para buscar en muchas combinaciones de valores de hiperparámetros.</p>
</div>
<div id="evaluación-de-modelos" class="section level3 hasAnchor" number="11.6.5">
<h3><span class="header-section-number">11.6.5</span> Evaluación de modelos<a href="id_130010.html#evaluación-de-modelos" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>De manera similar al punto anterior, aunque no se trata de transformaciones de la fase de preprocesamiento, se introduce aquí el concepto de evaluación de modelos. Tradicionalmente, el rendimiento de los modelos estadísticos se basaba en gran medida en pruebas de bondad de ajuste y evaluación de residuos. Desafortunadamente, pueden derivarse conclusiones equívocas de los modelos predictivos que pasan este tipo de evaluaciones.</p>
<p>En la actualidad, para analizar el rendimiento del modelo, se suele evaluar la precisión predictiva a través de funciones de pérdida (<em>loss functions</em>). Las funciones de pérdida son métricas que comparan los valores predichos con el valor real.
Al realizar métodos de remuestreo, se evalúan los valores pronosticados para un conjunto de validación en comparación con el valor objetivo real. Por ejemplo, en la regresión, una forma de medir el error es tomar la diferencia entre el valor real y el predicho para una observación determinada (esta es la definición habitual de residuo en la regresión lineal ordinaria). El error de validación general del modelo se calcula agregando los errores en todo el conjunto de datos de validación.
Es importante considerar el contexto del problema al identificar la métrica de rendimiento a usar. Además, al comparar varios modelos, hay que hacerlo con la misma métrica. Para los <strong>modelos de predicción</strong> caben destacar las siguientes métricas, todos con objetivo de minimizar menos el <span class="math inline">\(R^2\)</span> que es de maximizar:</p>
<ul>
<li>El error cuadrático medio (<strong>MSE</strong>) es el promedio del error cuadrático, siendo ésta (junto a la siguiente) la métrica de error más común utilizada. Siendo <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>, <span class="math inline">\(D\)</span> vectores dimensionales, y <span class="math inline">\(x_i\)</span> el valor de la <span class="math inline">\(i\)</span> dimensión de <span class="math inline">\(x\)</span>, <span class="math inline">\(MSE=\sum_{i=1}^{D}(x_i-y_i)^2\)</span></li>
<li><strong>RMSE</strong>: Raíz del error cuadrático medio. La raíz se toma para que su error esté en las mismas unidades que su variable de respuesta. <span class="math inline">\(RMSE=\sqrt{MSE}\)</span></li>
<li><strong>Desviación</strong>: Abreviatura de desviación residual media. En esencia, proporciona un grado en el que un modelo explica la variación en un conjunto de datos cuando se utiliza la estimación de máxima verosimilitud.</li>
<li><strong>MAE</strong>: Error absoluto medio. Similar a MSE pero en lugar de elevar al cuadrado, solo toma la diferencia media absoluta entre los valores reales y predichos. <span class="math inline">\(MAE=\sum_{i=1}^{D}|x_i-y_i|\)</span></li>
<li><strong>RMSLE</strong>: Raíz del error logarítmico cuadrático medio. Similar a RMSE pero realiza un log() en los valores reales y predichos antes de calcular la diferencia. <span class="math inline">\(RMSLE=\sqrt{(log(x_i+1)-log(y_i+1))^2)}\)</span>.</li>
<li><strong><span class="math inline">\(R^2\)</span></strong>: Esta es una métrica popular que representa la proporción de la varianza en la variable dependiente que es predecible a partir de la(s) variable(s) independiente(s).</li>
</ul>
<p>Para el caso de la <strong>clasificación</strong> destacan las siguientes medidas, todos con objetivo de minimizar:</p>
<ul>
<li><strong>Clasificación errónea</strong>: tasa de clasificación incorrecta en porcentaje.</li>
<li><strong>Error medio por clase</strong>: tasa de error promedio para cada clase.</li>
<li><strong>MSE</strong>: Error cuadrático medio. Es la distancia de 1.0 a la probabilidad sugerida.</li>
<li><strong>Entropía cruzada</strong> (también conocida como pérdida de registro o desviación): similar a MSE pero incorpora un registro de la probabilidad predicha multiplicada por la clase real. En consecuencia, esta métrica penaliza las predicciones en las que se predice una pequeña probabilidad para la clase verdadera.</li>
<li><strong>Índice de Gini</strong>: se utiliza principalmente con métodos basados en árboles y frecuentemente se conoce como una medida de purezax, donde un valor pequeño indica que un nodo contiene predominantemente observaciones de una sola clase.</li>
</ul>
Cuando se aplican modelos de clasificación, a menudo se usa una matriz de confusión para evaluar ciertas medidas de desempeño. Una matriz de confusión (figura <a href="id_130010.html#fig:matrizconfusion">11.7</a>) es una matriz que compara niveles categóricos reales (o eventos) con los niveles categóricos predichos. Cuando se predice el nivel correcto, se dice que se ha poducido un <strong>verdadero positivo</strong> (<span class="math inline">\(tp\)</span>). Sin embargo, si se predice un nivel o evento que no sucedió, esto se denomina <strong>falso positivo</strong> (<span class="math inline">\(fn\)</span>). Por otro lado, cuando no se predijo un nivel o evento y sucede, se denomina <strong>falso negativo</strong> (<span class="math inline">\(fn\)</span>) y si se predijo es un <strong>verdadero negativo</strong> (<span class="math inline">\(tn\)</span>).
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:matrizconfusion"></span>
<img src="img/matrizconfusion.png" alt="Matriz de confusión" width="60%" />
<p class="caption">
Figura 11.7: Matriz de confusión
</p>
</div>
<p>A partir de éstos, se pueden extraer diferentes niveles de rendimiento para clasificadores binarios como:</p>
<ul>
<li>Exactitud (<span class="math inline">\(\frac{tp+tn}{p+n}\)</span>).</li>
<li>Precisión (<span class="math inline">\(\frac{tp}{tp+fp}\)</span>).</li>
<li>Sensibilidad (<span class="math inline">\(\frac{tp}{p}\)</span>).</li>
<li>Especificidad (<span class="math inline">\(\frac{tn}{n}\)</span>).</li>
<li><strong>AUC</strong>: Es el área bajo la curva (<em>Area Under the Curve</em>). Un buen clasificador binario tendrá una alta precisión y sensibilidad, por lo que se trata de maximizarlo. Esto significa que el clasificador funciona bien cuando predice que un evento ocurrirá y no ocurrirá, lo que minimiza los falsos positivos y los falsos negativos. Para capturar este equilibrio, a menudo se usa una <strong>curva ROC</strong> (<em>Receiver Operating Characteristic</em>) como la de la Fig. <a href="id_130010.html#fig:roc2">11.8</a>.
La curva ROC traza la tasa de falsos positivos a lo largo del eje <span class="math inline">\(x\)</span> y la tasa de verdaderos positivos a lo largo del eje <span class="math inline">\(y\)</span>. Una línea que es diagonal desde la esquina inferior izquierda hasta la esquina superior derecha representa una suposición aleatoria. Cuanto más alta esté la ROC en la esquina superior izquierda, mayor será el área AUC bajo esta curva.</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:roc2"></span>
<img src="img/roc2.png" alt="Curva ROC" width="60%" />
<p class="caption">
Figura 11.8: Curva ROC
</p>
</div>
<div class="infobox_resume">
<p><strong>RESUMEN</strong></p>
<p>Se ha dado una visión de las principales transformaciones que se realizan en la fase de preprocesamiento del un proyecto de modelado predictivo, la selección de variables (<em>feature selection</em>), las transformaciones de la escala o distribución de la variable objetivo, la transformación de variables (<em>feature engineering</em>), la reducción de dimensionalidad y otro tipo de tratamientos.
La creación de variables predictoras a partir de los datos en bruto tiene una componente creativa, que requiere de herramientas adecuadas y de experiencia para
encontrar las mejores representaciones, apoyándose en lo posible en el conocimiento que se tenga de los datos.
Se ha utilizado fundamentalmente el paquete <code>caret</code> y <code>rsample</code> para realizar las tareas de procesamiento sobre dos conjuntos de datos, el de la tienda de “Beauty eSheep” y el de <code>Madrid_Sale</code>.</p>
</div>

</div>
</div>
</div>



<div class="footnotes">
<hr />
<ol start="15">
<li id="fn15"><p>Si la variable objetivo tiene valores negativos o ceros, una transformación logarítmica producirá <span class="math inline">\(NaN\)</span> e <span class="math inline">\(- Inf\)</span>, respectivamente. Si los valores de respuesta no positivos son pequeños (por ejemplo, entre <span class="math inline">\(-0.99\)</span> y <span class="math inline">\(0\)</span>), se puede aplicar una pequeña compensación como con la función <code>log1p()</code> que agrega 1 al valor antes de aplicar una transformación de registro. Si los datos consisten en valores <span class="math inline">\(\geq\)</span> <span class="math inline">\(-1\)</span>, se debe usar la transformación de Yeo-Johnson. Por tanto, si se usa una transformación log o Box-Cox, no se deben centrar los datos primero ni realizar ninguna operación que pueda hacer que los datos no sean positivos.<a href="id_130010.html#fnref15" class="footnote-back">↩︎</a></p></li>
<li id="fn16"><p>En el centro de la transformación de Box-Cox hay un exponente, lambda (<span class="math inline">\(\lambda\)</span>), que varía de <span class="math inline">\(-5\)</span> a <span class="math inline">\(5\)</span>. Se consideran todos los valores de <span class="math inline">\(\lambda\)</span> y se estima el valor óptimo para los datos dados a partir de los datos de entrenamiento. El “valor óptimo” es el que resulta en la mejor transformación a una distribución normal aproximada.<a href="id_130010.html#fnref16" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="id_130009.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="Funda-probab.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Ciencia_de_datos_con_r.pdf", "Ciencia_de_datos_con_r.epub"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
