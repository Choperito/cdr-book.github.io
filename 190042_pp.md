
# Procesos de puntos {#cap-pp}

*Jorge Mateu*$^{a}$ y *Mehdi Moradi*$^{b}$ 

$^{a}$Universidad Jaume I 
$^{b}$Umeå Universitet


## Introducción

La **estadística espacial**\index{estadística espacial} es una rama de la estadística que se ha desarrollado rápidamente durante los últimos treinta años, tanto en el plano teórico como en el práctico. A ello ha contribuido, de manera significativa, la creciente disponibilidad de potencia computacional y variedad en software, que han estimulado la capacidad de resolver problemas cada vez más complejos. Lo cierto es que estos problemas tienen como elemento común la estructura espacial. En general, se observa un desarrollo científico que ha ocurrido en el campo de la estadística espacial: problemas bien definidos con un carácter común saltaron a la agenda del investigador, y la disponibilidad de datos motivaron nuevos desarrollos teóricos. 

La estadística espacial reconoce y explota las ubicaciones espaciales de los datos al diseñar, recopilar, administrar, analizar y mostrar dichos datos. Los datos espaciales suelen ser dependientes, y se necesitan clases de modelos espaciales que permitan la predicción de procesos y la estimación de parámetros. **Patrones espaciales**\index{patrones espaciales} ocurren en una variedad sorprendentemente amplia de disciplinas científicas: los ecologistas estudian las interacciones entre plantas y animales, los silvicultores y agricultores deben investigar la capacidad de las plantas y tener en cuenta las variaciones del suelo en sus experimentos. Así pues, cualquier disciplina que trabaje con datos recopilados de diferentes ubicaciones espaciales, necesita desarrollar modelos que indican cuándo hay dependencia entre mediciones en diferentes lugares. Referencias modernas sobre estadística espacial incluyen los libros de @diggle2013; @cressie2015statistics; @montero2015spatial; @Wikletal19; @DiggGio19 entre otros.

Este capítulo se centra en **patrones espaciales de puntos**\index{patrones espaciales de puntos}. Datos en forma de un conjunto de puntos, distribuidos irregularmente dentro de una región del espacio, surgen en muchos contextos diferentes; por ejemplo localizaciones de incendios forestales (Fig. \@ref(fig:kernelnepal)), delitos (Fig. \@ref(fig:medker)), árboles en un bosque, nidos en una colonia de cría de pájaros, ubicación de núcleos en una sección microscópica de tejido, depósitos de oro mapeados en un estudio geológico, estrellas en un cúmulo estelar, accidentes de tráfico, terremotos, llamadas de teléfonos móviles, avistamientos de animales o casos de una enfermedad rara. 

Se llama **patrón espacial de puntos**, cualquier conjunto de datos de este tipo. La disposición espacial de los puntos es el principal foco de investigación. Son muchos los campos de la ciencia donde este tipo de estructuras son de interés; por ejemplo, en ecología, epidemiología, geociencia, astronomía, econometría e investigación criminal. 
El análisis estadístico de la disposición espacial de los puntos puede revelar características importantes, como una tendencia a que los yacimientos de oro se encuentren cerca de una gran falla geológica, o a que los casos de una enfermedad sean más frecuentes cerca de una fuente de contaminación.

El análisis de los datos de patrones de puntos ha proporcionado evidencia fundamental para importantes investigaciones, desde la transmisión del cólera hasta el comportamiento de los asesinos en serie y la estructura a gran escala
 del universo. Los puntos en un patrón de puntos pueden tener todo tipo de atributos. Un estudio forestal podría registrar cada ubicación, especie y diámetro del árbol; un catálogo de estrellas puede dar sus posiciones en el cielo, masas, formas y colores; las ubicaciones de los casos de enfermedades pueden estar vinculadas a registros clínicos detallados. Esta información auxiliar adjunta a cada punto en el patrón de puntos se llama **marca** y en ese caso se habla de un patrón de puntos marcado. La colección de localizaciones de un patrón puntual puede venir definida en una **región plana** (Sec. \@ref(secplana)) o bien en una **red lineal** (Sec. \@ref(seclineal)), haciendo que las distancias dejen de ser euclidianas para pasar a ser del camino más corto. Esto introduce ciertos cambios metodológicos en cuanto a las construcciones de ciertas características, que en el caso de intensidades de primer orden trataremos en este capítulo.  
   


## Patrones puntuales espaciales en $\mathbb R^2$ {#secplana}

La teoría de procesos puntuales espaciales constituye la base para el análisis de eventos observados geográficamente a través de sus coordenadas (longitud, latitud) en un espacio bi-dimensional.
Esta rama de los procesos puntuales pertenece al campo de la estadística espacial en conjunción con la de procesos estocásticos. De hecho, un proceso puntual espacial es un proceso estocástico cuyas realizaciones consisten en un conjunto numerable de puntos en el plano (patrón puntual). Heurísticamente, se trata de un conjunto de datos que se encuentran en una región concreta (o área de estudio).

Sea \({\mathbf x}=\{ x_1, x_2, \ldots, x_n \}, 0 \leq n < \infty,\) una realización (patrón puntual) observada de un proceso puntual simple (i.e. sin múltiples eventos por localización) y finito \(X\) en \(\mathbb R^2\) en la región \(W \subset \mathbb R^2\) y con la métrica (distancia) asociada \(d(u,v)\). En general, las realizaciones consisten en un conjunto numerable de puntos (llamados en muchas ocasiones eventos). Consultar las Figs. \@ref(fig:kernelnepal) y  \@ref(fig:medker) para ver algunos ejemplos de patrones puntuales. Para cualquier conjunto arbitrario \(A \subset \mathbb R^2\), el cardinal de \(X\) viene dado por la función de conteo
\begin{equation*}
N(X \cap A)
=
\sum\limits_{x \in X} \mathbf 1 \{ x \in A \} < \infty
.
\end{equation*}
Además, y gracias a la fórmula de Campbell [@BRT15], para cualquier función medible \(f: \mathbb R^2 \to [0, \infty)\) se cumple que
\begin{equation*}
\mathbb E
\left[
\sum\limits_{x \in X}
f(x)
\right]
=
\int_{\mathbb R ^2}
 f(u)
\lambda(u)
\mathrm{d} u,
(\#eq:compR2)
\end{equation*}
donde \(\lambda (\cdot)\) determina la **función de intensidad**\index{intensidad} de \(X\), y gobierna su distribución espacial. De hecho, \(\lambda(u)\) proporciona el valor esperado de eventos por unidad de área en un entorno de \(u \in \mathbb R^2\). Teniendo en cuenta que \(f(x) = {\mathbf 1} \{ x \in A \}\), se puede observar fácilmente la relación entre la función de intensidad \(\lambda(\cdot)\) y la de conteo \(N\), establecida como
\begin{equation*}
\mathbb E
\left[
N(X \cap A)
\right]
=
\int_A
\lambda(u)
\mathrm{d} u.
\end{equation*}
Si la función de intensidad \(\lambda(\cdot)\) es constante, i.e. \(\lambda(\cdot) = \lambda\), se dice que el proceso \(X\) es homogéneo, mientras que, en caso contrario, se dice que es inhomogéneo; en este último caso, la distribución espacial varía a lo largo de la región soporte. Para el lector con un mayor interés en conceptos y desarrollos, se aconseja consultar @moller2003;@illian2008;@diggle2013 y @BRT15.

En la práctica se suele observar sólo una única realización, y por ello es importante disponer de una estimación de $\lambda(\cdot)$ que pueda imitar la distribución  espacial del proceso subyacente, el cual ha generado el patrón observado. Por ello, se consideran diferentes tipos de **estimadores no paramétricos de la intensidad**.



### Estimación de la intensidad basada en funciones núcleo  {#secestimaciones}

Dos estimadores no paramétricos, basados en **funciones núcleo**\index{funciones núcleo}, de la función de intensidad ampliamente utilizados en patrones puntuales en \(\mathbb R^2\), vienen dados por
\begin{equation}
\widehat \lambda^{\text{U}}_{\sigma}(u)
=
\frac{1}{c_{\sigma,W}(u)} \sum_{i=1}^n \kappa_{\sigma}(u - x_i),
\quad u \in W,
(\#eq:kde2Dunif)
\end{equation}
y
\begin{equation}
\widehat \lambda^{\text{JD}}_{\sigma}(u)
=
\sum_{i=1}^n \frac{\kappa_{\sigma}(u - x_i)}{c_{\sigma,W}(x_i)},
\quad u \in W,
(\#eq:kde2DJD)
\end{equation}
donde \(\kappa_{\sigma}\) es una función de densidad de probabilidad en \(\mathbb R^2\) con parámetro de suavizado (ancho de banda) \(\sigma\), y

\begin{equation}
c_{\sigma,W}(u)
=
\int_W \kappa_{\sigma}(u - v) \mathrm{d} v,
\quad u \in W,
\end{equation}
es el área del núcleo centrado en \(u \in W\), y equivale a un corrector de borde que compensa por la falta de información fuera de \(W\). Hay que recordar que, en la práctica, sólo se observa una realización de \(X\) en la región acotada  \(W\). Más allá de la elección de \(\sigma\), el estimador \@ref(eq:kde2Dunif) es insesgado si la función de intensidad es constante [@diggle1985], mientras que el estimator \@ref(eq:kde2DJD) conserva la masa total [@jones1993simple]. Los estimadores \@ref(eq:kde2Dunif) y \@ref(eq:kde2DJD) suelen ser llamados  'uniformly-edge-corrected' y 'Jones-Diggle' [@rakshit2019fast]. En este capítulo, se considera en todo momento la función núcleo Gaussiana [@silverman1986].

En términos prácticos, la adecuación de los estimadores basados en núcleos depende del parámetro de suavizado, de forma que un suavizamiento pequeño lleva a un sesgo (por debajo) y varianza alta, mientras que un parámetro de suavizado alto resulta en un sesgo alto y poca varianza. Para un cierto patrón puntual  \({\mathbf x}\), los estimadores \@ref(eq:kde2Dunif) y \@ref(eq:kde2DJD) pueden ser calculados utilizando la función `density.ppp()` de `spatstat.core` especificando `diggle=FALSE` y `diggle=TRUE`, respectivamente.



#### Selección del parámetro de suavizado\index{parámetro de suavizado} {#secbw}

@scott1992 propuso elegir este parámetro a través de un regla un tanto naive (llamada _rule of thumb_), de la forma
$$
(s_x n^{-1/6}, s_y n^{-1/6}),
$$
para cada coordenada cartesiana \(x,y\), donde \(s_x, s_y\) son las desviaciones típicas de las coordenadas \(x,y\) de los eventos. Este procedimiento es útil para análisis exploratorios. La función 'bw.scott' de 'spatstat.explore' proporciona este estimador. Nótese que, en el caso de Scott, el parámetro de suavizado es, por construcción, un vector de dos componentes para suavizar ambas coordenadas cartesianas.

@cronie2018 propusieron encontrar el parámetro óptimo minimizando
$$
CvL(\sigma)
=
\left(
|W|
-
\sum\limits_{i=1}^n
1
/
\widehat{\lambda}^*_{\sigma}(x_i)
\right)^2,
$$
donde \(\widehat{\lambda}^*_{\sigma}(x_i)\) es un estimador de la intensidad sin corregir (bien sea \@ref(eq:kde2Dunif) o \@ref(eq:kde2DJD) pero sin el término de corrección) evaluado en \(x_i\) y con parámetro de suavizado \(\sigma\). La idea de este estimador proviene de la fórmula de Campbell, ya que
$$
\mathbb E
\left[
\sum\limits_{x \in X}
1
/
\lambda(x)
\right]
=
\int_W
(1
/
\lambda(x)
)
\lambda(x)
\mathrm{d} u
=
|W|.
$$
Para un patrón puntual \({\mathbf x}\), la función 'bw.CvL' de 'spatstat.explore' calcula el parámetro de suavizado mediante el método de Cronie y van Lieshout (se denotará por Cronie--van Lieshout).



### Ejemplos prácticos

En esta sección se hace uso de los estimadores de la intensidad anteriormente mostrados y de los diferentes métodos de selección del parámetro de suavizado para analizar la distribución espacial de dos conjuntos de datos: incendios forestales en Nepal (Fig. \@ref(fig:kernelnepal)), y eventos de crímenes en Medellín, Colombia (Fig. \@ref(fig:medker)). En este capítulo, haremos uso de las librerías 'spatstat, versión 2.3-0,' [@spatstat;@BRT15] para el análisis de patrones puntuales y 'raster, versión 3.5-15,' [@raster] para ciertas representaciones gráficas. Nótese que la librería 'spatstat' ha sido recientemente dividida en una familia de sub-librerías 'spatstat.utils', 'spatstat.data', 'spatstat.sparse', 'spatstat.geom', 'spatstat.random', 'spatstat.core', 'spatstat.linnet', 'spatstat.explore', 'spatstat.model', de forma que 'spatstat' actúa como una libería paraguas de todas ellas. Los lectores deben estar atentos a posibles futuros cambios en 'spatstat' para satisfacer ciertas restricciones de CRAN en relación con los tamaños de sus librerías.

#### Ejemplo 1: Incendios forestales\index{incendios forestales} en Nepal {#secnepalfire}

Por cortesía de Ganesh Prasad Sigdel, se dispone de localizaciones georeferenciadas de incendios forestales en Nepal durante 2016, datos cedidos por la institución ICIMOD-Nepal. En 2016, Nepal sufrió 5757 incendios, de los cuales 475 ocurrieron en el distrito de Surkhet, en la provincia de Karnali, en el medio-oeste de Nepal. Se comienza llamando a algunas librerías de **R**, útiles para nuestros propósitos.



```r
#library("raster")
#library("spatstat")
#library("CDR")
```

Utilizando los métodos descritos en la Sec. \@ref(secbw), se estima el correspondiente parámetro de suavizado. La regla de Scott proporciona los valores (50253.47 m, 21158.42 m) y el método de validación cruzada de Cronie--van Lieshout estima el valor como 36513.16 m. Mediante el argumento 'ns' de 'bw.CvL', se puede controlar mejor la búsqueda del parámetro óptimo a través de un grid más fino.


```r
data(nepal)
scott_nepal <- bw.scott(nepal) # Scott’s rule
CvL_nepal <- bw.CvL(nepal) # Cronie and van Lieshout’s criterio
```

Conocido el parámetro de suavizado, se estima la intensidad mediante los estimadores \@ref(eq:kde2Dunif) y \@ref(eq:kde2DJD). La función 'density.ppp' proporciona una estimación basada en  funciones núcleo para patrones en \(\mathbb R^2\), teniendo en cuenta que, por defecto, esta función hace uso del estimador con corrección uniforme para los bordes ('uniformly-edge-corrected estimator') \@ref(eq:kde2Dunif) con un núcleo Gaussiano. Se fija 'leaveoneout=FALSE' para no calcular el estimador _leave-one-out_, mientras que se establece 'positive=TRUE' para forzar valores positivos en la densidad. Esto último obedece a que, debido a errores numéricos en el cálculo de la Transformada Rápida de Fourier, se pueden obtener valores negativos en ciertas áreas (ver la ayuda de 'density.ppp').



```r
d_scott_nepal <- density.ppp(nepal, sigma = scott_nepal, leaveoneout = FALSE, positive = TRUE)
d_cvl_nepal <- density.ppp(nepal, sigma = CvL_nepal, leaveoneout = FALSE, positive = TRUE)
```

Se estima ahora la intensidad mediante el estimador de Jones-Diggle \@ref(eq:kde2Dunif) escribiendo 'diggle=TRUE' en 'density.ppp'.


```r
d_scott_dig_nepal <- density.ppp(nepal, sigma = scott_nepal, leaveoneout = FALSE, positive = TRUE, diggle = TRUE)
d_cvl_dig_nepal <- density.ppp(nepal, sigma = CvL_nepal, leaveoneout = FALSE, positive = TRUE, diggle = TRUE)
```

Tras obtener diferentes estimadores de la intensidad bajo diferentes métodos de selección del parámetro de suavizado, a continuación se muestran estas estimaciones y se comentan sus discrepancias. Para una mejor representación gráfica, se convierten las imágenes de intensidad dadas en la clase 'im' a objetos de clase 'raster' para luego juntarlas en un 'RasterStack'. La Fig. \@ref(fig:kernelnepal) muestra estas estimaciones, observándose una mayor intensidad en el sur y sur-oeste de Nepal, indicando una clara distribución no uniforme de dicha intensidad, lo que, a su vez, indica un alto grado de inhomogeneidad.


```r
sp_int_nepal <- stack(raster(d_scott_nepal), raster(d_cvl_nepal), raster(d_scott_dig_nepal), raster(d_cvl_dig_nepal))
sp_int_nepal <- sp_int_nepal * 10^7
names(sp_int_nepal) <- c("scott_gaus_U", "CvL_gaus_U", "scott_gaus_JD", "CvL_gaus_JD")

at <- c(seq(0, 1.4, 0.2))
pts_nepal <- as.data.frame(nepal)
coordinates(pts_nepal) <- ~ x + y
library("latticeExtra")
spplot(sp_int_nepal, at = at, scales = list(draw = FALSE), col.regions = rev(topo.colors(20)), colorkey = list(labels = list(cex = 3)), par.strip.text = list(cex = 3)) + layer(sp.points(pts_nepal, pch = 20, col = 1))
```

<div class="figure" style="text-align: center">
<img src="img/nepal2016.png" alt="Estimación basada en funciones núcleo para los incendios forestales (puntos negros) en Nepal en 2016. Las etiquetas de los nombres comienzan con el método de suavizado, seguido del núcleo utilizado y de la corrección de borde.  Los valores de la intensidad indican número de incendios por diez mil km cuadrados. Se usa JD y U para indicar los estimadores de 'Jones-Diggle' y 'uniformly-edge-corrected'." width="70%" />
<p class="caption">(\#fig:kernelnepal)Estimación basada en funciones núcleo para los incendios forestales (puntos negros) en Nepal en 2016. Las etiquetas de los nombres comienzan con el método de suavizado, seguido del núcleo utilizado y de la corrección de borde.  Los valores de la intensidad indican número de incendios por diez mil km cuadrados. Se usa JD y U para indicar los estimadores de 'Jones-Diggle' y 'uniformly-edge-corrected'.</p>
</div>

#### Ejemplo 2: Crímenes en Medellín

Medellín es la segunda ciudad con más población en Colombia [@DANE], con un territorio urbano de \(105\) km\(^2\), que ha sufrido de múltiples acciones criminales durante muchos años, como es bien concido. En 2018, la Secretaría de Seguridad de Medellín reportó que el \(40\%\) de los ciudadanos se sentía inseguro, proporcinando, a modo de ejemplo, 20607 quejas de robos [@Restrepo]. Adicionalmente, el departamento de policía reconocía la necesidad de contratar al menos 2000 policías más para luchar contra los homicidios, robos y micro-tráfico [@Monsalve].

En esta sección, sólo se analiza la distribución espacial de los eventos georeferenciados de crímenes ocurridos en Medellín durante 2005 [@sanabria2022]. En 2005, ocurrieron 910 crímenes, de los cuales el porcentaje de víctimas varones fue del \(66\%\), \(28\%\) fueron cometidos durante los fines de semana, el porcentaje de robos fue del \(42\%\), y el de víctimas con edades entre 20 y 40 fue del  \(60\%\).

Nótese notar que el conjunto de localizaciones de estos crímenes no necesariamente ocurrió en las calles de la ciudad, y por tanto se considera que el patrón puntual tiene como dominio de definición todo \(\mathbb R^2\).


```r
data(medellin)
scott_med <- bw.scott(medellin) # Scott’s rule
CvL_med <- bw.CvL(medellin) # Cronie and van Lieshout’s criterio
```


La regla de Scott estima el parámetro de suavizado en (691.31m, 954.20m) mientras que el criterio  de validación cruzada (CvL) nos lleva a 692.31m. Se hace uso de la función 'density.ppp' para obtener los correspondientes estimadores de la intensidad \@ref(eq:kde2Dunif) y \@ref(eq:kde2DJD) bajo los mismos escenarios que en la Sección \@ref(secnepalfire).



```r
d_scott_med <- density.ppp(medellin, sigma = scott_med, leaveoneout = FALSE, positive = TRUE)
d_cvl_med <- density.ppp(medellin, sigma = CvL_med, leaveoneout = FALSE, positive = TRUE)

d_scott_dig_med <- density.ppp(medellin, sigma = scott_med, leaveoneout = FALSE, positive = TRUE, diggle = TRUE)
d_cvl_dig_med <- density.ppp(medellin, sigma = CvL_med, leaveoneout = FALSE, positive = TRUE, diggle = TRUE)
```


La Fig. \@ref(fig:medker) muestra la intensidad estimada bajo diferentes parámetros de suavizado. Se observa, en general, una distribución no homogénea de los crímenes. Independientemente del método utilizado, se observan dos grandes hotspots en la zona central de Medellín, aunque con diferentes magnitudes. El efecto de la corrección de borde es sólo marginal.



```r
sp_int_med <- stack(raster(d_scott_med), raster(d_cvl_med), raster(d_scott_dig_med), raster(d_cvl_dig_med))
sp_int_med <- sp_int_med * 10^5
names(sp_int_med) <- names(sp_int_nepal)
at <- seq(0, 3, by = 0.2)
pts <- as.data.frame(medellin)
coordinates(pts) <- ~ x + y

sp::spplot(sp_int_med, at = at, scales = list(draw = FALSE), col.regions = rev(topo.colors(20)), colorkey = list(labels = list(cex = 3)), par.strip.text = list(cex = 3)) + layer(sp.points(pts, pch = 20))
```

<div class="figure" style="text-align: center">
<img src="img/med2005.png" alt="Estimación de la intensidad basada en funciones núcleo para los datos de Medellín (puntos negros), durante 2005. Las etiquetas de los nombres comienzan con el método de suavizado, seguido del núcleo utilizado y de la corrección de borde. Los valores de la intensidad indican número de crímenes por cien km cuadrados. Se usa JD y U para indicar los estimadores de 'Jones-Diggle' y 'uniformly-edge-corrected'." width="80%" />
<p class="caption">(\#fig:medker)Estimación de la intensidad basada en funciones núcleo para los datos de Medellín (puntos negros), durante 2005. Las etiquetas de los nombres comienzan con el método de suavizado, seguido del núcleo utilizado y de la corrección de borde. Los valores de la intensidad indican número de crímenes por cien km cuadrados. Se usa JD y U para indicar los estimadores de 'Jones-Diggle' y 'uniformly-edge-corrected'.</p>
</div>



### Estimación de la intensidad basada en funciones núcleo en dominios irregulares\index{dominios irregulares}

Los estimadores \@ref(eq:kde2Dunif) y \@ref(eq:kde2DJD) pueden mostrar deficiencias importantes como no cumplir la condición de que la integral sea el número de puntos, sesgo cerca de las fronteras o presentar suavizamientos artificiales que lleven a resultados inverosímiles en ciertas ocasiones [@baddeley2022diffusion]. Estos problemas son más aparentes en caso de dominios irregulares. Como remedio, @baddeley2022diffusion propusieron estimar la intensidad via una **función núcleo-calor**\index{función núcleo-calor} _(heat kernel)_, la cual puede ser definida como una densidad de probabilidad de transición de un movimiento Browniano en \(W\) que respeta las fronteras. De hecho, su propuesta, llamada **estimador de difusión**\index{estimador de difusión}, toma la forma
\begin{equation}
\widehat \lambda_t (u)
=
\sum\limits_{i=1}^n
\kappa_t (u|x_i)
,
(\#eq:heat)
\end{equation}
donde \(t= \sigma^2\) (\(\sigma\) es el parámetro de suavizado en \@ref(eq:kde2Dunif) y \@ref(eq:kde2DJD)) y \(\kappa_t (\cdot|x_i)\) es el núcleo-calor. Este estimador es insesgado (bajo homogeneidad) y preserva la masa (es decir, integra el número de puntos). @baddeley2022diffusion proponen algunos nuevos métodos de selección del parámetro de suavizado, adaptados a su estimador de difusión, incluyendo el de Cronie--van Lieshout. El estimador de difusión se puede calcular con la función 'densityHeat.ppp', y el criterio de Cronie--van Lieshout viene en la función 'bw.CvLHeat'. Todas estas funciones pertenecen al 'spatstat.explore'.


A continuación, se utiliza el estimador de difusión para analizar su comportamiento comparándolo, con el estimador \@ref(eq:kde2Dunif) sobre unos datos de incendios activos  en EEUU y América Central (sin considerar las islas) desde el 24 de Febrero al 3 de Marzo 2022\footnote{\href{https://firms.modaps.eosdis.nasa.gov/active\_fire/}{https://firms.modaps.eosdis.nasa.gov/active\_fire/}}. Hay que hacer notar que las localizaciones, en este caso, no necesariamente confirman  la existencia de un incendio, sino más bien píxeles susceptibles de existencia de incendio, los cuales han sido clasificados por medio de algoritmos preparados para ello. Este formato está relacionado con el contexto de datos en _Near Real-Time (NRT)_.

Los parámetros de suavizado para los estimadores \@ref(eq:kde2Dunif) y \@ref(eq:heat) siguen el criterio de Cronie--van Lieshout. Nótese que al considerar un área mucho más grande que en los ejemplos precedentes, se considera 'ns=50', es decir, se usa un vector de tamaño 50 para buscar el parámetro de suavizado óptimo (por defecto es 16), y 'dimyx=512' para obtener imágenes de intensidad con una mejor resolución (por defecto, las imágenes son de tamaño \(128\times 128\) píxeles). Los parámetros de suavizado elegidos para calcular \@ref(eq:kde2Dunif) y \@ref(eq:heat) son 556.3km y 104.9km.



```r
data(activefires)
CvL_northcentre <- bw.CvL(activefires, ns = 50)
d_CvL_northcentre <- density.ppp(activefires, sigma = CvL_northcentre, leaveoneout = FALSE, dimyx = 512)

heat_CvL_northcentre <- bw.CvLHeat(activefires, ns = 50)
dheat_CvL_northcentre <- densityHeat.ppp(activefires, sigma = heat_CvL_northcentre, leaveoneout = FALSE, dimyx = 512)
```

Ambas estimaciones se juntan en un objeto `RasterBrick` que se representa en la Fig. \@ref(fig:USfiresintensity). Obsérvese que el dominio no es regular pues los estados de Florida, California del Sur y América Central hacen que la región objeto de estudio sea ciertamente irregular. En este caso, sería poco realista si el estimador utilizado proporciona intensidad en zonas como el Golfo de California/Mexico. El mapa de intensidad que se muestra a la izquierda de la Fig. \@ref(fig:USfiresintensity) muestra que el estimador con corrección uniforme ('uniformly-edge-corrected') distribuye la masa total por toda la región, provocando una sobre-suavización. Sin embargo, el mapa de la derecha, construido con el estimador de difusión, muestra una situación más realista, distribuyendo la masa de la intensidad acorde a los sucesos ocurridos.



```r
d_northcentre_stack <- stack(raster(d_CvL_northcentre), raster(dheat_CvL_northcentre))
names(d_northcentre_stack) <- c("CvL_gaus_U", "Diffusion")
pts_northcentre <- as.data.frame(activefires)
coordinates(pts_northcentre) <- ~ x + y
d_northcentre_stack <- d_northcentre_stack * 10^6

spplot(d_northcentre_stack, scales = list(draw = FALSE), col.regions = rev(terrain.colors(20)), colorkey = list(labels = list(cex = 5)), par.strip.text = list(cex = 5)) + layer(sp.points(pts_northcentre, pch = 20, col = 1))
```

<div class="figure" style="text-align: center">
<img src="img/Fire_NorthCentralAmerica_int.png" alt="Estimación basada en función núcleo para incendios (puntos negros) en EEUU y Centro América (sin las islas) desde el 24 de Febrero hasta el 3 de Marzo de 2022. Izquierda: estimador con corrección uniforme con núcleo Gaussiano. Derecha: estimador de difusión. El parámetro de suavizado fue obtenido con el criterio de Cronie--van Lieshout. Los valores de la intensidad son fuegos por mil km cuadrados." width="80%" />
<p class="caption">(\#fig:USfiresintensity)Estimación basada en función núcleo para incendios (puntos negros) en EEUU y Centro América (sin las islas) desde el 24 de Febrero hasta el 3 de Marzo de 2022. Izquierda: estimador con corrección uniforme con núcleo Gaussiano. Derecha: estimador de difusión. El parámetro de suavizado fue obtenido con el criterio de Cronie--van Lieshout. Los valores de la intensidad son fuegos por mil km cuadrados.</p>
</div>

### Estimadores basados en teselaciones de Voronoi
\index{Voronoi}

Como se ha visto, el comportamiento de los estimadores basados en funciones núcleo depende del parámetro de suavizado, e incluso en situaciones en las que hay cambios abruptos en la distribución espacial de los puntos, un único valor constante de este parámetro no puede representar el suavizamiento necesario en toda la región. Para dar una solución a este problema, se propuso un parámetro con variación espacial (adaptable a la estructura espacial) aunque a costa de una mayor complejidad [@davies2018fast;@baddeley2022diffusion]. Sin embargo, y como alternativa a esta propuesta, se pueden utilizar estimadores basados en teselaciones de Voronoi, que son no paramétricos [@BSV10].

Para cada \(x\in{\mathbf x}\), su celda de Voronoi/Dirichlet \({\mathcal V}_{x}\), consistente en todos los \(u \in W\) que están más cercanos a \(x\) que a cualquier otro elemento \(y\in{\mathbf x}\setminus\{x\}\), viene dada por
\begin{align}
\mathcal V_{x}
=
\{ u \in W: d(x,u) \leq d(y,u) \text{ para todo } y\in \mathbf{x}\setminus\{x\}\}.
\end{align}
El **estimador basado en teselaciones de Voronoi**, evaluado en cualquier punto arbitrario \(u \in W\), es de la forma
\begin{align}
\widehat{\lambda}^{V}(u)
=
\sum_{x\in \mathbf{x}}
\frac{\mathbf 1\{u\in\mathcal V_{x}\}}{|\mathcal V_{x}|}.
(\#eq:Vor)
\end{align}
El estimador \(\widehat{\lambda}^{V}(u)\) conserva la masa (al igual que \(\widehat \lambda^{\text{JD}}_{\sigma}(u)\)), y es insesgado si la intensidad real es constante (igual que \(\widehat \lambda^{\text{U}}_{\sigma}(u)\)), propiedades compartidas por el estimador de difusión. Sin embargo,  @moradi2019resample demostraron que \(\widehat{\lambda}^{V}(u)\) tiene una varianza alta, lo que implica una infrasuavización en áreas densas de puntos y una sobresuavización en áreas con pocos puntos. Por tanto, estos autores proponen corregir el problema de \(\widehat{\lambda}^{V}(u)\) mediante un sub-muestreo\index{sub-muestreo} de \(m \geq 1\) copias re-escaladas \({\mathbf x}\) a través de adelgazamientos independientes _(independent \(p\)-thinning)_. Su propuesta viene dada por
\begin{align}
\widehat{\lambda}_{p,m}^{V}(u)
=
\frac{1}{m}\sum_{i=1}^m
\frac{\widehat{\lambda}_i^{V}(u)}{p}
,
\ u\in W,
(\#eq:SmoothVor)
\end{align}
donde \(\widehat{\lambda}_i^{V}(u)\) es el estimador de Voronoi del \(i\)-ésimo patrón adelgazado. La idea es que este nuevo estimador balancee mejor la varianza en función de la cantidad de puntos presentes en la subregión, y este efecto se consigue con muestras de menor tamaño procedentes del patrón original. El estimador  \(\widehat{\lambda}_{p,m}^{V}(u)\) se conoce como **estimador de remuestreo-suavizado**\index{estimador de remuestreo-suavizado} _(resample-smoothed)_, y adicionalmente a las propiedades estadísticas de \(\widehat{\lambda}^{V}(u)\), tiene una varianza bastante más pequeña. En este caso, también se debe seleccionar a priori (\(m,p\)); sin embargo, @moradi2019resample proponen tanto una _rule-of-thumb_ (\(m=400\) y \(p \leq 0.2\)) como una validación cruzada. Ambos estimadores \@ref(eq:Vor) y \@ref(eq:SmoothVor) son accesibles por medio de la función 'densityVoronoi.ppp' de 'spatstat.explore' y en la que los argumentos 'f' y 'nrep' controlan la probabilidad \(p\) y el número de adelgazamientos \(m\). Fijando 'f=1' se puede obtener el estimador basado en Voronoi \@ref(eq:Vor).

A modo de ejemplo, se estima la intensidad de los incendios en Nepal (Sec. \@ref(secnepalfire)) mediante el método de Voronoi resample-smoothed \@ref(eq:SmoothVor) considerando diferentes probabilidades de retención para el adelgazamiento correspondiente.



```r
d_vor_1_nepal <- densityVoronoi.ppp(nepal, f = 1, nrep = 1)
d_vor_2_nepal <- densityVoronoi.ppp(nepal, f = 0.8, nrep = 400)
d_vor_3_nepal <- densityVoronoi.ppp(nepal, f = 0.6, nrep = 400)
d_vor_4_nepal <- densityVoronoi.ppp(nepal, f = 0.5, nrep = 400)
d_vor_5_nepal <- densityVoronoi.ppp(nepal, f = 0.4, nrep = 400)
d_vor_6_nepal <- densityVoronoi.ppp(nepal, f = 0.2, nrep = 400)
d_vor_7_nepal <- densityVoronoi.ppp(nepal, f = 0.1, nrep = 400)
d_vor_8_nepal <- densityVoronoi.ppp(nepal, f = 0.05, nrep = 400)
```

Las estimaciones obtenidas, al igual que las que proceden de `density.ppp`, son de clase `im` y se unen en un objeto `RasterBrick` para su representación gráfica.


```r
sp_int_nepal_v <- stack(raster(d_vor_1_nepal), raster(d_vor_2_nepal), raster(d_vor_3_nepal), raster(d_vor_4_nepal), raster(d_vor_5_nepal), raster(d_vor_6_nepal), raster(d_vor_7_nepal), raster(d_vor_8_nepal))
names(sp_int_nepal_v) <- NULL
names <- as.character(sort(c(seq(.2, 1, .2), 0.1, 0.05, 0.5), decreasing = TRUE))
names <- paste("p =", names)

sp_int_nepal_v <- sp_int_nepal_v * 10^7
at <- c(0, 0.3, 0.7, seq(2, 5, 1), 30)

spplot(sp_int_nepal_v, at = at, colorkey = list(labels = list(cex = 3)), col.regions = topo.colors(20), scales = list(draw = FALSE), par.strip.text = list(cex = 3), names.attr = names)
```

La Fig. \@ref(fig:nepalvor) muestra las intensidades procedentes de los estimadores Voronoi  _resample-smoothed_ para los incendios de Nepal, y para diferentes probabilidades de retención. Se puede observar un menor suavizamiento y mayor varianza para altas probabilidades. Asímismo, se puede ver que para probabilidades de retención menores que 0.2, el estimador proporciona mejores suavizamientos locales que los basados en suavizamientos fijos.


<div class="figure" style="text-align: center">
<img src="img/nepal2016_V.png" alt="Estimaciones de la intensidad de Voronoi resample-smoothed para los incendios en Nepal en 2016 para diferentes probabilidades de retención. La intensidad proporciona el número de incendios por diez mil km cuadrados." width="90%" />
<p class="caption">(\#fig:nepalvor)Estimaciones de la intensidad de Voronoi resample-smoothed para los incendios en Nepal en 2016 para diferentes probabilidades de retención. La intensidad proporciona el número de incendios por diez mil km cuadrados.</p>
</div>

### Características de segundo orden\index{características de segundo orden}: la función $K$ de Ripley

La función de intensidad presentada en las secciones anteriores describe el número esperado de puntos por unidad de espacio, y no tiene en cuenta la estructura de dependencia entre dichos puntos. Esta estructura, sin embargo, viene caracterizada a través de lo que se llaman características de segundo orden. Las funciones de segundo orden determinan la estructura de dependencia espacial\index{estructura de dependencia espacial} (o en su caso espacio-temporal, si interviene el tiempo) inherente al patrón puntual. La literatura ha propuesto varias funciones de segundo orden, de entre las cuales la función $K$ de Ripley es posiblemnte la más utilizada. Esta función se define de forma pragmática como el número medio de eventos en un radio $r$ alrededor de cualquier otro evento. Dicho de otra forma, la función $K(r)$ representa el número medio de eventos dentro de un círculo de radio $r$ alrededor de un evento típico del patrón (sin contar dicho evento central). De esta forma, $K(r)$ describe características del proceso de puntos a muchas escalas (tantas como diferentes $r$ se consideren). Esta función puede venir corregida por la intensidad de primer orden en el caso de procesos inhomogéneos. Ambas versiones de la función $K$ vienen implementadas en 'spatstat' a través de las funciones 'Kest' y 'Kinhom' para los casos homogéneo e inhomogéneo. Una propiedad interesante de esta función es que tiene una forma cerrada bajo el caso de aleatoriedad espacial completa, es decir, bajo la situación en la que el patrón de puntos es totalmente aleatorio, sin dependencia espacial alguna (llamado, en este caso, **proceso de Poisson**). Como bajo esta suposición, $K(r)=\pi r^2$ se puede contrastar si un cierto patrón es o no aleatorio, construyendo bandas de confianza sobre la función $K$ evaluada bajo simulaciones de aleatoriedad y evaluando la función $K$ empírica procedente de los datos. La función 'envelope' construye tales intervalos de confianza.

También se han utilizado otras funciones para describir y contrastar patrones espaciales; estas funciones están basadas en la distribución de distancias entre puntos que existiría en un patrón de Poisson, como por ejemplo, la función de distribución de distancias al vecino más próximo, la función de distribución de distancias a un punto fijo aleatorio, o la función $J$, una combinación de las anteriores. Todas estas funciones, incluida la función $K$, son en cierta forma funciones de distribución ya que, a cada escala o distancia $r$, todos los pares de puntos separados por una distancia menor que $r$ se usan para estimar el valor de la correspondiente función. En ocasiones, puede ser necesario disponer de una función que caracterice de forma no acumulativa el patrón, es decir, que tenga en cuenta tan sólo los pares de puntos que se encuentran separados por una distancia exactamente igual o similar a la distancia $r$. La función de correlación de par $g(r)$ (_pair correlation function_) es la herramienta apropiada en este caso [@BRT15].

A continuación se muestra el código y resultados de llevar a la práctica la estimación de la funcion $K$ (inhomogénea) y los correspondientes intervalos de confianza bajo aleatoriedad para los casos de incendios en Nepal y delitos en Medellín.



```r
d_nepal <- density.ppp(nepal, bw.scott, leaveoneout = TRUE)
en_nepal <- envelope(nepal, fun = Kinhom, correction = "border", nsim = 99, simulate = expression(rpoispp(d_nepal)), sigma = bw.scott, normpower = 2)

d_med <- density.ppp(medellin, bw.scott, leaveoneout = TRUE)
en_med <- envelope(medellin, fun = Kinhom, correction = "border", nsim = 99, simulate = expression(rpoispp(d_med)), sigma = bw.scott, normpower = 2)
```


```r
en_nepal$mmean <- NULL
plot(en_nepal, main = "", lwd = 3, cex.axis = 2.5, cex.lab = 2.5, legend = FALSE)

en_med$mmean <- NULL
plot(en_med, main = "", lwd = 3, cex.axis = 2.5, cex.lab = 2.5, legend = FALSE)
```

<div class="figure" style="text-align: center">
<img src="img/nepalk.png" alt="Funciones $K$ de Ripley para incendios en Nepal y delitos en Medellín" width="40%" /><img src="img/medk.png" alt="Funciones $K$ de Ripley para incendios en Nepal y delitos en Medellín" width="40%" />
<p class="caption">(\#fig:kplots)Funciones $K$ de Ripley para incendios en Nepal y delitos en Medellín</p>
</div>


## Patrones puntuales espaciales sobre redes lineales\index{redes lineales} {#seclineal}

En los últimos diez años, los patrones de puntos en redes lineales han recibido mucha atención científica. Una red lineal es un conjunto de segmentos (o aristas) unidos por nodos con un formato lineal, tipo una combinación convexa entre dos nodos. La explicación inicial detrás de la consideración de redes lineales, como espacios de estado de algunos procesos puntuales, podría estar en el hecho de que los objetos definidos en tales estructuras no pueden usar todo el espacio, y sus movimientos dependen fuertemente de su libertad sobre tales estructuras [@OS12]. En consecuencia, entre otras cosas, la distribución espacial de los puntos, así como la correlación entre ellos, debe estudiarse con respecto a la red subyacente. Sin embargo, no ha sido tan fácil lidiar con este cambio de soporte cuando se pretende adaptar metodologías estadísticas para el análisis de patrones de puntos en redes lineales. Los principales desafíos no fueron solo matemáticos/estadísticos, sino también computacionales [@moradi2018spatial;@baddeley2021].

Una red lineal es una unión de segmentos de línea \(l_i=[u_i,v_i]=\{tu_i + (1-t)v_i:0\leq t\leq 1\} \subset \mathbb R^2\), y una elección común de métrica sobre dicha estructura ha sido inicialmente la distancia de ruta más corta _(shortest-path distance)_ \(d_L(u,v)\), aunque más tarde @rakshit2017second propusieron otros tipos de distancias, incluida la distancia euclidiana. La idea es que moverse por la red de segmentos implica respetar la geometría de dicha red y por tanto las lineas rectas (que sería el caso de usar distancias euclideanas) no son adecuadas. La distancia de ruta más corta si que permite adaptarse a esta geometría. Sea \(Y\) un proceso puntual en una red lineal \(L\), la fórmula de Campbell \@ref(eq:compR2) se adapta como
\begin{equation*}
\mathbb E
\left[
\sum\limits_{y \in Y}
f(y)
\right]
=
\int_{L}
 f(z)
\lambda(z)
\mathrm{d}_1 z,
\end{equation*}
 donde \(\mathrm{d}_1\) denota integración con respecto a la longitud de arco. En este caso, \(\lambda(z)\) proporciona el número esperado de puntos por unidad de longitud de \(L\) en una vecindad de \(z \in L\). Se han desarrollado distintos estimadores de la intensidad para patrones en redes considerando métricas adecuadas y resolviendo ciertos obstáculos matemáticos. El lector puede leer más al respecto en @moradi2018spatial y @baddeley2021; en particular, se recomienda leer sobre el método de estimación noparamétrica basado en convoluciones bi-dimensionales de [@rakshit2019fast]. Dada una realización \({\mathbf y}= \{ y_1, y_2, \ldots, y_n \}\) de un proceso puntual \(Y\) sobre una red lineal \(L\), estos autores propusieron
\begin{equation}
    \widehat{\lambda}^\text{U}_{\sigma}(z)
    =
    \frac{1}{c_{\sigma,L}(z)}
    \sum_{i=1}^{n}
    \kappa_{\sigma}(z-y_i),
    \qquad
    z \in L,
    (\#eq:eqlu)
\end{equation}
con una corrección uniforme, y

\begin{equation}
    \widehat{\lambda}^\text{JD}_{\sigma}(z)
    =
    \sum_{i=1}^{n}
    \frac{
    \kappa_{\sigma}(z-y_i)
    }{
    c_{\sigma,L}(y_i)
    },
    \qquad z \in L,
    (\#eq:eqljd)
\end{equation}
con la corrección de Jones-Diggle, donde \(\kappa_{\sigma}\) es una función núcleo bivariante con suavizado \({\sigma}\), y
\begin{equation*}
    c_{\sigma,L}(z)=\int_L \kappa_{\sigma}(z-v) \mathrm{d}_1 v,
\end{equation*}
es una corrección de borde.

Los dos estimadores anteriores tienen propiedades estadísticas similares a las de sus análogos para patrones de puntos espaciales en \(\mathbb R^2\) (es decir, los estimadores \@ref(eq:kde2Dunif) y
\@ref(eq:kde2DJD)), y se pueden calcular rápidamente incluso en redes grandes y para grandes anchos de banda (parámetros de suavización). El cálculo rápido se logra mediante la transformada rápida de Fourier (FFT) [@silverman1982algorithm]. Además, @rakshit2019fast propusieron utilizar las versiones adaptadas de la regla de Scott, a la cual se puede acceder a través de la funciones 'bw.scott.iso' de 'spatstat.linnet', para obtener un ancho de banda óptimo. Nótese que el cálculo rápido de los estimadores anteriores simplifica aún más el cálculo de los estimadores de intensidad basados en el núcleo adaptativo y el riesgo relativo sobre las estructuras de red [@rakshit2019fast].

También se recuerda que @moradi2019resample propusieron su enfoque de sub-muestreo basado en Voronoi para procesos de puntos generales; para patrones de puntos en redes lineales puede calcularse mediante la función 'densityVoronoi.lpp' de 'spatstat.linnet'.

Como ejemplo práctico para esta sección, se estudia la distribución espacial de delitos callejeros en Valencia. Valencia es la tercera ciudad más grande de España, siendo la capital de la Comunidad Valenciana. El territorio urbano de Valencia encierra un área de 134,65 km\(^2\), con más de 800000 habitantes en el municipio. El conjunto de datos consta de las ubicaciones de 90247 delitos callejeros como agresión (55610 casos), robo (25342 casos), robo contra la mujer con violencia (454 casos) y otros tipos de delitos (8841 casos). Estos delitos se cometieron entre 2010 y 2020. Sin embargo, en lo que sigue, el análisis únicamente se centra en los datos correspondientes al año 2020, que incluye 6868 casos, de los cuales 4077 son agresiones, 2060 son robos y 66 se relacionan con delitos contra la mujer con violencia. Este conjunto de datos es propiedad de la Generalitat Valenciana (GV), se obtuvieron a través del teléfono de emergencias 112, y se puso a disposición de los autores gracias a un convenio entre GV y la Universidad Jaume I.

A continuación, estimar el parámetro de suavizado utilizando la regla general de Scott, que da 584,1m. La función 'densityQuick.lpp' de 'spatstat.linnet' se usa para calcular cualquiera de los estimadores \@ref(eq:eqlu) y \@ref(eq:eqljd) en los que su valor predeterminado calcula el estimador de borde uniforme corregido \@ref(eq:eqlu).




```r
data(valencia)
scott_valencia <- bw.scott.iso(valencia) # Scott rule
d_scott_valencia <- densityQuick.lpp(valencia, sigma = scott_valencia, leaveoneout = FALSE, positive = TRUE, dimyx = 512)
d_scott_valencia <- d_scott_valencia * 1000
```

Las imágenes obtenidas son de tipo 'linim', y se convierten en objetos de clase 'im' antes de pasarlas a objetos 'raster'.


```r
par(mfrow = c(1, 2))
plot(valencia$domain$window, lwd = 4)
plot(valencia, pch = 20, main = "", lwd = 4, cex = 1, add = T, cols = "red", col = "blue")
plot(raster(as.im(d_scott_valencia)), main = "", axis.args = list(cex.axis = 4), legend.width = 2, zlim = c(0, 6))
plot(valencia$domain$window, add = TRUE, lwd = 4)
par(mfrow = c(1, 1))
```

La Fig. \@ref(fig:titleV) muestra la intensidad estimada junto con los eventos de delitos. Dicha intensidad identifica las zonas central y norte de la ciudad de  Valencia como áreas de alto riesgo junto
con otras zonas de bajo riesgo como son el este y la costa de la ciudad.

<div class="figure" style="text-align: center">
<img src="img/2020_vlc_scott.png" alt="Intensidad estimada por función núcleo, usando un estimador borde uniforme corregido (izquierda), para los datos de delitos (puntos rojos) en Valencia durante 2020 (derecha). Los valores de intensidad muestran número de crímenes por km lineal." width="40%" /><img src="img/2020_vlc.png" alt="Intensidad estimada por función núcleo, usando un estimador borde uniforme corregido (izquierda), para los datos de delitos (puntos rojos) en Valencia durante 2020 (derecha). Los valores de intensidad muestran número de crímenes por km lineal." width="40%" />
<p class="caption">(\#fig:titleV)Intensidad estimada por función núcleo, usando un estimador borde uniforme corregido (izquierda), para los datos de delitos (puntos rojos) en Valencia durante 2020 (derecha). Los valores de intensidad muestran número de crímenes por km lineal.</p>
</div>


```r
d_vlc <- densityQuick.lpp(valencia, sigma = scott_valencia, leaveoneout = TRUE, positive = TRUE, at = "points", dimyx = 512)
d_vlc_im <- densityQuick.lpp(valencia, sigma = scott_valencia, leaveoneout = TRUE, positive = TRUE, dimyx = 512)
```

Finalmente, se muestra la función $K$ de Ripley y el intervalo de confianza bajo un proceso de Poisson en una red lineal (ver Fig. \@ref(fig:VlcK) [@ang2012geometrically;@rakshit2019efficient]. Se observa que la función $K$ empírica cae dentro de las bandas indicando que el tipo de delitos considerados, en 2020, es compatible con un proceso aleatorio. Obsérvese que al no considerar el tiempo, podemos detectar clusters espaciales que no existen en realidad pues estos desaparecerían con la evolución temporal.


```r
sim_vlc <- rpoislpp(lambda = d_vlc_im, L = net_vlc, nsim = 199)
library(spatstat.Knet)
K_vlc <- Knetinhom(valencia, lambda = as.numeric(d_vlc))
r <- K_vlc$r

K_sim <- lapply(X = 1:199, function(i) {
  sigma <- bw.scott.iso(sim_vlc[[i]])
  lambda <- densityQuick.lpp(sim_vlc[[i]], sigma = sigma, leaveoneout = TRUE, positive = TRUE, at = "points", dimyx = 512)
  Ksim <- Knetinhom(sim_vlc[[i]], lambda = as.numeric(lambda), r = r)
  return(Ksim)
})
```


```r
K_nsim_df <- as.data.frame(do.call(cbind, d_nsim))
K_nsim_df_est <- K_nsim_df[, seq(3, 399, by = 2)]

maxn <- function(n) function(x) order(x, decreasing = TRUE)[n]
minn <- function(n) function(x) order(x, decreasing = FALSE)[n]

Kmin <- apply(K_nsim_df_est, 1, function(x) x[minn(5)(x)])
Kmax <- apply(K_nsim_df_est, 1, function(x) x[maxn(5)(x)])
```


```r
plot(r, Kmin, type = "n", col = "grey", ylim = c(0, 270), xlab = "r", ylab = expression(italic(K[inhom])))
points(r, Kmax, type = "n", col = "grey")
polygon(c(r, rev(r)), c(Kmax, rev(Kmin)), col = "grey", border = "grey")
points(r, K_vlc$est, type = "l")
```

<div class="figure" style="text-align: center">
<img src="img/vlcK.png" alt="Función $K$ para los delitos en Valencia, junto con la envoltura bajo un proceso de Poisson." width="50%" />
<p class="caption">(\#fig:VlcK)Función $K$ para los delitos en Valencia, junto con la envoltura bajo un proceso de Poisson.</p>
</div>


::: {.infobox_resume data-latex=""}
### Resumen {-}

La teoría de procesos puntuales espaciales constituye la base para el análisis de eventos observados geográficamente a través de sus coordenadas (longitud, latitud) en un espacio bi-dimensional.  Esta es una de las ramas del campo de la estadística espacial en conjunción con la de procesos estocásticos. De hecho, un proceso puntual espacial es un proceso estocástico cuyas realizaciones consisten en un conjunto numerable de puntos (llamados en muchas ocasiones eventos). Heurísticamente, se trata de un conjunto de datos que se encuentran en una región concreta (área de estudio). Los puntos pueden representar cualquier población espacialmente explícita, como localizaciones de animales, nidos de aves, epicentros de terremotos, galaxias, crímenes, etc. El modelo estadístico más conocido para el análisis de patrones puntuales espaciales es el proceso puntual espacial de Poisson (asociado a la condición de aleatoriedad espacial completa). A partir del modelo de Poisson se construyen modelos más complejos. La modelización pasa por determinar las intensidades de primer y segundo orden que caracterizarán las propiedades básicas del comportamiento de los puntos. En este capítulo se proponen varios estimadores de la función de intensidad de primer orden junto con sus elementos asociados relacionados con funciones núcleo, parámetro de suavizado y correcciones de borde. Se consideran también algunos aspectos de medidas de segundo orden. El capítulo finaliza con aspectos sobre el cambio de soporte del plano euclídeo a redes lineales.
:::
