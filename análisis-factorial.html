<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Capítulo 33 Análisis factorial | Fundamentos de ciencia de datos con R</title>
<meta name="author" content="Gema Fernández-Avilés y José-María Montero">
<meta name="description" content="José-María Montero\(^{a}\) y José Luis Alfaro Navarro\(^{a}\) \(^{a}\)Universidad de Castilla-La Mancha  33.1 Introducción Según el trabajo pionero de Harman (1976), el objeto del Análisis...">
<meta name="generator" content="bookdown 0.28 with bs4_book()">
<meta property="og:title" content="Capítulo 33 Análisis factorial | Fundamentos de ciencia de datos con R">
<meta property="og:type" content="book">
<meta property="og:image" content="/img/cover.png">
<meta property="og:description" content="José-María Montero\(^{a}\) y José Luis Alfaro Navarro\(^{a}\) \(^{a}\)Universidad de Castilla-La Mancha  33.1 Introducción Según el trabajo pionero de Harman (1976), el objeto del Análisis...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Capítulo 33 Análisis factorial | Fundamentos de ciencia de datos con R">
<meta name="twitter:description" content="José-María Montero\(^{a}\) y José Luis Alfaro Navarro\(^{a}\) \(^{a}\)Universidad de Castilla-La Mancha  33.1 Introducción Según el trabajo pionero de Harman (1976), el objeto del Análisis...">
<meta name="twitter:image" content="/img/cover.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.2/transition.js"></script><script src="libs/bs3compat-0.4.2/tabs.js"></script><script src="libs/bs3compat-0.4.2/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><link href="libs/tabwid-1.1.0/tabwid.css" rel="stylesheet">
<link href="libs/tabwid-1.1.0/scrool.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css">
<link rel="stylesheet" href="bs4_book.css">
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Fundamentos de ciencia de datos con <strong>R</strong></a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Buscar" aria-label="Buscar">
</form>

      <nav aria-label="Contenido"><h2>Contenido</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Prefacio</a></li>
<li><a class="" href="pr%C3%B3logo.html">Prólogo</a></li>
<li class="book-part">Ciencia, datos, software… y científicos</li>
<li><a class="" href="ciencia-datos.html"><span class="header-section-number">1</span> ¿Es la ciencia de datos una ciencia?</a></li>
<li><a class="" href="metodologia.html"><span class="header-section-number">2</span> Metodología en ciencia de datos</a></li>
<li><a class="" href="ch-110003.html"><span class="header-section-number">3</span> R para ciencia de datos</a></li>
<li><a class="" href="cap-etica.html"><span class="header-section-number">4</span> Ética en la ciencia de datos</a></li>
<li class="book-part">Bienvenidos a la jungla de datos</li>
<li><a class="" href="datos-sql.html"><span class="header-section-number">5</span> Gestión de bases de datos relacionales</a></li>
<li><a class="" href="cap-nosql.html"><span class="header-section-number">6</span> Gestión de bases de datos NoSQL</a></li>
<li><a class="" href="DGDQM.html"><span class="header-section-number">7</span> Gobierno, gestión y calidad del dato</a></li>
<li><a class="" href="id_130009.html"><span class="header-section-number">8</span> Integración y limpieza de datos</a></li>
<li><a class="" href="chap-feature.html"><span class="header-section-number">9</span> Selección y transformación de variables</a></li>
<li><a class="" href="chap-herramientas.html"><span class="header-section-number">10</span> Herramientas para el análisis en ciencia de datos</a></li>
<li><a class="" href="id_120006-aed.html"><span class="header-section-number">11</span> Análisis exploratorio de datos</a></li>
<li class="book-part">Fundamentos de estadística</li>
<li><a class="" href="Funda-probab.html"><span class="header-section-number">12</span> Probabilidad</a></li>
<li><a class="" href="Fundainfer.html"><span class="header-section-number">13</span> Inferencia estadística</a></li>
<li><a class="" href="muestreo.html"><span class="header-section-number">14</span> Muestreo y remuestreo</a></li>
<li class="book-part">Modelización estadística</li>
<li><a class="" href="cap-lm.html"><span class="header-section-number">15</span> Modelización lineal</a></li>
<li><a class="" href="cap-glm.html"><span class="header-section-number">16</span> Modelos lineales generalizados</a></li>
<li><a class="" href="cap-gam.html"><span class="header-section-number">17</span> Modelos aditivos generalizados</a></li>
<li><a class="" href="cap-mxm.html"><span class="header-section-number">18</span> Modelos mixtos</a></li>
<li><a class="" href="cap-sparse.html"><span class="header-section-number">19</span> Modelos sparse y métodos penalizados de regresión</a></li>
<li><a class="" href="cap-series-temp.html"><span class="header-section-number">20</span> Modelización de series temporales</a></li>
<li><a class="" href="cap-discriminante.html"><span class="header-section-number">21</span> Análisis discriminante</a></li>
<li><a class="" href="cap-conjunto.html"><span class="header-section-number">22</span> Análisis conjunto</a></li>
<li><a class="" href="tablas-contingencia.html"><span class="header-section-number">23</span> Análisis de tablas de contingencia</a></li>
<li class="book-part">Machine learning supervisado</li>
<li><a class="" href="cap-arboles.html"><span class="header-section-number">24</span> Árboles de clasificación y regresión</a></li>
<li><a class="" href="cap-svm.html"><span class="header-section-number">25</span> Máquinas de vector soporte</a></li>
<li><a class="" href="cap-knn.html"><span class="header-section-number">26</span> Clasificador \(k\)-vecinos más próximos</a></li>
<li><a class="" href="cap-naive-bayes.html"><span class="header-section-number">27</span> Naive Bayes</a></li>
<li><a class="" href="cap-bagg-rf.html"><span class="header-section-number">28</span> Métodos ensamblados: \(\bf \textit {bagging}\) y \(\bf \textit{random}\) \(\bf \textit{forest}\)</a></li>
<li><a class="" href="cap-boosting-xgboost.html"><span class="header-section-number">29</span> \(\bf \textit{Boosting}\) y el algoritmo XGBoost</a></li>
<li class="book-part">Machine learning no supervisado</li>
<li><a class="" href="an%C3%A1lisis-cl%C3%BAster-clusterizaci%C3%B3n-jer%C3%A1rquica.html"><span class="header-section-number">30</span> Análisis clúster: clusterización jerárquica</a></li>
<li><a class="" href="no-jerarquico.html"><span class="header-section-number">31</span> Análisis clúster: clusterización no jerárquica</a></li>
<li><a class="" href="acp.html"><span class="header-section-number">32</span> Análisis de componentes principales</a></li>
<li><a class="active" href="an%C3%A1lisis-factorial.html"><span class="header-section-number">33</span> Análisis factorial</a></li>
<li><a class="" href="escalamiento-multidimensional.html"><span class="header-section-number">34</span> Escalamiento multidimensional</a></li>
<li><a class="" href="correspondencias.html"><span class="header-section-number">35</span> Análisis de correspondencias</a></li>
<li class="book-part">Deep learning</li>
<li><a class="" href="capNN.html"><span class="header-section-number">36</span> Redes neuronales artificiales</a></li>
<li><a class="" href="cap-redes-convol.html"><span class="header-section-number">37</span> Redes neuronales convolucionales</a></li>
<li class="book-part">Ciencia de datos de texto y redes</li>
<li><a class="" href="mineria-textos.html"><span class="header-section-number">38</span> Minería de textos</a></li>
<li><a class="" href="grafos.html"><span class="header-section-number">39</span> Análisis de grafos y redes sociales</a></li>
<li class="book-part">Ciencia de datos espaciales</li>
<li><a class="" href="datos-espaciales.html"><span class="header-section-number">40</span> Trabajando con datos espaciales</a></li>
<li><a class="" href="geo.html"><span class="header-section-number">41</span> Geoestadística</a></li>
<li><a class="" href="cap-econom-esp.html"><span class="header-section-number">42</span> Modelos econométricos espaciales</a></li>
<li><a class="" href="cap-pp.html"><span class="header-section-number">43</span> Procesos de puntos</a></li>
<li class="book-part">Comunica y colabora</li>
<li><a class="" href="id_120007-informes.html"><span class="header-section-number">44</span> Informes reproducibles con R Markdown y Quarto</a></li>
<li><a class="" href="shiny.html"><span class="header-section-number">45</span> Creación de aplicaciones web interactivas con Shiny</a></li>
<li><a class="" href="github.html"><span class="header-section-number">46</span> Git y GitHub R</a></li>
<li><a class="" href="geoproces.html"><span class="header-section-number">47</span> Geoprocesamiento en nube</a></li>
<li class="book-part">Casos de estudio en ciencia de datos</li>
<li><a class="" href="cap-crimen.html"><span class="header-section-number">48</span> Análisis de una red criminal</a></li>
<li><a class="" href="cap-publicidad.html"><span class="header-section-number">49</span> Optimización de inversiones publicitarias</a></li>
<li><a class="" href="cap-twitter.html"><span class="header-section-number">50</span> ¿Cómo tuitea Elon Musk?</a></li>
<li><a class="" href="cap-periodismo.html"><span class="header-section-number">51</span> Análisis electoral: de RStudio a su periódico</a></li>
<li><a class="" href="paro-clm.html"><span class="header-section-number">52</span> El impacto de las crisis financiera y de la COVID-19 en el paro de CLM</a></li>
<li><a class="" href="cap-rfm.html"><span class="header-section-number">53</span> Segmentación de clientes en el comercio minorista</a></li>
<li><a class="" href="cap-medicina.html"><span class="header-section-number">54</span> Análisis de datos en medicina</a></li>
<li><a class="" href="cap-futbol.html"><span class="header-section-number">55</span> Messi y Ronaldo: dos ídolos desde la perspectiva de los datos</a></li>
<li><a class="" href="cambioclimatico.html"><span class="header-section-number">56</span> Una nota sobre el cambio climático</a></li>
<li><a class="" href="cap-sist-exp.html"><span class="header-section-number">57</span> Implementación de un sistema experto en el ámbito pediátrico</a></li>
<li><a class="" href="cap-ree.html"><span class="header-section-number">58</span> Predicción de consumo eléctrico con redes neuronales artificiales</a></li>
<li><a class="" href="nlp-textil.html"><span class="header-section-number">59</span> El procesamiento del lenguaje natural para tendencias de moda en textil</a></li>
<li><a class="" href="cap-fraude.html"><span class="header-section-number">60</span> Detección de fraude de tarjetas de crédito</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="info-session.html"><span class="header-section-number">A</span> Información de la sesión</a></li>
<li><a class="" href="referncias.html">Referncias</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="análisis-factorial" class="section level1" number="33">
<h1>
<span class="header-section-number">Capítulo 33</span> Análisis factorial<a class="anchor" aria-label="anchor" href="#an%C3%A1lisis-factorial"><i class="fas fa-link"></i></a>
</h1>
<p><em>José-María Montero</em><span class="math inline">\(^{a}\)</span> y <em>José Luis Alfaro Navarro</em><span class="math inline">\(^{a}\)</span></p>
<p><span class="math inline">\(^{a}\)</span>Universidad de Castilla-La Mancha</p>
<div id="introaf" class="section level2" number="33.1">
<h2>
<span class="header-section-number">33.1</span> Introducción<a class="anchor" aria-label="anchor" href="#introaf"><i class="fas fa-link"></i></a>
</h2>
<p>Según el trabajo pionero de <span class="citation">Harman (<a href="referncias.html#ref-harman1976">1976</a>)</span>, el objeto del <strong>Análisis Factorial</strong> (AF) es la representación de una variable <span class="math inline">\(X_j\)</span> en términos de varios factores subyacentes no observables.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Se asumirá la representación lineal, por sencillez, pero puede ser cualquier otra.&lt;/p&gt;"><sup>227</sup></a> En el marco lineal, y considerando <span class="math inline">\(p\)</span> variables,<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Por las mismas razones que en el análisis de componentes principales (ACP), se trabaja con las variables estandarizadas; véase Cap. &lt;a href="acp.html#acp"&gt;32&lt;/a&gt;.&lt;/p&gt;'><sup>228</sup></a> hay varias alternativas dependiendo del objetivo que se pretenda:</p>
<ul>
<li>La captura de la mayor cantidad posible de la varianza de dichas variables (o “explicación” de su varianza).</li>
<li>La mejor reproducción (o “explicación”) de sus correlaciones observadas.</li>
</ul>
<p>A modo introductorio, supónganse dos variables politómicas que surgen de las respuestas de <span class="math inline">\(N\)</span> futbolistas profesionales a dos preguntas: (1) ¿Está usted a gusto en el club? y (2) ¿Se quedaría en el club la siguiente temporada? Las posibles respuestas son: 1, 2, 3, 4, 5 (1 “en total desacuerdo” y 5 “totalmente de acuerdo”).</p>
<p>Cada variable tiene su varianza (nula si todos los futbolistas opinasen igual y máxima si la mitad marcase el 1 y la otra mitad el 5). Esta varianza puede ser <em>común</em> o <em>compartida</em> por las dos variables, o no. Lo normal es que cuanto más a gusto estén los futbolistas en su club mayor sea su deseo de permanecer en él la siguiente temporada, por lo que gran parte de la variabilidad de cada una de las variables sería compartida (ya que la relación –lineal– entre ellas es positiva). El resto de la variabilidad sería <em>específica</em> de cada variable (puede que un futbolista esté muy bien en el club, pero quiera ir a otro más prestigioso; o que esté mal, pero a su familia le encante la ciudad) o <em>residual</em> (normalmente debida a factores de medida). El porcentaje de <em>varianza compartida</em> se mide a través del coeficiente de determinación lineal, <span class="math inline">\(r^2\)</span>. El resto, hasta la varianza unidad, o el 100%, es <em>varianza única</em> de cada variable, que incluye tanto la específica como la residual.</p>
<p>
</p>
<p>De acuerdo con <span class="citation">De la Fuente (<a href="referncias.html#ref-santiagodelafuente2011">2011</a>)</span>, en el AF caben dos enfoques:<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;No son los únicos.&lt;/p&gt;"><sup>229</sup></a></p>
<ul>
<li>El análisis de toda la varianza (común y no común).</li>
<li>El análisis, únicamente, de la varianza común. </li>
</ul>
<p>Ambos caben bajo el paraguas genérico del AF; ambos se basan en las relaciones entre las variables para identificar grupos de ellas asociadas entre sí. Sin embargo, del primero se ocupa el ACP (Cap. <a href="acp.html#acp">32</a>) y, si se parte de la matriz de correlaciones (cuyas entradas fuera de la diagonal principal, al cuadrado, indican la proporción de varianza compartida por las variables que se cruzan en dicha entrada), esta lleva unos en la diagonal principal. Al segundo se le aplica la denominación de AF y en la matriz de correlaciones se sustituyen los unos de la diagonal principal por la varianza que cada variable comparte con las demás (su <strong>comunalidad</strong>). Por eso se dice que el objetivo del AF es la explicación de la varianza compartida o común de las variables en estudio mediante una serie de <strong>factores comunes</strong> latentes.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Ambos enfoques dan resultados similares cuando hay más de 30 variables y las comunalidades (véase Sec. &lt;a href="an%C3%A1lisis-factorial.html#modelobasicoaf"&gt;33.2.1&lt;/a&gt;) son superiores a 0,70, y se interpretan de manera casi idéntica.&lt;/p&gt;'><sup>230</sup></a></p>
<p>
</p>
<p>El AF puede ser exploratorio o confirmatorio. En el primero no se establecen consideraciones <em>a priori</em> sobre el número de factores comunes a extraer, sino que este se determina a lo largo del análisis. Por el contrario, en el segundo se trata de contrastar hipótesis relativas al número de factores comunes, así como sobre qué variables serán agrupadas o tendrán más peso en cada factor. Una práctica habitual es validar mediante el <strong>análisis factorial confirmatorio</strong> los modelos teóricos basados en los resultados del <strong>análisis factorial exploratorio</strong>. Sin embargo, <span class="citation">Pérez-Gil, Chacón, and Moreno (<a href="referncias.html#ref-rodriguez2000validez">2000</a>)</span> alertan de los peligros de esta práctica. Este capítulo considera la versión exploratoria del AF.</p>
<p>
A efectos prácticos, se utilizará la base de datos <code>TIC2021</code> del paquete <code>CDR</code>, ya trabajada en el Cap. <a href="acp.html#acp">32</a> para el ACP, relativa al uso (por empresas e individuos) y equipación (de los hogares) de las TIC en los países de la UE-27, así como la librería <code>psych</code> <span class="citation">(<a href="referncias.html#ref-Revelle2022">Revelle 2022</a>)</span> de <strong>R</strong>.</p>
<div class="sourceCode" id="cb452"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://personality-project.org/r/psych/">"psych"</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"CDR"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"TIC2021"</span><span class="op">)</span></span></code></pre></div>
</div>
<div id="elementos-teóricos-del-análisis-factorial" class="section level2" number="33.2">
<h2>
<span class="header-section-number">33.2</span> Elementos teóricos del análisis factorial<a class="anchor" aria-label="anchor" href="#elementos-te%C3%B3ricos-del-an%C3%A1lisis-factorial"><i class="fas fa-link"></i></a>
</h2>
<div id="modelobasicoaf" class="section level3" number="33.2.1">
<h3>
<span class="header-section-number">33.2.1</span> Modelo básico y terminología<a class="anchor" aria-label="anchor" href="#modelobasicoaf"><i class="fas fa-link"></i></a>
</h3>
<p>Considérense <span class="math inline">\(p\)</span> variables <span class="math inline">\(\{X_1, X_2,..., X_p\}\)</span> y <span class="math inline">\(N\)</span> elementos, objetos o individuos, siendo las matrices de datos, <span class="math inline">\(\bf X\)</span>, y datos estandarizados, <span class="math inline">\(\bf Z\)</span>, las siguientes:</p>
<p><span class="math display">\[\bf X=\left(\begin{matrix} x_{11} &amp; x_{12} &amp; \cdots &amp;x_{1N}\\
x_{21}&amp;x_{22}&amp;\cdots&amp;x_{2N}\\
\vdots&amp;\vdots&amp;\ddots     &amp;\vdots\\
x_{p1}&amp;x_{p2}&amp;\cdots&amp;x_{pN}\\
\end {matrix}\right),\quad
\bf Z=\left(\begin{matrix} z_{11} &amp; z_{12} &amp; \cdots &amp;z_{1N}\\
z_{21}&amp;z_{22}&amp;\cdots&amp;z_{2N}\\
\vdots&amp;\vdots&amp;\ddots     &amp;\vdots\\
z_{p1}&amp;z_{p2}&amp;\cdots&amp;z_{pN}\\
\end {matrix}\right),\]</span></p>
<p>donde el primer subíndice indica la variable y el segundo, el elemento.</p>
<p>Mientras que el enfoque de componentes principales está representado por:
<span class="math display" id="eq:eqaf1">\[\begin{equation} Z_{j}=a_{j1}F_1+a_{j2}F_2+ \cdots +a_{jp}F_p, \quad j=1,2,\cdots,p,
\tag{33.1}
\end{equation}\]</span>
en el enfoque AF clásico el modelo teórico es:
<span class="math display" id="eq:eqaf2">\[\begin{equation}
Z_{j}=a_{j1}F_1+a_{j2}F_2+ \cdots +a_{jk}F_k + b_jSP_j+c_jE_j, \quad j=1,2,\cdots,p,
\tag{33.2}
\end{equation}\]</span>
donde <span class="math inline">\(Z_{j}, \hspace{0,1cm} j=1,2,\cdots, p\)</span>, se modeliza, linealmente, en términos de <span class="math inline">\((i)\)</span> <span class="math inline">\(k\ll p\)</span> <strong>factores comunes</strong>, <span class="math inline">\(F_m,\hspace{0,1cm} m=1,2,\cdots,k\)</span>, que dan cuenta de la correlaciones entre las variables <span class="math inline">\(Z_{j}, \hspace{0,1cm} j=1,2,\cdots, p\)</span>, y <span class="math inline">\((ii)\)</span> un <strong>factor específico</strong>, <span class="math inline">\(SP_j, \hspace{0,1cm} j=1,2,\cdots,p\)</span>, y un término de error, <span class="math inline">\(E_j, \hspace{0,1cm} j=1,2,\cdots,p,\)</span> que dan cuenta de la <strong>varianza no compartida</strong> (específica y residual, respectivamente). Los coeficientes <span class="math inline">\(a_{jm}\)</span> se denominan <strong>cargas factoriales</strong> y, aunque su notación es igual que en el modelo de componentes principales, no tienen por qué coincidir; el problema básico del AF es precisamente la estimación de dichas cargas. En lo que sigue, se aunarán el factor específico y el término de error de <span class="math inline">\(Z_{j}\)</span> en un <strong>factor único</strong>, <span class="math inline">\(U_{j}\)</span>, con lo que:</p>
<p>
</p>
<p><span class="math display" id="eq:eqaf3">\[\begin{equation}
Z_{j}=a_{j1}F_1+a_{j2}F_2+ \cdots +a_{jk}F_k + d_jU_j, \quad j=1,2,\cdots,p.
\tag{33.3}
\end{equation}\]</span></p>
<p>Los supuestos del modelo <a href="an%C3%A1lisis-factorial.html#eq:eqaf3">(33.3)</a> son los siguientes:</p>
<ul>
<li>Como en la práctica los factores comunes y únicos son desconocidos, sin pérdida de generalidad pueden suponerse con media cero y varianza unitaria;</li>
<li>los factores únicos se suponen independientes entre sí y de los factores comunes; </li>
<li>y dado que los factores involucrados en el modelo se consideran variables aleatorias, si se asume normalidad, e independencia de los factores comunes, <span class="math inline">\(\{F_{1},F_{2},\cdots, F_k\}\)</span> sigue una distribución normal multivariante y <span class="math inline">\(Z_{j},\hspace{0,1cm} j=1,2,\cdots,p,\)</span> una distribución normal.</li>
</ul>
<p>En términos de valores observados, el modelo AF <a href="an%C3%A1lisis-factorial.html#eq:eqaf3">(33.3)</a> viene dado por:<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Aunque en el modelo figuran explícitamente los valores de los factores, en la práctica hay que estimarlos. En otras versiones del AF estos valores se estiman conjuntamente con los parámetros.&lt;/p&gt;"><sup>231</sup></a>
<span class="math display" id="eq:eqaf4">\[\begin{equation}
z_{ji}=a_{j1}f_{1i}+a_{j2}f_{2i}+ \cdots +a_{jk}f_{ki} + d_ju_{ji}, \quad i=1,2,\cdots,N; \hspace{0.1cm} j=1,2,\cdots,p.
\tag{33.4}
\end{equation}\]</span>
El modelo AF es muy parecido al de regresión lineal: una variable se describe como una combinación lineal de otro conjunto de variables más un residuo. Sin embargo, en el análisis de regresión las variables son observables, mientras que en el AF son construcciones hipotéticas que solo pueden estimarse a partir de los datos observados. Los propios factores se estiman en una etapa posterior del análisis.</p>
<p>En términos matriciales, y considerando:
<span class="math display">\[\bf z=\left(\begin{matrix}Z_{1}\\
Z_{2}\\
\vdots\\
Z_{p}\\
\end{matrix}\right),\quad \bf f=\left(\begin{matrix}
F_{1}\\
F_{2}\\
\vdots\\
F_{k}\\
\end{matrix}\right),\quad \bf u=\left(\begin{matrix} U_{1}\\
U_{2}\\
\vdots\\
U_{p}\\
\end{matrix}\right),\]</span>
<span class="math display">\[\bf A=\left(\begin{matrix} a_{11} &amp; a_{12} &amp; \cdots &amp;a_{1k}\\
a_{21}&amp;a_{22}&amp;\cdots&amp;a_{2k}\\
\vdots&amp;\vdots&amp;\ddots     &amp;\vdots\\
a_{p1}&amp;a_{p2}&amp;\cdots&amp;a_{pk}\\
\end {matrix}\right),\quad \bf D=\left(\begin{matrix} d_{1} &amp; 0 &amp; \cdots &amp;0\\
0&amp;d_{2}&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\ddots     &amp;\vdots\\
0&amp;0&amp;\cdots&amp;d_{p}\\
\end {matrix}\right),\]</span>
el modelo <a href="an%C3%A1lisis-factorial.html#eq:eqaf3">(33.3)</a> puede expresarse como <span class="math inline">\(\bf z=\bf A \bf f +\bf D \bf u\)</span>.</p>
<p>Centrándonos en el modelo <a href="an%C3%A1lisis-factorial.html#eq:eqaf3">(33.3)</a>, la varianza de <span class="math inline">\(Z_j\)</span> viene dada por:
<span class="math display" id="eq:eqaf4b">\[\begin{equation}
V(Z_j)=1= \sum_{m=1}^{k} a_{jm}^2+
2\sum_{m&lt; q }^{k} a_{jm} a_{jq} r_{(F_{mi},F_{qi})}  +d_j^2,
\tag{33.5}
\end{equation}\]</span>
y si los factores comunes están incorrelacionados, <span class="math inline">\(V(Z_j)=1= \sum_{m=1}^{k} a_{jm}^2+ d_j^2\)</span>.</p>
<p>De la expresión <a href="an%C3%A1lisis-factorial.html#eq:eqaf4b">(33.5)</a> surgen las siguientes definiciones:</p>
<ul>
<li>
<span class="math inline">\(a_{jm}^2\)</span> es la contribución de <span class="math inline">\(F_m\)</span> a la varianza de <span class="math inline">\(Z_j\)</span>.</li>
<li>
<span class="math inline">\(V_m=\sum_{j=1}^{p}a_{jm}^2\)</span> es la contribución de <span class="math inline">\(F_m\)</span> a la suma de las varianzas de todas las variables <span class="math inline">\(Z_j,\hspace{0,1cm} j=1,2,\cdots,p\)</span>.</li>
<li>
<span class="math inline">\(V=\sum_{m=1}^{k}V_m\)</span> es la contribución de todos los factores comunes a la varianza de todas las variables <span class="math inline">\(Z_j,\hspace{0,1cm} j=1,2,\cdots,p\)</span>.</li>
<li>
<span class="math inline">\(\frac{V} {p}\)</span> es un indicador de la <em>completitud</em> del análisis factorial. </li>
<li>
<span class="math inline">\(h_j^2=a_{j1}^2+a_{j2}^2+\cdots+a_{jk}^2\)</span> es la comunalidad de <span class="math inline">\(Z_j,\hspace{0,1cm} j=1,2,\cdots,p\)</span>, es decir la contribución de los factores comunes a la variabilidad de <span class="math inline">\(Z_j\)</span>.</li>
<li>
<span class="math inline">\(d_j^2\)</span> es la <em>unicidad</em> (o varianza única) de <span class="math inline">\(Z_j,\hspace{0,1cm} j=1,2,\cdots,p\)</span>, o contribución de <span class="math inline">\(U_j\)</span> a la varianza de <span class="math inline">\(Z_j\)</span>. Es un indicador de la medida en la que los factores comunes fracasan a la hora de representar la varianza (unitaria) de <span class="math inline">\(Z_j\)</span>. </li>
<li>Cuando se descompone el factor único en sus dos componentes, modelo <a href="an%C3%A1lisis-factorial.html#eq:eqaf2">(33.2)</a>, <span class="math inline">\(b_j^2\)</span> se denomina <strong>especificidad</strong> (o varianza específica) de <span class="math inline">\(Z_j\)</span> y es la varianza de <span class="math inline">\(Z_j\)</span> debida a la particular selección de las variables en el estudio, mientras que <span class="math inline">\(c_j^2\)</span> es la que se debe al error (normalmente de medida), que mide la “falta de fiabilidad”.
</li>
</ul>
</div>
<div id="patrón-y-estructura-factorial" class="section level3" number="33.2.2">
<h3>
<span class="header-section-number">33.2.2</span> Patrón y estructura factorial<a class="anchor" aria-label="anchor" href="#patr%C3%B3n-y-estructura-factorial"><i class="fas fa-link"></i></a>
</h3>
<p>Se denomina <strong>patrón factorial</strong> a la siguiente expansión del modelo <a href="an%C3%A1lisis-factorial.html#eq:eqaf3">(33.3)</a>,
<span class="math display" id="eq:eqaf5">\[\begin{equation}
\begin{split}
Z_{1}= a_{11}F_{1}+ a_{12}F_{2}+ \dotsb + a_{1k}F_{k}+ d_1U_1\\
Z_{2}= a_{21}F_{1} + a_{22}F_{2}+ \dotsb + a_{2k}F_{k}+d_2U_2 \\
\ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \ddots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \vdots \ \ \ \\
Z_{p}= a_{p1}F_{1}+ a_{p2}F_{2}+ \dotsb + a_{pk}F_{k}+ d_pU_p\\
\end{split}
\tag{33.6}
\end{equation}\]</span>
o simplemente a la tabla, o matriz, con los coeficientes <span class="math inline">\(a_{jm}\)</span> y <span class="math inline">\(d_j\)</span> (o únicamente, caso habitual, a la <em>matriz de cargas</em> <span class="math inline">\(\bf A\)</span>). <span class="math inline">\(k\)</span> determina la “complejidad del modelo”.</p>
<p>Se denomina <strong>estructura factorial</strong> al siguiente conglomerado de <span class="math inline">\(k\)</span> + 1 conjuntos de <span class="math inline">\(p\)</span> ecuaciones lineales, en <span class="math inline">\(\{a_{jm}\}\)</span>, los <span class="math inline">\(k\)</span> primeros, y en <span class="math inline">\(\{d_{j}\}\)</span>, el último, <span class="math inline">\(\hspace{0,1cm} j=1,2,\cdots, p; \hspace{0,1cm} m=1,2,\cdots, k\)</span>:<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;En caso de variables dicotómicas se utiliza el coeficiente &lt;span class="math inline"&gt;\(\phi\)&lt;/span&gt; (véase &lt;a href="tablas-contingencia.html#medidas"&gt;23.4&lt;/a&gt;) como medida de correlación momento-producto.&lt;/p&gt;'><sup>232</sup></a></p>
<p><span class="math display" id="eq:eqaf6">\[\begin{equation}
\begin{split}
r_{Z_jF_1} &amp; = a_{j1}r_{F_1F_1}+ a_{j2}r_{F_1F_2}+ \dotsb +a_{jm}r_{F_1F_m}+\dotsb + a_{jk}r_{F_1F_k}\\
\ \ \ &amp; \vdots \ \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ddots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ddots\ \ \ \ \ \ \ \ \vdots \ \ \ \\
r_{Z_jF_m} &amp; = a_{j1}r_{F_mF_1}+ a_{j2}r_{F_mF_2}+ \dotsb +a_{jm}r_{F_mF_m}+\dotsb + a_{jk}r_{F_mF_k}\\
\ \ \ &amp;  \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ddots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ddots\ \ \ \ \ \ \ \vdots \ \ \ \\
r_{Z_jF_k} &amp; = a_{j1}r_{F_kF_1}+ a_{j2}r_{F_kF_2}+ \dotsb +a_{jm}r_{F_kF_m}+\dotsb + a_{jk}r_{F_kF_k}\\ \\
\ \ \ &amp;  \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ddots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ddots \ \ \ \ \ \ \ \vdots \ \ \ \\
r_{Z_jU_j} &amp; = d_j\\
\ \ \ \ \ \ \ \ \ \ \ \ &amp; \vdots
\end{split}
\tag{33.7}
\end{equation}\]</span>
En la práctica, viene dada por una tabla, o matriz, <span class="math inline">\(\bf \Gamma\)</span>, con los coeficientes <span class="math inline">\(\{r_{Z_jF_m}\}\)</span>. Cuando todos los factores están incorrelacionados el patrón y la estructura coinciden.</p>
<p>El conjunto patrón factorial más estructura factorial se denomina <strong>solución factorial completa</strong>. El patrón factorial muestra la relación lineal de las variables en términos de los factores, como si de una regresión lineal se tratase, y puede usarse para reproducir la correlación entre las variables (y, por tanto, para determinar la bondad de la solución). La estructura factorial es útil para la identificación de los factores y la posterior estimación de las <strong>puntuaciones factoriales</strong>.</p>
<p>En términos matriciales, denominando
<span class="math display">\[\bf F=\left(\begin{matrix} f_{11} &amp; f_{12} &amp; \cdots &amp;f_{1N}\\
f_{21}&amp;f_{22}&amp;\cdots&amp;f_{2N}\\
\vdots&amp;\vdots&amp;\ddots     &amp;\vdots\\
f_{k1}&amp;f_{k2}&amp;\cdots&amp;f_{kN}\\
\end {matrix}\right),\quad \bf \Gamma=\left(\begin{matrix} r_{Z_1F_1} &amp; r_{Z_1F_2} &amp; \cdots &amp;r_{Z_1F_k}\\
r_{Z_2F_1}&amp;r_{Z_2F_2}&amp;\cdots&amp;r_{Z_2F_k}\\
\vdots&amp;\vdots&amp;\ddots     &amp;\vdots\\
r_{Z_pF_1}&amp;r_{Z_pF_2}&amp;\cdots&amp;r_{Z_pF_k}\\
\end {matrix}\right),\]</span>
el patrón factorial viene dado por <span class="math inline">\(\bf Z=\bf A \bf F + \bf D \bf U\)</span>. Multiplicando por <span class="math inline">\(\bf F^{\prime}\)</span> y realizando simples operaciones se tiene que <span class="math inline">\(\bf \Gamma = \bf A\bf \Phi\)</span>, donde <span class="math inline">\(\bf \Phi\)</span> es la matriz de correlaciones entre los factores comunes. Si los factores comunes están incorrelacionados, <span class="math inline">\(\bf \Gamma=\bf A\)</span>. </p>
<p>Por último, conviene resaltar que el AF es indeterminado, es decir, dado un conjunto de correlaciones, los coeficientes del patrón factorial no son únicos (dado <span class="math inline">\(\bf R\)</span>, se pueden encontrar infinitos sistemas de factores incorrelacionados u ortogonales)<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;La historia es más larga: el AF es indeterminado porque dada una matriz &lt;span class="math inline"&gt;\({\bf {C}}_{kxk}\)&lt;/span&gt; no singular, si se define otro vector de factores comunes &lt;span class="math inline"&gt;\(\bf f^*=\bf C^{-1}\bf f\)&lt;/span&gt; y otra matriz &lt;span class="math inline"&gt;\(\bf A^*=\bf A\bf C\)&lt;/span&gt;, entonces &lt;span class="math inline"&gt;\(\bf Z=\bf A \bf f+\bf D \bf u=\bf A^*\bf C^{-1}\bf C\bf f^*+\bf D \bf u=\bf A^*\bf f\bf^*+ D \bf u\)&lt;/span&gt; y ambos son equivalentes. Una solución es exigir la incorrelación de los factores comunes (&lt;span class="math inline"&gt;\(\bf \Phi=\bf I\)&lt;/span&gt;), con lo que la indeterminación se reduciría solo a cuando &lt;span class="math inline"&gt;\(\bf C\)&lt;/span&gt; sea ortogonal. En este caso, la solución será única salvo rotaciones ortogonales. &lt;/p&gt;'><sup>233</sup></a> consistentes con ella. Por ello, normalmente, tras obtener una solución que ajuste bien los datos originales, se lleva a cabo una rotación de la misma (que ajusta igual de bien dichos datos) que facilite la <em>interpretación de los factores</em>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Una solución determina el espacio &lt;span class="math inline"&gt;\(k\)&lt;/span&gt;-dimensional que contiene los &lt;span class="math inline"&gt;\(k\)&lt;/span&gt; factores comunes, pero no su posición exacta.&lt;/p&gt;'><sup>234</sup></a> </p>
</div>
</div>
<div id="el-análisis-factorial-en-la-práctica" class="section level2" number="33.3">
<h2>
<span class="header-section-number">33.3</span> El análisis factorial en la práctica<a class="anchor" aria-label="anchor" href="#el-an%C3%A1lisis-factorial-en-la-pr%C3%A1ctica"><i class="fas fa-link"></i></a>
</h2>
<p></p>
<div id="preanálisis-factorial" class="section level3" number="33.3.1">
<h3>
<span class="header-section-number">33.3.1</span> Preanálisis factorial<a class="anchor" aria-label="anchor" href="#prean%C3%A1lisis-factorial"><i class="fas fa-link"></i></a>
</h3>
<div id="procede-la-realización-de-un-análisis-factorial" class="section level4" number="33.3.1.1">
<h4>
<span class="header-section-number">33.3.1.1</span> ¿Procede la realización de un análisis factorial?<a class="anchor" aria-label="anchor" href="#procede-la-realizaci%C3%B3n-de-un-an%C3%A1lisis-factorial"><i class="fas fa-link"></i></a>
</h4>
<p>Antes de comenzar con el AF conviene determinar si procede o no; es decir, determinar si las variables se encuentran fuertemente intercorrelacionadas o no. En caso negativo, el AF no tendría sentido. Para ello, se pueden utilizar procedimientos sencillos como observar si el determinante de <span class="math inline">\(\bf R\)</span> es bajo (correlaciones altas) o elevado (correlaciones bajas); o calcular la <strong>matriz de correlaciones anti-imagen</strong>, cuyos elementos son los coeficientes de correlación parcial cambiados de signo. En la diagonal muestra la <strong>medida de adecuación muestral</strong> para esa variable, <span class="math inline">\(MSA_j\)</span>. Para que se den las condiciones de realización del AF, la mayoría de los elementos no diagonales deben ser pequeños y los diagonales deben estar próximos a la unidad.</p>
<p>
</p>
<p>Otras alternativas más sofisticadas incluyen las dos siguientes:</p>
<p><strong>Contraste de esfericidad de Bartlett</strong></p>
<p>Exige normalidad multivariante. Contrasta la incorrelación de las variables, es decir,</p>
<ul>
<li><p><span class="math inline">\(H_0:\bf R=\bf I\)</span> frente a</p></li>
<li><p><span class="math inline">\(H_1:\bf R\neq \bf I,\)</span></p></li>
</ul>
<p>(o <span class="math inline">\(H_{0}:|\bf{\bf R}|=1\)</span> frente a <span class="math inline">\(H_{1}:|\bf{\bf R}|\neq1\)</span>). El estadístico de contraste es <span class="math inline">\(d_{\bf R}= - \left( N-1-\frac{1}{6} (2p+5)\right) ln|\mathbf{R}|\)</span> y, bajo <span class="math inline">\(H_0\)</span>, sigue una <span class="math inline">\(\chi^2_{\frac{p(p-1)}{2}}\)</span>, siendo nulo en caso de incorrelación.</p>
<div class="sourceCode" id="cb453"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">TIC2021</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/psych/man/cortest.bartlett.html">cortest.bartlett</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">TIC2021</span><span class="op">)</span>, <span class="va">n</span><span class="op">)</span><span class="op">$</span><span class="va">chisq</span></span>
<span><span class="co">#&gt; [1] 149.7113</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/psych/man/cortest.bartlett.html">cortest.bartlett</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">TIC2021</span><span class="op">)</span>, <span class="va">n</span><span class="op">)</span><span class="op">$</span><span class="va">p.value</span></span>
<span><span class="co">#&gt; [1] 1.992514e-21</span></span></code></pre></div>
<p><strong>Medida de adecuación muestral de Kaiser, Meyer y Olkin (KMO)</strong>
</p>
<p>Se basa en la idea de que, entre cada par de variables, el coeficiente de correlación parcial (que mide la correlación existente entre cada par de ellas eliminando el efecto que el resto de variables tiene sobre las dos consideradas), debe ser cercano a cero, puesto que es una estimación de la correlación entre sus factores específicos, que se suponen incorrelacionados. Por tanto, si el número de coeficientes de correlación parcial no nulos es elevado, la solución factorial no es compatible con los datos.</p>
<p></p>
<p>En otros términos, cuando las variables incluidas en el análisis comparten gran cantidad de información debido a la presencia de factores comunes, la correlación parcial entre cualquier par de variables debe ser reducida. Por el contrario, cuando dos variables comparten gran cantidad de información entre ellas, pero no la comparten con las restantes variables (ni, consecuentemente, con los factores comunes), la correlación parcial entre ellas será elevada, lo cual es un mal síntoma de cara a la idoneidad del AF. </p>
<p>El índice KMO se define como <span class="math inline">\(KMO=\frac{\displaystyle\sum_{j}\displaystyle\sum_{s \neq j} r_{js}^2}{\displaystyle\sum_{j}\displaystyle\sum_{s \neq j} r_{js}^2+\displaystyle\sum_{j}\displaystyle\sum_{s \neq j}r_{js}^{*2}}\)</span>, donde <span class="math inline">\(r_{js}^{*}\)</span> es el coeficiente de correlación parcial entre las variables <span class="math inline">\(Z_j, \hspace{0,1cm} j=1,...,p,\)</span> y <span class="math inline">\(Z_s, \hspace{0,1cm} s=1,...,p\)</span>. Se considera que valores por encima 0,9 implican elevadísismas correlaciones en la matriz <span class="math inline">\(\bf R\)</span>; entre 0,5 y 0,9 permiten el AF; y por debajo de 0,5 resultan inaceptables para el AF.</p>
<p>Las <span class="math inline">\(MSA_j\)</span> mencionadas anteriormente son la versión del índice KMO limitado a cada variable: <span class="math inline">\(MSA_{j}= \frac{\displaystyle\sum_{s \neq j} r_{js}^2}{\displaystyle\sum_{s \neq j} r_{js}^2+\displaystyle\sum_{s \neq j} r_{js}^{*2}}.\)</span></p>
<p>La interpretación es similar a la de KMO, pero mide la adecuación de cada variable para realizar un AF, lo que permite no considerar aquellas variables con menor MSA de cara a mejorar el índice KMO. No obstante, para eliminar una variable del estudio es aconsejable tener en cuenta también las comunalidades de cada variable, los residuos del modelo e interpretar los factores obtenidos.</p>
<!-- $r_{ij}$ es la correlación observada entre las variables originales $X_{i}$ y $X_{j}$ y $a_{ij}$ es la correlación parcial. Si la muestra se adecua a un modelo de AF, los coeficientes de correlación parcial $(a_{ij})$ serán muy pequeños o nulos según lo dicho anteriormente y el índice KMO tenderá a 1. @Kaiser_1974 y @KaiserRice_1974 consideran valores por encima 0,9 muy buenos, entre 0,5 y 0,9 mediocres y por debajo de 0,5, inaceptables. Por lo tanto, valores por encima de 0,5 se van a considerar validos [@Hair_et_al1999]. Esta misma medida para cada variable de forma individual se denomina medida de adecuación a la muestra correspondiente a la variable i-ésima (MSA)\index{Medida de adecuación a la muestra de la variable i-ésima (MSA)}: \begin{equation} -->
<div class="sourceCode" id="cb454"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/psych/man/KMO.html">KMO</a></span><span class="op">(</span><span class="va">TIC2021</span><span class="op">)</span><span class="op">$</span><span class="va">MSA</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.83</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/psych/man/KMO.html">KMO</a></span><span class="op">(</span><span class="va">TIC2021</span><span class="op">)</span><span class="op">$</span><span class="va">MSAi</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;    ebroad    esales esocmedia      eweb    hbroad     hiacc      iuse </span></span>
<span><span class="co">#&gt;     0.850     0.671     0.934     0.856     0.808     0.764     0.875</span></span></code></pre></div>
<p>Como puede apreciarse en las dos últimas salidas, en el ejemplo TIC tanto el test de Barlett como el índice KMO y las <span class="math inline">\(MSA_j\)</span> indican que el AF se puede llevar a cabo con garantías.</p>
<!-- El valor de 0,83 de la KMO indica la adeucación de desarrollar un AF en el ejemplo analizado siendo el valor de la MSA correcto para todas las variables alcanzando el menor valor para la variable *esales* y el mayor para la variable *esocmedia*. -->
</div>
<div id="el-problema-de-la-comunalidad-yo-del-número-de-factores-comunes" class="section level4" number="33.3.1.2">
<h4>
<span class="header-section-number">33.3.1.2</span> El problema de la comunalidad y/o del número de factores comunes<a class="anchor" aria-label="anchor" href="#el-problema-de-la-comunalidad-yo-del-n%C3%BAmero-de-factores-comunes"><i class="fas fa-link"></i></a>
</h4>
<p>
</p>
<p>El objetivo del AF es encontrar una <em>matriz de correlaciones reproducida</em> a partir de los resultados obtenidos, <span class="math inline">\({\bf R}^{rep}\)</span>, con menor rango que la original, <span class="math inline">\(\bf R\)</span>, tal que su diferencia, la <em>matriz de correlaciones de los residuos</em> , <span class="math inline">\({\bf R}^{res}\)</span>, se atribuya únicamente a errores muestrales. <span class="math inline">\(\bf R\)</span> es una matriz gramiana: simétrica de números reales y de diagonal principal dominante, con lo cual es semidefinida positiva y sus autovalores son nulos o positivos. Por tanto, el número de factores comunes será igual al de autovalores positivos (<span class="math inline">\(k\leq p\)</span>). Si el punto de partida en el análisis es <span class="math inline">\(\bf R\)</span>, rara vez se obtienen menos factores comunes que variables originales, con lo cual el AF realmente es un ACP. Ahora bien, como el número de factores comunes coincide con el rango de <span class="math inline">\({\bf R}^{rep}\)</span>, y este se ve afectado por los valores de la diagonal principal, al sustituir los unos por las estimaciones de las comunalidades (en este caso se está realizando un AF), <span class="math inline">\({\bf R}^{rep}\)</span> no será, en general, gramiana y <span class="math inline">\(k&lt; p\)</span>. En conclusión: como la solución factorial (<span class="math inline">\(k&lt; p\)</span>) pasa por el conocimiento del rango de <span class="math inline">\(\bf R\)</span> o de las comunalidades, o se hipotetiza sobre dicho rango o se hipotetizan o estiman las comunalidades. Normalmente se sigue uno de estos dos caminos:</p>
<ol style="list-style-type: decimal">
<li><p>Se parte de un <span class="math inline">\(k\)</span> prefijado, se lleva a cabo el AF y se contrasta la hipótesis <span class="math inline">\(H_0\)</span>: número de factores comunes = <span class="math inline">\(k\)</span>.</p></li>
<li><p>Se estiman las comunalidades y se obtienen los factores comunes. </p></li>
</ol>
<p>En cuanto a prefijar un número <span class="math inline">\(k\)</span> de factores, se pueden seguir los criterios expuestos en el Cap. <a href="acp.html#acp">32</a> para determinar el número de componentes principales a retener (criterio de Kaiser, gráfico de sedimentación, porcentaje mínimo de varianza explicada…). </p>
<p>En cuanto a la estimación de las comunalidades, de las múltiples posibilidades existentes, las siguientes son interesantes por su sencillez y buenos resultados: </p>
<ul>
<li>Una de las más sencillas, si el número de variables es grande, es aproximar la comunalidad de una variable por su correlación más alta con las demás variables: <span class="math inline">\(\hat {h}_j^2=max(r_{j1},r_{j2},\cdots,r_{j(j-1)},r_{j(j+1)},\cdots, r_{jp})\)</span>.</li>
<li>Otra posibilidad es <span class="math inline">\(\hat{h}_j^2=\frac{r_{js}r_{jt}}{r_{ts}}\)</span>, donde <span class="math inline">\(Z_{s}\)</span> y <span class="math inline">\(Z_{t}\)</span> son, por este orden, las dos variables más correlacionadas con <span class="math inline">\(Z_{j}\)</span>. Este procedimiento modera el efecto que tendría en el anterior una correlación excepcionalmente elevada.</li>
<li>En la misma línea, otra posibilidad es el promedio de los coeficientes de correlación entre la variable en cuestión y las restantes: <span class="math inline">\(\hat{h}_j^2=\frac{\sum_{j \neq s}r_{js}}{p-1}\)</span>.</li>
<li>Otra alternativa es realizar un ACP y tomar como comunalidad de cada variable la varianza explicada por los factores retenidos con el criterio de autovalor mayor que la unidad.</li>
<li>También se puede utilizar el coeficiente de determinación lineal múltiple de cada variable con las demás como estimación de la cota inferior de sus comunalidades: <span class="math inline">\(\hat {h}_j^2 \geq r^2_{{Z_j};(Z_{j1},\cdots,Z_{2},\cdots Z_{j-1},Z_{j+1},\cdots, Z_{p})}=1-\frac{1}{r^{jj}}\)</span>, donde <span class="math inline">\(r^{jj}\)</span> es el <span class="math inline">\(j\)</span>-ésimo elemento de la diagonal de <span class="math inline">\({\bf R}^{-1}\)</span>.</li>
</ul>
<p>Un valor alto de la comunalidad, próximo a <span class="math inline">\(V(X_j)\)</span>, significa que dicha variable está bien representada en el espacio de factores.</p>
<!-- ## Determinación del número de factores. -->
<!-- Una vez analizada la adecuación para realizar un AF, una decisión clave es determinar el número de factores comunes a retener. A menudo un conjunto pequeño de factores comunes contiene gran cantidad de la información de las variables originales por lo que no es necesario usar un número de factores superior. Existen diversos criterios para decidir el número de factores que se deben considerar: -->
<!-- - *Determinación “a priori”*. Es necesario un conocimiento previo de la situación por parte del investigador con una elección correcta de los datos y variables a usar lo que le permite establecer previamente el número de factores que hay y cuáles son. -->
<!-- - *Regla de Kaiser*\index{Regla de Kaiser}. Se calculan los valores propios de la matriz de correlaciones R y se toman como número de factores el número de valores propios superiores a la unidad. Este criterio es una alusión del Análisis de Componentes Principales y se ha verificado en simulaciones que, generalmente, tiende a infraestimar el número de factores. Para ello, en primer lugar se desarrolla un análisis de componentes principales con la matriz de correlaciones. -->
<!-- ```{r valorsuperiormedia.ch12,echo=TRUE,eval=TRUE} -->
<!-- acp <- princomp(TIC2021,cor=T) -->
<!-- acp$sd^2 -->
<!-- ``` -->
<!-- - *Criterio del porcentaje de la varianza*. Consiste en tomar como número de factores el número mínimo necesario para que el porcentaje acumulado de la varianza explicado alcance un nivel satisfactorio, normalmente se usan valores del 75% u 80%. Tiene la ventaja de que se puede aplicar también cuando la matriz analizada es la de varianzas y covarianzas, pero no tiene ninguna justificación teórica o práctica. -->
<!-- ```{r porcentajevarianza.ch12,echo=TRUE,eval=TRUE} -->
<!-- round((acp$sdev)^2/sum((acp$sdev)^2),digits=4)*100 -->
<!-- ``` -->
<!-- - *Gráfico de Sedimentación*\index{Gráfico de sedimentación}. Se trata de la representación gráfica donde los factores están en el eje de abscisas y los valores propios en el de ordenadas. Los factores con varianzas altas suelen diferenciarse de los factores con varianzas bajas. Se pueden conservar los factores situados antes de este punto de inflexión. En simulaciones el criterio ha funcionado bien, tiene el inconveniente de que es muy subjetivo. -->
<!-- ```{r grafsediment.ch12,echo=TRUE,eval=TRUE} -->
<!-- screeplot(acp,type="l") -->
<!-- ``` -->
<!-- Los criterios analizados llevan a seleccionar tres factores comunes, número de factores que va a ser considerados a la hora de mostrar algunas salidas de R y que constituye uno de los argumentos necesarios en las funciones usadas para desarrollar el AF. -->
</div>
</div>
<div id="análisis-factorial-1" class="section level3" number="33.3.2">
<h3>
<span class="header-section-number">33.3.2</span> Análisis factorial<a class="anchor" aria-label="anchor" href="#an%C3%A1lisis-factorial-1"><i class="fas fa-link"></i></a>
</h3>
<p></p>
<div id="metodosdeextraccion" class="section level4" number="33.3.2.1">
<h4>
<span class="header-section-number">33.3.2.1</span> Métodos de extracción de los factores<a class="anchor" aria-label="anchor" href="#metodosdeextraccion"><i class="fas fa-link"></i></a>
</h4>
<p><strong>Método de componentes principales</strong> </p>
<p>Su objetivo es el análisis de toda la varianza, común y no común, (modelo <a href="an%C3%A1lisis-factorial.html#eq:eqaf1">(33.1)</a>). Por consiguiente, las entradas de la diagonal de <span class="math inline">\(\bf R\)</span><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Cuando se utiliza este método, se debe utilizar &lt;span class="math inline"&gt;\(\bf R\)&lt;/span&gt;, porque, de otra forma, los factores comunes no tendrían media cero y varianza unitaria.&lt;/p&gt;'><sup>235</sup></a> son unitarias y no se requiere la estimación <em>a priori</em> de las comunalidades; tampoco se requiere la estimación <em>a priori</em> del numero de factores comunes, que se determinan <em>a posteriori</em>. Para la exposición del método, así como para su ejemplificación con la base de datos <code>TIC2021</code> del paquete <code>CDR</code>, se remite al lector al Cap. <a href="acp.html#acp">32</a>. Aunque en el Cap. <a href="acp.html#acp">32</a> se utilizó la función <code><a href="https://rdrr.io/pkg/FactoMineR/man/PCA.html">PCA()</a></code> de la librería <code>FactoMineR</code>, también se puede utilizar la función <code><a href="https://rdrr.io/pkg/psych/man/principal.html">principal()</a></code> de la librería <code>psych</code>. </p>
<p>Este método tiene la ventaja de que siempre proporciona una solución. Sin embargo, al no estar basado en el modelo <a href="an%C3%A1lisis-factorial.html#eq:eqaf3">(33.3)</a>, puede dar estimaciones de las cargas factoriales muy sesgadas, sobre todo cuando hay variables con comunalidades pequeñas.</p>
<!-- Una vez analizada la adecuación para realizar un AF, una decisión clave es determinar el número de factores comunes a retener que contengan la mayor cantidad de la información de las variables originales. Los métodos para determinar el número de factores comunes a retener son los mismos que los usados para fijar el número de componentes principales visto en la sección XXX del capítulo XXX (**PONER REFERENCIA CRUZADA**). En el ejemplo práctico analizado recordar que los resultados llevaban a seleccionar dos componentes principales y, por lo tanto, en AF dos factores comunes. -->
<!-- Algunos de los métodos más usuales para la estimación de los parámetros del modelo (pesos de los factores comunes ($l_{ij}$) y varianzas específicas ($\psi_{i}$) son: -->
<!-- 1.- **Método de Componentes Principales** \index{Método de Componentes Principales} -->
<!-- Se trata de un método de carácter exploratorio. El modelo de componentes principales se especificó de la siguiente forma: \begin{equation} -->
<!-- \begin{split} -->
<!-- Y_{1}= a_{11}X_{1}+ a_{21}X_{2}+ \dotsb+ a_{p1}X_{p}\\ -->
<!-- Y_{2}= a_{12}X_{1}+ a_{22}X_{2}+ \dotsb+ a_{p2}X_{p}\\ -->
<!-- \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \vdots \ \ \ \\ -->
<!-- Y_{p}= a_{1p}X_{1}+ a_{2p}X_{2}+ \dotsb+ a_{pp}X_{p} -->
<!-- \end{split} -->
<!-- (\#eq:ecuacion6) -->
<!-- \end{equation} -->
<!-- El modelo anterior se transforma para llegar a la especificación del modelo de AF: \begin{equation} -->
<!-- \begin{split} -->
<!-- X_{1}= l_{11}F_{1}+ l_{12}F_{2}+ \dotsb + l_{1k}F_{k}+ \varepsilon_{1}\\ -->
<!-- X_{2}= l_{21}F_{1} + l_{22}F_{2}+ \dotsb + l_{2k}F_{k}+ \varepsilon_{2}\\ -->
<!-- \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \vdots \ \ \ \\ -->
<!-- X_{p}= l_{p1}F_{1}+ l_{p2}F_{2}+ \dotsb + l_{pk}F_{k}+ \varepsilon_{p}\\ -->
<!-- \end{split} -->
<!-- (\#eq:ecuacion7) -->
<!-- \end{equation} -->
<!-- Mientras que en el primer sistema las variables no observadas se especifican en función de las observadas, en el segundo es al contrario. Teniendo en cuenta que el número de variables no observadas es p, mientras que en el sistema objetivo (AF) es k+1, la solución consiste en englobar los últimos p-k términos de cada ecuación en uno, que llamaremos $\varepsilon$. Además, en este último sistema los factores comunes no tienen varianza unitaria, para salvar este problema, se tipifican las componentes principales dividiendo por la raíz del correspondiente autovalor (multiplicando los coeficientes por la misma cantidad para que no cambie nada).  -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- X_{1}= a_{11}\sqrt{\lambda_{1}} \frac{Y_{1}}{\sqrt{\lambda_{1}}}+ a_{12}\sqrt{\lambda_{2}} \frac{Y_{2}}{\sqrt{\lambda_{2}}}+ \dotsb+ a_{1k}\sqrt{\lambda_{k}} \frac{Y_{k}}{\sqrt{\lambda_{k}}}+ \varepsilon_{1}\\ -->
<!-- X_{2}= a_{21}\sqrt{\lambda_{1}} \frac{Y_{1}}{\sqrt{\lambda_{1}}}+ a_{22}\sqrt{\lambda_{2}} \frac{Y_{2}}{\sqrt{\lambda_{2}}}+ \dotsb+ a_{2k}\sqrt{\lambda_{k}} \frac{Y_{k}}{\sqrt{\lambda_{k}}}+ \varepsilon_{2}\\ -->
<!-- \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots\ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \vdots \ \ \ \\ -->
<!-- X_{p}= a_{p1}\sqrt{\lambda_{1}} \frac{Y_{1}}{\sqrt{\lambda_{1}}}+ a_{p2}\sqrt{\lambda_{2}} \frac{Y_{2}}{\sqrt{\lambda_{2}}}+\dotsb+ a_{pk}\sqrt{\lambda_{k}} \frac{Y_{k}}{\sqrt{\lambda_{k}}}+ \varepsilon_{p}\\ -->
<!-- \end{split} -->
<!-- (\#eq:ecuacion8) -->
<!-- \end{equation} -->
<!-- Los factores comunes $F_{j}$ serían $\frac{Y_{j}}{\sqrt{\lambda_{j}}}$ y los pesos $l_{ij}= a_{ij} \sqrt{\lambda_{j}}$. Sin embargo, hay que señalar que todos los factores específicos son combinaciones lineales de las mismas variables no observadas, luego no serían realmente específicos de cada variable original, este es el inconveniente fundamental de este método. Además, este método lleva a veces a confundir el análisis de componentes principales y el AF. 
<!-- En R este método se aplica usando la función "principal" de la librería **psych** . -->
<!-- ```{r factorialCP-ch12,echo=TRUE,eval=TRUE, , fig.cap="Gráfica de variables sobre el primer plano de componentes"} -->
<!-- afcp <- principal(TIC2021, nfactors=2, rotate="none") -->
<!-- afcp$loadings -->
<!-- factor.plot(afcp$loadings[,1:2], pch=20, xlim=c(-0.5,1), ylim=c(-1,1), xlab="Factor 1", ylab="Factor 2") -->
<!-- abline(h=0,lty=3) -->
<!-- abline(v=0,lty=3) -->
<!-- text(afcp$loadings[,1:2],pos=1,labels=names(TIC2021),cex=0.8) -->
<!-- ``` -->
<p><strong>Método de los factores principales</strong></p>
<p>Es la aplicación del método de componentes principales a la <em>matriz de correlaciones reducida</em>, <span class="math inline">\({\bf R}^*\)</span>, es decir, con comunalidades en la diagonal en vez de unos. Exige, por tanto, la estimación previa de las comunalidades y su objetivo es el análisis de la varianza compartida por todas las variables, tal y como muestra el modelo <a href="an%C3%A1lisis-factorial.html#eq:eqaf3">(33.3)</a>. Se trata de un procedimiento iterativo que consta de las siguientes etapas:</p>
<p></p>
<!-- Es un método iterativo que parte de estimar la comunalidad a través de los coeficientes de determinación obtenidos en la regresión lineal de cada variable sobre el resto. Los pasos son: -->
<ol style="list-style-type: decimal">
<li><p>Cálculo de la matriz de correlaciones muestrales.</p></li>
<li><p>Estimación inicial de las comunalidades utilizando el coeficiente de determinación lineal múltiple de cada variable con las demás.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Una estimación inicial de las comunalidades equivale a una estimación inicial de las varianzas únicas: &lt;span class="math inline"&gt;\(\hat{d}_{j(0)}^2=\hat{\sigma}_{j}^2-\hat{h}_{j(0)}^2\)&lt;/span&gt;; y si las variables originales están tipificadas: &lt;span class="math inline"&gt;\(\hat{d}_{j(0)}^2=1-\hat{h}_{j(0)}^2\)&lt;/span&gt;.&lt;/p&gt;'><sup>236</sup></a></p></li>
<li><p>Cálculo de la matriz de correlaciones reducida:</p></li>
</ol>
<p><span class="math display">\[\mathbf{R}^*
=\begin{pmatrix}
\hat{h}_{1(0)}^2 &amp; r_{12} &amp; \dotsb &amp; r_{1p}\\
r_{21} &amp; \hat{h}_{2(0)}^2 &amp; \dotsb &amp; r_{2p}\\
\vdots &amp; \vdots &amp; \dotsb &amp; \vdots \\
r_{p1} &amp; r_{p2} &amp; \dotsb &amp; \hat{h}_{p(0)}^2\\
\end{pmatrix}\]</span>.</p>
<ol start="4" style="list-style-type: decimal">
<li><p>Cálculo de los autovalores y autovectores asociados a <span class="math inline">\(\mathbf{R}^*\)</span> (matriz no necesariamente semidefinida positiva) y, a partir de ellos, obtención de las estimaciones de la matriz de cargas factoriales <span class="math inline">\(\bf{A}_{(0)}\)</span>. En este paso hay que determinar el número de factores utilizando los criterios del ACP.</p></li>
<li><p>A partir de la estimación de <span class="math inline">\(\bf{A}_{(0)}\)</span>, obtención de una nueva estimación de las comunalidades: <span class="math inline">\(\hat{h}_{j(1)}^2= \hat{a}_{j1(1)}^2+\hat{a}_{j2(1)}^2+ \dotsb +\hat{a}_{jk(1)}^2\)</span> y, por tanto, de una nueva estimación de la varianza única (o unicidad) <span class="math inline">\(\hat{d}_{j(1)}^2 =1 - \hat{h}_{j(1)}^2\)</span>.
</p></li>
<li><p>Comparación de <span class="math inline">\(\hat{h}_{j(1)}^2\)</span> con <span class="math inline">\(\hat{h}_{j(0)}^2\)</span>, <span class="math inline">\(j=1,2,\cdots,p\)</span>. Si hay diferencia significativa se vuelve al paso 3, y si la discrepancia no supera una cantidad prefijada se aceptan como válidas las últimas estimaciones disponibles.</p></li>
</ol>
<p>En el software <strong>R</strong>, el método de los factores principales se implementa con la función <code><a href="https://rdrr.io/pkg/psych/man/fa.html">fa()</a></code> de la librería <code>psych</code>, que parte de <span class="math inline">\({\bf R}^*\)</span>. <span class="citation">Harman (<a href="referncias.html#ref-harman1976">1976</a>)</span> muestra el procedimiento iterativo y <span class="citation">Revelle (<a href="referncias.html#ref-Revelle2022">2022</a>)</span> los detalles sobre la manera como <code><a href="https://rdrr.io/pkg/psych/man/fa.html">fa()</a></code> parte de <span class="math inline">\({\bf R}^*\)</span> y lleva a cabo la extracción de los factores.</p>
<div class="sourceCode" id="cb455"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">af_facprin</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/psych/man/fa.html">fa</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">TIC2021</span><span class="op">)</span>, nfactors <span class="op">=</span> <span class="fl">2</span>, rotate <span class="op">=</span> <span class="st">"none"</span>, fm <span class="op">=</span> <span class="st">"pa"</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb456"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">af_facprin</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt; Factor Analysis using method = pa</span></span>
<span><span class="co">#&gt; Call: fa(r = cor(TIC2021), nfactors = 2, rotate = "none", fm = "pa")</span></span>
<span><span class="co">#&gt; Standardized loadings (pattern matrix) based upon correlation matrix</span></span>
<span><span class="co">#&gt;              PA1     PA2    h2    u2   com</span></span>
<span><span class="co">#&gt; ebroad     0.678   0.189 0.495 0.5050 1.16</span></span>
<span><span class="co">#&gt; esales     0.503   0.547 0.553 0.4474 1.99</span></span>
<span><span class="co">#&gt; esocmedia  0.796   0.212 0.678 0.3218 1.14</span></span>
<span><span class="co">#&gt; eweb       0.872   0.239 0.818 0.1822 1.15</span></span>
<span><span class="co">#&gt; hbroad     0.816  -0.452 0.869 0.1306 1.56</span></span>
<span><span class="co">#&gt; hiacc      0.888  -0.439 0.982 0.0181 1.46</span></span>
<span><span class="co">#&gt; iuse       0.935  -0.023 0.875 0.1248 1.00</span></span>
<span><span class="co">#&gt;</span></span>
<span><span class="co">#&gt;                  PA1   PA2</span></span>
<span><span class="co">#&gt; SS loadings    4.435 0.835</span></span>
<span><span class="co">#&gt; Proportion Var 0.634 0.119</span></span>
<span><span class="co">#&gt; Cumulative Var 0.634 0.753</span></span></code></pre></div>
<p>
En la salida anterior, <span class="math inline">\(SS\)</span> <span class="math inline">\(loadings\)</span> son los autovalores de <span class="math inline">\({\bf R}^*\)</span>, que coinciden con la suma de los cuadrados de las cargas de las variables en cada factor (suma de las cargas al cuadrado por columnas). <span class="math inline">\(h2\)</span> son las comunalidades (suma de las cargas al cuadrado por filas; solo se muestran las cargas para los dos primeros factores puesto que entre ambos ya acumulan una varianza explicada de más del 75%). <span class="math inline">\(u2\)</span> son las varianzas de los factores únicos; finalmente, <span class="math inline">\(com_j\)</span> (en la salida <span class="math inline">\(com\)</span>) indica la complejidad de las cargas factoriales a la hora de explica la variabilidad de la variable <span class="math inline">\(Z_j\)</span>: <span class="math inline">\(com_j=\frac{\left( \sum_{j=1}^{p} a_{jm}^{2} \right)^2}{\sum_{j=1}^{p} a_{jm}^4}\)</span>. Cuanto mayor es <span class="math inline">\(com_j\)</span>, mejor es la calidad de la variable para participar en la extracción factorial (también podría interpretarse como el número de factores comunes necesarios para explicar la variabilidad de <span class="math inline">\(Z_j\)</span>). El promedio de las <span class="math inline">\(com_j\)</span> se denomina <strong>índice de complejidad de Hoffman</strong>. Una solución de estructura simple perfecta tiene una complejidad de uno (cada variable carga solo en un factor); una solución con elementos distribuidos uniformemente tiene una complejidad mayor que 1. Interesa que la estructura no sea simple y perfecta porque entonces no tendría sentido la reducción dimensional. Por tanto, el índice de Hofman deberá ser superior a la unidad. </p>
<p>Las comunalidades y unicidades son:
</p>
<div class="sourceCode" id="cb457"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">af_facprin2</span><span class="op">$</span><span class="va">communality</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt; ebroad esales esocmedia    eweb hbroad hiacc iuse</span></span>
<span><span class="co">#&gt;  0.495 0.553      0.678  0.818   0.869 0.982 0.875</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">af_facprin2</span><span class="op">$</span><span class="va">uniquenesses</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt; ebroad esales esocmedia    eweb hbroad hiacc iuse</span></span>
<span><span class="co">#&gt;  0.505 0.447      0.322   0.182 0.131  0.018 0.125</span></span></code></pre></div>
<p></p>
<p>Nótese que con el método de los factores principales, al aplicar ACP sobre <span class="math inline">\({\bf R}^*\)</span>, los factores obtenidos están incorrelacionados y la estructura coincide con el patrón.
</p>
<p>Los resultados son, en signo, aunque no tanto en valor, similares a los obtenidos por el método de componentes principales. Además, como era de esperar, no permiten una interpretación clara de los factores comunes. Para facilitar dicha interpretación, esos factores deberán ser rotados (véase Sec. <a href="an%C3%A1lisis-factorial.html#rotaciones">33.3.2.2</a>). </p>
<p><strong>Método de máxima verosimilitud</strong> </p>
<!-- Si se supone que la población se distribuye según una ley normal, se pueden obtener las estimaciones máximo verosímiles de los pesos de los factores, así como de las varianzas específicas. -->
<p>Exige normalidad multivariante y la determinación <em>a priori</em> del número de factores comunes, pero no la estimación de las comunalidades. Obedece al modelo <a href="an%C3%A1lisis-factorial.html#eq:eqaf3">(33.3)</a> y consiste en obtener las estimaciones máximo verosímiles de <span class="math inline">\(\bf A\)</span> y <span class="math inline">\(\bf D\)</span>. Dado que cualquier transformación ortogonal de <span class="math inline">\(\bf{A}\)</span> puede ser una solución, se impone la condición de que <span class="math inline">\(\bf A^{\prime}(DD^{\prime})^{-1}\bf A\)</span> sea diagonal. La log-verosimilitud viene dada por <span class="math inline">\(l=-\frac{N}{2}\left( log|2\pi{\bf {\Sigma}}|+ tr{\bf {\Sigma}}^{-1}\bf S \right)\)</span>, donde <span class="math inline">\(\bf\Sigma=\bf A \bf A^{\prime}+\bf D \bf D^{\prime}\)</span> y <span class="math inline">\(\bf S\)</span> son las matrices de covarianzas poblacional y muestral, respectivamente, de las variables <span class="math inline">\(X_j\)</span>, <span class="math inline">\(j= 1,2,\cdots,p\)</span>. <span class="math inline">\(\hspace{0,1cm} \bf D \bf D^{\prime}\)</span> es la matriz de covarianzas (diagonal) de los factores únicos, donde los elementos de la diagonal representan la parte de la varianza única de cada variable, y en la literatura sobre AF es conocida como <span class="math inline">\(\bf \Psi\)</span> (por ello, <span class="math inline">\(d_j^2=\psi_{jj})\)</span>.</p>
<p>La decisión sobre el número de factores comunes , <span class="math inline">\(k\)</span>, que en este método debe hacerse al principio, es muy importante, pues dos soluciones, una con <span class="math inline">\(k\)</span> factores y otra con <span class="math inline">\(k+1\)</span>, pueden tener matrices de cargas factoriales muy diferentes, al contrario que en el método de componentes principales, donde los primeros <span class="math inline">\(k\)</span> componentes son siempre iguales. Pues bien, una ventaja del método de máxima verosimilitud es que lleva asociado un test estadístico secuencial para determinar el número de factores (véase Sec. <a href="an%C3%A1lisis-factorial.html#postanalisis">33.3.3</a>). Otra ventaja es que las estimaciones máximo verosímiles son invariantes ante cambios de escala; en consecuencia, las matrices de covarianzas teórica y muestral involucradas en la expresión de log-verosimilitud pueden ser sustituidas por sus homónimas de correlación sin variación alguna en los resultados. Una desventaja es que puede haber un problema de grados de libertad o, en otros términos, el número de factores <span class="math inline">\(k\)</span> debe ser compatible con un número de grados de libertad positivo.</p>
<p>El método máximo verosímil se puede implementar en <strong>R</strong> con la librería <code>pysch</code> y la función <code><a href="https://rdrr.io/pkg/psych/man/fa.html">fa()</a></code> (con <code>fm=ml</code>). Otra posibilidad es utilizar la función <code><a href="https://rdrr.io/r/stats/factanal.html">factanal()</a></code>. En ambos casos hay que comprobar el cumplimiento de la hipótesis de normalidad. En el ejemplo <code>TIC</code> no procede su implementación al no cumplirse tal hipótesis.</p>
<!-- ```{r factorialMV-ch12,echo=TRUE,eval=TRUE} -->
<!-- afmv <- factanal(TIC2021, factors=3, rotation="none") -->
<!-- afmv$loadings -->
<!-- ``` -->
<!-- \textcolor{red}{**LO EJECUTAMOS O NO (NO HAY NORMALIDAD), SOLAMENTE DECIMOS LA FORMA DE HACERLO TAL Y COMO VIENE**} -->
<p><strong>Otros métodos</strong></p>
<p>Razones de espacio impiden comentar otros procedimientos de extracción de los factores. No obstante, hay que señalar que la función <code><a href="https://rdrr.io/pkg/psych/man/fa.html">fa()</a></code> de la librería <code>psych</code> también permite implementar los métodos <span class="math inline">\((i)\)</span> <code>minres</code> (mínimo residuo), que estima las cargas factoriales minimizando (sin ponderaciones) los cuadrados de los residuos no diagonales de <span class="math inline">\({\bf R}^{res}\)</span>; parte de una estimación de <span class="math inline">\(k\)</span> y, como el método máximo verosímil, no precisa estimar las comunalidades, que se obtienen como subproducto tras la estimación de las cargas; y <span class="math inline">\((ii)\)</span> <code>alpha</code>, que maximiza el alfa de Cronbach para los factores. Aunque <code><a href="https://rdrr.io/pkg/psych/man/fa.html">fa()</a></code> también proporciona el método del centroide y la descomposición triangular (que exigen la estimación de las comunalidades), así como el análisis imagen (que requiere el número de factores), en la actualidad están en desuso. </p>
<p>Otros métodos de extracción de los factores son los métodos de mínimos cuadrados no ponderados y mínimos cuadrados generalizados, que minimizan la suma de las diferencias cuadráticas entre las matrices de correlación observada y reproducida, en el último caso ponderando los coeficientes de correlación inversamente a la unicidad de las variables (alta unicidad supone baja comunalidad). Ambos son proporcionados por <code><a href="https://rdrr.io/pkg/psych/man/fa.html">fa()</a></code> y <code>FAiR</code>, que también es una librería muy recomendable.</p>
</div>
<div id="rotaciones" class="section level4" number="33.3.2.2">
<h4>
<span class="header-section-number">33.3.2.2</span> Rotaciones en el modelo de análisis factorial<a class="anchor" aria-label="anchor" href="#rotaciones"><i class="fas fa-link"></i></a>
</h4>
<p></p>
<p>La interpretación de los factores se lleva a cabo a través de la estructura factorial, que, si los factores comunes están incorrelacionados, coincide con el patrón factorial. Sin embargo, aunque el modelo obtenido sea representativo de la realidad, en ocasiones la interpretación de los factores es harto dificultosa porque presentan correlaciones similares con un gran número de variables. Como la solución AF no es única (si <span class="math inline">\(\bf{A}\)</span> es una solución factorial, también lo es cualquier transformación ortogonal), con la rotación se trata de que cada variable tenga una correlación próxima a 1 con un factor y a 0 con el resto, facilitando la interpretación de los factores.</p>
<p>Geométricamente, la <span class="math inline">\(j\)</span>-ésima fila de la matriz de cargas contiene las coordenadas de un punto (elemento, observación) en el espacio de las cargas correspondientes a <span class="math inline">\(X_j\)</span>. Al realizar la rotación se obtienen las coordenadas respecto a unos nuevos ejes, siendo el objetivo situarlos lo más cerca posible del mayor número de puntos. Esto asociaría cada grupo de variables con un solo factor, haciendo la interpretación más objetiva y sencilla.</p>
<p>Sea <span class="math inline">\(\bf T\)</span> una matriz ortogonal (<span class="math inline">\(\bf T^{\prime} \bf T=\bf T\bf T^{\prime}=\bf I\)</span>), denominada <strong>matriz de transformación</strong>. Entonces, el modelo <a href="an%C3%A1lisis-factorial.html#eq:eqaf3">(33.3)</a> puede escribirse como <span class="math inline">\(\bf Z=\bf A\bf T\bf T^{\prime}\bf F+ \bf U=\bf B \bf T^{\prime}\bf F+ \bf U\)</span>. Se trata de llegar a una <strong>estructura simple</strong>, que se caracteriza porque en <span class="math inline">\(\bf B\)</span>:</p>
<ul>
<li>Cada fila tiene al menos un cero.</li>
<li>Cada columna tiene, al menos, tantos ceros como factores comunes (<span class="math inline">\(k\)</span>).</li>
<li>Cada par de columnas debe ser tal que, para varias variables, una tenga cargas despreciables y la otra no.</li>
<li>Si <span class="math inline">\(k\geq 4\)</span>, cada par de columnas debe tener un número elevado de variables cuyas cargas sean nulas en ambas variables.</li>
<li>Para cada par de columnas, el número de variables con cargas no nulas en ambas columnas debe ser muy pequeño.</li>
</ul>
<p>Como se avanzó, se trata de que las variables se aglomeren lo más posible en
torno a los factores comunes, y de la manera más discriminatoria posible. Así mejora la interpretación de estos y, por lo general, aumenta su significado teórico. </p>
<p>Las rotaciones pueden ser ortogonales u oblicuas, dependiendo de si los nuevos factores siguen estando incorrelacionados (ejes perpendiculares) o no (ejes oblicuos).</p>
<div id="rotaciones-ortogonales" class="section level5" number="33.3.2.2.1">
<h5>
<span class="header-section-number">33.3.2.2.1</span> Rotaciones ortogonales<a class="anchor" aria-label="anchor" href="#rotaciones-ortogonales"><i class="fas fa-link"></i></a>
</h5>
<p>Preservan la perpendicularidad de los ejes y no varían las comunalidades, pues <span class="math inline">\({\bf{B}}{\bf{B}}^{\prime}= {\bf{A}} \bf{T} \bf{T}^{\prime} {\bf{A}}^{\prime}= \bf{A} \bf{A}^{\prime}\)</span>. Tampoco modifican los cuadrados de las comunalidades ni, por tanto, la suma de sus cuadrados (para todas las variables): <span class="math inline">\(\sum_{j=1}^p\sum_{m=1}^k b_{jm}^4+2\sum_{m&lt;r=1}^k\sum_{m=1}^k b_{jm}^2b_{jr}^2\)</span>. Y como esta expresión se mantiene invariante, minimizar el segundo término implica maximizar el primero.
</p>
<p>Las rotaciones ortogonales más usadas son:</p>
<p><strong>Rotación VARIMAX</strong></p>
<p>Se define <strong>simplicidad</strong> del factor <span class="math inline">\(m\)</span>-ésimo como la varianza de los cuadrados de las cargas factoriales (rotadas) <span class="math inline">\(b_{ji}\)</span>, <span class="math inline">\(j=1,2,\cdots,p\)</span>:
<span class="math display">\[{SMPL}_{m} = \frac{\sum_{j=1}^{p} {b}_{jm}^4}{p}- \left(\frac {\sum_{j=1}^{p}b_{jm}^2}{p}\right)^2.\]</span>
Cuanto mayor es la simplicidad de los factores, más sencilla es su interpretación. Por ello, el objetivo es que <span class="math inline">\(\bf{T}\)</span> sea tal que se maximice la varianza del cuadrado de las cargas en cada columna del patrón factorial, es decir, en cada factor.</p>
<p>Dicho lo anterior, la rotación VARIMAX consiste en la obtención de una <span class="math inline">\(\bf{T}\)</span> que maximice la suma de las simplicidades de todos los factores, <span class="math inline">\(V=\sum_{m=1}^{k}{SMPL}_{m}\)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Este criterio es compatible con el hecho de que en cada fila uno de los elementos esté próximo a cero y los demás a uno, porque la suma de los cuadrados de los elementos de una fila es la comunalidad fija de la variable correspondiente.&lt;/p&gt;"><sup>237</sup></a></p>
<p>Sin embargo, las variables con mayor comunalidad, y por tanto con mayores cargas factoriales, tendrán mayor influencia en la solución final porque la comunalidad no se ve afectada por la rotación ortogonal. Para evitar esto, Kaiser propuso la rotación VARIMAX normalizada,<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;La normalización Kaiser se aplica también en los demás tipos de rotación.&lt;/p&gt;"><sup>238</sup></a> donde las cargas se dividen entre la raíz cuadrada de la comunalidad de la variable correspondiente. Los valores obtenidos son los elementos de <span class="math inline">\(\bf B\)</span>.</p>
<p>El procedimiento de cálculo de las cargas de los factores rotados es iterativo, rotándose los factores por parejas hasta que se consigue maximizar la suma de simplicidades normalizadas.</p>
<p></p>
<p>La rotación VARIMAX es muy popular por la robustez de sus resultados, si bien se recomienda para un número no muy elevado de factores comunes. </p>
<p><strong>Rotación QUARTIMAX</strong></p>
<p>Su objetivo es maximizar la varianza de los cuadrados de todas las cargas factoriales, es decir, maximizar: <span class="math display">\[Q=\frac{\sum_{j=1}^{p}\sum_{m=1}^{k}{b}_{jm}^4}{pk}-\left( \frac {\sum_{j=1}^{p}\sum_{m=1}^{k}{b}_{jm}^2}{pk} \right)^2.\]</span></p>
<p></p>
<p>Nótese que, como la rotación ortogonal no modifica las comunalidades , <span class="math inline">\({h}_{j}^2=\sum_{m=1}^{k} b_{jm}^2\)</span>, el segundo término de la expresión anterior no se verá modificado, por lo que el criterio anterior equivale a maximizar <span class="math inline">\(\frac{\sum_{j=1}^{p}\sum_{m=1}^{k}{b}_{jm}^4}{pk}\)</span>.</p>
<p>QUARTIMAX es recomendable cuando el número de factores es elevado. Tiende a generar un factor general, el primero, sobre el que la mayor parte de las variables tienen cargas elevadas, lo cual contradice los objetivos que persigue la rotación.</p>
<p><strong>Rotación ORTOMAX</strong></p>
<p>Es una clase general de los métodos de rotación ortogonal que se construye como una composición ponderada de las dos rotaciones anteriores: <span class="math inline">\(\alpha Q+ \beta V\)</span>, donde <span class="math inline">\(V\)</span> se multiplica por <span class="math inline">\(p\)</span> por conveniencia, ya que una constante multiplicativa no afecta a la solución. Por tanto, su objetivo es maximizar la expresión: <span class="math display">\[ORT=\sum_{m=1}^{k} \left({\sum_{j=1}^{p} {b}_{jm}^4 - \left( \frac{\theta}{p}\sum_{j=1}^{p} b_{jm}^2 \right)^2}\right),\hspace{0,1cm} 0&lt; \theta=\frac{\alpha}{\alpha+\beta}&lt; 1.\]</span></p>
<p>Si <span class="math inline">\(\theta=1\)</span>, se tiene el criterio VARIMAX; si <span class="math inline">\(\theta=0\)</span>, se tiene el criterio QUARTIMAX; si <span class="math inline">\(\theta=0,5\)</span>, se tiene un criterio igualmente ponderado denominado BIQUARTIMAX; y si <span class="math inline">\(\theta=\frac{k}{2}\)</span>, se tiene el criterio EQUAMAX, recomendado por parte de la literatura. </p>
<p>Nótese que QUARTIMAX pone el énfasis en la simplificación de la descripción por filas (variables) de la matriz factorial, mientras que VARIMAX lo pone en la simplificación por columnas (factores) para satisfacer los requisitos de <strong>estructura simple</strong>; así, aunque se pueda conseguir la simplicidad de cada variable y que, a la vez, las cargas respecto del mismo factor sean grandes, tal factor queda excluido por la restricción impuesta por la simplificación sobre cada factor <span class="citation">(<a href="referncias.html#ref-harman1976">Harman 1976</a>)</span>.</p>
</div>
<div id="rotaciones-oblicuas" class="section level5" number="33.3.2.2.2">
<h5>
<span class="header-section-number">33.3.2.2.2</span> Rotaciones oblicuas <a class="anchor" aria-label="anchor" href="#rotaciones-oblicuas"><i class="fas fa-link"></i></a>
</h5>
<p>Superan la incorrelación u ortogonalidad de los factores y se suelen aplicar cuando: <span class="math inline">\((i)\)</span> se sospecha que, en la población, los factores tienen una fuerte correlación y/o <span class="math inline">\((ii)\)</span> cierta correlación entre los factores permite una gran ganancia en la interpretación de los mismos. Podrían aplicarse siempre, como norma general, puesto que, en realidad, la ortogonalidad es un caso particular de la oblicuidad.</p>
<p>Los procedimientos que proporcionan soluciones con estructura simple oblicua emanan de los mismos criterios objetivos que los que proporcionan soluciones con estructura simple ortogonal. De hecho, si se relajan las condiciones de ortogonalidad, algunos procedimientos de rotación ortogonal pueden adaptarse al caso oblicuo (tal es el caso, por ejemplo, del método OBLIMAX, a partir del criterio QUARTIMAX). Por otra parte, los métodos de rotación oblicua no solo son directos, sino que también pueden introducir los principios de estructura simple que se requieren para la solución factorial primaria de forma indirecta (métodos indirectos). Las rotaciones oblicuas exigen nuevos conceptos y nueva nomenclatura:</p>
<p> </p>
<ul>
<li><p><strong>Factores de referencia</strong>, <span class="math inline">\({G}_m\)</span>, <span class="math inline">\(m=1, 2,\cdots, k\)</span>: para cada factor rotado se puede encontrar un nuevo factor incorrelacionado con los rotados. A esos nuevos factores se les llama factores de referencia. En caso de rotación ortogonal, los factores de referencia coinciden con los primeros. </p></li>
<li><p><strong>Estructura factorial de referencia</strong>: hasta ahora, se denominaba estructura factorial a la matriz de correlaciones entre las variables <span class="math inline">\(Z_j\)</span>, <span class="math inline">\({j=1,2, \cdots, p}\)</span> y los factores rotados, que en el caso ortogonal coincide con la matriz de cargas factoriales rotadas. Pues bien, se denomina estructura factorial de referencia a la matriz de correlaciones entre las variables <span class="math inline">\(Z_j\)</span> y los factores de referencia. Si la rotación es ortogonal, coincide con la estructura factorial. </p></li>
<li><p><strong>Matriz de transformación</strong>: en el caso oblicuo se representa por <span class="math inline">\(\bf\Lambda\)</span>.</p></li>
<li><p><strong>Estructura factorial oblicua</strong>: <span class="math inline">\(\bf V\)</span>, tal que <span class="math inline">\(\bf V= \bf A \bf \Lambda\)</span>; sus elementos son <span class="math inline">\(v_{jm}\)</span>. </p></li>
<li><p><strong>Cargas</strong>: en el caso oblicuo el término “carga” se utiliza para denotar la correlación de la variable con el eje de referencia: <span class="math inline">\(v_{jm}=r_{Z_j;\Lambda_m}\)</span>.</p></li>
</ul>
<p>Mientras las rotaciones ortogonales intentan encontrar la estructura factorial más simple, las oblicuas hacen lo mismo pero con la estructura de referencia.</p>
<p>El método (directo) OBLIMAX maximiza la expresión: <span class="math display">\[K=\frac{\sum_{j=1}^{p}\sum_{m=1}^{k}v_{jm}^4}{\left(\sum_{j=1}^{p}\sum_{m=1}^{k}v_{jm}^{2}\right)^2}.\]</span></p>
<p>Nótese que se trata del criterio QUARTIMAX ortogonal, pero incorporando el denominador, puesto que en la rotación oblicua ya no es constante. </p>
<p>El QUARTIMIN directo, también derivado del QUARTIMAX ortogonal, minimiza el criterio: <span class="math display">\[H=\sum_{j=1}^{p}\sum_{m\leq q=1}^{k}v_{jm}^2v_{jq}^2,\]</span>
y recibe este nombre por minimizar términos de cuarto grado. </p>
<p>La generalización del criterio “minimizar <span class="math inline">\(H=\sum_{j=1}^{p}\sum _{m&lt;q=1}^k b_{jm}^2b_{jq}^2\)</span>” para factores oblicuos se denomina OBLIMIN, y da lugar a métodos indirectos. Entre ellos, destaca el COVARIMIN, que se obtiene relajando la condición de ortogonalidad en el VARIMAX, minimizando las covarianzas de los cuadrados de los elementos de:
<span class="math inline">\(\bf V:\)</span>
<span class="math display">\[C^*=\sum_{m\leq q=1}^{k}\left(p\sum_{j=1}^{p} v_{jm}^2v_{jq}^2-\sum_{j=1}^ {p}v_{jm}^2\sum_{j=1}^ {p}v_{jq}^2\right).\]</span>
La versión COVARIMIN normalizada minimiza:
<span class="math display">\[C=\sum_{m\leq q=1}^{k}\left(p\sum_{j=1}^{p} \frac{v_{jm}^2}{h_j^2}\frac{v_{jq}^2}{h_j^2}-\sum_{j=1}^ {p}\frac {v_{jm}^2}{h_j^2}\sum_{j=1}^ {p}\frac{v_{jq}^2}{h_j^2}\right).\]</span></p>
<p>Se ha comprobado empíricamente que QUARTIMIN tiende a ser demasiado oblicuo y COVARIMIN demasiado ortogonal. Una solución intermedia es la rotación BIQUARTIMIN, que consiste en minimizar <span class="math inline">\(B^*=H+\frac{C^*}{p}\)</span>, donde <span class="math inline">\(\frac{C^*}{p}\)</span> es la expresión completa del COVARIMIN. Una generalización de la rotación BIQUARTIMIN es <span class="math inline">\(B^*=\alpha H+\beta \frac{C^*}{p}\)</span>. Sencillas operaciones aritméticas llevan a:
<span class="math display">\[B^*=\sum_{m&lt; q=1}^{k}\left(p \sum_{j=1}^{p} v_{jm}^2 v_{jq}^2-\gamma \sum _{j=1}^{p}v_{jm}^2 \sum_{j=1}^{p}v_{jq}^2\right),\]</span>
con <span class="math inline">\(\gamma= \frac {\beta}{\alpha + \beta}\)</span>. La rotación QUARTIMIN se obtiene con <span class="math inline">\(\gamma=0\)</span>, la BIQUARTIMIN con <span class="math inline">\(\gamma=0,5\)</span> y la COVARIMIN con <span class="math inline">\(\gamma=1\)</span>. También se pueden obtener versiones normalizadas sin más que normalizar las cargas (dividirlas por <span class="math inline">\(h_{jm}^2\)</span>).</p>
<p>
</p>
<p>El criterio BINORMALMIN (normalizado) es una alternativa al BIQUARTIMIN para corregir el sesgo de oblicuidad del criterio COVARIMIN. Minimiza:
<span class="math display">\[D=\sum_{m&lt; q=1}^{k}\left( \frac{\sum_{j=1}^{p} \frac {v_{jm}^2}{h_j^2} \frac {v_{jq}^2}{h_j^2}} {\sum _{j=1}^{p} \frac{v_{jm}^2}{h_j^2}\sum _{j=1}^{p} \frac{v_{jq}^2}{h_j^2}}\right).\]</span></p>
<p>BINORMALMIN suele ser mejor con datos muy simples o muy complejos; BIQUARTIMIN es más recomendable con datos moderadamente complejos.</p>
<p>El método de rotación OBLIMIN directo, en vez de proceder como <span class="math inline">\(B^*\)</span>, que depende de los valores de la estructura, minimiza directamente una función de la matriz del patrón factorial primario:
<span class="math display">\[F{(\bf{A})}= \sum_{m&lt; q=1}^{k}\left(\sum_{j=1}^p a_{jm}^2 a_{jq}^2-\frac{\delta}{p}\sum_{j=1}^pa_{jm}^2\sum_{j=1}^pa_{jq}^2\right).\]</span> Cuando <span class="math inline">\(\delta=0\)</span>, se tiene el QUARTIMIN directo.</p>
<p>Hay otros tipos de transformaciones oblicuas, pero únicamente se mencionarán <span class="math inline">\((i)\)</span> la ORTOBLICUA, que llega a la solución oblicua mediante una serie de transformaciones ortogonales intermedias; y <span class="math inline">\((ii)\)</span> la PROMAX, muy popular, que actúa alterando los resultados de una rotación ortogonal (concretamente elevando las cargas de la rotación ortogonal a una potencia entre 2 y 4) hasta crear una solución con cargas factoriales lo más próximas a la estructura ideal. Cuanto mayor es esta potencia más oblicua es la solución obtenida.</p>
</div>
<div id="rotaciones-ortogonales-u-oblicuas" class="section level5" number="33.3.2.2.3">
<h5>
<span class="header-section-number">33.3.2.2.3</span> ¿Rotaciones ortogonales u oblicuas?<a class="anchor" aria-label="anchor" href="#rotaciones-ortogonales-u-oblicuas"><i class="fas fa-link"></i></a>
</h5>
<p>
</p>
<p>La selección del método de rotación, ortogonal u oblicua, depende del objetivo perseguido. Si se pretende reducir el número de variables originales a un conjunto mucho menor de variables incorrelacionadas para su uso posterior en otra técnica, por ejemplo, regresión, la rotación debe ser ortogonal. Si el objetivo es obtener unos factores teóricos significativos, puede resultar apropiada la aplicación de una rotación oblicua.</p>
<p>En <strong>R</strong> es muy sencillo implementar una rotación ortogonal u oblicua. Basta, por ejemplo, con utilizar la librería <code>GPArotation</code> <span class="citation">(<a href="referncias.html#ref-Coen2005">Bernaards and Jennrich 2005</a>)</span> e indicarlo en el argumento <code>rotate</code> de la función <code>fa</code>. A modo de ejemplo, extrayendo los factores por el método de los factores principales y utilizando una rotación VARIMAX normalizada, sería:</p>
<div class="sourceCode" id="cb458"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://optimizer.r-forge.r-project.org/GPArotation_www/">"GPArotation"</a></span><span class="op">)</span></span>
<span><span class="va">af_facprin2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/psych/man/fa.html">fa</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">TIC2021</span><span class="op">)</span>, nfactors <span class="op">=</span> <span class="fl">2</span>, rotate <span class="op">=</span> <span class="st">"varimax"</span>, fm <span class="op">=</span> <span class="st">"pa"</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb459"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">af_facprin2</span> <span class="co"># el objeto contiene información adicional no relevante en estos momentos</span></span>
<span><span class="co">#&gt; Factor Analysis using method = pa</span></span>
<span><span class="co">#&gt; Standardized loadings (pattern matrix) based upon correlation matrix</span></span>
<span><span class="co">#&gt;              PA1 PA2   h2    u2 com</span></span>
<span><span class="co">#&gt; ebroad     0.38 0.59 0.50 0.505 1.7</span></span>
<span><span class="co">#&gt; esales     0.02 0.74 0.55 0.447 1.0</span></span>
<span><span class="co">#&gt; esocmedia  0.46 0.68 0.68 0.322 1.7</span></span>
<span><span class="co">#&gt; eweb       0.50 0.75 0.82 0.182 1.7</span></span>
<span><span class="co">#&gt; hbroad     0.91 0.20 0.87 0.131 1.1</span></span>
<span><span class="co">#&gt; hiacc      0.96 0.26 0.98 0.018 1.1</span></span>
<span><span class="co">#&gt; iuse       0.72 0.60 0.88 0.125 1.9</span></span>
<span><span class="co">#&gt;</span></span>
<span><span class="co">#&gt;                 PA1  PA2</span></span>
<span><span class="co">#&gt; SS loadings    2.87 2.40</span></span>
<span><span class="co">#&gt; Proportion Var 0.41 0.34</span></span>
<span><span class="co">#&gt; Cumulative Var 0.41 0.75</span></span></code></pre></div>
<p>Nótese que la salida por defecto es la normalizada. También se puede utilizar la libería <code>stats</code> indicando <code>T</code> o <code>F</code> en el argumento <code>normalize</code>, dependiendo de que se quiera o no, respectivamente, una rotación VARIMAX (u otra) normalizada .</p>
<div class="sourceCode" id="cb460"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"stats"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/varimax.html">varimax</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/loadings.html">loadings</a></span><span class="op">(</span><span class="va">af_facprin</span><span class="op">)</span>, normalize <span class="op">=</span> <span class="cn">T</span><span class="op">)</span></span></code></pre></div>
<p>En el ejemplo del uso las TIC en los países de la UE-27, la rotación VARIMAX ha conseguido facilitar la interpretación de los factores comunes, ya que, tras la rotación, las variables relacionadas con el uso de las TIC a escala individual y de hogar cargan en el primer factor, mientras que las relacionadas con el uso de las TIC a nivel empresarial cargan en el segundo. Por tanto, ambos factores pueden considerarse indicadores de la dotación y uso de las TIC en los ámbitos familiar y empresarial, respectivamente. El lector puede probar (y comparar) con otras rotaciones sin más que incluirlas en el argumento <code>rotate</code>.</p>
</div>
</div>
</div>
<div id="postanalisis" class="section level3" number="33.3.3">
<h3>
<span class="header-section-number">33.3.3</span> Postanálisis factorial<a class="anchor" aria-label="anchor" href="#postanalisis"><i class="fas fa-link"></i></a>
</h3>
<p>Realizado el AF, los siguientes procedimientos permiten comprobar la bondad del modelo obtenido:
<strong>Análisis de las correlaciones residuales</strong></p>
<p>Se entiende por bondad de la solución factorial la medida del grado en que los factores del modelo explican las correlaciones entre las variables. Por ello, parece natural que tal medida se base en la comparación entre las correlaciones observadas y las que se derivan del modelo factorial (reproducidas) o, en términos matriciales, en la magnitud de las entradas de la matriz de correlaciones residuales <span class="math inline">\({\bf R}^{res}={\bf R} - {\bf R}^{rep}\)</span>, donde <span class="math inline">\({\bf R}=\frac{1}{N} \bf Z \bf Z^{\prime}\)</span> y <span class="math inline">\({\bf R}^{rep}=\bf A\bf \Phi\bf A^{\prime}=\bf \Gamma \bf A^{\prime}\)</span> (relación fundamental entre el patrón y la estructura factorial; en caso de incorrelación entre los factores, <span class="math inline">\(\bf \Phi=\bf I\)</span> y <span class="math inline">\({\bf R}^{rep}=\bf A {\bf A}^{\prime}\)</span>. La matriz <span class="math inline">\({\bf R}^{rep}\)</span> se obtiene sin más que sustituir <span class="math inline">\(\bf Z\)</span> por <span class="math inline">\(\bf A \bf F\)</span> en la expresión de <span class="math inline">\(\bf R\)</span>. </p>
<p>Ahora bien, ¿cuál es el criterio apropiado para concluir si una solución factorial es aceptable o no? Para que sea aceptable, los elementos (los residuos) de <span class="math inline">\({\bf R}^{res}\)</span> deben ser cercanos a cero, y como todos los factores comunes han sido considerados, se supone que no existen más vínculos entre las variables y que la distribución de dichos residuos debe ser como la de correlación cero en una muestra del mismo tamaño. Por tanto, como <span class="math inline">\(\sigma_{r=0}=\frac{1}{\sqrt{N-1}}\)</span>, entonces <span class="math inline">\(S_{r_{res}}\leq\frac{1}{\sqrt{N-1}}:\)</span><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Este criterio tiene como ventaja la simplicidad. Sin embargo, sería conveniente que tuviese en cuenta, al menos, el número de variables.&lt;/p&gt;"><sup>239</sup></a></p>
<ul>
<li>Si <span class="math inline">\(S_{r_{res}}\gg\frac{1}{\sqrt{N-1}}\)</span>, es razonable pensar que existen relaciones adicionales significativas entre las variables y hay que modificar la solución factorial. </li>
<li>Si <span class="math inline">\(S_{r_{res}}\ll\frac{1}{\sqrt{N-1}}\)</span>, es razonable pensar que la solución factorial incluye relaciones que no están justificadas.</li>
<li>Si <span class="math inline">\(S_{r_{res}}\leq pero \hspace{0,07cm} no\ll \frac{1}{\sqrt{N-1}}\)</span>, la solución es aceptable.</li>
</ul>
<p>Otra posibilidad, también muy sencilla, propuesta por <span class="citation">Revelle (<a href="referncias.html#ref-Revelle2022">2022</a>)</span>, es utilizar <span class="math inline">\(fit= 1-\frac{\sum \left (\bf R-{\bf FF}^{\prime}\right)^2}{\sum (\bf R)^2}\)</span>, que indica la reducción proporcional en la matriz de correlación debida al modelo factorial. Nótese que esta medida es sensible al tamaño de las correlaciones originales. Es decir, si los residuos son pequeños, pero las correlaciones son pequeñas, el ajuste es malo. Las medidas clásicas como el RMSE (raíz cuadrada del error cuadrático medio), o similares, también son susceptibles de uso.</p>
<p>En el ejemplo TIC seguido en este capítulo el ajuste realizado es muy bueno:</p>
<div class="sourceCode" id="cb461"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">af_facprin2</span><span class="op">$</span><span class="va">residual</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;           ebroad esales esocmedia   eweb hbroad  hiacc   iuse</span></span>
<span><span class="co">#&gt; ebroad     0.505 -0.068     0.008  0.068 -0.045  0.002  0.003</span></span>
<span><span class="co">#&gt; esales    -0.068  0.447     0.026  0.015  0.004 -0.012  0.021</span></span>
<span><span class="co">#&gt; esocmedia  0.008  0.026     0.322 -0.047  0.014 -0.005  0.017</span></span>
<span><span class="co">#&gt; eweb       0.068  0.015    -0.047  0.182  0.012  0.015 -0.042</span></span>
<span><span class="co">#&gt; hbroad    -0.045  0.004     0.014  0.012  0.131 -0.005  0.010</span></span>
<span><span class="co">#&gt; hiacc      0.002 -0.012    -0.005  0.015 -0.005  0.018  0.002</span></span>
<span><span class="co">#&gt; iuse       0.003  0.021     0.017 -0.042  0.010  0.002  0.125</span></span>
<span><span class="va">af_facprin2</span><span class="op">$</span><span class="va">rms</span></span>
<span><span class="co">#&gt; [1] 0.02907475</span></span>
<span><span class="va">af_facprin2</span><span class="op">$</span><span class="va">fit</span></span>
<span><span class="co">#&gt; [1] 0.9715865</span></span></code></pre></div>
<div class="infobox">
<p><strong>Nota</strong></p>
<p>Como se avanzó en la introducción, el AF está enfocado al ajuste de las correlaciones entre las variables observadas mediante el patrón factorial correspondiente al modelo <a href="an%C3%A1lisis-factorial.html#eq:eqaf2">(33.2)</a> (con los factores comunes y el factor único). Pues bien, si en el proceso reproductivo se utiliza el modelo solo con los factores comunes, la matriz de correlaciones que se reproduce es <span class="math inline">\(\bf R\)</span>, lo que implica el modelo ACP (modelo <a href="an%C3%A1lisis-factorial.html#eq:eqaf1">(33.1)</a>). Si se incluye también el factor específico, la matriz de correlaciones que se reproduce es <span class="math inline">\({\bf R}^*\)</span> (modelo AF). Si en dicha reproducción se utilizasen los factores comunes y el término de error, se reproduciría <span class="math inline">\(\bf R\)</span> con una diagonal principal cuyas entradas serían la unidad menos las estimaciones de las comunalidades. </p>
</div>
<p><strong>Test de bondad de ajuste</strong></p>
<p>Se trata de un contraste de razón de verosimilitudes que se puede llevar a cabo cuando se extraigan los factores por el método de máxima verosimilitud. La hipótesis nula es la suficiencia de <span class="math inline">\(k\)</span> factores comunes para explicación de las correlaciones entre las variables originales y de la varianza que comparten. </p>
<p>El estadístico del contraste es <span class="math inline">\(-2ln\lambda=np(\hat{a} - ln \hat{g} -1]\)</span>, donde <span class="math inline">\(\hat{a}\)</span> y <span class="math inline">\(\hat{g}\)</span> son las medias aritmética y geométrica, respectivamente, de los autovalores de la matriz <span class="math inline">\(\hat{\boldsymbol{\Sigma}}_{H_{0}}^{-1} \mathbf{S}\)</span>. Bajo <span class="math inline">\(H_0\)</span>, se distribuye asintóticamente como una <span class="math inline">\(\chi_{df}^2\)</span>, con <span class="math inline">\(df= \left( p+\frac{p(p+1)}{2}\right) - \left( p+pk+p-\frac{k(k-1)}{2}\right)= \frac{1}{2} (p-k)^2- \frac{1}{2}(p+k)\)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;&lt;span class="math inline"&gt;\(df\)&lt;/span&gt; indica la medida en que el modelo factorial ofrece una interpretación más simple que &lt;span class="math inline"&gt;\(\bf \Sigma\)&lt;/span&gt;.&lt;/p&gt;'><sup>240</sup></a></p>
<p>Este test se aplica de manera secuencial: se formula como hipótesis nula <span class="math inline">\(k=0\)</span>. Si no se rechaza, no hay factores comunes subyacentes. Si se rechaza, se sigue con <span class="math inline">\(k=1\)</span>. Si no se rechaza <span class="math inline">\(k=1\)</span>, se concluye que el modelo con un factor es una adecuada representación de la realidad; si se rechaza, se formula la hipótesis nula de que <span class="math inline">\(k=2\)</span>, y el proceso continúa hasta que no se rechace la hipótesis nula, siempre que el valor de <span class="math inline">\(k\)</span> sea compatible con un número de grados de libertad positivo.</p>
</div>
<div id="puntuaciones-factoriales" class="section level3" number="33.3.4">
<h3>
<span class="header-section-number">33.3.4</span> Puntuaciones factoriales <a class="anchor" aria-label="anchor" href="#puntuaciones-factoriales"><i class="fas fa-link"></i></a>
</h3>
<p>Las puntuaciones factoriales son las estimaciones de los valores de los factores aleatorios no observados, es decir, de los elementos de <span class="math inline">\({\bf F}_{mxm}\)</span>. Así, <span class="math inline">\(\hat{f}_{im}\)</span> será la estimación del valor del <span class="math inline">\(m\)</span>-ésimo factor para la <span class="math inline">\(i\)</span>-ésima observación (elemento, individuo, objeto…). Cuando se extraen los factores por componentes principales las puntuaciones son exactas.</p>
<p>Estas estimaciones pueden ser usadas como <em>inputs</em> para posteriores análisis (regresión, clúster, etc.) en los que se trabaje con los mismos elementos o individuos, sustituyendo las variables originales por los nuevos factores obtenidos. La cuestión es: ¿cómo calcular estas puntuaciones?, porque tanto los factores como los errores no son observables sino aleatorios.</p>
<p>Los métodos más populares para obtener la estimación de las puntuaciones factoriales son:</p>
<ul>
<li><p>El de regresión por mínimos cuadrados ordinarios (MCO), donde <span class="math inline">\(\hat{\bf F}=\left (\bf A^{\prime} \bf A \right)^{-1}\bf A^{\prime}\bf Z\)</span>.</p></li>
<li><p>El de Bartlett, basado en el método de estimación por mínimos cuadrados generalizados (MCG), con <span class="math inline">\(\hat{\bf F}=\left (\bf A^{\prime} \bf \Psi ^{-1}\bf A \right)^{-1}\bf A^{\prime}\Psi ^{-1}\bf Z\)</span>. El mismo estimador se puede obtener por máxima verosimilitud asumiendo normalidad multivariante.</p></li>
<li><p>El de Thompson (con un enfoque bayesiano), donde <span class="math inline">\(\hat{\bf F}=\left (\bf I+\bf A^{\prime} \bf \Psi ^{-1}\bf A \right)^{-1}\bf A^{\prime}\Psi ^{-1}\bf Z\)</span>. </p></li>
<li><p>El de Anderson-Rubin (que obtiene estimaciones MCG imponiendo la condición <span class="math inline">\(\bf F^{\prime}F =I\)</span> (<span class="math inline">\(\hat{\bf F}=\left (\bf A^{\prime} \bf \Psi ^{-1}\bf R \bf \Psi ^{-1}\bf A \right)^{-1}\bf A^{\prime}\Psi ^{-1}\bf Z\)</span>). </p></li>
</ul>
<p>Las ventajas y desventajas de cada uno de ellos pueden verse en <span class="citation">Mardia, Kent, and Bibby (<a href="referncias.html#ref-mardiaetal1979">1979</a>)</span> y <span class="citation">De la Fuente (<a href="referncias.html#ref-santiagodelafuente2011">2011</a>)</span>.</p>
<p>En el ejemplo de las TIC, las puntuaciones de los dos factores extraídos con el método de los factores principales y rotados con VARIMAX (la rotación no afecta a las puntuaciones), calculadas por el método de regresión, para los países de la UE-27 (se muestran los de Bélgica, Bulgaria y la República Checa), se obtienen en <strong>R</strong> como sigue:</p>
<div class="sourceCode" id="cb462"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">af_facprin3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/psych/man/fa.html">fa</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">TIC2021</span><span class="op">)</span>, nfactors <span class="op">=</span> <span class="fl">2</span>, rotate <span class="op">=</span> <span class="st">"VARIMAX"</span>, fm <span class="op">=</span> <span class="st">"pa"</span>, scores <span class="op">=</span> <span class="st">"regression"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/psych/man/factor.scores.html">factor.scores</a></span><span class="op">(</span><span class="va">TIC2021</span>, <span class="va">af_facprin3</span><span class="op">)</span><span class="op">$</span><span class="va">scores</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">3</span>, <span class="op">]</span></span>
<span><span class="co">#&gt;           PA1         PA2</span></span>
<span><span class="co">#&gt; BE  0.6256359  1.01289866</span></span>
<span><span class="co">#&gt; BG -2.1820404 -0.03439974</span></span>
<span><span class="co">#&gt; CZ -0.2189723  1.08635525</span></span></code></pre></div>
<p></p>
</div>
</div>
<div id="relaciones-y-diferencias-entre-el-af-y-el-acp" class="section level2" number="33.4">
<h2>
<span class="header-section-number">33.4</span> Relaciones y diferencias entre el AF y el ACP<a class="anchor" aria-label="anchor" href="#relaciones-y-diferencias-entre-el-af-y-el-acp"><i class="fas fa-link"></i></a>
</h2>
<p>ACF y AF son aparentemente muy similares, pero en realidad son muy diferentes.
Tanto ACP como AF son técnicas de reducción de la dimensionalidad que aparecen juntas en los paquetes estadísticos y persiguen objetivos muy similares, lo cual, en determinadas ocasiones, lleva al lector a pensar que son intercambiables entre sí, cuando eso no es cierto. Por ello, este capítulo finaliza con un breve comentario sobre las diferencias más relevantes entre ambos enfoques.</p>
<p>La primera es que ACP es una mera transformación de los datos en la que no se hace ningún supuesto sobre la matriz de covarianzas o de correlaciones. Sin embargo, AF asume que los datos proceden de un modelo bien definido, el modelo <a href="an%C3%A1lisis-factorial.html#eq:eqaf3">(33.3)</a>, en el que los factores subyacentes satisfacen unos supuestos bien definidos.</p>
<p>En segundo lugar, en ACP el énfasis se pone en el paso desde las variables observadas a las componentes principales, mientras que en AF se pone en el paso desde los factores latentes a las variables observadas. Es cierto que en ACP se pueden retener <span class="math inline">\(k\)</span> componentes y a partir de ellas aproximar (reproducir) las variables observadas; sin embargo, esta manera de proceder parece menos natural que la aproximación de las variables observadas en términos de los factores comunes y, además, al no tener en cuenta la unicidad de las variables, sobrestima las cargas factoriales y la dimensionalidad del conjunto de variables originales.</p>
<p>Una tercera diferencia es que, mientras que ACP obtiene componentes en función de las variables originales (los valores de las variables pueden ser estimados <em>a posteriori</em> en función de dichas componentes o factores), en AF las variables son, ellas mismas, combinaciones lineales de factores desconocidos. Es decir, mientras que en ACP la solución viene de la mano de la descomposición en valores singulares, en AF requiere procedimientos de estimación, normalmente iterativos.</p>
<p>La cuarta es que ACF es un procedimiento cerrado, mientras que AF es abierto, en el sentido de que explica la varianza común y no toda la varianza.</p>
<p>Finalmente, como pudo verse en la Sec. <a href="an%C3%A1lisis-factorial.html#metodosdeextraccion">33.3.2.1</a>, cuando las varianzas de los factores únicos son prácticamente nulas, el método de los factores principales es equivalente a ACP, y cuando son pequeñas ambos dan resultados similares. Sin embargo, cuando son grandes, en ACP las componentes principales (tanto las retenidas como las que no se retienen) las absorben, mientras que el AF las considera y les da su lugar.<br>
::: {.infobox_resume data-latex=““}</p>
<div id="resumen-32" class="section level3 unnumbered">
<h3>Resumen<a class="anchor" aria-label="anchor" href="#resumen-32"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>El análisis factorial es una técnica de reducción de la dimensionalidad que trata de dar una explicación de la varianza compartida, o común, de las variables objeto de estudio (no de toda la varianza, como hace el análisis de componentes principales) mediante un número mucho menor de factores comunes latentes. Por consiguiente, solo tiene sentido implementarlo si dichas variables se encuentran fuertemente correlacionadas.</p></li>
<li><p>Tras introducir al lector en los principales elementos teóricos del análisis factorial (el modelo básico y la solución factorial completa), se abordan las distintas etapas del procedimiento en su vertiente práctica:<br><span class="math inline">\((i)\)</span> el preanálisis factorial, que responde a la pregunta de si procede o no llevarlo a cabo;<br><span class="math inline">\((ii)\)</span> el análisis factorial propiamente dicho, prestando especial atención a los métodos de extracción de los factores y a las rotaciones de los mismos para facilitar su interpretación; y<br><span class="math inline">\((iii)\)</span> el postanálisis factorial, que incluye una serie de procedimientos para determinar si la solución factorial obtenida es o no aceptable.</p></li>
<li><p>Posteriormente, se aborda la cuestión de cómo estimar los valores de los factores obtenidos para cada elemento o individuo involucrado en el análisis, pues estas estimaciones pueden usarse como <em>inputs</em> en análisis posteriores (regresión, clúster, etc.) sustituyendo las variables originales por los factores obtenidos. El capítulo finaliza con algunos comentarios sobre las diferencias entre el análisis factorial y el de componentes principales, aparentemente muy similares, pero en realidad muy diferentes.
:::</p></li>
</ul>
</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="acp.html"><span class="header-section-number">32</span> Análisis de componentes principales</a></div>
<div class="next"><a href="escalamiento-multidimensional.html"><span class="header-section-number">34</span> Escalamiento multidimensional</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="Índice del capítulo"><h2>Índice del capítulo</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#an%C3%A1lisis-factorial"><span class="header-section-number">33</span> Análisis factorial</a></li>
<li><a class="nav-link" href="#introaf"><span class="header-section-number">33.1</span> Introducción</a></li>
<li>
<a class="nav-link" href="#elementos-te%C3%B3ricos-del-an%C3%A1lisis-factorial"><span class="header-section-number">33.2</span> Elementos teóricos del análisis factorial</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#modelobasicoaf"><span class="header-section-number">33.2.1</span> Modelo básico y terminología</a></li>
<li><a class="nav-link" href="#patr%C3%B3n-y-estructura-factorial"><span class="header-section-number">33.2.2</span> Patrón y estructura factorial</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#el-an%C3%A1lisis-factorial-en-la-pr%C3%A1ctica"><span class="header-section-number">33.3</span> El análisis factorial en la práctica</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#prean%C3%A1lisis-factorial"><span class="header-section-number">33.3.1</span> Preanálisis factorial</a></li>
<li><a class="nav-link" href="#an%C3%A1lisis-factorial-1"><span class="header-section-number">33.3.2</span> Análisis factorial</a></li>
<li><a class="nav-link" href="#postanalisis"><span class="header-section-number">33.3.3</span> Postanálisis factorial</a></li>
<li><a class="nav-link" href="#puntuaciones-factoriales"><span class="header-section-number">33.3.4</span> Puntuaciones factoriales </a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#relaciones-y-diferencias-entre-el-af-y-el-acp"><span class="header-section-number">33.4</span> Relaciones y diferencias entre el AF y el ACP</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#resumen-32">Resumen</a></li></ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Fundamentos de ciencia de datos con <strong>R</strong></strong>" coordinado por <a href="https://blog.uclm.es/gemafaviles/" class="text-light">Gema Fernández-Avilés y José-María Montero</a>. Generado por última vez el día 2023-11-13.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>Este libro ha sido generado con el paquete de R <a class="text-light" href="https://bookdown.org">bookdown</a>.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
