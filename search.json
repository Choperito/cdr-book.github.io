[{"path":"index.html","id":"prefacio","chapter":"Prefacio","heading":"Prefacio","text":"","code":""},{"path":"index.html","id":"hola-mundo","chapter":"Prefacio","heading":"¡Hola mundo!","text":"El siglo XXI está siendo testigo de grandes cambios vertiginosos en el contexto social y tecnológico, entre otros. Los tiempos han cambiado, la sociedad se ha globalizado y “exige” respuestas inmediatas problemas muy complejos. Vivimos en el mundo de la información, de los datos, o mejor, de las bases de datos masivas, y los ciudadanos y, sobre todo, las empresas y los gobiernos, dirigen su mirada hacia el mundo científico para que les ayude “oír las historias” que cuentan esos datos acerca de la realidad de la que han sido extraídos. Y dado su enorme volumen y sofisticación (en el nuevo mundo las imágenes y los textos, por ejemplo, también son datos), exigen algoritmos de nueva generación en el campo del machine learning, o incluso del deep learning, para “oír las historias” que cuentan. parecen mirar al “antiguo” investigador científico, sino al “nuevo” científico de datos.Ello, inevitablemente, se traduce en la necesidad de profesionales con una gran capacidad de adaptación este nuevo paradigma: los científicos de datos, también llamados por algunos los “nuevos hombres del Renacimiento”, para lo cual las Universidades y demás instituciones educativas especializadas se apresuran incluir el grado de Ciencia de Datos en su oferta educativa y ofrecer seminarios de software estadístico de acceso abierto para sus estudiantes de primeros cursos.Con la emergencia de la nueva sociedad, en la que el manejo de la ingente cantidad de información que genera se hace absolutamente necesario para circular por ella, la Ciencia de Datos ha venido para quedarse. Sin embargo, el mundo de la Ciencia de Datos es cualquier cosa menos sencillo. En él, cualquier ayuda, cualquier guía es bienvenida. Por ello, es muy recomendable que la persona que se quiera introducir en él, sea con fines de investigación o con fines profesionales, se agarre de la mano de un guía especializado que le lleve, de una manera amena, comprensible y eficiente, desde el planteamiento de su problema y la captura de la información necesaria para poderle dar una solución, hasta la redacción de las conclusiones finales que ha obtenido con los modernos informes reproducibles colaborativos. Y como en la parte central de ese camino tendrá que luchar con grandes gigantes (en la actualidad denominados técnicas estadísticas y algoritmos), el guía tendrá que explicarle, de manera sencilla y amena, en qué consiste la lucha (las técnicas y los algoritmos) y cómo llegar la victoria lo más rápido posible, enseñándole moverse por el mundo del software estadístico, en nuestro caso R, que le permitirá realizar los cálculos necesarios para vencer al problema planteado una velocidad vertiginosa.En resumen, la información masiva y el moderno tratamiento estadístico de la misma son la “mano invisible” que gobierna la sociedad del siglo XXI, y este manual pretende ser el guía anteriormente mencionado que le llevará de la mano cuando quiera caminar por ella.","code":""},{"path":"index.html","id":"por-qué-este-libro","chapter":"Prefacio","heading":"¿Por qué este libro?","text":"Lo dicho anteriormente ya justifica por sí solo la aparición de este manual. Afortunadamente, es el primero en la materia, pues son ya bastantes los materiales de calidad publicados sobre Ciencia de Datos. Sin embargo, quizás, éste pueda ser considerado el más completo. Y ello por varias razones.La primera es su completitud: este manual lleva de la mano al lector desde el planteamiento del problema hasta el informe que contiene la solución al mismo; o desde saber qué hacer con la información de la que dispone, hasta ser capaz de transformar tales bases de datos masivas, y casi imposibles de manejar, en respuestas problemas fundamentales de una empresa, institución o cualquier agente social.La segunda es su amplitud temática:Parte de las dos primeras preguntas que un neófito se puede hacer sobre esta temática: ¿qué es eso de la Ciencia de Catos que está en boca de todos? Y, ¿qué diablos es R y cómo funciona?Parte de las dos primeras preguntas que un neófito se puede hacer sobre esta temática: ¿qué es eso de la Ciencia de Catos que está en boca de todos? Y, ¿qué diablos es R y cómo funciona?Enseña cómo moverse en la jungla del Big Data y de los “nuevos” tipos de datos, siempre bajo el paraguas de la ética de los datos y del buen gobierno de dichos datos.Enseña cómo moverse en la jungla del Big Data y de los “nuevos” tipos de datos, siempre bajo el paraguas de la ética de los datos y del buen gobierno de dichos datos.Muestra al lector cómo obtener conocimiento de la oscuridad del enorme banco de información su disposición, que sabe cómo abordar ni manejar.Muestra al lector cómo obtener conocimiento de la oscuridad del enorme banco de información su disposición, que sabe cómo abordar ni manejar.deja nadie atrás, y de forma previa al contenido central del manual (las técnicas de Ciencia de Datos), incluye unas breves, pero magníficas, secciones sobre los rudimentos de la probabilidad, la inferencia estadística y el muestreo, para aquéllos familiarizados con estas cuestiones.deja nadie atrás, y de forma previa al contenido central del manual (las técnicas de Ciencia de Datos), incluye unas breves, pero magníficas, secciones sobre los rudimentos de la probabilidad, la inferencia estadística y el muestreo, para aquéllos familiarizados con estas cuestiones.Aborda una treintena de técnicas de Ciencia de Datos en el ámbito de la modelización, análisis de datos cualitativos, discriminación, machine learning supervisado y supervisado, con especial incidencia en las tareas de clasificación y clusterización -así como, en el caso supervisado, de reducción de la dimensionalidad, escalamiento multidimensional y análisis de correspondencias-, deep learning, análisis de datos textuales y de redes, y, finalmente, ciencia de datos espaciales (desde las perspectivas de la geoestadística, la econometría espacial y los procesos de punto).Aborda una treintena de técnicas de Ciencia de Datos en el ámbito de la modelización, análisis de datos cualitativos, discriminación, machine learning supervisado y supervisado, con especial incidencia en las tareas de clasificación y clusterización -así como, en el caso supervisado, de reducción de la dimensionalidad, escalamiento multidimensional y análisis de correspondencias-, deep learning, análisis de datos textuales y de redes, y, finalmente, ciencia de datos espaciales (desde las perspectivas de la geoestadística, la econometría espacial y los procesos de punto).Hace especial hincapié en la reproducibilidad en tiempo real (o ) entre los distintos miembros de un equipo (sea universitario, empresarial, o del tipo que sea) y en la difusión de los resultados obtenidos, enseñando al lector cómo generar informes reproducibles mediante RMarkdown y documentos Quarto o en otros modernos formatos.Hace especial hincapié en la reproducibilidad en tiempo real (o ) entre los distintos miembros de un equipo (sea universitario, empresarial, o del tipo que sea) y en la difusión de los resultados obtenidos, enseñando al lector cómo generar informes reproducibles mediante RMarkdown y documentos Quarto o en otros modernos formatos.Dedica un capítulo la creación de aplicaciones web interactivas (con Shiny).Dedica un capítulo la creación de aplicaciones web interactivas (con Shiny).Para aquéllos con pasión por la codificación, y que quieran compartir código y colaborar con otros desarrolladores, este manual aborda la gestión rápida y eficaz de proyectos (del tamaño que sean) mediante Git, un sistema de control de versiones distribuido, gratuito y de código abierto, y GitHub, un servicio de alojamiento de repositorios Git del cual, aquellos familiarizados con la cuestión de la codificación, o con aversión ella, podrán tomar el código que necesitan.Para aquéllos con pasión por la codificación, y que quieran compartir código y colaborar con otros desarrolladores, este manual aborda la gestión rápida y eficaz de proyectos (del tamaño que sean) mediante Git, un sistema de control de versiones distribuido, gratuito y de código abierto, y GitHub, un servicio de alojamiento de repositorios Git del cual, aquellos familiarizados con la cuestión de la codificación, o con aversión ella, podrán tomar el código que necesitan.Muestra al lector los primeros pasos para iniciarse en el geoprocesamiento en la nube.Muestra al lector los primeros pasos para iniciarse en el geoprocesamiento en la nube.Y, finalmente, aborda más de una docena de casos de uso (en medicina, periodismo, economía, criminología, marketing, moda, demanda de electricidad, cambio climático, reconocimiento de patrones en la forma de tuitear…) que ilustran la puesta en práctica de todos los conocimientos anteriormente adquiridos.Y, finalmente, aborda más de una docena de casos de uso (en medicina, periodismo, economía, criminología, marketing, moda, demanda de electricidad, cambio climático, reconocimiento de patrones en la forma de tuitear…) que ilustran la puesta en práctica de todos los conocimientos anteriormente adquiridos.La cuarta razón es que todo lo que el lector aprende en este manual lo puede reproducir y poner en práctica inmediatamente con R, puesto que el manual está trufado de chunks (o trozos de código R) que tiene más que cortar y pegar para reproducir los ejemplos que se muestran en el libro, cuyos datos están en el paquete CDR; o utilizar dichas chunks para abordar el problema que le ocupa con los datos que tenga su disposición. Una buena razón, sin duda. Por consiguiente, el manual es una buena combinación “teoría-práctica-software” que permite abordar cualquier problema que el científico de datos se plantee en cualquier disciplina o situación empresarial, médica, periodística…La quinta es su variedad de perspectivas. Son más de 40 los participantes en este manual. Algunos de ellos, prestigiosos profesores universitarios; otros, destacados miembros de instituciones públicas; otros, CEOs de empresas en la órbita de la ciencia de datos; otros, big names del mundo de R software… El manual es, sin duda, un magnífico ejemplo de colaboración Universidad-Empresa para buscar soluciones los problemas de las sociedades modernas.","code":""},{"path":"index.html","id":"a-quién-va-dirigido","chapter":"Prefacio","heading":"¿A quién va dirigido?","text":"Fundamentos de ciencia de datos con R está dirigido todos aquellos que desean desarrollar las habilidades necesarias para abordar proyectos complejos de Ciencia de Datos y “pensar con datos” (como lo acuñó Diane Lambert, de Google). El deseo de resolver problemas utilizando datos es su piedra angular. Por tanto, como se avanzó anteriormente, este manual deja nadie atrás, y lo único que requiere es “el deseo de resolver problemas utilizando datos”. excluye ninguna disciplina, excluye las personas que tengan un elevado nivel de análisis estadístico de datos, excluye nadie. Se ha procurado una combinación de rigor y sencillez, y de teoría y práctica, todo ello con sus correspondientes códigos en R, que satisfaga tanto los más exigentes como los principiantes.También está destinado todos aquellos que quieran sustituir la navegación por la web (la búsqueda del video, publicación de blog o tutorial online que solucione su problema –frustración tras frustración por la falta de consistencia, rigor e integridad de dichos materiales, así como por su sesgo hacia paquetes singulares para la implementación de las cuestiones que tratan–), por una “biblia de la ciencia de datos” rigurosa pero sencilla, práctica y de aplicación inmediata sin ser ni un experto estadístico ni un experto informático.Pero si alguien está destinado especialmente, es la comunidad hispano hablante. Este manual es un guiño dicha comunidad, para que tenga su disposición, en su lengua nativa, uno de los mejores manuales de Ciencia de Datos de la actualidad.","code":""},{"path":"index.html","id":"el-paquete-cdr","chapter":"Prefacio","heading":"El paquete CDR","text":"El paquete CDR contiene la mayoría de conjuntos de datos utilizados en este libro que están disponibles en otros paquetes. Para instalarlo use la función install_github() del paquete remotes.La lista de todos los conjuntos de datos puede obtenerse haciendo data().Este paquete ayudará al lector reproducir todos los ejemplos del libro. De acuerdo con las mejores prácticas en R, el paquete CDR sólo contiene los datos utilizados en el libro.","code":"\n# este comando sólo necesita ser ejecutado una vez\n# si el paquete remotes no está instalado, descomentar para instalarlo\n\n# install.packages(\"remotes\")\nremotes::install_github(\"cdr-book/CDR\")\nlibrary('CDR')\ndata(package = \"CDR\")"},{"path":"index.html","id":"por-qué-r","chapter":"Prefacio","heading":"¿Por qué R?","text":"R es un lenguaje de código abierto para computación estadística que se ha consolidado entre la comunidad científica internacional, en las últimas dos décadas, como una herramienta de primer nivel, consolidándose como líder permanente en el ámbito de la implementación de metodologías estadísticas para el análisis de datos. La utilidad de R para la Ciencia de Datos deriva de un fantástico ecosistema de paquetes (activo y en crecimiento), así como de un buen elenco de otros excelentes recursos: libros, manuales, blogs, foros y chats interactivos en las redes sociales, y una gran comunidad dispuesta colaborar, orientar y resolver diferentes cuestiones relacionadas con R.Por otra parte, R es el lenguaje estadístico y de análisis de datos más utilizado en la mayoría de los entornos académicos y, cómo , por una larga lista de importantes empresas, entre las que se cuentan Facebook (análisis de patrones de comportamientos relacionado con actualizaciones de estado e imágenes de perfil), Google (para la efectividad de la publicidad y la previsión económica), Twitter (visualización de datos y agrupación semántica), Microsoft (adquirió la empresa Revolution R), Uber (análisis estadístico), Airbnb (ciencia de datos), IBM (se unió al grupo del consorcio R), New York Times (visualización)…La comunidad R también es particularmente generosa e inclusiva, y hay grupos increíbles, como R-Ladies y Minority R Users, diseñados para ayudar garantizar que todos aprendan y usen las capacidades de R.","code":""},{"path":"index.html","id":"agradecimientos","chapter":"Prefacio","heading":"Agradecimientos","text":"queremos dar por finalizado este prefacio sin agradecer los 44 autores participantes en esta obra su esfuerzo por condensar, en más de 20 páginas, la teoría, práctica y tratamiento informático de la parte de la Ciencia de Datos que les fue encargada. Y sólo eso; el “más difícil todavía” fue que debían dirigirse un abanico de potenciales lectores tan grande como personas haya con “el deseo de resolver problemas utilizando datos”. Era misión imposible. Sin embargo, la vista del resultado, ha sido misión cumplida. El esfuerzo mereció la pena.Además, nos gustaría agradecer el apoyo incondicional recibido por (en orden alfabético): Itzcoátl Bueno, Ismael Caballero, Emilio L. Cano, Diego Henangómez, Ricardo Pérez, Manuel Vargas y Jorge Velasco.También queremos poner de manifiesto que la edición de este texto ha sido financiada por diversos entes de la Universidad de Castilla-La Mancha. En su mayor parte, por el Máster en Data Science y Business Analytics (con R software) (través de la orgánica: 02040M0280), pero también por la Facultad de Ciencias Jurídicas y Sociales de Toledo (través de su contrato programa: orgánica 00440710), el Departamento de Economía Aplicada (mediante sus fondos departamentales, DEAI 00421I126) y el Grupo de Investigación Economía Aplicada y Métodos Cuantitativos (que ha dedicado parte de sus fondos la edición de esta obra, orgánica 01110G3044-2023-GRIN-34336).todos, eternamente agradecidos por ayudarnos en este reto de transformar la oscuridad en conocimiento, de convertir en una ciencia y en un arte la difícil tarea de sacar valor de los datos, el petróleo del futuro. Quizás en este momento seamos conscientes de que hemos puesto nuestro granito de arena la ciencia que, buen seguro, juegue uno de los papeles más importantes de este siglo, caracterizado por el predominio de la información. Una ciencia, la Ciencia de Datos, que combina el análisis estadístico de datos, la algoritmia y el conocimiento del negocio para sacar valor del bien más abundante de la sociedad en la que vivimos: la información. Una disciplina cuyo dominio caracteriza los científicos de datos (también denominados los nuevos personajes del Renacimiento), profesión que ya fue calificada hace más de veinte años en la Harvard Business Review y en New York Times, entre otros, como la “más sexy del siglo XXI”.","code":""},{"path":"index.html","id":"nota","chapter":"Prefacio","heading":"Nota","text":"","code":""},{"path":"index.html","id":"este-manual-está-publicado-por-mcgraw-hill.-las-copias-físicas-están-disponibles-en-mcgraw-hill.-la-versión-online-se-puede-leer-de-forma-gratuita-en-httpscdr-book.github.io-y-tiene-la-licencia-de-creative-commons-reconocimiento-nocomercial-sinobraderivada-4.0-internacional.-si-tiene-algún-comentario-o-sugerencia-no-dude-en-contactar-con-los-editores-y-los-autores.-gracias","chapter":"Prefacio","heading":"0.1 Este manual está publicado por McGraw Hill. Las copias físicas están disponibles en McGraw Hill. La versión online se puede leer de forma gratuita en https://cdr-book.github.io/ y tiene la licencia de Creative Commons Reconocimiento-NoComercial-SinObraDerivada 4.0 Internacional. Si tiene algún comentario o sugerencia, no dude en contactar con los editores y los autores. ¡Gracias!","text":"","code":""},{"path":"ciencia-datos.html","id":"ciencia-datos","chapter":"Capítulo 1 ¿Es la ciencia de datos una ciencia?","heading":"Capítulo 1 ¿Es la ciencia de datos una ciencia?","text":"Gema Fernández-Avilés1Universidad de Castilla-La Mancha","code":""},{"path":"ciencia-datos.html","id":"ciencia","chapter":"Capítulo 1 ¿Es la ciencia de datos una ciencia?","heading":"1.1 ¿Qué se entiende por ciencia?","text":"es posible determinar si la ciencia de datos es una ciencia sin\npreviamente consensuar la definición de ciencia. La palabra ciencia\nderiva del latín (scientia), que deriva su vez del verbo latino\nscire (saber). En este sentido, Mario. Bunge (2004) distingue dos categorías\nfundamentales de conocimiento: el vulgar y el científico.\nEl conocimiento vulgar se adquiere de manera cotidiana partir de percepciones y\nsensaciones individuales, apoyándose en la evidencia (pruebas) y el sentido común.\nEste tipo de conocimiento constituye la base sobre la que se sustenta el\nconocimiento científico, que se enriquece al verificar, con la ayuda\nde un método, la validez de las observaciones realizadas. Se adquiere,\npor tanto, de forma consciente, deliberada y metódica. Puede ser\nsometido prueba y, llegado el caso, ser superado José María Montero (1997). Una definición generalmente aceptada de ciencia es la propuesta por\nBlaug (1980): “ciencia es el cuerpo de proposiciones sintéticas acerca\ndel mundo real que es susceptible, al menos en principio, de falsaciones\npor medio de la observación empírica, ya que excluye la posibilidad de\nque ciertos acontecimientos se produzcan. Así pues, la ciencia se\ncaracteriza por su método de formulación de proposiciones contrastables,\ny por su contenido, ni por su pretensión de certeza en el\nconocimiento; si alguna certeza proporciona la ciencia, esta será más\nbien la certeza de la ignorancia.”Si bien esta definición está encuadrada dentro del enfoque del falsacionismo de\nKarl Popper (para el cual las teorías son conjeturas, hipótesis generales que permiten explicar fenómenos), introduce la palabra clave de la discusión planteada, el método, y\nmás concretamente, el método científico.La palabra método procede del latín methodus”*, y esta del griego\n\\(\\mu \\varepsilon \\theta o \\delta 0 \\varsigma\\), que quiere decir\n“el camino seguir para ir más allá”. Es, pues, un procedimiento\npara conseguir algo; y como el fin que busca la ciencia es la verdad, el\nmétodo científico es el camino mediante el cual las ciencias pueden\nllegar encontrar sus respectivas verdades. El método científico es,\npor tanto, el conjunto de procedimientos por medio de los cuales se\nadquieren conocimientos rigurosos, ciertos y seguros acerca de un\nobjeto. El método científico ha de recorrerse siguiendo cuatro fases\n(Cancelo 1997): (\\(\\)) inventario previo de los fenómenos o de los hechos\nsignificativos rutinarios; (\\(ii\\)) planteamiento de un tema problemático\nque hace necesaria una explicación; (\\(iii\\)) ideación de conjeturas\ntendentes darla, y (\\(iv\\)) tratamiento de las diversas hipótesis hasta\nque sólo una se mantenga.Por tanto, aunque hay acuerdo la hora de dar una definición exacta\nde ciencia, sí lo hay la hora de aceptar el método\ncientífico como el elemento que define la ciencia: “el método científico y\nla finalidad la cual se aplica (el conocimiento objetivo del mundo)\nconstituyen la entera diferencia que existe entre la ciencia y la\n-ciencia […]. El método científico es un rasgo característico de la\nciencia, tanto de la pura como de la aplicada: donde hay método\ncientífico hay ciencia”, Mario. Bunge (2004). “Una ciencia es una disciplina\nque utiliza el método científico con la finalidad de hallar estructuras\ngenerales (leyes).” José María Montero (1997).Dado que la utilización del método científico es la piedra angular alrededor de la cual se articula el concepto de ciencia, continuación se aborda la cuestión de si, en base lo anterior, la ciencia de datos es o una ciencia. obstante, previamente, se abordará la cuestión de qué se entiende por ciencia de datos”","code":""},{"path":"ciencia-datos.html","id":"qué-es-la-ciencia-de-datos","chapter":"Capítulo 1 ¿Es la ciencia de datos una ciencia?","heading":"1.2 ¿Qué es la ciencia de datos?","text":" La Ciencia de datos es una disciplina emergente donde, diferencia de\notros saberes como, por ejemplo, las ciencias matemáticas, el corpus o\nla acumulación de conocimiento se ha generado en un lapso de tiempo\nrelativamente corto (y de una forma muy intensa), y lo largo de\nsiglos de historia. Su inicio data de la década de 1970,\naunque ya el término análisis de datos, acuñado por J. Tukey en 1962 en su artículo\nFuture Data Analysis (Tukey 1962)\nse puede considerar como un precedente del término ciencia de datos.\nEn dicho artículo, Tukey definió, por primera vez, el análisis de datos como:\n“procedimientos para analizar datos, técnicas\npara interpretar los resultados de dichos procedimientos, formas de\nplanificar la recopilación de datos para hacer su análisis más fácil,\nmás preciso o acertado, y toda la maquinaria y los resultados de las\nestadísticas matemáticas que se aplican al análisis de datos”\n(Tukey 1962). partir de este momento, toda una serie de\nacontecimientos fueron consolidando el término ciencia de datos\ncomo una nueva disciplina. Una breve descripción de los acontecimientos\nse muestran en la Fig. (1.1).\nFigura 1.1: Línea del tiempo de la ciencia de datos. Elaboración propia\nLa ciencia de datos implica la limpieza, la agregación y la manipulación\nde datos recabados de la web, de teléfonos inteligentes, de clientes,\nde pacientes, de sensores o de encuestas, entre otras fuentes, para\nllevar cabo un análisis de datos avanzado de los mismos,\nasí como su modelización, para ayudar detectar patrones,\ntendencias, comportamientos y, por tanto, facilitar\nla toma de decisiones. El crecimiento acelerado del volumen de fuentes\nde datos y, posteriormente, de datos, ha hecho que la ciencia de datos\nsea uno de los campos de más rápido desarrollo en todas las industrias.\nComo resultado, sorprende que surgiera la nueva profesión del\ncientífico de datos, para ayudar comprender y analizar los volúmenes\nmasivos de datos que se acumulaban en ese momento, trabajo que fue\ncalificado como el “trabajo más sexy del siglo XXI” por\nT. H. Davenport Patil (2012).\nLa ciencia de datos es, por tanto, una disciplina\nrelativamente nueva que combina la estadística, las matemáticas, la\ninformática y la programación, para obtener valor de los datos.Se utiliza en una amplia variedad de campos, como la astronomía, la\nmedicina, la economía, el marketing, las finanzas, la biología, la\nindustria, etc. Esta naturaleza transdisciplinaria de la ciencia de\ndatos añade cierta complejidad su caracterización pues, como se ha\navanzado, siendo una única disciplina, subsume en su ejercicio otras\ndisciplinas como las ciencias matemáticas y la estadística y la ciencia\nde la computación, que su vez son aplicadas un amplio rango de\ndominios de manera integral. La ciencia de datos se sirve de los métodos\nformales de las matemática y de las aplicaciones prácticas e\ningenieriles de las ciencias de la computación para la generación de\nconocimiento y para la resolución de problemas prácticos en múltiples\ncampos. Esta ubicuidad la sitúa, transversalmente,\nentre los saberes de primer orden. En otras palabras,\nla ciencia de datos va adoptando los paradigmas, modelos, teorías o\nconstructos propios del campo sustantivo en el que se ejerce, de forma\nque para resolver alguna problemática sobre personas, puede recurrir\nal corpus relativo de la psicología o de la sociología, y para\nprofundizar sobre alguna condición de salud, puede hacer lo propio con\nla medicina o la biología, por mencionar algunos ejemplos.","code":""},{"path":"ciencia-datos.html","id":"lo-científico-de-la-ciencia-de-datos","chapter":"Capítulo 1 ¿Es la ciencia de datos una ciencia?","heading":"1.3 Lo científico de la ciencia de datos","text":"En la Sec. 1.1 se manifestó que un aspecto fundamental de\nla ciencia es que utiliza el método científico con la finalidad de\nhallar estructuras generales (principios y leyes) con capacidad\npredictiva y comprobable (en el sentido amplio del término). Es por ello que el marco general de la\nmetodología científica ha sido bien fundamentado lo largo de las\núltimas décadas gracias las contribuciones de diferentes teóricos de\nla ciencia (Chalmers et al. (2000), Mario. Bunge (2004), Díez Moulines (2008)). Por otra\nparte, las ciencias se clasifican, según el objeto de estudio (M. Bunge 2018),\nen:2 empíricas y formales. Dado que la ciencia de datos\nsubsume diferentes disciplinas y se aplica diferentes campos, puede\ntener características tanto de las ciencias empíricas como de las\nformales.Si se analiza el conjunto de saberes científicos se aprecia que tienen\nen común una serie de características (M. Bunge 2018). Por tanto, la\npregunta fundamental en este punto es: ¿comparte la ciencia de datos\nestas características? De ser satisfechas, conferirían la ciencia de\ndatos el estatuto de ciencia que comparten otros saberes científicos:La actividad científica es metódica. Es decir, utiliza un método, se caracteriza\npor proceder de manera ordenada y planificada. Esta estructuración\nle otorga solidez y consistencia. En ciencia de datos también se\nactúa de manera metódica, través de diferentes metodologías, como\nKnowledge Discovery Databases (KDD), Sample, Explore, Modify,\nModel, Assess (SEMMA) y CRoss-Industry Standard Process Data\nMining (CRISP-DM), tal y como se expone en el Cap.\n2.La actividad científica es metódica. Es decir, utiliza un método, se caracteriza\npor proceder de manera ordenada y planificada. Esta estructuración\nle otorga solidez y consistencia. En ciencia de datos también se\nactúa de manera metódica, través de diferentes metodologías, como\nKnowledge Discovery Databases (KDD), Sample, Explore, Modify,\nModel, Assess (SEMMA) y CRoss-Industry Standard Process Data\nMining (CRISP-DM), tal y como se expone en el Cap.\n2.El conocimiento científico se fundamenta en hechos. En general,\nlos científicos disponen de diferentes instrumentos para observar y\nregistrar la realidad sobre la que conjeturan. Esta labor también\nla realizan los científicos de datos, quienes cuentan con un\nelevado número de instrumentos y metodologías para la recolección\nde datos. Tal es el caso de los cuestionarios, escalas\npsicométricas o datos transaccionales producidos por diferentes\ntecnologías.El conocimiento científico se fundamenta en hechos. En general,\nlos científicos disponen de diferentes instrumentos para observar y\nregistrar la realidad sobre la que conjeturan. Esta labor también\nla realizan los científicos de datos, quienes cuentan con un\nelevado número de instrumentos y metodologías para la recolección\nde datos. Tal es el caso de los cuestionarios, escalas\npsicométricas o datos transaccionales producidos por diferentes\ntecnologías.El saber científico es implica que las\nafirmaciones científicas puedan ser contrastadas través de los\nhechos. En ciencia de datos, esto también sucede, ya que, estadísticamente, los\nresultados los que se llega están ligados la subjetividad\ndel analista, sino la objetividad de los datos y las técnicas\nestadísticas de contrastación.El saber científico es implica que las\nafirmaciones científicas puedan ser contrastadas través de los\nhechos. En ciencia de datos, esto también sucede, ya que, estadísticamente, los\nresultados los que se llega están ligados la subjetividad\ndel analista, sino la objetividad de los datos y las técnicas\nestadísticas de contrastación.La ciencia es una actividad que trasciende los hechos. Es\ndecir, la ciencia parte de evidencias empíricas que tienden ser\nsuperadas, puesto que la explotación de las mismas suele generar\nnuevas evidencias que, su vez, pueden contribuir crear nuevos\nmarcos teóricos explicativos o ampliar los existentes. La ciencia\nde datos puede ejercerse en el mismo sentido. Por ejemplo, la\nconstrucción de un recomendador, como Netflix, parte de ciertos\ndatos, pero su uso genera nuevos inputs comportamentales que pueden\nser empleados para optimizar su sustrato algorítmico.La ciencia es una actividad que trasciende los hechos. Es\ndecir, la ciencia parte de evidencias empíricas que tienden ser\nsuperadas, puesto que la explotación de las mismas suele generar\nnuevas evidencias que, su vez, pueden contribuir crear nuevos\nmarcos teóricos explicativos o ampliar los existentes. La ciencia\nde datos puede ejercerse en el mismo sentido. Por ejemplo, la\nconstrucción de un recomendador, como Netflix, parte de ciertos\ndatos, pero su uso genera nuevos inputs comportamentales que pueden\nser empleados para optimizar su sustrato algorítmico.La investigación científica se caracteriza también por ser una\nactividad analítica. Es decir, tiende descomponer los\nproblemas en sus partes constitutivas. Cabe observar que la\nconsecuencia de ello es que se pueda hablar de una ciencia\ngeneral, sino de especializaciones. Naturalmente, la especialización\ntambién existe en esta disciplina; por eso, cuando la ciencia de\ndatos se aplica intensivamente en recursos humanos, por ejemplo, es\nposible hablar de Human Resource Analytics. Lo mismo ocurre en\nEconomía, con el Business Analytics, y así en un sinfín de\ndisciplinas.La investigación científica se caracteriza también por ser una\nactividad analítica. Es decir, tiende descomponer los\nproblemas en sus partes constitutivas. Cabe observar que la\nconsecuencia de ello es que se pueda hablar de una ciencia\ngeneral, sino de especializaciones. Naturalmente, la especialización\ntambién existe en esta disciplina; por eso, cuando la ciencia de\ndatos se aplica intensivamente en recursos humanos, por ejemplo, es\nposible hablar de Human Resource Analytics. Lo mismo ocurre en\nEconomía, con el Business Analytics, y así en un sinfín de\ndisciplinas.La ciencia es comunicable y, para ello, se sirve de sistemas\nrepresentacionales lógico-formales. Este atributo también se\naprecia en la ciencia de datos, puesto que los resultados tienden \nser compartidos través de diferentes estrategias, entre ellas, la\nvisualización de datos.La ciencia es comunicable y, para ello, se sirve de sistemas\nrepresentacionales lógico-formales. Este atributo también se\naprecia en la ciencia de datos, puesto que los resultados tienden \nser compartidos través de diferentes estrategias, entre ellas, la\nvisualización de datos.La ciencia, sin embargo, sólo puede describirse mediante sus\ncaracterísticas constitutivas, sino también funcionalmente (Hempel 2005).\nDe hecho, las características anteriormente citadas son\nlas que posibilitan las funciones descriptiva, explicativa y\npredictiva.La primera, la descriptiva, permite recabar información sobre el\nsuceso que se analiza para tratar de conocerlo en mayor profundidad\ny detalle. En ciencia de datos, usualmente, una de las primeras\ntareas consiste en describir el conjunto de datos para conocer en\ndetalle sus características, es decir, el número de variables, el\nnúmero de observaciones, los valores nulos, etc. Esta tarea se\nconoce como “comprensión de los datos” en la metodología\nCRIPS-DM (véase Sec. 2.3.La primera, la descriptiva, permite recabar información sobre el\nsuceso que se analiza para tratar de conocerlo en mayor profundidad\ny detalle. En ciencia de datos, usualmente, una de las primeras\ntareas consiste en describir el conjunto de datos para conocer en\ndetalle sus características, es decir, el número de variables, el\nnúmero de observaciones, los valores nulos, etc. Esta tarea se\nconoce como “comprensión de los datos” en la metodología\nCRIPS-DM (véase Sec. 2.3.La segunda, la explicativa, determina cómo se relacionan los\nfenómenos que se observan. En general, cuando un científico de\ndatos emplea un modelo de regresión lineal, lo que hace es\nestablecer una relación explicativa entre la variable dependiente y\nlas independientes. Esta parte se\nconoce como “modelado” en la metodología CRIPS-DM\n(véase Sec. 2.3).La segunda, la explicativa, determina cómo se relacionan los\nfenómenos que se observan. En general, cuando un científico de\ndatos emplea un modelo de regresión lineal, lo que hace es\nestablecer una relación explicativa entre la variable dependiente y\nlas independientes. Esta parte se\nconoce como “modelado” en la metodología CRIPS-DM\n(véase Sec. 2.3).La tercera, la predictiva, permite anticipar ciertos eventos en\nel tiempo o en el espacio. Tal es el caso de los científicos de\ndatos que ejercen su labor en el ámbito comercial y emplean, por\nejemplo, el análisis de series temporales para pronosticar las\nventas futuras y poder realizar una planificación del\naprovisionamiento de existencias con mayor eficiencia. Esta parte\nestá incluida en la fase de “validación” en la metodología CRIPS-DM\n(véase Sec. 2.3).La tercera, la predictiva, permite anticipar ciertos eventos en\nel tiempo o en el espacio. Tal es el caso de los científicos de\ndatos que ejercen su labor en el ámbito comercial y emplean, por\nejemplo, el análisis de series temporales para pronosticar las\nventas futuras y poder realizar una planificación del\naprovisionamiento de existencias con mayor eficiencia. Esta parte\nestá incluida en la fase de “validación” en la metodología CRIPS-DM\n(véase Sec. 2.3).la luz de lo expuesto hasta aquí, se puede sostener, sin lugar dudas,\nque la ciencia de datos emplea el método científico y comparte las\nprincipales funciones de la ciencia. Ahora bien, la ciencia de datos \npuede entenderse plenamente sin presuponer las disciplinas en las que se\naplica. Por tanto, uno de los interrogantes que deberán resolver los\nfuturos profesionales es si la ciencia de datos es un saber de primer\norden, que lidia directamente con la realidad, como la física o la\nquímica, o si, por el contrario, es un saber de segundo orden, es decir,\nuna suerte de disciplina que se sirve de otros saberes para desplegarlos\ny actualizarlos.","code":""},{"path":"ciencia-datos.html","id":"resumen","chapter":"Capítulo 1 ¿Es la ciencia de datos una ciencia?","heading":"Resumen","text":"Para determinar si la ciencia de datos es, realmente, una ciencia en\nprimer lugar se debe consensuar la definición de ciencia, que va\níntimamente ligada la definición de método científico.Para determinar si la ciencia de datos es, realmente, una ciencia en\nprimer lugar se debe consensuar la definición de ciencia, que va\níntimamente ligada la definición de método científico.Las ciencias tienen en común una serie de características, que deben ser satisfechas\npor la ciencia de datos para adquirir el estatus de ciencia.Las ciencias tienen en común una serie de características, que deben ser satisfechas\npor la ciencia de datos para adquirir el estatus de ciencia.Dado que la ciencia de datos emplea el método científico y comparte las\nprincipales funciones de la ciencia, se concluye que la ciencia de datos\nes una ciencia.Dado que la ciencia de datos emplea el método científico y comparte las\nprincipales funciones de la ciencia, se concluye que la ciencia de datos\nes una ciencia.","code":""},{"path":"metodologia.html","id":"metodologia","chapter":"Capítulo 2 Metodología en ciencia de datos","heading":"Capítulo 2 Metodología en ciencia de datos","text":"Gema Fernández-Avilés\\(^{}\\) y Ramón . Carrasco\\(^{b}\\)\\(^{}\\)Universidad de Castilla-La Mancha\\(^{b}\\)Universidad Complutense de Madrid","code":""},{"path":"metodologia.html","id":"preliminares","chapter":"Capítulo 2 Metodología en ciencia de datos","heading":"2.1 Preliminares","text":"En el Cap. 1 se puso de manifiesto que el método científico es el elemento\nque define la ciencia. M. Bunge (2018), al hablar del método científico, lo define como: “un procedimiento para\ntratar un conjunto de problemas. Cada clase de problemas requiere un conjunto de métodos o\ntécnicas especiales. Los problemas del conocimiento, diferencia de los del lenguaje o los de\nla acción, requieren la invención o la aplicación de procedimientos especiales adecuados para\nlos varios estadios del tratamiento de los problemas…”. De acuerdo con su concepción del\nmétodo, M. Bunge (2018) destaca ocho operaciones en la aplicación de este:\nEnunciar preguntas bien formuladas y verosímilmente fecundas.Enunciar preguntas bien formuladas y verosímilmente fecundas.Arbitrar conjeturas, fundadas y contrastables con la experiencia, para contestar las preguntas.Arbitrar conjeturas, fundadas y contrastables con la experiencia, para contestar las preguntas.Derivar consecuencias lógicas de las conjeturas.Derivar consecuencias lógicas de las conjeturas.Arbitrar técnicas para someter las conjeturas contraste.Arbitrar técnicas para someter las conjeturas contraste.Someter contraste esas técnicas para comprobar su relevancia y la validez que merecen.Someter contraste esas técnicas para comprobar su relevancia y la validez que merecen.Llevar cabo la contrastación e interpretar sus resultados.Llevar cabo la contrastación e interpretar sus resultados.Estimar la pretensión de verdad de las conjeturas y la fidelidad de las técnicas.Estimar la pretensión de verdad de las conjeturas y la fidelidad de las técnicas.Determinar los dominios en los cuales valen las conjeturas y las técnicas, y formular los\nnuevos problemas originados por la investigación.Determinar los dominios en los cuales valen las conjeturas y las técnicas, y formular los\nnuevos problemas originados por la investigación.su vez, sugiere una serie de reglas para la ejecución ordenada de las operaciones anteriores:Formular el problema con precisión y, al principio, específicamente.Formular el problema con precisión y, al principio, específicamente.Proponer conjeturas bien definidas y fundadas de algún modo, y suposiciones que \ncomprometan cuestiones concretas ni tampoco ocurrencias sin fundamento visible.Proponer conjeturas bien definidas y fundadas de algún modo, y suposiciones que \ncomprometan cuestiones concretas ni tampoco ocurrencias sin fundamento visible.Someter las hipótesis contrastación dura, laxa.Someter las hipótesis contrastación dura, laxa.declarar verdadera una hipótesis satisfactoriamente confirmada; considerarla, en el\nmejor de los casos, como parcialmente verdadera.declarar verdadera una hipótesis satisfactoriamente confirmada; considerarla, en el\nmejor de los casos, como parcialmente verdadera.Preguntarse por qué la respuesta es como es y de otra manera.Preguntarse por qué la respuesta es como es y de otra manera.Sin embargo, de acuerdo con José María Montero (1997), estas reglas son definitivas ni infalibles y necesitan de\nulterior perfeccionamiento, que se llevará cabo lo largo de la investigación científica. Además,\nlas reglas del método científico son autosuficientes, necesitan apoyarse en la inteligencia y\ncreatividad humanas.En resumen, es el tratamiento sistemático de los problemas, de la forma descrita, y la certeza\nde los resultados obtenidos o la utilización de las técnicas muy concretas y específicas, el que\ngarantiza el carácter científico de las conclusiones Cancelo (1997). La ciencia de los datos, como podía ser\nde otra forma, proporciona una serie de metodologías que guían el trabajo de los científicos de\ndatos. Las principales metodologías se presentan continuación.","code":""},{"path":"metodologia.html","id":"principales-metodologías-en-ciencia-de-datos","chapter":"Capítulo 2 Metodología en ciencia de datos","heading":"2.2 Principales metodologías en ciencia de datos","text":"\nEn un proyecto de ciencia de datos es muy importante la metodología, pues proporciona al\ncientífico de datos una estrategia y un marco con el que trabajar. Desde finales del siglo XX se\nhan ido proponiendo diversas metodologías, centradas en la resolución de problemas concretos\nmediante el uso de los datos, que hoy podrían englobarse bajo el paraguas común de la ciencia de\ndatos.Estas metodologías han nacido y se han desarrollado en el ámbito de los problemas de negocio,\naunque todas son extrapolables otros ámbitos de conocimiento (educación, ciencia, salud,\netc.). Por tanto, en este capítulo (y, en general, en todo el manual)\nel término de “negocio” (empleado en las propias metodologías\nfrecuentemente) debe de ser entendido en sentido amplio, abarcando los diversos ámbitos del\nconocimiento en los que se aplica la ciencia de datos.Por su amplio uso, destacan tres metodologías:Obtención de conocimiento en bases de datos (Knowledge Discovery Databases-KDD), propuesta\npor Fayyad et al. (1996) e inspirada en un trabajo previo de Brachman Anand (1994), fue\nla primera metodología aceptada por la comunidad científica. Se trata\ndel primer intento serio de sistematizar el proceso conocido hoy día como ciencia de datos y en\naquellos tiempos como conocimiento basado en bases de datos, pues se centraba en la minería\nde datos.Obtención de conocimiento en bases de datos (Knowledge Discovery Databases-KDD), propuesta\npor Fayyad et al. (1996) e inspirada en un trabajo previo de Brachman Anand (1994), fue\nla primera metodología aceptada por la comunidad científica. Se trata\ndel primer intento serio de sistematizar el proceso conocido hoy día como ciencia de datos y en\naquellos tiempos como conocimiento basado en bases de datos, pues se centraba en la minería\nde datos.SEMMA, acrónimo que coincide con las etapas de las que consta (en inglés,\nSample, Explore, Modify, Model Assess) fue desarrollada y mantenida\npor el Instituto SAS en 2012. Se define como el proceso de selección, exploración y modelización de grandes\nbases de datos para descubrir patrones de negocio desconocidos.SEMMA, acrónimo que coincide con las etapas de las que consta (en inglés,\nSample, Explore, Modify, Model Assess) fue desarrollada y mantenida\npor el Instituto SAS en 2012. Se define como el proceso de selección, exploración y modelización de grandes\nbases de datos para descubrir patrones de negocio desconocidos.CRISP-DM, acrónimo en inglés de Cross Industry Standard Process Data Mining, planteada\ninicialmente en 1996, publicada formalmente en Chapman et al. (2000a) y mantenida durante varios años por\nla compañía SPSS, posteriormente adquirida por IBM, que se ha encargado de mantenerla y\nrefinarla hasta la actualidad. Esta metodología define una secuencia flexible de\nseis fases que permiten la construcción e implementación de un modelo de minería de datos para\nser utilizado en un entorno real, que contribuya respaldar la toma de decisiones de negocio.\nSe considera la metodología más utilizada en la actualidad (Azevedo Santos (2008) y Shafique Qaiser (2014), entre otros) y\nse describe en la siguiente sección.CRISP-DM, acrónimo en inglés de Cross Industry Standard Process Data Mining, planteada\ninicialmente en 1996, publicada formalmente en Chapman et al. (2000a) y mantenida durante varios años por\nla compañía SPSS, posteriormente adquirida por IBM, que se ha encargado de mantenerla y\nrefinarla hasta la actualidad. Esta metodología define una secuencia flexible de\nseis fases que permiten la construcción e implementación de un modelo de minería de datos para\nser utilizado en un entorno real, que contribuya respaldar la toma de decisiones de negocio.\nSe considera la metodología más utilizada en la actualidad (Azevedo Santos (2008) y Shafique Qaiser (2014), entre otros) y\nse describe en la siguiente sección.","code":""},{"path":"metodologia.html","id":"met-crisp-dm","chapter":"Capítulo 2 Metodología en ciencia de datos","heading":"2.3 CRISP-DM para ciencia de datos","text":"La metodología CRISP-DM consta de seis etapas, que han variado desde su publicación en\n2000 (Fig. 2.1) y una serie de funciones que se han sido refinando en el tiempo\n(CRISP-DM, 2021).\nDe manera esquemática, dichas etapas son:\nFigura 2.1: Etapas de la metodología CRISP-DM.\nEntendimiento del negocio. Fundamental para el éxito del mismo.\nConsta de cuatro fases:\nDeterminación de los objetivos de negocio, consensuados\npreviamente con la organización. Es importante fijar los key\nperformance indicators (KPIs) que permitan medir fidedignamente el\ngrado de consecución de dichos objetivos.\nEvaluación de la situación actual. Inventariar las fuentes de datos que\nestarán disponibles, los recursos materiales y humanos con los que se podrá contar,\nlos factores de riesgo y el plan de contingencia para los mismos.\nDeterminación de los objetivos del proyecto, que debe alinearse con el correspondiente\nrendimiento de los modelos (por ejemplo, cuál debe de ser su nivel de precisión).\nPlan del proyecto, con los procesos realizar y recursos\nasignados.\nEntendimiento del negocio. Fundamental para el éxito del mismo.\nConsta de cuatro fases:Determinación de los objetivos de negocio, consensuados\npreviamente con la organización. Es importante fijar los key\nperformance indicators (KPIs) que permitan medir fidedignamente el\ngrado de consecución de dichos objetivos.Determinación de los objetivos de negocio, consensuados\npreviamente con la organización. Es importante fijar los key\nperformance indicators (KPIs) que permitan medir fidedignamente el\ngrado de consecución de dichos objetivos.Evaluación de la situación actual. Inventariar las fuentes de datos que\nestarán disponibles, los recursos materiales y humanos con los que se podrá contar,\nlos factores de riesgo y el plan de contingencia para los mismos.Evaluación de la situación actual. Inventariar las fuentes de datos que\nestarán disponibles, los recursos materiales y humanos con los que se podrá contar,\nlos factores de riesgo y el plan de contingencia para los mismos.Determinación de los objetivos del proyecto, que debe alinearse con el correspondiente\nrendimiento de los modelos (por ejemplo, cuál debe de ser su nivel de precisión).Determinación de los objetivos del proyecto, que debe alinearse con el correspondiente\nrendimiento de los modelos (por ejemplo, cuál debe de ser su nivel de precisión).Plan del proyecto, con los procesos realizar y recursos\nasignados.Plan del proyecto, con los procesos realizar y recursos\nasignados.Comprensión de los datos. Consta de cuatro fases que giran en torno los datos:\nRecopilación, tanto de datos internos como externos la organización. Esta fase\nincluye, si es necesario, la obtención de datos adicionales, y el etiquetado de casos \nclasificados con anterioridad.\nDescripción, especificando aspectos como la cantidad de datos disponibles,\nanticipando posibles problemas de rendimiento en el modelado posterior, tipología de las\nvariables (numéricas, categóricas, booleanas, etc.),\ncodificación de las mismas (especialmente para las categóricas), etc.\nExploración, tavés del análisis exploratorio de datos (AED). Esta tarea ayuda \nformular hipótesis sobre los datos y dirige las posteriores etapas de preparación y modelado.\nVerificación de la calidad, detectando problemas como la existencia de valores perdidos, errores en\ndatos (por ejemplo, tipográficos), errores de las mediciones (datos que son correctos\npero que están expresados en unidades de medida incorrectas), incoherencias en la\ncodificación (especialmente en las variables categóricas).\nComprensión de los datos. Consta de cuatro fases que giran en torno los datos:Recopilación, tanto de datos internos como externos la organización. Esta fase\nincluye, si es necesario, la obtención de datos adicionales, y el etiquetado de casos \nclasificados con anterioridad.Recopilación, tanto de datos internos como externos la organización. Esta fase\nincluye, si es necesario, la obtención de datos adicionales, y el etiquetado de casos \nclasificados con anterioridad.Descripción, especificando aspectos como la cantidad de datos disponibles,\nanticipando posibles problemas de rendimiento en el modelado posterior, tipología de las\nvariables (numéricas, categóricas, booleanas, etc.),\ncodificación de las mismas (especialmente para las categóricas), etc.Descripción, especificando aspectos como la cantidad de datos disponibles,\nanticipando posibles problemas de rendimiento en el modelado posterior, tipología de las\nvariables (numéricas, categóricas, booleanas, etc.),\ncodificación de las mismas (especialmente para las categóricas), etc.Exploración, tavés del análisis exploratorio de datos (AED). Esta tarea ayuda \nformular hipótesis sobre los datos y dirige las posteriores etapas de preparación y modelado.Exploración, tavés del análisis exploratorio de datos (AED). Esta tarea ayuda \nformular hipótesis sobre los datos y dirige las posteriores etapas de preparación y modelado.Verificación de la calidad, detectando problemas como la existencia de valores perdidos, errores en\ndatos (por ejemplo, tipográficos), errores de las mediciones (datos que son correctos\npero que están expresados en unidades de medida incorrectas), incoherencias en la\ncodificación (especialmente en las variables categóricas).Verificación de la calidad, detectando problemas como la existencia de valores perdidos, errores en\ndatos (por ejemplo, tipográficos), errores de las mediciones (datos que son correctos\npero que están expresados en unidades de medida incorrectas), incoherencias en la\ncodificación (especialmente en las variables categóricas).Preparación de los datos. Esta etapa suele ser la que requiere más tiempo y esfuerzo\ndel proyecto (frecuentemente más del 70 %). Consta de cinco fases:\nSelección: se toman decisiones sobre los casos o filas que hay que seleccionar y sobre\nlos atributos (variables) o columnas que hay que incluir.\nLimpieza: si en la subfase de verificación de la calidad de los datos se han detectado problemas, hay que subsanarlos. Los valores perdidos se pueden excluir o interpolar; los errores en los datos se pueden corregir con algún esquema lógico o manualmente; si hubiera incoherencias en la codificación se podría llevar cabo una recodificación que sustituyese la codificación original.\nConstrucción: partir de los ya disponibles, de nuevos atributos (variables) o columnas y de nuevas filas o registros.\nIntegración: necesaria para construir un concepto de negocio unificado (por ejemplo, el concepto de cliente) si se han usado diversas fuentes (tiquet de compra y registros de cliente). La fusión de columnas con algunas claves en común (join), adición de filas con las columnas en común (union), la agrupación, etc., se utilizan frecuentemente.\nFormateo: orientada las necesidades de los modelos que se usarán posteriormente. La conversión de variables categóricas numéricas (usando técnicas de one hot encoding) o viceversa, la normalización (usando normalizaciones min-max o z-score), etc., son tareas comunes en esta etapa.\nPreparación de los datos. Esta etapa suele ser la que requiere más tiempo y esfuerzo\ndel proyecto (frecuentemente más del 70 %). Consta de cinco fases:Selección: se toman decisiones sobre los casos o filas que hay que seleccionar y sobre\nlos atributos (variables) o columnas que hay que incluir.Selección: se toman decisiones sobre los casos o filas que hay que seleccionar y sobre\nlos atributos (variables) o columnas que hay que incluir.Limpieza: si en la subfase de verificación de la calidad de los datos se han detectado problemas, hay que subsanarlos. Los valores perdidos se pueden excluir o interpolar; los errores en los datos se pueden corregir con algún esquema lógico o manualmente; si hubiera incoherencias en la codificación se podría llevar cabo una recodificación que sustituyese la codificación original.Limpieza: si en la subfase de verificación de la calidad de los datos se han detectado problemas, hay que subsanarlos. Los valores perdidos se pueden excluir o interpolar; los errores en los datos se pueden corregir con algún esquema lógico o manualmente; si hubiera incoherencias en la codificación se podría llevar cabo una recodificación que sustituyese la codificación original.Construcción: partir de los ya disponibles, de nuevos atributos (variables) o columnas y de nuevas filas o registros.Construcción: partir de los ya disponibles, de nuevos atributos (variables) o columnas y de nuevas filas o registros.Integración: necesaria para construir un concepto de negocio unificado (por ejemplo, el concepto de cliente) si se han usado diversas fuentes (tiquet de compra y registros de cliente). La fusión de columnas con algunas claves en común (join), adición de filas con las columnas en común (union), la agrupación, etc., se utilizan frecuentemente.Integración: necesaria para construir un concepto de negocio unificado (por ejemplo, el concepto de cliente) si se han usado diversas fuentes (tiquet de compra y registros de cliente). La fusión de columnas con algunas claves en común (join), adición de filas con las columnas en común (union), la agrupación, etc., se utilizan frecuentemente.Formateo: orientada las necesidades de los modelos que se usarán posteriormente. La conversión de variables categóricas numéricas (usando técnicas de one hot encoding) o viceversa, la normalización (usando normalizaciones min-max o z-score), etc., son tareas comunes en esta etapa.Formateo: orientada las necesidades de los modelos que se usarán posteriormente. La conversión de variables categóricas numéricas (usando técnicas de one hot encoding) o viceversa, la normalización (usando normalizaciones min-max o z-score), etc., son tareas comunes en esta etapa.Modelado: se trata de que los modelos ingieran dichos datos y aprendan de ellos, de forma\nautomática, cómo resolver el problema de negocio planteado mediante técnicas, especialmente\nde machine learning. Las subfases de las que consta esta fase son:\nSelección de técnicas de modelado, si se va usar machine learning supervisado o supervisado y, especifícamente, el tipo de algoritmos usar en cada una de estas técnicas. Por supuesto, se tienen en cuenta los requisitos fijados en la primera fase, la cantidad y tipo de datos de los que se dispone, los requisitos concretos de cada modelo, etc.\nGeneración de un diseño de comprobación, través de medidas\ny criterios de bondad del modelo: el área bajo la curva ROC, el criterio de información de Akaike (AIC), el coeficiente de determinación lineal (\\(R^2\\)), la matrizde confusión, etc.\nGeneración de modelos, que se entrenan oportunamente para seleccionar, posteriormente, el\nmás adecuado.\nValidación del modelo, en función de los modelos generados y del plan de pruebas especificado.\nModelado: se trata de que los modelos ingieran dichos datos y aprendan de ellos, de forma\nautomática, cómo resolver el problema de negocio planteado mediante técnicas, especialmente\nde machine learning. Las subfases de las que consta esta fase son:Selección de técnicas de modelado, si se va usar machine learning supervisado o supervisado y, especifícamente, el tipo de algoritmos usar en cada una de estas técnicas. Por supuesto, se tienen en cuenta los requisitos fijados en la primera fase, la cantidad y tipo de datos de los que se dispone, los requisitos concretos de cada modelo, etc.Selección de técnicas de modelado, si se va usar machine learning supervisado o supervisado y, especifícamente, el tipo de algoritmos usar en cada una de estas técnicas. Por supuesto, se tienen en cuenta los requisitos fijados en la primera fase, la cantidad y tipo de datos de los que se dispone, los requisitos concretos de cada modelo, etc.Generación de un diseño de comprobación, través de medidas\ny criterios de bondad del modelo: el área bajo la curva ROC, el criterio de información de Akaike (AIC), el coeficiente de determinación lineal (\\(R^2\\)), la matrizde confusión, etc.Generación de un diseño de comprobación, través de medidas\ny criterios de bondad del modelo: el área bajo la curva ROC, el criterio de información de Akaike (AIC), el coeficiente de determinación lineal (\\(R^2\\)), la matrizde confusión, etc.Generación de modelos, que se entrenan oportunamente para seleccionar, posteriormente, el\nmás adecuado.Generación de modelos, que se entrenan oportunamente para seleccionar, posteriormente, el\nmás adecuado.Validación del modelo, en función de los modelos generados y del plan de pruebas especificado.Validación del modelo, en función de los modelos generados y del plan de pruebas especificado.Evaluación. Se debe comprobar que el modelo final generado cumple las expectativas de\nnegocio especificadas en la primera fase. Hay que hacer hincapié en este aspecto ya que\nsuele confundir en la práctica esta fase de evaluación con la subfase de la anterior etapa\nde validación del modelo. Ahora la evaluación se lleva cabo desde el punto de vista del negocio.\nAsí, por ejemplo, cabe plantearse si con el modelo elegido se pueden alcanzar las metas de\nnegocio especificadas y medidas con los correspondientes KPIs. Tras esta evaluación de los\nresultados del modelo se abre un proceso de revisión que permitirá valorar si\ncumple las expectativas o se tiene que volver etapas anteriores.Evaluación. Se debe comprobar que el modelo final generado cumple las expectativas de\nnegocio especificadas en la primera fase. Hay que hacer hincapié en este aspecto ya que\nsuele confundir en la práctica esta fase de evaluación con la subfase de la anterior etapa\nde validación del modelo. Ahora la evaluación se lleva cabo desde el punto de vista del negocio.\nAsí, por ejemplo, cabe plantearse si con el modelo elegido se pueden alcanzar las metas de\nnegocio especificadas y medidas con los correspondientes KPIs. Tras esta evaluación de los\nresultados del modelo se abre un proceso de revisión que permitirá valorar si\ncumple las expectativas o se tiene que volver etapas anteriores.Implementación. El conocimiento obtenido con el modelado es puesto en valor en esta\nfase de cara satisfacer los objetivos de negocio planteados en el proyecto. Este despliegue\ndepende mucho del tipo de proyecto que se esté realizando, aunque generalmente incluye\nlas actividades siguientes:\nPlanificación del despliegue: del modelado y/o del\nconocimiento obtenido.\nPlanificación del control y del mantenimiento. Así, por ejemplo, hay que verificar que el modelo está cumpliendo con las expectativas para las que se ha desarrollado, comprobar si hay que reentrenarlo o sustituirlo por otro, etc.\nCreación del informe final: para comunicar los resultados del proyecto y los pasos siguientes.\nRevisión final del proyecto: donde se establecen las conclusiones finales y se formalizan las lecciones aprendidas para incorporarlas futuros proyectos de ciencia de datos.\nImplementación. El conocimiento obtenido con el modelado es puesto en valor en esta\nfase de cara satisfacer los objetivos de negocio planteados en el proyecto. Este despliegue\ndepende mucho del tipo de proyecto que se esté realizando, aunque generalmente incluye\nlas actividades siguientes:Planificación del despliegue: del modelado y/o del\nconocimiento obtenido.Planificación del despliegue: del modelado y/o del\nconocimiento obtenido.Planificación del control y del mantenimiento. Así, por ejemplo, hay que verificar que el modelo está cumpliendo con las expectativas para las que se ha desarrollado, comprobar si hay que reentrenarlo o sustituirlo por otro, etc.Planificación del control y del mantenimiento. Así, por ejemplo, hay que verificar que el modelo está cumpliendo con las expectativas para las que se ha desarrollado, comprobar si hay que reentrenarlo o sustituirlo por otro, etc.Creación del informe final: para comunicar los resultados del proyecto y los pasos siguientes.Creación del informe final: para comunicar los resultados del proyecto y los pasos siguientes.Revisión final del proyecto: donde se establecen las conclusiones finales y se formalizan las lecciones aprendidas para incorporarlas futuros proyectos de ciencia de datos.Revisión final del proyecto: donde se establecen las conclusiones finales y se formalizan las lecciones aprendidas para incorporarlas futuros proyectos de ciencia de datos.Para concluir, subrayar que, aunque son varias las metodologías propuestas, CRISP-DM\nes la más completa, la más desarrollada y, además, puede ser implementada, como\ntodas las propuestas en la literatura, mediante el lenguaje R.","code":""},{"path":"metodologia.html","id":"resumen-1","chapter":"Capítulo 2 Metodología en ciencia de datos","heading":"Resumen","text":"El método científico es el elemento clave en la definición de ciencia.El método científico es el elemento clave en la definición de ciencia.M. Bunge (2018) establece una serie de reglas y características para la correcta aplicación de la metodología científica. En un proyecto de ciencia de datos es muy importante la metodología, pues proporciona al\ncientífico de datos una estrategia y un marco en el que trabajar. Entre ellas destaca\nel CRISP-DM como la más aceptada y utilizada por las empresas y científicos.M. Bunge (2018) establece una serie de reglas y características para la correcta aplicación de la metodología científica. En un proyecto de ciencia de datos es muy importante la metodología, pues proporciona al\ncientífico de datos una estrategia y un marco en el que trabajar. Entre ellas destaca\nel CRISP-DM como la más aceptada y utilizada por las empresas y científicos.El CRISP-DM se basa en la organización flexible de seis pilares: entendimiento del negocio, compresión de los\ndatos, preparación de los datos, modelado, evaluación e implementación.El CRISP-DM se basa en la organización flexible de seis pilares: entendimiento del negocio, compresión de los\ndatos, preparación de los datos, modelado, evaluación e implementación.","code":""},{"path":"ch-110003.html","id":"ch-110003","chapter":"Capítulo 3 R para ciencia de datos","heading":"Capítulo 3 R para ciencia de datos","text":"Emilio L. CanoUniversidad Rey Juan Carlos","code":""},{"path":"ch-110003.html","id":"introducción","chapter":"Capítulo 3 R para ciencia de datos","heading":"3.1 Introducción","text":"\nEl análisis estadístico de datos es una tarea fundamental\nen la transformación digital de las empresas y organizaciones.\nSiempre ha estado ahí, pero\nen la actualidad la disponibilidad de datos, la cantidad de los mismos,\ny la velocidad con la que se requieren resultados, está haciendo\nnecesario el capacitar los profesionales\npara su análisis con nuevas herramientas. Nuevas tendencias (muchas veces\nmalinterpretadas) como Inteligencia Artificial, Big Data, Industria 4.0, Internet Things (IoT),\no Data Science, aumentan el interés por parte de las empresas, los profesionales\ny los investigadores en estas técnicas.El tratamiento de datos y su análisis requiere el uso de\nsoftware avanzado. Aunque algunas tareas como, por ejemplo, mecanizar y almacenar datos, se pueden realizar\neficazmente con programas de hoja de cálculo\ncomo Excel,\nse debería\nutilizar software especializado para el análisis de datos. Existen distintos\npaquetes estadísticos comerciales, como SPSS, Statgraphics, Stata, SAS, JMP o Minitab. En los últimos\naños se ha abierto camino como alternativa el software estadístico y lenguaje de\nprogramación R (R Core Team 2021). Hay otras alternativas que, en su mayoría, o son parciales, referidas un\námbito concreto, o son más lenguajes de programación que software estadístico, como Python.\nR es software libre, pero su gratuidad sólo es una\nde sus ventajas, como se verá lo largo del libro. Su gran inconveniente\nes la curva de aprendizaje: es tan fácil de aprender y usar como un\nsoftware de ventanas, ya que el uso de R se basa en expresiones que\nhay que ejecutar desde scripts (archivos de código).R es un\nsistema para computación estadística: software de análisis de datos y\nlenguaje de programación. Ha sido ampliamente utilizado en investigación y docencia,\ny actualmente también\nen las empresas y organismos públicos. Es la evolución del trabajo de los laboratorios Bell con el\nlenguaje S (Venables Ripley 2002),\nllevado al mundo del software libre por Ross Ihaka y Robert Gentleman en los años 90 (Ihaka Gentleman 1996).\nLa version R 1.0.0 se publicó el 29 de febrero de 2000.Uno de los aspectos más espectaculares de R es la cantidad de paquetes\ndisponibles. Un paquete (package) de R es un componente con funcionalidad adicional que se puede\ninstalar en el sistema para ser utilizado por R. En el momento de\ncompilar este libro, el número de paquetes disponibles en el repositorio oficial\nes de 19724.Una vez conocido el mundo de R, se plantea la siguiente pregunta: ¿y por qué\nutilizar R? Es imposible dar un único motivo. continuación se enumeran\nalgunos de ellos:Es Free Open Source Software (FOSS). Gratis y libre. En inglés se suele decir free free beer, free free speech.Tiene una amplia comunidad de usuarios que proporciona recursos.Es multiplataforma.Se usa cada vez en más empresas e instituciones.Es posible obtener soporte comercial, por ejemplo través de Posit Software PBC3.Se ha alcanzado una masa crítica de usuarios que lo hace confiable.Es extensible (desde pequeñas funciones, hasta paquetes).Se puede implementar la innovación inmediatamente. En software comercial hay que esperar nuevas versiones, en el mejor de los casos.Posee características de “investigación reproducible”. En el Cap. ?? se tratará\nqué implica este enfoque. En contextos distintos la investigación, se puede hablar de informes reproducibles y trazabilidad del análisis.Por otra parte, el uso de R en las empresas está creciendo exponencialmente debido, principalmente, la necesidad de analizar y visualizar datos con herramientas potentes para explotar todo su potencial. Grandes empresas de todos los sectores llevan tiempo utilizándolo, si bien la popularización del software y su conocimiento entre los nuevos titulados está facilitando que empresas de todo tipo y tamaño aprovechen esta herramienta en su estrategia digital. Así, además de la visualización y presentación efectiva de los datos, equipos bien formados pueden descubrir relaciones entre variables clave, realizar predicciones, tomar mejores decisiones o mejorar sus procesos gracias al análisis avanzado de datos más allá de la hoja de cálculo.","code":""},{"path":"ch-110003.html","id":"id_110003-bases","chapter":"Capítulo 3 R para ciencia de datos","heading":"3.2 La sesión de R","text":"R es una aplicación de análisis estadístico\ny representación gráfica de datos, y además un lenguaje de programación.\nR es interactivo, en el sentido de que\nresponde través de un “intérprete” las entradas que recibe\ntravés de la consola.La interfaz de usuario de R (R GUI, Graphical User Interface) cumple las funciones básicas para\ninteractuar con R, pero es muy pobre la hora de trabajar con ella. En su lugar,\nes más conveniente utilizar el entorno de desarrollo RStudio Desktop (o su versión en la nube https://posit.cloud/),\nque es como un\n“envoltorio” del sistema R con más funcionalidades y ayudas, pero\nmanteniendo el mismo nivel de interacción: consola y scripts4. Al igual que R, RStudio es una aplicación de software libre, pero, en este caso, desarrollada y mantenida por la compañía privada Posit PBC.Una cosa muy importante en R es que las expresiones son sensibles mayúsculas,\ny por tanto los objetos datos y Datos son distintos.","code":""},{"path":"ch-110003.html","id":"instalación-de-r","chapter":"Capítulo 3 R para ciencia de datos","heading":"3.3 Instalación de R","text":"Durante todo el libro se utiliza la interfaz RStudio. Pero, como se avanzó anteriormente, RStudio\nes solo un “envoltorio” de R, por lo que previamente hay que tener instalado\nen el ordenador el sistema “base” de R. R está disponible para sistemas\nWindows, MacOS y Linux. Por cuestiones de espacio, se incluyen detalles en este libro, pero la instalación es sencilla siguiendo las instrucciones en sus correspondientes websites:Instalación de R: http://www.r-project.orgInstalación de RStudio: https://posit.coPara completar la instalación de R, se muestra cómo instalar5 los paquetes del tidyverse6 mediante expresiones en la consola o script con la función install.packages():Una vez instalado el paquete, se cargará con la instrucción library(\"nombre_paquete\") en la sesión de R donde se quiera utilizar.veces resulta útil usar directamente\nla función que se va utilizar en vez de cargar todo el paquete. Esto se hace con el operador ::, es decir, nombre_paquete::funcion(). La siguiente expresión serviría para usar\nla función select() del paquete dplyr sin cargar el paquete entero.","code":"\ninstall.packages(pkgs = \"tidyverse\")\nlibrary(\"tidyverse\")\ndplyr::select()"},{"path":"ch-110003.html","id":"id_110003-proyectos","chapter":"Capítulo 3 R para ciencia de datos","heading":"3.4 Trabajar con proyectos de RStudio","text":"La manera más eficiente de trabajar con R, es mediante proyectos\nde RStudio. Esto permite abstraerse de los detalles de la sesión de R\n(espacio de trabajo, directorio de trabajo, environment), ya que al abrir\nun proyecto estará todo preparado para seguir el trabajo donde se dejó,\no empezar de cero si se acaba de crear.\nPara crear un proyecto de RStudio, se despliega el menú de proyectos la derecha en la barra de herramientas y se selecciona “New Project…” También se puede hacer en el menú “File/New Project…”.Es aconsejable crear siempre una estructura\nde carpetas que permita tener todo organizado desde el principio,\nporque al final los proyectos crecen. La estructura perfecta existe,\ny depende del proyecto particular. Las siguientes carpetas pueden ser\nútiles en un amplio abanico de proyectos, y\nlas tres primeras se pueden usar prácticamente en cualquier proyecto:data: en esta carpeta se tienen los archivos de\ndatos, tanto aquellos orígenes de datos que se quieran importar, como los\nque se puedan guardar desde un script.R: para los scripts. Es posible que solamente haya un script\nen nuestro proyecto, pero si hubiera más se pueden guardar en esta carpeta.inform: aquí se pueden guardar los archivos Quarto o R Markdown\nque se utilicen para generar informes o presentaciones.img: si en nuestro proyecto se utilizan imágenes\nde cualquier tipo, es una buena idea tenerlas en una carpeta aparte.test: si se quieren separar los scripts que se utilicen para\npruebas y se quieren mezclar con los “buenos” en la carpeta R.aux, tmp, util, notas, doc, …:\neste tipo de carpetas vienen bien cuando\nhay información que está relacionada o es útil para un proyecto, pero el archivo es\ndel proyecto de análisis de datos en sí. Por ejemplo, unas especificaciones de\nun producto o servicio, un artículo científico, fotografías de una fábrica,\ncomunicaciones con clientes, etc.ejercicios, practicas, …: si nuestro proyecto\nforma parte de una asignatura, curso, o similar.Un aspecto importante cuando se trabaja en proyectos colaborativos es el control de versiones. Este tema se aborda en el Cap. 46.","code":""},{"path":"ch-110003.html","id":"tratamiento-de-datos-con-r","chapter":"Capítulo 3 R para ciencia de datos","heading":"3.5 Tratamiento de datos con R","text":"En este apartado se van empezar utilizar expresiones de R. Las expresionesse escribirn en scripts, que pueden contener “comentarios” (texto que se ejecutará) utilizando el símbolo “almohadilla” (#). Muchas de las expresiones que se usan son llamadas funciones7. La ayuda de cualquier función se puede obtener en la consola usando la expresión ?function, donde function es el nombre de la función u objeto del que se quiere obtener ayuda.","code":""},{"path":"ch-110003.html","id":"id_110003-estructuras","chapter":"Capítulo 3 R para ciencia de datos","heading":"3.5.1 Estructuras y tipos de datos","text":"Las estructuras y tipos de datos más frecuentes con las que se trabaja en R son las que se detallan continuación.Tablas de datos. Son colecciones de variables numéricas y/o atributos\norganizadas en columnas,\nen las que cada fila se corresponde con algún elemento en el que se han\nobservado las características que representan las variables. La forma más común\nes el data.frame. Cada columna del data.frame es, en realidad, otra estructura de datos, en concreto, un vector. Un ejemplo de data.frame es el conjunto de datos tempmin_data del paquete CDR que se analiza en el Cap. 40 y del que se muestran continuación las primeras tres filas con la función head().Un data.frame es un objeto de datos en dos dimensiones, en el que las filas\nson la dimensión 1 y las columnas la dimensión 2. Los datos se pueden “extraer”\nde un data.frame por filas, por columnas o por celdas. Para extraer una\nde las variables del data.frame se suele utilizar el operador $ después del\nnombre del data.frame, y continuación el nombre de la variable.El operador <- asigna al “símbolo” que hay su izquierda el resultado\nde la expresión que hay su derecha, y lo guarda con ese nombre\nen el espacio de trabajo8. Por ejemplo, la siguiente expresión extrae todas las filas de la\ncolumna tmin o, dicho de otra forma, el vector con todas las temperaturas mínimas registradas y lo guarda en el objeto temp_min.\nVectores y matrices. Ya se ha visto que una columna de una tabla de datos es un vector. También se pueden crear vectores con la función c() y los elementos del vector separados por comas. Una matriz es un vector organizado en filas y columnas. modo de ejemplo, la primera de las siguientes expresiones crea un vector llamado nombres con dos cadenas de texto, y la segunda crea una matriz numérica llamada coordenadas partir de las columnas 4 y 5 del conjunto de datos tempmin_data. Nótese que la extracción de valores de un conjunto de datos o de una matriz se puede realizar también por sus índices de filas y columnas entre corchetes separados por una coma. En este caso se extraen todas las filas (pues se especifica ninguna en la dimensión 1) de las columnas 4 y 5.\nFactor. Es un tipo especial de vector para representar variables categóricas (también denominadas atributos o factores). En general, una variable categórica suele tomar un número reducido\nde valores diferentes (categorías), identificados con etiquetas (labels) y que se llaman\nniveles del factor (levels).\nUn ejemplo es el dataset dp_entr del paquete CDR que se analiza en el Cap. 24. La columna ind_pro11 es un indicador que toma los valores S y N, mientras que des_nivel_edu toma tres posibles valores.\nListas. Son estructuras de datos que contienen una\ncolección de elementos indexados que, además, pueden tener un\nnombre.\nPueden ser heterogéneas,\nen el sentido de que cada elemento de la lista puede ser\nde cualquier tipo.modo de ejemplo, se muestran los nombres del objeto tempmax_data del paquete CDR, que contiene 6 elementos de distintas clases.\nFechas. Son un tipo de datos especial que algunas veces provoca problemas\nal compartir datos entre programas. El conjunto de datos tempmin_data contiene la columna fecha, que puede convertirse de manera inmediata tipo fecha (Date) porque viene en un formato estándar (véase la ayuda de strptime para especificar otros formatos). El paquete lubridate del tidyverse contiene funciones para hacer más fácil el trabajo con fechas.\nCadenas de texto. Son estructuras de datos que aparecen en forma de vector de caracteres. La columna indicativo del conjunto de datos tempmin_data es un ejemplo de este tipo de datos. La ayuda de ?regexpr proporciona la información necesaria sobre cómo extraer texto con expresiones regulares, y la de ?paste para aprender unir cadenas de texto. El paquete stringr del tidyverse contiene funciones para facilitar el trabajo con cadenas de texto.","code":"\nlibrary(\"CDR\")\nhead(tempmin_data, 3)\n#>        fecha indicativo tmin  longitud  latitud\n#> 1 2021-01-06      4358X -4.7 -5.880556 38.95556\n#> 2 2021-01-06      4220X -7.0 -4.616389 39.08861\n#> 3 2021-01-06      6106X  4.7 -4.748333 37.02944\ntemp_min <- tempmin_data$tmin\nnombres <- c(\"longitud\", \"latitud\")\ncoordenadas <- as.matrix(tempmin_data[, 4:5])\ndp_entr[1:5, c(1, 17)]\n#>     ind_pro11 des_nivel_edu\n#> 1           S         MEDIO\n#> 497         N         MEDIO\n#> 265         N        BASICO\n#> 534         N         MEDIO\n#> 415         N        BASICO\nlevels(dp_entr$des_nivel_edu)\n#> [1] \"ALTO\"   \"BASICO\" \"MEDIO\"\nnames(tempmax_data)\n#> [1] \"ESP\"             \"ESP_utm\"         \"grd_sf\"          \"grd_sp\"         \n#> [5] \"temp_max_utm_sf\" \"temp_max_utm_sp\"\ntempmin_data$fecha<- as.Date(tempmin_data$fecha)\nclass(tempmin_data$fecha)\n#> [1] \"Date\"\nhead(tempmin_data$indicativo)\n#> [1] \"4358X\" \"4220X\" \"6106X\" \"9698U\" \"4410X\" \"1331A\""},{"path":"ch-110003.html","id":"id_110003-importacion","chapter":"Capítulo 3 R para ciencia de datos","heading":"3.5.2 Importación de datos","text":"\nEn el apartado anterior se han utilizado tablas de datos que están incluidas en un paquete de R. Pero lo habitual es que los datos se tengan que importar de fuentes externas, como ficheros. continuación, se describen algunas de las formas de importar los tipos de ficheros más habituales9.Excel. Sin duda una forma muy popular de organizar los datos en ficheros es\nmediante hojas de cálculo como Microsoft Excel.\nHay varios paquetes con los que se puede trabajar con archivos de Excel. En este libro se utiliza\nel paquete readxl del tidyverse. Con la siguiente expresión se puede descargar un archivo Excel de ejemplo10.Una vez el archivo está en el directorio de trabajo de la sesión de R, se puede importar su contenido al espacio de trabajo con la siguiente expresión:Texto. Los archivos de texto son el formato más utilizado y conveniente para compartir datos. Es también\nmuy común que el equipamiento o el software genere datos en formato de texto. Estos\narchivos suelen tener extensión .csv (comma separated values) o .txt, aunque\npueden tener cualquier otra, o incluso tener extensión. modo de ejemplo, con la siguiente expresión se puede descargar un archivo csv.Si el archivo tiene extensión .csv, como el anterior, vendrá\nya con una especificación muy concreta, pudiéndose usar directamente las funciones\nread.csv() o read.csv2() para tener la tabla de datos en el espacio de trabajo.La función genérica de R para importar datos de texto es read.table(), que puede importar cualquier especificación cambiando los argumentos adecuados.\nPor ejemplo, la siguiente expresión tendría el mismo resultado que se ha\nobtenido con la función read.csv211:Para saber cómo importar datos desde sistemas gestores de bases de datos véase el\nCap. 5.Hay infinidad de otras fuentes de las que se pueden importar datos R. Por ejemplo, el paquete rvest, que forma parte\ndel tidyverse, se puede utilizar para obtener datos de páginas web y otras fuentes de Internet, lo que se suele llamar web scraping. Por ejemplo, supóngase que se quiere importar la tabla con los datos de comunidades y ciudades autónomas españolas del enlace https://www.ine.es/daco/daco42/codmun/cod_ccaa_provincia.htm.\nLas siguientes expresiones importan esta tabla al conjunto de datos ccaa_ine.La ruta o “xpath” se puede obtener usando las herramientas de desarrollo del navegador, y puede que una vez importada la tabla se requiera algún post-procesamiento antes de poder analizar los datos.","code":"\ndownload.file(url = \"http://emilio.lcano.com/b/adr/p/datos/RRHH.xlsx\",\n              destfile = \"data/RRHH.xlsx\",\n              mode = \"wb\")\nrrhh <- readxl::read_excel(\"data/RRHH.xlsx\")\ndownload.file(url = \"http://emilio.lcano.com/b/adr/p/datos/ejDatos.csv\",\n              destfile = \"data/ejDatos.csv\")\nmerma <- read.csv2(\"data/ejDatos.csv\")\nmerma <- read.table(file = \"data/ejDatos.csv\",\n                    header = TRUE,\n                    sep = \";\",\n                    dec = \",\",\n                    fileEncoding = \"utf-8\")\nlibrary(\"rvest\")\nurl <- \"https://www.ine.es/daco/daco42/codmun/cod_ccaa_provincia.htm\"\nccaa_ine <- url |> \n  read_html() |> \n  html_node(xpath = '//*[@id=\"contieneHtml\"]/table') |> \n  html_table(fill = TRUE)"},{"path":"ch-110003.html","id":"exportación-de-datos-y-archivos-de-datos-específicos-de-r","chapter":"Capítulo 3 R para ciencia de datos","heading":"3.5.3 Exportación de datos y archivos de datos específicos de R","text":"En algunos proyectos es necesario guardar algunos datos que se han ido creando o\ntransformando, bien para compartir con otras partes interesadas, bien para\nser utilizados en el mismo u otros proyectos.\nPara exportar los datos Excel, se utiliza la función write.xlsx() del paquete openxlsx (si está instalado, se instala de la forma habitual).\nSi lo que se quiere es exportarlo texto, se pueden utilizar los equivalentes\nlas funciones de importación write.csv(), write.csv2() o write.table().La siguiente expresión exporta la tabla de datos tempmin_data ficheros Excel y csv (formato en inglés).También se pueden guardar los datos en formato “nativo” de R. Los archivos .RData almacenan un espacio de trabajo entero, y por tanto\npueden guardar varios objetos en el mismo archivo. Cuando posteriormente se importe,\nlos objetos estarán en el espacio de trabajo con su nombre original.\nSe guardan con la función save() y se restauran con la función load(), como en el siguiente ejemplo.Los archivos .rds almacenan un único objeto en un archivo. Cuando posteriormente se quieran importar,\nhay que asignar el resultado al nombre que se quiera. Se guardan con la\nfunción writeRDS() y se restauran con la función readRDS(), como en el siguiente ejemplo.El paquete foreign de R base y otros paquetes especializados pueden\nexportar datos otros formatos de archivo, que se tratan\nen detalle en este capítulo.","code":"\nopenxlsx::write.xlsx(x = tempmin_data, \n                     file = \"data/temp_min_Filomena.csv\")\nwrite.csv(x = tempmin_data, file = \"data/temp_min_Filomena.csv\")\nsave(tempmin_data, tempmax_data, \n     file = \"data/datos_temperaturas.RData\")\nload(\"data/datos_temperaturas.RData\") #carga de nuevo el objeto\nsaveRDS(object = tempmin_data, \n        file = \"data/datos_temperaturas.rds\")\nnuevo_objeto <- readRDS(file = \"data/datos_temperaturas.rds\")"},{"path":"ch-110003.html","id":"id_110003-tidyverse","chapter":"Capítulo 3 R para ciencia de datos","heading":"3.6 Organización de datos con el tidyverse","text":"","code":""},{"path":"ch-110003.html","id":"el-tidyverse-y-su-flujo-de-trabajo","chapter":"Capítulo 3 R para ciencia de datos","heading":"3.6.1 El tidyverse y su flujo de trabajo","text":"\nEl tidyverse es, según se define en su propia página web12,\nun conjunto de paquetes de R “opinables” diseñados para ciencia de datos.\nLas principales ventajas (opinables) de utilizar el tidyverse son tres:Utiliza una gramática, estructuras de datos y filosofía de diseño común.El flujo de trabajo es más fluido y, una vez se comprenden las ideas principales, más intuitivo.Para la mayoría de las operaciones, es computacionalmente más eficiente.Uno de los paquetes más populares del tidyverse es ggplot2, que proporciona una “gramática de gráficos”\n(Hadley Wickham 2016a) y es una pieza clave del tidyverse actual, junto con los paquetes dplyr (gramática para la manipulación de datos) y tidyr (herramienta para crear datos tidy).\nEl flujo de trabajo propuesto por el tidyverse se describe en el libro\n“R Data Science” (H. Wickham Grolemund 2016) y se sintetiza en la Fig. 3.1.\nFigura 3.1: Flujo de trabajo en Ciencia de Datos propuesto por el tidyverse (fuente: Wickham, H. Grolemund, G. (2016))\nAdemás del mencionado libro, la web del tidyverse (http://tidyverse.org) contiene\ntoda la documentación de los paquetes, incluidos artículos para tareas concretas,\nque merece la pena leer alguna vez. En la web están también las conocidas como cheatsheets,\nalgunas de ellas disponibles también\nen la ayuda de RStudio (menú\nHelp/Cheatsheets).Dentro del flujo de trabajo de la Fig. 3.1, ya se ha tratado\nla primera etapa (Import) en la Sec. ??. Es importante\nseñalar que, al utilizar las funciones del tidyverse, los datos se organizan\nen objetos de clase tibble, que es una extensión del data.frame de R base.\nLas principales diferencias son:Permite una representación compacta en la consola al mostrar la tabla de datos.La selección con corchetes simples de una única variable siempre devuelve\notro tibble (diferencia de un data.frame, que devuelve un vector).Se puede forzar que una tabla de datos sea de un tipo u otro con las funciones .data.frame (de tibble data.frame) y as_tibble (de data.frame tibble).Siguiendo con el esquema de la Fig. 3.1, en este apartado se verán\nalgunas tareas de las etapas Tidy (organizar) y Transform (transformar), que\nserán ampliadas en los Cap. ?? y 9. La visualización\n(Visualise) se tratará específicamente en el Cap. ?? y transversalmente en muchos otros. La modelización (Model) se trata extensamente en los capítulos de las partes IV IX, y la comunicación (Communicate) se verá\nen los capítulos de la Parte X.\nUna de las características de la forma en que están programados los paquetes del\ntidyverse es que se puede trabajar13 con pipes.El pipe es, básicamente, un operador compuesto de dos caracteres, |>,\nque se puede obtener con el atajo de teclado CTRL+MAYUS+M. El operador\nse pone en medio de dos expresiones de R. Sean lado_izquierdo\ny lado_derecho las expresiones que se ponen izquierda y derecha\ndel pipe. Entonces se utiliza de la siguiente manera:El operador nativo de R, |>, apareció en la versión R-4.1.0. Hay un operador alternativo que proviene del paquete magrittr, %>%, que había que usar antes de esta versión, y mucha literatura y documentación está escrita usándolo. Hay diferencias, pero los efectos de este capítulo ambos operadores se pueden utilizar indistintamente.La expresión lado_izquierdo debe producir un valor, que puede\nser cualquier objeto de R.\nLa expresión lado_derecho debe ser una función, que tomará como primer\nargumento el valor producido en la parte izquierda.\nSi se desea guardar el resultado final, se debe asignar el resultado\nalgún nombre de objeto para que se almacene en el espacio de trabajo.\nLa siguiente expresión sería un ejemplo de uso.La ventaja de usar los pipes es que se pueden encadenar, de forma que el resultado de cada operación pasa la siguiente expresión del pipeline (secuencia de operaciones con pipe), como en el siguiente ejemplo:","code":"lado_izquierdo |> lado_derechonombre_objeto <- lado_izquierdo |>\n  lado_derecho\nlibrary(\"dplyr\")\ncontam_mad |> colnames() |> length()\n#> [1] 12"},{"path":"ch-110003.html","id":"transformación-de-datos-con-dplyr","chapter":"Capítulo 3 R para ciencia de datos","heading":"3.6.2 Transformación de datos con dplyr","text":"En la gramática del tidyverse, dentro del paquete dplyr se dispone de una\nserie de “verbos” (funciones) para una sola tabla, que se pueden\nagrupar en tres categorías: para trabajar con filas, para trabajar con\ncolumnas y para resumir datos.","code":""},{"path":"ch-110003.html","id":"operaciones-con-filas","chapter":"Capítulo 3 R para ciencia de datos","heading":"3.6.2.1 Operaciones con filas","text":"\nLos verbos definidos para estas operaciones son:filter(): elige filas en función de los valores de la columna.\n- arrange(): cambia el orden de las filas con algún criterio.\n- slice(): extrae filas por su índice. También hay una serie de funciones\n“asistentes” (helpers) para obtener los índices que se utilizan con\nfrecuencia. Por ejemplo:slice_head() y slice_tail() obtienen las primeras y últimas filas\nrespectivamente (por defecto, una). Se puede especificar n (número) o prop\n(proporción) de filas.slice_sample() obtiene una muestra aleatoria de n filas (o proporción prop).slice_min(), slice_max() obtienen las filas que contienen los menores o mayores\nvalores respectivamente de la variable indicada en el argumento order_by. Si \nse especifica n o prop, se obtienen sólo las filas que contienen el mínimo o el máximo. Nótese\nque puede haber más de una fila que cumpla la condición.Véase el resultado de los siguientes ejemplos:","code":"\npm10 <- contam_mad |> \n  filter(nom_abv == \"PM10\")   # se filtra por PM10\nzonas<- contam_mad |>\n  arrange(desc(zona), daily_mean)\npm10 |> slice(10:15) # extrae filas desde la 10 a la 15\npm10 |> slice_tail(n = 3) # extrae las tres últimas filas\npm10 |> slice_max(order_by = daily_mean) # día con mayor valor medio de PM10\nset.seed(1) # Para que la muestra aleatoria sea reproducible\npm10 |> slice_sample(n = 4) # muestra 4 registros"},{"path":"ch-110003.html","id":"operaciones-con-columnas","chapter":"Capítulo 3 R para ciencia de datos","heading":"3.6.2.2 Operaciones con columnas","text":"Los verbos definidos para estas operaciones son:\n- select(): indica cuando una columna se incluye o . Se pueden utilizar helpers para seleccionar columnas que cumplan cierta condición (por ejemplo, ser numéricas) y también para “quitar” columnas de la selección (con el signo menos (-)).En cuanto la modificación de datos, existen múltiples posibilidades. Algunas de ellas son:rename(): cambia el nombre de la columna.rename(): cambia el nombre de la columna.mutate(): cambia los valores de las columnas y crea nuevas columnas. La función transmute() funciona igual que mutate(), pero la tabla de datos\nresultante sólo contiene las nuevas columnas creadas.mutate(): cambia los valores de las columnas y crea nuevas columnas. La función transmute() funciona igual que mutate(), pero la tabla de datos\nresultante sólo contiene las nuevas columnas creadas.relocate(): cambia el orden de las columnas.relocate(): cambia el orden de las columnas.En este punto, es importante señalar que dentro de la función mutate()\nse puede usar cualquier función vectorizada para transformar las\nvariables. Por ejemplo, se podría transformar una columna con las funciones\n.xxx que se vieron en la Sec. ??, aplicar formatos\nfechas o usar funciones del paquete lubridate para trabajar con este\ntipo de datos. medida que se avance en el libro irán apareciendo aplicaciones\nque ahora, quizás, sean tan evidentes.","code":"\npm10 |> select(longitud, latitud, daily_mean, tipo)\npm10 |> select(where(is.numeric))\npm10 |> select(-c(id:latitud))\npm10 |> rename(zona_calidad_aire = zona)\npm10 |> relocate(fecha, .before = estaciones)\npm10_na <- pm10 |> mutate(isna = is.na(daily_mean))"},{"path":"ch-110003.html","id":"operaciones-de-resumen-y-agrupación","chapter":"Capítulo 3 R para ciencia de datos","heading":"3.6.2.3 Operaciones de resumen y agrupación","text":"La primera operación de resumen que puede surgir es\n“contar” filas. La función tally() devuelve el número de filas totales\nde un data.frame. La función count() proporciona también este número; si, además, se pasa como argumento alguna variable, lo que devuelve es el número\nde filas para cada valor diferente de dicha/s variable/s. Estos recuentos se\npueden añadir la tabla de datos con las funciones add_count() y add_tally(),\nlo que permite calcular frecuencias absolutas y relativas fácilmente.La función summarise() (o, equivalentemente, summarize()) aplica alguna\nfunción de resumen la/s variable/s que se especifiquen (mean(), max(), etc.).\nEl paquete dplyr tiene algunas funciones de resumen adicionales, como\nn() (número de filas), n_distinct() (número de filas con valores distintos) y\nfirst(), last(), nth() (primero, último y n-ésimo valor, en el orden en el que se encuentran, respectivamente).En muchas ocasiones, las operaciones de análisis se realizan en grupos\ndefinidos por alguna variable de agrupación. La función group_by()\n“prepara” la tabla de datos para realizar operaciones de este tipo.\nUna vez agrupados los datos, se pueden añadir operaciones de\nresumen como las vistas anteriormente. veces hay que “desagrupar” los\ndatos, para lo que se utiliza la función ungroup().continuación, se muestra una expresión un poco más compleja que las anteriores.\nEn el conjunto de datos contam_mad del paquete CDR, se filtra por el\nnombre de contaminante “NOx”. Después se agrupan los datos por zona y se\ncalculan algunos estadísticos resumen para cada zona.","code":"\npm10 |> tally()\n#>       n\n#> 1 53794\npm10 |> count(zona)\n#>            zona     n\n#> 1: Interior M30 20690\n#> 2:      Noreste 12414\n#> 3:     Noroeste  4138\n#> 4:      Sureste  8276\n#> 5:     Suroeste  8276contam_mad |>  \n  filter(nom_abv == \"NOx\") |> # se filtra por N0x\n  group_by(zona) |>\n  summarize(\n    min = min(daily_mean, na.rm = TRUE),\n    q1 = quantile(daily_mean, 0.25, na.rm = TRUE),\n    median = median(daily_mean, na.rm = TRUE),\n    mean = mean(daily_mean, na.rm = TRUE),\n    q3 = quantile(daily_mean, 0.75, na.rm = TRUE),\n    max = max(daily_mean, na.rm = TRUE)\n  )\n#> A tibble: 5 × 7\n    zona            min    q1 median  mean    q3   max\n    <chr>         <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl>\n#> 1 Interior M30 0.0833  32.4   54.1  72.9  90.0  759.\n#> 2 Noreste      1       23.8   39.6  56.2  68.9  516.\n#> 3 Noroeste     0       12.0   20.3  29.7  34.5  352.\n#> 4 Sureste      0       29.1   45.4  64.6  77.2  453 \n#> 5 Suroeste     0.667   33.5   59.6  90.5 114.   666."},{"path":"ch-110003.html","id":"combinación-de-datos","chapter":"Capítulo 3 R para ciencia de datos","heading":"3.6.3 Combinación de datos","text":"En el apartado anterior se han tratado los “verbos” de una tabla.\nEs muy común que haya que combinar datos de distintas\ntablas, para lo cual se utilizan lo que el tidyverse\nconsidera two tables verbs. En esencia, para combinar\ntablas que contienen información relacionada, hay que\nsaber cuáles son las columnas que se refieren lo mismo,\npara hacer las uniones (joins) utilizando esas\ncolumnas. Hay cuatro tipos de uniones que se pueden realizar,\nusando las siguientes funciones:inner_join(): se incluyen las filas de ambas tablas para las que coinciden las variables de unión.left_join(): se incluyen todas las filas de la primer tabla y sólo las de la segunda donde hay coincidencias.right_join(): se incluyen todas las filas de la segunda tabla y sólo las de la primera donde hay coincidencias.full_join(): se incluyen todas las filas de las dos tablas.Las funciones requieren como argumentos dos tablas de datos y\nla especificación de las columnas coincidentes. Si se especifica,\nhace las uniones por todas las columnas coincidentes en ambas tablas. Para\nlas filas que sólo están en una de las tablas, se añaden valores NA\ndonde haya coincidencias.modo de ejemplo, las siguientes expresiones unen dos datasets para combinar datos de municipios con su renta. En el Cap. ?? se verán estas uniones en la práctica.Otra forma de unir tablas es, simplemente, añadiendo columnas (que tengan el mismo\nnúmero de filas) o filas (que tengan el mismo número de columnas). Para ello se usan\nlas funciones bind_cols() y bind_rows(), respectivamente.\nOtra forma conveniente de añadir nuevas filas o columnas son las funciones\nadd_row() y add_column(). Se pueden añadir antes o después de una fila/columna\nespecificada con el argumento ., y pasando los valores como pares “variable = valor” para cada variable en el conjunto de datos.Como comentario final del paquete dplyr, una característica importante es que\nse pueden usar las funciones vistas sobre\ntablas de una base de datos, sin necesidad de utilizar sentencias SQL y con\nla ventaja de que las operaciones se realizan en el motor de la base de datos.\nEn el Cap. 5 se tratarán las cuestiones relacionadas con los gestores de bases de datos y SQL.","code":"\nlibrary(\"sf\")\nmunis_renta <- municipios |>\n  left_join(renta_municipio_data) |> \n  select(name, cpro, cmun, `2019`) \n#> Joining, by = \"codigo_ine\""},{"path":"ch-110003.html","id":"reorganización-de-datos","chapter":"Capítulo 3 R para ciencia de datos","heading":"3.6.4 Reorganización de datos","text":"\nlo largo del capítulo se ha visto la importancia de disponer los\ndatos de forma rectangular, de forma que se tenga una columna para\ncada variable y una fila para cada observación. Algunas veces es\nconveniente reorganizar los datos más “lo ancho” o más\n“lo largo” de lo que se encuentran.Para estas operaciones se utilizan las funciones pivot_longer() y\npivot_wider() del paquete tidyr del tidyverse de la siguiente forma:pivot_longer(): el argumento names_to asigna el nombre\nde la nueva variable que va indicar de qué columna vienen los datos; y el\nargumento values_to asigna el nombre de la nueva variable que va contener\nel valor de la tabla original.pivot_longer(): el argumento names_to asigna el nombre\nde la nueva variable que va indicar de qué columna vienen los datos; y el\nargumento values_to asigna el nombre de la nueva variable que va contener\nel valor de la tabla original.pivot_wider(): el argumento names_from indica el nombre\nde la variable que contiene los nombres de las nuevas columnas crear lo ancho; y el argumento values_from indica el nombre de la variable que contiene los\nvalores en la tabla original. Las observaciones deben estar identificadas de forma\núnica por varias variables. Si es el caso, se puede aplicar una función\nal estilo de las tablas dinámicas de las hojas de cálculo con el argumento values_fn.pivot_wider(): el argumento names_from indica el nombre\nde la variable que contiene los nombres de las nuevas columnas crear lo ancho; y el argumento values_from indica el nombre de la variable que contiene los\nvalores en la tabla original. Las observaciones deben estar identificadas de forma\núnica por varias variables. Si es el caso, se puede aplicar una función\nal estilo de las tablas dinámicas de las hojas de cálculo con el argumento values_fn.Las funciones pivot_longer() y pivot_wider() admiten otros argumentos\nnames_xx y values_xx para personalizar la forma de reestructurar los datos.\nEn la mayoría de las\nocasiones será suficiente con las comentadas (xx_from y xx_to). Si fuera necesario, se recomienda consultar la ayuda\nde las funciones, o la lectura del artículo sobre pivoting.modo de ejemplo, el conjunto de datos contam_mad tiene los datos “mezclados” de varias variables medioambientales en la columna daily_mean. La columna nom_abv contiene el parámetro al que se refiere la columna de datos. Entonces, interesa “extender” la tabla para tener cada parámetro en una columna, de forma que se pueda hacer un análisis de datos adecuado, como en el siguiente código:Se deja como ejercicio volver obtener la tabla original usando la función pivot_longer() partir del objeto extendida.El paquete tidyr también contiene funciones para reorganizar las columnas\nde la tabla uniendo columnas con la función unite(), o separando una columna\nen dos o más con la función separate() (véanse los detalles en la ayuda de las funciones).\nPara terminar este apartado de reorganización de datos, se da una\nprimera aproximación al tratamiento de valores perdidos, que se tratará en el\nCap. ??. En R, un valor\nperdido se representa por el valor especial NA (available). Brevemente,\nlas funciones más utilizadas en este campo son:drop_na() del paquete tidyr: permite\neliminar las filas que tienen valores perdidos en ciertas variables\n(o en cualquiera, si se especifica ninguna).drop_na() del paquete tidyr: permite\neliminar las filas que tienen valores perdidos en ciertas variables\n(o en cualquiera, si se especifica ninguna).replace_na(): sustituye los valores perdidos en cada variable por el valor\nespecificado.replace_na(): sustituye los valores perdidos en cada variable por el valor\nespecificado.fill(): permite “rellenar” valores perdidos con los últimos encontrados.fill(): permite “rellenar” valores perdidos con los últimos encontrados.Los datos de contaminación menudo tienen muchos valores perdidos. La siguiente expresión elimina las filas del conjunto de datos contam_mad con valores perdidos y, después, cuenta las filas.","code":"\nlibrary(\"tidyr\")\nextendida <- contam_mad |>\n  pivot_wider(names_from = \"nom_abv\",\n              values_from = \"daily_mean\",\n              values_fn = mean)\ncolnames(extendida)\n#>  [1] \"estaciones\" \"id\"         \"id_name\"    \"longitud\"  \n#>  [5] \"latitud\"    \"nom_mag\"    \"ud_med\"     \"fecha\"     \n#>  [9] \"zona\"       \"tipo\"       \"BEN\"        \"SO2\"       \n#> [13] \"NO2\"        \"EBE\"        \"CO\"         \"NO\"        \n#> [17] \"PM10\"       \"PM2.5\"      \"TOL\"        \"NOx\"\ncontam_mad |>  \n  drop_na() |> # se omiten los NAs para el análisis\n  count()\n#>         n\n#> 1: 505773"},{"path":"ch-110003.html","id":"resumen-2","chapter":"Capítulo 3 R para ciencia de datos","heading":"Resumen","text":"R es software libre y gratuito, mantenido por una enorme comunidad.La forma de interactuar con R es mediante expresiones, que se escriben en scripts, y al ejecutarlas se obtienen los resultados.Los objetos de datos que se vayan usar deben estar en el espacio de trabajo.RStudio es un “envoltorio” de R, y por tanto R tiene que estar instalado\nen el sistema para poder usar RStudio.Los paquetes se instalan una sola vez, y deben cargarse con library() para usar sus funciones.La tabla de datos o data.frame es la estructura de datos más adecuada para análisis de datos y cada columna es un vector.El tidyverse es un conjunto de paquetes que facilita las tareas de análisis de datos.El operador pipe, |>, permite “pasar” valores funciones de forma encadenada.Las operaciones básicas con una tabla son filtrado, selección y resumen.Para crear nuevas columnas en las tablas de datos se usa la función mutate.Para combinar tablas con columnas comunes se usan las funciones xx_join.","code":""},{"path":"cap-etica.html","id":"cap-etica","chapter":"Capítulo 4 Ética en la ciencia de datos","heading":"Capítulo 4 Ética en la ciencia de datos","text":"Mónica Villas\\(^{}\\) y Bilal Laouah\\(^{b}\\)\\(^{}\\)OdiseIA\n\\(^{b}\\)Alexandria Business Solutions Based Data","code":""},{"path":"cap-etica.html","id":"qué-es-la-ética","chapter":"Capítulo 4 Ética en la ciencia de datos","heading":"4.1 ¿Qué es la ética?","text":"La ética es una subdisciplina de la Filosofía que estudia de manera\nsistemática el comportamiento humano desde las nociones del bien y del\nmal y en relación con la moral. Ambas disciplinas están muy relacionadas,\npero son diferentes. La moral tiene un carácter normativo y\nprescriptivo: orienta las acciones de acuerdo con algún marco de valores\nespecífico (costumbres, creencias, códigos tradicionales, normas \nescritas, etc.). Por el contrario, la ética está por encima de cualquier\norientación particular, es decir, se basa en ningún código de\nmandatos y prohibiciones concreto, sino que pretende establecer los\nprincipios partir de los cuales evaluar las acciones y decisiones. La\nética es Filosofía práctica, por eso transmite juicios, sino que\nenseña juzgar.Las dos preguntas fundamentales que ha tratado de responder la ética \nlo largo del tiempo son: ¿qué debemos hacer? y ¿qué es valioso en la\nvida? La primera pregunta promueve el razonamiento ético, mientras que\nla segunda sirve para establecer el marco de valores desde el cual juzga\ny razona el sujeto ético. Desde una perspectiva histórica, Aristóteles\nes considerado el primer autor occidental en haber sistematizado la\nética. Su tratado es comúnmente conocido bajo el título de Ética \nNicómaco. Este capítulo se centra en la ética aplicada, que es la\nutilización de la ética en la práctica. Algunos ejemplos de ética\naplicada son la ética profesional (o deontología), la bioética o la ética\nmedioambiental. La ética aplicada la ciencia de datos hace referencia\nla reflexión que debe acompañar la toma de decisiones en el contexto\nde la praxis profesional de los científicos de datos; se puede\nconsiderar, por tanto, una concreción de la ética profesional. Y es que\nlos científicos de datos tienen que tomar decisiones lo largo del\nciclo de vida de un proyecto de datos que pueden tener consecuencias\nsobre las personas. Algunos procesos que pueden ser fuente de dilemas\néticos son: la recopilación de datos, su transformación, la definición\nde objetivos que se persiguen, el uso de algoritmos y la explicación de los\nresultados. Estos pasos se pueden ver de manera detallada en el\nCap.3. En todos estos pasos, el científico de datos\ndebe usar su pensamiento crítico y tomar decisiones éticas. Así, por\nejemplo, si el propósito es automatizar algún proceso, deberá\nreflexionar y anticipar los posibles impactos negativos que pueden\nderivarse ya que, si este proceso se realiza adecuadamente, la toma\nde decisiones automáticas puede perpetuar algunos de los problemas\néticos como son los sesgos. Para ser más específicos: supóngase que se\nquieren automatizar las contrataciones laborales. Para ello, el\ncientífico de datos necesita desarrollar un algoritmo que seleccione \nlos mejores profesionales para su compañía. Pues bien, algunas\ncuestiones que debe valorar son las siguientes: primero tiene que\nentender qué significa “los mejores profesionales” y definir los\natributos que los representan. Después, tiene que buscar datos\nhistóricos de la compañía, recopilar éstos y estar seguro de que esos\ndatos cumplen con la normativa de privacidad establecida, especialmente\nsi la compañía reside en Europa. Aquí, el científico de datos debe\npensar en temas como la procedencia de los datos, ¿cuál es la fuente?,\n¿quién pertenecen los datos?, ¿están los datos anonimizados para que\nse pueda identificar una persona de manera unívoca?, y algunas\npreguntas similares referidas la privacidad. Seguidamente, tendrá que\nasegurarse de que se tiene una muestra de casos cuyos atributos\n(edad, profesión, experiencia, raza, género, procedencia geográfica,\netc.) tienen sesgos; es decir, que, por\nejemplo, el porcentaje de personas de una determinada raza, o\nedad, o sexo, etc. es significativamente distinto del que hay en la población de la cual se tomó la muestra. En\ndefinitiva, debe asegurarse de que la muestra que tiene es\nsuficientemente representativa de la población con la que va trabajar\ny, si es así, tenerlo en cuenta la hora de analizar y comunicar los\nresultados. Además, ha de tener cuidado con los datos personales,\ncomo género, edad, raza, etc., dado que, en algunos casos de uso, la\ntoma de decisiones debería tener en cuenta estos atributos porque\npodrían inducir prácticas discriminantes, alejadas de los estándares\néticos. Como se puede ver en este sencillo ejemplo, el científico de\ndatos tiene que tomar decisiones, sólo técnicas, que influyen en el\nresultado de su trabajo y que pueden afectar otras personas.\nGeneralmente, los científicos de datos suelen ser profesionales que\nprovienen del mundo técnico, de carreras tecnológicas o relacionadas con\nlas Matemáticas y, diferencia de otros itinerarios de corte humanista,\nla presencia de la ética es menos frecuente, razón por la cual conviene\nfomentar la sensibilización respecto estas cuestiones.Mientras que para los profesionales de la salud existen códigos\ndeontólogicos bien establecidos y organismos que regulan la práctica de\nacuerdo los mejores estándares comportamentales, existe una guía\ncomún para el científico de datos en que se describa cómo debe\ncomportarse. pesar de todo, la guía de buenas prácticas que publica la\nasociación ACM (Association Computing Machinery) puede servir de\ninspiración, si bien, al tratarse de meras recomendaciones, sigue siendo\ninsuficiente para orientar éticamente el comportamiento de éstos.\n","code":""},{"path":"cap-etica.html","id":"los-principios-éticos","chapter":"Capítulo 4 Ética en la ciencia de datos","heading":"4.2 Los principios éticos","text":"Un principio es ni más ni menos que aquello que permite preservar\nlos derechos y libertades de las personas, sin frenar la innovación\ntecnológica (Olmeda Ibánez 2022). La mayoría de los principios se pueden\nagrupar en cuatro grandes categorías: autonomía, justicia, evitar\ndaños y generar beneficios. Algunos ejemplos de principios que se\npueden clasificar en alguna de estas categorías son: transparencia,\nexplicabilidad, privacidad, accesibilidad o equidad. Aunque hay aún\nun acuerdo nivel mundial sobre cuáles deberían ser los principios\nclaves de la inteligencia artificial (IA), sí que se están desarrollando proyectos supranacionales\ncomo el de la UNESCO14, que ha sido firmado recientemente por todos\nsus miembros.\nDesde principios de 2010, el crecimiento de la ciencia de datos ha sido\nexponencial y ha comenzado usarse en todas las industrias de manera\nsistemática, entre otras cosas gracias al Big Data. Actualmente, se\ndispone de más datos que nunca y sólo se analiza un 5% de ellos. Además,\nse han producido enormes mejoras en la computación con el surgimiento de\nnuevos procesadores y también han ocurrido grandes cambios en el área de\nla algoritmia, teniendo disponibles muchos más algoritmos que nunca, lo\nque facilita su reutilización. Por ello, la demanda de científicos de\ndatos que conviertan dichos datos en información clave\npara las empresas ha crecido enormemente en los últimos años.Asimismo, desde 2016, distintos organismos, asociaciones, empresas y\ngobiernos han publicado numerosos documentos, donde se resalta la\nimportancia de la necesidad de principios éticos para la ciencia de\ndatos. Google, IBM y Amazon, en el ámbito de las empresas privadas,\npublicaron sus principios éticos en el 2018. También son muy conocidos\nlos principios de Asilomar de 2016 o la declaración de Toronto de 2017.\nLa mayoría de estos documentos están desarrollados por perfiles\nmultidisciplinares: científicos de datos, abogados o expertos en ética,\nque resaltan la importancia de tener en cuenta los principios éticos en\nla toma de decisiones automáticas cuando se utiliza la ciencia de datos.En definitiva, las cuestiones éticas se están incorporando poco poco\nen los proyectos de ciencia de datos en todo el mundo, siendo la\nregulación europea publicada en abril de 2021 un ejemplo seguir. Esta\nregulación, diseñada lo largo de tres años, parte de un primer\ndocumento en 201815 que fue liderado por un grupo de expertos de\ntodos los países miembros: HLEGAI (High Level Expert Group Artificial\nIntelligence). partir de este primer documento se publicaron otros incluyendo los\ncomentarios y mejoras sugeridas por la sociedad civil, instituciones\npúblicas, empresas e instituciones académicas, hasta, finalmente, publicarse, en abril de 2021, el\nactual documento de regulación de IA.En el actual documento regulatorio, se eligió un enfoque basado en riesgos:Riesgo inaceptable, como el uso de aplicaciones de social\nscoring o de imágenes para procesos de\nadministración de justicia.Riesgo inaceptable, como el uso de aplicaciones de social\nscoring o de imágenes para procesos de\nadministración de justicia.Riesgo alto, como el uso de aplicaciones de contratación o\nmédicas, que deberán ser supervisadas por organismos designados antes\nde su publicación.Riesgo alto, como el uso de aplicaciones de contratación o\nmédicas, que deberán ser supervisadas por organismos designados antes\nde su publicación.Riesgo medio, como el uso de aplicaciones en las que hay que incluir las\nexplicaciones necesarias (dependiendo del tipo de algoritmo de que se trate) para que el usuario pueda entender el proceso de toma de decisiones.Riesgo medio, como el uso de aplicaciones en las que hay que incluir las\nexplicaciones necesarias (dependiendo del tipo de algoritmo de que se trate) para que el usuario pueda entender el proceso de toma de decisiones.Riesgo bajo, para cualquier otro tipo de aplicación.Riesgo bajo, para cualquier otro tipo de aplicación.En el resto del mundo, el progreso en este tipo de regulación está\nsiendo algo más lento, aunque países como Estados Unidos, que hasta\nahora habían puesto el foco en este tipo de regulaciones, están\nempezando trabajar en ello desde finales de 2021. Por otro lado,\nChina, conocida mundialmente por su falta de respeto la privacidad,\nestá empezando dar algún paso en esta área y comenzando cambiar su\npolítica en este sentido. Como ejemplo, en marzo de 2022, ha lanzado\nuna regulación en la que las empresas tienen que informar mejor los\nusuarios sobre sus algoritmos de recomendación. En definitiva, parece\nque la necesidad de la ética para proyectos de ciencia de datos está\navanzando poco poco en todas las geografías, y Europa es, por el\nmomento, un ejemplo seguir.Llegados este punto, conviene distinguir entre regulaciones legales y\nprincipios éticos. Generalmente, las regulaciones legales son\ncoercitivas y su incumplimiento puede tener consecuencias punitivas para\nquienes las ratifican e implementan. Estos principios legales, para\nque sean legítimos, deben fundamentarse e inspirarse en ciertos valores\néticos. Ahora bien, es imposible e indeseable regular legalmente todos\nlos aspectos del comportamiento humano, de ahí la necesidad de compartir\nun marco de valores. La ética permitirá al científico de datos\nconsiderar cuál es la mejor decisión cuando exista un vacío legal. El\nrazonamiento ético implica, pues, asumir la responsabilidad de pensar de\nmanera autónoma.Ahora bien, dado que hay un acuerdo nivel mundial sobre cuáles son\nlos principios éticos más importantes para la ciencia de datos, en este capítulo se han\nseleccionado la equidad y la explicabilidad, por estar entre los que más deben tener en cuenta los\nexpertos en ciencia de datos. Además, son dos de los principios en los\nque se centra la regulación europea.","code":""},{"path":"cap-etica.html","id":"equidad-la-importancia-de-los-sesgos","chapter":"Capítulo 4 Ética en la ciencia de datos","heading":"4.3 Equidad: la importancia de los sesgos","text":"El sesgo se puede definir como el resultado de dar un peso\ndesproporcionado favor o en contra de una persona o cosa en\ncomparación con otra, y normalmente de manera injusta. El término\n‘equidad’, se utiliza precisamente para tratar de que las decisiones \nestén afectadas por esos sesgos. Si se analiza la literatura al respecto,\nse pueden encontrar multitud de tipos de sesgos. En la ciencia de datos\ncuando se habla de sesgo, generalmente se hace referencia los sesgos\nalgorítmicos. Éstos, según la RAE, son “errores sistemáticos en los\nque se puede incurrir cuando, al hacer muestreos o ensayos, se\nseleccionan o favorecen unas respuestas frente otras”.Este sesgo algorítmico puede darse en cualquiera de los pasos que\nlleva cabo el científico de datos (véase Cap.\n2. En la Fig. 4.116, donde se\nrepresentan los distintos pasos la hora de diseñar un algoritmo de\nciencia de datos, se puede ver cuáles son los momentos críticos en los que,\nsin percibirlo, se puede caer en este tipo de sesgo. En primer lugar, un sesgo\nen la adquisición de los datos, partiendo de muestras que ya lo\ntengan. En este punto se encuadran, por ejemplo, los sesgos\nhistóricos o los sesgos de representación. También haber sesgos de medida, que son los sesgos algorítmicos derivados de la selección de las características que se eligen\npara la construcción del modelo. Además, se pueden presentar sesgos en el momento\ndel despliegue, denominados sesgos de implementación, que suceden\ncuando el contexto en el que se despliega el algoritmo es diferente del\ncontexto en que se entrenó.\nEl estudio detallado de estos sesgos algorítmicos está enfocado evitar\nque se aumenten o perpetúen sesgos de cualquier tipo, teniendo en cuenta\nque los algoritmos tienen como objetivo automatizar y generalizar. Como\nse veía en la sección anterior, mucha de la regulación que se está\ndesarrollando en Europa va enfocada mantener el principio de\nequidad, es decir, tratar de evitar los sesgos en la toma de\ndecisiones automáticas realizadas por los algoritmos que se diseñan\ngracias la ciencia de datos.\nFigura 4.1: Sesgos en el proceso de machine learning. Fuente: Adaptada de IBM\nUn ejemplo, que muestra la importancia de los sesgos históricos y de representación, es COMPAS (Correctional Offender\nManagement Profiling Alternative Sanction), una aplicación que da\nsoporte al sistema de justicia americana y que utiliza un algoritmo para evaluar el riesgo potencial de reincidencia de una persona que va ser juzgada.COMPAS evalua, para cada acusado, dos tipos de riesgo:\nde reincidir y de reincidir con violencia. El algoritmo que utiliza califica del 1 10 la posibilidad de que el acusado vuelva cometer un delito (sin y con violencia). De 1 4, el riesgo se califica de bajo; de 5 7, medio; y de 8 10,\nalto. Si la persona puede ser reincidente espera que ocurra el juicio\nen la cárcel, y, en caso contrario, tiene que ir la cárcel hasta que se celebre\nel juicio. Diversos estudios y organizaciones analizaron los datos y \nparecía que hubiera ningún problema de sesgo inicialmente. Sin embargo,\nla organización PROPUBLICA, con datos de 7300 personas correspondientes 2013 y 2014, demostró que la aplicación estaba sesgada. En concreto,\ndemostró que los acusados negros tenían muchas más probabilidades que los acusados blancos de ser clasificados, incorrectamente, como de riesgo de riesgo de reincidencia elevado, mientras que los acusados blancos tenían más probabilidades que los acusados negros de ser marcados incorrectamente como de riesgo bajo.El proceso que se siguió fue el siguiente:Partiendo del proceso de asignación de un riesgo, se construyó\nel historial delictivo del acusado.Para determinar la raza, se usó la clasificación establecida, de\nnegros, blancos, hispanos y asiáticos.Se revisó la definición de reincidencia y cómo se establecían\nlos riesgos en la aplicación de COMPAS.ünicamente se analizaron los riesgos para “reincidencia” y\n“reincidencia con violencia”.Se analizaron los índices de reincidencia y de reincidencia con violencia en\ndos años, así como su distribución por raza.Para contrastar la hipótesis de disparidad entre razas en el índice de riesgo, se\nutilizó una regresión logística que consideraba la raza, la edad,\nla historia criminal, la reincidencia futura, el grado de los cargos\ny el género.Para evaluar la exactitud del algoritmo se usó una regresión de Cox.Se utilizó una muestra de unos 7.300 acusados (de los que se tenía\ndatos de 2 años) para analizar la tasa de falsos positivos y falsos\nnegativos.El modelo logístico concluyó que el factor más predictivo de una puntuación de riesgo de reincidencia más alta era la edad. Los acusados menores de 25 años tenían 2,5 veces más probabilidades de obtener una puntuación más alta que los delincuentes de mediana edad, incluso cuando en el modelo se incluía como variable de control el número de los delitos anteriores, la delincuencia futura, la raza y el género. La raza también se consideró muy predictiva de una puntuación más alta. Si bien los acusados negros tenían tasas de reincidencia más altas en general, cuando se ajustaron por esta diferencia y otros factores, tenían un 45%o más de probabilidades de obtener una puntuación más alta que los blancos. En cuanto al sexo, las mujeres tenían un 19,4% más de probabilidades de obtener una puntuación más alta que los hombres, controlando los mismos factores. Esta conclusión resulta, cuando menos, sorprendente, dados que los niveles de criminalidad de las mujeres eran, en general, más bajos que los de los hombres.La herramienta predecía bien el riesgo de reincidencia en el 60% de los casos estudiados,\npero sólo en el 20% de ellos cuando se trataba del\nriesgo de reincidir con violenta. La Tabla 4.1 resume las principales conclusiones obtenidas en el estudio de PROPUBLICA.Tabla 4.1:  Principales conclusiones del estudio de PROPUBLICARaza negra: 45%Raza caucásica: 23%28% los de raza blanca48% los de raza negra< 25 años tenía 2.5\nveces más de\nprobabilidad de ser\nasignado un riesgo alto45% si eran de\nraza negraCasi un 20% si la\npersona era mujerEn este caso, el problema del sesgo tiene como consecuencia que personas\nque reincidirían permanezcan en la cárcel al asignarseles un índice de\nreincidencia más alto que el que realimente les corresponde, y que personas que sí podrían\nreincidir quedarían en libertad por asignarseles un índice más bajo del que realmente tienen.Hay multitud de ejemplos publicados respecto al tema de los sesgos. Una\nde las mejores referencias es O’neil (2016), que recopila una gran\nvariedad de casos en la que los sesgos pueden llevar toma de\ndecisiones erróneas y equitativas.","code":""},{"path":"cap-etica.html","id":"es-necesaria-la-explicabilidad","chapter":"Capítulo 4 Ética en la ciencia de datos","heading":"4.4 ¿Es necesaria la explicabilidad?","text":"La explicabilidad es otro de los principios clave de de la propuesta\neuropea de IA confiable y, sin duda, va ser clave en los próximos años\nen cuanto la regulación europea de IA entre en vigor.XAI (Explainable AI) es un término que acuñó DARPA (Defense\nAdvanced Research Project Agency) en el año 2017 y que agrupa dentro del término ‘explicabilidad’ sólo el concepto de interpretabilidad para los\nalgoritmos de machine learning sino también los aspectos de la Psicología que\nestán relacionados con proporcionar explicaciones, como se puede ver en\nla Fig. (4.2). se trata únicamente de entender la toma de\ndecisión del algoritmo, sino también de dar una expliación adecuada de por qué se toma dicha decisión, en función del tipo de usuario. Si se considera, por ejemplo,\nde un algoritmo que selecciona imágenes cuando contienen un posible\ntumor, serán las mismas explicaciones las que necesitará un\ncientífico de datos que un médico. Para el científico de datos será\nmucho más útil revisar las métricas propias del algoritmo (exactitud,\nprecisión, sensibilidad, etc.) y, además, saber cuáles de los atributos\nde entrada del algoritmo han tenido más peso en la decisión. En cambio,\nal médico lo que le interesará será una explicación menos técnica, más\ncualitativa, en la que se le explique con detalle, por ejemplo, por qué\nse seleccionó esa imagen frente otras, mencionando el tamaño, la\nforma o características de la imagen, aspectos con los que están familiarizados los profesionales médicos.\nFigura 4.2: Explicabilidad según DARPA\n\nLos algoritmos pueden clasificarse en algoritmos de caja blanca o\ntransparente (aquellos que son fácilmente interpretables) y\nopacos o de caja negra (los que son\ninterpretables y que requieren de herramientas adicionales para su\ninterpretación). Normalmente, se tiene que establecer un equilibrio\nentre la interpretabilidad y la exactitud, dado que son métricas que\nmantienen una relación inversa (véase Fig. 4.3. mayor exactitud, menor\ninterpretabilidad, y viceversa. Los algoritmos más interpretables\nson normalmente los más sencillos, como los algoritmos de\nclasificación, regresión lineal o los árboles de decisión. Otros, como\nlos modelos de random forest, XGboost o algoritmos de deep learning,\nson mucho más exactos pero tan interpretables, lo cual puede\nllevar ciertos problemas la hora de usarlos en la toma de\ndecisiones en las compañías, dado que es más difícil explicar el por qué de la\ndecisión. Cuando las decisiones afectan áreas clave para\nlas personas (decisiones médicas, de contratación, de concesión de\npréstamos, etc.,) es cuando es más relevante es proporcionar la\nexplicabilidad adecuada.\nFigura 4.3: Interpretabilidad vs. exactitud\nSe está avanzando muy rápido en la interpretabilidad de los algoritmos,\ny desde 2017 se proporcionan distintas técnicas y herramientas que\nayudan ello, como, por ejemplo, las librerías SHAP (SHapley Additive exPlanation) o LIME (Local Interpretable Model-agnostic Explanations), de código abierto. En la\nmayoría de las ocasiones, se trata de utilizar algoritmos más sencillos\nque ayuden explicar otros más complejos como redes neuronales o\nXGboost.Hay muchas taxonomías diferentes para la clasificación de los distintos\ntipos de algoritmos. Una de las más utilizadas clasifica los algoritmos\ncomo sigue:Metodologías globales o locales: cuando el método utiliza una\ninstancia para la interpretabilidad se denomina local y cuando éste\nusa todo el modelo se denomina global.Metodologías globales o locales: cuando el método utiliza una\ninstancia para la interpretabilidad se denomina local y cuando éste\nusa todo el modelo se denomina global.Metodologías intrínsecas o post-hoc: ‘intrínseca’ se refiere \ncuando el método es interpretable por si mismo y post-hoc cuando es\nnecesario usar otros algoritmos más sencillos para explicar los más\ncomplejos.Metodologías intrínsecas o post-hoc: ‘intrínseca’ se refiere \ncuando el método es interpretable por si mismo y post-hoc cuando es\nnecesario usar otros algoritmos más sencillos para explicar los más\ncomplejos.Metologías ligadas al modelo o agnósticas del modelo: las\nmetodologías ligadas al modelo son aquellas que se usan para un tipo\nde algoritmo concreto, mientras que las metodologías agnósticas\npermiten trabajar con cualquier tipo de modelo.Metologías ligadas al modelo o agnósticas del modelo: las\nmetodologías ligadas al modelo son aquellas que se usan para un tipo\nde algoritmo concreto, mientras que las metodologías agnósticas\npermiten trabajar con cualquier tipo de modelo.Es importante elegir la técnica más adecuada dependiendo del tipo de\nmodelo interpretar, así como poder combinarlas en aras de conseguir\nuna mejor interpretabilidad. Uno de los mejores libros al respecto que\nrecopila multitud de estas técnicas es Molnar (2020).","code":""},{"path":"cap-etica.html","id":"recursos-en-r-para-trabajar-en-sesgos-y-explicabilidad","chapter":"Capítulo 4 Ética en la ciencia de datos","heading":"4.5 Recursos en R para trabajar en sesgos y explicabilidad","text":"Para un científico de datos es muy relevante conocer las herramientas, tanto\nopen source como comerciales, disponibles para ser usadas en labores de\nanálisis de sesgos o explicabilidad. Todas las herramientas en esta área\nson relativamente recientes. Han ido surgiendo desde 2018 y siguen\nevolucionando rápidamente.En el caso de las herramientas para detectar sesgos, los proveedores\nque empiezan incluir estos análisis son Microsoft, IBM, Google,\nAequitas, Pymetric y Linkedin, siendo el resto open source. La mayoría de\nellas están abiertas contribuciones externas y todas ellas utilizan\nmecanismos para la detección de sesgos, aunque únicamente la de Microsoft\ne IBM incluyen algoritmos para su mitigación.En lo relativo las herramientas sobre explicabilidad, los proveedores más\nrelevantes son Google, IBM, Oracle y H20.ai; el resto, son open\nsource. La mayoría de ellas se pueden usar con algoritmos de caja\nblanca o negra. Respecto los tipos de explicaciones para\ndistintos usuarios, sólo la herramienta de IBM incluye esta\nfuncionalidad. Se puede resaltar también la facilidad con la que H20.ai\npermite elegir el nivel de exactitud y explicabilidad en el momento del\ndiseño del algoritmo. Para un mayor detalle se pueden consultar dos\ntablas comparativas sobre herramientas de explicabilidad y de sesgos que\nse incluyen en Olmeda Ibánez (2022).Algunas de estas herramientas comerciales incluyen implementaciones en\nPython o R, de ahí la importancia de revisarlas inicialmente antes de\nrecurrir otro tipo de recursos.Recursos en R para equidadTutorial de fairness (2021)17: explica las distintas\nmétricas usadas para medir la equidad (paridad demográfica, paridad\nproporcional, paridad predictiva, etc.) y permite crear la distintas\nmétricas y visualizarlas. El tutorial emplea los datos de COMPAS.Tutorial de fairness (2021)17: explica las distintas\nmétricas usadas para medir la equidad (paridad demográfica, paridad\nproporcional, paridad predictiva, etc.) y permite crear la distintas\nmétricas y visualizarlas. El tutorial emplea los datos de COMPAS.Librerías de R18 incluidas en IBM fairness360 (2020):\nincluye algoritmos para detectar el sesgo, pero también para\nmitigarlo.Librerías de R18 incluidas en IBM fairness360 (2020):\nincluye algoritmos para detectar el sesgo, pero también para\nmitigarlo.Librería EDFFair (2022)19: tiene una aproximación distinta, dado\nque permite al usuario ajustar el nivel de equidad frente al de\nexactitud, y así mantener el equilibrio requerido. Los detalles pueden verse en Matloff Zhang (2022).Librería EDFFair (2022)19: tiene una aproximación distinta, dado\nque permite al usuario ajustar el nivel de equidad frente al de\nexactitud, y así mantener el equilibrio requerido. Los detalles pueden verse en Matloff Zhang (2022).Recursos en R para explicabilidad:Algunas de las herramientas más conocidas en explicabilidad que\nmerecen mención aparte son SHAP20 y LIME21, disponibles en R y\nen Python y usadas en muchos paquetes comerciales. SHAP (2018)\nes uno de las librerías más usadas para la explicabilidad. Utiliza los\nvalores de Shapley para poder explicar cualquier tipo de modelo. Los\ndetalles se pueden encontrar en Aas, Jullum, Løland (2021), donde también se\nproporciona el código22. LIME (2017) es otra de las libreríasd más usadas para la explicabilidad. Para ello, se ajusta un modelo local\nalrededor de un punto concreto y lo que hace es estudiar los cambios\nalrededor de este modelo. Se puede encontrar una explicación\ndetallada, junto con el código necesario23, en Ribeiro, Singh, Guestrin (2016).Matloff Zhang (2022) hacen una recopilación de 27 librerías\nde R, incluyendo LIME y SHAP; el código para cada una de ellas puede encontrarse en\nGithub.24DALEX(Biecek 2018) es un paquete de R de reciente creación\npara ayudar crear explicaciones partiendo de un modelo (el código también puede encontrarse en Github25).Esta área está evolucionando mucho en los últimos años, y están\nsurgiendo multitud de técnicas nuevas alrededor de la\nexplicabilidad que van permitir entender mejor el proceso de\ndecisión de los algoritmos más complejos.","code":""},{"path":"cap-etica.html","id":"resumen-3","chapter":"Capítulo 4 Ética en la ciencia de datos","heading":"Resumen","text":"La ética, subdisciplina de la Filosofía, es el estudio sistemático del\ncomportamiento humano desde las categorías del bien y del mal. Se trata\nde una rama aplicada que proporciona métodos basados en la racionalidad\ncrítica para evaluar decisiones y acciones en base ciertos valores\ncompartidos. La aparición de las actuales metodologías que propone la\nciencia de datos supone un desafío que puede resolverse únicamente \npartir de criterios técnicos, puesto que muchas de las decisiones que se\ntoman en este campo pueden tener repercusiones sobre las personas.Para regular ciertas prácticas, se han desarrollado diversas\nlegislaciones en múltiples países y continentes. Estas regulaciones\ntienen un fundamento ético y ofrecen un marco para valorar qué acciones\nse ajustan la legalidad. Ahora bien, ningún cuerpo normativo cubre\ntodas las posibles casuísticas, razón por la cual el razonamiento ético\nes fundamental para orientar la praxis de los científicos de datos.Por otro lado, existe un acuerdo global en relación los principios\nque deben regir el comportamiento del científico de datos, pero la\nnumerosa literatura publicada desde 2016 parece estar de acuerdo en\nque todos ellos pueden agruparse en cuatro categorías: preservar la\nautonomía humana, generar beneficios, evitar daños y fomentar la\njusticia. Dado que hay un acuerdo global nivel mundial, este\ncapítulo se centra en los principios clave que señala la regulación\neuropea: equidad y explicabilidad.","code":""},{"path":"datos-sql.html","id":"datos-sql","chapter":"Capítulo 5 Gestión de bases de datos relacionales","heading":"Capítulo 5 Gestión de bases de datos relacionales","text":"Ismael Caballero\\(^{}\\), Ricardo Pérez del Castillo\\(^{}\\) y Fernando Gualo\\(^{,b}\\)\\(^{}\\)Universidad de Castilla-La Mancha y \\(^{b}\\)DQTeam SL","code":""},{"path":"datos-sql.html","id":"introducción-1","chapter":"Capítulo 5 Gestión de bases de datos relacionales","heading":"5.1 Introducción","text":"\nEl mundo real en el que estamos inmersos es puramente analógico: está lleno de entidades que se relacionan entre ellas o consigo mismas través de unos determinados eventos que representan determinados hechos. Tanto entidades como hechos tienen una colección de características observables (normalmente llamadas atributos26 en el ámbito del diseño de bases de datos), que pueden ser de interés en el contexto de una determinada aplicación.Para estas aplicaciones que demandan el uso de datos del mundo real, es preciso realizar observaciones de esos atributos de las entidades y de los hechos que son relevantes. Al conjunto de entidades y hechos del mundo real que son relevantes para una aplicación se les conoce como Universo del Discurso (Piattini et al. 2006). Para poder tener éxito en las aplicaciones, es importante capturar la semántica del Universo del Discurso mediante los modelos correspondientes.Los datos que son de interés para una determinada aplicación deben ser capturados mediante un proceso de observación y digitalización de los valores de los atributos relevantes de las entidades y hechos del mundo real. Durante el proceso de observación y captura se pueden producir errores que pueden derivar en problemas relacionados con la calidad de los datos (Price Shanks 2004). Por ejemplo, supóngase que las observaciones requieren una determinada frecuencia mínima de observación en relación con la velocidad en la producción de los hechos; si esta frecuencia es adecuada, la cantidad de observaciones realizada será insuficiente para modelar el hecho, llevando un estado inconsistente entre lo sucedido y lo observado.Una vez capturados estos datos, pueden ser usados en los procesos de negocio para una tarea determinada, o bien ser analizados para producir un conocimiento del mundo real que hasta ahora se tenía (T. Davenport Harris 2017).","code":""},{"path":"datos-sql.html","id":"concepto-de-base-de-datos","chapter":"Capítulo 5 Gestión de bases de datos relacionales","heading":"5.2 Concepto de base de datos","text":"\nPara poder habilitar el procesamiento o los análisis de forma automática mediante las potentes técnicas tratadas en el resto de capítulos de este manual, es necesario almacenar previamente los datos en algún lugar como repositorios o bases de datos, donde se puedan fácilmente añadir, borrar, recuperar o modificar los datos.De acuerdo con Piattini et al. (2006), una base de datos (BD) es una colección o depósito de datos integrados, almacenados en soporte secundario (volátil) y con redundancia controlada. En una base de datos, los valores correspondientes los atributos que han de ser compartidos por diferentes usuarios y aplicaciones deben mantenerse independientes de ellos, y su definición (estructura de la base de datos), única y almacenada junto con los datos, se ha de apoyar en un modelo de datos. Este modelo debe captar las interrelaciones y restricciones existentes en las entidades y hechos del mundo real al que representan. Existen diferentes tipos de modelos que permiten estructurar y representar la semántica de los datos, como por ejemplo, el modelo relacional (E. F. Codd 1970), que es el fundamento de las bases de datos relacionales en las que se centra este capítulo.\nEn el ámbito de los Sistemas de Información, se han desarrollado programas que dan soporte todo el proceso de creación y explotación de las bases de datos. estos programas se les conoce como Sistemas Gestores de Bases de Datos (SGBD). Como ejemplos de estos SGBD se pueden citar Microsoft Access, Microsoft SQL Server, Oracle Server, MySQL, MariaDB, Informix, MongoDB,… En cualquier caso, como se verá más adelante en este capítulo, los SGBD más utilizados son los conocidos como relacionales (SGBDR), aunque, con el auge del Big Data y del Machine Learning, esta tendencia está cambiando y empiezan desplegarse cada vez más SGBD conocidos como structured query language, NoSQL (solo lenguaje estructurado de consulta) (véase Cap. 6). Para evitar confusiones, es importante diferenciar entre la base de datos propiamente dicha (como una colección de datos almacenada en un fichero de datos) y el software SGBD específico, ya sea relacional o NoSQL: una misma base de datos, con las correspondientes adaptaciones, puede ser gestionada usando diferentes tipos de SGBD. Habitualmente, los tipos de SGBDR más usados son los que tienen capacidades multiproceso/multiusuario, ya que permiten acceder datos compartidos mediante el uso de interfaces de datos para ejecutar diferentes tipos de análisis, empleando lenguajes de programación más potentes -o versátiles- como R software o Python, que los lenguajes típicos de programación.","code":""},{"path":"datos-sql.html","id":"gestión-de-los-datos-en-una-base-o-repositorio-de-datos","chapter":"Capítulo 5 Gestión de bases de datos relacionales","heading":"5.2.1 Gestión de los datos en una base o repositorio de datos","text":"\nLas organizaciones usan datos para sus procesos de negocio. La Tabla ?? muestra la tipología de datos sugerida por Mahanti (2019).\nPara poder usar estos datos, las aplicaciones pueden realizar los siguientes cuatro tipo de operaciones (normalmente conocidas como ‘operaciones CRUD’):Crear datos (Create): inserta datos en el repositorio de datos.Leer datos (Read): recupera datos del repositorio para aprovisionar el proceso de negocio, o bien para realizar alguna operación de análisis específica.Actualizar datos (Update): modifica el valor de los atributos correspondientes los hechos o entidades para actualizarlos nuevas observaciones.Borrar datos (Delete): elimina en bloque o selectivamente los datos almacenados en el repositorio de los datos.En cualquier caso, estos procedimientos de inserción, actualización, recuperación y borrado deben garantizar siempre la seguridad del conjunto de los datos, de modo que sólo sean accesibles por aquellos usuarios que estén autorizados trabajar con ellos, y siempre para el propósito establecido para los datos (Piattini et al. 2006).La forma de implementar estas operaciones depende fuertemente del formato (modelo lógico) en el que estén almacenados los datos. Aunque existen diferentes modelos (estructurados, semi-estructurados, estructurados), en este capítulo el énfasis se pone en el modelo relacional (Edgar F. Codd 1970), ya que es el más ampliamente usado en el ámbito organizacional y el que implementan los SGBDR. Para poder dar soporte las operaciones CRUD anteriormente citadas en bases de datos relacionales, se desarrolló un lenguaje llamado ‘lenguaje estructurado de consulta’ (structured query language, SQL), que se aborda en la siguiente sección.\n","code":""},{"path":"datos-sql.html","id":"sql-el-lenguaje-estructurado-de-consulta","chapter":"Capítulo 5 Gestión de bases de datos relacionales","heading":"5.3 SQL: el lenguaje estructurado de consulta","text":"Los principios de SQL están establecidos en el estándar internacional ISO/IEC 9075:198927 como un mecanismo para identificar y regular las expresiones necesarias que permiten manejar bases de datos relacionales. Al ser un estándar, es importante señalar que cada fabricante de SGBDR, como Oracle con Oracle Database Manager Server28 o con MySQL29, Microsoft con SQL Server30, IBM con DB231…, implementa en sus productos su propia versión del estándar SQL. Y aunque son prácticamente iguales, hay ligeros matices que les permiten diferenciarse de la competencia y que, por tanto, deben ser conocidos cuando se utilicen los correspondientes productos comerciales. obstante, existen en el mercado algunas soluciones open source como MariaDB32 o PostgreSQL33. En este capítulo, todos los ejemplos que se han desarrollado trabajan contra un servidor MySQL 834.SQL tiene diferentes tipos de sentencias o instrucciones que dan soporte los diferentes aspectos de las interacciones con la base de datos. Cualquier manual de SQL permite tratar en profundidad todos los elementos sintácticos del lenguaje, pero es importante señalar que los detalles específicos de la sintaxis específica dependerán fuertemente del SGBDR empleado. Para los ejemplos propuestos en este libro, puede consultarse el manual de referencia de SQL de MySQL v8.035. Las siguientes secciones proporcionan una visión global de dichos grupos de sentencias.","code":""},{"path":"datos-sql.html","id":"sql-como-lenguaje-de-definición-de-datos","chapter":"Capítulo 5 Gestión de bases de datos relacionales","heading":"5.3.1 SQL como lenguaje de definición de datos","text":"Una base de datos relacional tiene una organización en forma de tabla y su su concepto fundamental es el de ‘relación’, que es el que el lector se puede imaginar primera vista. Una ‘relación’ representa un conjunto de entidades con las mismas propiedades y se compone de filas (o registros; también denominadas tuplas) cuyos valores dependen de los atributos que se representen en las columnas. Por ejemplo, una relación puede ser el conjunto de equipos de futbol de la primera división española, siendo los atributos, su presupuesto, nombre del entrenador, número de jugadores españoles…Las bases de datos relacionales se caracterizan por utilizar el lenguaje de consulta estructurado (SQL) y, por ello, son también denominadas bases de datos SQL. En particular, SQL se utiliza para definir todos los elementos necesarios para crear y modificar las tablas de datos. Se tienen tres tipos de instrucciones básicas para gestionar las tablas como estructuras de datos:Create: permite crear un componente de la base de datos, tal como la base de datos propiamente dicha, una tabla, una vista,… En el siguiente ejemplo, se crea, usando instrucciones SQL, primero una base de datos llamada Biblioteca, y luego una tabla Autor con seis atributos (CodAtutor, Nombre, Apellido1, Apellido2, Pseudonimo y Nacionalidad), donde se podrán almacenar los valores correspondientes dichos atributos, conformando así la base de datos:Alter: permite modificar la estructura de un componente, añadiendo por ejemplo, atributos una tabla, restricciones un atributo, o modificando el tipo de datos de algún atributo existente; también permite eliminar un atributo de una tabla existente. Siguiendo el ejemplo anterior, con la siguiente instrucción se añade un nuevo atributo, LocalidadNacimiento, la tabla Autor:Drop: sirve para eliminar un componente específico, como por ejemplo una tabla, una vista,…Pero sirve para eliminar los valores almacenados en una tabla. En el siguiente ejemplo, se eliminan las tablas Escribe, Autor y Libro:","code":"       create database Biblioteca\n        create Table Autor (\n          CodAutor nvarchar (20) primary key,\n          Nombre    nvarchar(40) not null,\n          Apellido1 nvarchar(50) not null,\n          Apellido2 nvarchar(50), \n          Pseudonimo nvarchar(50),\n          Nacionalidad nvarchar (50)\n        );   alter table Autor add LocalidadNacimiento nvarchar(50) not null   drop table Escribe;\n   drop table Autor;\n   drop table Libro;"},{"path":"datos-sql.html","id":"sql-como-lenguaje-de-manipulación-de-datos","chapter":"Capítulo 5 Gestión de bases de datos relacionales","heading":"5.3.2 SQL como lenguaje de manipulación de datos","text":"En esta sección se describen las instrucciones más importantes de SQL para el soporte las operaciones CRUD anteriormente introducidas. Existen, por tanto, cuatro tipos de sentencia para manipular los datos:Create: implementada mediante la instrucción insert, sirve para insertar registros (también llamados tuplas) en una base de datos. En los ejemplos siguientes se insertan diversas tuplas en varias tablas, siguiendo el mismo orden en el que se especificaron los atributos cuando se creó la tabla. Así por ejemplo, se crean los códigos “dbrown” (como valor para el atributo CodAutor) para “Dan Brown” y “cdv” (como valor para el atributo CodLibro) para su libro “El Código da Vinci”. El siguiente código SQL muestra las instrucciones necesarias:Read: implementada mediante la instrucción select, permite hacer consultas la base de datos. En los siguientes ejemplos se escribe el código que selecciona \\(()\\) el nombre y primer apellido de los autores con nacionalidad española, ordenados por orden alfabético del Apellido1 y \\((ii)\\) la lista todos los libros que haya escrito el autor “Pérez Reverte” (cuyo CodAutor es “perezreverte”).Update: permite actualizar los valores de las tuplas seleccionadas. En el siguiente ejemplo, se actualiza el valor del atributo pseudonimo al valor “El Manco de Lepanto” para el autor “Miguel de Cervantes”, con CodAutor “mcervantes”:Delete: su principal objetivo es eliminar, en bloque o de forma selectiva, una o varias tuplas o registros de datos que cumplan una determinada condición. En el siguiente código SQL se borra la(s) tupla(s) que contiene(n) datos del autor cuyo CodAutores “perezreverte”.","code":"    insert into Autor values ('dbrown', 'Dan', 'Brown', '', '', 'EstadoUnidense');\n    insert into Libro values ('cdv', 'El Código da Vinci', 'Random House', '2003-04-23');\n    insert into Escribe values ('dbrown', 'cdv');   Select Nombre, Apellido1 from Autor where Nacionalidad like 'Español' order by Apellido1;\n\n   Select Libro.Título from Autor, Escribe, Libro where ( Libro.CodLibro = Escribe.CodLibro and Autor.CodAutor = Escribe.CodAutor) and (Escribe.CodAutor ='perezreverte');   update Autor set pseudonimo=\"El Manco de Lepanto\" where CodAutor='mcervantes';   delete from Autor where CodAutor ='perezreverte';"},{"path":"datos-sql.html","id":"sql-como-lenguaje-de-administración-de-datos","chapter":"Capítulo 5 Gestión de bases de datos relacionales","heading":"5.3.3 SQL como lenguaje de administración de datos","text":"SQL también puede ser usado también para administrar los usuarios de una base de datos. Esto implica crear usuarios de la base de datos y asignarles diferentes tipos de permisos para realizar los diferentes tipos de operaciones vistos anteriormente sobre los distintos componentes de datos. Por ejemplo, para crear un usuario llamado Ismael.Caballero que tenga por contraseña LibroMDSR se puede usar la siguiente instrucción:y la siguiente instrucción se usa para asignar al usuario Ismael.Caballero los permisos necesarios para el acceso, lectura, selección, inserción, actualización y borrado de los valores de la base de datos Biblioteca, así como para poder modificar la estructura de los componentes de la base de datos Biblioteca creada anteriormente:Recuérdese que los ejemplos mostrados han sido realizados para MySQL 8, aunque la sintaxis debería ser muy diferente para otros SGBDR.","code":"   create user 'Ismael.Caballero' identified by 'LibroMDSR';   GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, ALTER ON biblioteca.*  TO 'Ismael.Caballero'@'%'; \n   FLUSH PRIVILEGES;"},{"path":"datos-sql.html","id":"acceso-y-explotación-de-bases-de-datos-desde-r","chapter":"Capítulo 5 Gestión de bases de datos relacionales","heading":"5.4 Acceso y explotación de bases de datos desde R","text":"\nComo el propósito de este manual es aprender los fundamentos de la Ciencia de Datos usando R, en las siguientes secciones se explicará cómo implementar las operaciones CRUD usando sentencias de paquetes específicos de R. Dado que en este capítulo se usa MySQL, se utiliza el driver específico de RMySQL (Ooms et al. 2022). En caso de que se hubiése usado otro sistema gestor de bases de datos, se hubiera tenido que recurrir al paquete específico que contuviera el driver correspondiente. En las siguientes secciones se explica cómo conectarse una base de datos usando las funciones correspondientes y cómo se implementan las operaciones CRUD con funciones del paquete RMySQL.","code":""},{"path":"datos-sql.html","id":"conexión-a-una-base-de-datos","chapter":"Capítulo 5 Gestión de bases de datos relacionales","heading":"5.4.1 Conexión a una base de datos","text":"Antes de poder realizar ninguna operación con las bases de datos gestionadas por MySQL es preciso tener instalado el paquete RMySQL y cargar los paquetes necesarios:Es necesaria también la librería DBI (R Special Interest Group Databases (R-SIG-DB), Wickham, Müller 2022) porque proporciona la infraestructura común para todos los drivers de acceso base de datos. Además del mencionado RMySQL, otros ejemplos de drivers son RPostgres (Hadley Wickham, Ooms, Müller 2023), RMariaDB (Müller, Ooms, et al. 2022), odbc (Hester Wickham 2023) o RSQLite(Müller, Wickham, et al. 2022), por citar algunos.Para desarrollar las explicaciones, se usa una base de datos llamada classicmodels, implementada en MySQL v8.0 y desplegada en un servidor con dirección IP 172.20.48.118 que está escuchando en el puerto 3306. El usuario que se conecta la base de datos es Ismael.Caballero, siendo su contraseña MdsR.2022. Otros usuarios tendrían que modificar los parámetros correspondientes para realizar las conexiones sus propias bases de datos. Se almacenan todos estos datos en variables para hacer más sencillo el mantenimiento de los scripts. Con dbConnect() se realiza la conexión. Con summary() o con dbGetInfo() se pueden mostrar los resultados de la conexión en caso de que ésta se haya realizado con éxito.\nUna vez terminadas todas las tareas con la base de datos, debería desconectarse mediante la instrucción dbDisconnect().","code":"\ninstall.packages(\"RMySQL\")\n\nlibrary(\"RMySQL\")\nlibrary(DBI)\nusuario   =   'Ismael.Caballero' \npasswd    =   'MdsR.2022#'\nnombrebd  =   'classicmodels'\n\nservidor  =   '172.20.48.118' \npuerto    =   3306\n\nmibbdd = dbConnect(MySQL(), user=usuario, password=passwd, dbname=nombrebd, \n                   host=servidor, port= puerto)\nsummary (mibbdd)\ndbGetInfo(mibbdd)\ndbDisconnect (mibdd)"},{"path":"datos-sql.html","id":"operaciones-de-lectura-consulta-selección-read-de-datos","chapter":"Capítulo 5 Gestión de bases de datos relacionales","heading":"5.4.2 Operaciones de lectura / consulta/ selección (read) de datos","text":"Entre las operaciones más frecuentes en cualquier tipo de bases de datos están las de lectura o consulta (read), implementadas en SQL con las sentencias de tipo select. El driver RMySQL ofrece distintas alternativas para realizar consultas de selección en R. La elección de la mejor operación dependerá de la complejidad de las consultas que se quieran realizar. Se listan continuación:dbReadTable(), que permite leer una tabla entera de una base de datos MySQL. Es recomendable usar este método si la tabla es excesivamente grande. Se pueden almacenar los resultados en un data.frame para hacer operaciones con ellos después. La ventaja es que, sabiendo manejar data.frames en R, se necesita aprender mucho más detalle del lenguaje SQL36 como lenguaje de manipulación de datos (LMD); sin embargo, la desventaja es que se acaba perdiendo parte del potencial expresivo de SQL para hacer algunas operaciones más fáciles y eficientes. En el siguiente ejemplo se muestra cómo cargar toda la tabla customers en un data.frame llamado tblCustomers, obteniéndose los resultados con la instrucción summary():dbGetQuery(), que tiene más flexibilidad que dbReadTable() porque permite, mediante una sentencia SQL select (véase más información en el tutorial de SQL en W3C37 o en la página oficial de “select” sobre MySQL38), particularizar la consulta la base de datos. Esto puede implicar la selección de atributos específicos o incluso el uso de filtros sobre los atributos seleccionados. Por ejemplo, si se quisieran recuperar el número y el nombre de los clientes de Madrid, se podría personalizar la consulta añadiendo las condiciones correspondientes en la cláusula where39, como se muestra en siguiente código. Por comodidad, se escribe aparte la consulta SQL, en una variable, para poder manejar más fácilmente la operativa en R. Escribir esta consulta puede ser lo que entrañe más dificultad. continuación, se ejecuta la consulta con dbGetQuery() y se almacenan los resultados en un data.frame para su uso posterior. Nuevamente, se comprueba el resultado con la instrucción summary().Teniendo los resultados en data.frames, ya es posible procesarlos en R como si fuesen cualquier otro tipo de datos.Obsérvese que las instrucciones siguientes serían equivalentes:dbSendQuery() combinado con dbFetch(). La principal diferencia entre dbSendQuery() y dbGetQuery() es que la primera recupera datos de la base de datos y hay que traerlos explícitamente con la función dbFetch(). En el siguiente fragmento de código se muestra la utilización de ambas funciones con un resultado exactamente igual que en el apartado anterior.En cualquier caso, para considerar la opción más adecuada deben considerarse los siguientes aspectos:La información de la consulta generada con dbSendQuery() puede mostrarse con la función dbGetInfo().La información de la consulta generada con dbSendQuery() puede mostrarse con la función dbGetInfo().Es posible recordar la consulta SQL que se utilizó en dbSendQuery() mediante la función dbGetStatement().Es posible recordar la consulta SQL que se utilizó en dbSendQuery() mediante la función dbGetStatement().La función dbFetch() tiene dos argumentos: la consulta y el número de registros recuperar; si se quieren recuperar todos los registros que haya podido producir la consulta, debe pasarse el argumento n=-1.La función dbFetch() tiene dos argumentos: la consulta y el número de registros recuperar; si se quieren recuperar todos los registros que haya podido producir la consulta, debe pasarse el argumento n=-1.Si se quiere saber el número de elementos que se han traído con la función dbFetch() se puede usar la función dbGetRowCount().Si se quiere saber el número de elementos que se han traído con la función dbFetch() se puede usar la función dbGetRowCount().La principal ventaja de dbSendQuery() combinado con dbFetch() es que el filtro se hace en el sistema gestor de bases de datos y sólo llegan la memoria de R los datos que se van utilizar, que es mejor que descargar toda la tabla la memoria de R y, después, hacer el filtro.Para extraer información de los resultados de la consulta, se puede usar la función dbColumnInfo():El driver RMySQL proporciona funciones para conocer directamente el tipo y tamaño de los atributos de una tabla. Mediante la función dbSendQuery() y dbColumnInfo() se puede obtener esta información haciendo una consulta que incluya los atributos en los cuales se está interesado. Por ejemplo, para conocer el tipo de datos y tamaño de los atributos de la tabla Employees se podría usar el siguiente fragmento de código:Con dbColumnInfo() se muestran los metadatos de implementación (operativos) de los atributos de la tabla Employees. Finalmente, con la instrucción dbClearResult(ConsultaEmployees) se pueden limpiar los resultados de la consulta para optimizar el sistema.","code":"\ntblCustomers <- dbReadTable(mibbdd, \"customers\")\nsummary (tblCustomers)\nSentenciaSQL_Nombres_Clientes =\"Select CustomerNumber, CustomerName from customers where city = 'Madrid'\"\n\nConsulta_Clientes_Madrid = dbGetQuery (mibbdd, SentenciaSQL_Nombres_Clientes)\nsummary (Consulta_Clientes_Madrid)\ndbReadTable(mibbdd,\"customers\") \ndbGetQuery (mibbdd, \"select * from customers\")\nSentenciaSQL_Nombres_Clientes =\"Select CustomerNumber, CustomerName from customers where city = 'Madrid'\"\n\nConsulta <- dbSendQuery(mibbdd, SentenciaSQL_Nombres_Clientes);\ndbGetInfo(Consulta)\n#> $statement\n#> [1] \"Select CustomerNumber, CustomerName from customers where city = 'Madrid'\" \n#>\n#> $isSelect\n#> [1] 1\n#>\n#> $rowsAffected\n#> [1] -1\n#>\n#> $rowCount\n#> [1] 0\n#>\n#> $completed\n#> [1] 0\n#>\n#> $fieldDescription\n#> $fieldDescription[[1]]\n#> NULL\n\nprint(paste(\"Consulta realizada:\", dbGetStatement(Consulta)) )\n\nConsulta_Clientes_Madrid_condbSendQuery <-  dbFetch(Consulta, n=-1)\nprint( paste(\"Número de elementos devueltos en la consulta\",dbGetRowCount(Consulta)))\n\nsummary (Consulta_Clientes_Madrid_condbSendQuery)\ndbColumnInfo(Consulta)\n#> name Sclass type length\n#> 1 CustomerNumber integer INTEGER 11\n#> 2 CustomerName character VAR_STRING 200\nSentenciaSQL_Tabla_Employees = \"Select * from employees\"\nConsulta_Employees <- dbSendQuery(mibbdd,SentenciaSQL_Tabla_Employees)\n\ndbColumnInfo(Consulta_Employees)\n\n#> name Sclass type length\n#> 1 employeeNumber integer INTEGER 11\n#> 2 lastName character VAR_STRING 200\n#> 3 firstName character VAR_STRING 200\n#> 4 extension character VAR_STRING 40\n#> 5 email character VAR_STRING 400\n#> 6 officeCode character VAR_STRING 40\n#> 7 reportsTo integer INTEGER 11\n#> 8 jobTitle character VAR_STRING 200\n\ndbClearResult(Consulta_Employees)\n#> [1] TRUE"},{"path":"datos-sql.html","id":"operaciones-de-inserción-create-y-actualización-update-de-datos","chapter":"Capítulo 5 Gestión de bases de datos relacionales","heading":"5.4.3 Operaciones de inserción (create) y actualización (update) de datos","text":"Antes de almacenar los datos en la base de datos, es necesario crear las estructuras necesarias, que, como se avanzó anteriormente, son las tablas y los atributos. Para ello se utilizan instrucciones especiales SQL como lenguaje de definición de datos (LDD); esto incluye instrucciones para crear tablas (create table)40, para modificarlas (alter table)41 o para borrarlas (drop table)42.Para poder hacer operaciones con los datos, es preciso crear usuarios y asignarles los privilegios adecuados sobre las tablas y atributos. Ello también requiere las instrucciones especiales SQL como lenguaje de administración de datos (LAD), que incluye instrucciones para crear usuarios (create user)43, modificar ciertos aspectos de los mismos (alter user)44 y borrarlos (drop user)45.Un usuario de la base de datos que tenga privilegios suficientes sobre las estructuras creadas puede crear (insert)46 o modificar (update)47 registros de datos usando las instrucciones específicas de SQL como lenguaje de manipulación de datos (LMD) -véase cómo otorgar privilegios un usuario para crear tablas48.obstante, y dado que el software en el que se centra este manual es en R, se deja fuera del alcance de este capítulo el uso de los aspectos LDD, LMD y LAD de SQL, y se cubrirán mediante la instrucciones dbWriteTable() de RMySQL los aspectos de inserción y de actualización de los registros. dbWriteTable() se usa por tanto para exportar datos de R una base de datos MySQL, y puede ser usado para las acciones que se exponen continuación, siempre y cuando el usuario que ejecute las acciones tenga suficientes permisos en el sistema gestor de bases de datos para realizarlas.","code":""},{"path":"datos-sql.html","id":"crear-una-nueva-tabla-con-datos","chapter":"Capítulo 5 Gestión de bases de datos relacionales","heading":"5.4.3.1 Crear una nueva tabla con datos","text":"La creación de una nueva tabla de datos se lleva cabo partir de un data.frame que se puebla con datos iniciales y que tendrá tantas columnas como atributos tenga la tabla. Por ejemplo, continuación se crea un data.frame llamado dfDatos_Prueba con dos columnas, una de tipo numérico llamada CodPrueba y otra de tipo texto llamada DatosPrueba. En este ejemplo, y modo ilustrativo, los datos son completamente arbitrarios. Después, se construye el data.frame dfDatos_Pruebas y mediante dbListTables() se comprueba que la tabla existe en la conexión la base de datos. Finalmente, con dbWriteTable(), se crea la nueva tabla. Es importante tener en cuenta las posibles conexiones simultáneas la base de datos porque se podrían generar problemas. Con dbWriteTable() se puede comprobar si la tabla se ha creado correctamente.Es interesante pensar en la utilidad de este método para duplicar tablas en caso necesario","code":"\nCodPrueba <- c(1:26)\nNombre_Prueba <- c(letters[1:26])\ndfDatos_Prueba <- data.frame(CodPrueba, Nombre_Prueba)\ndbListTables(mibbdd)\n#> [1] \"Autor\" \"DatosPrueba_16\" \"DatosPrueba_22\" \"Datos_Prueba_01\",\n#> [5] \"Make\" \"Pelicula\" \"customers\" \"employees\",\n#> [9] \"offices\" \"orderdetails\" \"orders\"\"payments\",\n#> [13] \"productlines\" \"products\"\n\ndbWriteTable(mibbdd, \"DatosPrueba\", dfDatos_Prueba, overwrite = TRUE, row.names =FALSE )\n#> [1] TRUE\n#> \ndbListTables(mibbdd)\n#> [1] \"Autor\" \"DatosPrueba\" \"DatosPrueba_16\" \"DatosPrueba_22\",\n#> [5] \"Datos_Prueba_01\" \"Make\" \"Pelicula\" \"customers\" ,\n#> [9] \"employees\" \"offices\" \"orderdetails\",\"orders\" ,\n#> [13] \"payments\" \"productlines\" \"products\""},{"path":"datos-sql.html","id":"sobreescribir-una-tabla-existente-con-datos-actualizados","chapter":"Capítulo 5 Gestión de bases de datos relacionales","heading":"5.4.3.2 Sobreescribir una tabla existente con datos actualizados","text":"Cuando se trata de actualizar algunos valores de los atributos de la tabla o de añadir nuevos registros la tabla, la operación es básicamente la misma que antes, pero primeramente habrá que leer la tabla y convertirla en un data.frame para actualizar en él los valores o añadir los nuevos valores (en este caso se añade una nueva fila); una ves hecho esto, se vuelve utilizar el comando dbWriteTable() añadiendo los parámetros overwrite = TRUE (para sobrescribir toda la tabla) y row.names = FALSE. En el siguiente ejemplo se actualizan los valores de una tupla específica.","code":"\ndfDatos_Prueba <- dbReadTable(mibbdd, \"DatosPrueba\")\n\ndfDatos_Prueba$NombrePrueba[25] <- \"en un lugar de la mancha\"\n\ndbWriteTable(mibbdd, \"DatosPrueba\", dfDatos_Prueba, overwrite = TRUE, row.names =FALSE  )\n#> [1] TRUE\n\ndfDatos_Prueba_Modificado <- dbReadTable(mibbdd, \"DatosPrueba\")"},{"path":"datos-sql.html","id":"añadir-nuevos-registros-a-una-tabla","chapter":"Capítulo 5 Gestión de bases de datos relacionales","heading":"5.4.3.3 Añadir nuevos registros a una tabla","text":"Existen dos estrategias para añadir registros una tabla. La primera es utilizar la técnica de obreescritura descrita anteriormente. Para ello, se procede como antes: se carga la tabla en un\ndata.frame (en este caso dfDatos_Prueba), se añaden nuevas filas (registros) al data.frame (cargadas previamente en el data.frame dfNuevoRegistro) usando rbind(), y continuación se sobreescribe la tabla usando dbWriteTable(). En el siguiente fragmento de código se muestra cómo añadir nuevos registros una tabla sobreescribiéndola completamente.La opción anterior puede ser interesante si la tabla tiene muchos registros y el coste computacional es muy grande. Pero si se tiene muchos registros es preferible usar otra estrategia para añadir un nuevo registro la tabla. En este caso, se puede hacer creando un data.frame compatible con la estructura de la tabla, y ejecutar la instrucción dbWriteTable() poniendo el parámetro append = TRUE. Esto añadirá el nuevo registro al final de la tabla. El siguiente fragmento de código muestra cómo realizar esta operación.","code":"\ndfDatos_Prueba <- dbReadTable(mibbdd, \"DatosPrueba\")\n\ndfNuevo_Registro <- as.list(dfDatos_Prueba)\n\ndfNuevo_Registro$CodPrueba <- c(27)\ndfNuevo_Registro$NombrePrueba <- c(\"Un Valor Nuevo\")\n\ndfDatos_Prueba <- rbind (dfDatos_Prueba, dfNuevo_Registro)\n\ndbWriteTable(mibbdd, \"DatosPrueba\", dfDatos_Prueba, overwrite = TRUE, row.names =FALSE)\n#> [1] TRUE\ndfDatos_Prueba_Nuevos <- as.list(dfDatos_Prueba)\n\ndfDatos_Prueba_Nuevos$CodPrueba <- 28\ndfDatos_Prueba_Nuevos$NombrePrueba <- \"Otro valor nuevo\"\n\ndfDatos_Prueba_Nuevos <- data.frame (dfDatos_Prueba_Nuevos)\n\ndbWriteTable(mibbdd, \"DatosPrueba\", dfDatos_Prueba_Nuevos, append = TRUE, row.names =FALSE)\n#> [1] TRUE"},{"path":"datos-sql.html","id":"inserción-con-consulta-sql-usando-la-instrucción-dbsendquery","chapter":"Capítulo 5 Gestión de bases de datos relacionales","heading":"5.4.3.4 Inserción con consulta SQL usando la instrucción dbSendQuery()","text":"Una última forma de insertar valores en una tabla es mediante la instrucción dbSendQuery, utilizando una consulta de inserción insert. En el siguiente ejemplo se muestra cómo insertar tuplas o registros mediante dbSendQuery(); en este caso, se añaden datos completamente arbitrarios modo de ejemplo.","code":"\nSentenciaSQL_Insercion =\"insert into DatosPrueba value (29, 'Una tercera forma')\"\n \ndbSendQuery (mibbdd, SentenciaSQL_Insercion)\n#> <MySQLResult:-365007472,0,23>"},{"path":"datos-sql.html","id":"operaciones-de-borrado-de-datos-delete","chapter":"Capítulo 5 Gestión de bases de datos relacionales","heading":"5.4.4 Operaciones de borrado de datos (delete)","text":"Finalmente, se describen las operaciones de borrado. Análogamente como se hacían las operaciones de inserción, se pueden hacer de dos formas:Borrado de valores usando dbWriteTable() con sobreescritura: esto implica extraer todos los datos de la tabla, borrar el registro o los registros correspondientes y sobreescribir nuevamente la tabla en la base de datos mediante la instrucción dbWriteTable() con la opción overwrite = TRUE; para ver el resultado se puede usar la función summary(). El siguiente fragmento de código muestra cómo hacerlo:Borrado de registros con consulta SQL en dbSendQuery(): se puede llevar cabo utilizando una sentencia SQL de borrado delete49 con la instrucción dbSendQuery() para borrar registros de la base de datos. El siguiente fragmento de código muestra cómo hacerlo:Finalmente, si fuera necesario eliminar toda la tabla, se podría usar una sentencia drop table50:","code":"\n\ndfDatos_Prueba <- dbReadTable(mibbdd, \"DatosPrueba\")\n\ndfDatos_Prueba <- dfDatos_Prueba[dfDatos_Prueba$CodPrueba < 25, ]\n\ndbWriteTable(mibbdd, \"DatosPrueba\", dfDatos_Prueba, overwrite = TRUE, row.names =FALSE)\n#> [1] TRUE\n# Se usa una sentencia SQL de borrado. El criterio de borrado es completamente arbitrario a efectos ilustrativos.\nSentenciaSQL_Eliminación =\"delete from DatosPrueba where CodPrueba > 10\"\ndbSendQuery (mibbdd, SentenciaSQL_Eliminación)\n#> <MySQLResult:1,0,30>\ndbSendQuery(mibbdd, \"drop table DatosPrueba\")\n#> <MySQLResult:0,0,31>\ndbDisconnect(mibbdd)\n#> [1] TRUE"},{"path":"datos-sql.html","id":"resumen-4","chapter":"Capítulo 5 Gestión de bases de datos relacionales","heading":"Resumen","text":"En este capítulo se han presentado los fundamentos de las bases de datos relacionales. Es importante tener presente los siguientes aspectos:Los datos en las bases de datos se corresponden valores de atributos relevantes de entidades del mundo real.Los datos de una base de datos son una percepción u observación del mundo real.Los datos son la materia prima de los procesos de negocio.Los sistemas de información dan soporte los procesos de negocio.Los datos son un elemento fundamental de los sistemas de información.SQL es el lenguaje más comúnmente utilizado en operaciones sobre el modelo físico de bases de datos relacionales.SQL se puede utilizar como Lenguaje de Definición de Datos (LDD), como Lenguaje de Manipulación de Datos (LMD), y como Lenguaje de Administración de Datos (LAD).La sintaxis de SQL depende fuertemente del sistema gestor de bases de datos relacionales que lo implemente.R Software, través del driver específico, permite manejar bases de datos implementando las operaciones CRUD.","code":""},{"path":"cap-nosql.html","id":"cap-nosql","chapter":"Capítulo 6 Gestión de bases de datos NoSQL","heading":"Capítulo 6 Gestión de bases de datos NoSQL","text":"Ricardo Pérez-Castillo\\(^{}\\) e Ismael Caballero\\(^{}\\)\\(^{}\\)Universidad de Castilla-La Mancha","code":""},{"path":"cap-nosql.html","id":"introducción-al-big-data","chapter":"Capítulo 6 Gestión de bases de datos NoSQL","heading":"6.1 Introducción al big data","text":"Actualmente se vive en la era de la información, con un teléfono móvil en cada bolsillo, un ordenador portátil en cada mochila y grandes sistemas de tecnología funcionando diariamente mandando datos y datos cada segundo. El mundo tiene más datos que nunca, pero esto es todo, ya que el volumen aumenta de forma exponencial (López 2012). Es la era de las bases de datos masivos, en inglés big data.En particular, el volumen de datos disponibles para las empresas aumentó drásticamente desde 2004. En 2004, la cantidad total de datos almacenados en Internet fue de 1 petabyte (equivalente 100 años de todo el contenido de televisión). En 2011 la cantidad total de información almacenada en todo el mundo ya era de 1 zettabyte (1 millón de petabytes o 36 millones de años de video de alta definición); en 2015 alcanzó los 7.9 zettabytes (o 7.9 millones de petabytes) y en 2020 se disparó 35 zettabytes (o 35 millones de petabytes). Este gran volumen de datos, y su crecimiento continuo y exponencial, supera las capacidades de las herramientas de datos tradicionales para capturarlos, almacenarlos, administrarlos y analizarlos (Kalyvas Overly 2014). Por este motivo, se hace necesario el uso de nuevos métodos, técnicas y herramientas de gestión de datos. Este espacio es el que cubre big data.Big data es un término abstracto que, en cierta medida, se ha puesto de moda en diferentes ámbitos: negocios, marketing, social media y diferentes ingenierías como informática, sistemas de información, almacenamiento y recuperación de datos, etc. Big data es un término que hace referencia al gran volumen de datos (tanto estructurados como estructurados) que inundan día día cualquier organización. Pero lo más relevante es la cantidad de datos. Lo que realmente importa es lo que las organizaciones pueden hacer con los datos. Los grandes volúmenes de datos se pueden analizar, por ejemplo, en busca de ideas conducentes una mejor toma de decisiones y movimientos comerciales estratégicos (SAS Institute Inc. 2017).Cuando se acumulan grandes volúmenes de datos, se plantea la necesidad de ver qué se puede hacer con ellos. Esto implica gestionar los datos con una finalidad organizativa y disponer de tecnología y metodologías específicas. La propia gestión de datos lleva generar información relevante en el contexto de la organización, es decir, generar conocimiento para la acción que sea aplicable: por ejemplo, la toma de decisiones, al diseño de acciones o la elaboración de planes estratégicos (García-Alsina 2017).Por tanto, cuando se habla de datos masivos, se está hablando también de gestión de la información y de generación de conocimiento para la acción. Este campo científico es el que proporciona las pautas metodológicas para gestionar grandes volúmenes de datos con el fin de crear valor mediante una serie de procesos y procedimientos. Pero exige contar con la tecnología para capturar los datos, procesarlos, analizarlos e interpretarlos de manera eficaz y eficiente Gómez García Conesa Caralt (2015).Por consiguiente, se puede concluir que big data es el “conjunto de datos masivos heterogéneos que supera la capacidad del software habitual para ser capturados, gestionados y procesados en un tiempo razonable”. Esta definición tiene en cuenta tres de las V’s del big data (véase Sec. 6.2): volumen, variabilidad y velocidad. Así, cuando se habla de big data se está haciendo referencia conjuntos de datos o combinaciones de conjuntos de datos cuyo tamaño (volumen), complejidad (variabilidad) y velocidad de crecimiento dificultan su captura, gestión, procesamiento y análisis mediante tecnologías y herramientas convencionales, tales como las bases de datos relacionales y los paquetes de visualización y técnicas estadísticas convencionales, en el tiempo necesario para que sean útiles.","code":""},{"path":"cap-nosql.html","id":"VsBigData","chapter":"Capítulo 6 Gestión de bases de datos NoSQL","heading":"6.2 Las V’s del big data","text":"El volumen se refiere la cantidad de datos que son generados cada segundo, minuto y día en nuestro entorno. Es la característica más asociada al big data, ya que hace referencia las cantidades masivas de datos que se almacenan con la finalidad de procesar dicha información, transformando los datos en acciones. Las personas están Cada vez más conectadas al mundo digital, por lo que se generan más y más datos. Para algunas empresas, el estar en el mundo digital es algo obligatorio, por lo que la cantidad de datos generados es aún mayor. Por ejemplo, una empresa que vende sus productos únicamente través de un canal online, le convendría implantar tecnología big data para procesar toda aquella información que recoge su página web rastreando todas las acciones que lleva cabo el cliente: conocer donde cliquea más veces, cuántas veces ha pasado por el carrito de la compra, cuáles son los productos más vistos, las páginas más visitadas, etc.La velocidad se refiere la rapidez con la que los datos son creados, almacenados y procesados en tiempo real. Así, en procesos como la detección de fraude en una transacción bancaria o la monitorización de un evento en redes sociales, el tratamiento de la información en tiempo real es imprescindible para que resulten útiles y den lugar acciones efectivas. Otros ejemplos son la gestión de catástrofes naturales o pandemias, o el seguimiento de una campaña analizando comentarios de los actores quienes ésta va dirigida, para ir reorientándola en función de la retroalimentación que fluye en redes sociales (García-Alsina 2017).La variedad se refiere que los sistemas de procesamiento del big data deben ser capaces de procesar datos de diversas formas, tipos y fuentes. Los datos pueden ser estructurados y fáciles de gestionar, como las bases de datos relacionales, o estructurados, entre los que se incluyen desde documentos de texto, correos electrónicos, datos de sensores, audios, vídeos o imágenes que se tienen en un dispositivo móvil, hasta publicaciones en los perfiles de redes sociales, artículos en blogs, secuencias de click que los usuarios hacen en una misma página, formularios de registro e infinidad de acciones más que se realizan desde un smartphone, una tablet o un ordenador. Este tipo de datos requiere un herramental específico, ya que su tratamiento es totalmente diferente al de los datos estructurados. Por ello, las empresas necesitan disponer de las herramientas apropiadas para integrar, observar y procesar este tipo de datos.51Con el tiempo, se han ido incorporando, progresivamente, otras V’s: las V’s de valor (de enorme interés en el análisis de datos), veracidad, viabilidad y visualización (IIC 2016).","code":""},{"path":"cap-nosql.html","id":"tipos-de-datos-en-entornos-big-data","chapter":"Capítulo 6 Gestión de bases de datos NoSQL","heading":"6.3 Tipos de datos en entornos big data","text":"En función de la estructura con la que se organizan los datos (forma en la que se agrupan, almacenan y se relacionan entre sí y manera en la que se puede acceder ellos, analizarlos o modificarlos), éstos pueden clasificarse en: estructurados, estructurados o semiestructurados.\nDatos estructurados: son aquellos que tienen longitud y formato, como las fechas, los números o las cadenas de caracteres. En esta categoría entran los que se compilan en los censos de población, los diferentes tipos de encuestas, los datos de transacciones bancarias, las compras en tiendas online, etc.Datos estructurados: son aquellos que tienen longitud y formato, como las fechas, los números o las cadenas de caracteres. En esta categoría entran los que se compilan en los censos de población, los diferentes tipos de encuestas, los datos de transacciones bancarias, las compras en tiendas online, etc.Datos estructurados: son los que carecen de un formato determinado y pueden ser almacenados en una tabla. Pueden ser de tipo texto (los que generan los usuarios de foros, redes sociales, documentos de Word, etc.), y los de tipo -texto (cualquier fichero de imagen, audio, vídeo, …). Este tipo de datos tiene campos fijos y normalmente se tiene poco control sobre ellos. Su manipulación requiere tecnología de bases de datos bigdata, también conocidas como bases de datos NoSQL (SQL).Datos estructurados: son los que carecen de un formato determinado y pueden ser almacenados en una tabla. Pueden ser de tipo texto (los que generan los usuarios de foros, redes sociales, documentos de Word, etc.), y los de tipo -texto (cualquier fichero de imagen, audio, vídeo, …). Este tipo de datos tiene campos fijos y normalmente se tiene poco control sobre ellos. Su manipulación requiere tecnología de bases de datos bigdata, también conocidas como bases de datos NoSQL (SQL).Datos semiestructurados: poseen organización interna o marcadores que facilitan el tratamiento de sus elementos. pertenecen bases de datos relacionales. Es el caso de documentos XML, HTML o los datos almacenados en bases de datos NoSQL, que tienen una cierta estructura, aunque sin llegar estar totalmente estructurados. También se pueden incluir en este tipo de datos los multi-estructurados o híbridos (datos de mercados emergentes, e-commerce, datos meteorológicos, etc.).Datos semiestructurados: poseen organización interna o marcadores que facilitan el tratamiento de sus elementos. pertenecen bases de datos relacionales. Es el caso de documentos XML, HTML o los datos almacenados en bases de datos NoSQL, que tienen una cierta estructura, aunque sin llegar estar totalmente estructurados. También se pueden incluir en este tipo de datos los multi-estructurados o híbridos (datos de mercados emergentes, e-commerce, datos meteorológicos, etc.).","code":""},{"path":"cap-nosql.html","id":"por-qué-bases-de-datos-nosql","chapter":"Capítulo 6 Gestión de bases de datos NoSQL","heading":"6.4 ¿Por qué bases de datos NoSQL?","text":"\nLo que hace que el big data sea tan útil para muchas empresas e instituciones es el hecho de que proporciona respuestas muchas preguntas que dichas empresas e instituciones ni siquiera sabían que tenían. En otras palabras, proporciona un punto de referencia. Con una cantidad tan grande de información, los datos pueden ser moldeados o probados de cualquier manera que la organización considere adecuada. Al hacerlo, las organizaciones son capaces de identificar los problemas de una forma más comprensible.La recopilación de grandes cantidades de datos y la búsqueda de tendencias en ellos permite que las organizaciones sean más ágiles y actúen mucho más rápidamente, sin problemas y de manera más eficaz. También les permite eliminar las áreas problemáticas antes de que los problemas acaben con sus beneficios o su reputación.El análisis de big data ayuda las organizaciones aprovechar sus datos y utilizarlos para identificar nuevas oportunidades. Eso, su vez, conduce movimientos de negocio (o de otro tipo) más inteligentes, operaciones más eficientes, mayores ganancias y clientes más satisfechos Las organizaciones más existosas con big data consiguen valor de las siguientes formas:Reducción de costes. Las grandes tecnologías de datos, como Hadoop y el análisis basado en la nube, aportan importantes ventajas en términos de costes cuando se trata de almacenar grandes cantidades de datos, además de identificar maneras más eficientes de hacer negocios.Mejores decisiones y más rápidas. Con la velocidad de las bases de datos NoSQL y la analítica en memoria, combinada con la capacidad de analizar nuevas fuentes de datos, las organizaciones pueden analizar la información inmediatamente y tomar decisiones basadas en lo que han aprendido.Nuevos productos y servicios. Con la capacidad de medir las necesidades de los clientes y su satisfacción través de análisis, viene la posibilidad de dar los clientes lo que quieren. Con la analítica de big data, cada vez son más las organizaciones que están creando nuevos productos para satisfacer las necesidades de los clientes.Existen ciertas diferencias entre las fuentes de datos tradicionales y las nuevas fuentes de datos que considera big data. Un resumen de las mismas puede verse en la Tabla 6.1.Tabla 6.1: . Diferencias entre tecnologías tradicionales y tecnologías big data.En primer lugar, la tecnología tradicional de almacenamiento y gestión de datos (desde final de los años 80) han sido las bases de datos relacionales. Aunque las bases de datos relacionales son, ni mucho menos, una tecnología en desuso, los entornos big data consideran otras tecnologías como, por ejemplo, las bases de datos NoSQL, que son bases de datos relacionales optimizadas para modelos de datos sin esquema y de desempeño escalable. También son muy conocidas por su facilidad de desarrollo, baja latencia y resiliencia (Amazon Web Services 2018).diferencia de las bases de datos basadas en SQL, las bases NoSQL usan tablas tradicionales con líneas y columnas para almacenar datos, sino que los organizan con técnicas más flexibles, como, por ejemplo, documentos, gráficos, pares de valores y columnas. Por ello, son ideales para aplicaciones en las que se procesan grandes volúmenes de datos y que requieren estructuras flexibles. Como los sistemas NoSQL hacen uso de clústeres de hardware y servidores de nube, las capacidades se distribuyen de manera uniforme y la base de datos funciona con fluidez, aunque el volumen de datos sea grande. diferencia de las bases de datos relacionales, cuyo rendimiento se reduce notablemente cuando aumenta el volumen de datos, las bases NoSQL suponen una solución potente, flexible y escalable incluso con grandes volúmenes de datos. Otra particularidad de los sistemas NoSQL es el escalamiento horizontal. Las bases de datos SQL relacionales cuentan con un escalamiento vertical y toda su capacidad de rendimiento se basa en un solo servidor. Sin embargo, en general, las soluciones NoSQL distribuyen los datos en varios servidores. Si aumenta el volumen de datos, simplemente se añaden nuevos servidores52.Otra gran diferencia respecto las tecnologías tradicionales es que los entornos big data sólo se centran en la consulta de datos, sino también en su captura y procesamiento (véase la Tabla 6.1). Además, las fuentes de datos pueden proveer datos heterogéneos con formatos heterogéneos. Estas diferencias hacen que siempre se suficiente un ámbito de trabajo puramente informático, tendiendo equipos multidisciplinares cuando se habla de proyectos big data (ingeniería, estadística, etc.).","code":""},{"path":"cap-nosql.html","id":"bases-de-datos-nosql","chapter":"Capítulo 6 Gestión de bases de datos NoSQL","heading":"6.5 Bases de datos NoSQL","text":"","code":""},{"path":"cap-nosql.html","id":"FundamentosNoSQL","chapter":"Capítulo 6 Gestión de bases de datos NoSQL","heading":"6.5.1 Fundamentos de las bases de datos NoSQL","text":"El término NoSQL se usa la primera vez en 1998 para referirse una base de datos relacional sin SQL (Strozzi 1998). NoSQL significa estar en contra de SQL, y de hecho esto suele ser una falacia encontrada en la literatura. Sin embargo, para determinados problemas hay otras soluciones de almacenamiento más apropiadas. En la actualidad el término NoSQL se refiere bases de datos que solo tienen SQL (SQL).Hay una gran variedad de sistemas de gestión de bases de datos que usan SQL como principal lenguaje de consultas. Los datos almacenados requieren estructuras fijas como tablas y se garantizan completamente los principios ACID (atomicity, consistency, isolation, durability) que sí deben cumplir las bases de datos relacionales (SQL):Atomicidad. Las transacciones se ejecutan completamente o . Si fallan es como si ni siquiera se hubieran intentado ejecutar.Consistencia. Un sistema consistente garantiza que cualquier transacción llevará la base de datos de un estado válido otro estado válido. Cualquier dato que se escriba en la base de datos tiene que ser válido de acuerdo con todas las reglas de integridad definidas en el modelo.Aislamiento. Cuando varias transacciones se ejecutan en paralelo, cada una de ellas ataca la base de datos de la misma manera como lo haría si se ejecutara cada operación individual de forma aislada o secuencial.Durabilidad. El resultado de las transacciones es un cambio en el estado del sistema persistente. Si se apaga la máquina y se arranca de nuevo, el cambio producido por la transacción aún está presente.NotaUna transacción en una base de datos se refiere un bloque de operaciones sobre los datos que debe completarse en su conjunto, o de lo contrario, en caso de un error puntual, se restaura el estado de la base de datos su estado anterior, antes del inicio de la transacción.pesar de todo, las bases de datos NoSQL se denominan “sólo SQL” para subrayar el hecho de que también pueden soportar lenguajes de consulta de tipo SQL.Se puede decir el término NoSQL aparece con la llegada de la web 2.0, ya que hasta ese momento sólo subían contenido la red aquellas empresas que tenían un portal. Pero con la llegada de aplicaciones como Facebook, Twitter o Youtube, entre otras, cualquier usuario podía subir contenido, provocando así un crecimiento exponencial de los datos (acens.com 2014).Es en este momento cuando empiezan aparecer los primeros problemas relacionados con la gestión de toda esa información almacenada en bases de datos relacionales 53. En un principio, para solucionar estos problemas de accesibilidad, las empresas optaron por utilizar un mayor número de máquinas, pero pronto se dieron cuenta de que esto solucionaba el problema, además de ser una solución muy cara. La otra opción era la creación de sistemas pensados para un uso específico que con el paso del tiempo han dado lugar soluciones robustas, apareciendo así el movimiento NoSQL (acens.com 2014).Por tanto, siguiendo de nuevo acens.com (2014), hablar de bases de datos NoSQL es hablar de estructuras que permiten almacenar información en aquellas situaciones en las que las bases de datos relacionales generan ciertos problemas debidos, principalmente, al aumento progresivo de la capacidad de almacenamiento (escalabilidad) y rendimiento al darse cita miles de usuarios concurrentes y con millones de consultas diarias.Además, como se esbozó al principio de la sección, las bases de datos NoSQL son sistemas de almacenamiento de información que cumplen con el esquema ‘entidad–relación’. Tampoco utilizan una estructura de datos en forma de tabla donde se van almacenando los datos, sino que para el almacenamiento hacen uso de otros formatos como clave–valor, mapeo de columnas o grafos.Las bases de datos relacionales modernas normalmente han mostrado poca eficiencia en determinadas aplicaciones que usan los datos de forma intensiva, incluyendo el indexado de un gran número de documentos, la presentación de páginas en sitios que tienen gran tráfico y en sitios de streaming audiovisual. Las implementaciones típicas de los sistemas gestores de bases de datos realacionales (SGBDR) se han afinado, bien para una cantidad pequeña pero frecuente de lecturas y escrituras, bien para un gran conjunto de transacciones que tiene pocos accesos de escritura. Sin embargo, NoSQL puede servir gran cantidad de carga de lecturas y escrituras.","code":""},{"path":"cap-nosql.html","id":"necesidades-no-cubiertas-por-las-bases-de-datos-relacionales","chapter":"Capítulo 6 Gestión de bases de datos NoSQL","heading":"6.5.2 Necesidades no cubiertas por las bases de datos relacionales","text":"Las bases de datos NoSQL son, principalmente, bases de datos distribuidas escalables horizontalmente y que se basan en esquemas de datos predefinidos, por lo que ofrecen una fácil replicación y un conjunto sencillo de operaciones y consultas para el acceso un gran volumen de datos.La necesidad de este tipo de bases de datos surge porque hay fuentes de datos que son difíciles de modelar en bases de datos relacionales. Algunos ejemplos son texto (datos estructurados), procesado en streaming (flujo continuo de datos) y bases de datos científicas (estructuras multidimensionales).Como se avanzó en la Sec. 6.5.1, las características de las nuevas aplicaciones de Internet, como las redes sociales, juegos online, etc., hacen que las bases de datos NoSQL sean necesarias para conseguir mayor velocidad, escalabilidad, independencia de la localización, disponibilidad y mejor gestión, sea cual sea el tipo de datos:Velocidad. Para demostrar la importancia de la velocidad en internet, sirva como ejemplo cómo dos grandes compañías monetizan la velocidad de acceso. Por un lado, Amazon tiene estudiado que cuando el tiempo de respuesta disminuye 100ms los ingresos aumentan en un 1%. Por otro lado, Yahoo asegura que el tráfico aumenta en un 9% cuando el rendimiento mejora en 400ms. De ahí la importancia de esta V del big data para la evolución hacia bases de datos NoSQL.Escalabilidad. Al principio, la web se consideró una interfaz más, pero es sólo eso; se ha convertido en un elemento generador y consumidor de datos (fundamentalmente semiestructurados y estructurados). En el contexto actual, las compañías necesitan mantener una respuesta rápida, aunque se incremente el número de usuarios simultáneos o el volumen de datos manejado. Además, la arquitectura de las bases de datos NoSQL permite: \\(()\\) escalar sin disminuir el rendimiento; \\((ii)\\) añadir nodos sobre la marcha, es decir, sin interrupciones del servicio; \\((iii)\\) evitar que se generen cuellos de botella. La Fig. 6.1 (adaptada de Lo (2017)) muestra una representación comparativa de la escalabilidad de las bases de datos NoSQL frente las relacionales. Como se observa, aunque las bases de datos relacionales tienen un mejor rendimiento para volúmenes de datos reducidos, éste se reduce drásticamente para grandes volúmenes. Mientras, el rendimiento de las bases de datos NoSQL tiende ser constante, por lo que escalan mejor para datos masivos.\nFigura 6.1: Comparativa de escalabilidad entre bases de datos relacionales y NoSQL.\nNotaEscalabilidad es un anglicismo que describe la capacidad de un negocio o sistema para crecer en magnitud.Independencia de la localización. La globalización del mercado en World Wide Web (WWW) obliga dar servicio rápido y en todas partes del mundo. Las bases de datos relacionales son distribuidas de acuerdo con diferentes arquitecturas como “nodo principal y nodo secundario”, o bien “peer--peer”.Disponibilidad. Similar la independencia de la localización, la disponibilidad en el mercado WWW es uno de los factores más críticos, ya que se espera una disponibilidad de servicio 24x7. Es decir, hay que pasar de una alta disponibilidad la disponibilidad continua, cuyas características son: \\(()\\) diseño que sigue el modelo principal-secundario; \\((ii)\\) centro multi-datos (multi-data center); \\((iii)\\) disponibilidad cloud; \\((iv)\\) copias de datos y funcionalidad en múltiples localizaciones.Gestión de todos los tipos de datos. Un factor clave en las bases de datos relacionales es la necesidad de manejar tanto datos estructurados, como estructurados y semiestructurados; y todo esto sin perder el enfoque de un almacenamiento eficiente. En ese sentido, menudo, las bases de datos NoSQL están altamente optimizadas para las operaciones de recuperar y agregar, y normalmente ofrecen mucho más que la funcionalidad de almacenar los registros (p.ej. almacenamiento clave-valor). La pérdida de flexibilidad en el tiempo de ejecución, comparado con las bases de datos SQL clásicas, se ve compensada por ganancias significativas en escalabilidad y rendimiento cuando se trata con ciertos tipos de almacenamiento de datos.","code":""},{"path":"cap-nosql.html","id":"tipos-de-almacenamiento-en-bases-de-datos-nosql","chapter":"Capítulo 6 Gestión de bases de datos NoSQL","heading":"6.5.3 Tipos de almacenamiento en bases de datos NoSQL","text":"Se pueden distinguir al menos 4 tipos de bases de datos NoSQL (Hecht Jablonski 2011): clave-valor, documental, en grafo y orientadas columnas.Almacenamiento clave-valor. Los datos se almacenan de forma similar los mapas o diccionarios de datos, donde se accede al dato partir de una clave única. Los valores (datos) son aislados e independientes entre ellos, y son interpretados por el sistema. Pueden ser variables simples, como enteros o caracteres, u objetos. Por otro lado, este sistema de almacenamiento carece de una estructura de datos clara y establecida, por lo que requiere un formateo de los datos muy estricto. Son útiles para operaciones simples basadas en las claves. Apache Cassandra es la tecnología de almacenamiento clave-valor más reconocida por los usuarios.Almacenamiento clave-valor. Los datos se almacenan de forma similar los mapas o diccionarios de datos, donde se accede al dato partir de una clave única. Los valores (datos) son aislados e independientes entre ellos, y son interpretados por el sistema. Pueden ser variables simples, como enteros o caracteres, u objetos. Por otro lado, este sistema de almacenamiento carece de una estructura de datos clara y establecida, por lo que requiere un formateo de los datos muy estricto. Son útiles para operaciones simples basadas en las claves. Apache Cassandra es la tecnología de almacenamiento clave-valor más reconocida por los usuarios.Almacenamiento documental. Este tipo de base de datos almacena datos semi-estructurados. Los datos se llaman documentos, y pueden estar formateados en XML (Extensible Markup Language), JSON (JavaScript Object Notation), BSON (Binary JSON) o el que acepte la propia base de datos, pero suele ser un formato de texto. Un ejemplo de cómo se usa es un blog: se almacena el autor, la fecha, el título, el resumen y el contenido del post. Todos los documentos tienen una clave única con la se puede acceder e identificarlos explícitamente. Estos documentos son opacos al sistema, por lo que se pueden interpretar y lanzar consultas sobre ellos (véase Fig. 6.2). CouchDB o MongoDB son, quizás, los sistemas de bases de datos más conocidas. Hay que hacer mención especial MapReduce, una tecnología de Google inicialmente diseñada para su algoritmo PageRank, que permite seleccionar un subconjunto de datos, agruparlos o reducirlos y cargarlos en otra colección, y Hadoop, que es una tecnología de Apache diseñada para almacenar y procesar grandes cantidades de datos. Por ejemplo, MongoDB es una base de datos orientada documentos. Los documentos se guardan en BSON, que es una forma de representar de forma binaria objetos JSON. De esta forma, con el comando insert y pasando un objeto JSON, MongoDB crea automáticamente un documento y lo añade en la base de datos generando un ObjectId para el nuevo documento (Rubenfa 2014). Este objeto está especialmente pensado para garantizar unicidad en entornos distribuidos como MongoDB. El campo está compuesto por 12 bytes. Los cuatro primeros bytes son un timestamp con los segundos; los tres siguientes bytes representan el identificador único de la máquina; los dos siguientes el identificador del proceso; y, para finalizar, los últimos tres bytes son un campo incremental. En definitiva, los nueve primeros bytes garantizan un identificador único por segundo, máquina y proceso. Los tres últimos bytes garantizan que cada segundo se pueden insertar 224 = 16.777.216 documentos con un identificador distinto. Esta composición del ObjectId proporciona funcionalidades muy útiles. La primera es que indicar el orden de creación de los documentos. También sirve para obtener la fecha de creación del documento.Almacenamiento documental. Este tipo de base de datos almacena datos semi-estructurados. Los datos se llaman documentos, y pueden estar formateados en XML (Extensible Markup Language), JSON (JavaScript Object Notation), BSON (Binary JSON) o el que acepte la propia base de datos, pero suele ser un formato de texto. Un ejemplo de cómo se usa es un blog: se almacena el autor, la fecha, el título, el resumen y el contenido del post. Todos los documentos tienen una clave única con la se puede acceder e identificarlos explícitamente. Estos documentos son opacos al sistema, por lo que se pueden interpretar y lanzar consultas sobre ellos (véase Fig. 6.2). CouchDB o MongoDB son, quizás, los sistemas de bases de datos más conocidas. Hay que hacer mención especial MapReduce, una tecnología de Google inicialmente diseñada para su algoritmo PageRank, que permite seleccionar un subconjunto de datos, agruparlos o reducirlos y cargarlos en otra colección, y Hadoop, que es una tecnología de Apache diseñada para almacenar y procesar grandes cantidades de datos. Por ejemplo, MongoDB es una base de datos orientada documentos. Los documentos se guardan en BSON, que es una forma de representar de forma binaria objetos JSON. De esta forma, con el comando insert y pasando un objeto JSON, MongoDB crea automáticamente un documento y lo añade en la base de datos generando un ObjectId para el nuevo documento (Rubenfa 2014). Este objeto está especialmente pensado para garantizar unicidad en entornos distribuidos como MongoDB. El campo está compuesto por 12 bytes. Los cuatro primeros bytes son un timestamp con los segundos; los tres siguientes bytes representan el identificador único de la máquina; los dos siguientes el identificador del proceso; y, para finalizar, los últimos tres bytes son un campo incremental. En definitiva, los nueve primeros bytes garantizan un identificador único por segundo, máquina y proceso. Los tres últimos bytes garantizan que cada segundo se pueden insertar 224 = 16.777.216 documentos con un identificador distinto. Esta composición del ObjectId proporciona funcionalidades muy útiles. La primera es que indicar el orden de creación de los documentos. También sirve para obtener la fecha de creación del documento.\nFigura 6.2: Ejemplo representativo de base de datos NoSQL documental. Adaptado de Sánchez (2017).\nAlmacenamiento en grafo. Este tipo de almacenamiento maneja datos semi-estructurados y está basado en la teoría de grafos (véase Cap. 39). En las bases de datos NoSQL se establece que la información son los nodos y las relaciones entre la información son las aristas (algo similar al modelo relacional). Su mayor uso se contempla en caso de tener que relacionar grandes cantidades de datos que pueden ser muy variables. Por ejemplo, los nodos pueden contener objetos, variables y atributos diferentes en unos y otros. Las operaciones de consulta con join se sustituyen por recorridos través del grafo, y se guarda una lista de adyacencias entre los nodos. modo de ejemplo, en Facebook se considera cada usuario como un nodo, que puede tener aristas de amistad con otros usuarios, o aristas de publicación con nodos de contenidos. Soluciones como Neo4J y GraphDB son las más conocidas dentro de las bases de datos orientadas grafos.Almacenamiento en grafo. Este tipo de almacenamiento maneja datos semi-estructurados y está basado en la teoría de grafos (véase Cap. 39). En las bases de datos NoSQL se establece que la información son los nodos y las relaciones entre la información son las aristas (algo similar al modelo relacional). Su mayor uso se contempla en caso de tener que relacionar grandes cantidades de datos que pueden ser muy variables. Por ejemplo, los nodos pueden contener objetos, variables y atributos diferentes en unos y otros. Las operaciones de consulta con join se sustituyen por recorridos través del grafo, y se guarda una lista de adyacencias entre los nodos. modo de ejemplo, en Facebook se considera cada usuario como un nodo, que puede tener aristas de amistad con otros usuarios, o aristas de publicación con nodos de contenidos. Soluciones como Neo4J y GraphDB son las más conocidas dentro de las bases de datos orientadas grafos.Almacenamiento orientado columnas. Es similar al almacenamiento documental. Su modelo de datos se define como “un mapa de datos multidimensional poco denso, distribuido y persistente” (Hecht Jablonski 2011). Se orienta almacenar datos con tendencia escalar horizontalmente, por lo que permite guardar diferentes atributos y objetos bajo una misma clave. diferencia del documental y del clave-valor, en este caso se pueden almacenar varios atributos y objetos, pero serán interpretables directamente por el sistema. Permite agrupar columnas en familias y guardar la información cronológicamente, mejorando el rendimiento. Esta tecnología se suele usar en casos con 100 o más atributos por clave. Su precursor es BigTable de Google, pero han aparecido nuevas soluciones como HBase o HyperTable.Almacenamiento orientado columnas. Es similar al almacenamiento documental. Su modelo de datos se define como “un mapa de datos multidimensional poco denso, distribuido y persistente” (Hecht Jablonski 2011). Se orienta almacenar datos con tendencia escalar horizontalmente, por lo que permite guardar diferentes atributos y objetos bajo una misma clave. diferencia del documental y del clave-valor, en este caso se pueden almacenar varios atributos y objetos, pero serán interpretables directamente por el sistema. Permite agrupar columnas en familias y guardar la información cronológicamente, mejorando el rendimiento. Esta tecnología se suele usar en casos con 100 o más atributos por clave. Su precursor es BigTable de Google, pero han aparecido nuevas soluciones como HBase o HyperTable.","code":""},{"path":"cap-nosql.html","id":"limitaciones-de-las-bases-de-datos-nosql","chapter":"Capítulo 6 Gestión de bases de datos NoSQL","heading":"6.5.4 Limitaciones de las bases de datos NoSQL","text":"Las bases de datos NoSQL sólo tienen ventajas; también tienen algunas limitaciones, tanto técnicas como de carácter tecnológico.Entre las técnicas se encuentran: \\(()\\) cómo se modelan los datos correctamente para maximizar las capacidades; \\((ii)\\) nivel bajo de seguridad; \\((iii)\\) soporte de transacciones; \\((iv)\\) falta de madurez en Business Intelligence; y \\((v)\\) problemas de compatibilidad ya resueltos en los modelos relacionales. Entre las de carácter tecnológico pueden citarse \\(()\\) la falta de expertos; \\((ii)\\) la resistencia al cambio; \\((iii)\\) la disponibilidad del vendedor; y \\((iv)\\) que el código abierto puede implicar problema de soporte para las empresas.","code":""},{"path":"cap-nosql.html","id":"integración-de-bases-de-datos-nosql-en-r","chapter":"Capítulo 6 Gestión de bases de datos NoSQL","heading":"6.6 Integración de bases de datos NoSQL en R","text":"En esta sección se verá como R puede ser utilizado para conectarse una base de datos NoSQL: en particular MongoDB. En la Sec. 6.6.1 se presenta una introducción MongoDB. En la Sec. 6.6.2 se explican los paquetes de R utilizados para acceder MongoDB. La Sec. 6.6.3 indica cómo conectarse una base de datos MongoDB remota. Las secciones 6.6.4 y 6.6.5 realizan consultas y análisis sobre una colección de viajes realizados por los usuarios de un servicio de bicicletas compartidas con sede en la ciudad de Nueva York.","code":""},{"path":"cap-nosql.html","id":"introMongo","chapter":"Capítulo 6 Gestión de bases de datos NoSQL","heading":"6.6.1 Introducción a MongoDB","text":"MongoDB (de la palabra inglesa “humongous”, que significa enorme) es un sistema de base de datos NoSQL orientado documentos, desarrollado bajo el concepto de código abierto. MongoDB forma parte de la nueva familia de sistemas de bases de datos NoSQL. En lugar de guardar los datos en tablas como se hace en las bases de datos relacionales, MongoDB guarda estructuras de datos en documentos similares JSON con un esquema dinámico (MongoDB utiliza una especificación llamada BSON), haciendo que la integración de los datos en ciertas aplicaciones sea más fácil y rápida. MongoDB soporta la búsqueda por campos, consultas de rangos y expresiones regulares. Las consultas pueden devolver un campo específico del documento, pero también puede ser una función JavaScript definida por el usuario. Cualquier campo en un documento de MongoDB puede ser indexado, al igual que es posible hacer índices secundarios. El concepto de índices en MongoDB es similar los encontrados en bases de datos relacionales. Tecnológicamente, MongoDB es una base de datos multiplataforma, orientada documentos, que brinda alto rendimiento, alta disponibilidad y facilita la escalabilidad.MongoDB trabaja con el concepto de colección y documento. La Tabla 6.2 muestra la relación de esta terminología respecto las bases de datos relacionales.Tabla 6.2: . Diferencias entre conceptos y terminología en bases de datos relacionales y MongoDB.Asimismo, MongoDB proporciona una función, MapReduce, que se puede utilizar para el procesamiento por lotes de datos y operaciones de agregación. El framework de agregación permite realizar operaciones similares las que se obtienen con el comando SQL GROUP . El framework de agregación está construido como un pipeline (flujo de trabajo) en el que los datos van pasando través de diferentes etapas en las cuales estos datos son modificados, agregados, filtrados y formateados hasta obtener el resultado deseado (véase ejemplo esquemático en la Fig. 6.3 (Morgan (2015))). Todo este procesado es capaz de utilizar índices, si existieran, y se produce en memoria.\nFigura 6.3: Ejemplo esquemático del pipeline de agregación en MongoDB. Adaptado de Morgan (2015)\n","code":""},{"path":"cap-nosql.html","id":"paquetesCaso","chapter":"Capítulo 6 Gestión de bases de datos NoSQL","heading":"6.6.2 Plataforma tecnológica para el caso práctico","text":"Para la realización del caso práctico se utiliza Atlas[^nosql-atlas], un servicio en la nube gratuito para manejar bases de datos MongoDB. Atlas es fácil de configurar y tiene conjuntos de datos de muestra para ejemplos de R con MongoDB. Puede cargar conjuntos de datos de muestra usando el botón “…” junto al de colecciones en la página de su cluster (el servidor). obstante, aunque en Atlas se puede crear un cluster específico, en este ejemplo práctico se parte de uno ya creado.Adicionalmente, modo de apoyo, se recomienda utilizar una herramienta cliente para conectarse MongoDB y poder inspeccionar los datos contenidos. Es muy útil para realizar las consultas. Puede considerarse la herramienta para la gestión de la instalación de MongoDB[^nosql-mongodb]. Además, si se crea el propio cluster en Atlas, éste tiene una interfaz amigable para inspeccionar los datos.Como complemento estas funciones, existe documentación de las colecciones de documentos y la información contenida en esta base de datos[^nosql-ejem] de ejemplo.Para la resolución de ejercicios puede consultarse el Manual de MongoDB[^nosql-mongo2], que contiene ejemplos y explicaciones de la sintáxis de MongoDB.","code":""},{"path":"cap-nosql.html","id":"paquetes-r-utilizados","chapter":"Capítulo 6 Gestión de bases de datos NoSQL","heading":"6.6.2.1 Paquetes R utilizados","text":"El controlador R preferido por la comunidad de MongoDB es mongolite; por ello se utiliza en los siguientes ejemplos. Es rápido y tiene una sintaxis similar la del shell MongoDB. Mongolite es el controlador R para MongoDB más reciente y puede realizar operaciones de indexación, canalizaciones de agregación, cifrado TLS (transport layer security) y autenticación SASL (simple authentication security layer), entre otras. Está basado en los paquetes jsonlite y mongo-c-driver. Se puede instalar desde CRAN o desde RStudio. Existen otros paquetes para conectar MongoDB y R, como, por ejemplo RMongo y rmongodb, aunque han estado muy activos GitHub últimamente, por lo que carecen de soporte.Para poder usar el paquete mongolite hay que instalarlo previamente, además de importar la librería con el siguiente comando.","code":"\nlibrary('mongolite')"},{"path":"cap-nosql.html","id":"conexionMongo","chapter":"Capítulo 6 Gestión de bases de datos NoSQL","heading":"6.6.3 Conexión y acceso a MongoDB desde R","text":"La variable cadena_conexion representa la cadena de conexión MongoDB en Atlas. Si se desea, se puede sustituir por otro servidor o cluster en Atlas, o por un servidor local.En opciones de seguridad se establece la validación de certificados SSL (secure sockets layer), para evitar que exista un error de conexión Atlas.NotaEn entornos reales de producción está desaconsejado evitar esta comprobación por razones de seguridad.Después de establecer la conexión MongoDB se recupera la colección trips (colección de viajes de la base de datos sample_training) con la función mongo() en código R. Esta colección contiene datos de viajes realizados por los usuarios de un servicio de bicicletas compartidas con sede en la ciudad de Nueva York.Se puede verificar que el código está conectado la colección mediante la consulta del número total de documentos en esta colección. Para hacerlo, se usa la función count().","code":"\ncadena_conexion <- \"mongodb+srv://user01:user01@cluster0.mcblc3z.mongodb.net/test\"\nopciones_conexion <- ssl_options(weak_cert_validation = T)\nviajes <- mongo(collection = \"trips\", db = \"sample_training\", url = cadena_conexion, options = opciones_conexion)\nviajes$count()\n#> [1] 10000"},{"path":"cap-nosql.html","id":"consultaViajes","chapter":"Capítulo 6 Gestión de bases de datos NoSQL","heading":"6.6.4 Obtención de datos en R desde MongoDB","text":"Ahora que hay una conexión establecida con la base de datos, se pueden leer los datos de la misma para ser procesados por R. Para recuperar datos de MongoDB y mostrarlos se puede usar la interfaz de usuario de Atlas (en este caso para ver los documentos de trip_collection). Se puede obtener cualquier documento de muestra de la colección usando la función $iterate().$one(), pudiéndose, así, examinar la estructura de los datos de la colección.Una vez se conoce la estructura de los documentos, se pueden realizar consultas más avanzadas, como buscar los tres viajes más largos y luego enumerar la duración en orden descendente. La consulta propuesta utiliza operadores de clasificación y límite54 para producir este conjunto de resultados.","code":"\nviajes$find(sort = '{\"tripduration\" : -1}', limit = 3)\n#>   tripduration start station id          start station name end station id\n#> 1       326222              391         Clark St & Henry St            310\n#> 2       279620             3165 Central Park West & W 72 St           3019\n#> 3       173357             3155     Lexington Ave & E 63 St           3083\n#>           end station name bikeid   usertype birth year\n#> 1      State St & Smith St  18591 Subscriber       1979\n#> 2        NYCBS Depot - DEL  17547   Customer           \n#> 3 Bushwick Ave & Powers St  15881   Customer           \n#>   start station location.type start station location.coordinates\n#> 1                       Point                -73.99345, 40.69760\n#> 2                       Point                -73.97621, 40.77579\n#> 3                       Point                -73.96649, 40.76440\n#>   end station location.type end station location.coordinates\n#> 1                     Point              -73.98913, 40.68927\n#> 2                     Point              -73.98193, 40.71663\n#> 3                     Point              -73.94100, 40.71248\n#>            start time           stop time\n#> 1 2016-01-01 01:58:20 2016-01-04 20:35:23\n#> 2 2016-01-02 17:07:26 2016-01-05 22:47:46\n#> 3 2016-01-02 15:25:36 2016-01-04 15:34:53"},{"path":"cap-nosql.html","id":"analisisViajes","chapter":"Capítulo 6 Gestión de bases de datos NoSQL","heading":"6.6.5 Análisis de datos de MongoDB en R","text":"Para analizar datos de MongoDB en R con más detalle, se puede usar el marco de agregación de datos de MongoDB . Este marco permite los operadores crear canalizaciones de agregación que ayuden obtener los datos con una sola consulta.Para saber cuántos suscriptores realizaron viajes de una duración mayor que 240 segundos y regresaron la misma estación donde comenzaron, la consulta usa la cláusula $expr, que compara dos campos en el mismo documento.Combinando estos operadores con código R, también se puede ver, por ejemplo, qué tipo de usuarios es más común: suscriptores o clientes únicos. Para ello, se pueden agrupar usuarios por el campo usertype, que define el tipo de usuario.Para comparar los resultados, se pueden visualizar (véase Fig. 6.4). Es conveniente convertir los datos obtenidos de mongolite en un data.frame(marco de datos) y, por ejemplo, usar el paquete ggplot2, para trazar estos datos.\nFigura 6.4: Suscripción por tipo de usuario.\n","code":"\nquery <- viajes$find('{\"usertype\":\"Subscriber\",\"tripduration\":{\"$gt\":240},\"$expr\": {\"$eq\": [\"$start station name\",\"$end station name\"]}}')\ntipos_usuario <- viajes$aggregate('[{\"$group\":{\"_id\":\"$usertype\", \"Count\": {\"$sum\":1}}}]')\nlibrary(\"ggplot2\")\n\ndf <- as.data.frame(tipos_usuario)\n\nggplot(df, aes(x = reorder(`_id`, Count), y = Count)) +\n  geom_bar(stat = \"identity\", color = \"blue\", fill = \"green\") +\n  geom_text(aes(label = Count)) +\n  coord_flip() +\n  xlab(\"Tipo de usuario\")"},{"path":"cap-nosql.html","id":"resumen-5","chapter":"Capítulo 6 Gestión de bases de datos NoSQL","heading":"Resumen","text":"En este capítulo se presenta el concepto de big data, por qué surje y qué aporta respecto soluciones previas.En particular, se discute qué son las bases de datos NoSQL y cuáles son sus diferencias con las bases de datos relacionales (más tradicionales). Posteriormente, se explican algunas limitaciones de las bases de datos NoSQL. Finalmente, como ejemplo, se muestra la integración de datos en R desde MongoDB, explicando cómo acceder este tipo de datos y cómo analizarlos en el caso concreto de una base de datos documental como MongoDB.","code":""},{"path":"DGDQM.html","id":"DGDQM","chapter":"Capítulo 7 Gobierno, gestión y calidad del dato","heading":"Capítulo 7 Gobierno, gestión y calidad del dato","text":"Ismael Caballero\\(^{}\\), Ricardo Pérez del Castillo\\(^{}\\), y Fernando Gualo\\(^{,b}\\)\\(^{}\\)Universidad de Castilla-La Mancha\n\\(^{b}\\)DQTeam SL","code":""},{"path":"DGDQM.html","id":"introducción-2","chapter":"Capítulo 7 Gobierno, gestión y calidad del dato","heading":"7.1 Introducción","text":" Los datos se han convertido en un elemento vital para el desarrollo económico de las organizaciones, ya que permiten una mayor eficiencia en el uso de los recursos y un aumento de su productividad.\nTanto es así, que la Unión Europea establece, través de la Estrategia Europea de Datos55, que en 2030 se establecerá un Espacio Único Europeo de Datos para fomentar un ecosistema con nuevos productos y servicios basados en los datos.\nPara ello, en esta Estrategia Europea de Datos -que prevé un incremento del 530% del volumen global de datos- se reclama la necesidad de implantar mecanismos de gobierno del dato través de políticas y directrices consensuadas alto nivel para alcanzar los objetivos de la estrategia organizacional y satisfacer tanto aspectos regulatorios genéricos (como las leyes europeas General Data Protection Regulacy (GPDR)56 o Data Governance Act57, o las españolas Esquema Nacional de Seguridad -ENS-58, o Esquema Nacional de Interoperabilidad -ENI-59) como aspectos sectoriales específicos (como Solvencia II60 para el sector seguros, o Basilea III61 para el sector financiero).Estos mecanismos de gobierno del dato deben abordar aspectos verticales relacionados con la adquisición, tenencia, compartición, uso y explotación de los datos en los procesos de negocio, abordando la vez aspectos transversales relacionados con su gestión: calidad de los datos, aspectos éticos y privacidad, interoperabilidad, gestión del conocimiento y el control sobre los activos de datos través de las políticas correspondientes, y despliegue de estructuras organizativas con una conveniente separación de los roles de gobierno del dato de los de gestión del dato (ISO 2017).\nPor tanto, puede decirse que el gobierno del dato marca la dirección de cómo la organización debe realizar la gestión del dato para alcanzar los objetivos establecidos en su(s) estrategia(s) del dato. Esto se consigue mediante la definición e implementación de una serie de políticas del dato.","code":""},{"path":"DGDQM.html","id":"concepto-de-gobierno-del-dato","chapter":"Capítulo 7 Gobierno, gestión y calidad del dato","heading":"7.2 Concepto de gobierno del dato","text":"El gobierno de los datos se ha convertido en un habilitador de la economía de los datos (Engels 2019a; Weber, Otto, Österle 2009), así como también en un pilar básico para la mejora de la transparencia y eficiencia de las administraciones públicas (OECD 2019; Osimo et al. 2020; Osorio-Sanabria, Amaya-Fernández, González-Zabala 2020). Aunque existen algunas aproximaciones académicas y profesionales al gobierno del dato, hay una definición consensuada de este concepto que permita aunar las distintas visiones. obstante, la definición más aceptada de gobierno del dato es la propuesta por DAMA en DMBoK2 (DAMA 2017): “colección de prácticas y procesos que ayudan asegurar la gestión formal de los activos de datos dentro de una organización mediante el ejercicio de autoridad, control y toma de decisiones compartidas, planificadas, monitorizadas y forzadas”.\nTeniendo en cuenta los matices que introduce, es también interesante la lectura de la propuesta por Soares (2015), que define gobierno del dato como “la formulación de políticas para optimizar, conseguir los niveles adecuados de seguridad y protección, y potenciar los datos como activos organizacionales mediante la alineación de los objetivos de diferentes funciones organizacionales; por su naturaleza, el gobierno del dato requiere cooperación interdepartamental para entregar oportuna y fielmente datos con el máximo valor para la toma de decisiones en la organización”.De alguna manera, se podría entender que gobernar los datos implica el diseño, implementación y mantenimiento de un sistema de gobierno del dato. El gobierno del dato tiene tres características destacables (Caballero, Piattini, Gualo 2022):Está dirigido por el valor de los datos: pues el principal objetivo del gobierno del dato es asegurar que los datos son tratados como activos de datos y que la gestión y uso que se hace de ellos permite alcanzar el máximo valor organizacional que se espera de ellos. Por tanto, todas las acciones están encaminadas la obtención de este valor organizacional.Está centrado en la arquitectura empresarial: para poder gobernar los datos adecuadamente, es preciso revisar o incorporar ciertos componentes la arquitectura empresarial tal que, o bien den el soporte adecuado, o bien forme parte del resultado del gobierno del dato.Es iterativo e incremental: pues para alcanzar un estado en el que se considere que los activos de datos están perfectamente gobernados es preciso desarrollar un conjunto de programas de gobierno del dato. Así, través de la ejecución de diferentes proyectos relacionados entre sí, se conseguirá el desarrollo y la puesta en valor de los artefactos típicos de un sistema de gobierno del dato (véase sección 7.2.2). Esto sólo se puede conseguir en incrementos relevantes (p.ej. la creación de más componentes del sistema de gobierno del dato o la inclusión de nuevos datos ser gobernados en el alcance de gobierno del dato).Un aspecto interesante es que, medida que se avanza en la ejecución de estos programas de gobierno del dato, más sensible se vuelve la organización hacia la importancia de los datos, más aprende gestionarlos y gobernarlos, y más amplio es el alcance del gobierno del dato; en definitiva, se puede decir que más madura se vuelve la organización en lo que se refiere al gobierno y la gestión del dato.","code":""},{"path":"DGDQM.html","id":"beneficiosDG","chapter":"Capítulo 7 Gobierno, gestión y calidad del dato","heading":"7.2.1 Beneficios del gobierno del dato","text":"Cuando se desarrolla un sistema de gobierno del dato, con cada incremento del sistema se espera conseguir uno o una combinación de los siguientes beneficios (ISACA 2019):Alineamiento estratégico: optimización del valor organizacional de los datos mediante el alineamiento con la estrategia organizacional.Realización de beneficios: aseguramiento de que los datos son entregados en condiciones aceptables los diferentes consumidores del dato.Optimización de riesgos: paliar o minimizar, dentro de la propensión al riesgo de la organización, los riesgos relacionados con la adquisición, uso y explotación de los datos, asegurando el cumplimiento de la normativa interna y regulatoria.Optimización de recursos: optimización de las capacidades de los recursos humanos y tecnológicos necesarios, y que son utilizados para dar un soporte más eficiente las distintas operaciones involucradas en la gestión del dato, minimizando el desperdicio de recursos al gestionar, usar y explotar los datos.Estos beneficios deben especificarse como parte de la estrategia del dato de la organización.\nAsí, por ejemplo, una organización que considere realizar un alineamiento estratégico y una optimización de riesgos, estará desarrollando una estrategia defensiva que debería implementarse través de un gobierno técnico; por otro lado, si una organización quiere, por ejemplo, maximizar la realización de beneficios, podría considerarse que estaría trazando una estrategia ofensiva que se podría materializar mediante un gobierno para el valor.","code":""},{"path":"DGDQM.html","id":"artefactosDG","chapter":"Capítulo 7 Gobierno, gestión y calidad del dato","heading":"7.2.2 Componentes de un sistema de gobierno del dato","text":"\nPara poder obtener los beneficios descritos anteriormente, las organizaciones deben realizar esfuerzos para implantar los mecanismos de gobierno del dato reclamados en la Estrategia Europea de Datos, particularizándolos su realidad y en función de su madurez.\nEstos mecanismos implican el desarrollo de un sistema de gobierno del dato, que involucra la creación o mantenimiento de forma interrelacionada y sujeto las restricciones correspondientes de una serie de componentes.\nDependiendo de si se tiene un gobierno técnico o un gobierno para el valor, la creación y uso de los distintos tipos de componentes será más o menos intensiva.\nEstos componentes son los siguientes (Caballero, Piattini, Gualo 2022):Procesos de gestión del dato, gestión de calidad del dato y gobierno del dato, que se refieren al diseño y posterior particularización e implantación de las buenas prácticas relacionadas con las tareas típicas de los datos nivel de las correspondientes disciplinas. Es posible obtener descripciones genéricas de estos procesos en diferentes modelos de referencia de procesos, tales como Data Maturity Model (DMM)(Mecca, Young, Halcomb 2014), Data Management Capability Assessment Model (DCAM)(Council 2020) o el Modelo Alarcos de Madurez de Datos (MAMD)(Caballero, Piattini, Rodríguez 2023)Procesos de gestión del dato, gestión de calidad del dato y gobierno del dato, que se refieren al diseño y posterior particularización e implantación de las buenas prácticas relacionadas con las tareas típicas de los datos nivel de las correspondientes disciplinas. Es posible obtener descripciones genéricas de estos procesos en diferentes modelos de referencia de procesos, tales como Data Maturity Model (DMM)(Mecca, Young, Halcomb 2014), Data Management Capability Assessment Model (DCAM)(Council 2020) o el Modelo Alarcos de Madurez de Datos (MAMD)(Caballero, Piattini, Rodríguez 2023)Estructuras organizacionales, que deben recoger las cadenas de responsabilidades y rendición de cuentas, haciendo una adecuada separación entre las responsabilidades propias del gobierno del dato y aquellas propias de la gestión del dato y de su calidad. Los roles que deben asumir estas responsabilidades son típicamente el de Chief Data Officer (Soares 2015; Treder 2020), con un punto de vista más ejecutivo/estratégico, y los de data stewards (Plotkin 2020), desde una perspectiva más táctica/operativa.\nEstructuras organizacionales, que deben recoger las cadenas de responsabilidades y rendición de cuentas, haciendo una adecuada separación entre las responsabilidades propias del gobierno del dato y aquellas propias de la gestión del dato y de su calidad. Los roles que deben asumir estas responsabilidades son típicamente el de Chief Data Officer (Soares 2015; Treder 2020), con un punto de vista más ejecutivo/estratégico, y los de data stewards (Plotkin 2020), desde una perspectiva más táctica/operativa.\nPrincipios, políticas y marcos de referencia, que deberían incluir todos los principios rectores en los que se basará el uso de los datos (tales como los Generaly Accepted Information Principles listados en Ladley (2019)), las directrices o políticas y los controles correspondientes asociados necesarios para modelar y gestionar el valor de los datos, el riesgo asumir y las restricciones considerar según se describe en ISO/IEC 38505-2 (ISO 2018b).Principios, políticas y marcos de referencia, que deberían incluir todos los principios rectores en los que se basará el uso de los datos (tales como los Generaly Accepted Information Principles listados en Ladley (2019)), las directrices o políticas y los controles correspondientes asociados necesarios para modelar y gestionar el valor de los datos, el riesgo asumir y las restricciones considerar según se describe en ISO/IEC 38505-2 (ISO 2018b). Datos e Información, que se deben gobernar como las descripciones necesarias través de los metadatos correspondientes. Para la parte del dato es fundamental poder establecer una adecuada arquitectura del dato con los correspondientes modelos que recojan la semántica del entorno de la organización y reflejen el cómo ésta usa los datos para desarrollar su actividad organizacional y/o económica. Para dar soporte al uso correspondiente, deben generarse y mantenerse los metadatos correspondientes, que pueden ser de varios tipos (DAMA 2017):\nmetadatos de negocio, recogidos típicamente en glosario de negocio y que describen la relación del dato con el negocio;\nmetadatos técnicos, recogidos habitualmente en los catálogos de datos , que describen detalles técnicos de los datos; y\nmetadatos operacionales, recogidos típicamente en los diccionarios de datos , que recogen aspectos relacionados con el procesamiento y acceso los datos. Es importante que todos estos metadatos estén reconciliados convenientemente entre ellos, ya que su visión conjunta permitirá una descripción adecuada de los datos bajo el gobierno del dato, y si esta descripción es suficiente será posible usarlos con las suficientes garantías de éxitos.\n Datos e Información, que se deben gobernar como las descripciones necesarias través de los metadatos correspondientes. Para la parte del dato es fundamental poder establecer una adecuada arquitectura del dato con los correspondientes modelos que recojan la semántica del entorno de la organización y reflejen el cómo ésta usa los datos para desarrollar su actividad organizacional y/o económica. Para dar soporte al uso correspondiente, deben generarse y mantenerse los metadatos correspondientes, que pueden ser de varios tipos (DAMA 2017):metadatos de negocio, recogidos típicamente en glosario de negocio y que describen la relación del dato con el negocio;metadatos técnicos, recogidos habitualmente en los catálogos de datos , que describen detalles técnicos de los datos; ymetadatos operacionales, recogidos típicamente en los diccionarios de datos , que recogen aspectos relacionados con el procesamiento y acceso los datos. Es importante que todos estos metadatos estén reconciliados convenientemente entre ellos, ya que su visión conjunta permitirá una descripción adecuada de los datos bajo el gobierno del dato, y si esta descripción es suficiente será posible usarlos con las suficientes garantías de éxitos.Cultura, ética y comportamiento, cuyo objetivo es identificar aquellos aspectos culturales y éticos que deben regular la forma en la que la organización abordará las tareas relacionadas con los datos para que estos tengan el valor organizacional deseado (Harrison et al. 2019).Cultura, ética y comportamiento, cuyo objetivo es identificar aquellos aspectos culturales y éticos que deben regular la forma en la que la organización abordará las tareas relacionadas con los datos para que estos tengan el valor organizacional deseado (Harrison et al. 2019).Personas, habilidades y competencias, componente que trata de organizar los roles que deben asumir las diferentes responsabilidades relacionadas con los diferentes procesos; también debe enfocarse en asegurar que esos roles tienen los conocimientos, habilidades y competencias necesarias para abordar las tareas asociadas mediante los programas formativos correspondientes; finalmente, este artefacto incluye asegurar que la organización tiene planes de contingencia ante la eventual rotación funcional de los recursos humanos dedicados las responsabilidades relacionadas con los datos (Plotkin 2020).Personas, habilidades y competencias, componente que trata de organizar los roles que deben asumir las diferentes responsabilidades relacionadas con los diferentes procesos; también debe enfocarse en asegurar que esos roles tienen los conocimientos, habilidades y competencias necesarias para abordar las tareas asociadas mediante los programas formativos correspondientes; finalmente, este artefacto incluye asegurar que la organización tiene planes de contingencia ante la eventual rotación funcional de los recursos humanos dedicados las responsabilidades relacionadas con los datos (Plotkin 2020).Servicios, infraestructuras y aplicaciones: este componente aborda todo lo relacionado con las tecnologías y sistemas de información para dar soporte las diferentes actividades de los procesos de gestión del dato, así como de la gestión de su calidad y de su gobierno.Servicios, infraestructuras y aplicaciones: este componente aborda todo lo relacionado con las tecnologías y sistemas de información para dar soporte las diferentes actividades de los procesos de gestión del dato, así como de la gestión de su calidad y de su gobierno.","code":""},{"path":"DGDQM.html","id":"marcos-y-metodologías-de-gobierno-del-dato","chapter":"Capítulo 7 Gobierno, gestión y calidad del dato","heading":"7.3 Marcos y metodologías de gobierno del dato","text":"En la literatura, tanto académica como profesional, existen algunas propuestas de creación de sistemas de gobierno del dato.\nEs interesante resaltar que en el ámbito académico se han desarrollado algunas revisiones sistemáticas de literatura científica para identificar los componentes del gobierno del dato, aunque de forma general, y salvo algunas referencias, se mantienen desconectados de las propuestas profesionales.\nTambién es importante mencionar que salvo COBIT 2019 (Control Objectives Information related Technology, Objetivos de control para la información y tecnologías relacionadas)62, la inmensa mayoría de estos marcos identifican explícitamente el concepto de “sistema de gobierno del dato”, sino que se establece bajo un paraguas más genérico de “gobierno del dato”. En cualquier caso, la idea es la misma.En los siguientes párrafos se resumen los aspectos más importantes de los marcos más relevantes, que pueden servir para que el lector encuentre el que mejor se adapta su circunstancia profesional:Abraham, Schneider, Brocke (2019) analizan la literatura para identificar los elementos de un marco de trabajo teórico que se clasifican en torno cuatro áreas del gobierno del dato: (1) alcance organizacional (aspectos intra e inter organizacionales), (2) alcance de los datos (datos tradicionales vs. big data), (3) alcance del dominio (calidad del dato, seguridad del dato, arquitectura del dato, ciclo de vida, metadatos, almacenamiento e infraestructura del dato), y (4) mecanismos de gobierno (estructurales, procedimentales y relacionales).Al-Ruithe, Benkhelifa, Hameed (2019) identifican, también través de una revisión sistemática de literatura, las áreas o retos del gobierno del dato donde merece la pena investigar. Éstas son tecnología (seguridad, privacidad, disponibilidad, rendimiento, clasificación de los datos y migración del dato), legalidad, y aspectos organizacionales o del negocio.Brous, Herder, Janssen (2016) derivan, de nuevo basándose en una revisión sistemática de la literatura, los principios para desarrollar de forma efectiva estrategias y aproximaciones para el gobierno del dato, que agrupan en torno cuatro conceptos fundamentales: (1) organización, (2) alineamiento, (3) cumplimiento y (4) entendimiento común de los datos.Carruthers Jackson (2020) identifican los posibles elementos que deben contemplarse en la transformación digital, la cual debe apoyarse en el gobierno del dato. Estos elementos (personas, datos, procesos, tecnologías) son representados mediante un triángulo, en cuyo centro están los datos. En la continuación de la obra de estos autores presentada en Jackson Carruthers (2019), se propone un modelo de transformación, convenientemente soportado en el gobierno del dato. DCAM (Data Management Capability Assessment Model) (Council 2020) es un modelo de referencia para la evaluación de la capacidad de gestión del dato desarrollado por el EDM Council. El modelo tiene ocho componentes agrupados en cuatro niveles (1) fundamentos (estrategia del dato y casos de negocio; programas de gestión del dato y financiación), (2) ejecución (arquitectura del dato y de negocio; arquitectura del dato y de tecnología; gestión de calidad del dato; gobierno del dato), (3) colaboración (entorno de control del dato) y (4) formalización del diseño e implementación de las actividades analíticas. DMBoKv2 (Data Management Body Knowledge) (DAMA 2017) es un marco de referencia de procesos desarrollado por DAMA que posiciona el gobierno del dato como la función que guía el resto de las acciones relacionadas con la gestión del dato. Identifica una serie de elementos que deben generarse partir del gobierno del dato: estrategia de gobierno del dato; estrategia del dato; hoja de ruta del gobierno del dato; principios de gobierno del dato, políticas de gobierno del dato, procesos; marco operativo de gobierno del dato; hoja de ruta y guía de implementación; plan de operaciones; glosario de términos; plan de operaciones; cuadro de mando de gobierno del dato; etc.Eryurek et al. (2021) identifica los “ingredientes” propios de un sistema de gobierno del dato (herramientas; personas y procesos; cultura del dato), así como las áreas en las que debería enfocarse el gobierno de datos lo largo del ciclo de vida de los datos (descubrimiento y limpieza del dato; gestión del dato; políticas de privacidad, seguridad y acceso).COBIT 2019 (ISACA 2019) identifica para el sistema de gobierno de tecnologías y de información los siguientes componentes: procesos; estructuras organizacionales; principios, políticas y marcos de referencia; información; cultura, ética y comportamiento; personas, habilidades y competencias; servicios, infraestructuras y aplicaciones.ISO 38505-1 (ISO 2017) e ISO 38505-2 (ISO 2018b) muestran los aspectos claves del gobierno del dato (valor de los datos, riesgo, y restricciones) e introducen seis principios (responsabilidad, estrategia, adquisición, rendimiento, cumplimiento y comportamiento humano). Identifica una serie de procesos (evaluar, dirigir, monitorizar) como áreas propias de actuación del gobierno del dato, distinguiéndolos de las operaciones propias de la gestión del dato y estableciendo las correspondientes relaciones con ellas. Sin embargo, describen actividades específicas para la creación de sistemas de gobierno del dato.Khatri Brown (2010) aducen que el gobierno del dato implica tomar decisiones sobre activos claves de datos en varios dominios de decisión (principios, gestión de calidad del dato, metadatos, acceso datos y ciclo de vida de los mismo).Janssen et al. (2020) exploran las capacidades de gobierno del dato necesarias para que las organizaciones dirigidas por datos puedan extraer el máximo beneficio de los sistemas algorítmicos basados en Big Data (Big Data Algorithmic Systems) y proponen un marco para la creación del gobierno del dato que permita optimizar estos sistemas.Ladley (2019) presenta un marco de gobierno del dato basado en cinco pilares: compromiso, estrategia, arquitectura y diseño, implementación y operación, y, por último, gestión del cambio.Lillie Eybers (2019) estudian la literatura existente para identificar los aspectos más interesantes sobre (1) el alcance y los constructos más importantes del gobierno y gestión del dato, y (2) las capacidades ágiles requeridas en el gobierno y la gestión de datos.En el llamado proceso Unificado de Gobierno del dato de IBM (Soares 2010) se identifican cinco ingredientes claves que deberían ser cubiertos por cualquier marco de gobierno del dato: (1) fuerte respaldo por parte de la organización con soporte de las TI, (2) centrarse en los elementos de datos críticos, (3) énfasis en los artefactos de datos, (4) alineación en torno métricas y aplicación de políticas, y (5) celebración de las victorias rápidas conseguidas como hitos en una hoja de ruta largo plazo.La Organización para la Cooperacióny el Desarrollo Económicos (Organisation Economic Co-operation Development (OECD), en su informe sobre gobierno del dato para administraciones públicas (OECD 2019), recoge las mejores prácticas llevadas cabo por diferentes administraciones de los países que la componen en lo que se refiere transparencia del gobierno del dato e incremento del valor de la información disponible sobre la ciudadanía de cara una mejor prestación de servicios públicos.Treder (2020) identifica algunos componentes específicos que debería tener un sistema de gobierno del dato (cadenas de valor; estrategia de dato; procesos de datos; descripción de los roles y sus responsabilidades; gestión del equipo de la oficina del dato), así como las áreas en las que debe enfocarse el gobierno del dato (casos de negocio; aspectos éticos y cumplimiento; gestión y análisis del dato).\nmodo de ejemplo, se van introducir más detalles sobre un marco de referencia basado en estándares internacionales ISO: el Modelo Alarcos de Madurez de Datos (MAMD) v4.0 (Caballero, Piattini, Rodríguez 2023).\nMAMDv4.0 es un marco de trabajo que se usa para la evaluación y mejora de la capacidad de los procesos de la organización relacionados con la gestión, la gestión de la calidad, y el gobierno del dato.\nTiene dos componentes principales:Un modelo de referencia de procesos (MRP), que contiene una descripción de los procesos de gestión del dato, de gestión de calidad del dato y de gobierno del dato. Está alineado con los principales estándares en el área (ISO 8000-61 (ISO 2016), e ISO/IEC 38505-2 (ISO 2018b)), así como con las buenas prácticas de otros modelos como DAMA, DMM o COBIT 2019 (Véase Fig.7.1).\nFigura 7.1: Modelo de Referencia de Procesos de MAMD; DM: gestión del dato; DQM: gestión de calidad del dato; DG: gobierno del dato\nEl modelo de evaluación de procesos (MEP), que sigue las directrices de evaluación y los niveles de capacidad y madurez descritas por ISO/IEC 33000 y adaptadas la evaluación de procesos de datos conforme al modelo de madurez propuesto en ISO 8000-62 (ISO 2018a) (véase Fig. 7.2).\nFigura 7.2: Modelo de Madurez Organizacional de MAMD; N: nivel; DM: gestión del dato; DQM: gestión de calidad del dato; DG: gobierno del dato\n","code":""},{"path":"DGDQM.html","id":"gestión-de-calidad-del-dato","chapter":"Capítulo 7 Gobierno, gestión y calidad del dato","heading":"7.4 Gestión de calidad del dato","text":"Los datos con niveles inadecuados de calidad acaban teniendo un impacto negativo para las organizaciones, bien en términos económicos, bien en términos de reputación (Redman 2016). Por eso es importante que las organizaciones cuiden del nivel de calidad de sus datos y se aseguren que dicho nivel permanece dentro de los permitidos para que la ejecución de los procesos de negocio se haga dentro del margen de riesgo de la organización.Se dice que un conjunto de datos tiene calidad cuando sirve para el propósito para el que fue recogido (fitness use) (Diane M. Strong, Lee, Wang 1997).\nPara determinar si un conjunto de datos tiene calidad suficiente para dicho propósito, es preciso identificar y seleccionar un conjunto de criterios (llamados en la literatura dimensiones (Wang 1998), o características de calidad del dato (ISO/IEC 2008a)) que permitan determinar si dicho conjunto cumple los requisitos de calidad que exige el usuario de tales datos.\nAl conjunto de dimensiones o características de calidad del dato seleccionadas se le denomina modelo de calidad del dato.\nLa Tab. ?? introduce una descripción de las características de calidad del dato incluidas en el estándar ISO/IEC 25012.Como puede observarse, estas características se clasifican en dos grandes bloques: inherentes y dependientes del sistema.\nLas inherentes se refieren al grado con el que las características de calidad de los datos tienen un potencial intrínseco para satisfacer las necesidades establecidas y necesarias cuando los datos son utilizados bajo condiciones específicas; las dependientes del sistema, por otro lado, permiten determinar el grado con el que la calidad del dato es alcanzada y preservada través de un sistema informático cuando los datos son utilizados bajo condiciones específicas.Para ilustrar el significado de algunas de estas características (p.ej. exactitud, completitud, o consistencia), continuación se introducen algunos ejemplos:Como ejemplo de nivel inadecuado de exactitud sintáctica podría ponerse el hecho de que el atributo Nombre de la entidad Persona toma un valor o dato “Marja” (existente en los datos de referencia de nombre) en lugar de “María” (que sí que está incluido).El hecho de que el atributo Nombre de la entidad Persona tome el valor de “George” en vez de “Jorge” para almacenar datos de la Persona llamada realmente “Jorge” es un ejemplo de nivel inadecuado de exactitud semántica. Ambos valores son sintácticamente correctos, pero George es otra persona distinta Jorge, y quien capturó y guardó los datos, simplemente se equivocó de persona.Supóngase que, para una determinada aplicación, se necesita recoger valores (o datos) para los siguientes atributos de una entidad Persona: DNI, Nombre, Apellido1, y Apellido2 para ser usados adecuadamente en un contexto de uso. En caso de faltar alguno de ellos (nivel inadecuado de completitud), podría ocurrir que los datos de la persona se pudieran utilizar; incluso, podrían faltar algunos atributos más, como por ejemplo email, pero si es relevante para la aplicación, habría ese problema de completitud.Un ejemplo de falta de consistencia puede darse, por ejemplo, cuando el valor (o dato) del atributo FechaNacimiento de la entidad Persona es posterior la fecha de hoy.Diferentes autores han proporcionado diferentes mecanismos para medir y evaluar la calidad de los datos usando las dimensiones o características de calidad seleccionadas.\nAunque para dar soporte este proceso, se han propuesto numerosas metodologías de evaluación (Batini, Scannapieco, et al. 2016), el principal problema de estas contribuciones es que, normalmente, se han realizado ad hoc y permiten ni generalizar los resultados obtenidos ni compararlos con los obtenidos por otras organizaciones (D. Loshin 2011).Para paliar estos problemas, se han desarrollado estándares que recogen los conocimientos y principios básicos comunes para medir y evaluar la calidad de los datos.\nEjemplos de estos estándares pueden ser la mencionada ISO/IEC 25012 (ISO/IEC 2008a); la ISO 8000-8 (ISO/IEC 2015) que recogen características de calidad; la ISO/IEC 25024 (ISO/IEC 2008b) que recoge aspectos específicos de cómo llevar cabo las mediciones de las características, o la ISO/IEC 25040 (ISO/IEC 2011), que proporciona una metodología de evaluación de calidad del software que puede ser adaptada la evaluación rigurosa y sistemática de la calidad de los datos.\nEn este punto es necesario introducir la principal diferencia entre medir y evaluar la calidad: medir consiste en determinar la cantidad de calidad del dato que tiene un conjunto de datos; mientras que evaluar implica determinar si, de acuerdo al nivel de riesgo que asume la organización, la cantidad de calidad del dato medida es suficiente y adecuada para usar los datos en el contexto de uso establecido para esos datos.\nLa evaluación de calidad del dato requiere primero medir la calidad.\nY para medir la calidad, primero deben definirse procedimientos de medición.En ese sentido, ISO 25024 (ISO/IEC 2008b) proporciona una serie de propiedades medibles para cada una de las características presentadas en la Tab. ??; además, para cada una de estas propiedades medibles, el estándar, proporciona un método de medición genérico, que permitirá, convenientemente particularizado, medir dichas propiedades y luego agruparlas para determinar el valor de la característica de calidad del dato.\nEn la Fig. 7.3 se muestran las propiedades medibles para las características de calidad identificadas como inherentes (véase Tab. ??).\nFigura 7.3: Algunas propiedades de las características inherentes de calidad del dato\nUna de las ventajas de usar estas propiedades medibles es que, en caso de niveles inadecuados de calidad de datos, es posible identificar mejor qué está causando que esto ocurra y por tanto, es más fácil actuar directamente sobre dichas causas.modo de ejemplo, supóngase quese quiere medir para medir el grado de exactitud de un conjunto de datos. Para ello se considera necesario medir las propiedades “exactitud sintáctica”, “exactitud semántica” y “rango de exactitud”; con lo resultados, habrá que hacer algún tipo de agrupación que tenga en cuenta la importancia o peso relativo de cada una de estas propiedades la hora de evaluar la exactitud. Supóngase que una organización, determina que la mejor forma de hacerlo es mediante una media aritmética ponderada de los resultados de la medición de las tres propiedades medibles. En base su nivel de riesgo para un determinado proceso de negocio, supóngase que la organziación considera que para una determinada aplicación, puede asignar, para la media aritmética ponderada, los siguientes pesos: 0,4, 0,4 y 0,2 para la exactitud semántica, para la exactitud sintáctica y para el rango de exactitud respectivamente.la hora de medir las propiedades medibles correspondientes las características de calidad del dato, es interesante tener en cuenta que la ISO/IEC 25024 proporciona procedimientos de medición cuya implementación depende fuertemente de la naturaleza de la propiedad y del objeto cuya calidad quiere medirse. La medición de algunas de estas propiedades implica contar el porcentaje de registros que violan las reglas de negocio que regulan la adecuación al uso de los datos en un contexto determinado (David Loshin 2002). Sin embargo, la hora de la medición, uno de los ejercicios más difíciles es recolectar y validar las reglas de negocio específicas que rigen la validez de los datos (Caballero et al. 2022).\nPara el ejemplo propuesto, imagínese que si se pretende medir el nivel de exactitud sintáctica del dato recogido en el atributo DNI se pudiera usar la siguiente regla de negocio “el DNI tiene que seguir la especificación para DNI, o para el NIE, correspondiente con la expresión regular (d{8})([-Z])”. Habría que comprobar bien manualmente, bien mediante algún tipo de script, cuántos registros verifican la anterior regla de negocio para su atributo DNI. Como se verá en la siguiente subsección, en algunos entornos académicos y profesionales, se utilizan técnicas de perfilado de datos para realizar mediciones.Supóngase que para el ejemplo, y tras haber realizado todas las mediciones de las propiedades, y haberlas agrupado realizando la media aritmética ponderada, es posible obtener un resultado de la medición para la exactitud de 70.Una vez realizada la medición, el siguiente paso es la evaluación propiamente dicha. La evaluación consiste en comparar el resultado obtenido (70 en el ejemplo) con el umbral mínimo de aceptación que depende del nivel de riesgo que decida asumir la organización al utilizar estos datos Redman (2016)). Si, por ejemplo, dicho umbral se hubiese establecido en 75 para el uso concreto que se le va dar estos datos, se concluiría que deberían ser utilizado. Esto significa que los datos puedan usarse en otro contexto en el que, por ejemplo, el valor umbral se estableciese en 65.En algunos contextos de uso, como se verá posteriormente en el Cap. ??, antes de usar los datos se realiza un proceso de preparación de los mismos que tiene como objetivo determinar y adecuar los niveles de calidad al uso que se pretende dar mediante un proceso de evaluación y mejora que se centra en la limpieza de los datos. Normalmente, en este proceso suele recurrirse métodos estadísticos, frente la aproximación basada en la medición de las características de calidad del dato presentada anteriormente.\nSe pierde entonces de alguna manera la capacidad de establecer una dirección más efectiva y sobre todo alineada las necesidades reales de la organización de las operaciones de evaluación y limpieza del dato.Finalmente, es interesante mencionar que, basándose en los estándares ISO/IEC 25012 (ISO/IEC 2008a) e ISO/IEC 25024 (ISO/IEC 2008b) es posible certificar el nivel de calidad del dato de un repositorio de datos.\nGualo et al. (2021) recoge experiencias de medición, evaluación y certificación de calidad del dato.","code":""},{"path":"DGDQM.html","id":"medición-de-calidad-de-datos-vs-perfilado-del-dato","chapter":"Capítulo 7 Gobierno, gestión y calidad del dato","heading":"7.4.1 Medición de calidad de datos vs perfilado del dato","text":"\nEn esta subsección se plantea el perfilado del dato como una técnica base para realizar la medición de las propiedades medibles de las características de calidad del dato o para descubrir nuevas reglas de negocio.\nAbedjan, Golab, Naumann (2015) clasifica los tipos de perfilado del dato en las siguientes categorías:Perfilado de columna simple, que implicaría tareas de identificación de cardinalidades, identificación de patrones y tipos de datos, distribución de valores de datos, clasificación de dominios.Perfilado de columna simple, que implicaría tareas de identificación de cardinalidades, identificación de patrones y tipos de datos, distribución de valores de datos, clasificación de dominios.Perfilado de columnas múltiples, que implicaría tareas de correlación y reglas de asociación, identificación de clusters y outliers, elaboración de resúmenes de datos y bocetos.Perfilado de columnas múltiples, que implicaría tareas de correlación y reglas de asociación, identificación de clusters y outliers, elaboración de resúmenes de datos y bocetos.Perfilado de dependencias, que su vez implica:\nDetección de reglas de unicidad, tales como la identificación de claves, identificación de condiciones e identificación de sinónimos.\nDetección de dependencias de inclusión, que puede abarcar el descubrimiento de claves ajenas o la identificación de dependencias condicionales de inclusión.\nDependencias funcionales, como pueden ser las dependencias condicionales.\nPerfilado de dependencias, que su vez implica:Detección de reglas de unicidad, tales como la identificación de claves, identificación de condiciones e identificación de sinónimos.Detección de dependencias de inclusión, que puede abarcar el descubrimiento de claves ajenas o la identificación de dependencias condicionales de inclusión.Dependencias funcionales, como pueden ser las dependencias condicionales.En R Software se puede utilizar el paquete dlookr Ryu (2022),que contiene algunas funciones interesantes que pueden ayudar realizar determinadas tareas de perfilado.\nPor ejemplo, la función overview() da información general sobre un conjunto de datos; resulta muy interesante la función diagnose(), que proporciona información realizando un perfilado de los valores únicos y los valores únicos de un conjunto de valores.En el siguiente fragmento se muestra el tipo de información proporcionada por diagnose (Madrid_POIS$City_Center): ´variables´ muestra el nombre de los atributos del conjunto de datos (en este caso Londe longitud y Lat de latitud); types muestra el tipo de dato de cada variable; missing_count, missing_percent, unique_count y unique_rate describen respectivamente el conteo de valores nulos, el porcentaje de dichos valores, el número de valores únicos o repetidos y su correspondiente porcentaje; <chr>, <int>, <dbl> se hacen referencia al tipo de dato de cada uno de los parámetros anteriores (carácter, integer, double)Si estas funciones de perfilado proporcionan información suficiente y adecuada, es posible usar los resultados para computar las mediciones de las propiedades medibles.\nPor ejemplo, se puede utilizar el resultado de la columna missing count para calcular el grado de completitud de las variables longitud y latitud, que se pueden establecer en 100% al ser missing count = 0 para las dos variables.\nIncluso se pueden utilizar funciones como plot_na_pareto() para visualizar un gráfico de Pareto mostrando las variables que tienen valores nulos.\nFinalmente, es interesante mencionar que el paquete dlookr incluye funciones como diagnose_paged_report(), que permiten elaborar informes que contienen información sobre las estructuras de datos del conjunto de datos, avisos, descripción de las variables, valores perdidos, valores únicos de las variables categóricas y numéricas, distribuciones de valores nulos y negativos, posibles outliers, … El siguiente fragmento de código explica cómo crear un informe de 15 páginas en formato PDF con toda esa información sobre la variable idealista18::Madrid_POIS$Metro :En ocasiones, y retomando la idea de las reglas de negocio, puede decirse que la información proporcionada por el perfilado del dato, puede usarse para derivar reglas de negocio partir del estado actual de los datos; y, para recoger información que se puede emplear durante el proceso de medición de determinadas características de calidad del dato.\nEn el Cap. ?? se profundizará en el proceso de estudio de dos caracteríticas de calidad del dato: completitud y consistencia.","code":"\nlibrary(\"dlookr\")\nlibrary(\"idealista18\")\ndiagnose(Madrid_POIS$City_Center)\n#diagnose_paged_report(idealista18::Madrid_POIS$Metro)"},{"path":"DGDQM.html","id":"mejora-del-dato","chapter":"Capítulo 7 Gobierno, gestión y calidad del dato","heading":"7.4.2 Mejora del dato","text":"Si los datos tienen el nivel de calidad necesario, es preciso mejorar su calidad para que arruinen los procesos de negocio.\nPara ello, partir de los resultados de las mediciones, los analistas de calidad del dato deben determinar las causas raíces de esos niveles inadecuados de calidad del dato.\nD. M. Strong, Lee, Wang (1997) identifican diez posibles obstáculos que pueden hacer que los datos tengan esos niveles adecuados de calidad:Múltiples fuentes de datos producen diferentes valores para el mismo atributo de la misma entidad.La realización de juicios subjetivos en la producción de los datos, puede llevar valores diferentes.Errores sistemáticos en la producción de información llevan la pérdida de información.Grandes volúmenes de información almacenada dificultan su acceso en tiempo razonable.Sistemas heterogéneos distribuidos llevan definiciones, formatos y valores inconsistentes.La información numérica es difícil de indexar.El análisis automatizado de los contenidos en colecciones de información pueden producir resultados adecuados.medida que las necesidades de los usuarios cambian, la información que es relevante y útil para la realización de una determinada tarea también cambia.Un acceso fácil la información puede entrar en conflicto con los requisitos de seguridad, confidencialidad y privacidad.La falta de recursos de computación limita el acceso los datos en circunstancias favorables.En función de la naturaleza del problema detectado, las acciones correctivas pueden ser de distinta naturaleza:Corrección de causas sistemáticas.\nSi se observa que los problemas se suceden de forma sistemática y repetida, entonces las acciones de mejora del dato deben estar orientada eliminar esas causas sistemáticas (véase D. M. Strong, Lee, Wang (1997)).\nPor ejemplo: si los errores de calidad del dato se deben que un proceso de negocio está mal diseñado, entonces hay que rediseñarlo; si las causas se deben que hay personas desempeñando ciertos roles para los que tienen los conocimientos o habilidades adecuadas, entonces hay que darle la formación adecuada; o si se deben que hay software (por ejemplo, procesos ETL) que falla, entonces hay que realizar el mantenimiento correctivo correspondiente.Corrección de causas sistemáticas.\nSi se observa que los problemas se suceden de forma sistemática y repetida, entonces las acciones de mejora del dato deben estar orientada eliminar esas causas sistemáticas (véase D. M. Strong, Lee, Wang (1997)).\nPor ejemplo: si los errores de calidad del dato se deben que un proceso de negocio está mal diseñado, entonces hay que rediseñarlo; si las causas se deben que hay personas desempeñando ciertos roles para los que tienen los conocimientos o habilidades adecuadas, entonces hay que darle la formación adecuada; o si se deben que hay software (por ejemplo, procesos ETL) que falla, entonces hay que realizar el mantenimiento correctivo correspondiente.Corrección de errores debidos causas aleatorias.\nSi es posible identificar cuáles son las causas raices, porque son completamente desconocidas o aleatorias, queda más remedio que actuar sobre los valores de los datos, cambiándolos para asegurarse que se cumplen las reglas de negocio que están establecidas.\neste proceso se le suele llamar depuración o limpieza del datos (data cleansing).\nIlyas Chu (2019) identifican diversas técnicas de limpieza del dato (que pueden incluir operaciones de imputación de datos - véase la sección ?? del Cap. ??, de normalización): esto implica realizar limpieza basadas en reglas de negocio, deduplicación de datos, transformación de datos, o limpieza guiadas por machine learning.\nEn este caso, sería posible utilizar algunas funciones del paquete dlookr relacionadas con la transformación de los datos tales como imputate_na() o imputate_outlier() que genera valores para evitar datos faltantes o valores que garantizan niveles adecuados de exactitud o de consistencia.Corrección de errores debidos causas aleatorias.\nSi es posible identificar cuáles son las causas raices, porque son completamente desconocidas o aleatorias, queda más remedio que actuar sobre los valores de los datos, cambiándolos para asegurarse que se cumplen las reglas de negocio que están establecidas.\neste proceso se le suele llamar depuración o limpieza del datos (data cleansing).\nIlyas Chu (2019) identifican diversas técnicas de limpieza del dato (que pueden incluir operaciones de imputación de datos - véase la sección ?? del Cap. ??, de normalización): esto implica realizar limpieza basadas en reglas de negocio, deduplicación de datos, transformación de datos, o limpieza guiadas por machine learning.\nEn este caso, sería posible utilizar algunas funciones del paquete dlookr relacionadas con la transformación de los datos tales como imputate_na() o imputate_outlier() que genera valores para evitar datos faltantes o valores que garantizan niveles adecuados de exactitud o de consistencia.","code":""},{"path":"DGDQM.html","id":"resumen-6","chapter":"Capítulo 7 Gobierno, gestión y calidad del dato","heading":"Resumen","text":"En este capítulo se presentan los fundamentos del gobierno del dato.\nEs importante tener en cuenta los siguientes aspectos:El gobierno del dato tiene como objetivo asegurar que los datos que se usan y gestionan en las organizaciones están alineadas las estrategias del dato de la organización, maximizando así su valor organizacional.Gobernar los datos implica el diseño, implementación y mantenimiento de un sistema de gobierno del dato. Un sistema de gobierno del dato tiene siete tipos de componentes: procesos de gestión del dato, gestión de calidad del dato y gobierno del dato; estructuras organizacionales; principios, políticas y marcos de referencia; datos y descripción de los datos; cultura, ética y comportamiento; personas, habilidades y competencias; servicios, infraestructuras y aplicaciones.Existen modelos de referencias que pueden ser usados como base para la creación de sistemas de gobierno del dato.El gobierno del dato persigue cuatro beneficios básicos para la organización: alineamiento estratégico, realización de beneficios, optimización de riesgos, optimización de recursos.La gestión de la calidad del dato es el proceso mediante el cual se garantiza que los datos tengan el nivel de calidad adecuado para las tareas para las que fueron recogidos.Para evaluar y medir la calidad se necesitan criterios; estos criterios se llaman características o dimensiones de calidad del dato.La evaluación y medición de calidad del dato requiere la identificación y clasificación de las reglas de negocio que rigen la validez de los datos. Las técnicas y herramientas de perfilado del dato se pueden utilizar como base para la identificación de reglas de negocio partir de los datos.Cuando los datos tienen calidad,partir de las mediciones y evaluaciones realizadas, deben investigarse cuáles son las posibles causas.\nSi las causas son sistemáticas, entonces hay que enfocar el problema desde un punto de vista organizacional; si las causas son aleatorias, se pueden usar las técnicas de limpieza del dato vistas.","code":""},{"path":"id_130009.html","id":"id_130009","chapter":"Capítulo 8 Integración y limpieza de datos","heading":"Capítulo 8 Integración y limpieza de datos","text":"Jorge Velasco López\\(^{}\\) y José-María Montero\\(^{b}\\)\\(^{}\\)Instituto Nacional de Estadística de España\n\\(^{b}\\)Universidad de Castilla-La Mancha","code":""},{"path":"id_130009.html","id":"introducción-3","chapter":"Capítulo 8 Integración y limpieza de datos","heading":"8.1 Introducción","text":"En los proyectos de ciencia de datos, generalmente es necesario realizar un preprocesamiento (o preparación) de los datos antes de iniciar las fases de modelado. Las labores de preprocesamiento son específicas para cada conjunto de datos, para los objetivos del proyecto y para las técnicas de modelización que se van utilizar. Sin embargo, hay una serie de tareas comunes, como las de integración (combinación de datos de distintas fuentes) partir de los datos en bruto (o sin procesar) y limpieza (identificación y corrección de posibles errores en los datos). Otras tareas que se suelen incluir en el proceso de preparación de datos son: la transformación de la variable objetivo, para cambiar su distribución de probabilidad (normalmente para hacerla gaussiana), la transformación de variables predictoras (o clasificadoras, en su caso) (feature engineering), la normalización y la reducción de la dimensionalidad. Estas tareas se abordarán en el Cap. 9.\nEn R, existen varios paquetes para llevar cabo estos trabajos: tidyverse, para la manipulación de ficheros y variables que se han ilustrado en el Cap. 3; dlookr (Staniak Biecek 2019); validate, errorlocate y dcmodify (Loo Jonge 2019), para realizar validaciones y transformaciones los datos; caret (Kuhn 2008), para imputar los datos faltantes o perdidos (missing data; sf (Edzer J. Pebesma et al. 2018), para el manejo de conjuntos de datos espaciales; y GGally (Schloerke et al. 2021) y naniar (N. J. Tierney Cook 2018) para labores de visualización.","code":""},{"path":"id_130009.html","id":"integración-de-datos","chapter":"Capítulo 8 Integración y limpieza de datos","heading":"8.2 Integración de datos ","text":"La integración es un conjunto de procesos técnicos y de negocio que se utilizan para combinar información proveniente de diferentes fuentes. En términos generales, se puede decir que consiste en acceder los datos desde todas las fuentes y localizaciones, tanto en entorno local, como en la nube o en una combinación de ambos, de modo que los registros de una fuente de datos enlacen con los registros de otra.Para ilustrar el proceso de integración, continuación se integra, por separado,63 el conjunto de datos Madrid_Sale (incluido en el paquete idealista18), que contiene el identificador de las viviendas en venta en el municipio de Madrid y 41 variables relativas dichos inmuebles (como su antigüedad y precio, por ejemplo) con otros dos conjuntos de datos del mismo paquete: Madrid_POIS, donde se listan, entre otras, las coordenadas de las estaciones de metro de la ciudad de Madrid; y Madrid_Polygons, que contiene los polígonos (en este caso, distritos) del municipio. Ello redundará en un enriquecimiento de los análisis que se lleven cabo, al disponer en un mismo conjunto de datos un número mayor de variables relativas al problema solucionar. modo de ejemplo, la integración de Madrid_Sale con Madrid_POIS permitirá determinar el número de estaciones de metro menos de 500 metros de la vivienda y la distancia de cada vivienda la estación de metro más cercana; la integración de Madrid_Sale y Madrid_Polygons permitirá la construcción un mapa de precios medios del metro cuadrado de vivienda por distritos. Ambos ejemplos se ilustrarán con detalle en las dos subsecciones siguientes.La función glimpse() permite mostrar la estructura de los tres conjuntos de datos incluidos en el paquete idealista18.La combinación de conjuntos de datos se realiza, fundamentalmente, con las funciones de unión. En el Cap. 3 se mostraban las cuatro funciones de unión principales del paquete tidyverse: left_join(), inner_join(), right_join() y full_join(). Sin embargo, también merece la pena mencionar las uniones de filtrado entre dos objetos \\(x\\) e \\(y\\), que se llevan cabo mediante las siguientes funciones: semi_join(): devuelve todas las filas de \\(x\\) con una coincidencia en \\(y\\).anti_join(): devuelve todas las filas de \\(x\\) que tengan una coincidencia en \\(y\\).nest_join(): devuelve todas las filas y columnas de \\(x\\) con una nueva columna anidada, que contiene todas las coincidencias de \\(y\\).","code":"\nlibrary(\"tidyverse\")\nlibrary(\"idealista18\")\nlibrary(\"sf\")\nlibrary(\"GGally\")\nlibrary(\"dlookr\")\n\nglimpse(Madrid_Sale)\nglimpse(Madrid_POIS)\nglimpse(Madrid_Polygons)"},{"path":"id_130009.html","id":"integración-de-los-ficheros-madrid_sale-y-madrid_pois","chapter":"Capítulo 8 Integración y limpieza de datos","heading":"8.2.1 Integración de los ficheros Madrid_Sale y Madrid_POIS","text":"Como se avanzó anteriormente, dos interesantes resultados que se podrían obtener mediante la integración de estos conjuntos de datos son: \\(()\\) la determinación del número de estaciones de metro menos de 500 metros de la localización de la vivienda de interés, y \\((ii)\\) la distancia la estación de metro más cercana. Para la integración entre los dos ficheros, se utiliza la función st_join(), función de unión para datos espaciales, del paquete sf.64Para proceder la integración de ambos ficheros, primeramente se crean las variables que indican cuál es el sistema de referencia de coordenadas (SRC) que se va utilizar y que permite determinar la posición de un punto en relación otro en base líneas imaginarias (en el ejemplo que nos ocupa, permite representar la ubicación de las viviendas en la superficie de la Tierra). En este caso, la asignación de coordenadas se realiza través de las variables projcrc_src y projcrs_dest, en las que se establecen los parámetros de:Nombre de la proyección (proj).Zona UTM (zone) donde se ubica el conjunto de viviendas.Nombre del elipsoide (ellips). La Tierra es una esfera y tiene accidentes geográficos, por lo cual hay que trabajar con elipsoides y explicitar los parámetros que definen su forma.Nombre del datum (datum). Define el origen y la orientación de los ejes de coordenadas, es decir, proporciona la información necesaria para dibujar el sistema de coordenadas en el elipsoide. El World Geodetic System (WGS84) es un standard en la industria nivel mundial; obstante, existen algunas variantes locales (la más famosa es el North American Datum (NAD83).Tipo de unidades (units); en este caso, metros.Seguidamente, se indica la distancia (en este caso en metros) que se va usar como radio en la variable radius_meters. Finalmente, se lleva cabo un procesamiento específico para datos espaciales: se crea un objeto espacial, se proyecta plano para pasar de tres dos dimensiones (hasta ahora se ha trabajado en la representación de la Tierra en tres dimensiones; sin embargo, estamos acostumbrados ver mapas, es decir, ver dos dimensiones), se cambia la geometría y, finalmente, se vuelve al sistema de coordenadas proyectadas.continuación, para cada una de las viviendas, se calcula el número de estaciones de metro menos de 500 metros (variable N_METRO_STOPS_500_M). Para ello, primero se realiza el cálculo del objeto sf metro_count, seleccionando la variable pois_metro y cruzando con la geometría.Los valores de N_METRO_STOPS_500_M se obtienen con el siguiente código:Al cruzar metro_count con vivs_madrid, se observa que hay casi 25.000 registos que cruzan (25.000 viviendas que tienen ninguna estación de metro menos de 500 metros). En consecuencia, se retiran del análisis puesto el objetivo de la integración de estos dos conjuntos de datos es \\(()\\) la determinación del número de estaciones de metro menos de 500 metros de la localización de las viviendas incluidas en el fichero Madrid_Sale.Posteriormente, se determina la estación más cercana cada vivienda la venta con la función st_nearest_feature().Por último, con la función st_distance(), se calcula la distancia de cada vivienda la estación de metro más cercana, creándose la variable METRO_STOP_MASCERCANO_DISTANCIA.","code":"\nvivs_madrid <- Madrid_Sale |>\n  st_join(Madrid_Polygons, left = TRUE)\nprojcrs_src <- \"+proj=longlat +datum=WGS84 +no_defs\"\nprojcrs_dest <- \"+proj=utm +zone=30 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\"\nradius_meters <- 500 # Se marca la distancia que interesa.\npois_metro <- Madrid_POIS$Metro |>\n  st_as_sf(coords = c(\"Lon\", \"Lat\"), crs = projcrs_src) |> # Crear objeto espacial sf\n  st_transform(crs = st_crs(projcrs_dest)) |> # Proyectar al plano (st_crs recupera la referencia de la coordenada y st_transform realiza la transformación)\n  st_buffer(dist = radius_meters) |> # Cambiar la geometría de punto a polígono (círculo)\n  st_transform(crs = st_crs(projcrs_src)) # Volver al sistema de coordenadas no proyectadas (Ángulos)\nmetro_count <- vivs_madrid |>\n  select(ASSETID) |>\n  st_join(pois_metro,\n    join = st_intersects,\n    left = FALSE\n  )\n# Se elimina la geometría para evitar ralentizar el cálculo\nst_geometry(metro_count) <- NULL\nmetro_count <- metro_count |>\n  group_by(ASSETID) |>\n  summarise(N_METRO_STOPS_500_M = n()) |>\n  ungroup()\nvivs_madrid <- vivs_madrid |>\n  inner_join(metro_count, by = \"ASSETID\")\npois_metro <- Madrid_POIS$Metro |>\n  st_as_sf(coords = c(\"Lon\", \"Lat\"), crs = projcrs_src) # Crear objeto espacial\n\nmascercano_metro_stops <- pois_metro[st_nearest_feature(vivs_madrid, pois_metro), ] # Cálculo de las paradas cercanas\nvivs_madrid <- vivs_madrid |>\n  mutate(METRO_STOP_MASCERCANO_DISTANCIA = as.numeric(st_distance(mascercano_metro_stops, geometry, by_element = T)))"},{"path":"id_130009.html","id":"integración-de-los-ficheros-madrid_sale-y-madrid_polygons","chapter":"Capítulo 8 Integración y limpieza de datos","heading":"8.2.2 Integración de los ficheros Madrid_Sale y Madrid_Polygons","text":"En esta subsección se muestran los detalles para construir un mapa de precio medio del metro cuadrado de la vivienda en la ciudad de Madrid, por distritos, tras la integración de los conjuntos de datos Madrid_Sale y Madrid_Polygons.Para proceder la integración de ambos ficheros, primeramente se realiza la conversión del conjunto de datos Madrid_Polygons objeto espacial y se le asocia la coordenada de referencia (de forma similar como se hizo en la integración de los ficheros Madrid_Sale y Madrid_POIS):continuación, se lleva cabo la unión entre el objeto espacial de Madrid_Polygons_sf y Madrid_Sale_sf para calcular su precio por metro cuadrado (preciopm2) y el área de la geometría (tract_area).partir del resultado de esta integración, se construye la Fig. 8.1, que muestra un el mapa del precio medio del metro cuadrado de las viviendas la venta en Madrid, escala de distrito, lo que da una visión clara de las zonas más o menos económicas.\nFigura 8.1: Precio por metro cuadrado de viviendas la venta en Madrid por distrito\n","code":"\n# Se convierten a objetos sf\nMadrid_Polygons_sf <- sf::st_as_sf(Madrid_Polygons, wkt = \"WKT\") # WKT (Well-known text) es un formato de vectores geométricos\nMadrid_Sale_sf <- st_as_sf(Madrid_Sale, coords = c(\"LONGITUDE\", \"LATITUDE\"))\n# se asocia la coordenada de referencia del objeto\nst_crs(Madrid_Sale_sf) <- \"+proj=longlat +datum=WGS84 +no_defs\"\nst_crs(Madrid_Polygons_sf) <- \"+proj=longlat +datum=WGS84 +no_defs\"\nMadrid_Sale_Polygons <- Madrid_Polygons_sf |>\n  dplyr::mutate(tract_area = st_area(WKT)) |>\n  sf::st_join(Madrid_Sale_sf) |>\n  dplyr:: group_by(LOCATIONNAME) |>\n  dplyr:: summarize(tract_area = unique(tract_area), preciopm2 = mean(PRICE / CONSTRUCTEDAREA))\nplot(Madrid_Sale_Polygons[\"preciopm2\"])"},{"path":"id_130009.html","id":"limpieza-de-datos","chapter":"Capítulo 8 Integración y limpieza de datos","heading":"8.3 Limpieza de datos","text":" Es más habitual de lo deseable que algunas variables presenten problemas en la calidad de sus datos. En el Cap. 7, se mencionaban una serie de causas y la posibilidad de realizar el perfilado para tener una medición de la calidad de los datos. Si los datos tienen el nivel de calidad adecuado, deben realizarse tareas de limpieza para transformarlos en datos consistentes, corrigiendo datos incorrectos, corruptos, con formato incorrecto, duplicados o incompletos.\nFigura 8.2: Flujo del proceso de limpieza de datos\nSi bien las técnicas utilizadas para la limpieza de datos pueden variar según el tipo de datos que se esté procesando, en general, se pueden dividir en cinco grupos:Corrección de errores estructurales.Filtrado de registros duplicados o irrelevantes.Gestión de valores atípicos.Gestión de valores faltantes (missing).Validación y control de la calidad de los datos.","code":""},{"path":"id_130009.html","id":"corrección-de-errores-estructurales","chapter":"Capítulo 8 Integración y limpieza de datos","heading":"8.3.1 Corrección de errores estructurales","text":"\nLos errores estructurales ocurren cuando se observan formatos erróneos, estructuras incorrectas o errores tipográficos que pueden dar lugar categorías o clases mal etiquetadas. Por tanto, puede haber errores estructurales nivel de conjunto de datos (data.frame , sf, tibble,…) y nivel de variable.","code":""},{"path":"id_130009.html","id":"madridsaleint","chapter":"Capítulo 8 Integración y limpieza de datos","heading":"8.3.1.1 A nivel de conjunto de datos","text":"Los cambios estructurales nivel de conjunto de datos consisten en modificar el tipo de objeto o eliminar o agregar variables.\nPor ejemplo, continuación se genera el conjunto de datos Madrid_Sale_int, con estructura de data.frame, partir del conjunto de datos vivs_madrid (de tipo sf), fruto del proceso de integración anterior. Además, para este objeto, se elimina la variable geometry, que se va usar en adelante y ralentiza la computación.También se crea un segundo conjunto de datos reducido, Madrid_Sale_red, con una selección de variables que se consideran de interés para ilustrar las tareas de limpieza que se exponen en este capítulo.Por último, se añade la variable LOCATIONID1, que indica el código de localización, al conjunto de datos Madrid_Polygons.En la siguiente sección, partir de estos conjuntos de datos, se lleva cabo un proceso de diagnosis y exploración.","code":"\nMadrid_Sale_int <- as.data.frame(vivs_madrid) |> select(-geometry)\nMadrid_Sale_red <- select(Madrid_Sale_int, ASSETID, PRICE, UNITPRICE, CONSTRUCTEDAREA, ROOMNUMBER, CONSTRUCTIONYEAR, HASNORTHORIENTATION, HASSOUTHORIENTATION, HASEASTORIENTATION, HASWESTORIENTATION, CONSTRUCTIONYEAR, DISTANCE_TO_METRO, METRO_STOP_MASCERCANO_DISTANCIA)\nMadrid_Polygons$LOCATIONID1 <- substr(Madrid_Polygons$LOCATIONID, 1, 10)"},{"path":"id_130009.html","id":"a-nivel-de-variable","chapter":"Capítulo 8 Integración y limpieza de datos","heading":"8.3.1.2 A nivel de variable","text":"Los errores estructurales nivel de variable se centran fundamentalmente en el tipo de dato de las variables.En primer lugar, se visualizan los datos con la función diagnose() de dlookr.NotaEl paquete dlookr se usa para tareas de diagnosis y exploración, y es de utilidad para la localización de valores duplicados, faltantes, atípicos, tipología de datos, etc. La función overview() permite obtener una visión genérica del conjunto de datos, y la función diagnose() proporciona información nivel de variable, como el tipo de dato (type), y sobre valores faltantes y únicos. Otras funciones útiles son diagnose_numeric() y diagnose_category(), que proporcionan información específica para valores numéricos y categóricos, respectivamente.Al ejecutar la función, se comprueba que todas las variables, excepto el identificador ASSETID, son de tipo numérico (o integer), lo que es correcto. En caso de tener que modificar el tipo de dato, por considerarse un error estructural, o porque sea conveniente para las fases de modelado, debería hacerse usando las funciones .factor(), .numeric() y .character(), según el caso.Corregir errores estructurales tipográficos de variables categóricas es especialmente relevante en algunas áreas de la ciencia de datos, como la minería de textos o text mining (que se verá con más profundidad en el Cap.38), donde la limpieza de textos consiste en eliminar todo aquello que aporte información sobre su temática, estructura o contenido. continuación, se muestra una función creada partir del paquete stringr que permite realizar una limpieza básica de un texto, y que se ejecuta sobre la variable Madrid_Polygons$LOCATIONNAME, generando la variable LOCATIONNAME1.","code":"\ndiagnose(Madrid_Sale_red)\nlibrary(\"stringr\")\nlimpieza_textos <- function(texto) {\n  # El orden de la limpieza no es arbitrario\n  # Se convierte todo el texto a minúsculas\n  nuevo_texto <- tolower(texto)\n  # Eliminación de páginas web (palabras que empiezan por \"http.\" seguidas\n  # de cualquier cosa que no sea un espacio)\n    nuevo_texto <- str_replace_all(nuevo_texto, \"http\\\\S*\", \"\")\n  # Eliminación de signos de puntuación\n  nuevo_texto <- str_replace_all(nuevo_texto, \"[[:punct:]]\", \" \")\n  # Eliminación de números\n  nuevo_texto <- str_replace_all(nuevo_texto, \"[[:digit:]]\", \" \")\n  # Eliminación de espacios en blanco múltiples\n  nuevo_texto <- str_replace_all(nuevo_texto, \"[\\\\s]+\", \" \")\n  return(nuevo_texto)\n}\nMadrid_Polygons$LOCATIONNAME1 <- limpieza_textos(texto = Madrid_Polygons$LOCATIONNAME)\nglimpse(Madrid_Polygons)\n#> $ LOCATIONNAME <fct> Conde Orgaz-Piovera, Pinar del Rey, Timón, Palacio,\n#> ...\n#> $ LOCATIONNAME1 <chr> \"conde orgaz piovera\", \"pinar del rey\", \"timón\","},{"path":"id_130009.html","id":"eliminación-de-observaciones-duplicadas-o-irrelevantes.","chapter":"Capítulo 8 Integración y limpieza de datos","heading":"8.3.2 Eliminación de observaciones duplicadas o irrelevantes.","text":"\nLas observaciones duplicadas aparecen frecuentemente durante la recogida de datos e integración de las bases de datos, por lo que dichas duplicidades deben ser eliminadas en esta fase de limpieza.continuación, se usa la función overview() del paquete dlookr sobre el conjunto de datos Madrid_Sale_int, obtenido en la Sec. 8.3.1.1.Entre otra información, como la existencia de valores faltantes (missing) en siete variables, se puede observar que hay valores duplicados después del proceso de integración. En caso contrario, se podrían usar las funciones base de R para \\(()\\) localizarlos, con duplicated(), y \\((ii)\\) extraer los registros únicos, con unique(). También se puede usar distinct(), del paquete dplyr, para eliminar los registros duplicados de un data.frame.Las observaciones irrelevantes son aquellas que encajan en el problema específico que se está analizando. Por ejemplo, si el objeto de estudio son datos de Madrid, se pueden eliminar las observaciones que correspondan dicho municipio.\ncontinuación, se puede advertir que todas las observaciones de Madrid_Polygons$LOCATIONID1 empiezan por el código correspondiente Madrid (0-EU-ES-28) y, por tanto, es necesario filtrar registros.En caso necesario, se pueden filtrar todos los registros de Madrid en el objeto Madrid_Polygons1 haciendo:","code":"\nhead(overview(Madrid_Sale_int), n = 9)\n#>     division               metrics    value\n#> 1       size          observations    70059\n#> 2       size             variables       46\n#> 3       size                values  3222714\n#> 4       size           memory size 21931336\n#> 5 duplicated duplicate observation        0\n#> 6    missing  complete observation    26394\n#> 7    missing   missing observation    43665\n#> 8    missing     missing variables        7\n#> 9    missing        missing values    48653\nhead(table(Madrid_Polygons$LOCATIONID1))\n#> \n#> 0-EU-ES-28 \n#>        135\nMadrid_Polygons1 <-\n  Madrid_Polygons |> filter(substr(Madrid_Polygons$LOCATIONID, 1, 10) != \"0-EU-ES-28\")"},{"path":"id_130009.html","id":"gestión-de-valores-atípicos-no-deseados","chapter":"Capítulo 8 Integración y limpieza de datos","heading":"8.3.3 Gestión de valores atípicos no deseados","text":"\nmenudo, hay observaciones distintas que, aparentemente, encajan en los datos que se están analizando. Si existe una razón coherente para eliminar un valor atípico (un outlier), como una entrada de datos incorrecta, hacerlo mejorará el rendimiento que proporcionan los datos con los que se está trabajando. Sin embargo, el hecho de que exista un valor atípico significa que sea incorrecto. Si un valor atípico resulta ser irrelevante para el análisis, o es un error, debe considerarse su eliminación.\nEl número de posibles valores atípicos en el conjunto de datos Madrid_Sale_red se determina con el siguiente código, que avisa de la posibilidad de que existan para cada una de las variables.Otra manera de localizar datos atípicos es través de la visualización. Por ejemplo, en la Fig. 8.3 se relaciona el precio de la vivienda por metro cuadrado con su localización, y se observa que la zona más cara es Recoletos y la más barata es San Cristobal. La simple observación aconsejaría un análisis de los casos extremos (muy baratos o caros en cada uno de los distritos).\n\nFigura 8.3: Precio medio del metro cuadrado por distritos\nLos box-plots y gráficos de dispersión de variables, para las categorías dadas de otra, así como las correlaciones entre dichas variables, también pueden utilizarse para detectar valores atípicos. Por ejemplo, se puede considerar la relación del precio del metro cuadrado de la vivienda con otras variables, como la superficie construida, la distancia al metro y el número de habitaciones. Para ello, primeramente se crea el conjunto de datos Madrid_Sale_red2 con la variable derivada price_bin (de tipo factor), cuyas categorías o clases (o bins) son los cuartiles de la variable PRICE.partir del conjunto de datos Madrid_Sale_red2 se puede crear construir la Fig. 8.4.\nFigura 8.4: Distribuciones y correlaciones cruzadas algunas variables de Madrid-Sale-red\nEn dicha figura, la diagonal descendente muestra la función de cuantía (para precio medio del metro cuadrado) y las funciones de densidad de CONSTRUCTEDAREA, DISTANCE_TO_METRO y ROOMNUMBER. Los tres últimos paneles de la primera columna muestran los histogramas de las estas tres últimas variables. Los tres últimos paneles de la primera fila, proporcionan los box-plots de estas variables para los cuatro bins de la variable price_bin (primer cuartil en rosa, segundo en verde, tercero en azul y cuarto en morado). Los paneles del triángulo lateral derecho muestran sus correlaciones, mientras que los del triángulo inferior izquierdo presentan sus gráficos de dispersión. Dicho lo anterior, por ejemplo, en la primera fila se observa que las viviendas más económicas suelen tener menos superficie construida (segunda columna), que suelen estar ligeramente más alejadas del metro (tercera) y suelen tener menos habitaciones. Sin embargo, se aprecian algunas cuestiones que llaman la atención. Por ejemplo, que hay una viviendas muy alejadas (casi 400 kilómetros) de la estación de metro más cercana, lo cual distorsiona algunas de las figuras e impide ver la información que contienen; o que hay viviendas cuyo precio por metro cuadrado pertenece la primera categoría de la variable price_bin (las más económicas) con muchas habitaciones o con mucha superficie construida. continuación, por ejemplo, se filtran las viviendas con 30 o más habitaciones (aunque la lógica sería válida para muchas menos). Se observa que la superficie construida es de menos de 120 metros lo que, sin mayor conocimiento del conjunto de datos, parece ser coherente y podrían excluirse (filtrarse) del conjunto de datos, o tratar de recabar la información correcta.Finalmente, detectados los valores atípicos, por cualquiera de los procedimientos anteriormente expuestos, el paquete dlookr, través de la función imputate_outlier(), permite llevar cabo sofisticadas imputaciones de los mismos, si bien sólo en el caso variables numéricas. Los métodos de imputación que se contemplan son: media, mediana, moda y capping (imputar los valores atípicos superiores con el percentil 95, y los inferiores con el percentil 5). Por ejemplo, se podría imputar la variable CONSTRUCTEDAREA_imp partir de CONSTRUCTEDAREA con el método media (mean): CONSTRUCTEDAREA_imp <- imputate_outlier(Madrid_Sale_red2, CONSTRUCTEDAREA, method = \"mean\").\nOtra opción es poner los valores atípicos como valores disponibles (available, NA) y proceder imputar dichos NA tal y como se muestra en el epígrafe siguiente.","code":"\ndiagnose_numeric(Madrid_Sale_red)\nggplot(Madrid_Sale_int, aes(x = reorder(LOCATIONNAME, PRICE / CONSTRUCTEDAREA, na.rm = TRUE), y = PRICE / CONSTRUCTEDAREA)) +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +\n  labs(x = \"Distrito\", y = \"Precio metro cuadrado\")\nMadrid_Sale_red2 <- mutate(Madrid_Sale_int, price_bin = cut2(PRICE, g=4)) |>\n  select(price_bin, CONSTRUCTEDAREA, DISTANCE_TO_METRO, ROOMNUMBER, LOCATIONNAME)\nggpairs(Madrid_Sale_red2,\n  column = 1:4, aes(color = price_bin, alpha = 0.5),\n  upper = list(continuous = wrap(\"cor\", size = 2))\n)\nMadrid_Sale_red2 |> filter(price_bin == \"[ 21000, 168000)\", ROOMNUMBER > 30)\n#>          price_bin CONSTRUCTEDAREA DISTANCE_TO_METRO ROOMNUMBER LOCATIONNAME\n#> 1 [ 21000, 168000)              90         0.3826137         33  Almendrales\nMadrid_Sale_red2$ROOMNUMBER[Madrid_Sale_red2$ROOMNUMBER >= 30 & Madrid_Sale_red2$price_bin == \"[ 21000, 168000)\"] <- NA"},{"path":"id_130009.html","id":"gestión-de-datos-faltantes-missing","chapter":"Capítulo 8 Integración y limpieza de datos","heading":"8.3.4 Gestión de datos faltantes (missing)","text":"\nLos datos pueden faltar por multitud de razones, aunque generalmente se suelen agrupar en dos categorías: valores faltantes informativos (Kuhn, Johnson, et al. 2013) y valores faltantes aleatorios (Little Rubin 2019). Los informativos implican una causa estructural, ya sea por deficiencias en la forma en que se recopilaron los datos o por anomalías en el entorno de observación. Los aleatorios son aquellos que tienen lugar independientemente del proceso de recopilación de datos.Dependiendo de si los valores faltantes son de uno u otro tipo, se procederá de una u otra manera. los informativos, en general, se les puede asignar un valor concreto (por ejemplo,\n“Ninguno”), ya que este valor puede afectar los resultados de las predicciones. Los aleatorios pueden manejarse mediante la eliminación o la imputación. Además, los diferentes algoritmos de aprendizaje automático manejan la falta de información de manera diferente. De hecho, la mayoría de los algoritmos incorporan mecanismos para manejarlos (por ejemplo, modelos lineales generalizados y derivados, redes neuronales y support vector machine) y, por lo tanto, requieren que se traten previamente. Sólo unos pocos modelos (principalmente basados en árboles) tienen procedimientos incorporados para tratar los valores faltantes.Como se avanzó anteriormente, en R, los valores nulos se representan con el símbolo NA. Es importante distinguirlos de los valores indefinidos (p. ej., dividir entre cero), que se representan con el símbolo NaN (Number).\nPara visualizar los patrones de datos faltantes de la variable price_bin del conjunto de datos Madrid_Sale_red2, se ejecuta el siguiente código.\nFigura 8.5: Visualización de valores faltantes\nLa gestión de los valores faltantes debe hacerse considerando la problemática que se quiera resolver. Una primera opción considerar sería excluirlos, si bien se estaría eliminando información. Para filtrar los registros faltantes, se podría utilizar la función .na(). En el caso de ROOMNUMBER:También se puede optar por reemplazarlos, por ejemplo por un 0, de la siguiente manera:obstante, estas dos opciones son acciones recomendables en primera instancia, porque eliminar los registros con valores faltantes, o introducir valores que podrían respetar la semántica de los datos, puede ocasionar un alto impacto negativo en los niveles globales de calidad de datos del conjunto de datos.Se puede ir más allá de la eliminación de valores faltantes. través de diversos métodos se pueden imputar valores que, con mayor o menor probabilidad, podrían ser los que realmente correspondieran estos valores faltantes. Estos métodos se conocen como métodos de imputación de valores. Para imputar valores faltantes se pueden usar diversas alternativas, como la función preProcess() del paquete caret o la función imputate_na() del paquete dlookr.\ncontinuación, se imputan los valores faltantes del conjunto de datos Madrid_Sale_red2 con dos métodos. En primer lugar, con el algoritmo de KNN (\\(k\\) vecinos más cercanos), que sustituye el valor faltante por la media de los valores de los \\(k\\) vecinos más próximos. Después de realizar el preprocesamiento, se comprueba que las imputaciones han sido realizadas.continuación, la imputación se realiza con la mediana (que suele ser preferible imputar la media, puesto que el promedio puede verse afectado por outliers).NotaEl paquete dlookr, través de la función imputate_na(), permite imputar valores faltantes. El predictor admite variables numéricas y categóricas. Los métodos que utiliza son: para numéricas media, moda, KNN, rpart y mice); y para categóricas: mode, rpart y mica.\nEl paquete recipes también es recomendable. Por ejemplo, para la imputación de los valores faltantes con la media de la variable se usaría step_meanimpute(all_numeric()).","code":"\nlibrary(\"naniar\")\ngg_miss_fct(x = `Madrid_Sale_red2`, fct = price_bin)\nMadrid_Sale_red3 <- Madrid_Sale_red2\nMadrid_Sale_red3 |>\n  filter(!is.na(ROOMNUMBER))\nMadrid_Sale_red3[is.na(Madrid_Sale_red3)] <- 0\n# También puede usarse la función `replace_na()`, que sustituye los valores perdidos en cada variable por el valor especificado.\nlibrary(\"caret\")\n# Se realiza el preprocesamiento:\npre_knn <- preProcess(Madrid_Sale_red2, method = \"knnImpute\", k = 2)\n# Se obtienen los datos\nimputed_knn <- predict(pre_knn, Madrid_Sale_red2)\n# Se comprueba que se ha imputado el valor faltante de la variable ROOMNUMBER\ndiagnose(imputed_knn)\n# Se realiza el preprocesamiento:\npre_median <- preProcess(Madrid_Sale_red2, method = \"medianImpute\")\n# Se obtienen los datos\nimputed_median <- predict(pre_median, Madrid_Sale_red2)\n# Se comprueba que se ha imputado el valor faltante de la variable ROOMNUMBER\ndiagnose(imputed_median)"},{"path":"id_130009.html","id":"validación-y-control-de-calidad","chapter":"Capítulo 8 Integración y limpieza de datos","heading":"8.3.5 Validación y control de calidad","text":"\nAl final del proceso de limpieza de datos, éstos deberían ser consistentes y seguir las reglas apropiadas para su campo de negocio. De ser así, los modelos que estimen en base ellos representarán convenientemente la realidad objeto de estudio y las conclusiones que se obtengan de dichos modelos serán de utilidad para dicha realidad.La verificación de si los datos son o consistentes y si siguen o las reglas del campo de negocio del cual proceden, se puede llevar cabo con el paquete tidyverse, que permite hacer selecciones, filtrados o tablas de frecuencias, entre otras acciones. modo de ejemplo, en el caso del precio medio del metro cuadrado de los distritos de la ciudad de Madrid, se puede usar la función count() para obtener la distribución de frecuencias de la variable METRO_STOP_MASCERCANO_DISTANCIA y comprobar si es consistente con el conocimiento que se tiene de esa variable y del conjunto de datos. Se muestran las distancias la estación más cercana para las viviendas correspondientes los seis primeros registros.Una opción más sofisticada es el paquete validate, donde se pueden introducir las reglas de negocio dentro del propio código o bien desde un fichero externo. continuación, se realiza un ejemplo con las reglas incrustadas en el propio código. Estas reglas pueden ser avisos o normas que indican error en esos datos. En este ejemplo, se han definido siete reglas: por ejemplo, PRICE\\(\\ge\\) 0, o que la suma de las variables HASNORTHORIENTATION, HASSOUTHORIENTATION HASEASTORIENTATION y HASWESTORIENTATION sea la unidad. La salida que se obtiene se presenta continuación. modo de ejemplo, la regla HASNORTHORIENTATION + HASSOUTHORIENTATION + HASEASTORIENTATION +HASWESTORIENTATION \\(=\\) 1 es la número 3, que, como se puede ver, se cumple en 48.446 ocasiones.NotaEl proceso de validación puede ser ser más o menos complejo, según afecte una única variable en un mismo registro, más de una variable de un mismo registro o más de una variable en más de un registro. En el último caso, además, se puede validar en un solo conjunto de datos o en más de uno.En un esquema tradicional de validación, además de las reglas de validación aportadas por los expertos en el tópico del que se trate, debe incluirse también un listado de reglas de corrección (igualmente aportado por los expertos en la materia) que indiquen cómo hay que corregir un registro cuando cumple con una determinada regla de validación. Este modo de proceder, además de suponer un doble esfuerzo, puede conducir inconsistencias o validaciones cíclicas.El Método de Fellegi y Holt65 (MFH) dá una solución este problema, evitando dichas inconsistencias, proporcionando un procedimiento que genera un conjunto completo de reglas de validación, incorporando reglas implícitas las formuladas por los expertos de manera explícita.En breves palabras, dicho método asegura el cumplimiento de las siguientes tres premisas:Minimizar el número de campos corregir en un registro para hacerlo pasar todas las validaciones.Mantiener, en la medida de lo posible, la distribución conjunta original del conjunto de datos.Derivar las reglas de corrección, directamente y de forma implícita, de las reglas de validación. Por tanto, dichas reglas de corrección son propuestas el experto o, en su caso, por el validador.Los detalles sobre el MFH pueden verse en Boskovitz, Goré, Hegland (2003).El MFH está exento de limitaciones. La primera es el incremento del coste computacional, que puede llegar constituir un problema en caso de que el número de reglas implícitas sea muy elevado, lo cual es muy frecuente. De hecho, hay casos en los que hay más reglas implícitas que registros. Para solucionar este problema, denominado “problema de localización del error”, que consiste, básicamente, en determinar el conjunto mínimo de variables corregir para cada validación, se han propuesto varias alternativas, que incluyen métodos de investigación de operaciones, árboles binarios y metaheurísticas como algoritmos genéticos y similares.efectos prácticos, el MFH se puede aplicar con la función locate_errors() del paquete errorlocate, determinándose así cuáles son las variables corregir para solventar los errores\nen las reglas de negocio establecidas (objeto rules). Por ejemplo, en el conjunto de datos Madrid_Sale_red2 (donde se definía la variable price_bin), se establecen ahora unas reglas básicas algo más laxas (específicamente una: más de 10 habitaciones los tres primeros cuartiles), obteniéndose que habría que depurar la variable ROOMNUMBER en dos ocasiones para que el conjunto de datos quedase totalmente limpio (o depurado).¿Y qué se debe hacer con los registros que cumplen las normas de validación? La respuesta es, como norma, “siempre que se disponga de información de negocio, ésta debe preponderar sobre cualquier tipo de imputación”.\npartir de este punto se puede proceder realizar imputaciones determinísticas para solucionar los problemas detectados.En el ejemplo anterior, se propone imputar el valor ROOMNUMBER=5 los casos de los tres primeros cuartiles (todos menos el más caro) que tengan más de 10 habitaciones. Para ello, se utiliza la función modify_so() del paquete dcmodify. Para comprobar que la imputación se ha llevado cabo con éxito, se pueden comparar los conjuntos de datos antes y después de la imputación con la función compare(), comprobándose que tal imputación se ha realizado exitosamente en los 2 registros que presentaban problemas con la regla \\(ROOMNUMBER >= 10\\).","code":"\nhead(count(as.data.frame(Madrid_Sale_red), METRO_STOP_MASCERCANO_DISTANCIA))\n#>   METRO_STOP_MASCERCANO_DISTANCIA n\n#> 1                        1.413845 1\n#> 2                        1.414032 1\n#> 3                        2.586694 1\n#> 4                        3.156593 1\n#> 5                        4.013776 1\n#> 6                        4.128947 1\nlibrary(\"validate\")\n\nMadrid_Sale_int |>\n  check_that(\n    HASLIFT >= 0,\n    PRICE >= 0,\n    HASNORTHORIENTATION + HASSOUTHORIENTATION + HASEASTORIENTATION + HASWESTORIENTATION == 1,\n    is.numeric(PRICE),\n    UNITPRICE * CONSTRUCTEDAREA == PRICE,\n    if (ROOMNUMBER > 3) PRICE > 100000,\n    nrow(.) >= 20000\n  ) |>\n  summary()\n#> name items passes fails NA error warning\n#> 1 V1 70059 70059     0   0 FALSE FALSE\n#> 2 V2 70059 70059     0   0 FALSE FALSE\n#> 3 V3 70059 21613 48446   0 FALSE FALSE\n#> 4 V4     1     1     0   0 FALSE FALSE\n#> 5 V5 70059 15280 54779   0 FALSE FALSE\n#> 6 V6 70059 70041    18   0 FALSE FALSE\n#> 7 V7     1     1    0    0 FALSE FALSE\nlibrary(\"errorlocate\")\n\nrules <- validator(if (ROOMNUMBER >= 10) price_bin == \"[502000,7138000]\")\nel <- locate_errors(Madrid_Sale_red2, rules) |>\n  summary(el)\nel\n# el$variable\n#   names              errors missing\n\n#     price_bin         0       0\n#   ROOMNUMBER          2       1\n#   CONSTRUCTEDAREA     0       0\n#   DISTANCE_TO_METRO   0       0\n#   LOCATIONNAME        0      42\nlibrary(\"dcmodify\")\n\nout <- Madrid_Sale_red2 |>\n  modify_so(if (ROOMNUMBER >= 10 & price_bin != \"[502000,7138000]\") ROOMNUMBER <- 5)\n\nrules <- validator(if (ROOMNUMBER >= 10) price_bin == \"[502000,7138000]\")\ncompare(rules, raw = Madrid_Sale_red2, modified = out)\n#> Object of class validatorComparison:\n#>\n#> compare(x = rules, raw = Madrid_Sale_red2, modified = out)\n#>\n# Status                 raw modified\n#   validations        70059    70059\n#   verifiable         70058    70058\n#   unverifiable           1        1\n#   still_unverifiable     1        1\n#   new_unverifiable       0        0\n#   satisfied          70056    70058\n#   still_satisfied    70056    70056\n#   new_satisfied          0        2\n#   violated               2        0\n#   still_violated         2        0\n#   new_violated           0        0"},{"path":"id_130009.html","id":"resumen-7","chapter":"Capítulo 8 Integración y limpieza de datos","heading":"Resumen","text":"En un proyecto de ciencia de datos deben realizarse procesos de integración y limpieza previos la fase de modelización, para asegurar niveles adecuados de calidad. Por ello, tras las labores iniciales de depuración, debe comprobarse si los datos son o consistentes, y si siguen o las reglas del campo de negocio del cual proceden. En este capítulo se abordan las cuestiones relativas la integración de conjuntos de datos, su limpieza y depuración, y se proponen procedimientos para la validación de los mismos.En un proyecto de ciencia de datos deben realizarse procesos de integración y limpieza previos la fase de modelización, para asegurar niveles adecuados de calidad. Por ello, tras las labores iniciales de depuración, debe comprobarse si los datos son o consistentes, y si siguen o las reglas del campo de negocio del cual proceden. En este capítulo se abordan las cuestiones relativas la integración de conjuntos de datos, su limpieza y depuración, y se proponen procedimientos para la validación de los mismos.El conjunto de datos utilizado en este capítulo está disponible en el paquete `idealista18; en concreto, se utilizan los datos de Madrid.El conjunto de datos utilizado en este capítulo está disponible en el paquete `idealista18; en concreto, se utilizan los datos de Madrid.partir de estos datos, se muestra un ejemplo de integración de datos espaciales y se diseña un marco de limpieza genérico basado en una serie de pasos básicos.partir de estos datos, se muestra un ejemplo de integración de datos espaciales y se diseña un marco de limpieza genérico basado en una serie de pasos básicos.Para la realización de estas funciones de integración y limpieza de datos, se proponen distintas funciones de los paquetes tidyverse (para la manipulación de ficheros y variables) y caret (para la imputación de valores perdidos).Para la realización de estas funciones de integración y limpieza de datos, se proponen distintas funciones de los paquetes tidyverse (para la manipulación de ficheros y variables) y caret (para la imputación de valores perdidos).Para el tratamiento de datos espaciales se utiliza el paquete sf. La visualización de los mismos se lleva cabo con GGAlly y naniar.Para el tratamiento de datos espaciales se utiliza el paquete sf. La visualización de los mismos se lleva cabo con GGAlly y naniar.Finalmente, se utiliza la función locate_errors(), del paquete errorlocate, para determinar cuáles son las variables con errores, de acuerdo las reglas establecidas y dcmodify para realizar imputaciones determinísticas.Finalmente, se utiliza la función locate_errors(), del paquete errorlocate, para determinar cuáles son las variables con errores, de acuerdo las reglas establecidas y dcmodify para realizar imputaciones determinísticas.","code":""},{"path":"chap-feature.html","id":"chap-feature","chapter":"Capítulo 9 Selección y transformación de variables","heading":"Capítulo 9 Selección y transformación de variables","text":"Jorge Velasco López\\(^{}\\) y José-María Montero\\(^{b}\\)\\(^{}\\)Instituto Nacional de Estadística de España\n\\(^{b}\\)Universidad de Castilla-La Mancha","code":""},{"path":"chap-feature.html","id":"introducción-4","chapter":"Capítulo 9 Selección y transformación de variables","heading":"9.1 Introducción","text":"Como se indicó en el Cap. ??, la preparación de datos, en un contexto de ciencia de datos, consiste en transformarlos de tal forma que se puedan utilizar adecuadamente en las fases posteriores de modelado. Esta preparación o pre-preprocesamiento puede ser un proceso laborioso e incluye tareas como la integración y limpieza de datos, que se detallaron en dicho capítulo.El presente capítulo aborda las tareas relativas la selección de variables (feature selection) y transformación de variables.\nLa selección de variables tiene como objetivo elegir el elenco de variables más relevantes para el análisis. La transformación de variables hace referencia, básicamente, al uso de determinados procedimientos para modificar la distribución de la variable objetivo, la ingeniería de variables (feature engineering), normalización y la reducción de la dimensionalidad del problema de interés.\nSe usará el conjunto de datos Madrid_Sale (disponibles en el paquete de R Idealista18), con datos inmobiliarios del año 2018 para el municipio de Madrid, y los paquetes caret (Kuhn 2008), para diversas tareas de preparación de datos y corrplot (Wei et al. 2017), para visualizar correlaciones, entre otros.","code":""},{"path":"chap-feature.html","id":"feature","chapter":"Capítulo 9 Selección y transformación de variables","heading":"9.2 Selección de variables","text":" Quizás, el primer gran reto al que se enfrenta el científico de datos cuando maneja grandes conjuntos de datos es la identificación de las variables que proporcionen información valiosa sobre la variable objetivo, bien se trate de un problema de regresión o de clasificación. En caso de que el científico de datos salga exitoso de este primer gran reto, un determinado subconjunto de variables del conjunto de datos de interés proporcionará la misma información sobre la variable objetivo que la totalidad de variables incluidas en el conjunto de datos.En consecuencia, la selección de variables involucra un conjunto de técnicas cuyo objetivo es seleccionar el subconjunto de variables predictoras más relevante para las fases de modelización. Esto es importante porque:Variables predictoras redundantes pueden distraer o engañar los algoritmos de aprendizaje, lo que posiblemente se traduzca en un menor rendimiento, solo predictivo (exactitud y precisión), sino también en términos de tiempo de computación.Igualmente, la inclusión de variables irrelevantes aumenta el coste computacional y dificulta la interpretabilidad.\nUna adecuada selección de variables tiene ventajas importantes: \\(()\\) elimina las variables con información redundante; \\((ii)\\) reduce el grado de complejidad de los modelos; \\((iii)\\) evita o reduce el sobreajuste; \\((iv)\\) incrementa de la precisión de las predicciones; y \\((iv)\\) reduce la carga computacional.obstante, es importante señalar que, antes de llevarse cabo la selección de variables propiamente dicha, debe comprobarse la magnitud de la varianza de las variables candidatas ser seleccionadas y de sus correlaciones dos dos, así como si existen combinaciones lineales entre ellas (multicolinealidad). Y ello, porque estas tres comprobaciones sirven para realizar una primera pre-selección de variables, si bien por razones técnicas y de capacidad de explicación del comportamiento de la variable respuesta.Los métodos de selección de variables (tras la pre-selección anteriormente mencionada) se suelen clasificar en: \\(()\\) los que utilizan la variable objetivo (supervisados); y \\((ii)\\) los que (supervisados). Debido la complejidad de la cuestión, se pasará revista únicamente los métodos supervisados más relevantes, que se pueden dividir en:Métodos tipo filtro, que puntúan de mayor menor cada variable predictora en base su capacidad predictiva y seleccionan un subconjunto de ellas en base dichas puntuaciones (Brownlee 2020).Métodos tipo filtro, que puntúan de mayor menor cada variable predictora en base su capacidad predictiva y seleccionan un subconjunto de ellas en base dichas puntuaciones (Brownlee 2020).Métodos tipo envoltura (wrapper), que eligen el subconjunto de variables que dan como resultado el modelo con mayores prestaciones en cuanto calidad de resultados y eficiencia: error de predicción o clasificación, precisión, tiempo de computación…Métodos tipo envoltura (wrapper), que eligen el subconjunto de variables que dan como resultado el modelo con mayores prestaciones en cuanto calidad de resultados y eficiencia: error de predicción o clasificación, precisión, tiempo de computación…Métodos intrínsecos (o embedded), que seleccionan las variables automáticamente como parte del ajuste del modelo durante el entrenamiento (tal es el caso de algunos modelos de regresión penalizados, como Lasso, árboles de decisión y bosques aleatorios (random forests)).Métodos intrínsecos (o embedded), que seleccionan las variables automáticamente como parte del ajuste del modelo durante el entrenamiento (tal es el caso de algunos modelos de regresión penalizados, como Lasso, árboles de decisión y bosques aleatorios (random forests)).\n","code":""},{"path":"chap-feature.html","id":"pre-selección-de-variables","chapter":"Capítulo 9 Selección y transformación de variables","heading":"9.2.1 Pre-selección de variables","text":"","code":""},{"path":"chap-feature.html","id":"salenum","chapter":"Capítulo 9 Selección y transformación de variables","heading":"9.2.1.1 Varianza nula","text":"\nUno de los aspectos fundamentales en la selección de variables es comprobar si su varianza es cero o cercana cero porque, si es así, sus valores son iguales o similares, respectivamente, y, por tanto, esas variables estarán perfectamente o cuasi-perfectamente correladas con el término independiente del modelo, con lo cual, en el mejor de los casos, solo añadirán ruido al modelo. Además, este tipo de variables causan problemas la hora de dividir el conjunto de datos en subconjuntos de entrenamiento, validación y test. Las causas de una nula o muy pequeña variabilidad pueden estar en haber medido la variable en una escala inapropiada para la variable o en haber expandido una variable politómica en varias dicotómicas (una por categoría), entre otras. En el primer caso, un cambio de escala puede evitar el problema de la colinealidad. Otra opción más drástica la eliminación de la variable.continuación, se comprueba si las variables del conjunto de datos Madrid_Sale tienen varianza cero. Para ello se utiliza la función nearZeroVar() del paquete caret.Se seleccionan en primer lugar las variables numéricas en el conjunto de datos Madrid_Sale_num.Se observa que se devuelve el valor nzv=FALSE (nzv: near zero variance) para casi todas las variables, con la excepción de PARKINGSPACEPRICE, ISDUPLEX, ISSTUDIO, ISINTOPFLOOR y BUILTTYPEID_1, que podrían descartarse como variables predictoras.Para filtrar (excluir) las variables que se descartan como predictoras, se procede como sigue:","code":"\nlibrary(\"idealista18\")\nlibrary(\"tidyverse\")\nlibrary(\"caret\")\n\nMadrid_Sale <- as.data.frame(Madrid_Sale)\nnumeric_cols <- sapply(Madrid_Sale, is.numeric)\nMadrid_Sale_num <- Madrid_Sale[, numeric_cols]\nvarianza <- nearZeroVar(Madrid_Sale_num, saveMetrics = T)\n# Con el argumento saveMetrics, se guardan los valores que se han utilizado para los cálculos.\n# Se muestran los primeros resultados\nhead(varianza, 2)\n#>       freqRatio  percentUnique   zeroVar   nzv\n#> PERIOD  2.019617   0.004218742   FALSE   FALSE\n#> PRICE   1.076923   2.911986500   FALSE   FALSE\nMadrid_Sale_num <- Madrid_Sale_num|>\n  select(-c(PARKINGSPACEPRICE, ISDUPLEX, ISSTUDIO, BUILTTYPEID_1))"},{"path":"chap-feature.html","id":"correlación-entre-variables","chapter":"Capítulo 9 Selección y transformación de variables","heading":"9.2.1.2 Correlación entre variables","text":"Como se avanzó anteriormente, otra de las cuestiones tener en cuenta en el proceso de selección de variables es la magnitud de las correlaciones entre las variables candidatas, pues la existencia de correlaciones elevadas tiene consecuencias perversas sobre la fiabilidad de las predicciones (o de la clasificación realizada). En el caso extremo el modelo tendrá problemas de colinealidad o multicolinealidad (véase Sec. 9.2.1.3).Para detectar las variables con muy elevada correlación entre ellas, se le pasa la función findCorrelation() de caret, con valor 0,9, la matriz de correlaciones lineales entre las variables susceptibles de ser seleccionadas.Con ello, se comprueba que la variable HASPARKINGSPACE tiene correlaciones superiores 0,9 con varias de las variables predictoras, procediéndose su eliminación.la Fig. 9.1, generada con el paquete corrplot, muestra las correlaciones existentes entre las primeras variables predictoras.\nFigura 9.1: Matriz de correlaciones topada en 0,9\nSe aprecia que ya hay variables altamente correlacionadas.","code":"\nmadrid_cor <- cor(Madrid_Sale_num[, 1:20]) \nalta_corr <- findCorrelation(madrid_cor, cutoff = .9)\nMadrid_Sale_num <- Madrid_Sale_num[, -alta_corr]\nlibrary(\"corrplot\")\n\nmatriz_corr <- cor(Madrid_Sale_num[, 1:8])\ncorrplot(matriz_corr, method = \"circle\")"},{"path":"chap-feature.html","id":"combinaciones-lineales","chapter":"Capítulo 9 Selección y transformación de variables","heading":"9.2.1.3 Combinaciones lineales","text":"En la práctica, en la mayoría de los casos, por ejemplo en las regresiones lineales, las variables que se utilizan como predictores son ortogonales sino que tienen cierto grado de dependencia lineal entre ellas. Si dicho grado es moderado, las consecuencias de la ortogonalidad en la predicción son graves, pero en los casos de dependencia lineal cuasi-perfecta las inferencias resultantes del modelo estimado distan mucho de la realidad. Dichas consecuencias son aún más graves en el caso de que las combinaciones lineales sean perfectas. Por ello, la existencia de colinealidad o combinaciones lineales entre las variables seleccionables también es una circunstancia evitar. En el caso de que los predictores (o varios de ellos) conformen una o varias combinaciones (o cuasi-combinaciones) lineales, se puede conocer el impacto específico de cada uno de ellos en la variable objetivo, pues dichos impactos se solapan unos con otros. Además, como se ha avanzado, las predicciones son fiables, entre otras cosas (véase (pena2002analisis?)). Y es que se le está pidiendo al conjunto de datos en estudio más información sobre la variable objetivo de la que realmente tiene. Entre otros modelos, la regresión lineal y la regresión logística parten del supuesto de colinealidad o multicolinealidad entre las variables, por lo que debería haber variables correlacionadas, ni dos dos, ni en forma de combinación lineal entre varias de ellas.\nLas principales fuentes de multicolinealidad son:\nEl método utilizado en la recogida de datos (subespacios).Restricciones en el modelo o en la población (existencia de variables correlacionadas).Especificación del modelo (polinomios).Más variables que observaciones.En cuanto al detalle de las consecuencias más importantes de la multicolinealidad, hay que señalar las siguientes:Los estimadores tendrán grandes varianzas y covarianzas.Las estimaciones de los coeficientes del modelos serán demasiado grandes.Los signos de los coeficientes estimados suelen ser distintos los esperados.Pequeñas variaciones en los datos, o en la especificación del modelo, provocarán grandes cambios en los coeficientes.En el ejemplo con los datos del conjunto Madrid_Sale se utiliza la función findLinearCombos() de caret para encontrar, en caso de que las haya, combinaciones lineales de las variables predictoras.Como puede comprobarse, se encuentra ninguna combinación lineal en las variables numéricas de Madrid_Sale. En caso de existir, una solución al problema de la multicolinealidad pasa por:Eliminar variables predictoras que se encuentren altamente relacionadas con otras que permanecen en el modelo.Eliminar variables predictoras que se encuentren altamente relacionadas con otras que permanecen en el modelo.Sustituir las variables predictoras por componentes principales (véase Cap. 32).Sustituir las variables predictoras por componentes principales (véase Cap. 32).Incluir información externa los datos originales. Esta alternativa implica utilizar estimadores contraídos (de Stein o ridge) o bayesianos.Incluir información externa los datos originales. Esta alternativa implica utilizar estimadores contraídos (de Stein o ridge) o bayesianos.continuación se muestra, caso de existir, cómo se eliminarían las combinaciones lineales:","code":"\nMadrid_Sale_num_na <- tidyr::drop_na(Madrid_Sale_num) # Es necesario eliminar los NA.\ncombos <- findLinearCombos(Madrid_Sale_num_na)\ncombos \n#$remove\n#NULL\nMadrid_Sale_num_na[, -combos$remove]"},{"path":"chap-feature.html","id":"métodos-de-selección-de-variables","chapter":"Capítulo 9 Selección y transformación de variables","heading":"9.2.2 Métodos de selección de variables","text":"Tras la pre-selección de variables llevada cabo en el epígrafe anterior, procede la selección de variables, propiamente dicha, de entre las que han superado la fase previa, en base, principalmente, criterios de capacidad predictiva. obstante, también se utilizan para:Simplificar de modelos para hacerlos más interpretables.Mejorar la precisión del modelo (si se ha escogido bien el subconjunto de variables).Reducir el tiempo de computación; sobre todo, entrenar algoritmos mayor velocidad.Evitar la maldición de la dimensionalidad (o efecto Huges), que se refiere las consecuencias deseadas que tienen lugar cuando la dimensionalidad de un problema es muy elevada.Reducir la probabilidad de sobreajuste.","code":""},{"path":"chap-feature.html","id":"metfiltro","chapter":"Capítulo 9 Selección y transformación de variables","heading":"9.2.2.1 Métodos tipo filtro","text":"\nLos métodos de selección de variables tipo filtro usan técnicas estadísticas para evaluar la relación entre cada variable predictora (o de entrada, o independiente) y la variable objetivo (o de salida, o dependiente). Generalmente, consideran la influencia de cada variable predictora sobre la variable objetivo por separado. Las puntuaciones obtenidas se utilizan como base para clasificar y elegir las variables predictoras que se utilizarán en el modelo.La elección de las técnicas estadísticas depende del tipo de variables (objetivo y predictoras). Por ejemplo, si las variables de entrada (predictoras) y salida (objetivo) fueran numéricas, se utilizaría\nel coeficiente de correlación de Pearson o el de Spearman (dependiendo de si la relación entre la variable predictora y la variable objetivo es lineal o ) o el método de información mutua (véase Vergara Estévez (2014)). Si ambas fuesen categóricas, podrían usarse medidas de asociación para tablas de contingencia \\(2\\times 2\\) o \\(R\\times C\\) (véanse Sec. 23.4 y 23.5). Si la de entrada fuese categórica y la de salida numérica, la técnica adecuada sería el Análisis de la Varianza (ANOVA, véase Sec. 15.4.6.1). Si la categórica fuese la de salida y la numérica la de entrada, entonces habría que acudir la regresión logística (véase Sec. ??), por ejemplo. Sin embargo, el conjunto de datos tiene porqué tener sólo un tipo de variable de entrada. Para manejar diferentes tipos de variables de entrada, se pueden seleccionar, por separado, variables de entrada numéricas y variables de entrada categóricas, usando en cada caso las técnicas apropiadas.\nEstos métodos suelen eliminar sólo las variables de menor interés la hora de predecir/clasificar. Permiten ahorrar tiempo y son especialmente robustos para el sobreaprendizaje. Sin embargo, tienen en cuenta las relaciones entre las variables, lo que puede dar lugar seleccionar variables redundantes si es que se ha llevado cabo una fase de pre-selección.NotaExisten diversos paquetes, como FSelector (Romanski, Kotthoff, Kotthoff 2013) y el mismo caret, para implementar técnicas de selección de variables. Aquí se utiliza FSinR, que contiene una serie de métodos de filtro y envoltura, que se combinan con algoritmos de búsqueda para obtener el subconjunto óptimo de variables, usando funciones para entrenar modelos de clasificación y regresión disponibles en el paquete caret.\nLa selección de variables o características se lleva cabo con la función FeatureSelection(), y la del algoritmo de búsqueda que se utilizará en el proceso de selección de funciones se realiza con la función searchAlgorithm(). Por su parte, los métodos de filtrado se implementan través de la función filterEvaluator().\ndebe olvidarse que, antes de realizar el proceso de selección de variables, el usuario tiene que dividir el conjunto de datos convenientemente para llevar cabo cada operación sobre el subconjunto correcto (véase Cap. 10). Igualmente, también de manera previa, se tiene que resolver el problema de los datos faltantes.continuación se muestra un ejemplo para variables predictoras numéricas. Para ello, se toma una muestra del conjunto de datos Madrid_Sale_num, obtenido en la Sec. 9.2.1.1. Una vez en disposición de la muestra, primeramente se transforma la variable objetivo en categórica, siendo las categorías (intervalos) cuatro cortes de la distribución de sus valores; dicha categorización se lleva cabo mediante binning.66 También se eliminan los registros con datos faltantes.Una vez discretizada la variable objetivo, se selecciona el conjunto de variables predictoras de la variable objetivo price_bin, que es la variable PRICE transformada mediante binning. Como método tipo filtro se utiliza minimum description length (MDLM), que es un método de selección de variables que se basa en una medida de la complejidad del modelo denominada “longitud mínima de la descripción” (de ahí el nombre del modelo), por lo que su objetivo es encontrar el modelo más sencillo que proporcione una explicación aceptable de los datos. Como algoritmo de búsqueda se utiliza sequential forward selection.67En este caso, con los argumentos propuestos, el modelo seleccionado para explicar el comportamiento de la variable objetivo price_bin contiene únicamente el término independiente y una variable predictora: HASTERRACE.","code":"\nlibrary(\"rsample\")\n\n# Se toma una muestra con el paquete rsample\nset.seed(7)\nMadrid_Sale_num_sample <- sample(1:nrow(Madrid_Sale_num), size = 5000, replace = FALSE)\nMadrid_Sale_num_sample <- Madrid_Sale_num[Madrid_Sale_num_sample, ]\n# Se realiza binning con cuatro bins\nMadrid_Sale_num_sample_bin <- Madrid_Sale_num_sample |>\n  mutate(price_bin = cut(PRICE, breaks = c(0, 250000, 500000, 750000, 10000000), labels = c(\"primerQ\", \"segundoQ\", \"tercerQ\", \"c\"), include.lowest = TRUE)) |>\n  select(price_bin, CONSTRUCTEDAREA, ROOMNUMBER, BATHNUMBER, HASTERRACE, HASLIFT)\n# Se eliminan los registros con valores missing\nMadrid_Sale_sample_na <- drop_na(Madrid_Sale_num_sample_bin)\nlibrary(\"FSinR\")\n\n# Método tipo filtro MDLC (Minimum-Description_Length-Criterion)\nevaluador <- filterEvaluator(\"MDLC\")\n# Se genera el algoritmo de búsqueda\nbuscador <- searchAlgorithm(\"sequentialForwardSelection\")\n# Se implementa el proceso, pasando a la función los dos parámetros anteriores\nresultados <- featureSelection(Madrid_Sale_sample_na, \"price_bin\", buscador, evaluador)\n# Se muestran los resultados\nresultados$bestFeatures\n#>      CONSTRUCTEDAREA ROOMNUMBER BATHNUMBER HASTERRACE HASLIFT\n#> [1,]               0          0          0          1       0\nresultados$bestValue\n#> [1] 355.3439"},{"path":"chap-feature.html","id":"métodos-de-selección-de-variables-tipo-envoltura-wrapper","chapter":"Capítulo 9 Selección y transformación de variables","heading":"9.2.2.2 Métodos de selección de variables tipo envoltura (wrapper)","text":"Este enfoque realiza una búsqueda través de diferentes combinaciones o subconjuntos de variables predictoras/clasificadoras para comprobar el efecto que tienen en la precisión del modelo (Saeys, Inza, Larranaga 2007).\nHay varias alternativas:Evaluar las variables individualmente y seleccionar las \\(n\\) variables principales que obtienen unas buenas prestaciones, aunque se pierde la información de las dependencias entre variables.Observar el rendimiento del modelo para todas las combinaciones de variables posibles. En este sentido, se puede utilizar un algoritmo de búsqueda global estocástica, como los algoritmos genéticos que, si bien pueden ser efectivos, también pueden ser computacionalmente muy costosos.Los métodos wrapper son de gran eficacia la hora de eliminar variables irrelevantes y/o redundantes (cosa que ocurre en los de tipo filtro porque se centran en el poder predictor de cada variable de forma aislada). Además, tienen en cuenta la circunstancia de que dos o más variables, aparentemente irrelevantes en cuanto su capacidad predictiva o clasificatoria cuando se consideran una por una, pueden ser relevantes cuando se consideran conjuntamente. Sin embargo, son muy lentos, ya que tienen que aplicar muchísimas veces el algoritmo de búsqueda, cambiando, cada vez, el número de variables, siguiendo, cada vez, algún criterio tanto de búsqueda como de paro. En lo que respecta los criterios de búsqueda, estos son similares los de los métodos tipo filtro. Por lo que se refiere los criterios de paro, los usados en los métodos wrapper son menos eficientes que los criterios basados en algún tipo de medida de ganancia de información, distancia o consistencia, entre el predictor y la\nvariable objetivo (o clase) que utilizan los de tipo filtro.NotaLas principales diferencias entre los métodos tipo filtro y tipo envoltura son las siguientes:• Los métodos de filtro cuantifican la relevancia de las variables por su correlación con la variable salida, mientras que los métodos de tipo envoltura cuantifican las prestaciones del modelo para diferentes subconjuntos de variables.• Los métodos de filtro tienen una carga computacional enormemente inferior la de los envolventes, ya que necesitan entrenar ningún modelo.• Los métodos de filtro utilizan métodos estadísticos para evaluar la selección de variables; los de tipo envoltura utilizan métodos de validación cruzada.• En la mayoría de ocasiones, la selección de variables realizada por los métodos tipo envoltura suele ser más exitosa que la proporcionada por los métodos de filtro.• Los métodos de envoltura tienen una probabilidad de sobreajuste mucho mayor que los de filtro.continuación, sobre el conjunto de datos Madrid_Sale_sample_na y sobre la variable objetivo price_bin se establecen tanto los parámetros de los algoritmos de búsqueda como los métodos de filtrado y se calculan los resultados, usando para ello de FSinR. Como método de selección de variables se utiliza un método wrapper (con la función wrapperEvaluator()de FSinR) y como algoritmo de búsqueda sequential forward selection.El resultado es el mismo que con el con el método tipo filtro anteriormente utilizado: el modelo seleccionado para explicar el comportamiento de la variable objetivo price_bin contiene únicamente el término independiente y una variable predictora: HASTERRACE.NotaSe puede sofisticar más el modelo ajustando los parámetros del modelo con parámetros de remuestreo, que son los mismos argumentos que se pasan la función trainControl() del paquete caret. En segundo lugar, se pueden establecer los parámetros de ajuste, que son los mismos que para la función de train de caret.","code":"\n# Se fijan los parámetros\nevaluador <- wrapperEvaluator(\"rpart1SE\")\nbuscador <- searchAlgorithm(\"sequentialForwardSelection\")\n# Se evalúan sobre Madrid_Sale_sample_na\nresults <- featureSelection(Madrid_Sale_sample_na, \"price_bin\", buscador, evaluador)\nresultados$bestFeatures\nresultados$bestValue"},{"path":"chap-feature.html","id":"métodos-de-selección-tipo-intrínseco-embedded","chapter":"Capítulo 9 Selección y transformación de variables","heading":"9.2.2.3 Métodos de selección tipo intrínseco (embedded) ","text":"Finalmente, hay algunos algoritmos de aprendizaje automático que realizan la selección automática de variables como parte del aprendizaje del modelo. Estos son los métodos de selección de tipo intrínseco, que aglutinan las ventajas de los métodos de filtro y envoltura.Un ejemplo son los relativos los modelos de regresión penalizados, como Lasso, o ridge (que tienen funciones de penalización incluidas para reducir el sobreajuste), árboles de decisión y bosques aleatorios.En el siguiente ejemplo se modeliza un bosque aleatorio (usando el paquete randomForest) y, tras dicha modelización, se identifica el conjunto óptimo de variables con la función varImp() de caret.Con este método de selección de variables, el modelo con mayor poder predictivo de la variable salida price_bin es el que contiene un término independiente y los predictores CONSTRUCTEDAREA, ROOMNUMBER y BATHNUMBER (ejecútese el código para comprobarlo).","code":"\nlibrary(\"randomForest\")\n\n# Usar random forest para la selección de variables\nrf_modelo <- randomForest(price_bin ~ ., data = Madrid_Sale_num_sample_bin)\n\n# Listar las variables más importantes\nvarImp(rf_modelo)"},{"path":"chap-feature.html","id":"transformación-de-variables","chapter":"Capítulo 9 Selección y transformación de variables","heading":"9.3 Transformación de variables","text":"La transformación y creación de variables predictoras partir de los datos en bruto tiene una componente técnica y otra más creativa; en esta última, son de gran relevancia la intuición y la experiencia en trabajos de modelado, así como el dominio de los datos en cuestión. Para labores de transformación también se utilizará el paquete caret.NotaCaret se ha elegido como herramienta principal para la parte de preprocesamiento por su amplia difusión y porque también se utiliza en la parte de machine learning supervisado de este libro. obstante, se podrían usar otros paquetes, como recipes, incluido en tidymodels. Este tipo de paquetes, comúnmente llamados metapaquete (meta-packages), permiten agrupar varios programas junto sus dependencias para su instalación de una vez. Por tanto, un metapaquete permite ahorrar tiempo y esfuerzo la vez que facilita la implementación de múltiples modelos en paralelo para, posteriormente, vincular sus resultados.\nLa fase de modelización puede condicionar la fase previa de preparación de datos. Por ejemplo, determinadas técnicas imponen requisitos y expectativas sobre el tipo y forma de las variables predictoras (Brad Boehmke Greenwell 2019). Así, podría ser necesario que la variable objetivo tenga una distribución de probabilidad específica, o la eliminación de variables predictoras altamente correlacionadas con otras y/o que estén fuertemente relacionadas con la variable objetivo.Generalmente, estas transformaciones son más útiles para algoritmos como los de regresión, métodos basados en instancias (también llamados memory-based learning methods, como k-vecinos más cercanos -KNN- y Learning Vector Quantization -LVQ-), máquinas de vectores de soporte -SVM- y redes neuronales -NN-, que para métodos basados en árboles y reglas.68","code":""},{"path":"chap-feature.html","id":"trans-vble-obj","chapter":"Capítulo 9 Selección y transformación de variables","heading":"9.3.1 Transformación de la distribución de la variable objetivo","text":"Aunque siempre es necesario, la transformación de la distribución de la variable objetivo puede llevar una mejora predictiva significativa, especialmente en el caso de modelos paramétricos. Por ejemplo, los modelos de regresión lineal ordinarios asumen que el término de error, y, por consiguiente, la variable objetivo, se distribuyen normalmente. Pero puede ocurrir, por ejemplo, que la variable objetivo tenga valores atípicos y la suposición de normalidad se cumpla por asimetricidad. .Para simetrizar la distribución de probabilidad de la variable objetivo (mejorando así la dispersión de valores y, veces, desenmascarando las relaciones lineales y aditivas entre los predictores y el objetivo) se puede usar una transformación log (entre otras). Para corregir la asimetría positiva de la distribución probabilística de la variable objetivo se suele utilizar una de las dos opciones siguientes:Normalizar con una transformación logarítmica, que proporciona buenos resultados en la mayoría de los casos. En la Fig. 9.2, se puede comprobar que, en el ejemplo que se viene arrastrando, una transformación logarítmica normaliza, en gran medida, la distribución de la variable PRICE. Nótese que, si la variable objetivo tiene valores negativos o cero, una transformación logarítmica producirá \\(NaN\\) y \\(- Inf\\), respectivamente. Si los valores de respuesta positivos son pequeños (por ejemplo, entre \\(-0.99\\) y \\(0\\)), se puede aplicar una pequeña compensación (por ejemplo, la función log1p() agrega un 1 al valor antes de aplicar la transformación).\nFigura 9.2: Normalización logarítmica\nComo segunda opción, se puede usar una transformación de la familia de transformaciones Box-Cox (o simplemente una transformación de Box-Cox), de carácter potencial y con mayor flexibilidad que la transformación logarítmica. Generalmente, se puede encontrar la función adecuada partir de una familia de transformadas de potencia, que llevarán la distribución de la variable transformada tan cerca como sea posible de la distribución normal [Sakia (1992)]69. obstante, igual que la transformación logarítmica, las transformaciones del tipo Box-Cox también tienen la limitación de ser sólo aplicables variables cuyos valores sean positivos. Por consiguiente, tanto si se usa una transformación log como una Box-Cox, se deben centrar los datos primero, ni realizar ninguna operación que pueda hacer que los valores de la variable transformada sean positivos.Como segunda opción, se puede usar una transformación de la familia de transformaciones Box-Cox (o simplemente una transformación de Box-Cox), de carácter potencial y con mayor flexibilidad que la transformación logarítmica. Generalmente, se puede encontrar la función adecuada partir de una familia de transformadas de potencia, que llevarán la distribución de la variable transformada tan cerca como sea posible de la distribución normal [Sakia (1992)]69. obstante, igual que la transformación logarítmica, las transformaciones del tipo Box-Cox también tienen la limitación de ser sólo aplicables variables cuyos valores sean positivos. Por consiguiente, tanto si se usa una transformación log como una Box-Cox, se deben centrar los datos primero, ni realizar ninguna operación que pueda hacer que los valores de la variable transformada sean positivos.En caso de valores nulos o negativos, una muy buena opción, la tercera, es la transformación Yeo-Johnson, que es una extensión de la transformación Box-Cox que está limitada los valores positivos.En caso de valores nulos o negativos, una muy buena opción, la tercera, es la transformación Yeo-Johnson, que es una extensión de la transformación Box-Cox que está limitada los valores positivos.Hay que tener en cuenta que, cuando se modela con una variable objetivo transformada, las predicciones también estarán en la escala transformada. Es posible que haya que deshacer (o volver transformar) los valores pronosticados su escala original, para que los responsables de la toma de decisiones puedan interpretar los resultados más fácilmente.NotaEl paquete recipes (recetas de cocina), incluido en tidymodels, permite la transformación de variables de forma secuencial. La transformación de la distribución probabilística de la variable objetivo con recipes se lleva cabo en 4 etapas: \\(()\\) recipe(), donde se especifica la fórmula (variables predictoras y variable objetivo); \\((ii)\\) step(), donde se definen los pasos seguir: imputación de valores perdidos, creación de variables ficticias (dummies), normalización, etc.; \\((iii)\\) prep (preparar, o en otros términos, entrenar), donde que se utiliza un conjunto de datos para analizar cada paso en él; y \\((iv)\\) bake (hornear/cocinar), donde, una vez aplicada la receta, se aplica al conjunto de datos. La idea detrás de recipes es similar caret::preProcess(). Sin embargo, diferencia de caret, maneja automáticamente las variables categóricas y requiere\ncrear variables ficticias manualmente.","code":"\nrespuesta_log <- log(Madrid_Sale$PRICE)\nrespuesta_boxcox <- preProcess(Madrid_Sale_num_sample, method = \"BoxCox\")\ntrainBC <- predict(respuesta_boxcox, Madrid_Sale_num_sample)\nrespuesta_boxcox\n#> Created from 5000 samples and 2 variables\n#>\n#> Pre-processing:\n#> - Box-Cox transformation (2)\n#> - ignored (0)\n#>\n#> Lambda estimates for Box-Cox transformation:\n#> -0.3, -0.3"},{"path":"chap-feature.html","id":"cambios-de-origen-y-escala-en-las-variables-normalizaciones","chapter":"Capítulo 9 Selección y transformación de variables","heading":"9.3.2 Cambios de origen y escala en las variables (normalizaciones)","text":"La escala en que se miden las variables individuales es una cuestión baladí la hora de la modelización. Los modelos que incorporan funciones lineales en las variables predictoras, son sensibles la escala de esas variables. Lo mismo puede decirse de los algoritmos que utilizan medidas de distancia, como los de agrupación y clasificación, o los de escalamiento multidimensional, entre otros; o los de reducción de la dimensionalidad. Cuando se estiman modelos, menudo es aconsejable modificar la escala de las variables predictoras; el objetivo es evitar que unas variables tengan mayor influencia que otras en el resultado obtenido. Por ejemplo, en el conjunto de datos Madrid_Sale la superficie de las viviendas, medida en metros cuadrados, tiene una media y una desviación típica mayores que la antigüedad de la misma, medida en años. En consecuencia, los algoritmos basados en la magnitud de los errores pueden dar más importancia las variables con mayor desviación típica, pero porque tengan mayor variabilidad real que las otras, sino porque la medida de dicha variabilidad (la desviación típica) es más grande debido la distinta escala en la que están medidas dichas variables. La consecuencia: efectos perniciosos indeseados sobre las predicción o la clasificación.La normalización de variables tiene como objetivo que las comparaciones entre estas variables, en cuanto su contribución al análisis de interés, sean objetivas; es decir, ponerlas en igualdad de condiciones en lo que respecta su influencia (más allá de la que realmente tienen) en la variable objetivo.La estandarización (o normalización z-score) es el método de normalización de variables más popular. Consiste en restar la media de la variable sus los valores y, posteriormente, dividir esta diferencia entre la desviación típica de la variable. De esta manera, las variables (numéricas) transformadas tendrán media nula y varianza unitaria, lo que proporciona una unidad de medida comparable común todas las variables: la distancia la media medida en términos de desviaciones típicas.modo de ejemplo, continuación se estandarizan las variables del conjunto de datos Madrid_Sale con la función preProcess() de caret y el method=c('center', 'scale'), de tal manera su media sea nula y su desviación típica unitaria.Otra normalización también popular es la min-max, que re-escala los valores de la variable entre 0 y 1, o entre -1 y 1, y cuya expresión general es:\\[X_{norm}=\\frac{X-\\min(X)}{\\max(X)-\\min(X)}.\\]\nSi se desea re-escalar entre dos valores arbitrarios, y b, la expresión anterior se transforma como sigue:\\[X_{norm}=+ \\frac{(X-\\min(X))(b-)}{\\max(X)-\\min(X)},\\]Otras opciones de normalización pueden verse en la amplia literatura sobre la cuestión.Finalmente, recordar que, cuando se lleva cabo un proceso de normalización de variables, hay que hacerlo tanto en el subconjunto de entrenamiento como en el de test, para que ambos se basen en la misma media y varianza.","code":"\nprep_centrado <- preProcess(Madrid_Sale_num, method = c(\"center\",\"scale\"))\npred_centrado<-predict(prep_centrado, Madrid_Sale_num)[1:3]\nhead(pred_centrado, n = 3)\n#>       PRICE    CONSTRUCTEDAREA     ROOMNUMBER\n#1    -1.523435    -0.64763050    -0.5764192    \n#2    -1.523435    -0.38628625    0.4062338    \n#3    -1.523435    -0.05541004    0.7717038    "},{"path":"chap-feature.html","id":"ingeniería-de-variables-feature-engineering","chapter":"Capítulo 9 Selección y transformación de variables","heading":"9.3.3 Ingeniería de variables (feature engineering) ","text":"La ingeniería de variables consiste en el proceso de conseguir, partir de la información disponible, las variables idóneas (y el en número apropiado) para que los modelos o clasificadores proporcionen los mejores resultados posibles, dados los datos disponibles y el modelo ejecutar. En otros términos, es el proceso de transformación de las variables seleccionadas, de forma que se obtenga el mejor rendimiento posible de los modelos de machine learning. Por ejemplo, transformar las variables relacionadas con la fecha de tal manera que se diferencie según el tipo de horario (“de oficina” y “de descanso”), o que se considere la cercanía al momento actual (los datos más cercanos contienen más información); los filtros de imagen (desenfocar una imagen) y la conversión de texto en números (utilizando el procesamiento avanzado del lenguaje natural, que asigna palabras un espacio vectorial) son también ejemplos interesantes.La mayoría de los modelos requieren que los predictores tengan forma numérica, por lo que, en caso de tener predictores de carácter categórico, hay que transformarlos en numéricos. Para implementar otro tipo de modelos, conviene transformar alguna(s) variable numérica en categórica. En el primer caso, conviene aplicar técnicas de agrupamiento (o binning), que crean agrupaciones o intervalos partir de variables continuas; en el segundo, las técnicas de codificación, permiten tratar variables categóricas como si fueran continuas. Hay casos, como el de los modelos basados en árboles, que manejan, de manera natural, variables numéricas y categóricas; pero incluso en estos modelos se puede mejorar su rendimiento si se preprocesan las variables categóricas.La identificación entre las labores de selección y de transformación de variables es bastante frecuente; sin embargo, es errónea, pues, si bien tienen algunos solapamientos, sus objetivos son claramente distintos. La ingeniería de variables tiene como objetivo la construcción de modelos más sofisticados y más interpretables que los que se pueden implementar con los datos tal y como están en el fichero raíz. La selección de variables permite que el modelo sea manejable, mejorando su interpretabilidad sin que por ello se reduzca significativamente el rendimiento del modelo.\nEl proceso de agrupamiento ya ha sido referido e ilustrado en la Sec. 9.2.2.1. En cuanto al proceso de codificación, se pueden distinguir dos tipos:Codificación de etiquetas: consiste en asignar cada etiqueta un entero o valor único según el orden alfabético. Es la codificación más popular y ampliamente utilizada.Codificación one-hot: consiste en crear una nueva variable ficticia (dummy) binaria por cada categoría existente en la variable codificar. Estas nuevas variables contendrán un \\(1\\) en aquellas observaciones que pertenezcan esa categoría, y un 0 en el resto.70Para ejemplificar este tipo de codificación, continuación, en el conjunto de datos Madrid_Sale_num_sample_bin, se crean dummies, una para cada cada categoría de las variables objeto de codificación. Para ello, se utiliza la función dummyVars() de caret. El resultado puede verse con la función predict().debe olvidarse, igual que para todas las transformaciones descritas, hacer las mismas transformaciones en el conjunto de test.","code":"\ndummies <- dummyVars(\"  ~ .\", data = Madrid_Sale_num_sample_bin)\nhead(predict(dummies, newdata = Madrid_Sale_num_sample_bin))"},{"path":"chap-feature.html","id":"reducción-de-dimensionalidad","chapter":"Capítulo 9 Selección y transformación de variables","heading":"9.4 Reducción de dimensionalidad","text":"La reducción de dimensionalidad es un enfoque alternativo para filtrar las variables informativas sin eliminarlas (como se hacía en la Sec. 9.2, que generalmente se usa para variables numéricas. La diferencia es que las técnicas de reducción de la dimensionalidad crean una proyección de los datos que da como resultado variables predictoras completamente nuevas, que son combinaciones lineales independientes formadas partir de las variables originales, solucionando así, también, los problemas de colinealidad y multicolinealidad (perfecta o cuasi-perfecta). Como se explica en el Cap. 32, el espacio de un conjunto de variables puede reducirse proyectándolo un subespacio de variables de menor dimensión utilizando componentes principales (la técnica de reducción de la dimensionalidad por antonomasia).","code":""},{"path":"chap-feature.html","id":"resumen-8","chapter":"Capítulo 9 Selección y transformación de variables","heading":"Resumen","text":"Se presentan las principales técnicas y métodos de feature selection para llevar cabo la selección (pre-selección y selección propiamente dicha) de las variables predictoras o clasificadoras más relevantes para obtener predicciones o clasificaciones exitosas.Se presentan las principales técnicas y métodos de feature selection para llevar cabo la selección (pre-selección y selección propiamente dicha) de las variables predictoras o clasificadoras más relevantes para obtener predicciones o clasificaciones exitosas.Se describen las principales transformaciones que se realizan en la fase de preprocesamiento de un proyecto de modelado predictivo: las transformaciones de la escala o de la distribución de la variable objetivo, la transformación de variables (feature engineering) y la reducción de la dimensionalidad.Se describen las principales transformaciones que se realizan en la fase de preprocesamiento de un proyecto de modelado predictivo: las transformaciones de la escala o de la distribución de la variable objetivo, la transformación de variables (feature engineering) y la reducción de la dimensionalidad.La creación de variables predictoras partir de los datos en bruto tiene una componente creativa, que requiere de herramientas adecuadas y de experiencia para encontrar las mejores representaciones, apoyándose, en la medida de lo posible en el conocimiento que se tenga de los datos.La creación de variables predictoras partir de los datos en bruto tiene una componente creativa, que requiere de herramientas adecuadas y de experiencia para encontrar las mejores representaciones, apoyándose, en la medida de lo posible en el conocimiento que se tenga de los datos.Las labores de selección y transformación de variables se ilustran con el conjunto de datos de Madrid_Sale, utilizándose los paquetes caret y rsample.Las labores de selección y transformación de variables se ilustran con el conjunto de datos de Madrid_Sale, utilizándose los paquetes caret y rsample.","code":""},{"path":"chap-herramientas.html","id":"chap-herramientas","chapter":"Capítulo 10 Herramientas para el análisis en ciencia de datos","heading":"Capítulo 10 Herramientas para el análisis en ciencia de datos","text":"José-María Montero\\(^{}\\) y Jorge Velasco López\\(^{b}\\)\\(^{}\\)Universidad de Castilla-La Mancha\n\\(^{b}\\)Instituto Nacional de Estadística de España","code":""},{"path":"chap-herramientas.html","id":"introducción-5","chapter":"Capítulo 10 Herramientas para el análisis en ciencia de datos","heading":"10.1 Introducción","text":"En este capítulo se describen una serie de herramientas necesarias para desarrollar proyectos de ciencia de datos. Son herramientas que se utilizan pre- o post- modelado de los datos y que aumentan significativamente el rendimiento de los modelos. Tal caja de herramientas incluye el particionado del conjunto de datos, el manejo de datos equilibrados71, los métodos de remuestreo, el equilibrio entre sesgo y varianza, el ajuste de hiperparámetros y la evaluación de modelos, entre otras.Para ilustrar el manejo de las herramientas anteriormente mencionadas, se utiliza el conjunto de datos Madrid_Sale (disponible en el paquete de R Idealista18), con datos inmobiliarios del año 2018 para el municipio de Madrid. En cuanto al software R, se utiliza caret (Kuhn 2008), para diversas tareas de preparación de datos, y rsample, para muestreo.\n","code":""},{"path":"chap-herramientas.html","id":"partición-del-conjunto-de-datos","chapter":"Capítulo 10 Herramientas para el análisis en ciencia de datos","heading":"10.2 Partición del conjunto de datos ","text":"El objetivo principal del proceso de ciencia de datos es encontrar el modelo o algoritmo que mejor resuelva la pregunta de investigación o, lo que es lo mismo, que proporcione mejores resultados. Por ejemplo, en el caso de los modelos de predicción (y en general de aquellos en los que el aprendizaje es supervisado), muy populares en la ciencia de datos, el que prediga con mayor exactitud los valores futuros de la variable objetivo partir de los predictores seleccionados en el conjunto de datos disponible. En otras palabras, un algoritmo que, sólo ajuste bien los datos pasados sino, lo que es más importante, que proporcione predicciones (futuras) acertadas (y precisas). Para ello, inicialmente, se dividen los datos en dos subconjuntos:de entrenamiento (train): se utiliza para desarrollar conjuntos de funciones, entrenar algoritmos, ajustar hiperparámetros, comparar modelos y realizar todas las demás actividades necesarias para seleccionar un modelo final.de prueba (test): se utiliza para validar la precisión del modelo seleccionado en la fase de entrenamiento.\nla hora de dividir el conjunto de datos en los dos subconjuntos anteriores, hay que tomar dos decisiones:¿Qué porcentaje de los datos (casos, observaciones) se incluye en cada subconjunto?¿Cómo se seleccionan los casos u observaciones que van cada subconjunto?Por lo que se refiere la primera decisión, cuanto más grande sea el subconjunto de entrenamiento mejor será el predictor (o clasificador), aunque las mejoras serán cada vez más\npequeñas. Por el contrario, cuanto más grande sea el subconjunto de prueba o test, más precisa será\nla estimación del error de predicción. En otros términos, lo ideal sería tener un conjunto de datos muy grande y que ambos subconjuntos fueran grandes. De esta manera, los errores de predicción serían pequeños y tendrían poca variabilidad. Sin embargo, con frecuencia, este es el caso en la práctica, y el dilema es elegir un buen predictor (o clasificador) o una buena estimación del error de predicción. En la práctica, lo más frecuente es incluir el 70% de los datos en el subconjunto de entrenamiento y el 30% restante en el de test, aunque los repartos 80%-20% y 60%-40% también son muy populares.En cuanto la segunda decisión, la respuesta es: mediante métodos de muestreo, siendo los más utilizados el muestreo aleatorio simple y el muestreo aleatorio estratificado (véase Cap. 13).","code":""},{"path":"chap-herramientas.html","id":"muestreo-aleatorio-simple","chapter":"Capítulo 10 Herramientas para el análisis en ciencia de datos","heading":"10.2.1 Muestreo aleatorio simple ","text":"La forma más sencilla de asignar los datos los subconjuntos de entrenamiento y prueba, es tomar una muestra aleatoria simple (m..s.) (véase Sec. 13.2) del conjunto de casos u observaciones del tamaño deseado, y asignarlos al subconjunto de entrenamiento, asignándose los restantes al conjunto de test.Un problema que puede surgir con las m..s. es que, cuando el conjunto de datos es pequeño y los valores de uno (o más) de los predictores estén muy desequilibrados (por ejemplo, el predictor es binario y el 95% de sus valores pertenecen una clase o categoría y el 5% restante la otra), hay una probabilidad nada desdeñable de que en alguno de los dos subconjuntos (sobre todo en el de test) dicho predictor esté representado. Si esta circunstancia ocurriese en el conjunto de entrenamiento, algunos algoritmos darían error al aplicarlos al conjunto de test (donde habría datos de un predictor más). Si, por el contrario, ocurriese en el conjunto de test, los problemas surgirían por haber un predictor menos que en el conjunto de entrenamiento. Los problemas se agravarían si la desproporción anterior tuviese lugar en la variable objetivo.continuación, se realiza una división72 70%-30% en el conjunto de datos Madrid_Sale_num, generado en el Cap. 9. Para que se pueda reproducir, se establece al principio una semilla determinada.Como puede comprobarse, de los 94.815 datos que contiene Madrid_Sale_num, 66.373 (el 70%), pasan formar parte del conjunto de entrenamiento y, el resto, 28.442, constituyen el conjunto de test.","code":"\nlibrary (\"caret\")\n\nset.seed(123) # para permitir reproducirlo\nindex <- createDataPartition(Madrid_Sale_num$PRICE, p = 0.7, list = FALSE)\ntrain <- Madrid_Sale_num[index, ]\ntest <- Madrid_Sale_num[-index, ]\ndim(Madrid_Sale_num) # 94815\ndim(train) # 66373\ndim(test) # 28442"},{"path":"chap-herramientas.html","id":"binning","chapter":"Capítulo 10 Herramientas para el análisis en ciencia de datos","heading":"10.2.2 Muestreo estratificado ","text":"Si se desea controlar el muestreo para que los subconjuntos de entrenamiento y prueba tengan distribuciones similares en las clases de la variable objetivo73, se puede usar muestreo estratificado (véase Sec. 14.3).74 Sin embargo, este tipo de muestreo, estratificando por la variable objetivo, garantiza que ocurra lo mismo con los predictores. Es decir, presenta la misma limitación que el muestreo aleatorio simple en caso de que algún (o algunos) predictores estén muy desequilibrados y se quiera garantizar que en ambos subconjuntos las clases de la variable respuesta estén representadas de forma similar. Una posible solución es la eliminación de dichos predictores (que tendrán varianza próxima cero), si bien ello implica pérdida de información.continuación, se estratifica el conjunto de datos Madrid_Sale_num_sample_bin del Cap. 9 por la variable objetivo (price_bin, el precio de venta con binning), que tiene cuatro categorías.Como puede comprobarse debajo, al generar muestras aleatorias estratificadas por la variable objetivo, la distribución de éstas en los subconjuntos de entrenamiento y de prueba es aproximadamente igual:","code":"\nlibrary(\"rsample\")\n\nset.seed(123) # para permitir reproducirlo\ntable(Madrid_Sale_num_sample_bin$price_bin) |> prop.table()\n#         0.4776    0.3062    0.1024    0.1116\nsplit_estrat <- initial_split(Madrid_Sale_num_sample_bin, prop = 0.7, strata = \"price_bin\")\ntrain_estrat <- training(split_estrat)\ntest_estrat <- testing(split_estrat)\ntable(train_estrat$price_bin) |> prop.table()\n#   0.4777015 0.2913093 0.1132075 0.1177816\ntable(test_estrat$price_bin) |> prop.table()\n#   0.4799886 0.3061750 0.1023442 0.1114923"},{"path":"chap-herramientas.html","id":"tecnicas","chapter":"Capítulo 10 Herramientas para el análisis en ciencia de datos","heading":"10.3 Técnicas para manejar datos no equilibrados ","text":"menudo, los datos utilizados en determinadas áreas tienen menos del 1% de eventos raros, pero precisamente su rareza es lo que los hace “interesantes”: por ejemplo, estafas en operaciones bancarias o usuarios que hacen clic en anuncios. En otros términos, una de las clases de la variable objetivo es dominante, pero la clase minoritaria es la que presenta interés. Sin embargo, la mayoría de los algoritmos funcionan bien con variables cuyas clases están desequilibradas (Kuhn, Johnson, et al. 2013). Hay varias técnicas para manejar este problema:\nDownsampling75: equilibra el conjunto de datos reduciendo el tamaño de las clases abundantes para que coincida con el de la clase menos prevalente. Este método es de utilidad cuando el tamaño del conjunto de datos es suficientemente grande para ser aplicado.Upsampling76: equilibra el conjunto de datos aumentando el tamaño de las clases más raras. En lugar de deshacerse de datos de las clases abundantes, se generan nuevos datos para las clases raras mediante repetición o bootstrapping. Este procedimiento es de utilidad cuando hay suficientes datos en la clase (o clases) rara.Creación de datos sintéticos: esta técnica consiste en equilibrar el conjunto de entrenamiento generando nuevos registros sintéticos, esto es, inventados, de la clase minoritaria. Existen diversos algoritmos que realizan esta tarea, siendo uno de los más conocidos la técnica de SMOTE (Synthetic Minority Oversampling Technique) (Chawla et al. 2002).Otras técnicas: como que el algoritmo implemente mecanismos para dar mayor peso los casos de la clase minoritaria, etc.modo de ejemplo, continuación se utiliza downsampling en el conjunto de datos Madrid_Sale_num_sample_bin, para mejorar la precisión del modelo, mediante el algoritmo gradient-boosting:existe una ventaja absoluta de un método sobre otro. La aplicación de estos métodos depende del caso de uso al que se aplique y del conjunto de datos. La función de caret para implementar estas técnicas está en ?caret::trainControl()).Una alternativa los métodos anteriores es la implementación de algoritmos que proporcionen un buen rendimiento con variables cuyas clases están desequilibradas, de tal manera que se refuerce el aprendizaje en la clase minoritaria. La idea detrás de esta segunda opción es incluir una penalización o un sesgo que pondere las clases de tal manera que se le dé más importancia la predicción, clasificación, etc. correcta en la clase minoritaria (para más detalles, véase (Garcı́Abad et al. 2021)).","code":"\n# Se especifica que el modelo se entrene con downsampling\nctrl <- trainControl(\n  method = \"repeatedcv\", repeats = 5,\n  classProbs = TRUE,\n  sampling = \"down\"\n)\nMadrid_Sale_num_sample_bin_downsample <- train(price_bin ~ .,\n  data = Madrid_Sale_num_sample_bin,\n  method = \"gbm\",\n  preProcess = c(\"range\"),\n  verbose = FALSE,\n  trControl = ctrl\n)"},{"path":"chap-herramientas.html","id":"enfoque-validacion","chapter":"Capítulo 10 Herramientas para el análisis en ciencia de datos","heading":"10.4 El enfoque de validación","text":" Anteriormente, se indicaba que los datos deben dividirse en dos subconjuntos, uno de entrenamiento y otro de prueba, y que debía usarse el subconjunto de prueba para evaluar la exactitud del modelo durante la fase de entrenamiento. Si la exactitud de las predicciones (por ejemplo, porcentaje de casos bien clasificados en un problema de clasificación) en el conjunto de test es (además de elevada) similar la que se obtiene en el conjunto de entrenamiento, entonces el modelo entrenado generalizará bien para otros conjuntos de datos y puede darse por bueno. En otro caso, el modelo ha entrenado bien. Por ejemplo, si la exactitud de las predicciones en el conjunto de entrenamiento es del 80% y en el de test es del 25% o del 97%, entonces el modelo puede darse por válido. En el caso del 97%, la diferencia de porcentajes está indicando un aprendizaje “excesivo” del conjunto de datos de entrenamiento, de tal manera que el modelo en cuestión puede proporcionar muy buenas predicciones para el conjunto de datos utilizado, pero para nuevos datos, o conjuntos de datos (esta circunstancia se conoce como sobreajuste u overfitting.77En el caso en el que la exactitud de las predicciones sea muy distinta en los subconjuntos de entrenamiento y test, hay varias opciones (excluyentes) para salvar dicha circunstancia: mejorar el modelo, ajustar sus hiperparámetros, incluir más casos en el conjunto de datos, modificar el preprocesado de los datos, ver si hay desequilibrio entre las clases, analizar si las variables predictoras son o las adecuadas, revisar el proceso de limpieza de datos… y, posteriormente, entrenar de nuevo el modelo y determinar si es o válido. Otra opción más drástica es, simplemente, cambiar de modelo.Una mejor opción sería utilizar desde el principio un enfoque de validación, que implica dividir el subconjunto de entrenamiento en dos partes: un subconjunto de entrenamiento propiamente dicho y un conjunto de validación. Así, se puede entrenar el modelo en el nuevo subconjunto de entrenamiento y estimar su exactitud en el conjunto de validación. Es importante tener claro que el subconjunto de validación es un subconjunto que se deje aparte, como el de test, durante la fase de entrenamiento, sino que se utiliza en dicha fase.\nEn resumen, con el enfoque de validación, para dar por válido un modelo, se procede como sigue:Dividir el conjunto de datos en subconjunto de entrenamiento y subconjunto de test.Dividir el subconjunto de entrenamiento en un subconjunto de entrenamiento propiamente dicho y un subconjunto de validación.Entrenar el modelo con los datos del subconjunto de entrenamiento propiamente dicho.Comprobar que la exactitud de las predicciones en dicho subconjunto de entrenamiento y en el de validación es similar (y aceptable para la exigencia que se requiere).Realizar predicciones con el conjunto de test y comprobar que se obtiene un porcentaje de buenas predicciones aceptable para los requisitos exigidos.Agregar el conjunto de test al de entrenamiento (global) y entrenar de nuevo el modelo (que será el definitivo); de esta manera se aprovecha el 100% de los datos. Este último entrenamiento debería mejorar el modelo final, aunque la única manera de comprobarlo es mediante su comportamiento en el entorno real.La limitación del enfoque de validación con un solo subconjunto de reserva (de validación) es que dicha validación puede ser muy variable y poco confiable, menos que se esté trabajando con conjuntos de datos muy grandes (Molinaro, Simon, Pfeiffer 2005). Y aquí es donde entran en juego los procedimientos de validación que utilizan remuestreo. El procedimiento de validación con remuestreo más utilizado es la validación cruzada (VC) k-grupos (k-fold cross validation). También es muy popular el que utiliza remustreo por bootstrapping, que se abordará tras el VC k-grupos.\nPara llevar cabo una VC k-grupos, se divide aleatoriamente el subconjunto de datos de entrenamiento en \\(k\\) grupos (folds) de aproximadamente el mismo tamaño. El modelo se ajusta en los \\(k-1\\) primeros grupos y el último se usa como conjunto de validación, para “validar” la bondad del modelo. continuación, se separa el penúltimo grupo y se ajusta el modelo con los restantes, usándose el penúltimo grupo como subconjunto de validación para validar la bondad del modelo. Después se separa el antepenúltimo grupo, y así sucesivamente, hasta separar el primero. Como resultado, se obtienen \\(k\\) conjuntos de errores, cuyo promedio (véase Sec.@ref(evaluación)) podría servir como estimación de la exactitud y precisión (o error78) esperada en un conjunto de datos nuevo.El procedimiento descrito puede repetirse varias veces (VC con repetición), mediante nuevas particiones aleatorias del conjunto de entrenamiento y procediendo igual que en la iteración anterior.En la práctica, normalmente se usa \\(k=5\\) o \\(k=10\\) (las Fig. 10.1 y 10.2 ilustran el caso de CV 5-grupos). existe una regla formal en cuanto al tamaño de \\(k\\), pero medida que \\(k\\) aumenta, la diferencia entre el rendimiento estimado y real precisión estimada y el real que se obtendrá en el conjunto de test, disminuirá. En el lado negativo de la balanza, un \\(k\\) demasiado grande puede aumentar notablemente la carga computacional y, además, generar mejoras significativas. este respecto, en Molinaro, Simon, Pfeiffer (2005) se concluye que CV con \\(k=10\\) funciona de manera similar CV con \\(k=n\\), la CV más extrema, también conocida como VC “dejando uno fuera” (leave one cross validation, LOOCV).\nFigura 10.1: CV 5-grupos ()\ny si la exactitud de las predicciones en el subconjunto de entrenamiento propiamente dicho y en el de validación es similar (y aceptable para la exigencia que se requiere):\nFigura 10.2: CV 5-grupos (ii)\nAunque \\(k\\geq 10\\) contribuye minimizar la variabilidad del error de predicción (es decir, tiende aumentar la precisión de las predicciones) , en general la CV k-grupos suele proporcionar mayores variabilidades que el bootstrapping (que se analiza continuación); ocurre lo mismo con el sesgo (Brad Boehmke Greenwell 2019). Kim (2009) demostró que repetir el CV k-grupos puede ayudar reducir la estimación del error de generalización.Una implementación del CV k-grupos, con tres repeticiones, utilizando el conjunto de datos Madrid_Sale_num_sample_bin, previa mejora de la precisión del modelo mediante downsampling, se llevó cabo en la Sec. 10.3 con la siguiente orden:Bootstrapping es un procedimiento de muestreo aleatorio con reemplazamiento (Efron Tibshirani 1986). Esto significa que, después de seleccionar un dato para incluirlo en el subconjunto que sea, sigue disponible para una selección posterior. Una muestra bootstrap tiene el mismo tamaño que el conjunto de datos original partir del cual se obtiene. Las observaciones originales seleccionadas (una o varias veces) en la muestra conforman el subconjunto de de entrenamiento, mientras que aquellas que aparecen en ella (se les denomina --bag) conforman el subconjunto de test.La Fig. 10.3, tomada de Brad Boehmke Greenwell (2019), muestra un esquema de muestreo bootstrap, donde cada muestra contiene 12 observaciones, al igual que en el conjunto de datos original. Como puede observarse, el muestreo bootstrap lleva aproximadamente la misma distribución de valores (representados por colores) que el conjunto de datos original.\nFigura 10.3: Remuestreo bootstrap\nEl hecho de que bootstrapping replique el conjunto de observaciones implica, como se dijo anteriormente, que la variablidad del error es menor que en CV k-grupos. Sin embargo, dicha replicación puede aumentar el sesgo de la estimación dicho error. Esto puede ser un problema con conjuntos de datos muy pequeños, pero para la mayoría de los conjuntos de datos, que suelen ser de tamaño medio o grande (por ejemplo, \\(n \\geq 1000\\)).Las muestras bootstrap pueden crearse fácilmente con rsample::bootstraps(), como se ilustra continuación.Si se usa la función trainControl(), se debe especificar: method = \"boot\".","code":"\ncontrol <- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\nbootstraps(Madrid_Sale_num_sample_bin, times = 10)"},{"path":"chap-herramientas.html","id":"compensacion","chapter":"Capítulo 10 Herramientas para el análisis en ciencia de datos","heading":"10.5 Compensación (trade off) entre sesgo y varianza","text":"\nEn el entorno predictivo, el objetivo es que el error de predicción, en términos generales, o por término medio, sea lo más pequeño posible. Sin embargo, se pueden promediar los errores porque se compensarían los positivos con los negativos. Por ello, se promedian elevados al cuadrado (considerando sólo su magnitud), denominandose dicho promedio “error cuadrático medio”, ECM (en este caso, de predicción): \\(E(y_j- \\hat y_j)^2\\) en términos probabilísticos, o \\(\\sum _{j=1}^ {N}(y_j- \\hat y_j)^2 \\frac {n_j}{N}\\) en términos descriptivos. Pues bien, el ECM se puede descomponer como suma de dos componentes:Uno debido la diferencia entre el valor correcto de la variable objetivo o respuesta y el que se espera que proporcione el modelo. Dicha diferencia se denomina sesgo (en ingés, bias), y aparece elevado al cuadrado en dicha descomposición.Otro debido que, dado un conjunto de valores de las variables predictoras, la respuesta del modelo es siempre la misma. Esta variabilidad aparece en la descomposición en forma de varianza, y por ello se denomina varianza del error de predicción o, simplemente, varianza de predicción.Lógicamente, el incremento/reducción de uno de los componentes implica la reducción/incremento del otro.Un sesgo muy elevado es un indicador de que el modelo es muy simple y ha ajustado bien los datos de entrenamiento (underfitting), lo cual se traduce en errores de predicción elevados. Una varianza de predicción elevada (es decir, pequeños cambios en los datos de entrada producen salidas muy distintas), es un signo de complejidad en el modelo y sobreajuste (overfitting) de los datos de entrenamiento.\nLos algoritmos con tendencia un elevado porcentaje del ECM debido al sesgo, tienen, lógicamente, un porcentaje del ECM debido la varianza de predicción pequeño; es decir, tienen los problemas que se derivan del infra-ajuste de los datos: malas predicciones (sesgadas) y, encima, muy precisas. Y al contrario, aquellos que tienen un porcentaje del ECM debido al sesgo pequeño, tienen un porcentaje del ECM por varianza de predicción elevado.Los modelos lineales (regresión lineal, análisis discriminante lineal, regresión logística…) suelen tener errores por sesgo elevados.79 Modelos como los árboles de decisión, el k-vecinos más cercanos y los support vector machine tienen errores por sesgo pequeños (y, por tanto, varianza de predicción grande, siempre en relación al ECM total), son muy adaptables y ofrecen una flexibilidad extrema en cuanto los patrones los que pueden ajustarse. Sin embargo, plantean sus propios problemas, especialmente el de sobreajuste de los datos de entrenamiento, cuya consecuencia es que el modelo se generalizará bien con datos nuevos.80Lógicamente el modelo predictivo o clasificador deseado es el que tenga el menor ECM posible.81 Si este fuera el caso, habría que encontrar la combinación sesgo cuadrático-varianza de predicción que minimizase el ECM. La Fig. 10.4, se ha generado partir de datos sintéticos de sesgo (al cuadrado) y varianza de predicción. En ella se puede apreciar que el valor mínimo del ECM, que es la suma de los valores del sesgo cuadrático y varianza de predicción determinados por la linea vertical amarilla.\nFigura 10.4: Trade-entre sesgo y varianza\n","code":""},{"path":"chap-herramientas.html","id":"ajuste-de-hiperparámetros","chapter":"Capítulo 10 Herramientas para el análisis en ciencia de datos","heading":"10.6 Ajuste de hiperparámetros","text":"Los denominados “hiperparametros” de un modelo son los valores de las configuraciones utilizadas durante el proceso de entrenamiento. diferencia de los parámetros, son valores que se obtienen partir de los datos, sino que los propone el científico de datos. Podría decirse que son conjeturas (buenas conjeturas) realizadas sin utilizar las observaciones disponibles.Los hiperparámetros, diferencia de los parámetros, se fijan antes del entrenamiento. Siendo más específicos, al entrenar un modelo de aprendizaje automático se fijan los valores de los hiperparámetros para que con estos se estimen los parámetros. Podría decirse que son los ajustes del modelo para que éste pueda resolver de manera óptima el problema de aprendizaje automático.Algunos ejemplos de hiperparámetros utilizados para entrenar los modelos son la ratio de aprendizaje en el algoritmo del descenso del gradiente, el número de vecinos en el algoritmo de k-vecinos más cercanos, la profundidad máxima en un árbol de decisión, el número de árboles en un bosque aleatorio (random forest)… Como puede apreciarse, sirven para controlar la complejidad de los algoritmos de aprendizaje automático y, por tanto, la compensación entre sesgo y varianza.En conclusión, hiperparámetros y parámetros son conceptos bien diferentes.la luz de la definición de hiperparámetro, lo natural sería que el científico de datos los fijase de acuerdo con su experiencia en el pasado en problemas similares, asignándoles los mismos, o parecidos, valores. Sin embargo, existen métodos más sofisticados para resolver el problema de la “optimización de hiperparámetros”, es decir, de la obtención del conjunto óptimo de valores de los mismos que proporciona la configuración que, tras el entrenamiento, dará lugar los mejores resultados. Entre estos procedimientos cabe destacar los siguientes por ser los que incorporan los algoritmos más populares:Optimización bayesiana: utiliza la moda para elegir qué hiperparámetros considerar, en función del rendimiento de las elecciones anteriores.Optimización bayesiana: utiliza la moda para elegir qué hiperparámetros considerar, en función del rendimiento de las elecciones anteriores.Búsqueda en cuadrícula (grid search): prueba con todas las combinaciones posibles de la cuadrícula.Búsqueda en cuadrícula (grid search): prueba con todas las combinaciones posibles de la cuadrícula.Búsqueda aleatoria: muestrea y evalúa aleatoriamente conjuntos de una distribución de probabilidad específica.Búsqueda aleatoria: muestrea y evalúa aleatoriamente conjuntos de una distribución de probabilidad específica.Optimización secuencial basada en modelos: son una formalización de la optimización bayesiana.Optimización secuencial basada en modelos: son una formalización de la optimización bayesiana.modo de ejemplo, los dos hiperparámetros que más influencia tienen en un modelo de random forest (véase Cap. (ref:ap-bagg-rf?)), son mtry (número de variables muestreadas aleatoriamente como candidatas en cada split) y ntree (número de\nárboles). Pues bien, continuación, como viene siendo habitual, se utiliza el conjunto de datos Madrid_Sale_num_sample_bin para buscar el valor óptimo de mtry mediante la técnica de búsqueda en cuadrícula (se usa la métrica Accuracy (exactitud), que hace referencia la proporción de predicciones correctas, véase Sec. @ref(evaluación).NotaLa implementación del algoritmo de random forest del paquete randomForest proporciona la función tuneRF(), que busca valores mtry óptimos dados los datos disponibles.Se entrena el modelo y se observa que la mayor exactitud, 68,76%, se obtiene con un mtree de 2.449489.","code":"\ncontrol <- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\nseed <- 7\nmetrica <- \"Accuracy\"\nset.seed(seed)\nmtry <- sqrt(ncol(Madrid_Sale_num_sample_bin))\ntunegrid <- expand.grid(.mtry = mtry)\nrf_default <- train(price_bin ~ ., data = Madrid_Sale_num_sample_bin, method = \"rf\", metric = metrica, tuneGrid = tunegrid, trControl = control)\nprint(rf_default)\n# Accuracy   \n# 0.6876006  "},{"path":"chap-herramientas.html","id":"evaluación","chapter":"Capítulo 10 Herramientas para el análisis en ciencia de datos","heading":"10.7 Evaluación de modelos","text":"La última fase del proceso de modelización contesta la pregunta: ¿Cómo de bueno es el modelo entrenado? ¿Cómo de bien generaliza los (buenos) resultados obtenidos en la fase de entrenamiento nuevos conjuntos de datos (datos --sample)? Por ello, en este epígrafe se presentan las métricas más populares de rendimiento de modelos en los entornos de regresión (predicción) y clasificación.En el entorno de regresión, es prácticamente imposible predecir valores exactos, y hay que conformarse con muy buenas aproximaciones dichos valores exactos, por lo que las métricas que se utilizan para medir la bondad del modelo entrenado suelen estar basadas en la diferencia entre los valores reales y los que predice el modelo, es decir, en los errores de predicción. La medida natural sería la media de los errores de predicción, pero, para evitar la compensación de los errores positivos con los negativos, y puesto que el interés se centra en la magnitud de los errores y en su signo, las métricas de evaluación más populares en el entorno de regresión son:\nError cuadrático medio (ECM): es la más utilizada en tareas de regresión. Como se avanzó en 10.5, se define como la media de las diferencias cuadráticas de entre los valores objetivo (\\(y_j\\)) y los predichos por el modelo (\\(\\hat{y}_i\\)), evitando así la compensación de errores positivos y negativos. Su expresión es, por tanto, \\(ECM=\\frac{1}{n}\\sum_{=1}^{n}(y_i-\\hat{y}_i)^2\\). Al considerar los errores de predicción al cuadrado, exacerba los errores grandes, por lo que hay que utilizar esta métrica con cuidado cuando se tengan valores anómalos en el conjunto de datos y hayan sido tratados en las fases previas la modelización y validación. En ese caso, incluso un modelo cuasiperfecto podría tener un ECM elevado. Si todos los errores son inferiores la unidad hay un riesgo elevado de subestimar lo malo que es el modelo (en caso de que lo fuese).Error cuadrático medio (ECM): es la más utilizada en tareas de regresión. Como se avanzó en 10.5, se define como la media de las diferencias cuadráticas de entre los valores objetivo (\\(y_j\\)) y los predichos por el modelo (\\(\\hat{y}_i\\)), evitando así la compensación de errores positivos y negativos. Su expresión es, por tanto, \\(ECM=\\frac{1}{n}\\sum_{=1}^{n}(y_i-\\hat{y}_i)^2\\). Al considerar los errores de predicción al cuadrado, exacerba los errores grandes, por lo que hay que utilizar esta métrica con cuidado cuando se tengan valores anómalos en el conjunto de datos y hayan sido tratados en las fases previas la modelización y validación. En ese caso, incluso un modelo cuasiperfecto podría tener un ECM elevado. Si todos los errores son inferiores la unidad hay un riesgo elevado de subestimar lo malo que es el modelo (en caso de que lo fuese).Error absoluto medio (EAM): es una alternativa al ECM que se define como la media del valor absoluto de los errores de predicción (para evitar compensaciones): \\(EAM=\\frac{1}{n}\\sum_{=1}^{n}|y_i-\\hat{y}_i|\\). Al elevar al cuadrado, magnifica los errores grandes, por lo que es menos sensible que el ECM valores anómalos, si bien tampoco es recomendable en caso de que éstos hayan sido tratados previamente. Una ventaja que tiene es que su unidad de medida es la misma que la de la variable objetivo.Error absoluto medio (EAM): es una alternativa al ECM que se define como la media del valor absoluto de los errores de predicción (para evitar compensaciones): \\(EAM=\\frac{1}{n}\\sum_{=1}^{n}|y_i-\\hat{y}_i|\\). Al elevar al cuadrado, magnifica los errores grandes, por lo que es menos sensible que el ECM valores anómalos, si bien tampoco es recomendable en caso de que éstos hayan sido tratados previamente. Una ventaja que tiene es que su unidad de medida es la misma que la de la variable objetivo.Raíz cuadrada del error cuadrático medio (RECM): “deshace”, en un sentido matemático, sino aproximadamente, la elevación al cuadrado de los errores en el ECM y, por consiguiente, viene dada en las mismas unidades que la salida del modelo, lo que la hace más interpretable. Su expresión es: \\(RECM=+\\sqrt{\\frac{1}{n}\\sum_{=1}^{n}(y_i-\\hat{y}_i)^2}\\).Raíz cuadrada del error cuadrático medio (RECM): “deshace”, en un sentido matemático, sino aproximadamente, la elevación al cuadrado de los errores en el ECM y, por consiguiente, viene dada en las mismas unidades que la salida del modelo, lo que la hace más interpretable. Su expresión es: \\(RECM=+\\sqrt{\\frac{1}{n}\\sum_{=1}^{n}(y_i-\\hat{y}_i)^2}\\).Coeficiente de determinación (\\(R^2\\)): se define como \\(R^2=1-\\frac{\\sum_{=1}^{n}(y_j-\\hat y_j)^2}{\\sum_{=1}^{n}(y_j-\\bar y)^2}\\) (véanse detalles adicionales en el Cap. 5 de José-María Montero (2007)) y su campo de variación es [\\(-\\infty\\),1]. En la práctica totalidad de las situaciones reales toma valores entre 0 y 1, puesto que sólo toma valores negativos cuando el modelo entrenado sea muy deficiente y prediga peor que cuando se establece como predicción la media de las salidas observadas, sean cuales sean los valores de los predictores. Obviando estas situaciones, y aquéllas en las que la varianza de la variable salida se pueda descomponer en varianza debida al modelo y varianza debida al error (por ejemplo, en el caso de una regresión potencial)82, \\(R^2\\) se puede interpretar como la reducción proporcional en el ECM que tiene lugar al predecir las salidas del modelo mediante los predictores (cualquiera que sea la función que los ligue con la salida) en vez de mediante la media de la variable output (que es la predicción óptima en ausencia de predictores). Mide, por tanto, lo bueno que es disponer de predictores para predecir los valores de la variable output o de salida; o, en otros términos, el porcentaje de varianza de la variable salida que explican los predictores través del modelo que liga ambos. Cuanto más cercano esté la unidad mejor es el modelo efectos predictivos.Coeficiente de determinación (\\(R^2\\)): se define como \\(R^2=1-\\frac{\\sum_{=1}^{n}(y_j-\\hat y_j)^2}{\\sum_{=1}^{n}(y_j-\\bar y)^2}\\) (véanse detalles adicionales en el Cap. 5 de José-María Montero (2007)) y su campo de variación es [\\(-\\infty\\),1]. En la práctica totalidad de las situaciones reales toma valores entre 0 y 1, puesto que sólo toma valores negativos cuando el modelo entrenado sea muy deficiente y prediga peor que cuando se establece como predicción la media de las salidas observadas, sean cuales sean los valores de los predictores. Obviando estas situaciones, y aquéllas en las que la varianza de la variable salida se pueda descomponer en varianza debida al modelo y varianza debida al error (por ejemplo, en el caso de una regresión potencial)82, \\(R^2\\) se puede interpretar como la reducción proporcional en el ECM que tiene lugar al predecir las salidas del modelo mediante los predictores (cualquiera que sea la función que los ligue con la salida) en vez de mediante la media de la variable output (que es la predicción óptima en ausencia de predictores). Mide, por tanto, lo bueno que es disponer de predictores para predecir los valores de la variable output o de salida; o, en otros términos, el porcentaje de varianza de la variable salida que explican los predictores través del modelo que liga ambos. Cuanto más cercano esté la unidad mejor es el modelo efectos predictivos.Coeficiente de determinación ajustado (\\(R_{ajd}^2)\\): una limitación importante del \\(R^2\\) es que su valor puede aumentarse artificialmente mediante la inclusión de más y más variables predictoras, pues la inclusión de las mismas o mantiene o mejora dicha métrica. Esta circunstancia puede dar lugar confusión, pues el hecho de que un modelo utilice más variables predictoras que otro, quiere decir que sea mejor. El \\(R_{ajd}^2\\) corrige dicha circunstancia penalizando la complejidad del modelo, entendiéndose que un modelo es más complejo que otro si utiliza un mayor número de variables predictoras que ese otro. Su expresión viene dada por \\(R_{ajd}^2=1-\\left(\\frac{n-1}{n-p-1}\\right)\\left(1-R^2\\right)\\), y su valor nunca supera el del \\(R^2\\).Coeficiente de determinación ajustado (\\(R_{ajd}^2)\\): una limitación importante del \\(R^2\\) es que su valor puede aumentarse artificialmente mediante la inclusión de más y más variables predictoras, pues la inclusión de las mismas o mantiene o mejora dicha métrica. Esta circunstancia puede dar lugar confusión, pues el hecho de que un modelo utilice más variables predictoras que otro, quiere decir que sea mejor. El \\(R_{ajd}^2\\) corrige dicha circunstancia penalizando la complejidad del modelo, entendiéndose que un modelo es más complejo que otro si utiliza un mayor número de variables predictoras que ese otro. Su expresión viene dada por \\(R_{ajd}^2=1-\\left(\\frac{n-1}{n-p-1}\\right)\\left(1-R^2\\right)\\), y su valor nunca supera el del \\(R^2\\).Deviance: es una métrica relacionada con la estimación de modelos (especialmente modelos lineales generalizados) por el método de la máxima verosimilitud. Compara, por cociente, la verosimilitud del modelo estimado con la del modelo saturado (aquél que tiene tantos parámetros como observaciones83 y que, por tanto, tiene la máxima verosimilitud alcanzable). Mide el grado en el que un modelo explica la variabilidad en un conjunto de datos cuando se utiliza la estimación de máxima verosimilitud. En términos de log-verosimilitud (\\(l\\)) se define como \\(D=2(l_{Modelo \\hspace{0,1cm} saturado}-l_{Modelo \\hspace{0,1cm} propuesto})\\), y, lógicamente, cuanto menor es la deviance mejor es el modelo.Deviance: es una métrica relacionada con la estimación de modelos (especialmente modelos lineales generalizados) por el método de la máxima verosimilitud. Compara, por cociente, la verosimilitud del modelo estimado con la del modelo saturado (aquél que tiene tantos parámetros como observaciones83 y que, por tanto, tiene la máxima verosimilitud alcanzable). Mide el grado en el que un modelo explica la variabilidad en un conjunto de datos cuando se utiliza la estimación de máxima verosimilitud. En términos de log-verosimilitud (\\(l\\)) se define como \\(D=2(l_{Modelo \\hspace{0,1cm} saturado}-l_{Modelo \\hspace{0,1cm} propuesto})\\), y, lógicamente, cuanto menor es la deviance mejor es el modelo.Raíz del error logarítmico cuadrático medio (RELCM): similar RMSE, pero tomando logaritmos en los valores reales y predichos. De especial interés cuando lo que importa es la magnitud relativa (porcentual) de los errores. se puede utilizar cuando la variable objetivo toma valores negativos. Para salvar la problemática de que la variable objetivo tome el valor cero, generalmente se agrega una constante los valores reales y predichos de la variable salida antes de aplicar la operación logarítmica. Dependiendo del problema, se puede elegir otro tipo de constante. Su expresión viene dada por: \\(RMSLE=\\sqrt{\\frac{1}{n}(log(y_i+1)-log(\\hat{y}_i+1))^2)}\\).En este manual se usan estas medidas en repetidas ocasiones. Por ejemplo, en el Cap. 19 se ajusta la regresión ridge en el subconjunto de entrenamiento y se evalúa su ECM en el subconjunto de test.En el entorno clasificatorio, las salidas del modelo pueden ser de clase (tal es el caso de los algoritmos de máquinas de vectores soporte y k-vecinos más cercanos, por ejemplo) o de probabilidad (caso de la regresión logística, los bosques aleatorios, el adaboost…). Dado que pasar de salidas probabilísticas salidas de clase consiste únicamente en fijar umbrales de probabilidad, y que algunos algoritmos ya proporcionan el paso de salidas de clase salidas probabilísticas, en lo que sigue se hará distinción entre ellas.En dicho entorno clasificatorio, es muy frecuente el uso de la matriz de confusión, que compara las clases (niveles categóricos) reales con las predichas en el subconjunto de test (cuyos resultados se conocen). La Fig. 10.5 muestra un ejemplo de clasificación multiclase (en concreto, 3 clases) basado en el famoso conjunto de datos “Flor iris” de Fisher, que considera tres especies (iris setosa, iris virginica e iris versicolor). La predicción de la clase la que pertenece una flor se hace en función del largo y el ancho del sépalo y del pétalo.En la diagonal ascendente figura el número de flores, de cada color, cuya clase ha sido correctamente predicha. Los elementos fuera de dicha diagonal indican las flores, de cada clase, que el clasificador utilizado ha clasificado erróneamente. Como puede apreciarse, 47 de las 50 flores iris que se consideran fueron bien clasificadas. Sin embargo, dicho clasificador clasificó una flor versicolor como virgínica, y dos virgínicas como versicolores.\nFigura 10.5: Matriz de confusión con tres clases\nAunque el concepto de matriz de confusión es muy sencillo, la terminología que lo rodea lo es tanto; incluso podría decirse que es confusa. La predicción proporcionada por el modelo puede ser (véase Fig. 10.6):Un verdadero positivo (VP): predicción de verdadero y verdadero en realidad.Un verdadero negativo (VN): predicción de falso y falso en realidad.Un falso positivo (FP): predicción de verdadero y falso en la realidad.Un falso negativo (FN): predicción de falso y verdadero en la realidad.\n\nFigura 10.6: Terminología de una matriz de confusión\nComo se avanzó anteriormente, esta terminología es confusa y, por ello, se ilustra continuación con un ejemplo. Supóngase que se está interesado en conocer si un determinado tratamiento médico tiene efectos positivos sobre una enfermedad. Echando mano de la teoría de la contrastación de hipótesis (véase Sec. 13.5), supóngase que se toma como hipótesis nula (\\(H_0\\)): SÍ y como hipótesis alternativa (\\(H_1\\)): . Pues bien: Si es cierto que el tratamiento en cuestión tiene un efecto positivo en la enfermedad y el modelo rechaza la hipótesis nula, se tiene un VP.Si la hipótesis nula es falsa, es decir, si el tratamiento tiene un efecto positivo sobre la enfermedad, y el modelo rechaza la hipótesis nula, entonces se tiene un VN.Si la hipótesis nula es falsa y el modelo concluye que se rechaza la hipótesis nula de que el tratamiento cura la enfermedad, entonces se tiene un FP.Si es cierto que el tratamiento tiene un efecto positivo en la enfermedad y el modelo rechaza la hipótesis nula, se tiene un FN.Las siguientes medidas se pueden calcular partir de una matriz de confusión (es decir, partir del número de VPs, VNs, FPs y FNs) para un clasificador binario:Exactitud. Es la proporción de predicciones correctas: \\(Exactitud=\\frac{VP+VN}{Total}\\). Responde la pregunta: ¿Con qué frecuencia funciona correctamente el clasificador? Lógicamente, la tasa de clasificación errónea se obtiene como \\(\\frac{FP+FN}{Total}\\). En caso de desequilibrio notable de clase, por ejemplo, la clase contiene 999 casos y la B tan sólo 1, siendo B la clase rara, la clase positiva, la precisión sería una métrica fiable para evaluar el rendimiento clasificatorio del modelo. Por ejemplo, el clasificador podría consistir en una regla que dijese que todos los casos pertenecen la clase , ¡y acertaría en el 99,9% de los casos! En estos casos, sería mejor utilizar como métricas la precisión y la sensibilidad.Exactitud. Es la proporción de predicciones correctas: \\(Exactitud=\\frac{VP+VN}{Total}\\). Responde la pregunta: ¿Con qué frecuencia funciona correctamente el clasificador? Lógicamente, la tasa de clasificación errónea se obtiene como \\(\\frac{FP+FN}{Total}\\). En caso de desequilibrio notable de clase, por ejemplo, la clase contiene 999 casos y la B tan sólo 1, siendo B la clase rara, la clase positiva, la precisión sería una métrica fiable para evaluar el rendimiento clasificatorio del modelo. Por ejemplo, el clasificador podría consistir en una regla que dijese que todos los casos pertenecen la clase , ¡y acertaría en el 99,9% de los casos! En estos casos, sería mejor utilizar como métricas la precisión y la sensibilidad.Precisión. Da respuesta la pregunta: cuando el clasificador predice “SÍ”, ¿con qué frecuencia predice correctamente? Su expresión viene dada por: \\(Precisión=\\frac{VP}{VP+FP}\\).Precisión. Da respuesta la pregunta: cuando el clasificador predice “SÍ”, ¿con qué frecuencia predice correctamente? Su expresión viene dada por: \\(Precisión=\\frac{VP}{VP+FP}\\).Sensibilidad84. Restringe el denominador de la precisión los SÍ reales. Responde la pregunta: ¿cuándo en realidad es un SÍ, cuál es el porcentaje de aciertos del clasificador? Su expresión es \\(Sensibilidad=\\frac{VP}{VP+FN}\\). Por consiguiente, la sensibilidad es una medida de la probabilidad de que un caso real positivo se clasifique como positivo.Sensibilidad84. Restringe el denominador de la precisión los SÍ reales. Responde la pregunta: ¿cuándo en realidad es un SÍ, cuál es el porcentaje de aciertos del clasificador? Su expresión es \\(Sensibilidad=\\frac{VP}{VP+FN}\\). Por consiguiente, la sensibilidad es una medida de la probabilidad de que un caso real positivo se clasifique como positivo.Especificidad. Se define como \\(Especificidad=\\frac{VN}{FP+VN}\\). Es, por tanto, el porcentaje de verdaderos negativos respecto de todo lo que debería haber sido clasificado como negativo. Su complementario, la 1-especificidad (\\(\\frac{FP}{FP+VN}\\)) es, básicamente, una medida de la frecuencia (relativa) con la que se producirá una falsa alarma, o la frecuencia con la que un caso real negativo se clasifique como positivo.Especificidad. Se define como \\(Especificidad=\\frac{VN}{FP+VN}\\). Es, por tanto, el porcentaje de verdaderos negativos respecto de todo lo que debería haber sido clasificado como negativo. Su complementario, la 1-especificidad (\\(\\frac{FP}{FP+VN}\\)) es, básicamente, una medida de la frecuencia (relativa) con la que se producirá una falsa alarma, o la frecuencia con la que un caso real negativo se clasifique como positivo.Puntuación F1. Es la media armónica de la precisión y la sensibilidad. Su campo de variación es [0, 1], donde 0 indica falta total de precisión y sensibilidad y 1 significa precisión y sensibilidad perfectas. La puntuación F1 penaliza los valores extremos y se suele utilizar en caso de conjuntos de datos desequilibrados.Puntuación F1. Es la media armónica de la precisión y la sensibilidad. Su campo de variación es [0, 1], donde 0 indica falta total de precisión y sensibilidad y 1 significa precisión y sensibilidad perfectas. La puntuación F1 penaliza los valores extremos y se suele utilizar en caso de conjuntos de datos desequilibrados.Área bajo la curva de características operativas del receptor (área bajo la curva ROC). Al graficar la sensibilidad (tasa de verdaderos positivos) frente la tasa de falsos positivos (también denominada 1-especificidad), se obtiene la curva ROC. La diagonal ascendente representa la aleatoriedad. Cuanto más grande sea el área bajo la curva ROC, mejor será la precisión obtenida. Es una medida recomendable en el caso de clases desequilibradas. Un ejemplo de área bajo la curva ROC puede verse en el Cap. 16, y se reproduce en la parte derecha de la Fig. 10.7.\nFigura 10.7: Ejemplo de curva ROC partir de la estimación de un modelo lineal generalizado (parte derecha)\nÍndice de Gini. Bien conocido en la literatura estadística sobre concentración, se trata de un indicador útil en el caso de clases desequilibradas. Su campo de variación es [0,1], donde 0 representa la igualdad perfecta y 1 la concentración en una única clase. Puede calcularse partir del área bajo la curva ROC de la siguiente manera: \\(IG=2\\hspace{0,1cm} \\acute{}rea\\hspace{0,1cm} bajo \\hspace{0,1cm} la\\hspace{0,1cm}curva\\hspace{0,1cm} ROC-1\\). En caso de IG=0, el área bajo la curva ROC es 1/2 y la curva ROC coincide con la diagonal ascendente. En caso de IG=1, el área bajo la curva ROC será 0 y dicha curva vale cero para todos los valores del eje de abscisas, excepto para el último, para el cual vale 1. En caso de que las observaciones se vayan acumulando de una en una para la configuración de la curva bajo ROC, o para el cálculo directo del índice de Gini, es mejor utilizar una versión del mismo denominada “índice E” (IE, véase Cap. 3 de José-María Montero (2007)), pues el índice de Gini tan sólo proporcionará una aproximación de la concentración existente; buena aproximación, pero aproximación.Índice de Gini. Bien conocido en la literatura estadística sobre concentración, se trata de un indicador útil en el caso de clases desequilibradas. Su campo de variación es [0,1], donde 0 representa la igualdad perfecta y 1 la concentración en una única clase. Puede calcularse partir del área bajo la curva ROC de la siguiente manera: \\(IG=2\\hspace{0,1cm} \\acute{}rea\\hspace{0,1cm} bajo \\hspace{0,1cm} la\\hspace{0,1cm}curva\\hspace{0,1cm} ROC-1\\). En caso de IG=0, el área bajo la curva ROC es 1/2 y la curva ROC coincide con la diagonal ascendente. En caso de IG=1, el área bajo la curva ROC será 0 y dicha curva vale cero para todos los valores del eje de abscisas, excepto para el último, para el cual vale 1. En caso de que las observaciones se vayan acumulando de una en una para la configuración de la curva bajo ROC, o para el cálculo directo del índice de Gini, es mejor utilizar una versión del mismo denominada “índice E” (IE, véase Cap. 3 de José-María Montero (2007)), pues el índice de Gini tan sólo proporcionará una aproximación de la concentración existente; buena aproximación, pero aproximación.Índice de Jaccard. Mide las similitudes entre el conjunto clases reales y predichas por el clasificador. Se define como el cociente entre el número de coincidencias en los conjuntos real y predicho (clases predichas correctamente) y el tamaño de la unión de los dos conjuntos (el doble del número de clases etiquetar menos el número de clases predichas correctamente): \\(I_{Jaccard}=\\frac{VP+VN}{2Total-(Fp+FN)}\\).Índice de Jaccard. Mide las similitudes entre el conjunto clases reales y predichas por el clasificador. Se define como el cociente entre el número de coincidencias en los conjuntos real y predicho (clases predichas correctamente) y el tamaño de la unión de los dos conjuntos (el doble del número de clases etiquetar menos el número de clases predichas correctamente): \\(I_{Jaccard}=\\frac{VP+VN}{2Total-(Fp+FN)}\\).Otras medidas de interés son la pérdida logarítmica, el coeficiente de correlación de Matthews, el gráfico de Kolmogorov-Smirnov y el gráfico de ganancia y elevación. Éstas, entre otras, pueden verse en Dembla (2020), Hernández-Orallo, Flach, Ramirez (2011) y Vujović et al. (2021), entre otros.Con tantas métricas, ¿cómo decidir entre ellas? Será el conocimiento de la problemática que se esté estudiando (el negocio) el que normalmente guie la elección de la métrica: si se trata de un problema de predicción de la producción de un determinado bien cuyo coste de almacenaje es elevado, lo más prudente sería utilizar el ECM para penalizar la sobreproducción; si lo que interesa son valores altos de la precisión y la sensibilidad (tal es el caso en numerosas situaciones del ámbito sanitario), la mejor medida sería la puntuación F1. obstante, se habrá de tener siempre en cuenta que la exactitud, la precisión, la sensibilidad, la especificidad y el índice de Jaccard son buenas formas de evaluar clasificadores cuando las clases están equilibradas. En caso contrario, el área bajo la curva ROC y el IG (o el IE) son dos buenas alternativas.","code":"\nridge_pred <- predict(ridge.mod, s = 1e10, newx = x[test, ])\nmean((ridge_pred - y.test)^2)"},{"path":"chap-herramientas.html","id":"resumen-9","chapter":"Capítulo 10 Herramientas para el análisis en ciencia de datos","heading":"Resumen","text":"En este capítulo se describen una serie de herramientas necesarias para desarrollar proyectos de ciencia de datos. Son herramientas que se utilizan se utilizan pre- o post- modelado de los datos y que aumentan significativamente el rendimiento de los modelos. Tal caja de herramientas incluye el particionado del conjunto de datos, el manejo de datos equilibrados, los métodos de remuestreo, el equilibrio entre sesgo y varianza, el ajuste de hiperparámetros y la evaluación de modelos, entre otras.Se hace especial hincapié en las métricas para evaluar modelos, tanto en el entorno de regresión como en el de clasificación, y, en ambos casos, se ofrece un amplio abanico de ellas.efectos ilustrativos, se utiliza, fundamentalmente, el paquete caret y conjuntos de datos derivados del dataset Madrid_Sale.","code":""},{"path":"id_120006-aed.html","id":"id_120006-aed","chapter":"Capítulo 11 Análisis exploratorio de datos","heading":"Capítulo 11 Análisis exploratorio de datos","text":"Emilio L. CanoUniversidad Rey Juan Carlos","code":""},{"path":"id_120006-aed.html","id":"introducción-6","chapter":"Capítulo 11 Análisis exploratorio de datos","heading":"11.1 Introducción","text":"\nEl análisis exploratorio de datos (AED), y en particular su visualización,\nes el primer análisis que se debe hacer sobre cualquier conjunto de datos.\nEl AED se realiza mediante dos herramientas: los resúmenes numéricos\ny las visualizaciones gráficas.\nLa “historia” que nos esté contando el gráfico de los datos nos guiará\nhacia las técnicas de aprendizaje estadístico más adecuadas. Incluso,\nen muchas ocasiones será suficiente el AED para tomar\nuna decisión sobre el problema en estudio.","code":""},{"path":"id_120006-aed.html","id":"el-cuarterto-de-anscombe","chapter":"Capítulo 11 Análisis exploratorio de datos","heading":"11.1.1 El cuarterto de Anscombe","text":"Un ejemplo clásico de la importancia del AED y, concretamente, de las representaciones gráficas es el “cuarteto de Anscombe” (Anscombe 1973),\nel cual está compuesto por 11 filas de 8 variables numéricas que conforman 4 conjuntos de datos (disponibles en el objeto anscombe), con los mismos resúmenes estadísticos pero con propiedades muy distintas, lo que se ve fácilmente cuando se representan en forma gráfica. Si se calcula, por ejemplo, la media y la desviación típica de cada variable, se observa que son prácticamente iguales. Incluso los coeficientes de correlación de cada \\(X\\) con su \\(Y\\) son también prácticamente idénticos.Sin embargo, la Fig. 11.1 muestra que, pesar de tener medias y desviaciones típicas prácticamente iguales, los datos son muy diferentes.\nFigura 11.1: Representación de las variables del cuarteto de Anscombe\nSi en el análisis por separado ya se ve la necesidad de hacer un gráfico,\nésta es más evidente cuando se analizan las variables conjuntamente. La\nFig. 11.2 muestra los cuatro gráficos que constituyen\n“el cuarteto de Anscombe” y que se puede obtener de la propia ayuda del\nconjunto de datos (example(anscombe)). La línea de regresión que se ajusta\nes prácticamente la misma, y los coeficientes de correlación entre las variables X e Y de los cuatro gráficos, idénticos: 0.8163.\nEs evidente que la relación entre las variables es muy distinta en cada uno\nde los casos, y si se visualizan los datos para elegir el mejor modelo\nde regresión y después interpretarlo, se pueden tomar decisiones\nerróneas. El cuarteto de Anscombe es muy ilustrativo, al igual que Datasaurus Dozen (Matejka Fitzmaurice 2017) en https://www.autodeskresearch.com/publications/samestats.\nFigura 11.2: Los cuatro gráficos que constituyen el cuarteto de Anscombe junto con un ajuste lineal\n","code":"\nlibrary(\"dplyr\")\nanscombe |> summarise(across(.fns = mean))\n#>  x1 x2 x3 x4       y1     y2     y3     y4\n#>  9  9  9  9    7.5009  7.5009  7.5   7.5009\nanscombe |> summarise(across(.fns = sd))\n#>    x1    x2       x3       x4     y1      y2    y3      y4\n#> 3.316  3.316   3.316   3.316   2.031   2.031   2.030   2.030"},{"path":"id_120006-aed.html","id":"conceptos-generales","chapter":"Capítulo 11 Análisis exploratorio de datos","heading":"11.1.2 Conceptos generales","text":"Muy brevemente, se presentan una serie de conceptos esenciales para la mejor\ncomprensión de este manual85. Los datos que se analizan,\nprovienen de una determinada población, y son más que\nuna muestra, es decir, un subconjunto de toda la población.\nLa Estadística Descriptiva se ocupa del AED en sentido amplio, que se aplica\nsobre los datos concretos de la muestra. La\nInferencia Estadística (véase Cap. 13) hace referencia los métodos mediante los cuales, través de los datos muestrales, se toman decisiones, se analizan relaciones o se hacen predicciones sobre la población. Para ello, se hace uso de la Probabilidad aplicando el modelo adecuado (véase Cap. 12). Además, es muy importante considerar el método de obtención de la muestra (véase Cap. 14) que, en términos generales, debe ser representativa de la población para que las conclusiones sean válidas.\nLa Fig. 11.3 representa la esencia de la Estadística y sus métodos.\nFigura 11.3: La esencia de los métodos estadísticos\nLas características observar en los elementos de una población pueden dar lugar diferentes tipos de datos o variables. El análisis realizar dependerá del tipo de variable, que puede ser:Cuantitativa (se puede medir o contar). Se denomina variable cuantitativa cualquier característica observable que pueda expresarse en valores numéricos. Se clasifican como variables discretas (se puede contar el número de valores que toma) y continuas (pueden tomar cualquier valor en un intervalo dado).Cuantitativa (se puede medir o contar). Se denomina variable cuantitativa cualquier característica observable que pueda expresarse en valores numéricos. Se clasifican como variables discretas (se puede contar el número de valores que toma) y continuas (pueden tomar cualquier valor en un intervalo dado).Cualitativa (se puede expresar como un número). Se denomina variable cualitativa, atributo o factor cualquier característica observable que indica una cualidad o atributo. Éstas pueden tener varios niveles (politómicas) o solo dos (dicotómicas). Si en una variable categórica se pueden ordenar las categorías, entonces se denomina variables ordinal.Cualitativa (se puede expresar como un número). Se denomina variable cualitativa, atributo o factor cualquier característica observable que indica una cualidad o atributo. Éstas pueden tener varios niveles (politómicas) o solo dos (dicotómicas). Si en una variable categórica se pueden ordenar las categorías, entonces se denomina variables ordinal.","code":""},{"path":"id_120006-aed.html","id":"componentes-de-un-gráfico-y-su-representación-en-r","chapter":"Capítulo 11 Análisis exploratorio de datos","heading":"11.1.3 Componentes de un gráfico y su representación en R","text":"De los diferentes sistemas que tiene R para representar gráficos (los “base”, paquete graphics, y los “grid”, paquete lattice (Sarkar 2008)),\neste capítulo se centra en el paquete ggplot2 (Hadley Wickham 2016a), que forma parte del tidyverse, por su amplio uso y popularidad.El flujo de trabajo con ggplot2 se puede resumir en los siguientes pasos:Proporcionar una tabla de datos la función ggplot. Es el primer argumento (data) y\nse puede utilizar el operador pipe.Proporcionar una tabla de datos la función ggplot. Es el primer argumento (data) y\nse puede utilizar el operador pipe.Proporcionar las columnas de la tabla de datos que serán representadas en el gráfico.\nEste será el segundo argumento (mapping) de la función ggplot, y se\nespecifica con la función aes (aesthetics)\ncomo una lista de pares aesthetic = variable, de forma que el elemento especificado\ncomo aesthetic será “mapeado” los valores de la variable. Esta especificación\nse puede hacer también en las funciones que añaden capas, que se explican continuación.\nLos aesthetics más comunes (para muchos tipos de gráficos obligatorios) son x e y,\nes decir, las columnas que se usarán para el eje horizontal y el eje vertical respectivamente.\nAdemás, se pueden especificar columnas para el color, el tamaño, el símbolo de los puntos,\nel tipo de línea, el texto, y otros específicos del tipo de gráfico. Los aesthetics se\npueden especificar también de forma “fija” (sin depender de ninguna variable) fuera de\nla función aes.Proporcionar las columnas de la tabla de datos que serán representadas en el gráfico.\nEste será el segundo argumento (mapping) de la función ggplot, y se\nespecifica con la función aes (aesthetics)\ncomo una lista de pares aesthetic = variable, de forma que el elemento especificado\ncomo aesthetic será “mapeado” los valores de la variable. Esta especificación\nse puede hacer también en las funciones que añaden capas, que se explican continuación.\nLos aesthetics más comunes (para muchos tipos de gráficos obligatorios) son x e y,\nes decir, las columnas que se usarán para el eje horizontal y el eje vertical respectivamente.\nAdemás, se pueden especificar columnas para el color, el tamaño, el símbolo de los puntos,\nel tipo de línea, el texto, y otros específicos del tipo de gráfico. Los aesthetics se\npueden especificar también de forma “fija” (sin depender de ninguna variable) fuera de\nla función aes.Añadir las capas del gráfico con los geoms, es decir, los objetos geométricos que representan cada variable. Esto se indica con el operador +, como si se “sumasen” componentes al gráfico mediante funciones geom_xxx.Añadir las capas del gráfico con los geoms, es decir, los objetos geométricos que representan cada variable. Esto se indica con el operador +, como si se “sumasen” componentes al gráfico mediante funciones geom_xxx.Añadir otras capas al gráfico: por ejemplo, una capa de etiquetas del gráfico\n(función labs); de ejes, para modificar los ejes y leyendas creados por defecto\n(funciones scale_*_xxx); de estadísticos, para crear nuevas variables representar basadas en los datos (funciones stat_xxx).Añadir otras capas al gráfico: por ejemplo, una capa de etiquetas del gráfico\n(función labs); de ejes, para modificar los ejes y leyendas creados por defecto\n(funciones scale_*_xxx); de estadísticos, para crear nuevas variables representar basadas en los datos (funciones stat_xxx).Añadir un tema al gráfico: por ejemplo, en blanco y negro, o con especificaciones\nconcretas, como el posicionamiento de la leyenda.Añadir un tema al gráfico: por ejemplo, en blanco y negro, o con especificaciones\nconcretas, como el posicionamiento de la leyenda.Añadir “facetas” (facets). De esta forma se divide el gráfico en varios subgráficos\nbasándose en los valores de una o más variables discretas (normalmente categóricas).Añadir “facetas” (facets). De esta forma se divide el gráfico en varios subgráficos\nbasándose en los valores de una o más variables discretas (normalmente categóricas).En las secciones que siguen se verán ejemplos de todo el proceso. El siguiente es un ejemplo de una expresión que contiene los elementos anteriores. Se anima al lector que los identifique con dicha lista:","code":"\nggplot(data, aes(x = variable)) + geom_histogram() + \n  labs(title = \"Título\") + theme_bw() + facet_wrap(~factor)"},{"path":"id_120006-aed.html","id":"id_120006-aeduni","chapter":"Capítulo 11 Análisis exploratorio de datos","heading":"11.2 Análisis exploratorio de una variable","text":"","code":""},{"path":"id_120006-aed.html","id":"variables-cualitativas","chapter":"Capítulo 11 Análisis exploratorio de datos","heading":"11.2.1 Variables cualitativas","text":"El resumen numérico de variables cualitativas se muestra en la tabla de frecuencias,\nla cual se puede representar con un gráfico de barras o con un gráfico\nde sectores86. Las frecuencias absolutas son el número de observaciones en cada categoría y las frecuencias relativas son la proporción de observaciones en cada categoría con respecto al total. Por ejemplo, el conjunto de datos accidentes2020_data\ndisponible en el paquete CDR describe los datos de accidentes de tráfico\ncon víctimas y/o daños al patrimonio en la ciudad de Madrid registrados por Policía Municipal. Entre sus variables, contiene la variable cualitativa tipología del accidente tipo_accidente. Un resumen puede obtenerse tanto con la función table()\ncomo con el paquete dplyr, como se vio en la Sec. ??.\nEn variables cualitativas, la categoría más frecuente se denomina moda de la variable87.Para representar el gráfico de barras con la función ggplot(), se añade la capa de geometría con la función geom_bar() (véase Fig. 11.4).\nFigura 11.4: Gráfico de barras con ggplot2\nEl código anterior es la forma más básica de hacer un gráfico con ggplot2.\nOpciones más avanzadas pueden encontrarse en H. Wickham Grolemund (2016).Ya se ha comentado que los gráficos de sectores se recomiendan menos que se incluya en ellos información numérica. El paquete ggstatsplot realiza gráficos que incluyen análisis estadísticos. Por ejemplo, la función ggpiestats() proporciona un gráfico de sectores con algunos tests estadísticos (véase la ayuda de la función) y podría utilizarse para determinar en qué medida un conjunto de 80 ayuntamientos de distinto signo político presta o un determinado servicio serv (véase el conjunto de datos en el paquete del libro ?CDR::ayuntam). El siguiente código produce el gráfico de la Fig. 11.5.\nFigura 11.5: Gráfico de sectores con tests. Prestación o de un determinado servicio X en ayuntamientos de distinto signo político\nUna alternativa los gráficos de sectores son los waffle charts (gráficos de gofre o de tableta de chocolate). La siguiente expresión produce de la Fig. 11.6 usando el paquete waffle. Con el argumento use_glyph se pueden incluir iconos en vez de cuadrados.\nFigura 11.6: Gráfico waffle: Prestación o de un determinado servicio X en 80 ayuntamientos de distinto signo político\n","code":"\nlibrary(\"CDR\")\nlibrary(\"dplyr\")\naccidentes2020_data |>\n  count(tipo_accidente) |>\n  mutate(porc = 100 * n / sum(n))\n#>                  tipo_accidente    n       porc\n#>  1:                      Alcance 7294 22.4936010\n#>  2:           Atropello a animal   75  0.2312887\n#>  3:          Atropello a persona 2127  6.5593487\n#>  4:                        Caída 2118  6.5315940\n#>  5: Choque contra obstáculo fijo 4667 14.3923274\n#>  6:             Colisión frontal  899  2.7723810\n#>  7:      Colisión fronto-lateral 8081 24.9205909\n#>  8:             Colisión lateral 4386 13.5257656\n#>  9:            Colisión múltiple 2231  6.8800691\n#> 10:                Despeñamiento    2  0.0061677\n#> 11:                         Otro  251  0.7740463\n#> 12:        Solo salida de la vía  151  0.4656613\n#> 13:                       Vuelco  145  0.4471582\nlibrary(\"ggplot2\")\nlibrary(\"CDR\")\naccidentes2020_data |>\n  ggplot() +\n  geom_bar(aes(y=tipo_accidente), fill = \"pink\")\nlibrary(\"ggstatsplot\")\nayuntam |>\n  ggpiestats(x = serv)\nlibrary(\"waffle\")\nfreq <- ayuntam |> \n  count(serv)\nm <- setNames(freq$n, freq$serv)\nwaffle(m,\n  rows = 4, colors = c(\"red\", \"green\"))"},{"path":"id_120006-aed.html","id":"variables-cuantitativas","chapter":"Capítulo 11 Análisis exploratorio de datos","heading":"11.2.2 Variables cuantitativas","text":"Los estadísticos descriptivos más importantes que se utilizan en un AED\nse dividen en tres grandes grupos:Medidas de posición, que su vez se dividen en () centrales: media (mean()), mediana (median()) y moda y (ii) centrales: cuantiles quantile(), mínimo (min()) y máximo (max()).\nMedidas de posición, que su vez se dividen en () centrales: media (mean()), mediana (median()) y moda y (ii) centrales: cuantiles quantile(), mínimo (min()) y máximo (max()).\nMedidas de dispersión. Las más importantes son: varianza (var()), desviación típica (sd()), rango intercuartílico (IQR()), desviación absoluta mediana (mad()) y coeficiente de variación (sd(x)/mean(x)).\nMedidas de dispersión. Las más importantes son: varianza (var()), desviación típica (sd()), rango intercuartílico (IQR()), desviación absoluta mediana (mad()) y coeficiente de variación (sd(x)/mean(x)).\nMedidas de forma: asimetría (skewness) y apuntamiento (kurtosis).\nMedidas de forma: asimetría (skewness) y apuntamiento (kurtosis).\nLa función summary() de R base es una función de las llamadas “genéricas”\ny solo aborda las medidas de posición.Sin embargo, los estadísticos descriptivos suelen presentarse juntos “describiendo” el conjunto de datos. Existen distintos paquetes, como summarytools, que proporcionan un resumen completo de un vector numérico con la función descr() así como de un conjunto de datos completo (ver opciones del paquete).continuación se proporciona una breve definición de las anteriores medidas y su fórmula matemática, como referencia general y por su uso en otras partes del libro. En las fórmulas siguientes, \\(x_i\\) representa cada valor de la variable en la muestra y \\(n\\) es el número de observaciones en dicha muestra.Media (Mean), \\(\\overline{x}\\): es el centro de gravedad de los datos, en torno al cual varían.\\[\\bar{x}= \\frac{\\sum\\limits_{=1}^n x_i}{n}.\\]Desviación típica (Std.Dev), \\(s\\): es la raíz cuadrada de la varianza \\(s^2\\). Representa la variación promedio alrededor de la media, en las unidades de la variable y en sus unidades al cuadrado respectivamente. La siguiente fórmula corresponde la varianza muestral, dividiendo por \\(n-1\\). Para la varianza poblacional, se dividiría por el tamaño de la población \\(N\\). Para calcular la desviación típica, bastaría con hacer la raíz cuadrada.\\[s^2= \\frac{\\sum\\limits_{=1}^n (x_i- \\bar{x})^2}{n-1}.\\]El mínimo (Min) y el máximo (Max) son los extremos de los datos.El mínimo (Min) y el máximo (Max) son los extremos de los datos.La mediana (Median) es el segundo cuartil \\(Q_2\\). Es el punto central de los datos, dejando la mitad de las observaciones por abajo y la otra mitad por arriba. Los cuartiles \\(Q_1\\) y \\(Q_3\\), dividen los datos dejando por debajo de su valor el 25% y el 75%, respectivamente.La mediana (Median) es el segundo cuartil \\(Q_2\\). Es el punto central de los datos, dejando la mitad de las observaciones por abajo y la otra mitad por arriba. Los cuartiles \\(Q_1\\) y \\(Q_3\\), dividen los datos dejando por debajo de su valor el 25% y el 75%, respectivamente.La desviación absoluta mediana (MAD) es la mediana de las desviaciones la mediana.La desviación absoluta mediana (MAD) es la mediana de las desviaciones la mediana.El rango intercuartílico es la diferencia entre el tercer y el primer cuartil. Es decir, el rango del 50% de las observaciones centrales:El rango intercuartílico es la diferencia entre el tercer y el primer cuartil. Es decir, el rango del 50% de las observaciones centrales:\\[IQR = Q_3-Q_1.\\]El coeficiente de variación es el cociente entre la desviación típica y la media (en valor absoluto). Es una medida de variabilidad relativa muy útil para comparar la variabilidad de distintas muestras o poblaciones.\\[CV = \\frac{s}{|\\overline{x}|}.\\]Coeficiente de asimetría (Skewness). En variables simétricas es nulo. Si es mayor de cero, los datos presentan asimetría positiva (una cola la derecha, en los valores altos con respecto la media) y si es menor de cero, los\ndatos presentan asimetría negativa (una cola la izquierda, en valores bajos con respecto la media).\\[g_1 = \\frac{\\frac{1}{n}\\sum\\limits_{=1}^n(x_i-\\bar x ) ^3}{s^3}.\\]Coeficiente de apuntamiento (Kurtosis). Si los datos presentan un apuntamiento similar al de la distribución normal, será próximo cero. Cuanto más grande, más apuntado, y cuanto más pequeño, más aplanado.\\[g_2 = \\frac{\\frac{1}{n}\\sum\\limits_{=1}^n(x_i-\\bar x ) ^4}{s^4}-3.\\]Los otros tres valores que aparecen en la salida de la función descr() son el error típico del coeficiente de asimetría (SE.Skewness), el número total de valores válidos (N.Valid) y el porcentaje respecto al total de datos (Pct.Valid). El complementario de este último, por tanto, es el porcentaje de valores perdidos (missing).La representación gráfica de la tabla de frecuencias de una variable cuantitativa\nes el histograma88.\nPara representarlo, se cuenta el número de observaciones (frecuencia) por intervalo (bin). Una posible regla sería el método de Sturges89, que se puede hallar con la función nclass.Sturges().Para obtener la tabla de frecuencias de la renta neta per cápita en 2019\nusando el número de intervalos con la regla de Sturges se procede como sigue:Sin embargo, esta regla siempre es la más apropiada,\ncomo se verá en la Sec. 40.4,\npues debe estudiarse bien la naturaleza de la variable analizar.El histograma proporciona mucha información sobre la variable: () si es aproximadamente simétrica, (ii) si tiene forma de campana (se parece la distribución Normal), (iii) si hay valores extremos y cómo son de frecuentes,\ny (iv) si puede haber mezcla de poblaciones (más de una moda).La función geom_histogram() del paquete ggplot2 añade una capa con un\nhistograma al gráfico. El color de las barras se controla con el aesthetics fill y la altura puede representar las frecuencias absolutas (recuentos) o\nrelativas (proporciones). El número de intervalos se indica con el argumento\nbins, o alternativamente la anchura de intervalo con bin_width, véase la Fig. 11.7.\nFigura 11.7: Histogramas de la renta neta per cápita en 2019 con distintos bins. Izquierda: bins por defecto (n=30); Centro: bins con la regla de Strurges; Derecha: bins = 20\nUna representación alternativa al histograma es la línea de densidad, que sustituye\nlas barras por una línea continua, generalmente suavizada. continuación, se añade\nla linea de densidad uno de los histogramas de la Fig. 11.7 y el\nresultado se puede ver en la Fig. 11.8.\nFigura 11.8: Histograma y linea de densidad de la renta neta per capita española en 2019\nOtras representaciones gráficas muy útiles de las variables continuas son el\ngráfico de caja y bigotes y el diagrama de violín, que se obtienen fácilmente combinando en ggplot() las capas geom_boxplot() y geom_violin(), respectivamente (véase Fig. 11.9).\nFigura 11.9: Boxplot y violin plot de la renta neta per cápita en 2019\nOtra visualización básica para una variable numérica es la visualización\nsecuencial de las observaciones, bien través de puntos (geom_point()) o través de líneas (geom_line()). El orden de las observaciones puede indicar cuándo se ha producido un cambio u otros patrones.","code":"\nsummary(renta_municipio_data$`2019`)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n#>    4053    9914   11595   12247   13690   32183    5697\nlibrary(\"summarytools\")\nrenta_municipio_data |>\n  select(`2019`) |>\n  descr()\n\n#>Descriptive Statistics  \n#>2019  \n#>N: 55273  \n#>                        2019\n#>----------------- ----------\n#>             Mean   12246.84\n#>          Std.Dev    3562.94\n#>              Min    4053.00\n#>               Q1    9914.00\n#>           Median   11595.00\n#>               Q3   13690.50\n#>              Max   32183.00\n#>              MAD    2742.81\n#>              IQR    3776.25\n#>               CV       0.29\n#>         Skewness       1.82\n#>      SE.Skewness       0.01\n#>         Kurtosis       5.77\n#>          N.Valid   49576.00\n#>        Pct.Valid      89.69\nrenta_municipio_data |>\n  mutate(clases_sturges_renta = cut(`2019`,\n    breaks = nclass.Sturges(`2019`)\n  )) |>\n  count(clases_sturges_renta)\np <- renta_municipio_data |>\n  tidyr::drop_na() |>\n  ggplot(aes(`2019`))\n\nh1 <- p + geom_histogram(color = \"yellow\", fill = \"pink\")\nh2 <- p + geom_histogram(\n  color = \"yellow\", fill = \"pink\",\n  bins = nclass.Sturges(renta_municipio_data$`2019`)\n)\nh3 <- p + geom_histogram(color = \"yellow\", fill = \"pink\", bins = 20)\n\nlibrary(\"patchwork\")\nh1 + h2 + h3\np <- renta_municipio_data |>\n  tidyr::drop_na() |>\n  ggplot(aes(`2019`))\np + geom_histogram(aes(y = after_stat(density)), \n                   position = \"identity\", \n                   color = \"yellow\", fill = \"pink\") +\n  geom_density(lwd = 1, colour = 4)\np <- renta_municipio_data |>\n  tidyr::drop_na() |>\n  ggplot(aes(x=0, y= `2019`)) \nboxplot <- p + geom_boxplot(color = \"yellow\", fill = \"pink\")\nviolin <- p + geom_violin(aes(), color = \"yellow\", fill = \"pink\")\nboxplot + violin"},{"path":"id_120006-aed.html","id":"id_120006-aedmulti","chapter":"Capítulo 11 Análisis exploratorio de datos","heading":"11.3 Análisis exploratorio de varias variables","text":"En la Sec. ?? se ha realizado un AED de\nvariables aisladas, pero lo usual es incluir las relaciones entre variables\nen el AED.\nLas herramientas estadísticas utilizadas son: () las tablas de frecuencias conjuntas, que, en el caso de dos atributos, son tablas de doble entrada,\ncon un atributo en filas y el otro en columnas, para determinar si existe asociación entre dichos atributos, como se verá en el Cap. 23;\n(ii) los resúmenes numéricos, como la covarianza, el coeficiente de correlación, coeficientes de asociación, etc. y (iii) los gráficos en los que se puede representar más de una variable.","code":""},{"path":"id_120006-aed.html","id":"variables-cualitativas-1","chapter":"Capítulo 11 Análisis exploratorio de datos","heading":"11.3.1 Variables cualitativas","text":"El resumen numérico sigue siendo la tabla\nde frecuencias, en este caso conjuntas para las distintas combinaciones de los niveles\nde las variables. Este tipo de tablas se denominan tablas de contingencia (véase Cap. 23). Para dos atributos, se puede representar en\nforma de tabla de doble entrada.El resultado de la función table() se puede utilizar dentro de las funciones prop.table() y addmargins() para obtener las frecuencias relativas, añadir los totales marginales, o ambas cosas. Para el ejemplo de la prestación de un servicio “X” o por parte de 80 ayuntamientos, table() podría utilizarse para dar respuesta la siguiente pregunta: ¿La prestación pública del servicio X es independiente del signo político del Ayuntamiento o depende de dicho signo?obstante, la representación gráfica más habitual siguen siendo los gráficos de barras, como se muestra en la Fig. 11.10 producida con el siguiente código:\nFigura 11.10: Grafico de barras de la prestación pública del servicio X por parte de 80 Ayuntamientos de distinto signo político. Izquierda: frecuencias absolutas. Derecha: frecuencias relativas.\nUna visualización interesante de tablas de doble entrada son los gráficos en los que\nse representan las frecuencias conjuntas por medio de puntos cuya área es proporcional la frecuencia. La Fig. 11.11 muestra gráficamente la tabla de frecuencias conjunta de los atributos signo_gob y serv del conjunto de datos ayuntam.\nFigura 11.11: Representación gráfica de tabla de frecuencias con la función ballonplot()\nPara representar dos o más factores la vez en un único gráfico, se dispone de los gráficos de mosaico con la función mosaicplot() de R base, o bien el paquete\nggmosaic, que incluye una función geom_mosaic() para usar en gráficos ggplot2. El siguiente ejemplo produce el gráfico de la Fig. 11.12:\nFigura 11.12: Gráfico de mosaico para relacionar el tipo de accidente y el sexo en los datos de accidentes\nEn cualquier caso, se pueden representar más variables creando “subgráficos” o\nfacetas (facets). Basta con añadir una capa al gráfico ggplot2\ncon la función facet_wrap() y el argumento facets una lista de variables\n(categóricas o discretas) para cuyos valores se quiere hacer un gráfico\ndistinto. Con el siguiente código90 se construye un gráfico para cada nivel del factor\ntipo_accidente. Cada uno de estos gráficos es una “faceta” del gráfico, que se\nmuestra en la Fig. 11.13.\nFigura 11.13: Representación de tres atributos mediante gráficos de barras conjuntos y facetas\n","code":"\ntable(ayuntam$signo_gob , ayuntam$serv)\n#>             \n#>              No Sí\n#>   Avanzados  14 28\n#>   Ilustrados  6 32\np <- ayuntam |>\n  ggplot(aes(signo_gob, fill = serv))\n\nfrecuencias <- p + geom_bar() \nproporciones <- p + geom_bar(position = position_fill())\n\nfrecuencias + proporciones\nlibrary('gplots')\nballoonplot(table(ayuntam$signo_gob , ayuntam$serv))\nlibrary('ggmosaic')\naccidentes2020_data |>\nggplot() +\n  geom_mosaic(aes(x = product(tipo_accidente, sexo), \n                  fill=sexo)) \nniveles <- levels(factor(accidentes2020_data$tipo_accidente))\netiquetas <- set_names(str_wrap(niveles, width = 20), \n                       niveles)\naccidentes2020_data  |>\n  ggplot(aes(sexo, fill = estado_meteorológico)) +\n  facet_wrap(vars(tipo_accidente), \n             labeller = as_labeller(etiquetas)) +\n  geom_bar() +\n  labs(fill = \"Estado Meteorológico\") +\n  theme(axis.text.x  = element_text(angle = 90))"},{"path":"id_120006-aed.html","id":"variables-cuantitativas-1","chapter":"Capítulo 11 Análisis exploratorio de datos","heading":"11.3.2 Variables cuantitativas","text":"\nLa descripción conjunta de variables numéricas se\npuede resumir con el vector de medias (medias de cada variable) y la\nmatriz de varianzas-covarianzas. La covarianza \\(s_{xy}\\) (función var()) es una medida\ndel grado de dependencia lineal entre dos variables numéricas.\nSi la covarianza es cero, hay relación lineal (pero podría\nhaber otro tipo de relación, recuérdese el cuarteto de Anscombe).\nPero la covarianza es una medida que depende de la escala\nde las variables, por lo que\nes más fácil interpretar el coeficiente de correlación lineal \\(r_{xy}\\) (función cor()), que\nestá acotado entre -1 y 1. Cuanto más se acerque 1, en valor absoluto,\nmás fuerte será la dependencia lineal. Las fórmulas para calcular ambos estadísticos son\nlas siguientes:\\[s_{xy} = \\frac{1}{n-1} \\sum\\limits_{=1}^n(x_i-\\bar x)(y_i-\\bar y),\\]\\[r_{xy}=\\frac{s_{xy}}{s_x \\cdot s_y}.\\]Además, la matriz de correlación suele ser un punto de partida\nen las técnicas de reducción de la dimensionalidad (Véanse los Cap. 32, ?? y ??). Si, por ejemplo, se desea\ncalcular la matriz de correlaciones del conjunto de datos TIC2021,\nque presenta las estadísticas de uso de las TIC en la Unión Europea 2021,\nse puede utilizar el paquete corrplot, que proporciona una forma elegante y versátil de representarla91 (véase la Fig. 11.14).\nFigura 11.14: Representación gráfica de la matriz de correlaciones entre las variables del conjunto de datos TIC2021\nLa matriz de correlaciones se puede representar mediante “mapas de calor”\n(heatmap), es decir, un cuadrado que representa las filas y columnas de\nla matriz de correlaciones (variables) y donde el color de las celdas es\nuna gradación que depende del valor de las mismas. Un mapa de calor de la matriz de correlaciones guardada anteriormente, mcor_tic,\npuede obtenerse con la expresión heatmap(mcor_tic).En cuanto los resúmenes gráficos, el diagrama de dispersión es el gráfico más popular. La función geom_point() de ggplot2 añade una capa con los puntos (x, y), que ya nos da una idea de la relación entre las variables, y permite interpretarla\nconjuntamente con el coeficiente de correlación. Se puede añadir una línea de regresión, incluida una banda de confianza, por diversos métodos (función geom_smooth() por defecto, una curva loess o gam dependiendo del número de filas).\nAlternativamente los puntos como objeto geométrico, se pueden representar líneas (geom_line()).Por ejemplo, antes de llevar cabo un ajuste lineal, o de otro tipo, con los datos airquality, tal y como se hará en los Cap. 15 19, se podría hacer un AED\nprevio entre las variables Ozone y Temp con el gráfico de la Fig. 11.15.\nFigura 11.15: Gráfico de dispersión del Ozono frente la Temperatura\nUn caso particular es cuando la variable explicativa es el tiempo. En este caso, se tiene una serie temporal, y la representación con líneas es más adecuada (véase la Fig. 11.16).\nFigura 11.16: Concentración media semanal de NOx en las estaciones de medición de Madrid (enero 2011- marzo 2022)\n","code":"\nlibrary('corrplot')\nmcor_tic <- cor(TIC2021)  \ncorrplot.mixed(mcor_tic, order = 'AOE')\nairquality |>\n  dplyr::select(Ozone,Temp) |>\n  ggplot(aes( x= Temp, y=Ozone)) +\n  geom_point() +\n  geom_smooth()\nlibrary(\"dplyr\")\ncontam_mad |>  \n  filter(nom_abv == \"NOx\") |> \n  group_by(fecha, nom_mag) |>\n  summarise(media_estaciones = mean(daily_mean, na.rm = TRUE))|>\n  ggplot(aes(x = fecha, y = media_estaciones)) +\n  geom_line(aes(color = nom_mag)) +\n  geom_smooth(linewidth = 0.5, color = \"black\", se = TRUE) +\n  theme(legend.position = \"none\") "},{"path":"id_120006-aed.html","id":"variables-cualitativas-y-cuantitativas","chapter":"Capítulo 11 Análisis exploratorio de datos","heading":"11.3.3 Variables cualitativas y cuantitativas","text":"Cuando se trabaja en un proyecto de ciencia de datos, lo normal es tener tanto variables cualitativas como cuantitativas.\nPara representar conjuntamente ambos tipos de variables\nexisten múltiples posibilidades, algunas\nde las cuales se enumeran continuación, con el tipo\nde gráfico adecuado:Una variable numérica y una variable categórica: gráficos de cajas o de violín\npara cada nivel de la categórica (Fig. 11.17), o bien gráficos de densidad para cada categoría (Fig. 11.17).\nFigura 11.17: Comparación de los niveles de PM10 en las Zonas de la ciudad de Madrid efectos de Calidad del Aire durante la Calima de marzo de 2022\n\nFigura 11.18: Comparación de concentraciones de NOx por tipo de estación de medición\nDos variables numéricas y varias variables categóricas: gráfico de dispersión\npara las numéricas y mapeado del color, tamaño y símbolo por cada nivel de las categóricas,\ncomo en la Fig. 11.19.\nFigura 11.19: Grafíco de dipersión de las variables NOx, PM10, zona y tipo (de emplazamiento) durante el estado de alarma en la ciudad de Madrid (todas las estaciones de medición)\nMás de dos variables numéricas y más de una categórica: gráfico de dispersión\ny mapeado de las otras variables otros aesthetics. Combinación de geometrías\ny aesthetics. Por ejemplo, añadir puntos con efecto jitter un gráfico de cajas.En todos los casos anteriores se pueden crear “facetas” para hacer un gráfico\npor cada combinación de variables categóricas, de forma que se tenga un buen número de\nvariables representadas en un mismo “lienzo”, como en la Fig. 11.20.\nFigura 11.20: Grafíco de dipersión de las variables NOx, PM10, zona y tipo (de emplazamiento) por estación de medición durante el estado de alarma en la ciudad de Madrid\n","code":"\ncontam_mad |>  \n  na.omit() |>  \n  filter(nom_abv == \"PM10\") |> \n  filter(between(fecha, left = as.Date(\"2022-03-10\"), right = as.Date(\"2022-03-20\"))) |>\n  ggplot(aes(zona, daily_mean)) +\n  geom_violin() +\n  geom_jitter(height = 0, width = 0.01) +\n  aes(x = zona, y = daily_mean, fill =zona)\nlibrary('ggridges')\ncontam_mad |>\n  filter(nom_abv == \"NOx\") |>\n  ggplot(aes(x = daily_mean, y = tipo, fill = tipo)) +\n  geom_density_ridges() \n# periodo del estado de alarma\npm10_nox_mad <- contam_mad |>\n  na.omit() |>\n  filter(nom_abv %in% c(\"PM10\", \"NOx\")) |>\n  filter(between(fecha, left = as.Date(\"2020-03-14\"), right = as.Date(\"2020-06-30\"))) |> \n  select(estaciones, zona, tipo, nom_abv, daily_mean, fecha) |>\ntidyr::pivot_wider(names_from = \"nom_abv\", values_from = \"daily_mean\", values_fn = mean)\npm10_nox_mad |>\nggplot(\n  aes(x = PM10, y = NOx, colour = tipo, size = zona )) +\ngeom_point()\npm10_nox_mad |>\n  tidyr:: drop_na() |>\n  ggplot(aes(y=NOx, x= PM10, colour = tipo, shape = zona)) +\n  geom_point() +\n  geom_smooth() +\n  facet_wrap(vars(estaciones)) "},{"path":"id_120006-aed.html","id":"resumen-10","chapter":"Capítulo 11 Análisis exploratorio de datos","heading":"Resumen","text":"Análisis exploratorio de una variableEl análisis exploratorio es una tarea fundamental antes de abordar cualquier otra técnica estadística.El análisis exploratorio es una tarea fundamental antes de abordar cualquier otra técnica estadística.Las variables cualitativas se resumen con tablas de frecuencias y gráficos de barras.Las variables cualitativas se resumen con tablas de frecuencias y gráficos de barras.Las variables cuantitativas discretas se pueden resumir también con tablas de frecuencias y gráficos de barras, pero si hay muchos valores distintos también pueden ser apropiados los histogramas.Las variables cuantitativas discretas se pueden resumir también con tablas de frecuencias y gráficos de barras, pero si hay muchos valores distintos también pueden ser apropiados los histogramas.Las variables cuantitativas continuas se pueden resumir con tablas de frecuencias por intervalos, medidas de posición y de dispersión, histogramas y gráficos de cajas.Las variables cuantitativas continuas se pueden resumir con tablas de frecuencias por intervalos, medidas de posición y de dispersión, histogramas y gráficos de cajas.Los gráficos de caja sirven, además, para identificar valores atípicos.Los gráficos de caja sirven, además, para identificar valores atípicos.Análisis exploratorio de varias variablesLas variables cualitativas se pueden resumir con tablas de frecuencias conjuntas y su\nrepresentación gráfica y con combinaciones de gráficos de barras.Las variables cualitativas se pueden resumir con tablas de frecuencias conjuntas y su\nrepresentación gráfica y con combinaciones de gráficos de barras.La principal medida conjunta de dos variables cuantitativas es el coeficiente de correlación. Para más de dos variables se suelen representar en forma de matriz.La principal medida conjunta de dos variables cuantitativas es el coeficiente de correlación. Para más de dos variables se suelen representar en forma de matriz.El gráfico de dispersión es la representación básica para dos variables cuantitativas. Se pueden representar estos gráficos por pares en forma de matriz de gráficos.El gráfico de dispersión es la representación básica para dos variables cuantitativas. Se pueden representar estos gráficos por pares en forma de matriz de gráficos.Para añadir más variables, se pueden “mapear” variables aesthetics (tamaño, color, etc.), añadiendo más objetos geométricos, o bien añadiendo “facetas” (subgráficos) para cada variable o para cada posible valor de una variable cualitativa.Para añadir más variables, se pueden “mapear” variables aesthetics (tamaño, color, etc.), añadiendo más objetos geométricos, o bien añadiendo “facetas” (subgráficos) para cada variable o para cada posible valor de una variable cualitativa.","code":""},{"path":"Funda-probab.html","id":"Funda-probab","chapter":"Capítulo 12 Probabilidad","heading":"Capítulo 12 Probabilidad","text":"Mª Leticia Meseguer Santamaría\\(^{}\\) y Manuel Vargas Vargas\\(^{}\\)\\(^{}\\) Universidad de Castilla-La Mancha","code":""},{"path":"Funda-probab.html","id":"introducción-a-la-probabilidad","chapter":"Capítulo 12 Probabilidad","heading":"12.1 Introducción a la probabilidad","text":"La incertidumbre es inevitable en muchos campos científicos, producto de la imposibilidad de predeterminar el resultado de un fenómeno repetido bajo idénticas condiciones, el desconocimiento de todas o algunas causas que pueden influir en él, o una información limitada sobre los condicionantes que rigen su comportamiento. De hecho, gran parte del avance científico consiste en reducir o controlar el nivel de incertidumbre, mejorando el proceso de obtención e interpretación de datos o estableciendo modelos que expliquen los resultados.Producto de la incertidumbre, las decisiones que se toman (o la validez de los resultados que se obtienen) conllevan un riesgo, que puede concretarse en enunciados equivocados, modelos con escaso poder predictivo o decisiones con resultados deseados. Sin embargo, se prescinde de tomar decisiones en ambientes de incertidumbre, sino que se intenta evaluar y minimizar los riesgos asociados.Así pues, resulta importante poder “medir” la incertidumbre, es decir, cuantificar su magnitud y establecer reglas de medida que permitan su tratamiento –la estimación de riesgo– y ayuden la toma de decisiones. La teoría de la probabilidad se puede entender como un ente que proporciona reglas de comportamiento que ayudan conseguir los objetivos anteriores, siendo el campo de aplicación tan amplio que puede cubrir cualquier rama de las ciencias sociales, técnicas y naturales.El concepto de probabilidad apareció en la antiguedad, asociado los juegos de azar, y se ha ido refinando y formalizando lo largo de la historia. Sin embargo, la mayoría de las definiciones tradicionales presentan limitaciones que impiden su uso riguroso en cualquier situación. Aún así, siguen estando en el subconsciente colectivo, de forma que se entienden expresiones como “es muy probable que llueva mañana”, “es improbable que toque la lotería de Navidad”, o “es probable que se obtenga en breve una vacuna contra cierta enfermedad”, cuando responden conceptualizaciones diferentes y, en muchos casos, vagas e imprecisas.Aunque sigue habiendo debates filosóficos y epistemológicos sobre el concepto de probabilidad (véase, por ejemplo, Hajek Hitchcock (2016)), su uso generalizado en muchos campos científicos está más relacionado con el desarrollo de su carácter de medida de la incertidumbre que con un tratamiento matemático que permite su aplicación práctica (véase, por ejemplo, Ross (2012), Morin (2016) o Balakrishnan, Koutras, Politis (2019)). Es este enfoque el que se desarrolla sucintamente en este capítulo.","code":""},{"path":"Funda-probab.html","id":"probabilidad-elementos-básicos-definición-y-teoremas","chapter":"Capítulo 12 Probabilidad","heading":"12.2 Probabilidad: elementos básicos, definición y teoremas","text":"El desarrollo del concepto de probabilidad, entendido como medida de la incertidumbre sobre la ocurrencia de un evento, precisa de algunos requisitos previos que permitan una aplicación operativa.En primer lugar, es necesario definir en qué situaciones se puede aplicar. Se entenderá por experimento cualquier acción u observación de la realidad que pueda repetirse varias veces en idénticas condiciones, dando lugar resultados identificables y conocidos antes de ser realizado. Cuando, dadas las condiciones, se conoce qué resultado se producirá, se dice que el experimento es determinista; en caso contrario, si dadas las condiciones, se puede saber cuál de los posibles resultados ocurrirá, el experimento se denomina aleatorio o estocástico. Así pues, sólo se puede hablar de probabilidad sobre experimentos aleatorios.Ahora, dado un experimento aleatorio (\\(E\\)) y el conjunto de posibles resultados, denominados genéricamente sucesos (\\(\\Omega\\)), se define probabilidad como una medida del grado de creencia en la ocurrencia de cada posible suceso, \\(S \\\\Omega\\). Como se ve, la definición es muy amplia, por lo que precisa de algún requisito para evitar que una asignación concreta de grados de creencia produzca inconsistencias. Dicho requisito supone el cumplimiento de una estructura (matemática) concreta, que se adopta de forma axiomática. La más conocida, debida Andrei Kolmogorov, se puede formalizar de la siguiente forma:Axiomática de Kolmogorov: se considera un experimento aleatorio \\(E\\), el conjunto de posibles sucesos \\(\\Omega\\), y una función real \\(P\\) que asigna cada suceso un número real. Se dice que \\(P\\) es una medida de probabilidad si cumple:\\(P(S)\\geq 0 , \\forall S \\\\Omega\\).\n\\(P(S)\\geq 0 , \\forall S \\\\Omega\\).\\(P(\\Omega)=1\\).\n\\(P(\\Omega)=1\\).Dada una sucesión numerable de sucesos \\(\\left\\{ S_i \\right\\}\\) disjuntos dos dos, es decir \\(S_i \\cap S_j = \\emptyset\\ \\forall ,j\\) (donde \\(\\emptyset\\) es el suceso imposible), la probabilidad del suceso unión es la suma de sus probabilidades:\n\\(P \\left( \\underset {}{\\cup} S_i \\right) = \\underset{}{\\sum} P(S_i)\\).\nDada una sucesión numerable de sucesos \\(\\left\\{ S_i \\right\\}\\) disjuntos dos dos, es decir \\(S_i \\cap S_j = \\emptyset\\ \\forall ,j\\) (donde \\(\\emptyset\\) es el suceso imposible), la probabilidad del suceso unión es la suma de sus probabilidades:\n\\(P \\left( \\underset {}{\\cup} S_i \\right) = \\underset{}{\\sum} P(S_i)\\).Así, una probabilidad es una medida que cumple esta axiomática, asignando cada suceso un número real (entre 0 y 1) que expresa el grado de creencia en la ocurrencia de dicho suceso, entendiendo que 0 indica que se cree que ocurre nunca y 1 que ocurre seguramente (véase, por ejemplo Finetti (2017) para su fundamentación).Algunas consecuencias que se derivan de la axiomática de Kolmogorov de forma inmediata son:\\(P(\\emptyset)=0\\).\\(P(\\emptyset)=0\\).Denominando \\(\\bar S\\) al suceso complementario, \\(P(\\bar S)=1-P(S)\\).Denominando \\(\\bar S\\) al suceso complementario, \\(P(\\bar S)=1-P(S)\\).Dados dos sucesos cualesquiera, \\(P(S_1 \\cup S_2)=P(S_1)+P(S_2)-P(S_1 \\cap S_2)\\).Dados dos sucesos cualesquiera, \\(P(S_1 \\cup S_2)=P(S_1)+P(S_2)-P(S_1 \\cap S_2)\\).Sin embargo, esta definición es formal, en el sentido de que indica qué requisitos debe cumplir para evitar inconsistencias, pero determina qué valor concreto de probabilidad asignar cada suceso. Históricamente, se han propuesto varias concepciones para resolver este problema:Concepción clásica (o de Laplace): dado un experimento aleatorio \\(E\\) con \\(n\\) posibles resultados elementales mutuamente excluyentes e igualmente verosímiles, la probabilidad de un suceso \\(S_i\\) es:\n\\[\\begin{equation}\nP(S_i)=\\frac {\\text{casos favorables la ocurrencia de } S_i}{\\text{casos posibles}}=\\frac{n_i}{n}=f_i.\n\\end{equation}\\]\nPor “igualmente verosímiles” se entiende que “hay razón para afirmar que uno suceda más veces que otro”, conocido como “principio de razón insuficiente”. Es fácilmente comprobable que esta regla cumple la axiomática de Kolmogorov e interpreta la probabilidad como la “frecuencia” de ocurrencia de cada suceso. pesar de sus limitaciones (utiliza la equiprobabilidad de los sucesos elementales para definir la probabilidade y asume un conjunto finito de ellos), su fácil comprensión y utilidad en casos sencillos hace que esta regla sea muy utilizada (e incluso confundida con una “definición” de probabilidad).Concepción clásica (o de Laplace): dado un experimento aleatorio \\(E\\) con \\(n\\) posibles resultados elementales mutuamente excluyentes e igualmente verosímiles, la probabilidad de un suceso \\(S_i\\) es:\n\\[\\begin{equation}\nP(S_i)=\\frac {\\text{casos favorables la ocurrencia de } S_i}{\\text{casos posibles}}=\\frac{n_i}{n}=f_i.\n\\end{equation}\\]\nPor “igualmente verosímiles” se entiende que “hay razón para afirmar que uno suceda más veces que otro”, conocido como “principio de razón insuficiente”. Es fácilmente comprobable que esta regla cumple la axiomática de Kolmogorov e interpreta la probabilidad como la “frecuencia” de ocurrencia de cada suceso. pesar de sus limitaciones (utiliza la equiprobabilidad de los sucesos elementales para definir la probabilidade y asume un conjunto finito de ellos), su fácil comprensión y utilidad en casos sencillos hace que esta regla sea muy utilizada (e incluso confundida con una “definición” de probabilidad).Concepción frecuentista: se consideran \\(n\\) repeticiones de un experimento aleatorio, manteniendo idénticas condiciones. Sea \\(n_i\\) el número de veces que se presenta el suceso \\(S_i\\); entonces se le asigna la probabilidad:\n\\[\\begin{equation}\nP(S_i)=\\underset {n \\\\infty}{lim} \\frac {n_i}{n}.\n\\end{equation}\\]\nEsta concepción extiende la versión clásica, identificando la probabilidad con la frecuencia relativa de cada suceso cuando el experimento se repite un gran número de veces.Concepción frecuentista: se consideran \\(n\\) repeticiones de un experimento aleatorio, manteniendo idénticas condiciones. Sea \\(n_i\\) el número de veces que se presenta el suceso \\(S_i\\); entonces se le asigna la probabilidad:\n\\[\\begin{equation}\nP(S_i)=\\underset {n \\\\infty}{lim} \\frac {n_i}{n}.\n\\end{equation}\\]\nEsta concepción extiende la versión clásica, identificando la probabilidad con la frecuencia relativa de cada suceso cuando el experimento se repite un gran número de veces.El siguiente paso es formalizar cómo influye la ocurrencia de un suceso sobre la probabilidad de que ocurran otros. Así, dado un suceso \\(\\) con \\(P()>0\\), la probabilidad de que ocurra otro, \\(B\\), condicionado que ha ocurrido \\(\\), \\(P(B/)\\), se calcula como:\n\\[\\begin{equation}\n\\tag{12.1}\nP(B/)= \\frac {P(B \\cap )}{P()}.\n\\end{equation}\\]\nes decir, la probabilidad de que ocurran simultáneamente ambos dividida entre \\(P()\\) para que \\(P(\\Omega / )=1\\).Esta nueva medida se denomina probabilidad condicionada92 y permite obtener resultados fundamentales para el cálculo de probabilidades:Independencia de sucesos: dos sucesos, y B, se dicen independientes si \\(P(/B)=P() \\Rightarrow P(\\cap B)=P()P(B)\\).Independencia de sucesos: dos sucesos, y B, se dicen independientes si \\(P(/B)=P() \\Rightarrow P(\\cap B)=P()P(B)\\).Teorema de la probabilidad total: dado un conjunto de sucesos \\(\\left \\{ A_i \\right\\}\\) disjuntos y cuya unión es \\(\\Omega\\) (denominado partición de \\(\\Omega\\)), la probabilidad de cualquier suceso \\(B\\) compatible con los \\(A_i\\) es:\n\\[\\begin{equation}\nP(B)= \\underset {}{\\sum} P(B/A_i)P(A_i).\n\\tag{12.2}\n\\end{equation}\\]\nEste teorema permite determinar la probabilidad de un suceso \\(B\\), que puede tener varias causas, o darse bajo diversas alternativas, \\(A_i\\), mediante la suma de las probabilidades de que aparezca \\(B\\) condicionada cada una de las causas ponderadas por la probabilidad de cada causa o alternativa.Teorema de la probabilidad total: dado un conjunto de sucesos \\(\\left \\{ A_i \\right\\}\\) disjuntos y cuya unión es \\(\\Omega\\) (denominado partición de \\(\\Omega\\)), la probabilidad de cualquier suceso \\(B\\) compatible con los \\(A_i\\) es:\n\\[\\begin{equation}\nP(B)= \\underset {}{\\sum} P(B/A_i)P(A_i).\n\\tag{12.2}\n\\end{equation}\\]\nEste teorema permite determinar la probabilidad de un suceso \\(B\\), que puede tener varias causas, o darse bajo diversas alternativas, \\(A_i\\), mediante la suma de las probabilidades de que aparezca \\(B\\) condicionada cada una de las causas ponderadas por la probabilidad de cada causa o alternativa.Teorema de Bayes: dada una partición de \\(\\Omega\\), y un suceso \\(B\\) con \\(P(B)>0\\), la probabilidad de cada elemento de la partición condicionada que ha ocurrido \\(B\\) es:\n\\[\\begin{equation}\n\\tag{12.3}\nP(A_i/B)={P(A_i\\cap B) \\P(B)} = {P(B/A_i)P(A_i) \\\\underset {j}{\\sum} P(B/A_j)P(A_j)}.\n\\end{equation}\\]\nEste teorema, aplicación directa de la definición de probabilidad condicionada y del teorema de la probabilidad total, es un resultado tan importante que su uso ha dado nombre una rama entera de la estadística, la conocida como estadística bayesiana.93 También es utilizado en la moderna inteligencia artificial, en técnicas como Naive Bayes (véase Cap. 27)Teorema de Bayes: dada una partición de \\(\\Omega\\), y un suceso \\(B\\) con \\(P(B)>0\\), la probabilidad de cada elemento de la partición condicionada que ha ocurrido \\(B\\) es:\n\\[\\begin{equation}\n\\tag{12.3}\nP(A_i/B)={P(A_i\\cap B) \\P(B)} = {P(B/A_i)P(A_i) \\\\underset {j}{\\sum} P(B/A_j)P(A_j)}.\n\\end{equation}\\]\nEste teorema, aplicación directa de la definición de probabilidad condicionada y del teorema de la probabilidad total, es un resultado tan importante que su uso ha dado nombre una rama entera de la estadística, la conocida como estadística bayesiana.93 También es utilizado en la moderna inteligencia artificial, en técnicas como Naive Bayes (véase Cap. 27)","code":""},{"path":"Funda-probab.html","id":"variable-aleatoria-y-su-distribución-de-probabilidad","chapter":"Capítulo 12 Probabilidad","heading":"12.3 Variable aleatoria y su distribución de probabilidad","text":"Una limitación operativa de la probabilidad, tal como se ha utilizado hasta ahora, es que hace referencia sucesos y operaciones entre conjuntos, lo que dificulta su tratamiento. Sin embargo, en muchos casos los sucesos están caracterizados por valores numéricos, por lo que podrían ser utilizados en sustitución de los primeros para facilitar los cálculos. esta idea corresponde la noción de variable aleatoria (v..), que es una función que asigna un valor numérico cada suceso de un experimento aleatorio. Para trabajar con probabilidades sobre números, cada uno se le asigna la probabilidad de los sucesos que están caracterizados por dicho valor.94Dada una v.. \\(X\\), su función de distribución asigna cada número real \\(x\\) la probabilidad de que la variable tome un valor menor o igual que \\(x\\),\\[\\begin{equation}\n\\tag{12.4}\nF_X (x) = P(X \\leq x).\n\\end{equation}\\]Una variable se dice discreta si sólo puede tomar un conjunto finito (o infinito numerable) de valores con probabilidad positiva. ese conjunto de valores y sus probabilidades \\(\\left\\{ x_i ; P(X=x_i) \\right\\}\\) se le denomina función de cuantía.Una variable se denomina continua, si su función de distribución es continua y existe su primera derivada y es continua. Como consecuencia, la probabilidad en un valor concreto siempre será cero, \\(P(X=x_i)=0\\), por lo que sólo habrá probabilidades positivas sobre intervalos. Se denomina función de densidad la derivada de la función de distribución\n\\(f(x)=F^{\\prime}(x)=\\frac{dF(x)}{dx}\\).95Dado que una v.. \\(X\\) está caracterizada por su distribución de probabilidad (través de la función de distribución o de la de cuantía-densidad), se han desarrollado modelos de distribución de probabilidad que permiten modelizar el comportamiento aleatorio de las v.. y calcular probabilidades de forma sencilla.","code":""},{"path":"Funda-probab.html","id":"modelos-de-distribución-de-probabilidad","chapter":"Capítulo 12 Probabilidad","heading":"12.4 Modelos de distribución de probabilidad","text":"En esta sección se presentan los modelos más utilizados en la práctica, distinguiendo entre modelos discretos y continuos, según la naturaleza de la v.. Para una visión completa, se puede consultar, por ejemplo, Johnson, Kemp, Kotz (2008).","code":""},{"path":"Funda-probab.html","id":"modelos-discretos","chapter":"Capítulo 12 Probabilidad","heading":"12.4.1 Modelos discretos","text":"Los modelos de distribución discretos más populares son el binomial, el binomial negativo y el de Poisson. Los dos primeros se asientan sobre el fenómeno de Bernoulli con independencia, que, de manera general, consiste en un experimento dicotómico (o que puede considerarse dicotómico), es decir, que se consideran sólo dos posibles resultados (uno identificado con el éxito del experimento, cuya probabildad se denota por \\(p\\), y el otro con el fracaso, con probabilidad \\(q=(1-p)\\)) tal que los resultados producidos por el experimento son independientes de los precedentes.Distribución Binomial B(\\(n\\),\\(p\\))La distribución binomial \\((n,p)\\) es una distribución de probabilidad discreta que asigna probabilidades al número de éxitos en una secuencia de n experimentos independientes de Bernoulli con una probabilidad fija de éxito \\(p\\). Puede tomar los valores \\(x=0,1,...,n\\) y su función de cuantía es:\n\\[\\begin{equation}\nP(X=x)=\\binom{n}{x}p^xq^{n-x} \\equiv \\frac {n!}{x!(n-x)!} p^xq^{n-x}.\n\\end{equation}\\]\nSu esperanza, valor esperado o media es \\(E(X)=\\mu=np\\), y su varianza \\(Var(X)=\\sigma^2=npq\\).La representación gráfica de las funciones de cuantía y distribución se muestra en la Fig. 12.1\nFigura 12.1: Función de cuantía y de distribución para una variable B(10,0.5)\nEn el caso particular de que n=1, B(1,p), se denomina distribución Bernoulli, B(p).Distribución Binomial negativa o de Pascal BN(\\(r\\),\\(p\\))La distribución binomial negativa surge en el contexto de una serie de experimentos de Bernoulli independientes, con probabilidad constante de éxito \\(p\\), donde la v.. \\(X\\) denota el número de experimentos fracasados (\\(x\\)) hasta que se produce un número determinado de éxitos (\\(r\\)). Puede tomar los valores \\(x=0,1,...\\) y su función de cuantía es:\n\\[\\begin{equation}\nP(X=x)=\\binom{x+r-1}{x}q^xp^r \\equiv \\frac {(x+r-1)!}{(r-1)!x!}q^xp^r,\n\\end{equation}\\]\ncon media \\(E(X)=r\\frac{q}{p}\\) y varianza \\(Var(X)=r\\frac{q}{p^2}\\).La representación gráfica de las funciones de cuantía y distribución se muestra en la Fig. 12.2\nFigura 12.2: Función de cuantía y de distribución para una variable BN(3,0.35)\nEn el caso particular de que \\(r=1\\), BN(1,\\(p\\)), se denomina distribución geométrica G(\\(p\\)).Distribución de Poisson P(\\(\\lambda)\\)Se denominan fenómenos de Poisson aquéllos en los que la ocurrencia de un suceso se encuentra distribuida lo largo de un tiempo (o espacio) dado, cumpliendo que el proceso es estable, con una media de ocurrencias \\(\\lambda\\) por unidad de tiempo, ocurrencias que se presentan de forma aleatoria e independiente. La variable que mide el número \\(x\\) de ocurrencias puede tomar los valores \\(x=0,1,2,...\\) y se dice que sigue una distribución de Poisson, con función de cuantía:\n\\[\\begin{equation}\nP(X=x)=\\frac{e^\\lambda\\lambda^x}{x!}.\n\\end{equation}\\]\nSu media es \\(E(X)=\\lambda\\) y su varianza \\(Var(X)=\\lambda\\).La representación gráfica de las funciones de cuantía y distribución se muestra en la Fig. 12.3\nFigura 12.3: Función de cuantía y de distribución para una variable P(2.5)\n","code":"\npar(mfrow=c(1,2))\nx<-0:10\ndens <- dbinom(0:10, size=10, prob=0.5)\nplot(x, y=dens, type=\"h\", xlab=\"x\",ylab=\"P(x)\",main=\"Función de cuantía\", col=\"red\", lwd=2)\nplot(x, pbinom(x,10,0.5),ylab=\"F(x)\",xlab=\"x\",type=\"s\",main=\"Función de distribución\",lwd=2)\npar(mfrow=c(1,2))\nx<-0:20\nplot(x,dnbinom(x,3,0.35),type=\"h\",ylab=\"P(x)\",xlab=\"x\",main=\"Función de cuantía\", col=\"pink\", lwd=2)\nplot(x, pnbinom(x,3,0.35),ylab=\"F(x)\",xlab=\"x\",type=\"s\",main=\"Función de distribución\",lwd=2)\npar(mfrow=c(1,2))\nx<-0:10\nplot(x,dpois(x,3),type=\"h\",ylab=\"P(x)\",xlab=\"x\",main=\"Función de cuantía\", col=\"darkred\",lwd=2)\nplot(x, ppois(x,3),ylab=\"F(x)\",xlab=\"x\",type=\"s\",main=\"Función de distribución\",lwd=2)"},{"path":"Funda-probab.html","id":"modelos-continuos","chapter":"Capítulo 12 Probabilidad","heading":"12.4.2 Modelos continuos","text":"Los modelos de distribución para variables continuas más habituales son el Normal, el Gamma, el Chi-cuadrado, el \\(t\\)-student y el \\(F\\)-Snedecor.Distribución Normal N\\((\\mu , \\sigma)\\)La distribución Normal, de Gauss o gaussiana, tiene una gran importancia debido que un gran número de fenómenos aleatorios se pueden modelizar partir de ella (véase la Sec. 12.5 sobre el teorema central del límite). Además, es la distribución que se toma como supuesto y en la que se basan muchas de las técnicas estadísticas que se ven en este libro.Una v.. se dice que sigue una distribución normal de parámetros \\(\\mu\\) y \\(\\sigma\\) si puede tomar cualquier valor real y su función de densidad es de la forma:\\[\\begin{equation}\nf(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}.\n\\end{equation}\\]\nSu media es \\(E[X]=\\mu\\) y su varianza, \\(Var(X)=\\sigma^2\\). Por ello, se suele decir que la normal está caracterizada por su media y su desviación típica.La gráfica de la función de densidad tiene forma de campana (conocida como campana de Gauss) y es simétrica respecto de la media, con mayor probabilidad en las colas conforme aumenta la desviación típica, como muestra la Fig. 12.4.\nFigura 12.4: Función de densidad y de distribución de variables Normales, con media 0 y desviación típica 1 (azul), 1.5 (rojo), y 2 (verde)\nUna característica importante de la distribución normal es que verifica la\npropiedad aditiva o reproductiva, es decir, que las combinaciones lineales de distribuciones normales independientes siguen siendo distribuciones normales. Si se consideran \\(n\\) variables aleatorias independientes con distribuciones \\(N(\\mu_i,\\sigma_i)\\), cualquier combinación lineal cumple:\n\\[\\begin{equation}\n\\beta_0+\\beta_1X_1+...+\\beta_nX_n=\\beta_0 + \\sum_{=1}^{n}\\beta_iX_i \\sim N \\left( \\beta_0+\\sum_{=1}^{n}\\beta_i\\mu_i, \\sqrt{\\sum_{=1}^{n}\\beta_i^2\\sigma_i^2} \\right).\n\\end{equation}\\]En particular, si \\(X\\sim N(\\mu,\\sigma)\\), la variable \\(Z=\\frac{X-\\mu}{\\sigma} \\sim N(0,1)\\), y se conoce como normal estándar o normal tipificada.96Distribución Gamma \\(\\Gamma ( \\alpha, \\beta )\\)La distribución Gamma es útil en el contexto de los fenómenos de Poisson o cuando se trata de asignar probabilidades al tiempo de espera (o la vida útil) hasta que ocurre un número determinado de sucesos (\\(\\alpha\\)), suponiendo que \\(\\beta\\) es el tiempo medio entre ocurrencias de un suceso.Esta distribución toma valores positivos y su función de densidad viene dada por la expresión:\n\\[\\begin{equation}\nf(x) = \\frac {1}{\\beta^\\alpha \\Gamma (\\alpha)}x^{\\alpha-1}e^{-x/\\beta},\n\\end{equation}\\]\ndonde \\(\\Gamma( \\alpha)= \\int_0^\\infty x^{\\alpha-1}e^{-x}dx=(\\alpha -1)!\\) si \\(\\alpha\\) es un número natural.Su media es \\(E(X)=\\alpha\\beta\\) y su varianza, \\(Var(X)=\\alpha\\beta^2\\). El parámetro \\(\\alpha\\) es conocido como parámetro de forma; si \\(\\alpha\\leq1\\), la función de densidad tiene forma de “L”; si \\(\\alpha>1\\), la distribución es campaniforme, con asimetría positiva, y conforme va aumentando, el centro de la distribución se desplaza hacia la derecha. \\(\\beta\\) se conoce como parámetro de escala y determina el alcance de la asimetría positiva. La Fig. 12.5 muestra la representación gráfica de las funciones de densidad y distribución para varias combinaciones de valores de los parámetros (\\(\\beta=2\\); \\(\\alpha=1\\) (azul), \\(\\alpha=2\\) (morado), \\(\\alpha=5\\) (rojo) y \\(\\alpha=10\\) (verde)).\nFigura 12.5: Función de densidad y de distribución de variables Gamma\nEl caso particular de que \\(\\alpha=1\\) se denomina distribución exponencial de parámetro \\(\\beta\\).Distribución \\(\\chi_n ^2\\) de PearsonSean \\(X_1, X_2, ....,X_n\\) v.. independientes, todas distribuidas según una \\(N(0,1)\\). La suma de sus cuadrados sigue una distribución \\(\\Gamma (\\frac {n}{2} , 2 )\\), que se denomina distribución Chi-cuadrado:97\n\\[\\begin{equation}\n\\sum_{=1}^n X_i^2 \\sim \\chi^2_n \\equiv \\Gamma (\\frac {n}{2} , 2 ).\n\\end{equation}\\]Al parámetro \\(n\\) se le llama grados de libertad. Su media es \\(E(X)=n\\) y su varianza es \\(Var(X)=2n\\) y la forma funcional de su densidad y distribución son casos particulares de la Gamma. De hecho, la Fig. 12.5 corresponde distribuciones \\(\\chi^2_n\\) con \\(n=2,4,10, \\text{y } 20\\).Distribución \\(t-Student\\)La distribución \\(t-Student\\) surge, entre otros contextos, en el muestreo de poblaciones normales (véase el Sec. 13.6), asociada al uso de medias. Se dice que una v.. \\(X\\) sigue una distribución \\(t-student\\) con \\(n\\) grados de libertad si es el cociente entre una distribución normal estándar y la raiz de una \\(\\chi^2_n\\) dividida entre sus grados de libertad, ambas independientes: \\(X \\sim \\frac{N(0,1)}{\\sqrt{\\chi^2_n /n}}\\).Su función de densidad viene dada por:\n\\[\\begin{equation}\nf(x)=\\frac{\\Gamma((n+1)/2)}{\\sqrt{n\\pi}\\Gamma(n/2)}(1+x^2/n)^{-(n+1)/2},\n\\end{equation}\\]\ncon media \\(E(X)=0\\) y varianza \\(Var(X)=\\frac{n}{n-2}\\), siendo \\(n>2\\). Su densidad tiene forma acampanada, simétrica respecto cero y parecida la de la Normal, pero con mayor probabilidad en las colas. En la Fig. 12.6 se muestran las funciones de densidad y distribución para tres \\(t-Student\\).98\nFigura 12.6: Función de densidad y de distribución de variables t-Student, con 3 (verde), 10 (rojo) y 100 (azul) grados de libertad\nDistribución \\(F\\) de SnedecorEste modelo también está asociado al muestreo sobre poblaciones normales, en este caso, la comparación de varianzas. Se define una distribución \\(F\\) de Snedecor con \\(n\\) y \\(m\\) grados de libertad como el cociente de dos distribuciones \\(\\chi^2\\) independientes divididas entre sus grados de libertad, \\(F_{n,m} = \\frac {\\chi^2_{n}/n}{\\chi^2_{m}/m}\\).La función de densidad \\(F_{n,m}\\) viene dada por:\n\\[\\begin{equation}\nf(x)=\\frac{\\Gamma(\\frac{n+m}{2})}{\\Gamma(\\frac{n}{2})\\Gamma(\\frac{m}{2})}\\left(\\frac{n}{m}\\right)^{\\frac{n}{2}}\\frac{x^{\\frac{n-2}{2}}}{(1+\\frac{nx}{m})^{\\frac{n+m}{2}}}\n\\end{equation}\\]\ncon media \\(E(X)=\\frac{m}{m-2}\\), siendo \\(m\\)>2 y varianza \\(Var(X)=\\frac{2m^2(n+m-2)}{n(m-2)^2(m-4)}\\), cuando \\(m\\)>4.La gráfica de la función de densidad es parecida la de la \\(\\chi_n^2\\). Así, sólo está definida para el semieje positivo y su apariencia variará según los grados de libertad. La Fig. 12.7 muestra las funciones de densidad y distribución para varias distribuciones F-Snedecor.\nFigura 12.7: Función de densidad y de distribución de variables F-Snedecor, en azul con (5,10) grados de libertad, en rojo con (10,5) y en verde con (5,5)\n","code":"\npar(mfrow=c(1,2))\nx<-seq(-5, 5, 0.01)\nplot(x,dnorm(x,0,1), ylab=\"P(x)\", xlab=\"x\", main=\"Función de densidad\",type=\"l\",col=\"blue\")\ncurve(dnorm(x,0,1.5), ylab=\"P(x)\", add=TRUE,type=\"l\",col=\"red\")\ncurve(dnorm(x,0,2), ylab=\"P(x)\", add=TRUE ,type=\"l\",col=\"darkgreen\")\nplot(x, pnorm(x,0,1),ylab=\"F(x)\",xlab=\"x\",type=\"s\",main=\"Función de distribución\",col=\"blue\")\ncurve(pnorm(x,0,1.5), ylab=\"P(x)\", add=TRUE ,type=\"l\",col=\"red\")\ncurve(pnorm(x,0,2), ylab=\"P(x)\", add=TRUE ,type=\"l\",col=\"darkgreen\")\npar(mfrow=c(1,2))\nx<-seq(0, 10, 0.01)\nplot(x, dgamma(x,2,2),type=\"l\", ylab=\"f(x)\",main=\"Función de densidad\",col=\"purple\",lwd=2)\ncurve(dgamma(x,5,2),type=\"l\", add=TRUE,col=\"red\",lwd=2)\ncurve(dgamma(x,10,2),type=\"l\", add=TRUE,col=\"green\",lwd=2)\ncurve(dgamma(x,1,2),type=\"l\", add=TRUE,col=\"blue\",lwd=2)\nplot(x, pgamma(x,2,2),type=\"l\", ylab=\"f(x)\",main=\"Función de distribución\",col=\"purple\",lwd=2)\ncurve(pgamma(x,5,2),type=\"l\", add=TRUE,col=\"red\",lwd=2)\ncurve(pgamma(x,10,2),type=\"l\", add=TRUE,col=\"green\",lwd=2)\ncurve(pgamma(x,1,2),type=\"l\", add=TRUE,col=\"blue\",lwd=2)\npar(mfrow=c(1,2))\nx<-seq(-3, 3, 0.01)\nplot(x, dt(x,df=100),type=\"l\", ylab=\"f(x)\",main=\"Función de densidad\",col=\"blue\",lwd=2)\ncurve(dt(x,df=10),type=\"l\", add=TRUE,col=\"red\",lwd=2)\ncurve(dt(x,df=3),type=\"l\", add=TRUE,col=\"darkgreen\",lwd=2)\nplot(x, pt(x,df=100),type=\"l\", ylab=\"f(x)\",main=\"Función de distribución\",col=\"blue\",lwd=2)\ncurve(pt(x,df=10),type=\"l\", add=TRUE,col=\"red\",lwd=2)\ncurve(pt(x,df=3),type=\"l\", add=TRUE,col=\"darkgreen\",lwd=2)\npar(mfrow=c(1,2))\nx<-seq(0, 4, 0.01)\nplot(x, df(x,5,10),type=\"l\", ylab=\"f(x)\",main=\"Función de densidad\",col=\"blue\")\ncurve(df(x,10,5),type=\"l\", add=TRUE,col=\"red\")\ncurve(df(x,5,5),type=\"l\", add=TRUE,col=\"darkgreen\")\nplot(x, pf(x,5,10),type=\"l\", ylab=\"f(x)\",main=\"Función de distribución\",col=\"blue\")\ncurve(pf(x,10,5),type=\"l\", add=TRUE,col=\"red\")\ncurve(pf(x,5,5),type=\"l\", add=TRUE,col=\"darkgreen\")"},{"path":"Funda-probab.html","id":"tcl","chapter":"Capítulo 12 Probabilidad","heading":"12.5 Teorema central del límite","text":"veces es difícil encontrar la distribución muestral de algunos estadísticos o estimadores, o, incluso, es imposible determinar la distribución de la variable de interés; entonces es útil aplicar algunos teoremas de convergencia, en especial el Teorema Central del Límite (TCL).El TCL permite, bajo ciertas condiciones, usar la distribución normal para aproximar otras distribuciones o para modelizar el comportamiento de variables de las que se desconozca su distribución.Teorema central del límite: si \\(X_1,..., X_n\\) son v.. independientes e idénticamente distribuidas (iid) con media \\(\\mu\\) y desviación típica \\(\\sigma\\), entonces \\(\\sum_{=1}^n X_i\\) tiene asintóticamente una distribución normal de media \\(n\\mu\\) y desviación típica \\(\\sqrt{n}\\sigma\\),\n\\[\\begin{equation}\n  \\frac{\\sum_{=1}^n X_i-n\\mu}{\\sqrt{n}\\sigma} \\underset {n\\rightarrow \\infty}\\longrightarrow N(0,1).\n\\end{equation}\\]\nEl TCL indica que la distribución de la suma de \\(n\\) variables aleatorias independientes tiende una distribución normal, cuando \\(n\\) es muy grande. Es decir, aunque cada uno de los efectos sea raro o difícil de estudiar, si lo que se quiere estudiar es la suma de los mismos, se sabe que, bajo ciertas condiciones y siempre que sean independientes, ésta se comportará como una distribución normal. Así se explica el hecho constatado de que muchas distribuciones de variables observadas en la naturaleza o en experimentos físicos sean aproximadamente normales; por ejemplo, las medidas del cuerpo humano, altura, peso, longitud de los dedos, etc.","code":""},{"path":"Funda-probab.html","id":"distprobR","chapter":"Capítulo 12 Probabilidad","heading":"12.6 Distribuciones de probabilidad en R","text":"En R están implementadas las distribuciones de probabilidad más importantes, de tal forma que, en el paquete Rlab, se aplica cada nombre del modelo un determinado prefijo para calcular una función específica: d para la función de cuantía o densidad, p para la función de distribución, q para los cuantiles (o percentiles) y r para generar muestras pseudo-aleatorias.En la Tabla 12.1 se exponen los modelos de distribución vistos, indicando el tipo, con su notación y la función utilizada en R para su cálculo:Tabla 12.1:  Funciones de distribución en RA continuación se realizan dos ejemplos con R, uno para modelos discretos y otro para la normal. La adaptación cualquier otro modelo de probabilidad consiste, básicamente, en sustituir las funciones de R.","code":""},{"path":"Funda-probab.html","id":"ejemplo-de-distribuciones-discretas-con-r","chapter":"Capítulo 12 Probabilidad","heading":"12.6.1 Ejemplo de distribuciones discretas con R","text":"Sea un algoritmo de identificación que trata un número muy elevado de imágenes, teniendo acreditada una tasa de error del 20% en caso de personas y del 5% para el resto de imágenes. Supóngase que las imágenes de personas son el 25% del total de imágenes.Si se analizan 10 imágenes de personas, calcúlese la probabilidad de que identifique correctamente siete.Denominando \\(X\\) al número de imágenes de personas correctamente clasificadas, se tiene que \\(X \\sim B(10,0.8)\\). Se pide \\(P(X=7)\\):Para el resto de imágenes, calcúlese la probabilidad de que identifique correctamente como mucho 50 hasta que se produzca el segundo error.Denominando \\(Y\\) al número de imágenes correctamente identificadas hasta el segundo error, se tiene que \\(Y \\sim BN(2,0.05)\\). Se pide \\(P(Y \\leq 50)\\):Históricamente, el número medio diario de imágenes incorrectamente clasificadas es de 7. Calcular la probabilidad de que un día seleccionado al azar clasifique incorrectamente entre 6 y 9 imágenes.Denominando \\(T\\) al número de imágenes incorrectamente identificadas en un día, se tiene que \\(T \\sim P(\\lambda = 7)\\). Se pide \\(P(6 \\leq T \\leq 9) = P(T \\leq 9) - P(T \\leq 5)\\):Calcúlese la probabilidad de que en un lote de 20 imágenes del mismo tipo todas sean correctamente clasificadas.Como se especifica el tipo de imágenes, hay que calcular dicha probabilidad condicionada cada grupo y utilizar el teorema de la probabilidad total: \\(P(acierto)=P(acierto/personas)*P(personas)+P(acierto/otras)*P(otras)\\).Si se han clasificado correctamente las 20 imágenes del lote, calcúlese la probabilidad de que correspondan imágenes de personas.En este caso hay que utilizar el teorema de Bayes: \\(P(personas/acierto)= \\frac {P(acierto/personas)*P(personas)}{P(acierto)}\\).","code":"\ndbinom(7,size=10,prob=0.8)\n#> [1] 0.2013266\npnbinom(50,size=2,prob=0.05)\n#> [1] 0.7405031\nppois(9,7,lower.tail = TRUE) - ppois(5,7,lower.tail = TRUE)\n#> [1] 0.5297877\nacierto_personas<-dbinom(0,20,0.2)\nacierto_otras<-dbinom(0,20,0.05)\nacierto_total<-acierto_personas*0.25+acierto_otras*0.75\nacierto_total\n#> [1] 0.2717467\nacierto_personas*0.25/acierto_total\n#> [1] 0.01060658"},{"path":"Funda-probab.html","id":"ejemplo-de-una-distribución-normal-con-r","chapter":"Capítulo 12 Probabilidad","heading":"12.6.2 Ejemplo de una distribución normal con R","text":"Las calificaciones (de 0 10) en un curso de estadística siguen una de distribución normal N(6, 1.25), \\(X \\sim N(6, 1.25)\\). Calcúlese:La probabilidad de que una persona obtenga una calificación inferior 4.El número esperado de personas que obtendrán sobresaliente (9 o más) en un grupo de 60 personas.la nota mínima para estar en el 30% de personas con mejores calificaciones.En un curso de informática las calificaciones siguen una de distribución normal \\(N(5, 1.75)\\), independientes de las de estadística. Calcular la probabilidad de que una persona matriculada en ambos cursos saque mayor calificación en estadística.Llamando \\(X=C_e-C_i\\) la diferencia entre las calificaciones en estadística (\\(C_e\\)) y en informatica (\\(C_i\\)), ambas normales e independientes, la distribución de \\(X\\) será \\(X \\sim N \\left ( 6-5, \\sqrt{1.25^2+1.75^2} \\right )\\). Se pide \\(P(X>0)\\), que se representa en la Fig. 12.8.\nFigura 12.8: P(X>0) representada como el área bajo la función de densidad de X\n","code":"\npnorm(4,mean=6,sd=1.25)\n#> [1] 0.05479929\np<-pnorm(9,6,1.25,lower.tail=FALSE)\np\n#> [1] 0.008197536\n# En un grupo de 60 personas, redondeando a un número entero\nround(60*p)\n#> [1] 0\nqnorm(0.7,6,1.25)\n#> [1] 6.655501\npnorm(0,mean=1,sd=sqrt(1.25^2+1.75^2),lower.tail = FALSE)\n#> [1] 0.6790309\nmedia<-1\ndesv<-sqrt(1.25^2+1.75^2)\narea_n<-function (media, desv ,lb , ub,...)\n{\n  x<-seq(media-4*desv, media+4*desv, 0.05)\n  if (missing(lb)) {lb<-min(x)}\n  if (missing(ub)) {ub<-max(x)}\n  plot(x,dnorm(x, media, desv), ylab=\"P(x)\", xlab=\"x\", main=\"Probabilidad \",type=\"l\", lty=1, lwd=2)\n# Nueva rejilla de valores para x2, para el área.\n  x2<-seq(0, 10, 0.05)\n  y<-dnorm(x2, media, desv)\n  polygon(c(0, x2, 10), c(0, y, 0),col=\"lightblue\")\n}\narea_n(media,desv, 0, 10)"},{"path":"Funda-probab.html","id":"resumen-11","chapter":"Capítulo 12 Probabilidad","heading":"Resumen","text":"La teoría de la probabilidad proporciona reglas de comportamiento que permiten la ordenación y toma de decisiones en situaciones donde prevalecen condiciones de incertidumbre.Los modelos de distribución de probabilidad más usuales en la práctica son, para variables discretas, el binomial, el binomial negativo y el Poisson (junto sus casos particulares). Para el caso de variables continuas, los modelos más frecuentes son el normal, el gamma, el t-Student, el Chi-cuadrado y el F-Snedecor (así como sus casos particulares). Estos modelos facilitan el cálculo de probabilidades en, prácticamente, cualquier proyecto de las ciencias sociales, técnicas y naturales.","code":""},{"path":"Fundainfer.html","id":"Fundainfer","chapter":"Capítulo 13 Inferencia estadística","heading":"Capítulo 13 Inferencia estadística","text":"Mª Leticia Meseguer Santamaría\\(^{}\\) y Manuel Vargas Vargas\\(^{}\\)\\(^{}\\) Universidad de Castilla-La Mancha","code":""},{"path":"Fundainfer.html","id":"introinfer","chapter":"Capítulo 13 Inferencia estadística","heading":"13.1 Introducción","text":"Cuando se estudian fenómenos mediante variables aleatorias, el objetivo estadístico básico es determinar cuáles son las distribuciones probabilísticas que rigen dichas variables o algunas características determinadas por ellas. Es este comportamiento aleatorio el que permite hacer predicciones con unos márgenes de error conocidos, analizar y cuantificar la relación entre variables, evaluar si hipótesis o modelos teóricos son congruentes con los datos disponibles, etc. Así, en la práctica, cuando se estudia una variable \\(X\\), lo habitual es que se desconozca su distribución probabilística, \\(F(x)\\), pero que se disponga de un conjunto de realizaciones \\((x_1,...,x_n)\\), también llamado muestra, valores concretos de dicha variable partir de los cuales “aproximar” la distribución desconocida.La inferencia estadística proporciona las herramientas y técnicas que permiten, partir de la información muestral, extrapolar resultados la distribución poblacional con márgenes de error conocidos. Un primer objetivo (más detallado en el Cap. 14) es analizar qué condiciones debe cumplir la muestra para que su información sea válida y extrapolable toda la población (es la conocida como teoría de muestreo). Un segundo objetivo es establecer los mecanismos que permitan dicha extrapolación manteniendo controlados los errores de muestreo.Es habitual que se conozca (o se asuma) que la distribución poblacional \\(F(x)\\) pertenezca alguna familia paramétrica, es decir, que se asuma su forma funcional pero que dependa de algunos parámetros (lo más frecuente es que se asuma la normalidad, pero podría ser cualquiera de los modelos paramétricos existentes). Se habla entonces de inferencia paramétrica, ya que se usa la información muestral para determinar los “mejores” valores (bajo algún criterio) de los parámetros que rigen la distribución poblacional, existiendo tres planteamientos básicos: estimación puntual (Sec. 13.3), por intervalo (Sec. 13.4) y contraste de hipótesis (Sec. 13.5).\nTambién hay situaciones en las que la forma funcional de la distribución poblacional es desconocida, o se duda de que la familia paramétrica considerada sea adecuada. En estos casos, bajo el nombre genérico de inferencia paramétrica, se plantean contrastes que buscan determinar cuándo es posible asumir un modelo concreto de distribución, entre los que destacan, por su frecuente uso, los contrastes de normalidad (Sec. 13.8). Otra alternativa que permite aproximar características poblacionales sin asumir ninguna distribución poblacional concreta es el remuestreo, fundamentalmente el denominado “bootstrap” (se aborda en el Cap. 14).","code":""},{"path":"Fundainfer.html","id":"mas","chapter":"Capítulo 13 Inferencia estadística","heading":"13.2 Muestreo aleatorio simple","text":"Al estudiar una variable poblacional, \\(X\\), de la que se desconoce su distribución, llamada distribución poblacional, \\(F(x)\\), se utiliza la información suministrada por una muestra obtenida por algún método de muestreo probabilístico que garantice que sea representativa de la variable poblacional.En la mayoría de los casos y técnicas estadísticas se asume que la muestra está obtenida mediante el método básico de muestreo, conocido como muestreo aleatorio simple, consistente en seleccionar totalmente al azar y con reemplazo los individuos de la muestra, por lo que todos tienen la misma probabilidad de formar parte de ella. De esta forma, dada una distribución poblacional \\(F(x)\\), una muestra aleatoria simple (m..s.) es una realización de un conjunto de \\(n\\) variables aleatorias independientes e idénticamente distribuidas \\(X=(X_1,...,X_n)\\), denominadas variables muestrales y cuya distribución conjunta es de la forma:\\[\\begin{equation}\nF(X_1,...,X_n)=F_{X_1}(x_1)...F_{X_n}(x_n)=F(x_1)...F(x_n).\n\\tag{13.1}\n\\end{equation}\\]Una herramienta básica para la inferencia es la distribución empírica de la muestra, definida como:\\[\\begin{equation}\n\\tag{13.2}\n\\hat{\\mathbb{F}}_n (x)= {1 \\n} \\sum_{=1}^n \\mathbb{}_{(-\\infty ,x]}(X_i),\n\\end{equation}\\]donde \\(\\mathbb{}_{(-\\infty ,x]}(X_i)\\) es una función indicadora que toma el valor 1 si \\(X_1 \\leq x\\) y 0 en caso contrario.99La gran ventaja del muestreo aleatorio simple consiste en que, dada una m..s. \\((X_1,..., X_n)\\):\\[\\begin{equation}\n\\tag{13.3}\n\\underset {n \\rightarrow \\infty}{lim} \\ E \\left [ {\\left ( \\hat{\\mathbb{F}}_n (x)- F(x) \\right )^2} \\right ] = 0,\n\\end{equation}\\]expresión conocida como teorema de Glivenko-Cantelli. Este resultado es fundamental en inferencia, pues garantiza que el muestreo aleatorio simple produce muestras representativas de la población, ya que, medida que aumenta el tamaño muestral, la distribución empírica de la muestra se aproxima cada vez más la distribución poblacional (véase Fig. 13.1).100 Así, cualquier característica (media, varianza…) de una distribución poblacional puede ser aproximada por su equivalente en la distribución empírica.\nFigura 13.1: Distribución empírica para muestras de diferente tamaño de una distribución Normal\nEs muy frecuente que, efectos de inferencia, se estudie el comportamiento aleatorio de toda la muestra (su distribución conjunta) sino que interese el comportamiento de una función de la muestra que dependa de ningún valor desconocido, \\(T(X)=T(X_1,...,X_n)\\), llamada genéricamente estadístico muestral; dicho comportamiento vendrá determinado por la distribución en el muestreo del estadístico \\(T(X)\\). El hecho de utilizar una m..s. permite establecer resultados de interés sobre los estadísticos o, en algunos casos, incluso obtener la distribución en el muestreo exacta de los estadísticos más usuales (Sec. 13.6).Así, dadas una variable poblacional \\(X\\) con varianza finita y una m..s., se define la media muestral (aleatoria) como:\\[\\begin{equation}\n\\tag{13.4}\n\\bar X = \\frac {X_1, + ... + X_n}{n}.\n\\end{equation}\\]El hecho de utilizar una m..s. garantiza que:\\[\\begin{equation}\nE[\\bar X] = E[X] \\ \\text{;} \\ Var(\\bar X)=\\frac{Var(X)}{n}.\n\\end{equation}\\]Este resultado es muy útil, ya que indica que la variabilidad de la media muestral es más pequeña que la variabilidad de la variable poblacional, siendo inversamente proporcional al tamaño muestral.Otro estadístico muy utilizado es la varianza muestral,101 que se define como:\\[\\begin{equation}\n\\tag{13.5}\nS^2 = {\\sum_{=1}^n \\left ( X_i - \\bar X \\right ) ^2 \\n}.\n\\end{equation}\\]En este caso, su esperanza es:\\[\\begin{equation}\nE[S^2] = \\frac {n-1}{n} Var[X],\n\\end{equation}\\]que coincide con la varianza poblacional. Para evitar este hecho, se define la cuasivarianza muestral (aleatoria):\\[\\begin{equation}\n\\tag{13.6}\nS_c^2 = {\\sum_{=1}^n \\left ( X_i - \\bar X \\right ) ^2 \\{n-1}},\n\\end{equation}\\]estadístico para el que sí se cumple que \\(E[S_c ^2] = Var[X]\\), ya que existe una relación de proporcionalidad entre ambos estadísticos \\(nS^2 = (n-1)S_c ^2\\).102","code":"\npar(mfrow = c(1, 3))\nset.seed(196)\nx1 <- rnorm(20)\nplot.ecdf(x1, main = \"n=20\")\ncurve(pnorm, add = TRUE, col = \"red\")\nx2 <- rnorm(50)\nplot.ecdf(x2, main = \"n=50\")\ncurve(pnorm, add = TRUE, col = \"red\")\nx3 <- rnorm(200)\nplot.ecdf(x3, main = \"n=200\")\ncurve(pnorm, add = TRUE, col = \"red\")"},{"path":"Fundainfer.html","id":"estimpuntual","chapter":"Capítulo 13 Inferencia estadística","heading":"13.3 Estimación puntual","text":"Sea una población caracterizada por una distribución poblacional, \\(F (x,\\theta)\\), de una familia paramétrica de la que se desconoce el valor del parámetro \\(\\theta \\\\Theta\\), donde \\(\\Theta\\) es el espacio paramétrico (conjunto de posibles valores de \\(\\theta\\)). Dada una m..s. \\(X=(X_1,...,X_n)\\), se considera como estimador de \\(\\theta\\) un estadístico muestral cuyo resultado sea un posible valor del parámetro:\\[\\begin{equation}\n\\tag{13.7}\n\\hat{\\theta}=T(X)=T(X_1,...,X_n) \\\\Theta.\n\\end{equation}\\]La siguiente expresión corresponde al error cuadrático medio de un estimador:\\[\\begin{equation}\n\\tag{13.8}\nECM_\\theta (\\hat{\\theta})=E_\\theta \\left[ { \\left ( \\hat{\\theta}-\\theta \\right ) ^2 } \\right],\n\\end{equation}\\]que proporciona un valor medio del error que se comete al “aproximar” el verdadero valor \\(\\theta\\) por el resultado del estimador \\(\\hat{\\theta}\\).\nAsí, el criterio de “mínimos cuadrados” propone utilizar el estimador que minimiza el error cuadrático medio:\\[\\begin{equation}\n\\hat{\\theta}_{MC}= \\underset {\\hat{\\theta}} {min} E_\\theta \\left[ {\\left ( \\hat{\\theta}-\\theta \\right )^2} \\right].\n\\end{equation}\\]Desarrollando la expresión del ECM (\\(\\ref{eq:ecm}\\)), éste se puede re-expresar como\\[\\begin{equation}\nECM_\\theta (\\hat{\\theta})=Var_\\theta (\\hat{\\theta}) + \\left ( E_{\\theta}(\\hat{\\theta}) - \\theta \\right ) ^2  = Var_\\theta (\\hat{\\theta}) + b_\\theta ^2 (\\hat{\\theta}),\n\\end{equation}\\]donde \\(b_\\theta(\\hat{\\theta}) =\\left ( E_{\\theta}(\\hat{\\theta}) - \\theta \\right )\\) se conoce como sesgo del estimador (bias, en inglés). Así, el ECM de un estimador depende de su varianza y de su sesgo al cuadrado.Por tanto, la determinación del “mejor” estimador, bajo el criterio de mínimos cuadrados, se puede llevar cabo en dos pasos:Seleccionar estimadores “insesgados”, es decir, de sesgo cero, o sea, \\(E(\\hat{\\theta})=\\theta\\) (el valor medio del estimador coincide con el parámetro).Seleccionar estimadores “insesgados”, es decir, de sesgo cero, o sea, \\(E(\\hat{\\theta})=\\theta\\) (el valor medio del estimador coincide con el parámetro).De entre los estimadores inesgados, seleccionar el de varianza mínima, \\(Var(\\hat{\\theta}_{MC})= \\underset {\\hat{\\theta}} {min} Var(\\hat{\\theta})\\).De entre los estimadores inesgados, seleccionar el de varianza mínima, \\(Var(\\hat{\\theta}_{MC})= \\underset {\\hat{\\theta}} {min} Var(\\hat{\\theta})\\).Queda fuera del objetivo de este capítulo plantear la obtención del estimador de mínimos cuadrados para cualquier distribución poblacional y parámetros, que el lector interesado puede encontrar en cualquier texto teórico de inferencia estadística (Casella Berger (2007), Blais (2020), Almudevar (2021)).Otro planteamiento para encontrar estimadores puntuales se basa en la función de densidad conjunta de la muestra, que depende de ésta y del parámetro que caracteriza la distribución poblacional:\\[\\begin{equation}\n\\tag{13.9}\nf(x_1,...,x_n;\\theta)=f(x_1;\\theta)...f(x_n;\\theta)=L(\\theta;x_1,...,x_n).\n\\end{equation}\\]Considerando el parámetro como fijo, la función se interpreta como la densidad de probablilidad de la muestra. Sin embargo, si se considera que la muestra está dada, entonces se puede interpretar como una función del parámetro que mide la verosimilitud (likelihood, en inglés) de cada valor del parámetro en función de la muestra obtenida. Así, el criterio para determinar el “mejor” estimador puede ser seleccionar aquél que maximiza la función de verosimilitud; se obtiene entonces el conocido como estimador máximo verosímil :\\[\\begin{equation}\n\\hat{\\theta}_{MV}= \\underset {\\theta} {max} L(\\theta;x_1,...,x_n).\n\\end{equation}\\]Para el cálculo del estimador máximo verosímil se suele utilizar la función de verosimilitud, sino su logaritmo (que alcanza los máximos y mínimos en los mismos puntos), derivando respecto al parámetro e igualando cero (ecuación de verosimilitud).Este método suele proporcionar estimadores con buenas propiedades estadísticas y, en muchos casos, suele conducir al mismo resultado que el método de mínimos cuadrados.103 En las distribuciones usuales, es relativamente sencillo obtener la ecuación de verosimilitud y resolverla, por lo que se dispone de estimadores máximo verosímiles conocidos. En modelos más elaborados, la resolución de la ecuación de verosimilitud se puede complicar, hasta el extremo de que haya que recurrir métodos numéricos de aproximación.Una alternativa computacionalmente más sencilla es la basada en el conocido como método de los momentos. El planteamiento básico es expresar el parámetro en función de los momentos poblacionales (esperanza, varianza, etc.) y utilizar como estimador la misma función pero de los momentos muestrales (media muestral, varianza muestral, etc.). En las distribuciones más usuales, los parámetros suelen ser momentos poblacionales o tranformadas simples de éstos, por lo que el método de los momentos es muy sencillo. Como contrapartida, es más difícil evaluar las propiedades estadísticas de estos estimadores, salvo que coincidan con los de mínimos cuadrados o de máxima verosimilitud.En R, el paquete fdistrplus dispone de la función fitdist(), que permite la obtención de los estimadores para las distribuciones usuales por diversos métodos, incluidos el de máxima verosimilitud (mle) y el de los momentos (mme).","code":""},{"path":"Fundainfer.html","id":"estimintervalos","chapter":"Capítulo 13 Inferencia estadística","heading":"13.4 Estimación por intervalos","text":"Dado que todo estimador es una variable aleatoria, su valor concreto, la “estimación” del parámetro \\(\\hat\\theta\\), depende de la muestra. Esta variación muestral ocasiona incertidumbre sobre la estimación. Una forma de incluir esta variabilidad en la estimación puede consistir en sustituir la estimación puntual por un intervalo de valores en el que se tenga un cierto nivel de confianza de que contenga al verdadero valor del parámetro.El método más extendido para obtener intervalos de confianza consiste en utilizar un estimador puntual y su distribución en el muestreo para construir un intervalo que contenga, con cierta probabilidad \\((1-\\alpha)\\), el verdadero valor \\(\\theta\\):\\[\\begin{equation}\n\\tag{13.10}\nIC_{(1-\\alpha)}=[LIC , LSC] \\ \\text{tal que} \\ P \\left ( LIC \\leq \\theta \\leq LSC  \\right ) = (1-\\alpha),\n\\end{equation}\\]donde los límites inferior (LIC) y superior (LSC) de confianza, denominados valores críticos, dependen de la desviación típica del estimador y de constantes asociadas su distribución y al nivel de confianza \\((1-\\alpha)\\). En esta ecuación, tanto el LIC como el LSC son variables aleatorias; cuando se utilizan los datos de una muestra, se convierten en valores reales, por lo que se puede hablar de “probabilidad de que el parámetro esté dentro del intervalo”, sino que se habla de “confianza en que el intervalo contenga el valor del parámetro”.En R, el paquete Rlab permite obtener los valores críticos de las distribuciones usuales través de los cuantiles, anteponiendo q al nombre de la distribución (véase la Tabla 12.1 “Funciones de distribución en R”); por ejemplo, usando las funciones qbinom(), qnorm(), qt(), qf(), etc.. Igualmente, el paquete DescTools dispone de funciones para calcular intervalos de confianza en poblaciones normales para la media (MeanCI()), la diferencia de medias (MeanDiffCI()), la mediana (MedianCI()), cualquier cuantil (QuantileCI()) o la varianza (VarCI()). Por último, en el caso de conocer la distribución en el muestreo del estimador, se puede recurrir al remuestreo por bootstrap, que se detallará en el Cap. 14, indicando el método boot en las funciones anteriores.","code":""},{"path":"Fundainfer.html","id":"contrhip","chapter":"Capítulo 13 Inferencia estadística","heading":"13.5 Contrastes de hipótesis","text":"Hay situaciones donde interesa tanto estimar el valor de un parámetro sino decidir si la información muestral es congruente con algún valor concreto del parámetro. En estos casos, se puede establecer como hipótesis que el parámetro toma un valor concreto y contrastar si es verosímil haber obtenido el resultado muestral dado. Este planteamiento se conoce como contrastes de significación.Así, se establece una hipótesis, históricamente conocida como hipótesis nula, que determina un valor del parámetro:\\[\\begin{equation}\nH_0 \\equiv \\theta = \\theta_0.\n\\end{equation}\\]Suponiendo cierta la hipótesis nula, la distribución muestral del estimador permite obtener la probabilidad de observar un valor del estimador más “distante” del valor del parámetro fijado en la hipótesis nula que el obtenido en la muestra, probabilidad conocida como p-valor: si es muy pequeño, es muy poco probable que se observe el valor obtenido en la muestra cuando la hipótesis es cierta, por lo que la evidencia empírica es congruente con ella; si es pequeño, dicho valor es probable que se observe (bajo la hipótesis nula), por lo que habría evidencia empírica “en contra” de ella.Se habla de p-valor bilateral o “dos colas” cuando la distancia se considera tanto por la derecha como por la izquierda de la distribución del estimador bajo la hipótesis nula. En caso de que se considere sólo por la izquierda o por la derecha, se habla de p-valor unilateral (la izquierda o la derecha, respectivamente) o “una cola”. La comparación (distancia) entre el valor del parámetro establecido en la hipótesis nula y el del estimador de dicho parámetro puede llevarse cabo por diferencia (tal es el caso del contraste de medias) o por cociente (caso de los contrastes de varianzas).Habitualmente, se considera que un p-valor por debajo de 0.05 ya indica que la evidencia empírica permite asumir como cierta la hipótesis nula, expresándose como que el valor del parámetro es “significativamente distinto (menor o mayor)” que \\(\\theta_0\\). También es posible interpretar el p-valor como “la probabilidad máxima de cometer el error de rechazar la hipótesis nula cuando es cierta”, abreviado como “tamaño del error si se rechaza la hipótesis nula”.Estos contrastes de significación, originalmente desarrollados por Ronald Fisher, fueron incluidos en un esquema de toma de decisiones por Jerzy Neyman y Egon Pearson, planteando que, de ser cierta la hipótesis nula, se debe plantear una hipótesis alternativa \\(H_1\\). La decisión de qué hipótesis resulta más congruente con los datos se basa en la comparación por cociente de las verosimilitudes de la muestra bajo cada una de ellas, decidiendo el rechazo de la hipótesis nula favor de la alternativa cuando dicho cociente es, en probabilidad, inferior un valor prefijado, \\(\\alpha\\), conocido como nivel de significación. Dependiendo de la estructura de las hipótesis (simples, si sólo determinan un valor del parámetro, o compuestas, si determinan más de uno; su vez, unilaterales si los valores son todos menores, o mayores, que uno dado, o bilaterales en caso contrario) la regla de decisión del contrate resulta más o menos compleja de obtener.Cuando se adopta el planteamiento decisional de Neyman-Pearson, el nivel de significación permite evaluar la probabilidad de rechazar la hipótesis nula cuando es cierta (conocida también como probabilidad de error de tipo , \\(\\alpha\\)), pero también la probabilidad de aceptar como cierta \\(H_0\\) cuando es más correcta \\(H_1\\) (denominada probabilidad de error de tipo II, \\(\\beta\\)) o, equivalentemente, su complementario: la probabilidad de rechazar \\(H_0\\) cuando \\(H_1\\) es más correcta, probabilidad conocida como potencia del contraste, \\((1-\\beta)\\). Si la hipótesis alternativa es simple, es posible evaluar la potencia, por lo que se tiene una medida probabilística de la magnitud de ambos errores (de tipo y de tipo II), lop cual permite una valoración completa del resultado de la regla de decisión (contraste de hipótesis). Sin embargo, si la hipótesis \\(H_1\\) es compuesta, la magnitud de la potencia es una función evaluada en el rango de valores que establezca dicha hipótesis. En este caso, se dispone de una medida probabilística del error de tipo pero del error de tipo II, puesto que depende de valores concretos del parámetro que son especificados en la hipótesis alternativa, \\(H_1\\).Computacionalmente, dada la información muestral, es más fácil calcular el p-valor que plantear el esquema de decisión de Neyman-Pearson, por lo que es la estrategia utilizada en la práctica.Dado el carácter breve e introductorio de este capítulo, se profundizará más en este esquema de decisión, que puede consultarse, por ejemplo, en Casella Berger (2007), Blais (2020) o Almudevar (2021), entre otros muchos.","code":""},{"path":"Fundainfer.html","id":"pobnormales","chapter":"Capítulo 13 Inferencia estadística","heading":"13.6 Inferencia estadística paramétrica sobre poblaciones normales","text":"Como consecuencia del teorema central del límite (Sec. 12.5), el supuesto de que la distribución poblacional es una normal es el caso más habitual en la práctica, siendo requisito básico en muchísimas técnicas estadísticas. En este caso, las distribuciones muestrales de los estimadores de los parámetros poblacionales, tanto de la media \\(\\mu\\) como de la varianza \\(\\sigma^2\\), son conocidas, lo que facilita la construcción de intervalos de confianza y contrastes de hipótesis.Así, dada una distribución poblacional normal y una m..s. de tamaño \\(n\\),Para estimar la varianza poblacional, \\(\\sigma ^2\\), el estimador máximo verosímil es la varianza muestral (13.5), que es sesgado. El estimador insesgado es la cuasivarianza muestral (13.6). Sus distribuciones en el muestreo, en el caso habitual de que la media poblacional sea desconocida, son:\\[\\begin{equation}\n{n S^2 \\\\sigma^2} = {(n-1) S_c^2 \\\\sigma^2} \\sim \\chi^2_{n-1}.\n\\end{equation}\\]Así, el intervalo de confianza nivel \\((1-\\alpha)\\) es:\\[\\begin{equation}\nIC_{(1-\\alpha)} =\\left [ {n s^2 \\{\\chi^2_{n-1,\\alpha /2}}} , {n s^2 \\{\\chi^2_{n-1, 1-\\alpha /2}}}  \\right ],\n\\end{equation}\\]o, equivalentemente, usando la proporcionalidad entre varianza y cuasivarianza muestrales:\\[\\begin{equation}\nIC_{(1-\\alpha)} = \\left [ {(n-1) s_c^2 \\{\\chi^2_{n-1,\\alpha /2}}} , {(n-1) s_c^2 \\{\\chi^2_{n-1, 1-\\alpha /2}}}  \\right ],\n\\end{equation}\\]donde \\(\\chi ^2 _{n-1,\\alpha / 2}\\) representa el cuantil en la distribución.104Para el contraste de \\(H_0 \\equiv \\sigma^2 = \\sigma_0^2\\), la “distancia” es \\({n s^2 \\\\sigma^2_0} = {(n-1) s_c^2 \\\\sigma_0^2}\\), lo que permite calcular los p-valores mediante una distribución \\(\\chi^2_{n-1}\\).105Para el caso de querer estimar la desviación típica, basta con calcular la raiz cuadrada del estimador de la varianza, o si se busca un intervalo de confianza, la raíz de los extremos del intervalo para la varianza. Los contrastes de hipótesis son equivalentes, ya que \\(\\sigma^2=\\sigma_0^2 \\equiv \\sigma = \\sigma_0\\).Para estimar el parámetro \\(\\mu\\) se utiliza el estimador media muestral \\(\\hat{\\mu} = \\bar X\\), en el que coinciden los métodos de mínimos cuadrados, de máxima verosimilitud y de los momentos, siendo insesgado y de varianza mínima.Si la varianza poblacional es conocida, la distribución en el muestreo de la media muestral es:\\[\\begin{equation}\n\\bar X \\sim N \\left ( \\mu , {\\sigma \\{\\sqrt{n}}} \\right ) \\equiv {\\bar X - \\mu \\{\\sigma / \\sqrt n}} \\sim N(0,1).\n\\end{equation}\\]El intervalo de confianza nivel \\((1-\\alpha)\\) es:\\[\\begin{equation}\nIC_{(1-\\alpha)} =\\left [ \\bar x - z_{\\alpha / 2} {\\sigma \\\\sqrt n} , \\bar x + z_{\\alpha / 2} {\\sigma \\\\sqrt n}  \\right ],\n\\end{equation}\\]donde \\(z_{\\alpha/2}\\) representa el cuantil en una distribución normal estándar.Para el contraste de \\(H_0 \\equiv \\mu = \\mu_0\\), la “distancia” es \\(\\bar X - \\mu_0 \\\\sigma / \\sqrt n\\), lo que permite calcular los p-valores, directamente, mediante una distribución \\(N(0,1)\\).Si la varianza poblacional es desconocida, se sustituye por su estimación, por lo que la distribución en el muestreo de la media muestral es:\\[\\begin{equation}\n{\\bar X - \\mu \\{S / \\sqrt {n-1}}} \\equiv {\\bar X - \\mu \\{S_c / \\sqrt n}} \\sim t_{n-1}.\n\\end{equation}\\]El intervalo de confianza nivel \\((1-\\alpha)\\) es:\\[\\begin{equation}\nIC_{(1-\\alpha)} =\\left [ \\bar x - t_{n-1,\\alpha / 2} {s \\\\sqrt {n-1}} , \\bar c + t_{n-1, \\alpha / 2} {s \\\\sqrt {n-1}}  \\right ],\n\\end{equation}\\]o, equivalentemente:\\[\\begin{equation}\nIC_{(1-\\alpha)} = \\left [ \\bar x - t_{n-1,\\alpha / 2} {s_c \\\\sqrt n} , \\bar x + t_{n-1, \\alpha / 2} {s_c \\\\sqrt n}  \\right ],\n\\end{equation}\\]donde \\(t_{n-1,\\alpha/2}\\) representa el cuantil de la distribución t-Student.Para el contraste de \\(H_0 \\equiv \\mu = \\mu_0\\), la “distancia” es \\({\\bar X - \\mu_0 \\S / \\sqrt {n-1}} = {\\bar X - \\mu_0 \\S_c / \\sqrt n}\\), lo que permite calcular los p-valores mediante una distribución \\(t_{n-1}\\).continuación, el interés se centra en la comparación de dos poblaciones normales independientes, X e Y, partir de muestras \\((X_1,...,X_n)\\) y \\((Y_1,...,Y_m)\\):Para la comparación de las varianzas poblacionales (una es mayor que la otra, o al revés; y que se lleva cabo mediante el cociente de las correspondientes varianzas o cuasivarianzas muestrales), se tiene que:\\[\\begin{equation}\n{{{m S_Y^2} \\{(m-1)\\sigma_Y^2}} \\{{n S_X^2} \\{(n-1)\\sigma_X^2}}} \\equiv {S_{cY}^2 / \\sigma_Y^2 \\S_{cX} ^2 / \\sigma_X^2} \\sim F_{m-1,n-1},\n\\end{equation}\\]lo cual permite calcular intervalos de confianza de forma idéntica la expuesta los casos anteriores pero con la distribución \\(F\\). Un caso muy frecuente es querer contrastar si ambas varianzas poblacionales son iguales (el cociente entre ellas es la unidad).Para la comparación de las medias poblacionales (que se lleva cabo mediante la diferencia de las correspondientes medias muestrales), el caso más común es asumir que las varianzas (aunque desconocidas) son iguales, por lo que el estimador es:\\[\\begin{equation}\n{(\\bar X - \\bar Y)-(\\mu_X - \\mu_Y) \\\\sqrt{{nS_X^2+mS_Y^2} \\n+m-2} \\sqrt{{1\\n} + {1 \\m}}} \\sim t_{n+m-2},\n\\end{equation}\\]si se utiliza la varianza muestral como estimador de su homónima poblacional, o:\\[\\begin{equation}\n{(\\bar X - \\bar Y)-(\\mu_X - \\mu_Y) \\\\sqrt{{(n-1)S_{cX}^2+(m-1)S_{cY}^2} \\n+m-2} \\sqrt{{1\\n} + {1 \\m}}} \\sim t_{n+m-2},\n\\end{equation}\\]si se utiliza la cuasivarianza muestral.Al utilizarse distribuciones t-Student, los intervalos de confianza y contrastes de hipótesis son similares los del caso de una única población con las correcciones pertinentes.","code":""},{"path":"Fundainfer.html","id":"ejemplopobnorm","chapter":"Capítulo 13 Inferencia estadística","heading":"13.7 Inferencia sobre poblaciones normales con R","text":"Los datos sobre calidad del aire en la ciudad de Nueva York (airquality) incluyen la variable Wind, que recoge, en mph, la velocidad del viento entre el día 1 de mayo y el 30 de septiembre DE 1973. Los datos de dicha variable se dividen en dos variables \\(X=\\text{Velocidad del viento hasta el 15 de julio}\\) e \\(Y=\\text{Velocidad del viento desde el 16 de julio}\\). Asumiendo que las distribuciones poblacionales son normales, se propone:Obtener una estimación de la velocidad media y de la desviación típica de ambas variables, usando el método de máxima verosimilitud.El resultado muestra las estimaciones de la media y la desviación típica de la velocidad del viento ( junto al “error estandar” o desviación típica de los estimadores respectivos) para las dos variables anteriormente creadas.La orden plot(mle_x) permite visualizar la congruencia entre la muestra y la distribución probabilística basada en las estimaciones realizadas; por ejemplo, optando por el primer período, se obtiene la Fig.13.2, que representa el histograma de los valores muestrales junto la distribución teórica construida con las estimaciones.\nFigura 13.2: Resultados gráficos de la estimación por máxima verosimilitud\nConstruir un intervalo de confianza para la velocidad media del viento hasta el 15 de julio, con un nivel de confianza del 95%.Calcular un intervalo de confianza para la desviación típica de la velocidad del viento desde el 16 de julio, con un nivel de confianza del 90%.¿Se puede considerar que las varianzas poblacionales en ambos períodos son iguales, con un nivel de significación del 1%?El estadístico F-Snedecor de contraste arroja un valor de 1.4062, con un p-valor de 0.1406. Como este p-valor es pequeño (es superior al nivel de significación prefijado), hay suficiente evidencia empírica como para rechazar la hipótesis nula de igualdad de varianzas.Teniendo en cuenta los resultados del apartado anterior, ¿se puede afirmar que la velocidad media del viento en el primer período es mayor que la del segundo, con un nivel de significación del 1%?Un p-valor tan bajo (0.008, inferior al nivel de significación prefijado) indica que existe suficiente evidencia empírica como para rechazar la hipótesis nula de igualdad de medias; en otros términos, la evidencia empírica es suficiente para rechazar, con un nivel de confianza del 99%, que la velocidad media del viento en el primer período es superior la del segundo.","code":"\nlibrary('Rlab')\nlibrary('fitdistrplus')\nx <- airquality$Wind[1:76]\ny <- airquality$Wind[77:153] # Se particiona la muestra en los dos períodos\nmle_x <- fitdist(x, distr = \"norm\", method = \"mle\")\nmle_y <- fitdist(y, distr = \"norm\", method = \"mle\")\nmle_x\n#> Fitting of the distribution ' norm ' by maximum likelihood \n#> Parameters:\n#>       estimate Std. Error\n#> mean 10.640789  0.4274723\n#> sd    3.726618  0.3022685\nmle_y\n#> Fitting of the distribution ' norm ' by maximum likelihood \n#> Parameters:\n#>      estimate Std. Error\n#> mean 9.283117  0.3581657\n#> sd   3.142891  0.2532613\nplot(mle_x)\nlibrary('DescTools')\nMeanCI(x, conf.level = 0.95)\n#>      mean    lwr.ci    upr.ci \n#> 10.640789  9.783563 11.498016\nsqrt(VarCI(y, conf.level = 0.9))\n#>      var   lwr.ci   upr.ci \n#> 3.163501 2.795147 3.655468\nvar.test(x, y, conf.level = 0.99, alternative = \"two.sided\")\n#> \n#>  F test to compare two variances\n#> \n#> data:  x and y\n#> F = 1.4062, num df = 75, denom df = 76, p-value = 0.1406\n#> alternative hypothesis: true ratio of variances is not equal to 1\n#> 99 percent confidence interval:\n#>  0.7727136 2.5616496\n#> sample estimates:\n#> ratio of variances \n#>           1.406197\nt.test(x, y, conf.level = 0.99, alternative = \"greater\", var.equal = TRUE)\n#> \n#>  Two Sample t-test\n#> \n#> data:  x and y\n#> t = 2.4212, df = 151, p-value = 0.008328\n#> alternative hypothesis: true difference in means is greater than 0\n#> 99 percent confidence interval:\n#>  0.03918338        Inf\n#> sample estimates:\n#> mean of x mean of y \n#> 10.640789  9.283117"},{"path":"Fundainfer.html","id":"contrnormalidad","chapter":"Capítulo 13 Inferencia estadística","heading":"13.8 Inferencia estadística no paramétrica: contrastes de normalidad","text":"Hasta ahora, se ha supuesto que la distribución muestral del estimador era “funcionalmente” conocida, aunque dependiente de un parámetro (o varios). Sin embargo, hay situaciones donde se conoce cómo se distribuyen los datos, debiendo decidir qué distribución los ha generado. Es lo que se conoce como inferencia estadística paramétrica. En este capítulo se aborda un planteamiento sistemático de esta rama, sino que se presenta la situación más habitual en la práctica, que es decidir si se puede mantener que una muestra proviene de una distribución normal, supuesto básico en muchas técnicas estadísticas.Posiblemente el test más potente para contrastar la normalidad sea la prueba de Shapiro-Wilks, que asume como hipótesis nula que los datos están generados por una distribución normal. Un rechazo de esta hipótesis (p-valor muy bajo) debería hacer reflexionar sobre la adecuación de muchas técnicas y la interpretación de los resultados. En R, la función shapiro.test() proporciona dicho contraste de normalidad.Una alternativa es el uso del test de Kolmogorov-Smirnov, diseñado para comparar las distribuciones de dos muestras, fijando que una de ellas sea la distribución normal (este test puede ser igualmente utilizado para cualquier otra distribución usual). La función ks.test() permite en R obtener los resultados de este contraste.Para ilustrar el uso del test de Shapiro-Wilk en R se recurre de nuevo los datos sobre calidad del aire en la ciudad de Nueva York del ejemplo anterior (Sec. 13.7) y se contrasta si se puede asumir que las variables Temp y Wind están generadas por distribuciones normales:Para la variable Temp el p-valor (0,0093) es muy bajo en comparación con los niveles de significación habituales (0,01, 0,05), por lo que hay suficiente evidencia empírica como para rechazar que dicha variable tenga una distribución normal. Por el contrario, en el caso de la variable Wind, el p-valor (0.1178) es pequeño, por lo que hay suficiente evidencia como para rechazar que esté generada por una distribución normal. La Fig. 13.3 muestra la comparación entre los cuantiles empíricos de ambas variables y los teóricos de una distribución normal.\nFigura 13.3: Q-Q Plots normales para las variables Temp (izq) y Wind (der)\n","code":"\nshapiro.test(airquality$Temp)\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  airquality$Temp\n#> W = 0.97617, p-value = 0.009319\nshapiro.test(airquality$Wind)\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  airquality$Wind\n#> W = 0.98575, p-value = 0.1178\npar(mfrow = c(1, 2))\nqqnorm(airquality$Temp, main = \"Normal Q-Q Plot for Temp\")\nqqnorm(airquality$Wind, main = \"Normal Q-Q Plot for Wind\")"},{"path":"Fundainfer.html","id":"resumen-12","chapter":"Capítulo 13 Inferencia estadística","heading":"Resumen","text":"La inferencia estadística permite estimar la distribución poblacional de una variable partir de la información suministrada por una muestra. Se abordan los métodos de estimación puntual de los principales parámetros poblacionales y la construcción de intervalos de confianza para ellos, así se implementan y resuelven una serie de contrastes de significación sobre diversas hipótesis.Para el caso de poblaciones normales, se desarrollan las expresiones operativas de los métodos anteriores. Igualmente, en el ámbito de la inferencia paramétrica, se presenta un contraste de normalidad que permite decidir cuándo el supuesto de normalidad es adecuado o .","code":""},{"path":"muestreo.html","id":"muestreo","chapter":"Capítulo 14 Muestreo y remuestreo","heading":"Capítulo 14 Muestreo y remuestreo","text":"Mª Leticia Meseguer Santamaría\\(^{}\\) y Manuel Vargas Vargas\\(^{}\\)\\(^{}\\) Universidad de Castilla-La Mancha","code":""},{"path":"muestreo.html","id":"introducción-al-muestreo","chapter":"Capítulo 14 Muestreo y remuestreo","heading":"14.1 Introducción al muestreo","text":"Muchas investigaciones científicas abordan el estudio de características de un conjunto de elementos, que se denomina “población”. Sin embargo, siempre es posible estudiar la totalidad del colectivo, por problemas de accesibilidad, confidencialidad, costes económicos o temporales, etc. En estos casos, se recurre extraer un subconjunto de la población que se pretende que sea representativo de ésta respecto las características estudiadas. El objetivo es obtener información relevante que pueda extrapolarse al total de la población, proceso denominado genéricamente “muestreo”.En otros casos, la conveniencia de muestrear una población está relacionada con la credibilidad de los resultados obtenidos o de las propiedades de los modelos utilizados. Así, muchas técnicas cuantitativas dividen la información (muestra) disponible en un subconjunto de “entrenamiento” o “estimación” y otro de “contraste” o “validación”; una elección inadecuada puede alterar los resultados e invalidar las conclusiones. Por último, caso muy común en ciencias sociales, se puede acceder la medición directa de los fenómenos (por ser una población muy grande o un fenómeno subjetivo), por lo que se accede ella mediante encuestas, que precisan de una metodología de muestreo rigurosa y diseñada previamente.Para fijar terminología, se define población como el conjunto de casos de interés los que se quiere generalizar los resultados de la investigación, denominados genéricamente individuos; veces, sólo es posible acceder una parte de la población, por lo que se utiliza el término población objetivo para el conjunto total y población muestreable al conjunto al que se tiene acceso. Por coherencia, cuando ambos colectivos coincidan, los resultados deben extrapolarse sólo al último de ellos.Las características de interés de la población se consideran “variables”, en el sentido estadístico del término, por lo que son estudiadas mediante su distribución. En algunos casos, ésta será completamente desconocida, siendo de interés su determinación completa; en otros muchos, se buscará determinar sólo algunos aspectos (medidas de posición, dispersión, etc.) o los parámetros que rigen una distribución funcionalmente conocida. En todo caso, el objetivo del estudio es determinar la distribución estadística de estas variables, conocida como “distribución poblacional”; veces, por simplicidad en el lenguaje, esta distribución o, incluso, las variables, se les denomina población (por ejemplo, es infrecuente encontrar enunciados del tipo “la población sigue una distribución binomial”, identificando población con variable, o “la población es una distribución normal”, identificando población con distribución).Se define muestra como un subconjunto de la población, que será utilizado para caracterizar la distribución poblacional y unidad muestral cada individuo de la muestra. Si la muestra tiene la misma distribución que la población, se dice que es representativa, mientras que en caso contrario se denomina sesgada.Siempre será preferible un método de muestreo que proporcione muestras representativas, característica ligada la forma de seleccionar la muestra y al tamaño de ésta. Así, siempre que sea posible, se recomienda utilizar un muestreo probabilístico, basado en la selección aleatoria de la muestra (conociéndose, por tanto, la probabilidad de que cada individuo salga seleccionado), lo que permite extender los resultados toda la población, cuantificando posibles sesgos y detallando un error máximo dentro de un nivel de confianza seleccionado al inicio del proceso.siempre será posible utilizar un muestreo probabilístico, por lo que existe una colección de métodos de muestreo probabilístico (conveniencia, bola de nieve, cuotas, etc.), que se detallarán en este capítulo. La característica común todos ellos es que los resultados obtenidos de la muestra se deben extrapolar la población, ya que está garantizada la representatividad.En el resto del capítulo se presentarán brevemente los métodos de muestreo probabilístico más usuales. Para mayor detalle, se pueden consultar las referencias Chaudhuri Stenger (2005), Arnab (2017) o C. Wu Thompson (2020).","code":""},{"path":"muestreo.html","id":"MAS","chapter":"Capítulo 14 Muestreo y remuestreo","heading":"14.2 Muestreo aleatorio simple","text":"El método básico de muestreo es el conocido como muestreo aleatorio simple (m..s.), ya introducido en la Sec. 13.2, consistente en seleccionar totalmente al azar los individuos de la muestra, por lo que todos tienen la misma probabilidad de formar parte de ella. Si cada individuo sólo puede aparecer una vez en la muestra, se habla de muestreo aleatorio sin reemplazamiento o muestreo aleatorio irrestricto, mientras que, en caso contrario, se denomina muestreo con reemplazamiento o, en general, muestreo aleatorio simple.106Este procedimiento es el que se asume en la inmensa mayoría de las técnicas estadísticas convencionales, pero presenta dos inconvenientes. En primer lugar, presupone que existe un registro, o listado, completo de todos los individuos de la población (lo que siempre es posible), y puede resultar costosa (en medios, tiempo y dinero) su aplicación práctica. En segundo lugar, presupone que la característica estudiada es homogénea en todos los individuos de la población, es decir, la distribución poblacional es idéntica en todos los individuos. Frecuentemente, esta homogeneidad poblacional se cumple, por lo que sería necesario abordar otros métodos de muestreo que se expondrán más adelante. En todo caso, si se utiliza un m..s., la heterogeneidad induce un aumento de la variabilidad muestral, hecho que debe ser tenido en cuenta en la interpretación de resultados.Además de la forma de selección, el factor que determina la representatividad de una muestra es su tamaño (véase el teorema de Glivenko-Cantelli (13.3)). Al utilizar la información muestral para aproximar los aspectos o parámetros desconocidos en la población se comete el llamado error muestral, que representa el margen de error que se está dispuesto aceptar (por tanto, está íntimamente relacionado con los intervalos de confianza)107; si el tamaño de la muestra está determinado, el margen de error muestral marca el grado de precisión con el que se pueden extrapolar los resultados. Una alternativa es predeterminar un error muestral cierto nivel de confianza y calcular cuál es el menor tamaño muestral que cumple ese requisito.El margen de error (o simplemente error muestral), \\(\\epsilon\\), depende del aspecto o parámetro poblacional que se quiera conocer (frecuentemente, la media, el total poblacional o la proporción), del estimador utilizado y del nivel de confianza. Si se asume una distribución poblacional normal, la expresión general sería:\\[\\begin{equation}\n\\tag{14.1}\n\\epsilon_\\alpha = z_{1-\\alpha/2}\\sigma(\\hat \\theta),\n\\end{equation}\\]expresión que es frecuentemente extrapolada distribuciones poblacionales normales.Para el caso de estimar la media poblacional utilizando la media muestral, sustituyendo en la ecuación anterior (14.1), la relación entre error muestral, nivel de confianza y tamaño muestral sería:\\[\\begin{equation}\n\\tag{14.2}\n\\epsilon_\\alpha = z_{1-\\alpha/2}\\sqrt{(1-{n\\N}) {s^2 \\n}}.\n\\end{equation}\\]Operando y despejando el tamaño muestral, se obtiene la expresión:\\[\\begin{equation}\n\\tag{14.3}\nn = {z^2_{1-\\alpha/2} Ns^2 \\N\\epsilon^2_\\alpha + z^2_{1-\\alpha/2}s^2}.\n\\end{equation}\\]En algunos casos se considera que se está muestreando una población de tamaño infinito, lo que produce una simplificación de la fórmula de obtención del tamaño muestral:\\[\\begin{equation}\n\\tag{14.4}\n\\epsilon_\\alpha = z_{1-\\alpha/2}\\sqrt{s^2 \\n} \\Longrightarrow n = {z^2_{1-\\alpha/2} s^2 \\\\epsilon^2_\\alpha}.\n\\end{equation}\\]Si se está interesado en estimar el total poblacional, un procedimiento análogo conduce las ecuaciones:\\[\\begin{equation}\n\\tag{14.5}\n\\epsilon_\\alpha = z_{1-\\alpha/2}\\sqrt{N^2(1-{n\\N}) {s^2 \\n}} \\Longrightarrow n = {z^2_{1-\\alpha/2} N^2s^2 \\\\epsilon^2_\\alpha + z^2_{1-\\alpha/2}Ns^2}.\n\\end{equation}\\]Por último, si se desea estimar la proporción poblacional, \\(P\\), de individuos que cumplen algún criterio, se puede particularizar el caso del estimador de la media poblacional sobre una población binomial, por lo que el resultado obtenido es:\\[\\begin{equation}\n\\tag{14.6}\nn = {z^2_{1-\\alpha/2} Npq \\(N-1)\\epsilon^2_\\alpha + z^2_{1-\\alpha/2}pq},\n\\end{equation}\\]siendo \\(q=1-p\\).En la práctica es muy frecuente que se desconozca la varianza poblacional, por lo que se suele recurrir alguna estimación previa, con lo que se tiene una aproximación al tamaño muestral requerido.","code":""},{"path":"muestreo.html","id":"ejemMAS","chapter":"Capítulo 14 Muestreo y remuestreo","heading":"14.2.1 Ejemplo de m.a.s.","text":"Para ejemplificar el proceso de obtención de una muestra aleatoria simple en R, se usará el paquete samplingbook y el conjunto de datos iris, correspondiente las medidas, en centímetros, de largo y ancho de los sépalos y pétalos de 150 flores, equidistribuidas entre las especias setosa, versicolor y virginica.En este caso, por simplicidad, se considerará que la población es el conjunto de las 150 flores disponibles, y que se desea una muestra aleatoria simple con reemplazamiento para determinar la longitud media de los sépalos con un error de 0.3 centímetros al 95% de confianza. La función sample.size.mean() permite calcular el tamaño de muestra necesario para cumplir estos requisitos.108Así, basta con una muestra aleatoria simple de tamaño 25 para poder estimar la longitud media de los sépalos con los requisitos dados. Para obtener la muestra concreta, la función sample proporciona los valores obtenidos (conjunto de 25 valores aleatorios entre 1 y N=150) y permite seleccionar los casos que conforma la muestra:Para finalizar, usando la función Smean() se puede obtener la estimación de la media poblacional, así como su error estándar y un intervalo de confianza. Aunque en la práctica el valor poblacional es desconocido, en este ejemplo sí se puede obtener partir del conjunto de todos los datos, lo que permite comparar la estimación con el verdadero valor buscado.En este ejemplo, el error cometido sería de \\(5.976-5.843=0.133\\) cm.Si interesa estimar el total poblacional, basta con multiplicar la estimación de la media por el tamaño poblacional, \\(N\\). Por último, si se desea estimar una proporción poblacional, el proceso sería idéntico al descrito, pero usando la función Sprop().","code":"\nlibrary('samplingbook')\ndatos_ej<-data.frame(iris)\nsd <- sd(datos_ej$Sepal.Length) # Se considera como la desviación típica poblacional\nN <- nrow(datos_ej) # Tamaño de la población\ne <- 0.3 # Margen de error prefijado\nsample.size.mean(e, sd, N, level = 0.95)\n#> \n#> sample.size.mean object: Sample size for mean estimate\n#> With finite population correction: N=150, precision e=0.3 and standard deviation S=0.8281\n#> \n#> Sample size needed: 25\nset.seed(196) # Fija la semilla de aleatorización para poder reproducir los resultados\nmuestra <- sample(1:N, 25, replace = TRUE) # Si se quisiera un muestreo sin reemplazo, se utilizaría la sentencia replace=FALSE\ndatos_muestra <- datos_ej[muestra, ] # Se seleccionan los datos que conforman la muestra\nhead(datos_muestra)\n#>      Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n#> 122           5.6         2.8          4.9         2.0  virginica\n#> 133           6.4         2.8          5.6         2.2  virginica\n#> 104           6.3         2.9          5.6         1.8  virginica\n#> 95            5.6         2.7          4.2         1.3 versicolor\n#> 95.1          5.6         2.7          4.2         1.3 versicolor\n#> 73            6.3         2.5          4.9         1.5 versicolor\nSmean(datos_muestra$Sepal.Length, N, level = 0.95)\n#> \n#> Smean object: Sample mean estimate\n#> With finite population correction: N=150\n#> \n#> Mean estimate: 5.976\n#> Standard error: 0.1115\n#> 95% confidence interval: [5.7575,6.1945]\nmean(datos_ej$Sepal.Length) # Valor de la media poblacional\n#> [1] 5.843333"},{"path":"muestreo.html","id":"muestestra","chapter":"Capítulo 14 Muestreo y remuestreo","heading":"14.3 Muestreo estratificado","text":"Una consecuencia del muestreo aleatorio simple es que “reproduce” las características de la población, entre otros aspectos, su variabilidad, que se asumen comunes todos los individuos. Sin embargo, es frecuente que haya “grupos” de individuos que presenten diferencias en los parámetros que rigen la distribución poblacional (por ejemplo, en sus medias o en sus varianzas) de la característica que se quiere estudiar.En el ejemplo anterior (14.2.1), se ha muestreado para estimar la longitud media de los sépalos de las 150 flores recogidas en los datos. Sin embargo, al calcular la media y la desviación típica agrupando según la especie:puede observarse que las tres especies se comportan igual respecto al parámetro de interés (longitud media de los sépalos). La especie setosa presenta unos valores menores de longitud media y desviación típica, mientras que la especie virginica presenta los valores más elevados.Si se considerara la especie, un muestreo aleatorio simple podría sesgar los resultados, por ejemplo, con un predominio de las setosas o de las virginicas. En todo caso, la variabilidad del conjunto de datos es más elevada que en cualquiera de las especies, pues la variación dentro de cada especie se une la variación entre especies. Este hecho hace aumentar el tamaño muestral necesario para estimar con un margen de erro prefijado.En general, cuando existen grupos de individuos con un comportamiento más homogéneo dentro del grupo y diferenciado entre grupos, resulta apropiado aplicar un m..s. En estos casos, es recomendable el denominado muestreo estratificado, donde se realiza previamente una partición de la población en estratos y se selecciona una m..s. dentro de cada grupo.La estratificación presenta ventajas, como el aumento de la representatividad de la muestra (se necesita un menor tamaño muestral total que en el m..s.), la reducción del error muestral (la variabilidad es menor en cada estrato) y el incremento de probabilidad de representación en la muestra de grupos con características diferenciadas. Por el contrario, siempre resulta evidente la relación entre la variable de estratificación y las de interés.Una de las decisiones que se han de tomar en el muestreo estratificado es el reparto de tamaño muestral entre los distintos estratos, procedimiento conocido como afijación. Las dos opciones más utilizadas son la afijación proporcional, que reparte el tamaño muestral en función de los tamaños poblacionales de cada estrato, y la afijación óptima, que considera también los diferentes valores de la variabilidad dentro de cada estrato.Para ejemplificar el proceso de muestreo estratificado en el caso de la base de datos utilizada, se procede considerar cada especie de iris como un estrato, ya que se ha comprobado que presentan distintas distribuciones poblacionales respecto la variable longitud del sépalo. Como en la Sec. 14.2.1, se quiere estimar la longitud media de la variable con un margen de error de 0.3 al 95% de confianza. Dentro del paquete samplingbook se puede utilizar la función stratasize() para determinar el tamaño muestral que cumple estos requisitosComo se aprecia, para garantizar al 95% de confianza un margen de error de 0.3 cm es necesario un tamaño muestral de 11, sensiblemente inferior al requerido con un m..s. (25). Una vez determinado el tamaño, el criterio de afijación elegido distribuye la muestra entre los estratos.Como los tres estratos tienen el mismo tamaño poblacional, la afijación proporcional distribuye la muestra equitativamente; sin embargo, la afijación óptima, al considerar las diferencias en variabilidad, asigna más muestra al estrato con mayor variabilidad y menos muestra al de menor variabilidad (como los tamaños muestrales son necesariamente números enteros, se puede producir una ligera diferencia entre el tamaño muestral calculado globalmente y la suma de los tamaños de cada estrato).Con la afijación óptima estimada, se procede la selección de la submuestra en cada estrato (m..s.) y la obtención de los datos que conforman la muestra.Finalmente, usando la función Smean() se obtiene la estimación de la media poblacional, así como su error estándar y un intervalo de confianza, al igual que se hizo con el m..s.","code":"\nlibrary('plyr')\nestratos <- ddply(datos_ej, .(Species), summarize, media.sl = mean(Sepal.Length), \n                  desv.sl = sd(Sepal.Length))\nestratos\n#>      Species media.sl   desv.sl\n#> 1     setosa    5.006 0.3524897\n#> 2 versicolor    5.936 0.5161711\n#> 3  virginica    6.588 0.6358796\nstratasize(e, Nh = c(50, 50, 50), Sh = estratos[, 3], level = 0.95)\n#> \n#> stratamean object: Stratified sample size determination\n#> \n#> type of sample: prop\n#> \n#> total sample size determinated: 11\nstratasamp(n = 11, Nh = c(50, 50, 50), Sh = estratos[, 3], type = \"prop\")\n#>              \n#> Stratum 1 2 3\n#> Size    4 4 4\nstratasamp(n = 11, Nh = c(50, 50, 50), Sh = estratos[, 3], type = \"opt\")\n#>              \n#> Stratum 1 2 3\n#> Size    3 4 5\nset.seed(195) # Fija la semilla de aleatorización\nmuestra1 <- sample(1:50, 3, replace = TRUE)\nmuestra2 <- sample(51:100, 4, replace = TRUE)\nmuestra3 <- sample(101:150, 5, replace = TRUE) # m.a.s. en cada estrato\nmuestra_estr <- c(muestra1, muestra2, muestra3)\ndatos_muestra_estr <- datos_ej[muestra_estr, ] # Selección de los datos que conforman la muestra\ndatos_muestra_estr\n#>      Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n#> 26            5.0         3.0          1.6         0.2     setosa\n#> 38            4.9         3.6          1.4         0.1     setosa\n#> 5             5.0         3.6          1.4         0.2     setosa\n#> 61            5.0         2.0          3.5         1.0 versicolor\n#> 61.1          5.0         2.0          3.5         1.0 versicolor\n#> 65            5.6         2.9          3.6         1.3 versicolor\n#> 58            4.9         2.4          3.3         1.0 versicolor\n#> 147           6.3         2.5          5.0         1.9  virginica\n#> 146           6.7         3.0          5.2         2.3  virginica\n#> 134           6.3         2.8          5.1         1.5  virginica\n#> 130           7.2         3.0          5.8         1.6  virginica\n#> 141           6.7         3.1          5.6         2.4  virginica\nSmean(datos_muestra_estr$Sepal.Length, N, level = 0.95)\n#> \n#> Smean object: Sample mean estimate\n#> With finite population correction: N=150\n#> \n#> Mean estimate: 5.7167\n#> Standard error: 0.2393\n#> 95% confidence interval: [5.2476,6.1857]"},{"path":"muestreo.html","id":"otrosmuestreos","chapter":"Capítulo 14 Muestreo y remuestreo","heading":"14.4 Otros tipos de muestreo probabilístico","text":"Existen otros métodos de muestreo probabilístico que buscan simplificar la extracción de una muestra representativa, entre los que destacan el muestreo por conglomerados y el muestreo sistemático.Cuando la población es muy grande, es frecuente que se puedan establecer (o construir partir de alguna variable) subgrupos, o clusters, que tengan las mismas características que todo el conjunto respecto la variable de interés. En esos casos, efectos de estimación, sería equivalente muestrear toda la población o sólo un cluster, con el consiguiente ahorro de tamaño muestral, tiempo y coste. Es el conocido como muestreo por conglomerados.Por ejemplo, se puede estar interesado en estimar el tiempo medio que el alumnado de E.S.O. dedica estudiar matemáticas en España. Obtener una muestra para todo el pais puede ser costoso en tiempo, recursos materiales y tamaño muestral; sin embargo, se puede asumir que existen diferencias entre provincias respecto esta variable, por lo que sería posible muestrear sólo en una provincia (o pocas). En este caso, el muestreo por conglomerados consistiría en una primera etapa de selección aleatoria de clusters (provincia/s en este ejemplo) y, posteriormente, aplicar un método de muestreo sobre dicha selección (que, su vez, podría ser un m..s. o un muestreo estratificado).conviene confundir los conceptos de estrato y cluster, aunque ambos sean subgrupos de la población total. En el primer caso, los individuos de cada estrato son muy homogéneos entre sí y diferenciados del resto de estratos. En el segundo caso, los individuos de cada cluster tienen la misma variabilidad que el conjunto de la población, habiendo diferencias entre clusters respecto la variable de interés. Así, la ganancia en el muestreo estratificado proviene de trabajar con menores variabilidades intra-estratos, mientras que en el muestreo por conglomerados proviene de utilizar una subpoblación más pequeña.En otras situaciones, si se dispone de un marco poblacional (listado completo de los individuos), es posible plantear un mecanismo sencillo de obtención de la muestra. Si se tiene un tamaño poblacional N y se quiere una muestra de tamaño n, se pueden establecer \\(k=N/n\\) bloques, elegir al azar un número entre 1 y \\(k\\) (que permite seleccionar el primer elemento de la muestra) y, partir de esa posición, dar saltos de magnitud \\(k\\) en el listado para seleccionar el resto de unidades muestrales. Es el método conocido como muestreo sistemático o, más técnicamente, muestreo sistemático uniforme de paso \\(k\\).Como ejemplo, supóngase que se quiere obtener una muestra de 50 individuos en una población de 2000. El paso sería \\(k=2000/50=40\\) unidades y se selecciona aleatoriamente una unidad entre las 40 primeras (supóngase que la número 13); el resto de la muestra se obtendría sumándole el paso la primera seleccionada (13, 53, 93, 133, 173, y así hasta la 1973).Por último, los métodos expuestos son incompatibles, sino que se pueden combinar por etapas, dando lugar los conocidos como muestreos polietápicos. Por ejemplo, la Encuesta de Población Activa, elaborada por el Instituto Nacional de Estadística, adopta un muestreo bietápico, en primer lugar, estratificado entre secciones censales y, en segundo lugar, muestreando entre las viviendas familiares de cada sección.","code":""},{"path":"muestreo.html","id":"bootstrap","chapter":"Capítulo 14 Muestreo y remuestreo","heading":"14.5 Técnicas de remuestreo: Bootstrap","text":"Cuando se infiere una característica poblacional partir de una muestra, sólo se dispone del valor concreto que el estadístico toma sobre dicha muestra. Salvo en raras ocasiones, se dispone de su distribución en el muestreo, o sólo se tiene una aproximación asintótica, por lo que se pueden evaluar sus propiedades estadísticas con tamaños muestrales elevados. En otros casos, es la complejidad analítica de muchas técnicas actuales de análisis de datos la que dificulta la determinación de la distribución de las estimaciones de los parámetros.En estos casos, el método bootstrap propone sustituir la distribución poblacional (desconocida) por una estimación conocida (como puede ser la distribución empírica o una aproximación paramétrica) que, mediante remuestreo, sirva para generar muestras aleatorias partir de la muestra original. Se obtiene así una distribución de remuestreo, llamada también distribución bootstrap, cuyo comportamiento sobre la estimación aproxima la de la distribución muestral en torno al parámetro, lo que permite evaluar la precisión de las estimaciones.El método bootstrap más sencillo, llamado bootstrap uniforme o bootstrap naïve, parte de la aproximación de la distribución poblacional por la distribución empírica de la muestra. Supóngase que se tiene una muestra \\(X=(x_1,...,x_n)\\) que es utilizada para obtener un estimador \\(T(X)=\\hat{\\theta}\\) para un parámetro poblacional \\(\\theta\\). Utilizando su distribución empírica (véase ecuación (13.2)):Se genera una primera muestra \\(X^{*1}=(x_1^{*1},...,x_n^{*1})\\), obtenida mediante muestreo aleatorio simple con reemplazamiento de la muestra original, que permite evaluar el estadístico \\(T^{*1}(X^{*1})=\\hat{\\theta^{*1}}\\).Se genera una primera muestra \\(X^{*1}=(x_1^{*1},...,x_n^{*1})\\), obtenida mediante muestreo aleatorio simple con reemplazamiento de la muestra original, que permite evaluar el estadístico \\(T^{*1}(X^{*1})=\\hat{\\theta^{*1}}\\).Siguiendo el mismo procedimiento, se puede generar un número elevado (\\(B\\)) de muestras bootstrap, \\(X^{*1}, ..., X^{*B}\\), que permiten obtener el valor del estadístico sobre cada una de ellas \\(T^{*1}(X^{*1}), ..., T^{*B}(X^{*B})\\)Siguiendo el mismo procedimiento, se puede generar un número elevado (\\(B\\)) de muestras bootstrap, \\(X^{*1}, ..., X^{*B}\\), que permiten obtener el valor del estadístico sobre cada una de ellas \\(T^{*1}(X^{*1}), ..., T^{*B}(X^{*B})\\)Con estos valores, se obtiene la distribución bootstrap. Por ejemplo, se puede utilizar esta distribución para calcular la media bootstrap del estadístico \\(T(X)\\), \\(\\bar {T}^*= {1 \\B} \\sum_{b=1}^B T^*(X^{*b})\\), y cuyo error estándar es:Con estos valores, se obtiene la distribución bootstrap. Por ejemplo, se puede utilizar esta distribución para calcular la media bootstrap del estadístico \\(T(X)\\), \\(\\bar {T}^*= {1 \\B} \\sum_{b=1}^B T^*(X^{*b})\\), y cuyo error estándar es:\\[\\begin{equation}\n\\tag{14.7}\n\\hat{S}_{boot}= \\sqrt{{1 \\{B-1}} \\sum_{b=1}^B \\left ( T(X^{*b})-\\bar{T}^* \\right ) ^2},\n\\end{equation}\\]expresión que aproxima al error del estadístico \\(T(X)\\) para estimar \\(\\theta\\).Ejemplo: los datos sobre calidad del aire en la ciudad de Nueva York (airquality) recogen la variable Temp que mide la temperatura, en grados Fahreheit, entre el día 1 de mayo y el 30 de septiembre. Se ha visto en la sección 13.7, que se puede asumir que dicha variable esté generada por una distribución normal, por lo que se podrían utilizar los intervalos de confianza mostrados en la Sec. 13.6.\nEl objetivo es calcular una estimación de la temperatura media y dar un intervalo al 95% de confianza.En este ejemplo, se utiliza la muestra para obtener un valor del estadístico media muestral (\\(\\bar X = 77.88\\)), estimador insesgado de la media poblacional. Sin embargo, al conocer la distribución en el muestreo (se asume ningún tipo de distribución poblacional ni se puede hacer uso de aproximaciones asintóticas), se podría construir un intervalo de confianza.Aplicando el método bootstrap (en su versión uniforme), se van obtener 5000 muestras de tamaño 20, mediante remuestreo con reemplazamiento,Para construir los intervalos de confianza, se calculan los valores críticos al 95% de confianza sobre la distribución empírica de los valores medios remuestreados:Así, con el método bootstrap, es necesario asumir ninguna distribución en el muestreo. Aún así, la representación gráfica de la distribución empírica de los valores medios remuestreados y los extremos del intervalo de confianza (Fig. 14.1), muestra cómo la distribución de las remuestras sobre el estadístico se asemeja la distribución del estadístico sobre el parámetro.\nFigura 14.1: Distribución empírica de la media remuestreada\nEl paquete boot de R permite también obtener réplicas de un estadístico sobre una muestra. La función básica de este paquete es boot(), que permite utilizar distintos métodos de remuestreo. En su estructura más simple, basta con indicar los datos originales, el estadístico que se quiere remuestrear y el número de réplicas.Así, si se quiere estimar, por ejemplo, la mediana de la población con 1000 remuestras, se puede recurrir la función boot():Como resultado, se obtiene el valor del estadístico sobre la muestra original, el sesgo estimado y el error estándar.\nPara calcular los intervalos de confianza, basta con utilizar la función boot.ci() sobre la muestra bootstrap obtenida, indicando el nivel de confianza y el tipo de intervalo (por defecto, proporciona todos los intervalos disponibles; en el ejemplo, se usa el método de los percentiles)La función boot() permite modificaciones del bootstrap uniforme mediante parámetros adicionales. Por ejemplo, el parámetro strata se utiliza para generar remuestreos estratificados cuando la muestra original también lo es. Aunque se han comentado dado el carácter introductorio de este capítulo, existen otros métodos bootstrap, que se pueden obtener especificándolos mediante el parámetro sim. Por defecto, el valor es ordinary, que corresponde al bootstrap uniforme; otras alternativas pueden ser parametric para bootstrap paramétrico, balanced, permutation o antithetic para otros métodos más avanzados.ResumenEl muestreo probabilístico busca seleccionar una muestra representativa de una población, que permita inferir la distribución poblacional o alguno de sus parámetros. Las decisiones básicas para un correcto proceso de muestreo son el método utilizado (aleatorio simple, estratificado, polietápico, etc.), que depende de la estructura de la población, y la determinación del tamaño muestral que garantice el margen de error asumible.\nLa técnica bootstrap de remuestreo permite aproximar la distribución de estadísticos muestrales sin asumir ninguna hipótesis sobre la distribución poblacional, ventaja muy útil para evaluar la precisión de los estimadores en muchísimas técnicas complejas.","code":"\nset.seed(196) #Se fija la semilla para permitir la reproducibilidad\nB<-5000 #Se fija el número de remuestras\nmuestras_boot<-numeric(B) #Se almacenan todos los valores del estadístico\nfor (k in 1:B) {\n    remuestra <- sample(airquality$Temp, 20, replace = TRUE)\n    muestras_boot[k] <- mean(remuestra)\n}\nmedia_boot<-mean(muestras_boot)\nmedia_boot #Media de los 5000 valores medios de las remuestras\n#> [1] 77.87309\ndesv_boot<-sd(muestras_boot)\ndesv_boot #Desviación típica de los 5000 valores medios de las remuestras\n#> [1] 2.090255\nval_crit<-quantile(muestras_boot,c(0.025,0.975))\nval_crit #Valores críticos\n#>  2.5% 97.5% \n#> 73.75 81.95\nhist(muestras_boot, freq=FALSE)\nlines(density(muestras_boot))\nabline(v=val_crit)\nlibrary(boot)\nestadistico<-function(data,i){\n  median(data[i]) #Se especifica aquí el estadístico remuestreado\n}\nset.seed(196) \nmediana_boot<-boot(airquality$Temp,estadistico,R=1000)\nmediana_boot\n#> \n#> ORDINARY NONPARAMETRIC BOOTSTRAP\n#> \n#> \n#> Call:\n#> boot(data = airquality$Temp, statistic = estadistico, R = 1000)\n#> \n#> \n#> Bootstrap Statistics :\n#>     original  bias    std. error\n#> t1*       79  -0.052    1.071655\nboot.ci(mediana_boot,conf=0.95,type=\"perc\")\n#> BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\n#> Based on 1000 bootstrap replicates\n#> \n#> CALL : \n#> boot.ci(boot.out = mediana_boot, conf = 0.95, type = \"perc\")\n#> \n#> Intervals : \n#> Level     Percentile     \n#> 95%   (77, 81 )  \n#> Calculations and Intervals on Original Scale"},{"path":"cap-lm.html","id":"cap-lm","chapter":"Capítulo 15 Modelización lineal","heading":"Capítulo 15 Modelización lineal","text":"Víctor Casero-Alonso\\(^{}\\) y\nMaría Durbán\\(^{b}\\)\\(^{}\\)Universidad de Castilla-La Mancha\\(^{b}\\)Universidad Carlos III de Madrid","code":""},{"path":"cap-lm.html","id":"modelización","chapter":"Capítulo 15 Modelización lineal","heading":"15.1 Modelización","text":"Se acude los modelos de regresión para intentar explicar la relación entre dos o más variables. Para ello se predefine un modelo que pretende explicar el comportamiento de la variable respuesta o dependiente, denotada por \\(Y\\), utilizando la información proporcionada por las variables explicativas, también llamadas independientes o predictoras, denotadas por \\(X_1,\\ldots,X_p\\).\nPero dichas variables pueden ser de distinto tipo. Si la variable respuesta es continua, más concretamente, si se puede asumir que sigue una distribución de probabilidad Normal, y al menos una de las variables explicativas es también continua, se puede acudir la modelización lineal que se desarrolla en este capítulo. Sin embargo, si la variable respuesta fuese de otro tipo, por ejemplo, dicotómica, la modelización lineal sería adecuada. En el Cap. 16, en el que se aborda el modelo lineal generalizado, quedará más clara esta distinción.El primer paso en el proceso de modelización es intentar explicar una variable respuesta, que de aquí en adelante se supone continua y con distribución Normal, partir de una sola de las variables explicativas, de forma lineal (modelo lineal simple). Dicho modelo probablemente será “bueno”, explicará bien el comportamiento de la variable respuesta si la realidad que se pretende explicar es compleja, pero podría ser suficiente para el propósito del estudio109.NotaSe entiende por modelo lineal aquel cuya relación entre las variables viene determinada por una combinación lineal de los parámetros, por ejemplo:\\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\).\\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\epsilon\\).\\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_1^2 + \\beta_3 X_2 + \\epsilon\\).El último ejemplo refleja un modelo lineal en los parámetros, pero lineal en las variables, por el término \\(X_1^2\\). Ejemplos de modelos lineales en los parámetros son:\\(Y = \\beta_0\\cdot e^{\\beta_1 X_1} + \\epsilon\\).\\(Y = \\beta_0 + X_1^{\\beta_1} + \\epsilon\\).Por ejemplo, se sabe que el peso de una persona está relacionado con muchos factores, pero uno de los más determinantes es la altura.\nSi se recogen datos de pesos y alturas de un conjunto de personas se puede ajustar el modelo y obtener una explicación suficiente, aunque parcial, del peso de una persona partir de su altura.\nEs claro que la inclusión de otras variables en el modelo puede ayudar explicar mejor la variable respuesta. Se llega así al denominado modelo de regresión lineal múltiple que se puede expresar matemáticamente como:\n\\[\\begin{equation}\nY_j = \\beta_0 + \\beta_1 X_{1j} + \\ldots + \\beta_p X_{pj} + \\epsilon_j, \\quad \\epsilon_j \\sim N(0, \\sigma^2), \\hspace{0,2cm} j=1,\\ldots,N.\n\\tag{15.1}\n\\end{equation}\\]\ndonde:\\(\\beta_0\\) es el término independiente o constante del modelo,\\(\\beta_1, \\ldots, \\beta_p\\) son los coeficientes de regresión o parámetros del modelo, que se estimarán partir de los datos observados \\((x_{1j}, \\ldots, x_{p_j},y_j)\\) y reflejan la magnitud del efecto lineal (constante) sobre la variable explicada \\(Y\\) de incrementos unitarios en las variables explicativas \\(X_i\\).y \\(\\epsilon_j\\) es el término de error del modelo, la parte de \\(Y\\) que es capaz de explicar la parte determinista del mismo (\\(\\beta_0 + \\beta_1 X_{1j} + \\ldots + \\beta_p X_{pj}\\)), que se supone sigue una distribución de probabilidad Normal, con media 0 y varianza constante \\(\\sigma^2\\);además, se asume que las observaciones son independientes.Siguiendo con el ejemplo del peso, añadir alguna variable genética, el sexo u otras, ayudará mejorar la “bondad” del modelo lineal.\nOtro ejemplo consiste en pretender explicar el salario en un determinado sector económico en función de los años de experiencia, la formación, la situación familiar, el sexo, etc., de los trabajadores.\nNótese que entre las variables explicativas sí puede haber variables de distinto tipo, continuas, categóricas, etc110.\nAhora bien, la interpretación de los coeficientes dependerá del tipo de variable al que van asociados, como se verá en los casos prácticos (Sec. 15.4).Un par de referencias para ampliar conocimientos sobre este tema utilizando R son Faraway (2002) y G. James et al. (2013).","code":""},{"path":"cap-lm.html","id":"procedimiento-de-modelización","chapter":"Capítulo 15 Modelización lineal","heading":"15.2 Procedimiento de modelización","text":"","code":""},{"path":"cap-lm.html","id":"Bondad","chapter":"Capítulo 15 Modelización lineal","heading":"15.2.1 Estimación del modelo","text":"Los datos recogidos u observados sirven para especificar la relación predefinida de antemano, mediante la estimación de los coeficientes \\(\\beta_i\\) que mejor ajustan dicha relación, utilizando el método de mínimos cuadrados.\nAdemás, los correspondientes contrastes permiten decidir si cada coeficiente es significativamente distinto de 0, esto es, si tiene un efecto significativo sobre la respuesta,111 en cuyo caso tiene sentido mantener en el modelo la variable la que va asociado.\nEn la práctica, el coeficiente estimado es significativo si su p-valor (definido en la Sec. 13.5) asociado es suficientemente pequeño.NotaSe acepta, mayoritariamente, como “suficientemente pequeño” un p-valor inferior 0.05, lo que supone un nivel de confianza en las estimaciones del 95%.\nPero dicho valor es arbitrario y podrían considerarse otros valores de referencia. Por ejemplo, en las salidas de R aparecen otros tres niveles de referencia: 0,1, 0,01 y 0,001. En general, cuanto menor sea el p-valor más confianza se tendrá en las conclusiones.Como se avanzó anteriormente, si algún coeficiente es significativo, procede eliminar del modelo la variable explicativa asociada.\nEn tal caso, se vuelven estimar los coeficientes de las variables que se mantienen hasta llegar un modelo con todos los coeficientes significativos, iterando las veces necesarias112. Para facilitar esta labor, se han desarrollado métodos automáticos de selección de variables, basados en la comparación de la varianza residual (haciendo uso del test \\(F\\)), mediante el estadístico AIC (criterio de información de Akaike), etc.113\nJunto con los contrastes, se pueden aportar los intervalos de confianza de los coeficientes, que, si son significativos, contendrán el valor 0.la par del contraste de significación de cada coeficiente, se obtiene el contraste de significación global del modelo.\nLa hipótesis nula es que todos los coeficientes \\(\\beta_1, \\ldots, \\beta_p\\) son 0. Dicho de otro modo, que el conocimiento de las variables \\(X_1, \\ldots, X_p\\) aporta información alguna para explicar los valores de \\(Y\\).También se ha de obtener la bondad del ajuste del modelo, normalmente medida por el coeficiente de determinación lineal, \\(R^2\\) (adimensional, que toma valores entre 0 y 1).\nPara comparar entre diferentes modelos, se utiliza el \\(R^2\\) ajustado/corregido, que tiene en cuenta la composición/complejidad del modelo (número de variables, etc.).\nCuanto mejor ajuste el modelo los datos observados, más próximo 1 será el valor de \\(R^2\\) (1 indica una relación lineal perfecta entre la variable respuesta y las predictoras). Por el contrario, un \\(R^2\\) cercano 0 indica que el modelo estimado ajusta mal los datos.Es habitual valorar conjuntamente la significación global del modelo, su bondad de ajuste y la significación de cada uno de los coeficientes, considerándose apropiados aquellos modelos que son globalmente significativos y tienen la suficiente “bondad”, aunque tengan coeficientes significativos.","code":""},{"path":"cap-lm.html","id":"sec-valida-lm","chapter":"Capítulo 15 Modelización lineal","heading":"15.2.2 Validación del modelo","text":"Aunque el modelo sea significativo se debe validar, es decir, se deben someter contraste los supuestos estadísticos que subyacen al modelo. Para ello se utilizan los residuos del modelo, la parte de \\(Y\\) que explica la regresión estimada o, en otros términos, la diferencia entre los valores observados y los estimados. Matemáticamente,\n\\[e_j = y_j - \\hat{y}_j = y_j - (\\hat{\\beta}_0 +\\hat{\\beta}_1 x_{1j} + \\ldots + \\hat{\\beta}_p x_{pj}), \\hspace{0,2cm} j = 1, \\ldots, N.\\]Los supuestos contrastar son:los residuos han de tener varianza constante (por definición tienen media cero).los residuos han de seguir la distribución de probabilidad Normal.las observaciones tienen que ser independientes.la relación entre la variable respuesta y las explicativas se supone lineal.las variables explicativas son linealmente independientes: ninguna puede ser explicada como combinación lineal de las otras. En caso contrario, se tendría el conocido problema de la multicolinealidad y debería quitarse del modelo la variable explicada por el resto.","code":""},{"path":"cap-lm.html","id":"Sec:interp","chapter":"Capítulo 15 Modelización lineal","heading":"15.2.3 Interpretación de los coeficientes","text":"Una vez validado el modelo, se procede la interpretación de los coeficientes significativos.\nTeniendo en cuenta la expresión del modelo de regresión lineal múltiple (15.1), la regla general de interpretación de cada uno de los coeficientes de regresión estimado \\(\\hat{\\beta}_i\\) es simple y directa: el cambio/impacto medio en el valor de la variable respuesta \\(Y\\) ante un cambio unitario de una variable explicativa cuantitativa o ante un cambio de categoría (desde la que se toma como referencia) si la variable es categórica. Y ello ceteris paribus, esto es, manteniendo constante el valor de las demás variables explicativas.Habrá que tener en cuenta la magnitud de cada variable, porque la influencia real en la respuesta podría ser de poca magnitud (quizá por las unidades o escala utilizada), pero estadísticamente significativa .","code":""},{"path":"cap-lm.html","id":"predicción","chapter":"Capítulo 15 Modelización lineal","heading":"15.2.4 Predicción","text":"La utilidad del modelo estimado (especificado) queda plasmada en su utilización para predecir nuevos valores, \\(\\hat{y_j}\\), partir del conocimiento/asignación de nuevos valores de las variables explicativas, \\(\\{x_{1j}, \\ldots, x_{pj}\\}\\). obstante, dichas predicciones son valores esperados (medios), pudiéndose construir sus correspondientes intervalos de confianza.","code":""},{"path":"cap-lm.html","id":"procedimiento-con-r-la-función-lm","chapter":"Capítulo 15 Modelización lineal","heading":"15.3 Procedimiento con R: la función lm()","text":"R tiene implementada la función lm() para ajustar/estimar modelos de regresión lineal múltiple:El argumento mínimo necesario es formula, donde se predeterminará la relación entre las variables respuesta y explicativas de una forma bastante intuitiva:Y ~ X, es la fórmula utilizar para definir un modelo simple donde Y denota la variable respuesta y X la variable explicativa: \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\).Y ~ X1 + X2, define un modelo lineal múltiple con 2 variables explicativas: \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\\).Y ~ X1 + X2 + X3 - 1 elimina el término independiente, \\(\\beta_0\\), del modelo lineal múltiple de 3 variables explicativas.…En el segundo argumento, data, se indica el conjunto de datos donde se encuentran las variables de trabajo. es especificarlo si están en el Environment.Hay que hacer notar que R considera, por defecto, las variables explicativas como cuantitativas. Si se tienen variables categóricas codificadas con números hay que indicarle que las trate como categóricas, usando la función factor()114. De hacerlo, la función las consideraría numéricas, con el consecuente error de interpretación de los coeficientes asociados.partir de los datos disponibles de \\(Y, X1, ..., Xp\\), la función lm() estima los coeficientes \\(\\hat{\\beta}_i\\) asociados cada variable \\(X_i\\), mediante el método de mínimos cuadrados, y calcula sus errores estándar, con los que obtiene sus estadísticos de contraste (de la \\(t\\) de Student)115 y su significación. En el objeto lm que se genera también se almacenan los valores ajustados, residuos, etc., que se pueden mostrar través de funciones genéricas disponibles en R. Algunas de ellas son:print(): muestra un breve resumen.summary(): proporciona un resumen completo.coef(): proporciona las estimaciones de los coeficientes del modelo.confint(): construye intervalos de confianza para los coeficientes.fitted.values(): muestra los valores ajustados del modelo (para cada observación del data.frame).residuals(): calcula los residuos del modelo (también para cada observación del data.frame).","code":"\nlm(formula, data = ..., ... )"},{"path":"cap-lm.html","id":"Casos","chapter":"Capítulo 15 Modelización lineal","heading":"15.4 Casos prácticos","text":"En esta Sección se utilizan los datos airquality116,\nque consisten en 154 medidas (de 6 variables) de calidad del aire en Nueva\nYork. Las variables consideradas aquí son las cuatro siguientes:Ozone: Concentración media de ozono en la atmósfera (en ppb, partes por billón).Solar.R: Radiación solar (en lang, Langleys).Wind: Velocidad media del viento (en mph, millas por hora).Temp: Temperatura máxima diaria (en grados Fahrenheit).El objetivo es establecer la relación entre la concentración de ozono en la atmósfera, variable respuesta, y las variables meteorológicas Solar.R, Wind y Temp, variables explicativas.\nLos valores disponibles de las cuatro variables permiten considerarlas como variables continuas.\nFigura 15.1: Gráficos de dispersión de las variables explicativas frente la variable respuesta\nAntes de proceder con el ajuste múltiple se pueden realizar los ajustes simples, individuales.\nLa Fig. 15.1 representa 3 regresiones lineales simples, de la variable respuesta Ozone sobre cada una de las 3 variables explicativas.\nCada gráfico muestra un diagrama de dispersión de sólo dos variables, la explicativa en el eje \\(X\\) y la respuesta en el eje \\(Y\\), obteniéndose la popularmente denominada nube de puntos.\nEn tales diagramas se puede ver si entre las variables hay relación lineal, o , y en caso de que la haya, si es positiva/directa (mayores valores de \\(X\\), mayores valores de \\(Y\\)) o negativa/inversa.\nEn cada gráfico se ha añadido la correspondiente recta de regresión lineal (con su correspondiente intervalo de confianza), que podría ser la más apropiada, como parece que ocurre en las regresiones de Ozone sobre Wind y sobre Temp. En ambos casos, la relación parece más bien lineal, aunque una recta podría ser suficiente (en función del interés del estudio) para explicar relativamente bien el comportamiento del nivel de concentración de ozono. El código para el obtener el primer gráfico es:En regresión lineal múltiple es posible visualizar en un sólo gráfico la relación entre la variable respuesta y varias variables explicativas, salvo si son sólo 2, en cuyo caso se tendría un gráfico en 3 dimensiones, generalmente difícil de visualizar.","code":"\nlibrary(\"ggplot2\")\nggplot(airquality, aes(Solar.R, Ozone)) +  \n  geom_point() +\n  theme(aspect.ratio=1) +\n  geom_smooth(method = \"lm\") "},{"path":"cap-lm.html","id":"estimación-de-los-coeficientes","chapter":"Capítulo 15 Modelización lineal","heading":"15.4.1 Estimación de los coeficientes","text":"Se comienza ajustando el siguiente modelo lineal múltiple:117:\n\\[Ozone = \\beta_0 + \\beta_1 Solar.R + \\beta_2 Wind + \\beta_3 Temp  + \\epsilon\\]La definición en R del modelo se puede ver como primer argumento de la función lm(). El objeto que genera la función lm() se guarda bajo el nombre de airq_lm y, continuación, se muestra su resumen con summary():La salida del summary() proporciona las estimaciones de los coeficientes del modelo (columna Estimate).\nEl término independiente aparece como (Intercept) y toma el valor \\(\\beta_0 =\\) -64.3421, el coeficiente asociado Solar.R es \\(\\beta_1 =\\) 0.0598, etc.\nTambién aparecen sus p-valores asociados (columna Pr(>|t|)), pudiéndose comprobar que los 4 coeficientes son significativos al 5%.\nSegún la leyenda Signif. codes, mayor número de asteriscos mayor significación del coeficiente (menor p-valor). Así, los coeficientes de Temp y Wind son más significativos que el de Solar.R.También se pueden apreciar (penúltima línea) dos medidas de la bondad del ajuste del modelo considerado: el R cuadrado múltiple y el R cuadrado múltiple ajustado. En el ejemplo, el \\(R^2\\)(ajustado) es 0.5948, que se podría considerar “suficiente” o en función del objetivo del estudio, aunque, en este caso, está claro que el modelo explica suficientemente bien la concentración de ozono.En la última linea de la salida aparece información sobre el contraste global del modelo: valor del estadístico \\(F\\), grados de libertad y p-valor asociado. Como se aprecia, el modelo es globalmente significativo (p-valor del orden de \\(10^{-16}\\)).","code":"\nairq_lm <- lm(Ozone ~ Solar.R + Wind + Temp, data=airquality)\nsummary(airq_lm)\n#> \n#> Call:\n#> lm(formula = Ozone ~ Solar.R + Wind + Temp, data = airquality)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -40.485 -14.219  -3.551  10.097  95.619 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -64.34208   23.05472  -2.791  0.00623 ** \n#> Solar.R       0.05982    0.02319   2.580  0.01124 *  \n#> Wind         -3.33359    0.65441  -5.094 1.52e-06 ***\n#> Temp          1.65209    0.25353   6.516 2.42e-09 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 21.18 on 107 degrees of freedom\n#>   (42 observations deleted due to missingness)\n#> Multiple R-squared:  0.6059, Adjusted R-squared:  0.5948 \n#> F-statistic: 54.83 on 3 and 107 DF,  p-value: < 2.2e-16"},{"path":"cap-lm.html","id":"validación","chapter":"Capítulo 15 Modelización lineal","heading":"15.4.2 Validación","text":"Lo anterior carece de validez si se satisfacen las hipótesis del modelo mencionadas en la Sec. 15.2.2, principalmente las relativas varianza constante (homocedasticidad) y normalidad. Para ello se realiza un análisis de residuos. La función autoplot() del paquete ggfortify proporciona los gráficos que se muestran en la Fig. 15.2.\nFigura 15.2: Gráficos de residuos\nPor un lado, el gráfico de residuos frente valores ajustados (fitted) muestra cierta heterocedasticidad (varianza cambiante con el valor en el eje X) y linealidad (ya apreciable de forma individual en la Fig. 15.1).\nPor su parte, el gráfico Normal Q-Q, que enfrenta los residuos estandarizados con los cuantiles de la distribución Normal, indica que los residuos presentan desviaciones de la normalidad en ambas colas.Para completar el análisis gráfico se puede acudir contrastes de hipótesis vistos en la Sec. 13.5. El más habitual para contrastar normalidad es el de Shapiro-Wilk, que se implementa en R con la función shapiro.test()118. Para contrastar la homocedasticidad se puede utilizar alguno de los tres tests implementados para tal fin en el paquete lmtest(): el de Breusch-Pagan bptest(), el de Goldfeld-Quandt gqtest() o el de Harrison-McCabe hmctest.\nFigura 15.3: Histogramas de las variables Ozone y log(Ozone)\nEl contraste de homocedasticidad lleva rechazar tal supuesto (p-valor > 0.05), pero el contraste de Shapiro-Wilk confirma la falta de normalidad (p-valor < 0.05).\neste respecto, en la Fig. 15.3 se muestra el histograma de la variable Ozone, apreciándose que los datos recogidos presentan asimetría incompatible con la normalidad, asumida por defecto para la variable respuesta.\nUna posible solución sería el uso de una transformación logarítmica, que produce cierta simetría en la distribución de la variable, acercándola, por tanto, la normalidad.Para el análisis de colinealidad se pueden representar gráficos 2 2 de las variables explicativas, para comprobar si están o correlacionadas (Fig. 15.4).\nFigura 15.4: Gráfico de dispersión por pares de las variables explicativas\nPero este es un análisis parcial, puesto que una de las variables explicativas podría venir explicada por el resto o varias de ellas. Por si este fuera el caso, conviene calcular también los factores de inflación de la varianza (VIF), que indican el incremento de la varianza estimada del coeficiente de regresión de una determinada variable explicativa como consecuencia de la colinealidad con las demás (para más detalle, véase el Cap. 3 de G. James et al. (2013)).El mínimo valor de VIF es 1, existiendo límite superior. Una regla general para interpretar los VIF es la siguiente: Si el VIF de una variable explicativa \\(Xi\\) es 1, hay correlación entre ella y cualquier otra variable explicativa del modelo. Si está entre 1 y 5 la correlación es moderada y provoca graves problemas. Si es mayor que 5 la correlación es fuerte y, probablemente, las estimaciones de los coeficientes y los p-valores resultantes de la estimación del modelo sean confiables.Los VIF se pueden obtener mediante la función vif() del paquete car:En la Fig. 15.4 se aprecia que los gráficos de dispersión muestran ausencia de correlación entre Solar.R y Wind; sin embargo, la correlación entre Wind y Temp parece despreciable. obstante, todos los VIF son prácticamente unitarios, por lo que se puede concluir que el modelo presenta multicolinealidad.","code":"\nlibrary(\"ggfortify\")\nautoplot(airq_lm) +\n  theme_minimal()\nshapiro.test(airq_lm$residuals)\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  airq_lm$residuals\n#> W = 0.91709, p-value = 3.618e-06\nlmtest::bptest(airq_lm)\n#> \n#>  studentized Breusch-Pagan test\n#> \n#> data:  airq_lm\n#> BP = 5.0554, df = 3, p-value = 0.1678\nlibrary(\"GGally\")\nggpairs(airquality[ ,2:4]) \ncar::vif(airq_lm)\n#>  Solar.R     Wind     Temp \n#> 1.095253 1.329070 1.431367"},{"path":"cap-lm.html","id":"interpretación-de-los-coeficientes","chapter":"Capítulo 15 Modelización lineal","heading":"15.4.3 Interpretación de los coeficientes","text":"De acuerdo con lo dicho en la Sec. 15.2.3 se tiene que:Un incremento en Temp de un grado Fahrenheit, manteniéndose constantes los valores de Wind y Solar.R119, provoca un aumento (por ser positivo el coeficiente) promedio en el nivel de concentración de ozono en el aire de 1,6521 ppb.El coeficiente de Wind es negativo, por lo que un aumento en la variable Wind, ceteris paribus, reduce la concentración de ozono. En concreto, dicha reducción, es de 3,3336 ppb por cada milla por hora que se incremente la variable Wind.La influencia de Solar.R en el nivel de concentración de ozono en la atmósfera es positiva, como la de Temp, pero de mucha menor magnitud (por las unidades de una y otra). Concretamente, por cada langley (Ly) que se incremente Solar.R el nivel de concentración de ozono se eleva, ceteris paribus, en 0,0598 ppb.Por tanto, el impacto promedio de un incremento unitario en la magnitud de las variables explicativas depende de la variable y de la magnitud de su coeficiente.Conviene mencionar que las interpretaciones realizadas deben extrapolarse valores fuera del rango que toman las variables explicativas, porque en esas regiones podrían darse otros efectos distintos del lineal que presupone el modelo estimado.","code":""},{"path":"cap-lm.html","id":"predicción-1","chapter":"Capítulo 15 Modelización lineal","heading":"15.4.4 Predicción","text":"Aunque el modelo estimado es adecuado, por la falta de normalidad, linealidad, etc., detectadas, continuación se ilustra cómo obtener predicciones con la función predict().\nPara ello, se asignan los valores de interés las variables explicativas del modelo, con formato data.frame, obteniéndose predicciones del valor medio de la variable respuesta, junto con sus intervalos de confianza o predicción, según se proporcione al argumento interval los valores confidence o prediction, respectivamente.\nEn el siguiente ejemplo se obtienen predicciones de niveles de concentración de ozono para un par de casos elegidos arbitrariamente (el primero corresponde Solar.R=50, Wind=5 y Temp=62):Como se puede observar, en ambos casos la predicción puntual (fit) es la misma y se obtiene sustituyendo en el modelo estimado los valores de las variables explicativas para los cuales se desea realizar la predicción.\nSin embargo, los intervalos de confianza son distintos. Con confidence se obtienen intervalos de confianza para el valor medio de las predicciones correspondientes los días en los que los valores de las variables predictoras sean unos dados. Con prediction, el intervalo de confianza es para la predicción de un valor individual, es decir, para la predicción de un día concreto con esas condiciones meteorológicas. Los intervalos de predicción consideran tanto la incertidumbre de la estimación de un valor (debida la estimación de los parámetros desconocidos) como la variación aleatoria de los valores individuales muestreados (las observaciones muestrales son variables aleatorias). Esto significa que el intervalo de predicción es siempre más ancho que el intervalo de confianza.","code":"\nnueva_meteo <- data.frame(Solar.R = c(50, 300),\n                          Wind = c(5, 17),\n                          Temp = c(62, 90))\npredict(airq_lm, newdata = nueva_meteo, interval = \"confidence\")\n#>        fit      lwr      upr\n#> 1 24.41075 11.01412 37.80739\n#> 2 45.62141 31.46838 59.77444\npredict(airq_lm, newdata = nueva_meteo, interval = \"prediction\")\n#>        fit        lwr      upr\n#> 1 24.41075 -19.662967 68.48448\n#> 2 45.62141   1.311914 89.93090"},{"path":"cap-lm.html","id":"nuevo-ajuste-con-logozone","chapter":"Capítulo 15 Modelización lineal","heading":"15.4.5 Nuevo ajuste con log(Ozone)","text":"Ante los problemas de falta de normalidad de la variable Ozone, se ajusta un nuevo modelo con la variable \\(\\log(Ozone)\\) como respuesta (véase Sec. 9.3.1).\nSe aprovecha para introducir una variable dicotómica para explicar su interpretación.\nSe define Temp_f dicotomizando Temp (tomando sólo dos valores): 1, si la temperatura está por encima de su mediana; y 0, si está por debajo.Al redefinirse la variable respuesta y una variable explicativa del modelo, las estimaciones de los coeficientes cambian respecto al modelo anterior.\nTodos los coeficientes siguen siendo significativos al 5% (incluso al 0.1%), el modelo global también es significativo y el \\(R^2\\)(ajustado) es similar al del modelo anterior.\nEn el Cap. 17, en el que se abordan los modelos aditivos generalizados, se verá cómo se puede modelar la relación entre Ozone y el resto de variables de una forma más satisfactoria.\nobstante, este segundo modelo puede ser útil para ilustrar la relación entre las variables, sin olvidar que se ha de comprobar su validez.\nPara ello, se haría de nuevo el análisis de residuos (que se deja como tarea al lector, al obtenerse de manera idéntica al anterior).\nEn los gráficos de residuos se observará mayor homocedasticidad, linealidad y normalidad que en el caso anterior.","code":"\nmediana <- median(airquality$Temp)\nTemp_f <- factor(as.numeric(airquality$Temp>mediana))\nlairq_lm <- lm(log(Ozone)~  Wind + Solar.R + Temp_f, data=airquality)\nsummary(lairq_lm)\n#> \n#> Call:\n#> lm(formula = log(Ozone) ~ Wind + Solar.R + Temp_f, data = airquality)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -2.55347 -0.29689  0.02409  0.37171  1.18373 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  3.3879872  0.2232099  15.178  < 2e-16 ***\n#> Wind        -0.0885666  0.0161038  -5.500 2.61e-07 ***\n#> Solar.R      0.0030723  0.0005973   5.143 1.23e-06 ***\n#> Temp_f1      0.6999123  0.1158384   6.042 2.25e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.5572 on 107 degrees of freedom\n#>   (42 observations deleted due to missingness)\n#> Multiple R-squared:  0.5972, Adjusted R-squared:  0.5859 \n#> F-statistic: 52.89 on 3 and 107 DF,  p-value: < 2.2e-16"},{"path":"cap-lm.html","id":"coeficientes-de-variables-categóricas","chapter":"Capítulo 15 Modelización lineal","heading":"15.4.6 Coeficientes de variables categóricas","text":"continuación, se aborda la interpretación de coeficientes asociados variables categóricas. Temp_f toma los valores 0 y 1, según la temperatura sea menor o mayor que la mediana respectivamente.\nEn la salida anterior de R sólo aparece Temp_f1.\nEl 1 final indica que el coeficiente está asociado la categoría 1 de Temp_f.\nPara los cálculos con estas variables categóricas, R toma una categoría como referencia120 y proporciona un coeficiente para cada una de las restantes categorías, que representa el cambio medio al pasar desde la categoría de referencia cada una de ellas (técnicamente utiliza variables dummy) o diferencia entre la media de la variable respuesta en las observaciones correspondientes una categoría específica y la categoría que sirve de referencia.\nLa categoría de referencia está considerada en los cálculos del término independiente del modelo.\nPor lo tanto, el coeficiente de Temp_f1 indica que, ceteris paribus,\nla concentración media de ozono de los días con temperaturas por encima de la mediana (categoría 1) es\n0.6999 ppb mayor que la de los días con temperaturas inferiores ella (categoría 0).","code":""},{"path":"cap-lm.html","id":"anova","chapter":"Capítulo 15 Modelización lineal","heading":"15.4.6.1 Comparativa: regresión frente a ANOVA","text":"En la Fig. 15.1 se pueden apreciar regresiones simples puras (variable continua sobre variable continua).\nSi se regresa la variable Ozone sobre la variable categórica Temp_f, se obtendrá un gráfico similar.121\nobstante, el gráfico ayudará comparar visualmente las medias de la variable respuesta en cada categoría.\nEn realidad, al incluir un factor en el modelo se está realizando un contraste t de Student para averiguar si existen diferencias entre la media de la variable respuesta para cada categoría con respecto la categoría de referencia.\nTécnicamente, tales contrastes dos dos son equivalentes al contraste ANOVA (análisis de la varianza), aunque este permite comparar si las medias de la variable respuesta en todas las categorías son iguales o .\nEl ANOVA es un caso particular de regresión lineal en los parámetros, concretamente, cuando todas las variables explicativas son categóricas.","code":""},{"path":"cap-lm.html","id":"comentarios-finales","chapter":"Capítulo 15 Modelización lineal","heading":"15.5 Comentarios finales","text":"En capítulos posteriores se abordarán modelizaciones más complejas, como por ejemplo, los modelos lineales generalizados, GLM (Cap. 16), los modelos aditivos generalizados, GAM (Cap. 17) y los modelos mixtos (Cap. 18). También se verán modelos sparse y métodos penalizados de regresión (Cap. 19), como la regresión ridge, que permite manejar los problemas que genera la presencia de multicolinealidad.Queda fuera de este capítulo la “consideración” de variables de confusión. Ejemplos típicos de tal tipo de variables son la edad y el sexo. De ser incluidas en el modelo, la magnitud e interpretación de las estimaciones de los coeficientes, las predicciones, etc. pueden ser erróneas, pues el efecto de las variables de confusión puede mezclarse con el de otras variables explicativa incluidas en el modelo (por ejemplo, la influencia de la edad/sexo en enfermedades). Si se incluyen, se podrá obtener el efecto de cualquier variable \\(Xi\\) en la respuesta \\(Y\\), ceteris paribus, es decir, independientemente de los valores y/o categorías de las variables de confusión (edad, sexo, etc.).También podrían haberse considerado interacciones entre variables, que se suelen interpretar como sinergias o antagonismos. Pero dada la limitación de espacio y el carácter introductorio de este capítulo se ha considerado oportuno, pues, además, la interpretación de dichas interacciones suele ser compleja. En el Cap. 3 de G. James et al. (2013) puede encontrarse un ejemplo.","code":""},{"path":"cap-lm.html","id":"resumen-13","chapter":"Capítulo 15 Modelización lineal","heading":"Resumen","text":"En este capítulo se introduce el modelo de regresión lineal. En particular:Se presenta el modelo de regresión lineal múltiple indicando los pasos del análisis de regresión: estimación, validación, interpretación y predicción. La regresión lineal simple se plantea como un caso particular de la múltiple.Se muestra el uso de R para el ajuste de este tipo de modelos.Se presentan diversos casos prácticos para ilustrar la interpretación de los coeficientes de regresión, tanto asociados variables continuas como categóricas, la interpretación de las predicciones y el resto de análisis.Se mencionan distintos problemas de modelización que el análisis ayuda detectar, proponiendo su vez soluciones para solventarlos.","code":""},{"path":"cap-glm.html","id":"cap-glm","chapter":"Capítulo 16 Modelos lineales generalizados","heading":"Capítulo 16 Modelos lineales generalizados","text":"Víctor Casero-Alonso\\(^{}\\) y\nMaría Durbán\\(^{b}\\)\\(^{}\\)Universidad de Castilla-La Mancha\\(^{b}\\)Universidad Carlos III de Madrid","code":""},{"path":"cap-glm.html","id":"introducción-7","chapter":"Capítulo 16 Modelos lineales generalizados","heading":"16.1 Introducción","text":"Como se ha mencionado en el Cap. 15 de modelización lineal, el objetivo detrás del uso de modelos es el de intentar explicar el comportamiento de una variable en función del comportamiento de otras que se cree que influyen en él. Por ejemplo, podría interesar predecir:si un empleado abandonará la empresa, o , en función de sus años de experiencia, su formación, etc.;\no si un paciente sufrirá, o , una enfermedad en función de su edad, sexo, nivel de colesterol, etc.si un empleado abandonará la empresa, o , en función de sus años de experiencia, su formación, etc.;\no si un paciente sufrirá, o , una enfermedad en función de su edad, sexo, nivel de colesterol, etc.el número de días que un empleado puede estar de baja laboral en función del tipo de enfermedad, su antigüedad, salario, etc.;\no el número de días que un paciente puede estar hospitalizado en función de la dolencia por la que acude urgencias, su edad, sexo, etc.el número de días que un empleado puede estar de baja laboral en función del tipo de enfermedad, su antigüedad, salario, etc.;\no el número de días que un paciente puede estar hospitalizado en función de la dolencia por la que acude urgencias, su edad, sexo, etc.Estos casos pueden analizarse correctamente con el modelo de regresión lineal múltiple visto en el Cap. 15, porque la variable respuesta, la que interesa predecir en cada ejemplo, sigue una distribución de probabilidad Normal (supuesto necesario para utilizar la regresión lineal) o ni siquiera es continua. Concretamente:abandonar, o , la empresa o sufrir, o , una enfermedad se puede modelizar mediante una variable dicotómica/binaria asignando los valores 0 y 1 las dos posibles respuestas, lo que encaja perfectamente con una distribución de probabilidad de Bernoulli (muy distinta de la Normal, ya que es discreta, aunque podría parecerse; véase Cap. 12.abandonar, o , la empresa o sufrir, o , una enfermedad se puede modelizar mediante una variable dicotómica/binaria asignando los valores 0 y 1 las dos posibles respuestas, lo que encaja perfectamente con una distribución de probabilidad de Bernoulli (muy distinta de la Normal, ya que es discreta, aunque podría parecerse; véase Cap. 12.el número de días de baja (en empleados), de hospitalización (en pacientes)… son variables de tipo recuento (sólo cero o valores positivos) modelizables mediante una variable discreta que podría seguir una distribución de Poisson (que es también distinta la Normal, por ser discreta, aunque también podría parecerse).el número de días de baja (en empleados), de hospitalización (en pacientes)… son variables de tipo recuento (sólo cero o valores positivos) modelizables mediante una variable discreta que podría seguir una distribución de Poisson (que es también distinta la Normal, por ser discreta, aunque también podría parecerse).En este Capítulo se aborda el modelo lineal generalizado (GLM), que generaliza el caso en que la variable respuesta sea Normal cualquier tipo de distribución de probabilidad perteneciente la familia exponencial y que permite varianzas constantes en los errores. En concreto, se centra en la regresión logística y la regresión de Poisson, casos particulares de este modelo, que permiten modelizar correctamente los dos casos planteados anteriormente. Un buen libro de referencia para este Capítulo es G. James et al. (2013).","code":""},{"path":"cap-glm.html","id":"el-modelo-y-sus-componentes","chapter":"Capítulo 16 Modelos lineales generalizados","heading":"16.2 El modelo y sus componentes","text":"El modelo lineal generalizado122 se puede escribir como:\n\\[\\mu = g^{-1}(\\eta),\\]\nen el que se tienen los siguientes componentes:\\(\\mu=E(Y)\\), el componente aleatorio: la media de la variable respuesta \\(Y\\), que puede seguir cualquier distribución de probabilidad de la familia exponencial. Entre ellas están las más habituales: la Normal (por tanto el modelo de regresión lineal es un caso particular del GLM), la Bernoulli/binomial (utilizada en la regresión logística), la Poisson, la gamma, etc.\\(\\mu=E(Y)\\), el componente aleatorio: la media de la variable respuesta \\(Y\\), que puede seguir cualquier distribución de probabilidad de la familia exponencial. Entre ellas están las más habituales: la Normal (por tanto el modelo de regresión lineal es un caso particular del GLM), la Bernoulli/binomial (utilizada en la regresión logística), la Poisson, la gamma, etc.\\(\\eta = X \\beta\\), el componente sistemático, el predictor lineal, la “estructura” que aportan las variables explicativas/predictoras \\(X=(X_1, \\ldots , X_p)\\), que intentan explicar el comportamiento de la variable respuesta, donde \\(\\beta=(\\beta_1, \\ldots , \\beta_p)\\) es el vector de coeficientes (parámetros) estimar del modelo.\\(\\eta = X \\beta\\), el componente sistemático, el predictor lineal, la “estructura” que aportan las variables explicativas/predictoras \\(X=(X_1, \\ldots , X_p)\\), que intentan explicar el comportamiento de la variable respuesta, donde \\(\\beta=(\\beta_1, \\ldots , \\beta_p)\\) es el vector de coeficientes (parámetros) estimar del modelo.\\(g(\\cdot)\\), la novedad de los GLM, la denominada función de enlace, que relaciona los dos componentes anteriores. Esta función puede tomar distintas formas, como se verá en la siguiente Sección.\\(g(\\cdot)\\), la novedad de los GLM, la denominada función de enlace, que relaciona los dos componentes anteriores. Esta función puede tomar distintas formas, como se verá en la siguiente Sección.Igual que en el modelo lineal, las dos partes o etapas fundamentales del análisis de un GLM son:La especificación de la relación o estructura predefinida de antemano, mediante la estimación de los coeficientes que mejor ajustan dicha relación, utilizando el método de máxima verosimilitud123.\nMás adelante se verá cómo se interpretan tales coeficientes y cómo se puede comprobar la adecuación del modelo.La utilización del modelo estimado (especificado) para predecir nuevas respuestas, según sea el caso: valores, probabilidades de ocurrencia, etc.Para la correcta aplicación de los GLM es crucial la elección tanto de la variable respuesta como de las explicativas (que podrían ser de distinto tipo: numéricas -continuas o discretas- o categóricas/cualitativas -dicotómicas o politómicas124), así como de la distribución de probabilidad más apropiada para la respuesta.\nComo se adelantó en el Cap.15, como muestra sirva percatarse de que una misma variable podría ser explicativa o respuesta, por ejemplo “diabetes” (sí o ), según se tenga interés en explicar la influencia de la diabetes en otra variable o la influencia de otras variables en la diabetes.\nTambién una misma variable podría considerarse y utilizarse como variable de distinto tipo; por ejemplo, la edad puede considerarse como variable discreta (años cumplidos) o como categórica ordinal (grupos de edad), aunque es una variable continua (puede tomar cualquier valor en un intervalo).","code":""},{"path":"cap-glm.html","id":"función-enlace","chapter":"Capítulo 16 Modelos lineales generalizados","heading":"16.2.1 Función enlace","text":"Cada distribución de probabilidad tiene asociada una función de enlace canónica125:Para la Normal es la identidad: \\(g(\\mu)=\\mu\\).Para la Bernoulli, es la función logit: \\(g(\\mu)=logit(\\mu)=log (\\mu / (1-\\mu))\\).Para la Poisson, es el logaritmo: \\(g(\\mu)=log(\\mu)\\).Para la Gamma es la inversa: \\(g(\\mu)=1/\\mu\\), …Tanto en el caso de la regresión logística (variable respuesta tipo Bernoulli) como en la regresión de Poisson aparece el logaritmo (neperiano) en la función de enlace, lo que conduce efectos multiplicativos de los factores o covariables \\(X_i\\) sobre la respuesta, como se verá más claramente en la Sec. 16.4.3. Este es un punto que las distingue de la regresión lineal, en la que los efectos son aditivos.","code":""},{"path":"cap-glm.html","id":"procedimiento-con-r-la-función-glm","chapter":"Capítulo 16 Modelos lineales generalizados","heading":"16.3 Procedimiento con R: la función glm()","text":"En el paquete stats (de la distribución base de R) se encuentra la función glm() que se utiliza para llevar cabo el ajuste de un GLM:formula: para definir el predictor lineal; por ejemplo, Y ~ X1 + X2 + X3.family: para indicar la distribución de la variable respuesta (gaussian, binomial, poisson …) que determina la función de enlace (binomial \\(\\rightarrow\\) logit, etc.; consúltese ?family o ?glm para más detalles).Las “herramientas” utilizadas para lm() también se pueden usar para glm() (aunque algunas interpretaciones varían). Así, se puede usar summary() para detectar los predictores importantes, fitted() para obtener los valores ajustados, etc.","code":"\nglm(formula, family = ..., data = ..., ...)"},{"path":"cap-glm.html","id":"regresión-logística","chapter":"Capítulo 16 Modelos lineales generalizados","heading":"16.4 Regresión logística","text":"La regresión logística es el caso más “famoso” de GLM, de gran relevancia en distintos contextos: Medicina, Economía, etc.\nSe utiliza cuando la variable respuesta es dicotómica, del tipo pertenencia, o , un determinado grupo (fumadores, enfermos, morosos, …).\nHabitualmente se considera que toma el valor \\(Y=1\\) si la observación pertenece al grupo de interés e \\(Y=0\\) en caso contrario.\nTal tipo de variable se puede modelizar con una distribución de Bernoulli, caracterizada por un parámetro \\(p\\) que indica la probabilidad de pertenecer al grupo de interés.El objetivo principal suele ser predecir el grupo al que pertenece un nuevo individuo/elemento, sobre la base de la información sobre dicho elemento/individuo que proporcionan las variables explicativas. Para ello, se estima el modelo con los datos disponibles, determinándose qué variables influyen significativamente en la variable respuesta126.¿Por qué tiene cabida aquí el uso del modelo de regresión lineal múltiple? Al ajustar un modelo del tipo: \\(Y=\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p + \\epsilon,\\)\nlas estimaciones \\(\\hat{Y}\\) serán números reales que rara vez\ncoincidirán con 0 ó 1 (que son los valores admisibles de la variable respuesta).\nDicho de otro modo, si sólo se tuviese una variable explicativa, al ajustar el modelo de regresión lineal simple, la recta sobrepasaría, o alcanzaría, los valores posibles de respuesta (0 ó 1), como ocurre en la Fig. 16.1 (izquierda) obtenida partir de los datos del ejemplo de enfermedad coronaria que se manejará en la Sec. 16.6.1.El modelo de regresión logística múltiple se define como:\n\\[\\begin{equation}\n   \\text{logit}(p)=\\log{ \\Big(\\dfrac{p}{1-p}  \\Big)}=\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p + \\epsilon,\n   \\tag{16.1}\n\\end{equation}\\]\nque permite estimar la ratio entre\nla probabilidad de pertenecer al grupo de interés, \\(p\\),\ny la de pertenecer dicho grupo, \\(1-p\\).\nUtilizando la función de enlace se puede transformar el predictor lineal \\(\\eta=\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p\\) para obtener valores admisibles para una probabilidad.\nLa función logística, definida como\n\\[\\begin{equation}\n  p = \\frac{e^\\eta}{1+e^\\eta}\n  \\tag{16.2}\n\\end{equation}\\]\ny representada en la Fig. 16.1 (derecha), es la más habitual, y por ello da el nombre la regresión logística. Con ella se obtiene la probabilidad de pertenecer al grupo de interés, \\(p = P[Y = 1]\\), e inmediatamente la de pertenecer dicho grupo, \\(1-p = P[Y = 0]\\).\nFigura 16.1: Gráfico de dispersión del diagnóstico frente la edad para el ejemplo de enfermedad coronaria (izquierda) y función logística (derecha)\nLa ventaja del modelo logístico es que la función logística tiende cero por la izquierda y uno por la derecha, por lo que la predicción será\nsiempre un valor válido para una probabilidad. Como contrapartida, medida que las probabilidades se acercan cero o uno la relación entre el predictor y la probabilidad deja de ser lineal, lo que complica la interpretación de los coeficientes.","code":""},{"path":"cap-glm.html","id":"procedimiento-de-ajuste","chapter":"Capítulo 16 Modelos lineales generalizados","heading":"16.4.1 Procedimiento de ajuste","text":"partir de los datos disponibles de las variables predictoras y respuesta:Se estiman los coeficientes \\(\\beta_i\\) del modelo, para especificar la relación entre las variables, contrastando si tales estimaciones, \\(\\hat\\beta_i\\), son o significativas.Se valora la eliminación de variables significativas, atendiendo la significación global del modelo, al modelo teórico subyacente, etc.Se comprueba la adecuación del modelo final obtenido.En caso afirmativo, se interpretan los coeficientes y el modelo queda listo para hacer predicciones de probabilidades o para clasificar individuos/elementos.NotaComo se menciona en el Cap. 15, un coeficiente es significativo cuando su p-valor asociado es lo suficientemente pequeño (como norma general, inferior 0.05). obstante, pueden tomarse otros valores de referencia; por ejemplo, en las salidas de R aparecen otros tres niveles de referencia, 0.1, 0.01 y 0.001.","code":""},{"path":"cap-glm.html","id":"secADECUACION","chapter":"Capítulo 16 Modelos lineales generalizados","heading":"16.4.2 Adecuación del modelo","text":"Para comprobar si el modelo estimado es adecuado, se compara con el modelo más simple, el que solo incluye el término independiente.\nPara ello, se pueden utilizar distintos contrastes basados en la deviance,127 una medida que juega el papel de la suma de cuadrados de los residuos.\nEn el modelo de regresión logística, la deviance de un modelo es menos dos veces el logaritmo de la verosimilitud de dicho modelo.\nLa diferencia entre la deviance de un modelo más elaborado y el simple se distribuye como una \\(\\chi^2\\) con tantos grados de libertad como restricciones impuestas los parámetros, lo que permite contrastar cuál de los dos modelos ajusta mejor los datos.\nP-valores bajos indican que el modelo ajustado es inadecuado, debiendo investigarse otras posibles variables predictoras, si se incumple la hipótesis de linealidad o existe sobredispersión (por ejemplo por exceso de ceros).Adicionalmente, se puede aplicar el contraste de la razón de verosimilitudes. Este test contrasta la significación de cada cada variable predictora, basándose en la deviance que se genera al añadir cada variable secuencialmente al modelo que contiene las anteriores, lo que ayuda decidir si mantenerla o eliminarla del modelo.Para contrastar la bondad de ajuste del modelo, el contraste más popular en la literatura es el de Hosmer-Lemeshow, aplicable modelos con al menos una variable cuantitativa.\nP-valores bajos indican falta de ajuste.Continuando con las medidas de bondad de ajuste, en el modelo de regresión logística tiene sentido calcular el coeficiente de determinación lineal, \\(R^2\\), pero existen varias alternativas equivalentes para hacerse una idea de la variabilidad de la respuesta explicada por el modelo.\nLas tres más populares son el Pseudo \\(R^2\\) de McFadden, el \\(R^2\\) de Cox y Snell (que por construcción puede alcanzar el 1)\ny el \\(R^2\\) de Nagelkerke (una corrección del de Cox y Snell).","code":""},{"path":"cap-glm.html","id":"SECCinterp","chapter":"Capítulo 16 Modelos lineales generalizados","heading":"16.4.3 Interpretación de resultados","text":"La interpretación de los coeficientes es tan directa y sencilla como en el modelo lineal.\npartir de las estimaciones del predictor lineal \\(\\hat{\\eta}\\), modelo (16.1), se puede estimar la probabilidad de que un individuo pertenezca al grupo de interés utilizando la expresión (16.2). Alternativamente se puede estimar el odds, con la siguiente expresión, que se deduce de (16.1):\n\\[\\begin{equation}\n\\mathit{odds} = \\frac{\\hat{p}}{1-\\hat{p}} =  e^{\\hat{\\eta}} = e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 + \\ldots + \\hat{\\beta}_k X_k}.\n\\tag{16.3}\n\\end{equation}\\]\nEl odds se puede explicar como cuántas veces es más probable pertenecer al grupo de interés (\\(Y=1\\)) que al otro grupo (\\(Y=0\\)).\nPor ejemplo, si \\(\\hat{p} = 0.75\\) el odds es \\(0.75/0.25=3\\), entonces es tres veces más probable pertenecer al grupo de interés que pertenecer128.La ecuación (16.3) se puede expresar, equivalentemente, como:\n\\[\\begin{equation}\n\\mathit{odds}=e^{\\hat{\\beta}_0} \\cdot e^{\\hat{\\beta}_1 X_1} \\cdot \\ldots \\cdot e^{\\hat{\\beta}_k X_k}.  \n\\tag{16.4}\n\\end{equation}\\]\nLa interpretación de los \\(\\hat{\\beta}_i\\) carece de sentido. Lo interpretable son los \\(e^{\\hat\\beta_i}\\), denominados odd ratios () .\nComo el modelo (16.4) es multiplicativo (sus términos aparecen multiplicando, sumando), por lo que valores de \\(e^{\\hat{\\beta}_i}\\) inferiores 1 implican disminución en el valor del odds –se reduce la probabilidad de pertenecer al grupo de interés respecto la de pertenecer– y valores por encima de 1 implican incremento del valor del odds, ceteris paribus129. En concreto, \\(100(e^{\\hat{\\beta}_i}-1)\\) indica la variación porcentual en el \\(odds\\) ante un incremento unitario en la variable explicativa \\(X_i\\), mientras que \\(100(e^{-\\hat{\\beta}_i}-1)\\) indica el cambio porcentual que se opera en el \\(odds\\) debido un decremento unitario de dicha variable explicativa, \\(ceteris \\hspace{0,1cm} paribus\\).El modelo de regresión logística se acompañar del denominado riesgo relativo (RR, relative risk). Es una razón de\nprobabilidades como el odds, pero en vez de comparar la probabilidad de pertenecer un grupo\no otro de la variable respuesta, compara las probabilidades de pertenecer al grupo de referencia (\\(Y=1\\)) según los valores del predictor categórico \\(X_i\\).\nEl riesgo relativo es similar al de la variable \\(X_i\\) sólo cuando la probabilidad de \\(Y=1\\) es pequeña; se suele dar como referencia que sea inferior al 10%. Por lo tanto, y RR coinciden siempre.","code":""},{"path":"cap-glm.html","id":"predicción.-curva-roc-y-auc","chapter":"Capítulo 16 Modelos lineales generalizados","heading":"16.4.4 Predicción. Curva ROC y AUC","text":"Como se ha mencionado anteriormente, el uso habitual de la regresión logística es la predicción, tanto de probabilidades, sino de la clasificación en un grupo u otro de futuros individuos/elementos en base la información conocida de ellos través de las variables explicativas.\nLa regla de clasificación en los grupos depende de la elección del punto de corte de la probabilidad, por ejemplo \\(\\hat{Y}=1\\) si \\(\\hat p>0.7\\) e \\(\\hat{Y}=0\\) en otro caso.\nPara seleccionar dicho punto de corte, se acude al análisis de los casos clasificados correctamente o por el modelo ajustado: análisis de la sensibilidad y la especificidad del modelo, su visualización más popular, la curva ROC (receiver operating characteristic) y la medición del área bajo dicha curva, AUC (area curve ROC); véase Cap. 9.La sensibilidad, o tasa de verdaderos positivos, se define como el cociente entre estos y la suma de los verdaderos positivos y los falsos negativos.\nLa especificidad, o tasa de verdaderos negativos, es el cociente entre estos y la suma de los verdaderos negativos y los falsos positivos. Por consiguiente, la sensibilidad es una medida de la probabilidad de que un caso real positivo (\\(Y=1\\)) sea clasificado correctamente como positivo por el modelo (\\(\\hat{Y}=1\\)). Equivalentemente, la especificidad es la probabilidad de clasificar correctamente casos negativos.Gráficamente, se pueden obtener los valores de la sensibilidad y especificidad para distintos puntos de corte (entre 0 y 1).\nLa Fig. 16.2 (izquierda) muestra este gráfico para uno de los casos prácticos130.\nPero la forma más popular de analizar la sensibilidad y la especificidad es\nmediante la curva ROC, que representa, para los distintos puntos de corte considerados, la sensibilidad frente la tasa de falsos positivos, esto es la 1-especificidad (Fig. 16.2 (derecha)).\nSu AUC sirve para comparar distintos modelos de regresión logística (u otros modelos con el mismo fin131).\nCuanto mayor poder discriminante tenga el modelo, más próxima la unidad estará la AUC.\nUn clasificador aleatorio presentaría la curva ROC coincidente con la diagonal, con una AUC de 0.5.","code":""},{"path":"cap-glm.html","id":"regresión-de-poisson","chapter":"Capítulo 16 Modelos lineales generalizados","heading":"16.5 Regresión de Poisson","text":"Se utiliza cuando la variable respuesta es discreta,sólo toma valores negativos y su distribución de probabilidad es modelizable mediante la distribución de Poisson. Por ejemplo, las variables de tipo “número de”, como la del ejemplo motivador del principio: número de días de hospitalización.\nEl parámetro que caracteriza la distribución de Poisson es \\(\\lambda\\), que representa tanto la media como la varianza de la variable aleatoria (nivel teórico).Dados los anteriores condicionantes, en esta tesitura tampoco tiene cabida el modelo de regresión lineal, principalmente porque las estimaciones \\(\\hat{Y}\\) podrían arrojar valores negativos.De nuevo el objetivo suele ser la predicción: en el ejemplo, el número de días que un (nuevo) paciente estará hospitalizado (\\(0, 1, 2, \\ldots\\)), en base la información proporcionada por las variables explicativas.\nPrevio la predicción, se estima el modelo, partir de los datos disponibles, para determinar las variables explicativas que influyen significativamente sobre la variable respuesta. Y, nuevamente, los efectos serán multiplicativos, dado que la función de enlace es de tipo logarítmico, como se ha visto en la Sec. 16.4.3. Por ello, todo lo visto anteriormente para la regresión logística es válido para la regresión de Poisson.","code":""},{"path":"cap-glm.html","id":"casos-prácticos","chapter":"Capítulo 16 Modelos lineales generalizados","heading":"16.6 Casos prácticos","text":"","code":""},{"path":"cap-glm.html","id":"secEJreglog","chapter":"Capítulo 16 Modelos lineales generalizados","heading":"16.6.1 Ejemplos de regresión logística","text":"Para llevar cabo estos ejemplos, se utiliza el conjunto de datos cleveland incluido en el paquete CDR que acompaña este libro. Se quiere explicar la variable diag (diagnóstico; dicotómica: 1, ha sufrido una enfermedad coronaria; 0, la ha sufrido) partir de otras variables. La variable respuesta diag ya aparece como factor en la base de datos.Estimación\nPrimeramente, se considera un primer modelo con dos variables explicativas continuas edad y dep (depresión en el segmento ST):En la salida se muestran las estimaciones de los coeficientes del modelo (columna Estimate) y su significatividad (columna Pr(>|z|)). En este ejemplo, los dos coeficientes de interés (los correspondientes edad y dep) son significativos (un nivel de significación del 5%).Puede observarse que, dicha salida, también incluye la null deviance y la residual deviance. La primera indica lo “bien” que predice el modelo sin variables explicativas (tan sólo con el término independiente o intercepto); la segunda indica lo “bien” que predice el modelo con variables explicativas. Como se avanzó anteriormente, un contraste Chi-cuadrado permitirá dilucidar si el modelo con variables explicativas predice significativamente mejor que el que solo tiene un término independiente y su predicción, sea cual sea el valor de las variables explicativas, es siempre la media de los valores de la variable respuesta.Se completa el modelo anterior añadiendo variables categóricas –concretamente, tdolor (politómica, tipo de dolor) y sexo (dicotómica)– para poder incluir más adelante la interpretación de los coeficientes asociados este tipo de variables.\nPara su correcta interpretación, se deben introducir en el predictor como variables de tipo factor132. De lo contrario, el procedimiento las considera numéricas, obteniéndose una salida que llevaría una interpretación errónea.Adecuación del modelo\nPara evaluar la bondad de ajuste de los modelos se lleva cabo el contraste de Hosmer-Lemeshow:En ambos casos, los p-valores son “altos”, indicando un ajuste suficiente.Para realizar el test de la razón de verosimilitudes con R se acude la función anova():Las deviances correspondientes añadir secuencialmente cada variable o factor al modelo que contiene los anteriores permite concluir que dicha inclusión secuencial de tales variables predictoras es significativa respecto los modelos que las incluyen.Para completar, se podría contrastar también el efecto de una variable sobre la respuesta comparando la deviance del modelo con dicha variable y sin ella:El resultado indica que la variable edad se considera un predictor significativo en el modelo.\nSe deja al lector realizar este ejercicio con las restantes variables, que le llevarán concluir que todas son predictores significativos.El valor del Pseudo \\(R^2\\) de McFadden se obtiene como sigue:Para el primer modelo el Pseudo \\(R^2\\) de McFadden es 0.16, mientras que para el segundo es 0.37.Interpretación de los coeficientes\ncontinuación se muestran los coeficientes estimados del segundo modelo y sus correspondientes (sus exponenciales) que son los interpretables:Los coeficientes asociados las variables continuas son significativos y positivos, por tanto sus son significativamente superiores 1.\nIndican que, ceteris paribus,por cada año más, el odds de padecer la enfermedad frente padecerla se incrementa133 un 5.78% (al ser el valor del asociado la edad 1.0578). ¿Y cuál sería el incremento en el odds ante un aumento de la edad en 20 años? En tal caso, el odds ser multiplicaría por \\(e^{\\hat{\\beta}_1*20} =\\) 3.078, es decir, más que se triplica. Se debe ser cuidadoso con la interpretación: se triplica la probabilidad sino el odds.Ante un aumento de una unidad en la variable dep más que se duplica el odds del paciente, pues su es 2.2454.NotaEn regresión lineal, que un coeficiente sea significativo quiere decir que es significativamente distinto de cero, por el hecho de que el modelo considera efectos aditivos de las variables. El modelo de regresión logística es de efectos multiplicativos y, por ello, el valor “neutro” es el 1. Por tanto, un coeficiente significativo se interpreta como significativamente distinto de 1.¿Y cómo se interpretan los coeficientes asociados las variables categóricas?\n¿Qué significan los valores de sexo1, tdolor2…? Representan el cambio (promedio) en la variable respuesta al pasar de la categoría de referencia la mostrada.134el de sexo1 significa que el odds de padecer la enfermedad en los hombres (sexo=1) es más de 5 veces superior que en las mujeres (sexo=0).el de tdolor4 indica que los individuos con este tipo de dolor (asintomáticos) presentan un odds 13 veces superior al de los de tdolor=1 (angina típica).135Nótese que se muestra \\(e^{\\beta_0}\\), pues su interpretación carece de sentido práctico: sería el de las personas de 0 años, dep=0, sexo=0 y tdolor=1.Predicciones\nUna vez estimado el modelo y comprobada su bondad, se puede usar para obtener predicciones.\nPara ello, se deben asignar valores las variables explicativas: edad, dep, sexo y tdolor (se han escogido arbitrariamente).Con la función predict() se puede obtener tanto el valor predicho para el predictor lineal (valor interpretable), \\(\\eta\\), como directamente la probabilidad de que dicho individuo sufra la enfermedad coronaria, \\(p\\):partir del valor de \\(p\\) (o de \\(\\eta\\)) se puede obtener el odds correspondiente:Es decir, un individuo con 50 años, dep=3, sexo=1 (hombre) y tdolor=4 (asintomático) tiene una probabilidad de padecer la enfermedad en cuestión 17 veces mayor que de padecerla.Riesgo relativo\nSe ha visto que el para la categoría hombre en el segundo modelo es 5.45. El riesgo relativo de sufrir la enfermedad, en el caso de los varones, para un paciente de 50 años, valor dep=3 y tdolor=4 (asintomático) es:El riesgo relativo de sufrir la enfermedad, en el caso de los varones, es \\(RR\\)=0.9447/0.7582 = 1.2459, es\ndecir, un varón tiene un 24.6% más de posibilidades de tener la enfermedad que una mujer con sus mismos valores o categorías en las variables que se usan como predictores. En este caso, el y el RR son valores cercanos; ello es debido que el diagnóstico 1 es frecuente (concretamente lo presentan el 45.9% de los pacientes de la base de datos).Curva ROC\nEl análisis de sensibilidad y especificidad del modelo, así como la curva ROC y su AUC, para este ejemplo, se pueden obtener con el siguiente código:\nFigura 16.2: Gráfico de sensibilidad y especificidad según puntos de corte de discriminación (izquierda) y curva ROC (derecha) para el segundo modelo de regresión logística\nLa Fig. 16.2 (izquierda) muestra que tomando como punto de corte una probabilidad cercana 0.45 se obtienen valores de sensibilidad y especificidad alrededor de 0.8. obstante, se deben evaluar los costes y riesgos de una mala clasificación (por ejemplo, dar tratamiento cuando hace falta y darlo cuando es necesario).La Fig. 16.2 (derecha) representa la curva ROC de reg_log2. La curva está por encima de la diagonal, con lo que es mejor que un clasificador aleatorio. La AUC es de 0.878, con una sensibilidad de 78.4% y una especificidad de 84.1%.\nSi se realiza el análisis para reg_log se comprobará que la AUC es de 0.759, por lo que el segundo modelo es mejor para discriminar.\nAdemás, el gráfico también indica el valor del punto de corte óptimo, 0.518 (el que proporciona la mayor AUC, 0.878) junto con los correspondientes valores de sensibilidad, especificidad, etc. También muestra las estimaciones y los errores estándar (s.e.) de los coeficientes del modelo considerado.","code":"\nlibrary(\"CDR\")\nreg_log <- glm(diag ~ edad + dep, \n               family = \"binomial\", data = cleveland)\nsummary(reg_log)\n#> \n#> Call:\n#> glm(formula = diag ~ edad + dep, family = \"binomial\", data = cleveland)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.3804  -0.8905  -0.6210   1.0021   1.9644  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept) -3.08080    0.81939  -3.760  0.00017 ***\n#> edad         0.03738    0.01476   2.532  0.01134 *  \n#> dep          0.86851    0.13791   6.298 3.02e-10 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 417.98  on 302  degrees of freedom\n#> Residual deviance: 350.64  on 300  degrees of freedom\n#> AIC: 356.64\n#> \n#> Number of Fisher Scoring iterations: 4\nreg_log2 <- update(reg_log, ~ . + sexo + tdolor)\nlibrary(\"ResourceSelection\")\nhoslem.test(reg_log$y, reg_log$fitted.values)\n#> \n#>  Hosmer and Lemeshow goodness of fit (GOF) test\n#> \n#> data:  reg_log$y, reg_log$fitted.values\n#> X-squared = 5.631, df = 8, p-value = 0.6885\nhoslem.test(reg_log2$y, reg_log2$fitted.values)\n#> \n#>  Hosmer and Lemeshow goodness of fit (GOF) test\n#> \n#> data:  reg_log2$y, reg_log2$fitted.values\n#> X-squared = 4.8171, df = 8, p-value = 0.7769\nanova(reg_log2, test = \"Chisq\")\n#> Analysis of Deviance Table\n#> \n#> Model: binomial, link: logit\n#> \n#> Response: diag\n#> \n#> Terms added sequentially (first to last)\n#> \n#> \n#>        Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \n#> NULL                     302     417.98              \n#> edad    1   15.447       301     402.54 8.487e-05 ***\n#> dep     1   51.894       300     350.64 5.858e-13 ***\n#> sexo    1   23.982       299     326.66 9.726e-07 ***\n#> tdolor  3   62.153       296     264.51 2.037e-13 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## se elimina edad del segundo modelo\nreg_log3 <- update(reg_log2, ~ . - edad)\nanova(reg_log3, reg_log2, test = \"Chisq\")\n#> Analysis of Deviance Table\n#> \n#> Model 1: diag ~ dep + sexo + tdolor\n#> Model 2: diag ~ edad + dep + sexo + tdolor\n#>   Resid. Df Resid. Dev Df Deviance Pr(>Chi)   \n#> 1       297     274.45                        \n#> 2       296     264.51  1   9.9488  0.00161 **\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nnull <- glm(diag ~ 1, family = \"binomial\", data = cleveland)\n1 - logLik(reg_log) / logLik(null)\n#> 'log Lik.' 0.1611088 (df=3)\n1 - logLik(reg_log2) / logLik(null)\n#> 'log Lik.' 0.3671819 (df=7)\nsummary(reg_log2)\n#> \n#> Call:\n#> glm(formula = diag ~ edad + dep + sexo + tdolor, family = \"binomial\", \n#>     data = cleveland)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.5346  -0.6604  -0.2436   0.6568   2.3975  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept) -6.67669    1.30016  -5.135 2.82e-07 ***\n#> edad         0.05621    0.01828   3.075   0.0021 ** \n#> dep          0.80890    0.16597   4.874 1.10e-06 ***\n#> sexo1        1.69477    0.36801   4.605 4.12e-06 ***\n#> tdolor2      0.65668    0.67357   0.975   0.3296    \n#> tdolor3      0.19465    0.59654   0.326   0.7442    \n#> tdolor4      2.58230    0.57549   4.487 7.22e-06 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 417.98  on 302  degrees of freedom\n#> Residual deviance: 264.51  on 296  degrees of freedom\n#> AIC: 278.51\n#> \n#> Number of Fisher Scoring iterations: 5\nexp(coef(reg_log2))[-1]\n#>      edad       dep     sexo1   tdolor2   tdolor3   tdolor4 \n#>  1.057824  2.245432  5.445391  1.928371  1.214880 13.227480\nindividuo <- data.frame(edad = 50, dep = 3, \n                        sexo = \"1\", tdolor = \"4\" #entre comillas por ser factores\n                        ) \n#(eta <- predict(reg_log2, individuo))\n(p <- predict(reg_log2, individuo, type = \"response\"))\n#>         1 \n#> 0.9446843\np / (1 - p) # exp(eta)\n#>        1 \n#> 17.07805\nhom <- data.frame(edad = 50, dep = 3, sexo = \"1\", tdolor = \"4\")\nmuj <- data.frame(edad = 50, dep = 3, sexo = \"0\", tdolor = \"4\")\n(ph <- predict(reg_log2, hom, type = \"response\"))\n#>         1 \n#> 0.9446843\n(pm <- predict(reg_log2, muj, type = \"response\"))\n#>         1 \n#> 0.7582346\nph / pm # riesgo relativo hombre/mujer\n#>      1 \n#> 1.2459\npar(mfrow = c(1, 2))\nlibrary(\"Epi\")\nROC(\n  form = diag ~ edad + dep + sexo + tdolor, data = cleveland,\n  plot = \"sp\"\n)\nROC(\n  form = diag ~ edad + dep + sexo + tdolor, data = cleveland,\n  plot = \"ROC\", las = 1\n)"},{"path":"cap-glm.html","id":"ejemplo-de-regresión-de-poisson","chapter":"Capítulo 16 Modelos lineales generalizados","heading":"16.6.2 Ejemplo de regresión de Poisson","text":"partir del mismo conjunto de datos, cleveland, ahora se considera como variable explicar, variable respuesta, dhosp, el número de días de hospitalización de un paciente.\nComo variables explicativas se seleccionan las siguientes: diag, edad, sexo y tdolor, que son de distinto tipo, lo que permitirá ilustrar sus distintas interpretaciones.Visualización ilustrativa\nFigura 16.3: Gráfico de barras de dhosp según tipo de diagnóstico (izquierda) y gráficos de cajas de ‘edad’ según el número de días de hospitalización (derecha)\nla vista de los gráficos de la Fig. 16.3, generados con las sentencias anteriores, diag parece ser una buena variable predictora del número de días de hospitalización, mientras que edad tiene una influencia tan clara.Ajuste e interpretaciónAl especificar family = \"poisson\", la función glm() selecciona automáticamente la función de enlace apropiada: el logaritmo (efectos multiplicativos).De todas las variables introducidas en el modelo, sólo diag y sexo son significativas (al 5%). Se confirma así lo visto en la Fig. 16.3 (derecha): que edad influye en la respuesta. Al ser las dos variables significativas de tipo dicotómico, su interpretación, como en la regresión logística, se hace en función de la categoría de referencia, y ceteris paribus (esto en cualquier caso):El coeficiente de diag es 1.0903 pero se le debe aplicar la exponencial para tener como unidades de medida días.\nAsí, el número medio de días de estancia en el hospital es 2.98 veces mayor con diag=1 que con diag=0 (categoría tomada como referencia).El coeficiente de diag es 1.0903 pero se le debe aplicar la exponencial para tener como unidades de medida días.\nAsí, el número medio de días de estancia en el hospital es 2.98 veces mayor con diag=1 que con diag=0 (categoría tomada como referencia).Algo similar se puede decir para sexo: un hombre (sexo=1) se espera que esté en el hospital, en media, 1.24 días por cada día que esté una mujer.Algo similar se puede decir para sexo: un hombre (sexo=1) se espera que esté en el hospital, en media, 1.24 días por cada día que esté una mujer.De la misma manera se interpretarían los coeficientes asociados tdolor, si bien son significativos: cada uno de ellos expresaría la diferencia con los pacientes del grupo tdolor=1.De la misma manera se interpretarían los coeficientes asociados tdolor, si bien son significativos: cada uno de ellos expresaría la diferencia con los pacientes del grupo tdolor=1.Aunque el coeficiente asociado edad tampoco es significativo, y su magnitud es ínfima, se da su interpretación, al ser la única variable continua:\npor cada año que aumenta la edad, el número medio de días en el hospital se ve multiplicado por\n1.0001 (ínfimo)136.Aunque el coeficiente asociado edad tampoco es significativo, y su magnitud es ínfima, se da su interpretación, al ser la única variable continua:\npor cada año que aumenta la edad, el número medio de días en el hospital se ve multiplicado por\n1.0001 (ínfimo)136.Para posteriores comparaciones, se considera otro modelo en el que se elimina tdolor (por ser significativa):Adecuación\nEn los ajustes reg_pois y reg_pois2 hay información sobre la Null y la Residual deviance, con las que se puede realizar el contraste de comparación de modelos (el simple frente al elaborado) mencionado en la Sec. 16.4.2.Al ser los p-valores superiores 0.05, ambos modelos se pueden considerar que explican “mejor” que el modelo nulo, el que únicamente contiene la constante y el término aleatorio.Predicción\nCon los modelos ajustados, y comprobada su adecuación, se pueden predecir valores, que en este caso son el número medio de días de hospitalización.Se han escogido, arbitrariamente, valores de las variables explicativas para 4 pacientes: los pacientes 1 y 2 presentan la enfermedad coronaria (diag=1) mientras que los pacientes 3 y 4 ; los pacientes 1 y 3 son hombres (sexo=1), mientras que el 2 y el 4 son mujeres; los cuatro tienen 50 años y son asintomáticos (tdolor=4).\nLas predicciones obtenidas indican que el paciente 1 (hombre con enfermedad coronaria) estará hospitalizado más días, en media, que el resto, aunque le sigue de cerca la paciente 2 (mujer con enfermedad coronaria).También se pueden dibujar las predicciones modelo reg_pois de todo el conjunto de datos, que glm() ha guardado como fitted.values.\nFigura 16.4: Predicciones de todo el conjunto de datos del modelo regpois (izquierda) y regpois2 (derecha)\nLa Fig. 16.4 muestra las predicciones del número de días (promedio) de hospitalización para los 303 individuos considerados en el conjunto de datos y para cada uno de los dos modelos de regresión de Poisson considerados.Cabe recordar que en el primer modelo se consideran las variables diag, edad, sexo y tdolor, mientras que en el segundo se ha eliminado la variable tdolor. Lo primero que hay que destacar es que los puntos se “ordenan” en forma de lineas de puntos horizontales ordenadas verticalemtne. Ello se debe que () en el eje de abscisas figura la variable Èdad, que influye significativamente en los días de hospitalización (lineas horizontales); y (ii) las demás variables son categóricas, por lo que desplazan las lineas verticalmente, sin modificar su pendiente. El desplazamiento vertical diferenciado por colores, mediante la variable diag, es claro: la predicción del número de días (promedio) de hospitalización para personas con diag=1 es un valor en torno 3, mientras que para aquellos con diag=0 es un valor en torno 1.\nEn ambos gráficos se ha incluido identificación para la variable sexo, podría hacerse añadiendo apropiadamente facet_wrap(vars(sexo)) lo que permitiría ver que es el sexo el que genera las diferencias verticales (significativas, porque la variable sexo es significativa) de las dos líneas horizontales tanto en torno hosp=3 como hosp=1 (apreciables claramente en el gráfico de la derecha).\nPor último, como la diferencia entre el gráfico de la izquierda y el de la derecha es la inclusión o de tdolor queda claro que es dicha variable la que genera las distintas líneas horizontales de puntos (gráfico de la izquierda) con poca variabilidad vertical (por ser significativa).","code":"\np1 <- ggplot(cleveland, aes(dhosp, fill = diag)) +\n  geom_bar(position = \"dodge\")\np2 <- ggplot(cleveland, aes(x = factor(dhosp), y = edad)) +\n  geom_boxplot()\nlibrary(\"patchwork\")\np1 + p2\nreg_pois <- glm(dhosp ~ diag + edad + sexo + tdolor,\n  data = cleveland, family = \"poisson\")\nsummary(reg_pois)$coef\n#>                  Estimate  Std. Error     z value     Pr(>|z|)\n#> (Intercept) -0.2230728726 0.332125111 -0.67165314 5.018045e-01\n#> diag1        1.0902902179 0.109731744  9.93595999 2.903654e-23\n#> edad         0.0001026277 0.004877904  0.02103931 9.832143e-01\n#> sexo1        0.2160286255 0.103297627  2.09132226 3.649919e-02\n#> tdolor2      0.1408464230 0.205232539  0.68627725 4.925383e-01\n#> tdolor3      0.1217757202 0.189404790  0.64293897 5.202637e-01\n#> tdolor4      0.1215388103 0.178729482  0.68001546 4.964947e-01\nexp(coef(reg_pois))[-1]\n#>    diag1     edad    sexo1  tdolor2  tdolor3  tdolor4 \n#> 2.975137 1.000103 1.241138 1.151248 1.129501 1.129233\nreg_pois2 <- glm(dhosp ~ diag + sexo + edad,\n  data = cleveland,   family = \"poisson\")\npchisq(reg_pois$deviance, reg_pois$df.residual, lower.tail = F)\n#> [1] 0.1325654\npchisq(reg_pois2$deviance, reg_pois2$df.residual, lower.tail = F)\n#> [1] 0.155157\npacientes <- data.frame(\n  diag = c(\"1\", \"1\", \"0\", \"0\"),\n  edad = c(50, 50, 50, 50),\n  sexo = c(\"1\", \"0\", \"1\", \"0\"),\n  tdolor = c(\"4\", \"4\", \"4\", \"4\"))\npredict(reg_pois, pacientes, type = \"response\")\n#>         1         2         3         4 \n#> 3.3532035 2.7017171 1.1270752 0.9080983\ncleveland$hat <- reg_pois$fitted.values\ng1 <- ggplot(cleveland, aes(x = edad, y = hat, colour = diag)) +\n  geom_point() +\n  labs(x = \"Edad\", y = \"Días hospitalización\")\ncleveland$hat2 <- reg_pois2$fitted.values\ng2 <- ggplot(cleveland, aes(x = edad, y = hat2, colour = diag)) +\n  geom_point() +\n  labs(x = \"Edad\", y = \"Días hospitalización\")\ng1 + g2"},{"path":"cap-glm.html","id":"resumen-14","chapter":"Capítulo 16 Modelos lineales generalizados","heading":"Resumen","text":"En este capítulo se introduce el modelo de regresión lineal generalizado, indicado cuando las respuestas son gaussianas (Normales). En particular:Se introduce la función de enlace, que juega un papel importante en estos modelos.Se describen los casos particulares de regresión logística y de regresión de Poisson.Se muestra el uso de R para el ajuste de estos modelos.Se ilustra la interpretación de los coeficientes, tanto asociados variables cuantitativas como categóricas, y los demás resultados obtenidos con R, mediante casos prácticos.Se incluye el uso de la regresión logística como clasificador y se dan indicaciones al respecto mediante la curva ROC y la AUC.","code":""},{"path":"cap-gam.html","id":"cap-gam","chapter":"Capítulo 17 Modelos aditivos generalizados","heading":"Capítulo 17 Modelos aditivos generalizados","text":"\\(^{}\\)Universidad de Castilla-La Mancha\\(^{b}\\)Universidad Carlos III de Madrid","code":""},{"path":"cap-gam.html","id":"introducción-8","chapter":"Capítulo 17 Modelos aditivos generalizados","heading":"17.1 Introducción","text":"Los modelos lineales, o los lineales generalizados (GLM), vistos en los\ncapítulos 15 y 16 tienen la ventaja de ser fáciles de ajustar e\ninterpretar. Además, se dispone de técnicas para contrastar las\nhipótesis del modelo. Sin embargo, cuando la variable respuesta está\nrelacionada de forma lineal con las variables explicativas tiene\nsentido utilizar modelos lineales (generalizados o ) y hay que acudir\nmodelos que flexibilicen esta relación, que, en el caso de una una\núnica variable explicativa, se puede expresar como sigue:\n\\[Y=\\beta_0+f(X)+\\varepsilon.\\]Puede que la función \\(f()\\) sea conocida de antemano, como ocurre en\nmuchos modelos biológicos, donde existe una dependencia de tipo\nexponencial, \\(f(x)=e^{\\beta_0+\\beta_1x}.\\) En otras ocasiones, dicha\nfunción es desconocida y se puede utilizar una aproximación. Por\nejemplo, mediante la regresión polinómica, muy utilizada en la práctica: \\[\\begin{equation}\nY = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\ldots + \\beta_p X^p + \\varepsilon, \\quad \\varepsilon \\sim N(0,\\sigma^2).\n(\\#eq:polinom)\n\\end{equation}\\]Sin embargo, la regresión polinómica tiene un gran inconveniente: que \nse lleva cabo de forma local y, por tanto, cada vez que se cambia un coeficiente\ndel modelo, el cambio impacta los valores ajustados en todo el rango\nde la variable explicativa. obstante, es posible utilizar técnicas\n(como las que se presentan en este capítulo) en las que el valor predicho\nen un punto dado sólo depende de las observaciones en ese punto y de las\nobservaciones vecinas, es decir, el ajuste se lleva cabo de forma\nlocal.En el caso de disponer de más de una variable explicativa, la extensión\ndel modelo de regresión múltiple sería el modelo\naditivo (en el caso de variable respuesta\ngaussiana), donde se asume que la relación entre la variable respuesta y cada una de\nlas variables explicativas tenga que ser lineal: \\[\\begin{equation}\nY= f(X_{1})+\\ldots +f(X_j)+\\epsilon, \\quad \\varepsilon \\sim N(0,\\sigma^2).\n(\\#eq:aditivo)\n\\end{equation}\\] Las funciones \\(f()\\) incluyen también las funciones\nlineales vistas en el Capítulo 15. Los modelos aditivos generalizados,\nGAM, extienden el modelo anterior respuestas gaussianas, como lo\nhacen los GLM respecto de los modelos lineales con respuesta gaussiana\n(véase Cap. 16).","code":""},{"path":"cap-gam.html","id":"splines-con-penalizaciones","chapter":"Capítulo 17 Modelos aditivos generalizados","heading":"17.2 Splines con penalizaciones","text":"Las funciones de la Eq. (??) se estiman mediante técnicas de suavizado o smoothers, cuyo objetivo es extraer\nlas tendencias (o señales) existentes en la relación entre la variable\nrespuesta y las variables explicativas, sin presuponer una forma\nfuncional priori para ellas; sólo se asume que la relación entre\n\\(Y\\) y \\(X\\) es suave (tiene poco ruido). Las predicciones obtenidas\nmediante estas técnicas tienen menos variabilidad que la variable respuesta; de ahí que estas técnicas se les denomine “suavizadores” (la regresión lineal es un suavizador llevado al extremo). Las siguientes son algunas de las técnicas de suavizado más populares:Regresión polinomial local con pesos, lowess.Kernels.Splines.Este capítulo se centra en uso de los splines, ya que es la técnica de suavizado más utilizada. Los splines son funciones polinómicas \ntrozos de la variable explicativa, que se unen en puntos llamados nodos.\nExisten muchos tipos de splines (naturales, cíclicos,\nB-splines, O-splines, etc.). Independientemente del tipo de spline, este capítulo se centra en los splines con penalizaciones (P-splines), que se basan en: \\(()\\) hacer una\naproximación de la función \\(f()\\) mediante una base de funciones, y \\((ii)\\)\nañadir una penalización la hora de estimar el modelo, de manera que se\npueda controlar la variabilidad de la curva que se quiere estimar. Hay\nmuchas maneras de representar una función través de una base de funciones (un\nejemplo sencillo de una base de funciones es el caso de la regresión\npolinómica, en la que la base de funciones, es decir, la matriz de\nregresión, es una matriz cuyas columnas son las potencias de la\nvariable explicativa: \\([X:X^2:\\ldots : X^p]\\)). Una de las mejores\nopciones son los B-splines (De Boor 2001), debido sus buenas propiedades\nnuméricas. La penalización se añade en la función de verosimilitud y se\nconstruye partir de la derivada de la curva que se quiere penalizar. Generalmente se utilizan penalizaciones de orden 2, lo que implica que se\npenaliza todo aquello que es lineal en la función; por tanto,\nsi la penalización es muy grande la curva estimada es simplemente una\nlínea recta. La penalización está controlada por un parámetro llamado\nparámetro de suavizado Véanse P. H. Eilers Marx (2010) y P. H. Eilers, Marx, Durbán (2015) para más detalle).la hora de ajustar este tipo de modelos hay que tomar dos decisiones\nimportantes:El número de nodos del B-spline: generalmente\nse utiliza esta regla: \\[\\begin{equation}\n\\text{número de nodos}=min\\{40,\\text{valores únicos de }X/4\\}\n(\\#eq:nodos)\n\\end{equation}\\] (por ejemplo, si se tienen 100 observaciones diferentes, se\nelegirían 100/4=25 nodos).El valor del parámetro de suavizado: se puede estimar por distintos métodos: validación cruzada, validación cruzada generalizada, máxima verosimilitud, máxima verosimilitud restringida, etc. Se recomienda re-expresar el modelo como un modelo mixto (véase Cap. 18) y estimarlo mediante el método de máxima verosilimitud restringida, REML137, para así poder estimar el parámetro de suavizado junto con los demás parámetros del modelo.La Fig. 17.1 muestra el impacto que el parámetro de\nsuavizado tiene en el ajuste final de la curva (los datos\ncorresponden al dataset fossil del paquete Semipar).\nFigura 17.1: Regresión con P-splines para diferentes valores del parámetro de suavizado\n","code":""},{"path":"cap-gam.html","id":"aspectos-metodológicos","chapter":"Capítulo 17 Modelos aditivos generalizados","heading":"17.3 Aspectos metodológicos","text":"Al igual que en el caso de los modelos lineales y los modelos GLM, en\nlos modelos GAM es necesario conocer algunos aspectos metodológicos que\nson fundamentales para llevar cabo un ajuste correcto de los modelos y\nentender los resultados obtenidos en el ajuste. continuación se\nmuestran los más relevantes.","code":""},{"path":"cap-gam.html","id":"estimación-de-los-parámetros-del-modelo","chapter":"Capítulo 17 Modelos aditivos generalizados","heading":"17.3.1 Estimación de los parámetros del modelo","text":"La estimación de los modelos GAM se lleva cabo mediante máxima\nverosimilitud penalizada. Supóngase el caso de una sola variable\nexplicativa y que se quiere ajustar el modelo: \\[Y= f(X)+\\epsilon.\\]\nComo se comentó anteriormente, los modelos GAM tienen como punto de\npartida la aproximación de la función estimar mediante una matriz\nformada por B-splines; es decir, se busca transformar el modelo lineal o\nlineal generalizado tradicional de tal forma que \\(f(X)\\) sea el producto\nde una matriz multiplicada por unos coeficientes (esa matriz está\nformada por los B-splines). En otros términos, se elige una base\n(una matriz \\(\\textbf{B}\\)) que permita escribir la función \\(f(X)\\) como\nuna combinación lineal de sus elementos (los elementos de esta base son\nconocidos ya que se calculan partir de las variables explicativas):\n\\[f(X)=\\sum_{l=1}^k b_l(X)\\theta_l,\\] donde \\(b_l(X)\\) son las funciones\nB-spline que componen la base. En forma matricial:\n\\[f(X)=\\textbf{B}\\boldsymbol{\\theta}.\\] Los parámetros\n\\(\\boldsymbol{\\theta}\\) se estiman minimizando la siguiente expresión (en\nel caso de asumir gaussianidad para los errores, y por tanto para la variable respuesta, los mínimos cuadrados penalizados son\nequivalentes la máxima verosimilitud penalizada):\\[(\\bf{y}-\\bf{B}\\boldsymbol{\\theta})^\\prime(\\bf{y}-\\bf{B}\\boldsymbol{\\theta}) + \\lambda\\boldsymbol{\\theta}^\\prime\\bf{P}\\boldsymbol {\\theta},\\]\ndonde \\(\\boldsymbol{P}\\) es la matriz de penalización y \\(\\lambda\\) es el\nparámetro de suavizado. Dado un valor del parámetro de suavizado, las\nestimaciones de los parámetros vienen dadas por138: \\[\\begin{equation}\n\\hat{\\boldsymbol{\\theta}} = (\\bf{B}^{\\prime}T \\bf{B} +\\lambda \\bf{P} )^{-1}\\bf{B}^\\prime\\bf{y},\n(\\#eq:thetas)\n\\end{equation}\\] y las estimaciones de la variable respuesta se obtienen\ncomo:\n\\(\\hat{ \\bf{y}}= \\underbrace{\\bf{B}(\\bf{B}^\\prime\\bf{B} +\\lambda \\bf{P} )^{-1}\\bf{B}^\\prime}_{\\bf{H}}\\bf{y}\\).\nLa matriz \\(\\bf{H}\\) juega un papel importante, ya que la suma de\nsu diagonal da una idea de la complejidad de la curva ajustada (la curva\nmás compleja sería la que interpola los datos). Dicha suma se denomina grados de libertad\nefectivos (que se corresponden\ncon el número de parámetros ajustados).","code":""},{"path":"cap-gam.html","id":"inferencia-sobre-las-funciones-suaves","chapter":"Capítulo 17 Modelos aditivos generalizados","heading":"17.3.2 Inferencia sobre las funciones suaves","text":"Para saber si la relación estimada entre \\(Y\\) y \\(X\\) es o \nestadísticamente significativa, se debe proceder al contraste:\n\\[\\begin{eqnarray*}\nH_0: f(X)=0  & \\text{ (efecto)}\\\\\nH_1: f(X)\\neq 0 & \\text{ (efecto)}.\n\\end{eqnarray*}\\]Dado que la función \\(f(X)\\) depende de los coeficientes que acompañan \nlas bases de B-splines, el contraste anterior es equivalente al\ncontraste: \\[\\begin{eqnarray*}\nH_0:   &&\\boldsymbol{\\theta}=0  \\\\\nH_1:  && \\boldsymbol{\\theta}\\neq 0.\n\\end{eqnarray*}\\] La distribución del estadístico de contraste dependerá\nde si la variable respuesta sigue una distribución Normal o : en caso afirmativo el estadístico de contraste sigue un distribución \\(F\\). En caso negativo, sigue una distribución \\(\\chi^2\\).Comparación de modelosCuando se trabaja con un modelo aditivo\n(??) en el que hay más de una variable explicativa, puede\nser de interés comparar versiones de ese modelo que contengan distintos\nconjuntos de variables. La comparación dependerá de la relación entre\nlos modelos comparar:Modelos anidados. La comparación se basa, igual que en los\nGLM, en la diferencia en la deviance residual.\nSi se quieren comparar dos modelos \\(m_1\\) y \\(m_2\\) (donde\n\\(m_1\\subset m_2\\)), entonces:En el caso de variable respuesta Normal, el estadístico de contraste es:\n\\[\\frac{(DR(m_1)-DR(m_2))/(df_2-df_1)}{DR(m_2)/(n-df_2)}\\approx F_{(df_2-df_1), (n-df_2)},\\]donde \\(DR\\) es la deviance residual (suma de cuadrados residual) y \\(df\\)\nson los grados de libertad asociados con cada modelo.En otro caso, se utiliza como estadístico de contraste el siguiente:\n\\[DR(m_1)-DR(m_2)\\approx \\chi^2_{df_2-df_1}.\\]En otro caso, se utiliza como estadístico de contraste el siguiente:\\[DR(m_1)-DR(m_2)\\approx \\chi^2_{df_2-df_1}.\\]Modelos anidados. En este caso los contrastes anteriores \nson válidos y se utilizarán criterios basados en el AIC (criterio de\ninformación de Akaike).","code":""},{"path":"cap-gam.html","id":"suavizado-mutidimensional-y-para-datos-no-gaussianos","chapter":"Capítulo 17 Modelos aditivos generalizados","heading":"17.3.3 Suavizado mutidimensional y para datos no Gaussianos","text":"Para el suavizado penalizado en 2 dimensiones (o más) también se necesita\nuna base y una penalización. El modelo sería:\n\\[{Y} = \\beta_0+  f\\left({X}_{1},{X}_{2}\\right)+{\\epsilon},\\]\ndonde \\(f()\\) es una función de las dos covariables \\({X}_{1}\\) y\n\\({X}_{2}\\). Dicha función se aproxima mediante el producto\ntensorial de las bases de B-splines marginales para cada una de las\ncovariables y la penalización dependerá de dos parámetros de suavizado.\nLos términos de suavizado multidimensional se pueden combinar con\ntérminos unidimensionales y términos lineales. En este caso, la penalización dependería de dos parámetros de suavizado (uno para cada covariable).La extensión de los modelos de suavizado al caso en el que la variable\nrespuesta sea Gaussiana, se hace de forma similar al caso lineal,\ncuando se pasa de un modelo de regresión lineal un GLM. Al igual que\nen el caso de los GLMs,\n\\(g({\\boldsymbol\\mu})=\\boldsymbol{\\eta}=f(\\bf{X})=\\bf{B} \\boldsymbol{\\theta}\\),\ny se añade la penalización la función de verosimilitud de la\ndistribución correspondiente:\n\\[\\ell_p(\\boldsymbol{\\theta})=\\ell(\\boldsymbol{\\theta})+\\lambda \\boldsymbol{\\theta}^\\prime P \\boldsymbol{\\theta} ,\\]\ndonde \\(\\ell(\\boldsymbol{\\theta)}\\) es la log-verosimilitud.","code":""},{"path":"cap-gam.html","id":"procedimiento-con-r-la-función-gam-del-paquete-mgcv","chapter":"Capítulo 17 Modelos aditivos generalizados","heading":"17.4 Procedimiento con R: la función gam() del paquete mgcv","text":"Aunque hay muchas librerías disponibles, la principal es mgcv, que\nimplementa una gran variedad de modelos de suavizado través de la\nfunción gam() (generalized additive models)139.formula es el argumento principal de esta función; es la ecuación\ndel modelo: por ejemplo, y ~ x1+x2+s(x3).\nLo primero que se tiene que elegir es la base utilizar para\nrepresentar las funciones suaves, s(x) (véase ?s o\n?smooth.terms), o te(x1,x2) en el caso de suavizado\nbidimensional. Por defecto se usan los llamados thin plate\nsplines. El tipo de base usada se puede modificar utilizando el\nargumento bs dentro de s(x, bs = \"ps\"); en este caso ps\nindica el uso de B-splines con penalizaciones. continuación se\ndescriben otras alternativas:\nLo primero que se tiene que elegir es la base utilizar para\nrepresentar las funciones suaves, s(x) (véase ?s o\n?smooth.terms), o te(x1,x2) en el caso de suavizado\nbidimensional. Por defecto se usan los llamados thin plate\nsplines. El tipo de base usada se puede modificar utilizando el\nargumento bs dentro de s(x, bs = \"ps\"); en este caso ps\nindica el uso de B-splines con penalizaciones. continuación se\ndescriben otras alternativas:m indica el orden de la penalización; por defecto es 2.k es el número de nodos para construir la base. El número por\ndefecto suele ser demasiado bajo, por lo que siempre se recomienda\nque el usuario elija el número utilizando la regla dada en\n(??).debe igualarse una variable numérica o factor de la misma\ndimensión de cada covariable, para hacer interacciones entre curvas y\nvariables.id se utiliza para forzar que diferentes términos suaves utilicen\nla misma base y la misma cantidad de suavizado.method selecciona método para estimar el parámetro de suavizado. Se\nse puede elegir entre: REML (máxima verosimilitud restringida),\nML (máxima verosimilitud), GCV.Cp (validación cruzada\ngeneralizada), GACV.Cp (validación cruzada aproximada\ngeneralizada). En la práctica, como se indicó anteriormente, se prefiere REML.family permite elegir la distribución de la variable respuesta\n(binomial, Poisson, etc.); por defecto asume Gaussiana.select=TRUE contrasta si una variable debe entrar o en el\nmodelo.Es importante reseñar que si el método elegido para estimar el parámetro de suavizado es REML, entonces internamente, el modelo se transforma en un modelo mixto y lo estima junto con el resto de los parámetros del modelo (véase 18).","code":"\ngam(formula, method=\"\", select=\"\", family=gaussian())"},{"path":"cap-gam.html","id":"casos-prácticos-1","chapter":"Capítulo 17 Modelos aditivos generalizados","heading":"17.5 Casos prácticos","text":"En este apartado se ven una serie de aplicaciones que permiten mostrar\nlos diferentes usos de este tipo de modelos.","code":""},{"path":"cap-gam.html","id":"modelo-unidimensional-con-fossil","chapter":"Capítulo 17 Modelos aditivos generalizados","heading":"17.5.1 Modelo unidimensional con fossil","text":"Se empieza ilustrando el uso de la función gam() con el conjunto de\ndatos fossil del paquete SemiPar. El objetivo es estimar la relación\nentre la edad de los fósiles y la proporción de isotopos de estroncio.\nFigura 17.2: Edad de los fósiles con respecto la proporción de isótopos de estroncio\nla vista de la Fig. 17.2, está claro que se necesita\najustar una curva (y una línea) para estimar la relación entre ambas\nvariables. Para ello se utiliza la función gam(), que devuelve un\nobjeto de tipo \"gam\" y que se puede usar con las típicas funciones\nprint(), summary(), fitted(), plot(), residuals(), etc.Como se puede ver, la relación entre la variable\nrespuesta (\\(Y\\), proporción de estroncio) y la variable explicativa (\\(X\\),\nedad) se ha especificado mediante un spline, s(), de tipo penalizado,\nps, con 25 nodos. Se ha seleccionado REML como método para estimar\nel parámetro de suavizado (los parámetros del spline se estiman también\nmediante REML, ya que da lugar las mismas estimaciones que máxima\nverosimilitud).En la primera parte de la salida anterior aparecen los términos que\nentran linealmente en el modelo (en este caso sólo el término independiente o intercepto); en la parte de abajo se muestran los términos de suavizado. Como se\nindicó anteriormente, dado que se ha usado select=TRUE, se está\ncontrastando si la variable edad debe entrar en el modelo o . En\neste caso, es claro que ha de entrar ya que el p-valor de s(x) es\npequeño y los grados de libertad asociados son aproximadamente 10, lo\nque indica que la relación entre \\(Y\\) y \\(X\\) está lejos de la linealidad.La función gam.check() devuelve los gráficos de residuos usuales\n(residuos frente valores ajustados, gráficos de cuantiles para\ncomprobar la normalidad, etc.), pero además proporciona información\nsobre el proceso de ajuste del modelo.\nFigura 17.3: Gráficos de residuos obtenidos con gam.check()\nEl test que aparece en la parte de abajo está contrastando si el número\nde nodos elegido es suficiente. Si el valor de k está muy próximo al de\nedf, entonces se debería reajustar el modelo con más nodos.El comando plot() permite dibujar la función suave que relaciona Y con X. La curva estimada que aparece en la Fig. 17.4 está\ncentrada (la función plot() siempre lo hace de esta forma), el\nargumento shade hace que se sombree el intervalo de confianza y\nseWithMean hace que la incertidumbre sobre el término independiente se\nincluya en el cálculo del intervalo de confianza.\nFigura 17.4: Curva ajustada e intervalo de confianza\n","code":"\nlibrary(\"SemiPar\")\ndata(fossil)\nY <- 10000*fossil$strontium.ratio\nX <- fossil$age\nplot(X,Y, xlab=\"Edad\", ylab = \"Proporción de estroncio\")\nlibrary(\"mgcv\")\nfit_gam <- gam(Y ~ s(X,k=25,bs=\"ps\"), method=\"REML\", select=TRUE)\n# se eligen 25 nodos ya que se lavariable tiene 106 observaciones\nsummary(fit_gam)\n#> \n#> Family: gaussian \n#> Link function: identity \n#> \n#> Formula:\n#> Y ~ s(X, k = 25, bs = \"ps\")\n#> \n#> Parametric coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 7.074e+03  2.435e-02  290504   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Approximate significance of smooth terms:\n#>        edf Ref.df     F p-value    \n#> s(X) 10.22     24 35.89  <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> R-sq.(adj) =  0.891   Deviance explained = 90.2%\n#> -REML = 23.946  Scale est. = 0.062849  n = 106\ngam.check(fit_gam,cex=1.2)#> \n#> Method: REML   Optimizer: outer newton\n#> full convergence after 5 iterations.\n#> Gradient range [-4.557319e-06,5.900236e-06]\n#> (score 23.94602 & scale 0.06284944).\n#> Hessian positive definite, eigenvalue range [4.557347e-06,53.03185].\n#> Model rank =  25 / 25 \n#> \n#> Basis dimension (k) checking results. Low p-value (k-index<1) may\n#> indicate that k is too low, especially if edf is close to k'.\n#> \n#>        k'  edf k-index p-value\n#> s(X) 24.0 10.2    1.03    0.66\nplot(fit_gam,shade=TRUE,seWithMean=TRUE,pch=19,1,cex=.55)"},{"path":"cap-gam.html","id":"modelo-aditivo-con-airquality","chapter":"Capítulo 17 Modelos aditivos generalizados","heading":"17.5.2 Modelo aditivo con airquality","text":"En esta sección se analizan de nuevo los datos airquality (ver airquality140), que consisten en 154 medidas de calidad del\naire en Nueva York, de mayo septiembre 1973. El objetivo es establecer\nla relación entre las variables meteorológicas y el nivel de concentración de ozono en\nla atmósfera. Ya se ha analizado dicha relación en el Cap. 15, donde los ajustes lineales realizados eran\nsatisfactorios pero se encontraban problemas en los residuos del\nmodelo, lo cual impedía validar la modelización realizada. Allí se sugería que\nla relación entre la variable respuesta y alguna explicativa fuese \nlineal. Además, se consideró la transformación logarítmica de la\nvariable Ozone, y con dicha trasformación se obtenía una distribución\nmás similar la distribución Normal.En consecuencia, se va ajustar el modelo incluyendo las variables\nexplicativas sin imponerles linealidad; en particular, se van incluir las\nvariables Wind, Temp y Solar.R. Las variables Wind y Temp\ntienen sólo 31 y 40 valores únicos, respectivamente, aunque el conjunto de datos tiene 154 valores; por eso, para estas dos variables, se decide establecer el número de nodos en 10 y más; para la variable Solar.R el número de nodos se fija en 20.Los resultados indican que todas las variables son significativas\n(p-valores pequeños), estando la variable Temp próxima la linealidad\n(los grados de libertad efectivos asociados la variable son 1.8). El \\(R^2\\)\najustado es 0.69, por lo que el modelo ajusta moderadamente bien los\ndatos.La Fig. ?? muestra las tres curvas ajustadas junto con sus correspondientes intervalos de confianza. También incluye los denominados residuos parciales que corresponden , por ejemplo, en el caso del gráfico del viento,\n\\(log(Ozone)-\\hat \\beta_0-\\hat f(Temp)- \\hat f(Solar.R)\\), es decir, lo\nque queda sin explicar después de haber ajustado los demás términos del\nmodelo.\nFigura 17.5: Curvas estimadas para Wind, Temp y Solar\n","code":"\n\nairq_gam=gam(log(Ozone)~s(Wind,bs=\"ps\",k=10) +   \n         s(Temp,bs=\"ps\",k=10)+s(Solar.R,bs=\"ps\",k=20),\n         method=\"REML\",select=TRUE,data=airquality,na.action=na.omit)           \nsummary(airq_gam)\n#> \n#> Family: gaussian \n#> Link function: identity \n#> \n#> Formula:\n#> log(Ozone) ~ s(Wind, bs = \"ps\", k = 10) + s(Temp, bs = \"ps\", \n#>     k = 10) + s(Solar.R, bs = \"ps\", k = 20)\n#> \n#> Parametric coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  3.41593    0.04586   74.49   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Approximate significance of smooth terms:\n#>              edf Ref.df     F  p-value    \n#> s(Wind)    2.318      9 2.255 3.13e-05 ***\n#> s(Temp)    1.852      9 6.128  < 2e-16 ***\n#> s(Solar.R) 2.145     19 1.397 2.31e-06 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> R-sq.(adj) =  0.689   Deviance explained = 70.7%\n#> -REML = 86.106  Scale est. = 0.23342   n = 111\nlibrary(\"mgcViz\")\n# getViz es otra opción para dibujar los términos de un modelo gam()\nb <- getViz(airq_gam)\npl <- plot(b) + l_points() + l_fitLine(linetype = 2) + l_ciLine(colour = 2)\nprint(pl,pages=1)"},{"path":"cap-gam.html","id":"modelo-semiparamétrico-con-onions","chapter":"Capítulo 17 Modelos aditivos generalizados","heading":"17.5.3 Modelo semiparamétrico con onions","text":"Es un caso particular del modelo aditivo, pues en este modelo todas las\nvariables entran de forma lineal excepto una:\n\\[{Y}= \\beta_0 +\\beta_1 {X}_{1}+\\ldots +\n\\beta_{p-1}{X}_{p-1}+f({X}_p)+\\epsilon.\\]La forma de ajustar el modelo es exactamente igual la anterior. Pero\nhay un caso que merece especial interés: cuando en la parte paramétrica\nse incluye una variable categórica con dos o más niveles. Al igual que\nen el caso de regresión lineal, se puede plantear si se quieren ajustar\ndos o más rectas paralelas (modelo aditivo) o paralelas (modelo con\ninteracción).Para ilustrar este caso se acude al data.frame onions (librería\nSemiPar). Contiene 84 observaciones de un experimento sobre la\nproducción de un tipo de cebolla en dos localidades: (Purnong Landing (la localidad de referencia) y Virginia. El objetivo es relacionar el logaritmo de la producción de\ncebollas con la densidad de plantas por metro cuadrado, dens. El modelo lineal\nbásico sería:\n\\[ \\log(\\text{yield}_j) = \\beta_0 + \\beta_1\\text{location}_{j} + \\beta_2 \\text{dens}_j + \\epsilon_j\\]\ndonde \\[\\text{location}_{j} =\n\\left\\{\\begin{array}{cl}\n0 & \\mbox{si la observación $j$ es de Purnong Landing} \\\\\n1 & \\mbox{si la observación $j$ es de Virginia}\n\\end{array}\\right.\\]Se comienza por ajustar el siguiente modelo: \\[\n\\log(\\text{yield}_j) = \\beta_0 + \\beta_1\\text{location}_{j} + f(\\text{dens}_j) + \\epsilon_j\\]En este ejemplo se ve que en la parte lineal aparecen dos parámetros,\nambos significativos: la ordenada en el origen o intercepto y el coeficiente de la categoría Virginia de la variable location, que es negativo,\nindicando que la producción media en Purnong Landing es mayor que en Virginia. El término de suavizado también es significativo.En este caso, función plot.gam() sólo dibuja una curva, pues las curvas para\nlas dos localizaciones son paralelas y la diferencia entre ellas es igual al valor del parámetro correspondiente localización. Para dibujar las curvas para cada localización se utiliza\nla función plot_smooth() de la librería tidymv. Los argumentos son,\nprimero el modelo, después la variable explicativa y por último la variable categórica.\nFigura 17.6: curvas ajustadas para ambas localidades\nAsumir curvas paralelas para ambas localidades implica que el descenso\nen la producción de cebollas medida que aumenta la densidad de plantas es\nel mismo para ambas localidades, y esto tiene por qué ser cierto.\nPara relajar esta hipótesis se puede ajustar un modelo con interacción\n(de manera similar lo que se hace en el caso de regresión lineal):\n\\[\\log(\\text{yield}_j) =\\beta_0 +  \\beta_1\\text{location}_{j} +  f(\\text{dens}_J){L(j)} + \\epsilon_j\\]\ndonde \\[L(j) =\n\\left\\{\\begin{array}{cl}\n0 & \\mbox{si la $j$-ésima observación es de Purnong Landing} \\\\\n1 & \\mbox{si la $j$-ésima observación es de Virginia}\n\\end{array}\\right.\\] Para hacerlo en R, se introduce el argumento\n=location dentro de la curvaAhora aparecen dos términos suaves, uno para cada localidad, de modo que\nestas curvas tienen por qué ser paralelas, sino que cada una se\najustará la forma que tengan los datos. En este caso, la Fig.\n17.7, generada de nuevo con plot_smooths, muestra como las\ncurvas se van alejando medida que aumenta la densidad de plantas.\nFigura 17.7: Curvas ajustadas para ambas localidades permitiendo que sean paralelas\nPara finalizar se comparan ambos modelos con el criterio AIC.Dado que el menor valor se alcanza en el segundo modelo, se escogería el\nmodelo que incluye la interacción entre la variable densidad y la\nlocalidad.","code":"\nlibrary(\"mgcv\")\nlibrary(\"SemiPar\")\ndata(onions)\n#Se indica a R que la variable locationVirginia es categórica\nonions$location <- factor(onions$location)\n#Se recodifica la variable\nlevels(onions$location) <- c(\"Purnong Landing\",\"Virginia\")\nfit1 <- gam(log(yield) ~ location + s(dens,k=20,bs=\"ps\"), \n            method=\"REML\", select=TRUE, data=onions)\nsummary(fit1)\n#> \n#> Family: gaussian \n#> Link function: identity \n#> \n#> Formula:\n#> log(yield) ~ location + s(dens, k = 20, bs = \"ps\")\n#> \n#> Parametric coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)       4.85011    0.01688  287.39   <2e-16 ***\n#> locationVirginia -0.33284    0.02409  -13.82   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Approximate significance of smooth terms:\n#>           edf Ref.df     F p-value    \n#> s(dens) 4.568     19 72.76  <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> R-sq.(adj) =  0.946   Deviance explained = 94.9%\n#> -REML = -54.242  Scale est. = 0.011737  n = 84\nlibrary(\"tidymv\")\nlibrary(\"ggplot2\")\nplot_smooths(fit1, dens, location) +\n  theme(text = element_text(size = 12))\nfit2 <- gam(log(yield) ~ location + s(dens,k=20,bs=\"ps\",by=location),\n            method=\"REML\", data=onions)\nsummary(fit2)\n#> \n#> Family: gaussian \n#> Link function: identity \n#> \n#> Formula:\n#> log(yield) ~ location + s(dens, k = 20, bs = \"ps\", by = location)\n#> \n#> Parametric coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)       4.84415    0.01603  302.19   <2e-16 ***\n#> locationVirginia -0.33018    0.02270  -14.54   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Approximate significance of smooth terms:\n#>                                   edf Ref.df     F p-value    \n#> s(dens):locationPurnong Landing 3.096  3.834 176.9  <2e-16 ***\n#> s(dens):locationVirginia        4.742  5.795 153.0  <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> R-sq.(adj) =  0.952   Deviance explained = 95.7%\n#> -REML = -58.541  Scale est. = 0.010446  n = 84\nAIC(fit1); AIC(fit2)\n#> [1] -125.2307\n#> [1] -131.2181"},{"path":"cap-gam.html","id":"modelo-aditivo-generalizado-y-multidimensional-con-smacker","chapter":"Capítulo 17 Modelos aditivos generalizados","heading":"17.5.4 Modelo aditivo generalizado y multidimensional con smacker","text":"En este epígrafe se analizan los datos smacker del paquete sm. El objetivo es\nver cómo influyen las condiciones del mar (temperatura de agua, etc.) en\nla ausencia o presencia de huevos de jurel en el mar Cantábrico. Además,\nse incorporará al modelo la posición geográfica mediante las covariables latitud y longitud; de esta forma se podrá captar el efecto espacial.\nFigura 17.8: Área donde se constató la ausencia/presencia de huevos de jurel\nDado que la variable respuesta es dicotómica, se utiliza un modelo\nde regresión logística en el que se flexibiliza la relación lineal de las variables explicativas con la respuesta y, además, se usa una superficie para estimar el efecto de\nla localización como una función en dos dimensiones (latitud y longitud). En este caso, en vez de usar te() se usa s() también para el caso de 2 dimensiones. La\ndiferencia fundamental con te() es que s() asume un suavizado\nisotrópico, es decir, el mismo parámetro de suavizado para la latitud y\nlongitud. se debe usar s() para el suavizado en dos dimensiones si\nlas covariables están medidas en unidades diferentes. En este caso, como tanto la longitud como la latitud están medidas en las mismas unidades, se puede usar el suavizado isotrópico.\nFigura 17.9: Efectos suaves estimados por el modelo para las variables. Efecto de la profundidad y temperatura en la fila superior y efecto espacial en la inferior\nEn la Fig. 17.9 se aprecia que la relación entre la probabilidad de presencia de huevos y la temperatura es lineal, mientras que sí lo es en el caso de la profundidad. El \\(R^2\\) es tan sólo \\(0.4\\), por lo que convendría utilizar más variables explicativas para obtener buenas predicciones.Las probabilidades predichas se pueden obtener con la función predict.","code":"\nlibrary(\"sm\")\ndata(smacker)\nlibrary(\"dplyr\")\nsmacker <- smacker |> \n  mutate(Presence = ifelse(Density>0, 1, 0),\n         smack.long = -smack.long,\n         ldepth = log(smack.depth))\nlibrary(\"maps\")\npar(pty=\"s\")\nPosition <- cbind(smacker$smack.long, smacker$smack.lat)\nplot(Position,col=NULL,xlim=c(-10,-1),ylim=c(43,48),cex=1.2,xlab=\"longitud\", ylab=\"latitud\")\nmap(\"world\",add=TRUE,fill=TRUE,col=\"grey\")\npoints(Position[smacker$Presence==1,],pch=1,cex=.5,col=4)\npoints(Position[smacker$Presence==0,],pch=16,cex=.5,col=2)\nlegend(\"topleft\",c(\"Presencia \", \"Ausencia\"), col=c(4,2),pch=c(1,16),cex=.85)\nlogit1 <- gam(Presence~s(ldepth)+ s(Temperature)+ s(smack.long, smack.lat,k=60), \n              family=binomial, select=TRUE, data=smacker)\nb <- getViz(logit1)\nprint(plot(b, allTerms = T), pages = 1)\nprob=predict(logit1,type=\"response\")"},{"path":"cap-gam.html","id":"resumen-15","chapter":"Capítulo 17 Modelos aditivos generalizados","heading":"Resumen","text":"En este capítulo se introducen los modelos aditivos generalizados.\nEn particular:Se presentan distintos aspectos metodológicos de carácter inferencial en este tipo de modelos.Se muestra el uso de R para llevar cabo su ajuste.Se presentan diversos casos prácticos que ilustran la\nversatilidad de estos modelos para analizar datos complejos.","code":""},{"path":"cap-mxm.html","id":"cap-mxm","chapter":"Capítulo 18 Modelos mixtos","heading":"Capítulo 18 Modelos mixtos","text":"\\(^{}\\)Universidad de Castilla-La Mancha\\(^{b}\\)Universidad Carlos III de Madrid","code":""},{"path":"cap-mxm.html","id":"conceptos-básicos","chapter":"Capítulo 18 Modelos mixtos","heading":"18.1 Conceptos básicos","text":"Los modelos mixtos (MM) para variables de respuesta continuas son\nmodelos estadísticos en los que los residuos siguen una distribución\nNormal pero puede que sean independientes o tengan varianza\nconstante. Son necesarios en muchas situaciones, sobre todo en\nexperimentos donde se realiza algún tipo de muestreo:Estudios con datos agrupados, como por ejemplo, alumnos en una\nclase, individuos en una ciudad.Estudios longitudinales o de medidas repetidas, donde un elemento o individuo\nes medido repetidamente lo largo del tiempo o bajo condiciones\ndistintas.Este tipo de estudios se pueden encontrar en diferentes áreas como la\nmedicina, biología, ciencias experimentales y sociales.","code":""},{"path":"cap-mxm.html","id":"tipo-y-estructura-de-los-datos","chapter":"Capítulo 18 Modelos mixtos","heading":"18.1.1 Tipo y estructura de los datos","text":"La estructura de los datos con la que se trabaja es el factor\ndeterminante para saber si se han de utilizar modelos mixtos, y en su\ncaso, qué tipo de modelo.","code":""},{"path":"cap-mxm.html","id":"datos-jerárquicos-o-agrupados","chapter":"Capítulo 18 Modelos mixtos","heading":"18.1.1.1 Datos jerárquicos (o agrupados)","text":"En este tipo de datos, la variable dependiente (de respuesta, de interés)\nse mide una sola vez en cada unidad de análisis (individuos, objetos, elementos …), y los\nindividuos141\nestán agrupados (o anidados) en unidades mayores. Muchos tipos\nde datos tienen una estructura jerárquica: alumnos en escuelas, personas\nen municipios, pacientes en hospitales, plantas en una parcela…Las jerarquías son una forma de representar la relación de dependencia\nque hay entre los individuos y los grupos los que pertenecen. Por\nejemplo, supóngase que se quiere hacer un estudio sobre el tiempo de\nrecuperación en pacientes hospitalizados por COVID-19 en diferentes\nhospitales. Se tiene la siguiente estructura con dos niveles:Muchos individuos en el nivel \\(1\\) (pacientes).Agrupados en unas pocas unidades en el nivel \\(2\\) (hospitales).En cada nivel de la jerarquía se pueden medir variables. Algunas estarán\nmedidas en su nivel natural; por ejemplo, en el nivel “hospital” se\npodría medir el tamaño y en el nivel “pacientes” situación socio-económica. Además, se pueden mover las variables de un\nnivel otro mediante agregación o desagregación:Agregación: la variable correspondiente al nivel más bajo se mueve un nivel\nmás alto; por ejemplo, se puede asociar cada hospital la media\ndel nivel socioeconómico de sus pacientes.Desagregación: mover las variables un nivel más bajo; por\nejemplo, asignarle cada paciente una variable que indique el\ntamaño de su hospital de referencia.","code":""},{"path":"cap-mxm.html","id":"medidas-repetidas-y-datos-longitudinales","chapter":"Capítulo 18 Modelos mixtos","heading":"18.1.1.2 Medidas repetidas y datos longitudinales","text":"En este tipo de datos, la variable dependiente se mide más de una vez en\nun mismo individuo (Singer Willet 2003). Por ejemplo, se miden los niveles de\nglucosa de un enfermo antes y después de haberle inyectado insulina.\nEste tipo de datos también puede ser considerado como datos multinivel\n(o jerárquicos) donde el nivel 2 representa los individuos y el nivel\n1 las diferentes medidas tomadas. Dado que las medidas se\ntoman un mismo individuo, es probable que sean\nindependientes, por lo que utilizar un modelo lineal ordinario sería\napropiado.Por datos longitudinales se entienden datos en los que la variable\ndependiente se ha medido en distintos instantes de tiempo en cada una de\nlas unidades de análisis. En algunos casos, cuando la variable\ndependiente se mide lo largo del tiempo, puede ser difícil identificar\nsi los datos son medidas repetidas o datos longitudinales. Desde el\npunto de vista del análisis de los datos mediante MM esta distinción \nes un elemento crítico. Lo importante es que en ambos tipos de datos la\nvariable dependiente se ha medido repetidas veces en la misma unidad de\nanálisis, y que, por tanto, las observaciones son independientes.","code":""},{"path":"cap-mxm.html","id":"efectos-fijos-o-aleatorios","chapter":"Capítulo 18 Modelos mixtos","heading":"18.1.2 ¿Efectos fijos o aleatorios?","text":"En un modelo mixto la clave se encuentra en la distinción entre efectos\nfijos y aleatorios (Snijers 2003). Esto es importante porque la inferencia y\nel análisis de ambos efectos es distinta.Los efectos fijos son variables en las cuales el investigador ha\nincluido sólo los niveles (o tratamientos) que son de su interés. Por\nejemplo, en un experimento se puede estar interesado en comparar dos\ngrupos, uno al que se le aplica un tratamiento y otro de control. En\neste caso, el estudio compara los grupos y interesa\ngeneralizar los resultados otros tratamientos que podrían haber sido\nincluidos. Otro ejemplo sería el caso en el que se hace una encuesta y se\neligen 10 ciudades. Si sólo interesan los resultados para esas 10\nciudades y se quieren generalizar al resto de ciudades\nque podrían haber sido seleccionadas, la variable ‘ciudad’ es un\nefecto fijo. Si se eligen las ciudades de forma aleatoria de una\npoblación grande de ciudades, la variable ‘ciudad’ es un efecto aleatorio.Una cantidad se considera aleatoria cuando cambia sobre las unidades de\nuna población. Cuando un efecto en un modelo estadístico es considerado\naleatorio, se está asumiendo que se quieren extraer conclusiones sobre\nla población de la cual se han elegido las unidades observadas, y se\ntiene interés en esas unidades en particular. En este contexto se habla\nde intercambiabilidad, en el sentido de que se podría cambiar una\nunidad de la muestra por otra de la población y sería indiferente. Este\nes el caso de los factores de agrupamiento o diseño, como las\nparcelas en un experimento agrícola, o los días cuando un experimento se\nlleva cabo en días distintos, o el técnico de laboratorio cuando hay\nvarios haciendo el experimento; también lo serían los sujetos en un\ndiseño de medidas repetidas o las localizaciones donde se recogen\nmuestras en un río, si el objetivo es generalizar todo el río.Los métodos estándar utilizados para construir tests e intervalos de\nconfianza para los efectos fijos son válidos para los efectos\naleatorios, pues en este último caso los efectos observados son sólo una muestra de todos\nlos posibles efectos.La clave para distinguir, estadísticamente hablando, entre efectos fijos\ny aleatorios, es si los niveles de la variable se pueden interpretar como\nextraídos de una población con una cierta distribución de probabilidad.\nEn el caso de un efecto fijo, normalmente interesa comparar los\nresultados de la variable dependiente para los distintos niveles de la\nvariable explicativa, es decir, interesa la diferencia entre las\nmedias. En el caso de efectos aleatorios, interesa específicamente\ncomparar si las medias son distintas, sino cómo el efecto aleatorio\nexplica la variabilidad en la variable dependiente. Por lo tanto, para\nque un efecto pueda considerarse aleatorio, es necesario que la variable\ndependiente presente cierta variabilidad explicada asociada con las\nunidades del efecto aleatorio.La Fig. 18.1 puede ayudar determinar si un efecto es fijo\no aleatorio:\nFigura 18.1: Cuestiones para determinar si un efecto es fijo o aleatorio\nPor ejemplo, en un estudio sobre satisfacción en el trabajo (variable\ndependiente) de los empleados (unidades observadas) de un cierto número\nde empresas (efecto aleatorio), si el nivel de satisfacción de los\nempleados de unas empresas es mayor que el de otras y el investigador \nlo tiene en cuenta, habrá una cierta variabilidad residual asociada con\nel efecto ‘empresa’. Si esta variabilidad fuera próxima cero, sería\nnecesario incluir el efecto aleatorio asociado con la empresa.¿Por qué hay que utilizar modelos mixtos?Cuando las observaciones están agrupadas en niveles o siguen una cierta\njerarquía, las unidades se ven afectadas por el grupo al que pertenecen.\nLas jerarquías (o niveles) permiten representar la relación de\ndependencia entre los individuos y los grupos los que pertenecen. Los\nalumnos que están en una misma escuela se parecen más entre sí que si se\nhubieran seleccionado aleatoriamente de entre toda la población de\nalumnos. Los modelos mixtos permiten tener en cuenta que las\nobservaciones son independientes.","code":""},{"path":"cap-mxm.html","id":"formulación-del-modelo-con-efectos-aleatorios-o-modelos-mixtos","chapter":"Capítulo 18 Modelos mixtos","heading":"18.2 Formulación del modelo con efectos aleatorios o modelos mixtos","text":"El nombre de modelos mixtos lineales viene del hecho de que estos\nmodelos son lineales en los parámetros y en las covariables y pueden\nimplicar efectos fijos o aleatorios. Son, por lo tanto, una extensión de\nlos modelos lineales de regresión.","code":""},{"path":"cap-mxm.html","id":"formulación-general","chapter":"Capítulo 18 Modelos mixtos","heading":"18.2.1 Formulación general","text":"La formulación general de un modelo mixto tiene la siguiente forma:\n\\[\\begin{equation}\n\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{Z} \\boldsymbol{u} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{u}\\sim N(0, \\boldsymbol{G}), \\quad \\boldsymbol{\\epsilon} \\sim N(0, \\boldsymbol{R}),\n\\tag{18.1}\n\\end{equation}\\]\ndonde:\\(\\boldsymbol{X}\\) es una matriz \\(n \\times k\\) (\\(k\\) es el número de efectos fijos).\\(\\boldsymbol{Z}\\) es una matriz \\(n \\times p\\) (\\(p\\) es el número de efectos\naleatorios).\\(\\boldsymbol{\\beta}\\) es el vector de efectos fijos y \\(\\boldsymbol{u}\\) el de efectos aleatorios.\\(\\boldsymbol{G}\\) es la matriz de varianzas-covarianzas de los efectos aleatorios,\ncon dimensión \\(p \\times p\\).\\(\\boldsymbol{R}\\) es la matriz de varianzas-covarianzas del error.","code":""},{"path":"cap-mxm.html","id":"estimación-de-boldsymbolbeta-y-boldsymbolu","chapter":"Capítulo 18 Modelos mixtos","heading":"18.2.1.1 Estimación de \\(\\boldsymbol{\\beta}\\) y \\(\\boldsymbol{u}\\)","text":"Se hace mediante las llamadas ecuaciones de Henderson (Henderson 1953). Permiten\nobtener el mejor estimador lineal insesgado de \\(\\boldsymbol{\\beta}\\) y el mejor\npredictor lineal insesgado de \\(\\boldsymbol{u}\\). Se obtienen maximizando la densidad\nconjunta de \\(\\boldsymbol{y}\\) y \\(\\boldsymbol{u}\\):\n\\[\\begin{equation}\nf(\\boldsymbol{y},\\boldsymbol{u}) = f(\\boldsymbol{y} |\\boldsymbol{u})f(\\boldsymbol{u}), \\quad \\boldsymbol{y} | \\boldsymbol{u} \\sim\n    N(\\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{Z}\\boldsymbol{u}, \\boldsymbol{R})\\quad \\boldsymbol{u}\\sim N(0, \\boldsymbol{G}).\n    \\tag{18.2}\n    \\end{equation}\\]\nDerivando con respecto \\(\\boldsymbol{\\beta}\\) y \\(\\boldsymbol {u}\\) e igualando cero se obtienen las ecuaciones de\nHenderson, cuya solución es:\n\\[\\begin{eqnarray}\n\\hat{\\boldsymbol{\\beta}}&=& \\left ( \\boldsymbol{X}^\\prime\\boldsymbol{V}^{-1}\\boldsymbol{X} \\right )^{-1} \\boldsymbol{X}^\\prime\\boldsymbol{V}^{-1} \\boldsymbol {y} \\\\\n\\hat{\\boldsymbol{u}} &=&\\boldsymbol{G}\\boldsymbol{Z}^\\prime\\boldsymbol{V}^{-1} ( \\boldsymbol{y} -\\boldsymbol{X} \\hat{\\boldsymbol{\\beta}}),\n\\tag{18.3}\n\\end{eqnarray}\\]donde \\(\\boldsymbol{V} =\\boldsymbol{Z}\\boldsymbol{G}\\boldsymbol{Z}^\\prime + \\boldsymbol{R}\\). Sin embargo, \\(\\boldsymbol{V}\\) depende de los parámetros de la\nvarianza en el modelo, que forman parte de \\(\\boldsymbol{G}\\) y \\(\\boldsymbol{R}\\) y que es necesario\nestimar, como se muestra continuación.","code":""},{"path":"cap-mxm.html","id":"estimación-de-los-componentes-de-la-varianza","chapter":"Capítulo 18 Modelos mixtos","heading":"18.2.1.2 Estimación de los componentes de la varianza","text":"Los métodos más comunes para la estimación de los parámetros de las\nmatrices de covarianzas son: máxima verosimilitud (ML) y máxima\nverosimilitid restringida (REML). existe una solución cerrada para los estimadores, y se estiman de forma\nnumérica o mediante algoritmos iterativos. REML tiene en cuenta los grados de libertad utilizados para estimar los\nefectos fijos en el modelo. Si \\(n\\) es pequeño, REML dará mejores\nestimaciones que ML; si \\(n\\) es grande, habrá prácticamente ninguna\ndiferencia. El método preferido es REML.","code":""},{"path":"cap-mxm.html","id":"inferencia-y-selección-del-modelo","chapter":"Capítulo 18 Modelos mixtos","heading":"18.2.2 Inferencia y selección del modelo","text":"","code":""},{"path":"cap-mxm.html","id":"contrastes-de-hipótesis-para-los-efectos-fijos-boldsymbolbeta","chapter":"Capítulo 18 Modelos mixtos","heading":"18.2.2.1 Contrastes de hipótesis para los efectos fijos, \\(\\boldsymbol{\\beta}\\)","text":"Utilizando la distribución aproximada:\n\\[\\hat{\\boldsymbol{\\beta}} \\sim N \\left ( \\boldsymbol{\\beta}, \\underbrace{(  \\boldsymbol{X}^\\prime \\hat{ \\boldsymbol{V}}^{-1} \\boldsymbol{X} )^{-1}}_{Var(\\hat{\\boldsymbol{\\beta}})} \\right )\\]Si se contrastan parámetros individuales, se utiliza el t-test para\nun solo efecto.Si se contrasta un conjunto de parámetros, se utiliza el F-test para\nmás de un efecto.También se pueden comparar modelos usando el test de la razón de\nverosimilitud, LRT por sus siglas en inglés:\n\\[\\begin{equation}\nLRT = -2\\left [ ln(l_{H_0})- ln(l_{H_1}) \\right ]\\approx \\chi^2_{df}.\n\\tag{18.4}\n\\end{equation}\\]NotaEn este caso hay que utilizar ML para estimar los parámetros de la varianza.","code":""},{"path":"cap-mxm.html","id":"contrastes-de-hipótesis-para-los-parámetros-de-varianza","chapter":"Capítulo 18 Modelos mixtos","heading":"18.2.2.2 Contrastes de hipótesis para los parámetros de varianza","text":"Al usar el test LRT (Eq. (18.4)) se ha de tener en cuenta\nque la distribución asintótica del estadístico del test depende de si el\nvalor del parámetro bajo la hipótesis nula (\\(H_0\\)) está o en la frontera del\nespacio paramétrico142Caso 1: El valor de los parámetros de varianza bajo \\(H_0\\) \nestá en la frontera del espacio paramétrico (por ejemplo, al\ncontrastar si los parámetros de varianza de dos efectos aleatorios\nson iguales o ). En ese caso se utiliza el test normalmente.Caso 1: El valor de los parámetros de varianza bajo \\(H_0\\) \nestá en la frontera del espacio paramétrico (por ejemplo, al\ncontrastar si los parámetros de varianza de dos efectos aleatorios\nson iguales o ). En ese caso se utiliza el test normalmente.Caso 2: El valor de los parámetros de varianza bajo \\(H_0\\)\nestá en la frontera del espacio paramétrico (por ejemplo, si se\nquiere contrastar si la varianza del efecto aleatorio es cero o\n). La distribución asintótica del estadístico del test es una\nmixtura entre \\(\\chi^2_p\\) y \\(\\chi^2_{p-1}\\),\nconcretamente \\(0.5 \\chi^2_p + 0.5\\chi^2_{p-1}\\),\ndonde \\(p\\) es el número de parámetros de la varianza que se hacen\ncero bajo la \\(H_0\\).Caso 2: El valor de los parámetros de varianza bajo \\(H_0\\)\nestá en la frontera del espacio paramétrico (por ejemplo, si se\nquiere contrastar si la varianza del efecto aleatorio es cero o\n). La distribución asintótica del estadístico del test es una\nmixtura entre \\(\\chi^2_p\\) y \\(\\chi^2_{p-1}\\),\nconcretamente \\(0.5 \\chi^2_p + 0.5\\chi^2_{p-1}\\),\ndonde \\(p\\) es el número de parámetros de la varianza que se hacen\ncero bajo la \\(H_0\\).","code":""},{"path":"cap-mxm.html","id":"diagnosis-del-modelo","chapter":"Capítulo 18 Modelos mixtos","heading":"18.2.3 Diagnosis del modelo","text":"En el caso de modelos mixtos, se ha de contrastar la hipótesis de\nnormalidad tanto para los residuos al nivel más bajo como para los efectos aleatorios.En este tipo de modelos se utilizan los residuos\ncondicionales, que son la diferencia entre los valores observados y el\nvalor predicho condicional:\n\\[\\hat\\epsilon = y-X \\hat \\beta -Z \\hat u. \\]\nEstos residuos tienden estar correlados y sus varianzas pueden cambiar\nde un grupo otro, aunque en el verdadero modelo los residuos están\nincorrelados y tienen varianza constante. Para solucionar este problema se\npueden escalar los residuos por sus desviaciones estándar (o las\nestimaciones de éstas), dando lugar los residuos estandarizados (si\nlas desviaciones estándar son conocidas), o los residuos studentizados (si son desconocidas y se utilizan estimaciones de las\nmismas). Con estos residuos se hace un análisis similar al caso de los\nmodelos de regresión lineal.","code":""},{"path":"cap-mxm.html","id":"procedimiento-con-r-para-ajustar-modelos-mixtos","chapter":"Capítulo 18 Modelos mixtos","heading":"18.3 Procedimiento con R para ajustar modelos mixtos","text":"Hay varios paquetes de R para el ajuste de modelos mixtos. Los más usados son nlme y lme4. El segundo es una versión del primero que\nincluye modelos más generales y mejora los gráficos. continuación se\ndescribe la función principal del paquete lme4.","code":""},{"path":"cap-mxm.html","id":"la-función-lmer","chapter":"Capítulo 18 Modelos mixtos","heading":"18.3.1 La función lmer()","text":"Esta función permite el uso de efectos aleatorios anidados y de errores\ncorrelados y/o heterocedásticos dentro de los grupos. En general, para\ndefinir un modelo mixto se necesita especificar la estructura de la\nmedia y de la parte aleatoria del modelo, incluidos los factores de\nagrupamiento, así como la estructura de correlación (si la hay).También se puede especificar el método de estimación: ‘’REML’’ o ‘’ML’’.La parte aleatoria del modelo se incluye entre paréntesis en la ecuación y “\\(|\\)” separa las variables de agrupamiento de las\npredictoras. Si hay variables predictoras para la parte aleatoria se pone un 1.La función VarCorr() aplicada un objeto lmer proporciona información sobre la estructura de componentes de varianza.","code":""},{"path":"cap-mxm.html","id":"caso-práctico","chapter":"Capítulo 18 Modelos mixtos","heading":"18.4 Caso práctico","text":"En esta sección se comienza viendo cómo construir diferentes modelos con efectos aleatorios según qué nivel estén medidas las variables explicativas y se termina dando una guía de construcción de estos modelos en la práctica. Los datos con los que se va trabajar se encuentran en el dataframe Hsb82 del paquete mlmRev y provienen de un estudio titulado High School Beyond. Los datos corresponden 7.185 estudiantes repartidos en 160 escuelas,\nel número de alumnos por escuela varía entre 14 y 67.\nLa variable de interés, mAch, es el nivel estandarizado alcanzado en\nmatemáticas. Una cuestión inicial que se puede plantear es si el nivel socioeconómico (cses)\ndel alumno predice las diferencias en el nivel de matemáticas. Para ello\nse ajusta el modelo:\n\\[y_j = \\beta_0 + \\beta_1x_j + \\epsilon_j, \\]\nque ignora que los alumnos provienen de distintos centros (por\neso solo aparece el subíndice \\(j\\), que es el que representa las unidades\nde nivel más bajo, en este caso los alumnos).La ordenada en el origen es 12.75 y la pendiente 2.19, lo que indica que\npor cada unidad que aumenta el nivel socio-económico, la puntuación del\ntest aumenta en 2.19 unidades; además se puede ver que el coeficiente es\nsignificativo.Supóngase que ocurre la situación mostrada en la Fig. 18.2:\nFigura 18.2: Ilustración de posibles escenarios para dos escuelas\nLos alumnos de la escuela (rombos negros) sacan, en promedio, mejores notas que las\nque le asigna el modelo ajustado; con la escuela B (círculos azules) ocurre lo\ncontrario. El gráfico indica que la ordenada en el origen 8el intercepto) debería ser\nla misma para todos los centros, sino que debería ser distinta para\ndistintos centros. Es decir, el valor predicho debe ajustarse hacia\narriba o hacia abajo. Eso se puede conseguir permitiendo que cada escuela\ntenga su propia ordenada en el origen:\n\\[y_{ij} = \\beta_{0i} + \\beta_1x_{ij} + \\epsilon_{ij}\\]Este modelo es similar al anterior añadiendo el subíndice \\(\\) para\nidentificar el centro al que pertenece cada alumno. En realidad, se\nutiliza una variable categórica con tantas categorías como escuelas.Se están considerando las escuelas como un efecto fijo y aleatorio,\nes decir, implícitamente se está suponiendo que solo interesan estas\nescuelas en particular.La situación se pueden complicar más: es posible que el efecto del nivel\nsocio-económico sea distinto para cada centro, es decir, que un aumento\nde una unidad en ese nivel pueda dar lugar un aumento distinto en la\nnota del test en cada centro. En la Fig. 18.3 se ve como la\npendiente de la recta para la escuela C es distinta la dos\nanteriores.\nFigura 18.3: Ilustración de posibles escenarios para tres escuelas\nEl modelo que permite tener en cuenta esta situación es:\n\\[y_{ij} = \\beta_{0i} + \\beta_{1i}x_{ij} + \\epsilon_{ij}\\]\ndonde aparece ahora el sub-índice \\(\\) también en la pendiente, lo que indica que cada centro\ntiene una pendiente diferente.El códigogeneraría 159 coeficientes más (uno por cada escuela), que son los que se incluirían con\nla interacción. Pero interesan estas escuelas en\nconcreto, sino la población de la que estas escuelas son una muestra.Con un modelo con efectos aleatorios, sin embargo, se pueden contestar preguntas como: ¿Cuáles\nson las causas de esta variabilidad? ¿Qué variables pueden explicarla?","code":"\nlibrary(\"mlmRev\")\nHsb82$school = factor(Hsb82$school, ordered=F)\nmulti0 <- lm(mAch ~ cses, data = Hsb82)\nsummary(multi0)\n#> \n#> Call:\n#> lm(formula = mAch ~ cses, data = Hsb82)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -17.8660  -5.1165   0.2966   5.3880  14.8705 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 12.74785    0.07933  160.69   <2e-16 ***\n#> cses         2.19117    0.12010   18.24   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 6.725 on 7183 degrees of freedom\n#> Multiple R-squared:  0.04429,    Adjusted R-squared:  0.04415 \n#> F-statistic: 332.8 on 1 and 7183 DF,  p-value: < 2.2e-16\nmulti1 <- lm(mAch ~ cses + school, data = Hsb82)\nmulti2 <- lm(mAch ~ cses*school, data = Hsb82)"},{"path":"cap-mxm.html","id":"modelo-con-ordenada-en-el-origen-aleatoria","chapter":"Capítulo 18 Modelos mixtos","heading":"18.4.1 Modelo con ordenada en el origen aleatoria","text":"Es el modelo mixto más sencillo. Se considera que los datos tienen\nuna estructura con dos niveles: los alumnos están en el nivel 1 y están\nagrupados en escuelas, nivel 2. Se empieza suponiendo que se\ndispone de ninguna variable explicativa, y que por lo tanto el\núnico interés es la diferencia entre las notas medias del test de\nmatemáticas en los distintos centros.Los dos niveles del modelo son:\n\\[\\text{Nivel 1: } y_{ij} = \\beta_{0i} + \\epsilon_{ij}\\]El subíndice \\(j\\) corresponde alumnos y el \\(\\) escuelas, si se\nconsidera las escuelas como un efecto aleatorio,\\(\\beta_{0i}\\) (la media de cada escuela) vendría dada por:\n\\[\\text{Nivel 2: } \\beta_{0i} = \\beta_{0} + u_{},\\]\\(\\beta_{0}\\) es la media de todos los alumnos,\\(u_{}\\) es la desviación de la media de la escuela \\(\\) respecto de la media\nde todas las escuelas.Poniendo las dos ecuaciones juntas:\n\\[\\begin{equation}\ny_{ij} = \\beta_{0} + u_{} + \\epsilon_{ij}, \\quad = 1, \\ldots, m, \\quad j = 1, \\ldots n_m\n\\tag{18.5}\n\\end{equation}\\]La media de \\(y\\) para el grupo \\(\\) es \\(\\beta_0 + u_i\\),Los residuos nivel individual \\(\\epsilon_{ij}\\) son la diferencia\nentre el valor de la variable respuesta del individuo \\(j\\) y la media\ndel grupo al que pertenece,\\(u_i\\sim N( 0, \\sigma^2_u )\\), \\(\\epsilon_{ij}\\sim N(0, \\sigma^2 )\\), y\nambos son independientes, es decir, las observaciones que provienen\nde distintas escuelas son independientes.En el ejemplo de las escuelas:La media total estimada es 12.64,La media estimada para la escuela \\(\\) es: 12.64\\(+ \\hat u_i\\), donde \\(\\hat u_i\\)\nes el efecto aleatorio predicho para dicha escuela.Para obtener los valores predichos de los efectos aleatorios, y ver si siguen una distribución Normal, se utiliza la función ranef().La Fig. 18.4**} permite ver los efectos aleatorios junto con sus intervalos de\nconfianza (las escuelas han sido ordenadas atendiendo su media para\napreciar mejor la variabilidad entre las mismas). Para ello se ajusta el modelo con la función lmer():\nFigura 18.4:  Efectos aleatorios junto con sus intervalos de confianza\nLa etiqueta sería Fig. 18.4Una primera aproximación para contrastar si hay o diferencias entre\nlos grupos sería calcular el intervalo de confianza para \\(\\sigma_u\\):pudiéndose apreciar que el intervalo para sig01 contiene al cero. Sin embargo, la forma más correcta de hacerlo\nsería utilizando el LRT (véase Eq. (18.4)) con:\n\\[\\begin{array}{ll}\nH_0: \\quad \\sigma^2_u = 0 \\Rightarrow y_{ij} = \\beta_0 + \\epsilon_{ij} \\\\\nH_1: \\quad \\sigma^2_u\\neq 0 \\Rightarrow y_{ij} = \\beta_0 + u_i + \\epsilon_{ij}.\n\\end{array}\\]El resultado del test, se compara con el valor de una\nmixtura de distribuciones Chi-cuadrado \\(0.5 \\chi^2_0 + 0.5\\chi^2_1\\), concretamente \\(0.5 \\chi^2_p + 0.5\\chi^2_{p-1}\\), donde \\(p\\) es el número de parámetros de la varianza que se hacen\ncero bajo \\(H_0\\).Conclusión: el efecto aleatorio es necesario en el modelo.El siguiente paso sería introducir las variables explicativas (en este caso solo hay una), ya estén\nal nivel 1 o al 2.","code":"\nlibrary(\"lme4\")\nModelo0 <- lmer(mAch ~ 1+(1 | school), data = Hsb82)\nModelo0\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: mAch ~ 1 + (1 | school)\n#>    Data: Hsb82\n#> REML criterion at convergence: 47116.79\n#> Random effects:\n#>  Groups   Name        Std.Dev.\n#>  school   (Intercept) 2.935   \n#>  Residual             6.257   \n#> Number of obs: 7185, groups:  school, 160\n#> Fixed Effects:\n#> (Intercept)  \n#>       12.64\nlibrary(\"lattice\")\nqqmath(ranef(Modelo0, condVar = TRUE))$school\nconfint(Modelo0)\n#>                 2.5 %    97.5 %\n#> .sig01       2.594729  3.315880\n#> .sigma       6.154803  6.361786\n#> (Intercept) 12.156289 13.117121\nModelo_NULL <- lm(mAch ~ 1, data = Hsb82)\ntest = -2*logLik(Modelo_NULL, REML = T) + 2*logLik(Modelo0, REML = T)\nmean(pchisq(test, df = c(0, 1), lower.tail = F))  \n#> [1] 9.320673e-217"},{"path":"cap-mxm.html","id":"variables-explicativas-en-el-nivel-1-individuos","chapter":"Capítulo 18 Modelos mixtos","heading":"18.4.1.1 Variables explicativas en el nivel \\(1\\) (individuos)","text":"Como la variable explicativa está medida al nivel 1, se introduce en la\necuación del nivel 1:\\(\\text{Nivel 1:}\\quad y_{ij} = \\beta_{0i} + \\beta_1x_{ij} + \\epsilon_{ij}\\)\\(\\text{Nivel 2:} \\quad \\beta_{0i} = \\beta_0 + u_i\\)Si \\(X\\) es una variable continua, este modelo asume que la pendiente de\nla recta es la misma para todas las escuelas (por eso \\(\\beta_1\\) lleva\nel subíndice \\(\\)). Poniendo las dos ecuaciones juntas:\n\\[y_{ij} = \\underbrace{\\beta_0 + \\beta_1x_{ij}}_{\\text{efectos fijos}} +\n\\underbrace{u_{} + \\epsilon_{ij}}_{\\text{efectos aleatorios}}\\]En este modelo, la relación global entre \\(Y\\) y \\(X\\) viene representada\npor la línea recta con ordenada en el origen \\(\\beta_0\\) y pendiente\n\\(\\beta_1\\). Sin embargo, la ordenada en el origen para una determinada\nescuela \\(\\) viene dada por \\(\\beta_0 + u_i\\). Será mayor o menor que que la\nordenada en el origen global \\(\\beta_0\\) en una cantidad \\(u_i\\). Aunque la\nordenada en el origen varía de grupo grupo, la pendiente es la misma\npara todos los grupos. Todas las líneas rectas ajustadas para cada grupo\nson paralelas.En el ejemplo de las escuelas, se introduce como variable explicativa\ncses (nivel socioeconómico centrado en su media):Ahora se tienen dos efectos fijos:\n\\[\\begin{array}{l}\n\\hat \\beta_0 = 12.64\\\\\n\\hat \\beta_1 = 2.19\\\\\n\\end{array}\\]\\(\\hat \\beta_0\\) es la nota media para alumnos con nivel socieconómico\nmedio (la variable está centrada). La recta media vendría dada por:\n\\[E[y | cses] = 12.64 + 2.19~cses\\]Para contrastar si la variable cses es significativa se utiliza el\nLRT (Eq. (18.4)). Primero se tienen que\najustar de nuevo los modelos que se quieren comparar usando máxima\nverosimilitud (en vez de REML). Si se utiliza la función lmer() para\najustar el modelo es necesario reajustar con ML pues la función anova\nlo hará automáticamente, mientras que si se usa la función lme() sí será necesario hacerlo.Por lo tanto, el nivel socioeconómico afecta los resultados escolares.\nComparado con el modelo sin la variable explicativa (Modelo0), la\ninclusión del nivel socio-económico (Modelo1) ha reducido la\nvariabilidad nivel del alumno en un 2.8% (6.084 - 6.257)/6.257 = -0.028).","code":"\nModelo1 <- lmer(mAch ~ cses+(1 | school), data = Hsb82)\nModelo1\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: mAch ~ cses + (1 | school)\n#>    Data: Hsb82\n#> REML criterion at convergence: 46724\n#> Random effects:\n#>  Groups   Name        Std.Dev.\n#>  school   (Intercept) 2.945   \n#>  Residual             6.084   \n#> Number of obs: 7185, groups:  school, 160\n#> Fixed Effects:\n#> (Intercept)         cses  \n#>      12.636        2.191\nanova(Modelo0, Modelo1)\n#> Data: Hsb82\n#> Models:\n#> Modelo0: mAch ~ 1 + (1 | school)\n#> Modelo1: mAch ~ cses + (1 | school)\n#>         npar   AIC   BIC logLik deviance Chisq Df Pr(>Chisq)    \n#> Modelo0    3 47122 47142 -23558    47116                        \n#> Modelo1    4 46728 46756 -23360    46720 395.4  1  < 2.2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"cap-mxm.html","id":"variables-explicativas-en-el-nivel-2-grupos","chapter":"Capítulo 18 Modelos mixtos","heading":"18.4.1.2 Variables explicativas en el nivel \\(2\\) (grupos)","text":"Si las variables explicativas se miden al nivel 2, entonces:\n\\[\\begin{array}{l}\n\\text{Nivel 1:}\\quad y_{ij} = \\beta_{0i} + \\epsilon_{ij} \\\\\n\\text{Nivel 2:}\\quad \\beta_{0i} = \\beta_0 + \\beta_{2}s_{} + u_{}\\\\\n\\\\\ny_{ij} = \\underbrace{\\beta_0 + \\beta_2s_{}}_{\\text{efectos fijos}}+\\underbrace{u_{} + \\epsilon_{ij}}_{\\text{efectos aleatorios}}.\\\\\n\\end{array}\\]En nuestro ejemplo, la variable utilizada es sector (público o\ncatólico):\n\\[mAch = \\beta_0 + \\beta_2~sector + u_{} + \\epsilon_{ij}\\]Se ajusta el modelo usando la función lmer():\\[E[y | sector] = 11.39 + 2.8~sector,\\]\no equivalentemente\n\\[\\begin{array}{l}\nE[y | sector = 0] = 11.39\\\\\nE[y | sector = 1] = 11.39 + 2.8 = 14.19.\\\\\n\\end{array}\\]\nLa nota de un alumno en una escuela católica se espera que sea 2.8 unidades\nmayor que la de un alumno en una escuela pública (este resultado sólo es válido para las escuelas de la muestra sino que se puede generalizar todas las escuelas,\npues se asume que las escuelas son un efecto aleatorio). La varianza del\nefecto aleatorio de nivel 2, \\(\\sigma^2_u\\), ha\ndescendido: \\((2.935^2-2.584^2)/2.935^2 =\\) \\(0.22\\), es decir, al introducir\nla variable sector la variabilidad nivel de escuela se ha reducido en\nun \\(22 \\%\\).Para contrastar si la variable sector es significativa se usa de nuevo el test LRT:Por lo tanto, el hecho de que la escuela sea pública o católica influye\nen el resultado académico de los alumnos.","code":"\nModelo2 <- lmer(mAch ~ sector + (1 | school), data = Hsb82)\nModelo2\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: mAch ~ sector + (1 | school)\n#>    Data: Hsb82\n#> REML criterion at convergence: 47080.13\n#> Random effects:\n#>  Groups   Name        Std.Dev.\n#>  school   (Intercept) 2.584   \n#>  Residual             6.257   \n#> Number of obs: 7185, groups:  school, 160\n#> Fixed Effects:\n#>    (Intercept)  sectorCatholic  \n#>         11.393           2.805\nanova(Modelo0, Modelo2)\n#> Data: Hsb82\n#> Models:\n#> Modelo0: mAch ~ 1 + (1 | school)\n#> Modelo2: mAch ~ sector + (1 | school)\n#>         npar   AIC   BIC logLik deviance  Chisq Df Pr(>Chisq)    \n#> Modelo0    3 47122 47142 -23558    47116                         \n#> Modelo2    4 47087 47115 -23540    47079 36.705  1  1.374e-09 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"cap-mxm.html","id":"modelo-con-pendiente-aleatoria","chapter":"Capítulo 18 Modelos mixtos","heading":"18.4.2 Modelo con pendiente aleatoria","text":"En este tipo de modelos se supone que la relación entre la variable\nrespuesta y las variables explicativas es distinta para las\ndistintas unidades de nivel 2, es decir, la relación puede cambiar de un\ncentro educativo otro. Por ejemplo, el efecto del nivel socioeconómico en las\nnotas puede ser distinto en distintos centros, de modo que se puede\nrelajar el modelo anterior, en el que la pendiente era la misma para\ntodos los grupos, permitiendo que la pendiente varíe aleatoriamente\nentre los grupos.\n\\[\\begin{array}{ll}\n\\text{Nivel 1:} &\\quad y_{ij} = \\beta_{0i} + \\beta_{1i}x_{ij} + \\epsilon_{ij} \\\\\n\\text{Nivel 2:} & \\quad \\beta_{0i} = \\beta_0 + u_{}\\\\\n& \\quad \\beta_{1i} = \\beta_{1} + v_{} \\\\\n\\end{array}\\]\nPoniendo las dos ecuaciones juntas:\n\\[y_{ij} = \\underbrace{\\beta_0 + \\beta_{1}x_{ij}}_{\\text{efectos\nfijos}} +\n\\underbrace{u_{} + v_{}x_{ij} + \\epsilon_{ij}}_{\\text{efectos aleatorios}}, \\quad  \\left (\\begin{array}{c} u_{}\\\\ v_{}\\\\\n  \\end{array}\\right ) \\sim N(0, \\boldsymbol{G}_{}), \\quad \\boldsymbol{G}_{} = \\left\n( \\begin{array}{cc}\n\\sigma^2_{u} &\\\\\n\\sigma_{uv}& \\sigma^2_{v}\\\\\n\\end{array}\\right ), \\]donde \\(\\sigma_{uv}\\) es la covarianza entre las ordenadas en el\norigen y las pendientes de los grupos (\\(\\beta_{0i}\\) y \\(\\beta_{0i}\\), respectivamente). Un valor positivo de la\ncovarianza implica que los grupos con un valor del efecto de grupo \\(u_i\\)\nelevado tienden tener valores elevados de \\(v_i\\), o equivalentemente,\ncentros educativos con ordenada en el origen alta, tienen pendiente alta.El modelo en R sería:El efecto del nivel socieconómico en la escuela \\(\\) se estima como\n\\(2.19 + \\hat u_i\\), y la varianza de las pendientes para las escuelas es\n\\(0.833^2 = 0.694\\). Para la escuela promedio se predice un aumento de\n\\(2.19\\) en la puntuación cuando el nivel socioeconómico aumenta en una\nunidad.Ahora se tienen las siguientes estimaciones de los parámetros de varianza:\n\\[\\hat \\sigma^2_{u} = 8.68\\quad \\hat \\sigma^2_{v} = 0.694\\quad\n\\hat \\sigma_{uv} = \\rho \\sigma_{u} \\sigma_{v} = 0.051\\quad\n\\hat \\sigma^2 = 36.7\\]\nLa varianza de la ordenada en el origen estimada, \\(8.68\\), se interpreta como\nla variabilidad (de la nota) entre las escuelas para un nivel socioeconómico medio (valor nulo de la variable por estar centrada).El parámetro de covarianza estimado es \\(\\sigma_{uv}=0.051\\), por lo que se puede plantear si es necesario o .Para comprobarlo, el contraste de hipótesis sería en este caso:\n\\[H_0:\n\\sigma_{uv} = 0 \\quad \\text{ y } \\quad H_1: \\sigma_{uv}\\neq 0\\]Cuando se quiere que haya un efecto aleatorio para la ordenada en el\norigen y para la pendiente pero que estén incorrelados, en la función\nsolo hay que poner doble barra en vez\nde simple.En este caso es necesario utilizar la mixtura de distribuciones Chi-cuadrado para contrastar \\(H_0:\\)\n\\(\\sigma_{uv} = 0\\), pues \\(\\sigma_{uv}\\) puede tomar cualquier valor.Por lo tanto, se puede suponer que la covarianza es 0.El siguiente paso sería contrastar si es necesario que las rectas tengan\npendientes diferentes, es decir, \\(H_0\\): \\(\\sigma^2_v = 0\\), \\(H_1\\):\n\\(\\sigma^2_v>0\\). En este caso sí se necesita la aproximación:Por lo tanto, la pendiente es diferente en las distintas escuelas.Además, se puede usar algún criterio de información para comparar los\nmodelos:veces la covariable medida nivel 2 (nivel de grupo, en este caso\nescuelas) puede explicar tanto la variabilidad de la ordenada en el\norigen como de la pendiente:\n\\[\\begin{array}{ll}\n  \\text{Nivel 1:} &\\quad y_{ij}=\\beta_{i0}+\\beta_{1i}x_{ij}+\\epsilon_{ij} \\\\\n\\text{Nivel 2:} & \\quad \\beta_{i0}=\\beta_0+\\beta_{2}s_i+u_{}\\\\\n& \\quad \\beta_{1i}=\\beta_{1}+\\beta_{3}s_i +v_{} \\\\\n\\end{array}\\]\n\\[y_{ij} = \\underbrace{\\beta_0 +\\beta_1x_{ij} + \\beta_{2}s_{}+\\beta_{3} x_{ij}s_i}_{\\text{efectos fijos}} +\n\\underbrace{u_{} +v_ix_{ij}+ \\epsilon_{ij}}_{\\text{efectos aleatorios}}.\\]\nAl introducir la variable medida al nivel 2, la parte fija se modifica\n(con respecto al Modelo 3), pero la parte aleatoria:Los centros católicos tienen una nota media más alta que los públicos\n(2.81 puntos más), y una pendiente más suave que la de dichos centros públicos\n(-1.35). Esto último indica que en un colegio católico la mejora de la nota con respecto al\nnivel socio-económico es más lenta que un colegio público.","code":"\nModelo3 <- lmer( mAch~ cses + (cses | school), data = Hsb82)\nModelo3\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: mAch ~ cses + (cses | school)\n#>    Data: Hsb82\n#> REML criterion at convergence: 46714.23\n#> Random effects:\n#>  Groups   Name        Std.Dev. Corr\n#>  school   (Intercept) 2.9464       \n#>           cses        0.8331   0.02\n#>  Residual             6.0581       \n#> Number of obs: 7185, groups:  school, 160\n#> Fixed Effects:\n#> (Intercept)         cses  \n#>      12.636        2.193\nModelo3.1 <- lmer(ses ~ cses + (cses || school), data = Hsb82)\nanova(Modelo3.1, Modelo3)\n#> Data: Hsb82\n#> Models:\n#> Modelo3.1: ses ~ cses + ((1 | school) + (0 + cses | school))\n#> Modelo3: mAch ~ cses + (cses | school)\n#>           npar     AIC     BIC logLik deviance Chisq Df Pr(>Chisq)\n#> Modelo3.1    5 -226164 -226129 113087  -226174                    \n#> Modelo3      6   46723   46764 -23355    46711     0  1          1\ntest <- -2 * logLik(Modelo1, REML = T) + \n         2 * logLik(Modelo3.1, REML = T)\nmean(pchisq(test, df = c(0, 1), lower.tail = F)) \n#> [1] 0\nAIC(logLik(Modelo3)) \n#> [1] 46726.23\nAIC(logLik(Modelo3.1))  \n#> [1] -226113.6\nAIC(logLik(Modelo1)) \n#> [1] 46732\nModelo4 <- lmer(mAch ~ cses*sector + (cses || school), \n                     data = Hsb82)\nModelo4\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: mAch ~ cses * sector + ((1 | school) + (0 + cses | school))\n#>    Data: Hsb82\n#> REML criterion at convergence: 46648.85\n#> Random effects:\n#>  Groups   Name        Std.Dev.\n#>  school   (Intercept) 2.5971  \n#>  school.1 cses        0.5182  \n#>  Residual             6.0580  \n#> Number of obs: 7185, groups:  school, 160\n#> Fixed Effects:\n#>         (Intercept)                 cses       sectorCatholic  \n#>              11.393                2.784                2.805  \n#> cses:sectorCatholic  \n#>              -1.346"},{"path":"cap-mxm.html","id":"cómo-construir-el-modelo-en-la-práctica","chapter":"Capítulo 18 Modelos mixtos","heading":"18.4.3 ¿Cómo construir el modelo en la práctica?","text":"Se ajusta el modelo con todos los efectos fijos y aleatorios\nposibles.Se contrasta qué efectos aleatorios son significativos, sin mover\nlos efectos fijos.Primero se contrasta si la covarianza entre efectos fijos y aleatorios es cero\no :Como lo es, se tendría que contrastar nada más. Los efectos\naleatorios son los que se han incluido en el Modelo5. Si hubiera sido\nsignificativa, se continuaría contrastando si la pendiente aleatoria es\nsignificativa y si la ordenada en el origen lo es.Una vez elegidos los efectos aleatorios que se mantienen en el modelo,\nse eligen los efectos fijos:Dado que la interacción es significativa, se opta por dejar los efectos fijos cses y sector en el modelo (en aras de facilitar las interpretaciones), por lo que sería necesario contrastar su significatividad y este sería el modelo final.","code":"\nModelo5 <- lmer(mAch ~ cses * sector+(cses | school), data = Hsb82)\n#Se ajusta el modelo con covarianza = 0\nModelo5.1 <- lmer(ses ~ cses * sector+(cses||school),\n                 data = Hsb82)\nanova(Modelo5.1, Modelo5)\n#> Data: Hsb82\n#> Models:\n#> Modelo5.1: ses ~ cses * sector + ((1 | school) + (0 + cses | school))\n#> Modelo5: mAch ~ cses * sector + (cses | school)\n#>           npar     AIC     BIC logLik deviance Chisq Df Pr(>Chisq)\n#> Modelo5.1    7 -220069 -220021 110042  -220083                    \n#> Modelo5      8   46650   46705 -23317    46634     0  1          1\nModelo6 = update(Modelo5, . ~ .-cses:sector)\nanova(Modelo6, Modelo5)\n#> Data: Hsb82\n#> Models:\n#> Modelo6: mAch ~ cses + sector + (cses | school)\n#> Modelo5: mAch ~ cses * sector + (cses | school)\n#>         npar   AIC   BIC logLik deviance  Chisq Df Pr(>Chisq)    \n#> Modelo6    7 46678 46726 -23332    46664                         \n#> Modelo5    8 46650 46705 -23317    46634 29.983  1  4.358e-08 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"cap-mxm.html","id":"resumen-16","chapter":"Capítulo 18 Modelos mixtos","heading":"Resumen","text":"En este capítulo se introducen los modelos mixtos o modelos con efectos aleatorios. En particular:Se dan las claves para distinguir entre efectos fijos y aleatorios.Se presenta la formulación del modelo y indica cómo llevar cabo la estimación del mismo.Se explican las etapas del proceso seguir para el ajuste de este tipo de modelos.Se muestra el uso de R para ajustar estos modelos.Se ilustra el análisis de modelos multinivel como caso particular de un modelo con efectos aleatorios.","code":""},{"path":"cap-sparse.html","id":"cap-sparse","chapter":"Capítulo 19 Modelos sparse y métodos penalizados de regresión","heading":"Capítulo 19 Modelos sparse y métodos penalizados de regresión","text":"María DurbánUniversidad Carlos III de Madrid","code":""},{"path":"cap-sparse.html","id":"introducción-9","chapter":"Capítulo 19 Modelos sparse y métodos penalizados de regresión","heading":"19.1 Introducción","text":"El modelo de regresión lineal múltiple: \\(y=\\beta_0+\\beta_1 X_1+\\ldots + \\beta_pX_p+\\varepsilon,\\) visto en el Cap. 15, pesar de su simplicidad, tiene importantes ventajas como la interpretabilidad y su buen poder predictivo en muchas situaciones.En este capítulo enseña cómo se puede hacer el modelo aún más interpretable y mejor predictor, y para conseguirlo se reemplaza el método de estimación de mínimos cuadrados por un método alternativo. Más concretamente, el objetivo de este capítulo es presentar técnicas para mejorar la:Precisión de la predicción: en particular, cuando el número de variables es mayor que el número de observaciones: \\(p>n\\) (algo que ocurre con mucha frecuencia hoy en día). En este caso se pueden utilizar mínimos cuadrados ya que la matriz de diseño es de rango completo y, por lo tanto, se puede encontrar una solución única al problema de minimización. Por ello, se necesita reducir el número de variables, que además, evitará que se sobreajusten los datos.Interpretabilidad del modelo: al eliminar las variables irrelevantes (es decir, haciendo cero los correspondientes coeficientes) se obtendrá un modelo más fácil de interpretar.En base lo anterior,continuación se presentan varios métodos para llevar cabo de forma automática la reducción de variables en el modelo, actividad también denominada selección de variables. Tales métodos son:Selección del mejor subconjunto: su objetivo es identificar el subconjunto de \\(k<p\\) predictores que contenga sólo los que mejor expliquen el comportamiento de la variable respuesta.Shrinkage: en este caso se quieren seleccionar variables explícitamente, sino que se añade una penalización que penaliza el número de coeficientes o su tamaño.Reducción de la dimensión: el objetivo es proyectar los \\(p\\)-predictores en un subespacio de dimensión más pequeña (mediante el uso de combinaciones lineales de las variables predictoras, las cuales se usarán como “nuevos” predictores). Dichas combinaciones lineales se llaman componentes principales y su análisis se dedica el Cap. 32.En este Capítulo se ven los dos primeros métodos. Para el tercero, se remite al lector al Cap. 32.","code":""},{"path":"cap-sparse.html","id":"selección-del-mejor-subconjunto","chapter":"Capítulo 19 Modelos sparse y métodos penalizados de regresión","heading":"19.2 Selección del mejor subconjunto","text":"Supóngase que se tiene acceso \\(p\\) variables predictoras, pero se quiere un modelo más simple que involucre sólo un subconjunto de esos \\(p\\) predictores. La forma lógica de conseguirlo es considerar todos los posibles subconjuntos de los \\(p\\) predictores y elegir el mejor modelo de entre todos los modelos construidos con cada uno de los subconjuntos de variables. Los pasos seguir serían:Se crea el modelo nulo, \\(M_0\\), que es aquel que únicamente contiene la ordenada en el origen y ningún predictor. Este modelo simplemente predice la media muestral para cada observación.Se crea el modelo nulo, \\(M_0\\), que es aquel que únicamente contiene la ordenada en el origen y ningún predictor. Este modelo simplemente predice la media muestral para cada observación.Para cada valor de \\(k=1,2,\\ldots , p\\), se calculan los \\(\\binom{p}{k}\\) modelos que contienen \\(k\\) predictores. Es decir, los \\(p\\) modelos que contienen 1 predictor, los \\(p\\times (p-1)/2\\) modelos que contienen 2 predictores, etc.Para cada valor de \\(k=1,2,\\ldots , p\\), se calculan los \\(\\binom{p}{k}\\) modelos que contienen \\(k\\) predictores. Es decir, los \\(p\\) modelos que contienen 1 predictor, los \\(p\\times (p-1)/2\\) modelos que contienen 2 predictores, etc.Para cada valor de \\(k\\), se elige el mejor entre los \\(\\binom{p}{k}=\\frac{p!}{(p-k)!k!}\\) posibles modelos y se denota por \\(M_k\\). Es decir, \\(M_1\\) sería el mejor modelo entre los \\(p\\) modelos con una única variable, \\(M_2\\) sería el mejor modelo entre los modelos con dos variables, etc. En este caso, el mejor modelo sería aquel cuyo \\(RSS\\) (suma de residuos al cuadrado) sea menor, o equivalentemente, aquel cuyo \\(R^2\\) sea mayor.Para cada valor de \\(k\\), se elige el mejor entre los \\(\\binom{p}{k}=\\frac{p!}{(p-k)!k!}\\) posibles modelos y se denota por \\(M_k\\). Es decir, \\(M_1\\) sería el mejor modelo entre los \\(p\\) modelos con una única variable, \\(M_2\\) sería el mejor modelo entre los modelos con dos variables, etc. En este caso, el mejor modelo sería aquel cuyo \\(RSS\\) (suma de residuos al cuadrado) sea menor, o equivalentemente, aquel cuyo \\(R^2\\) sea mayor.Finalmente, entre los modelos: \\(M_1,\\ldots ,M_p\\) ase elige el mejor utilizando un criterio como AIC (criterio de información de Akaike), BIC (criterio de información bayesiano) o \\(R^2\\) ajustado.Finalmente, entre los modelos: \\(M_1,\\ldots ,M_p\\) ase elige el mejor utilizando un criterio como AIC (criterio de información de Akaike), BIC (criterio de información bayesiano) o \\(R^2\\) ajustado.Este método se puede usar también en el caso de GLMs, si bien, en este caso, se usa la deviance en vez de \\(RSS\\).","code":""},{"path":"cap-sparse.html","id":"procedimiento-con-r-la-función-regsubset","chapter":"Capítulo 19 Modelos sparse y métodos penalizados de regresión","heading":"19.2.1 Procedimiento con R: la función regsubset()","text":"En esta subsección se aplica el método descrito al conjunto de datos Hitters del paquete ISRL2. El objetivo es predecir el sueldo, Salary, de jugadores de béisbol partir de varias variables asociadas con su rendimiento el año anterior.La variable Salary está disponible para algunos de los jugadores. Éstos se pueden identificar con la función .na(). La función sum() permite ver cuántos hay. Se utiliza na.omit() para eliminarlos.La función regsubsets() del paquete leaps lleva cabo la selección del mejor subconjunto de variables predictoras, identificando el mejor modelo que contiene un número dado de ellas (1,2,3, etc.) atendiendo \\(RSS\\). La sintaxis usada es similar la de la función lm().Los resultados se pueden ver usando summary(), donde se muestra el mejor modelo para cada número específico de variables. Las variables incluidas en cada modelo se indican con un asterisco. Por ejemplo, el mejor modelo con dos variables incluye Hits y CRBI.\nPor defecto, regsubsets() solo muestra los resultados de los modelos que contienen hasta ocho variables. La opción nvmax se puede usar para incrementar esta cantidad, por ejemplo hasta 19 variables (que es el número de variables predictoras en el conjunto de datos):La función summary() devuelve diferentes medias de bondad de ajuste: \\(R^2\\), \\(RSS\\), \\(R^2\\) ajustado, \\(C_p\\) y \\(BIC\\) que se utilizan para elegir el mejor de entre todos los modelos.En el ejemplo, el \\(R^2\\) ajustado mayor corresponde al modelo con 11 variables.Los resultados también se se pueden mostrar y dibujar simultáneamente; por ejemplo, los valores de \\(RSS\\) y \\(R^2\\) ajustado de todos los modelos se muestran en la Fig. 19.1.\nFigura 19.1: Valores de \\(R^2\\) y \\(R^2\\) ajustados corerspondientes modelos con distinto número de variables\nOtra manera de visualizar los resultados es:\nFigura 19.2: Variables seleccionadas en cada uno de los modelos y su correspondiente valor de \\(R^2\\) ajustado.\nLa primera fila tiene un cuadrado negro en cada una de las variables explicativas del modelo con mayor \\(R^2\\) ajustado (en este caso, sería similar para los otros criterios).Varios modelos tienen un valor de \\(R^2\\) ajustado próximo \\(0.52\\), pero es el modelo con 11 variables el que alcanza el mayor valor. La función coef() permite ver los coeficientes estimados de este modelo.","code":"\nlibrary(\"ISLR2\")\nHitters <- na.omit(Hitters)\nlibrary(\"leaps\")\nregfit_full <- regsubsets(Salary ~ ., Hitters)\nregfit_full <- regsubsets(Salary ~ .,\n  data = Hitters,\n  nvmax = 19\n)\nreg_summary <- summary(regfit_full)\nnames(reg_summary)\n#> [1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"\nreg_summary$adjr2\n#>  [1] 0.3188503 0.4208024 0.4450753 0.4672734 0.4808971 0.4972001 0.5007849\n#>  [8] 0.5137083 0.5180572 0.5222606 0.5225706 0.5217245 0.5206736 0.5195431\n#> [15] 0.5178661 0.5162219 0.5144464 0.5126097 0.5106270\nplot(regfit_full, scale = \"adjr2\")\ncoef(regfit_full, 11)\n#>  (Intercept)        AtBat         Hits        Walks       CAtBat        CRuns \n#>  135.7512195   -2.1277482    6.9236994    5.6202755   -0.1389914    1.4553310 \n#>         CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n#>    0.7852528   -0.8228559   43.1116152 -111.1460252    0.2894087    0.2688277"},{"path":"cap-sparse.html","id":"selección-stepwise","chapter":"Capítulo 19 Modelos sparse y métodos penalizados de regresión","heading":"19.2.2 Selección stepwise","text":"Cuando el número de variables predictoras, \\(p\\), es grande, el método anterior es computacionalmente muy costoso ya que el número de posibles combinaciones de variables crece de una manera alarmante. En general, la función regsubset() puede lidiar con hasta 30-40 variables predictoras. Además, otro problema es el sobreajuste. Si se tienen 40 variables, se estarían ajustando millones de modelos, y puede que el modelo elegido funcione muy bien en los datos utilizados para su construcción, pero tan bien en un nuevo conjunto de datos. Una alternativa es el método stepwise.\nLa idea detrás de este método es similar la anterior, pero se busca el mejor modelo entre un conjunto mucho más pequeño de modelos.Hay dos posibilidades de hacer stepwise: forward y backward. Ambas son bastante parecidas; la principal diferencia es el modelo del que se parte: del modelo sin ninguna variable predictora (forward) o del modelo con todas ellas (backward).","code":""},{"path":"cap-sparse.html","id":"forward-stepwise","chapter":"Capítulo 19 Modelos sparse y métodos penalizados de regresión","heading":"19.2.2.1 Forward stepwise","text":"En este caso se comienza con el modelo nulo, \\(M_0\\), y se van añadiendo variables secuencialmente. En particular, en cada paso (step) la variable que proporciona la mayor mejora al ajuste es la que se añade al modelo. Los pasos seguir serían:Se crea el modelo nulo, \\(M_0\\).Se crea el modelo nulo, \\(M_0\\).Para cada valor de \\(k=0,1,2,\\ldots , p\\):\n) Se consideran todos los \\(p-k\\) modelos que surgen de aumentar el modelo \\(M_k\\) con un predictor.\nb) Se elige el mejor de esos \\(p-k\\) modelos, que se denota \\(M_{k+1}\\). El término mejor significa tener el \\(RSS\\) más bajo o el \\(R^2\\) más alto.Para cada valor de \\(k=0,1,2,\\ldots , p\\):) Se consideran todos los \\(p-k\\) modelos que surgen de aumentar el modelo \\(M_k\\) con un predictor.b) Se elige el mejor de esos \\(p-k\\) modelos, que se denota \\(M_{k+1}\\). El término mejor significa tener el \\(RSS\\) más bajo o el \\(R^2\\) más alto.Se elige el mejor de los modelos \\(M_0,\\ldots ,M_p\\) en función de un criterio que tenga en cuenta la complejidad del modelo, como AIC, BIC o \\(R^2\\) ajustado.Se elige el mejor de los modelos \\(M_0,\\ldots ,M_p\\) en función de un criterio que tenga en cuenta la complejidad del modelo, como AIC, BIC o \\(R^2\\) ajustado.Este enfoque tiene ventajas computacionales claras, ya que el número de modelos ajustados es mucho menor, pero garantiza que el modelo elegido sea el mejor modelo posible, especialmente si existe correlación entre las variables predictoras.","code":""},{"path":"cap-sparse.html","id":"backward-stepwise","chapter":"Capítulo 19 Modelos sparse y métodos penalizados de regresión","heading":"19.2.2.2 Backward stepwise","text":"En este caso, se comienza con el modelo que incluye las \\(p\\) las variables predictoras y se van eliminando de forma iterativa hasta llegar al modelo nulo (\\(M_0\\)). Los pasos serían:Se ajusta el modelo \\(M_p\\), que contiene todas (\\(p\\)) las variable predictoras.Para cada valor de \\(k=p,p-1,\\ldots , 1\\):\n) Se consideran todos los \\(k\\) modelos que surgen de reducir en el modelo \\(M_k\\) un predictor, es decir, modelos con \\(k-1\\) variables predictoras.\nb) Se elige el mejor de esos \\(k\\) modelos, que se denota \\(M_{k-1}\\). Dicho modelo será el que tenga el \\(RSS\\) más bajo o el \\(R^2\\) más alto.Entre los modelos \\(M_0,\\ldots ,M_p\\) se elige el mejor en función de un criterio como AIC, BIC o \\(R^2\\) ajustado.Tanto en el caso forward como en el caso backward, se busca el mejor modelo “sólo” entre \\(1+p(p+1)/2\\) modelos, lo que los hace recomendables frente la selección del mejor subconjunto de variables cuando \\(p\\) es demasiado grande..El método backward stewise necesita que el número de observaciones \\(n\\) sea mayor que el de variables predictoras \\(p\\) (ya que necesita ajustar el modelo con todas las variables). Por el contrario, el método forward stepwise se puede usar incluso cuando \\(n<p\\).","code":""},{"path":"cap-sparse.html","id":"procedimiento-con-r-la-función-regsubset-1","chapter":"Capítulo 19 Modelos sparse y métodos penalizados de regresión","heading":"19.2.2.3 Procedimiento con R: la función regsubset()","text":"La función regsubset() permite utilizar los métodos forward y backward, usando los argumentos method = \"forward\" o method = \"backward\":Los métodos mejor subconjunto de variables, forward stepwise y backward stepwise tienen por qué seleccionar el mismo (mejor) modelo. Ni siquiera tienen por qué seleccionar el mismo modelo para cada número de predictores en la fase previa la selección final. Así ocurre, por ejemplo, cuando el número de variables predictoras es \\(k=2\\):En la etapa final de selección, el modelo seleccionado tiene por qué ser el mismo en función de los distintos criterios de selección (aunque normalmente lo es). Lo habitual es decidir un criterio para elegir el mejor modelo (\\(R^2\\) ajustado, \\(BIC\\), etc.) y seleccionarlo en función de él. En el ejemplo, seleccionando el criterio del \\(R^2\\) ajustado, el mejor modelo es el que tiene 11 variables, tanto con el criterio forward como con el backward143:Con el criterio \\(BIC\\) también se selecciona el modelo con 11 variables predictoras:Otra posibilidad es utilizar como criterio de selección el error de predicción, y para ello se puede echar mano de algún esquema de validación cruzada. continuación se ilustra el caso en el que se divide la muestra en dos subconjuntos: training y testing, pero se puede utilizar cualquier otro método (validación cruzada k-grupos*, etc. Véase Sec. 10.4.Se utiliza regsubsets() en la muestra de entrenamiento para obtener los modelos con distinto número de variables predictoras:Para calcular el error de predicción, dado que la función regsubset() tiene asociada una función predict(), se han de calcular “manualmente” los valores predichos para la muestra de test. Para eso se necesita la matriz de diseño del modelo.Ahora, para cada modelo de tamaño \\(k\\), se extraen los coeficientes de\nregfit_best para el mejor modelo de ese tamaño, se multiplica el vector de coeficientes por la matriz de diseño y se obtienen las predicciones; continuación se calcula el error cuadrático medio (\\(MSE\\)).Con este criterio, el mejor modelo es el que contiene 7 variables:","code":"\nregfit_fwd <- regsubsets(Salary ~ .,\n  data = Hitters,\n  nvmax = 19, method = \"forward\"\n)\nregfit_bwd <- regsubsets(Salary ~ .,\n  data = Hitters,\n  nvmax = 19, method = \"backward\"\n)\ncoef(regfit_full, 2)\n#> (Intercept)        Hits        CRBI \n#> -47.9559022   3.3008446   0.6898994\ncoef(regfit_fwd, 2)\n#> (Intercept)        Hits        CRBI \n#> -47.9559022   3.3008446   0.6898994\ncoef(regfit_bwd, 2)\n#> (Intercept)        Hits       CRuns \n#> -50.8174029   3.2257212   0.6614168\nwhich.max(summary(regfit_fwd)$adjr2)\n#> [1] 11\nwhich.max(summary(regfit_bwd)$adjr2)\n#> [1] 11\nwhich.min(summary(regfit_fwd)$bic)\n#> [1] 6\nwhich.min(summary(regfit_bwd)$bic)\n#> [1] 8\nset.seed(1)\nentreno <- sample(c(TRUE, FALSE), nrow(Hitters), replace = TRUE)\ntest <- (!entreno)\nregfit_best <- regsubsets(Salary ~ ., data = Hitters[entreno, ], nvmax = 19)\ntest.mat <- model.matrix(Salary ~ ., data = Hitters[test, ])\nval_errors <- rep(NA, 19)\nfor (i in 1:19) {\n  coefi <- coef(regfit_best, id = i)\n  pred <- test.mat[, names(coefi)] %*% coefi\n  val_errors[i] <- mean((Hitters$Salary[test] - pred)^2)\n}\nval_errors\n#>  [1] 164377.3 144405.5 152175.7 145198.4 137902.1 139175.7 126849.0 136191.4\n#>  [9] 132889.6 135434.9 136963.3 140694.9 140690.9 141951.2 141508.2 142164.4\n#> [17] 141767.4 142339.6 142238.2"},{"path":"cap-sparse.html","id":"métodos-shrinkage","chapter":"Capítulo 19 Modelos sparse y métodos penalizados de regresión","heading":"19.3 Métodos shrinkage","text":"Los métodos anteriores se basan en el ajuste de modelos mediante mínimos cuadrados ordinarios. Los métodos shrinkage, sin embargo, se basan en una modificación del procedimiento de mínimos cuadrados ordinarios que consiste en añadir una penalización que encoje los coeficientes del modelo (normalmente hacia \\(0\\)). Una de las ventajas de este tipo de métodos es que reduce la varianza de los coeficientes estimados.Recuérdese que en el ajuste por mínimos cuadrados las estimaciones de \\(\\beta_0, \\beta_1, \\ldots , \\beta_p\\) son los valores que minimizan:\n\\[RSS=\\sum_{=1}^n \\left ( y_i-\\beta_0-\\sum_{j=1}^p \\beta_jx_{ij}\\right )^2.\\]","code":""},{"path":"cap-sparse.html","id":"regresión-ridge","chapter":"Capítulo 19 Modelos sparse y métodos penalizados de regresión","heading":"19.3.1 Regresión ridge","text":"La regresión ridge144 añade un término de penalización controlado por un parámetro (que habrá que elegir) que penalizará la magnitud de los coeficientes. Cuanto más grande es el coeficiente mayor es la penalización. En consecuencia, en la regresión ridge la expresión que se minimiza para obtener las estimaciones de los parámetros del modelo es:\n\\[\\begin{equation}\n\\sum_{=1}^n \\left ( y_i-\\beta_0-\\sum_{j=1}^p \\beta_jx_{ij}\\right )^2+\\lambda \\sum_{j=1}^p \\beta_j^2=RSS+\\lambda \\sum_{j=1}^p \\beta_j^2.\n\\tag{19.1}\n\\end{equation}\\]\nEn realidad, lo que se está haciendo es hacer pagar al modelo un precio (en términos de ajuste) por el hecho de que los coeficientes sean cero, y el precio será tanto mayor cuanto más grande sea la magnitud del coeficiente. esta penalización se le llama penalización shrinkage porque “anima” los coeficientes que se contraigan hacia \\(0\\) (así es como este método favorece la simplicidad de los modelos). La magnitud de dicha contracción está gobernada por lambda, el parametro de afinado o regulación (también conocido en la jerga como “de tuneado”). Si \\(\\lambda=0\\), se está en el caso de mínimos cuadrados ordinarios, y cuanto mayor sea \\(\\lambda\\), mayor será el precio pagar para que esos coeficientes sean distintos de \\(0\\). Si \\(\\lambda\\) es extremadamente grande, los coeficientes estarán muy próximos \\(0\\), para que el segundo término pequeño (recuérdese que se minimiza \\(RSS\\) más la penalización). Aunque valores más grandes de los coeficientes proporcionasen un mejor ajuste (y por lo tanto un menor \\(RSS\\)), el término de penalización aumentaría se hará grande y se alcanzaría el mínimo. Por lo tanto \\(\\lambda\\) gobierna el equilibrio entre un buen ajuste del modelo y el tamaño de los coeficientes (y por lo tanto el número de coeficientes distintos de cero).La elección del valor de \\(\\lambda\\) es un punto crucial de este tipo de regresión. Para su determinación se suelen utilizar procedimientos de validación cruzada.","code":""},{"path":"cap-sparse.html","id":"escalado-de-variables-predictoras","chapter":"Capítulo 19 Modelos sparse y métodos penalizados de regresión","heading":"19.3.1.1 Escalado de variables predictoras","text":"Un punto importante en regresión ridge es si las variables predictoras están escaladas o .El método de mínimos cuadrados ordinarios es invariante la escala (scale-invariant), es decir, que si se multiplica una variable predictora \\(X_j\\) por una constante \\(c\\), el coeficiente estimado se multiplicada por \\(1/c\\), pero \\(X_j\\hat \\beta_j\\) cambia. Sin embargo, en el caso de la regresión ridge los coeficientes estimados pueden cambiar sustancialmente ante un cambio de escala (es decir, si se multiplica una variable predictora por una constante), ya que todos los coeficientes forman parte del término de penalización. Por lo tanto, antes de utilizar la regresión ridge (o cualquier método de regularización) es importante estandarizar las variables predictoras, dividiendo cada variable por su desviación estándar, de forma que todas tengan desviación estándar igual \\(1\\):\n\\[\\tilde x_{ij}= \\frac{x_{ij}}{\\sqrt{\\frac{1}{N}\\sum_{=1}^N (x_{ij}-\\overline{x}_{ij})^2}}.\\]\nCon esto se consigue que los coeficientes estén en “igualdad de condiciones”.En muchas ocasiones la regresión ridge da lugar un menor \\(MSE\\) que el obtenido con mínimos cuadrados ordinarios. Sin embargo, por muy grande que sea \\(\\lambda\\) los coeficientes serán \\(0\\), sino que estarán próximos cero, por lo que este método es realmente un método de selección de variables.Sin embargo, la regresión ridge puede ser muy útil cuando hay variables predictoras altamente correlacionadas pero se desea mantener todas en el modelo. En estos casos, la regresión ridge soluciona los problemas de multicolinealidad.","code":""},{"path":"cap-sparse.html","id":"procedimiento-con-r-la-función-glmnet","chapter":"Capítulo 19 Modelos sparse y métodos penalizados de regresión","heading":"19.3.1.2 Procedimiento con R: la función glmnet()","text":"Para llevar cabo la regresión ridge (y para otros métodos de regresión shrinkage) se usa el paquete glmnet.\nLa función principal en este paquete se llama también glmnet(). Esta función tiene una sintaxis un poco diferente las funciones usuales para el ajuste de distintos modelos en R. Es necesario pasarle la matriz \\(\\boldsymbol X\\) de variables predictoras (sin la columna correspondiente la ordenada en el origen) y el vector \\(\\boldsymbol y\\) con la variable respuesta. Para ilustrar su uso se utilizan los datos anteriores sobre béisbol.La función glmnet() tiene un argumento, alpha, que determina el tipo de penalización que se añade en el modelo. En el caso de regresión ridge, alpha=0.Por defecto, la función glmnet() elige de forma automática el rango de valores de \\(\\lambda\\). Sin embargo, modo ilustrativo, se va elegir la rejilla de valores que van desde \\(\\lambda=10^{10}\\) hasta \\(\\lambda=10^{-2}\\), cubriendo de esta forma una gran gama de escenarios, desde el modelo nulo (solo la ordenada en el origen) hasta el caso de mínimos cuadrados ordinarios. Más adelante se verá que se puede llevar cabo el ajuste del modelo para un valor determinado de \\(\\lambda\\) que esté entre los de la rejilla inicial.Por defecto, la función glmnet() estandariza las variables predictoras para que estén en la misma escala. Si por alguna razón se quisiera hacer, se usaría standardize = FALSE.Asociado con cada valor de \\(\\lambda\\) hay un vector de coeficientes estimados mediante regresión ridge almacenados en un matriz accesible utilizando coef(). En este caso, el tamaño de la matriz es \\(20 \\times 100\\), donde las \\(20\\) filas corresponden cada uno de los predictores más la ordenada en el origen y las 100 columnas cada valor de \\(\\lambda\\). Lo esperable es que los coeficientes estimados sean más pequeños cuanto mayor sea el valor de \\(\\lambda\\). continuación, se muestra el valor de los coeficientes cuando \\(\\lambda = 11.498\\), así como la suma de sus cuadrados, \\(\\sum_{j=1}^p\\beta_j^2\\):Por el contrario, si \\(\\lambda\\) es más pequeño, \\(705\\), el valor de su suma de cuadrados es mucho mayor.La Fig. 19.3 muestra el efecto de \\(\\lambda\\) en los coeficientes del modfelo:\nFigura 19.3: Coeficientes estimados para distintos valores del parámetro de penalización (en la escala logarítmica)\nEl lado izquierdo de la Fig 19.3 corresponde valores de \\(\\lambda\\) muy pequeños, y por lo tanto existen restricciones sobre los coeficientes. Conforme aumenta el valor de \\(\\lambda\\) los coeficientes se aproximan rápidamente cero. Pero todos se aproximan cero de la misma manera: hay un conjunto de variables cuyo coeficiente es prácticamente cero para cualquier valor de \\(\\lambda\\), mientras que para un valor de \\(log(\\lambda)=3\\) parece que hay sólo \\(4\\) coeficientes distintos de \\(0\\).La función predict() se puede utilizar con diferentes propósitos. Por ejemplo, se pueden obtener los coeficientes de la regresión ridge para un valor específico de \\(\\lambda\\), por ejemplo \\(\\lambda=50\\):En lo que sigue, se ilustra el ajuste de una regresión ridge y se computa el \\(MSE\\) de predicción para distintos valores de \\(\\lambda\\). Primeramente, se divide el conjunto de datos en un subconjunto de entrenamiento y otro de test:continuación se ajusta la regresión ridge los datos del subconjunto de entrenamiento usando un valor específico de \\(\\lambda\\) (por ejemplo \\(\\lambda=4\\)). Posteriormente, se evalúa su \\(MSE\\) con los datos del subconjunto de test. Para ello se usa la función predict(). En este caso, para obtener las predicciones para la muestra de test, se reemplaza type = \"coefficients\" por el argumento newx.El \\(MSE\\) es \\(142{,}199\\). Si se usa un valor muy alto de \\(\\lambda\\), por ejemplo \\(10^{10}\\) (esto sería equivalente ajustar un modelo solo con la ordenada en el origen), el resultado es muy distinto:Por lo tanto, en este caso, ajustar un modelo de regresión ridge con \\(\\lambda=4\\) da un \\(MSE\\) mucho menor que el obtenido cuando el modelo sólo contiene la ordenada en el origen.continuación se compara el resultado para \\(\\lambda=4\\) con el obtenido utilizando mínimos cuadrados ordinarios (\\(\\lambda=0\\)).Se observa que el \\(MSE\\) es menor cuando se usa regresión ridge (con \\(\\lambda=4\\)) que cuando se usan mínimos cuadrados ordinarios.Hasta ahora se ha elegido el valor \\(\\lambda=4\\) de forma arbitraria. En la siguiente sección se aborda la cuestión de cómo seleccionar el valor de dicho parámetro de una forma automática.","code":"\nx <- model.matrix(Salary ~ ., Hitters)[, -1]\ny <- Hitters$Salary\nlibrary(\"glmnet\")\ngrid <- 10^seq(10, -2, length = 100)\nridge_mod <- glmnet(x, y, alpha = 0, lambda = grid)\nridge_mod$lambda[50]\n#> [1] 11497.57\nsum(coef(ridge_mod)[-1, 50]^2)\n#> [1] 40.45739\nridge_mod$lambda[60]\n#> [1] 705.4802\nsum(coef(ridge_mod)[-1, 60]^2)\n#> [1] 3261.554\nplot(ridge_mod, xvar = \"lambda\", label = TRUE)\npredict(ridge_mod, s = 50, type = \"coefficients\")[1:20, ]\nset.seed(1)\nentreno <- sample(1:nrow(x), nrow(x) / 2)\ntest <- (-entreno)\ny_test <- y[test]\nridge_mod <- glmnet(x[entreno, ], y[entreno], alpha = 0, lambda = grid)\nridge_pred <- predict(ridge_mod, s = 4, newx = x[test, ])\nmean((ridge_pred - y_test)^2)\n#> [1] 142226.5\nridge_pred <- predict(ridge_mod, s = 1e10, newx = x[test, ])\nmean((ridge_pred - y_test)^2)\n#> [1] 224669.8\nridge_pred <- predict(ridge_mod, s = 0, newx = x[test, ], exact = T, \n                      x = x[entreno, ], y = y[entreno])\nmean((ridge_pred - y_test)^2)\n#> [1] 167018.2"},{"path":"cap-sparse.html","id":"selección-del-parámetro-de-penalización","chapter":"Capítulo 19 Modelos sparse y métodos penalizados de regresión","heading":"19.3.2 Selección del parámetro de penalización","text":"En la subsección anterior se ha visto que el valor de \\(\\lambda\\) tiene un gran impacto en los resultados obtenidos cuando se utiliza un modelo con penalización.Una buena manera de elegir \\(\\lambda\\) es usar validación cruzada (cross-validation). Por ejemplo, se puede usar validación cruzada con 10 grupos (k-fold cross-validation) :Se dividen los datos en \\(k\\) grupos, se ajusta el modelo ridge \\(k-1\\) de esos grupos (para una rejilla de valores de \\(\\lambda\\)) y se calcula el error de predicción para el otro grupo.La acción anterior se repite tomando como muestra de test cada uno de los \\(k\\) grupos y se suman los errores de predicción.Al final se dispondrá de una curva con los errores para cada valor de \\(\\lambda\\) y se elegirá el que dé el mínimo error.En la práctica, el procedimiento anterior se puede hacer con la función cv.glmnet(). Por defecto, esta función usa un \\(10\\)-fold cross-validation, pero el número de grupos se puede cambiar usando el argumento nfolds.En el ejemplo del béisbol:\nFigura 19.4: Valor del error cudrático medio y su intervalo de confianza (calculado sobre los 10 grupos) para distintos valores del parámetro de penalización\nEn la Fig. 19.4, los puntos rojos corresponden la media del \\(MSE\\) para los 10 grupos y las barras superior e inferior corresponden esa cantidad más/menos una desviación estándar (el ancho será tanto menor cuanto mayor sea el número de grupos). La primera línea vertical corresponde al valor de \\(\\lambda\\) que hace mínimo el \\(MSE\\) y la segunda es el valor que está una distancia de una desviación típica del \\(\\lambda\\) mínimo (usar este último valor podría ser una buena opción para evitar el sobre-ajuste, es decir dejar demasiadas variables en el modelo).El valor mínimo del \\(MSE\\) se calcula como sigue:Como se puede apreciar, hay una apreciable mejora en el error de predicción que se había obtenido cuando el parámetro de penalización se había fijado en \\(\\lambda=4\\).","code":"\nset.seed(1)\ncv_out <- cv.glmnet(x[entreno, ], y[entreno], alpha = 0)\nplot(cv_out)\nmejorlam <- cv_out$lambda.min\nmejorlam\n#> [1] 326.0828\nridge_pred <- predict(ridge_mod, s = mejorlam, newx = x[test, ])\nmean((ridge_pred - y_test)^2)\n#> [1] 139833.6"},{"path":"cap-sparse.html","id":"regresión-lasso","chapter":"Capítulo 19 Modelos sparse y métodos penalizados de regresión","heading":"19.3.3 Regresión lasso","text":"Uno de los puntos débiles de la regresión ridge es que hace selección de variables (los coeficientes pueden estar próximos cero pero ser exactamente cero). En el modelo final se incluyen todos los coeficientes y, por lo tanto, la regresión ridge sólo es útil cuando la mayoría de las variables predictoras tienen un impacto significativo en la respuesta.La regresión lasso (least absolute srinkage selection operator, por sus siglas en inglés) , introducida por R. Tibshirani (1996), es una alternativa la regresión ridge cuyo objetivo es precisamente corregir la limitación anteriormente mencionada de la regresión ridge, y es útil cuando la mayoría de las variables predictoras son relevantes en el modelo. Los coeficientes lasso, \\(\\hat \\beta^L\\), minimizan la siguiente cantidad:\n\\[\\sum_{=1}^n \\left ( y_i-\\beta_0-\\sum_{j=1}^p \\beta_jx_{ij}\\right )^2+\\lambda \\sum_{j=1}^p |\\beta_j|=RSS+\\lambda \\sum_{j=1}^p |\\beta_j|.\\]\nAhora los coeficientes se contraen hacia cero utilizando la suma de los coeficientes en valor absoluto en vez de la suma de los cuadrados de dichos coeficientes. esta norma se le llama \\(l_1\\), \\(\\|\\beta\\|_1=\\sum_{j=1}^p|\\beta_j|\\). El cambio que supone es sutil pero importante. En ambos casos los coeficientes se contraen hacia \\(0\\), pero en el caso de la regresión lasso cuando \\(\\lambda\\) es suficientemente grande los coeficientes serán \\(0\\), de modo que se está haciendo una selección de variables. Por consiguiente, la regresión lasso anulará los coeficientes de las variables que son importantes la hora de explicar el comportamiento de la variable respuesta mediante un valor de \\(\\lambda\\) es suficientemente grande. En este sentido el modelo de regresión \\(lasso\\) es lo que se llama un modelo sparse (un modelo con un número sparse, o escaso, de parámetros).¿Por qué lasso hace que los coeficiente se contraigan exactamente hacia cero?\nPara entenderlo se va ver una formulación equivalente la de los mínimos cuadrados penalizados en el caso de la regresión lasso:\n\\[\\sum_{=1}^n \\left ( y_i-\\beta_0-\\sum_{j=1}^p \\beta_jx_{ij}\\right )^2\\quad \\text{sujeto } \\quad \\sum_{j=1}^p |\\beta_j|<s.\\]\nDicha formulación equivalente corresponde mínimos cuadrados con una restricción, o lo que es lo mismo, con un presupuesto en la norma \\(l_1\\) sobre los coeficientes. Las dos formulaciones son equivalentes en el sentido de que si se tiene un presupuesto \\(s\\), habrá un \\(\\lambda\\) en la primera formulación que corresponda al presupuesto \\(s\\) en la segunda, y viceversa.\nSupóngase que se hacen mínimos cuadrados y se obtienen las estimaciones de los parámetros (coeficientes) tal que la suma de sus valores absolutos es 10,\npero alguien dice que nuestro presupuesto es \\(5\\) (la suma de los valores absolutos de los coeficientes puede ser mayor que esa cantidad). Entonces, hay que resolver el problema de mínimos cuadrados pero los coeficientes pueden tomar cualquier valor, ya que se tiene una restricción sobre los mismos. Cuanto más pequeño sea el presupuesto, más próximos cero serán los coeficientes. Si el presupuesto es \\(0\\), todos los coeficientes serán también \\(0\\). Si el presupuesto es muy alto, hay libertad para que los coeficientes tomen el valor que quieran, y se estaría en el caso de mínimos cuadrados. El presupuesto impone que haya un equilibrio entre el ajuste los datos y el tamaño de los coeficientes.La Fig. 19.5 (tomada de G. James et al. (2013)) muestra por qué el modelo de regresión lasso es sparse. El gráfico corresponde un modelo de regresión con dos variables predictoras. El punto donde está el vector de coeficientes, \\(\\hat{\\boldsymbol{\\beta}}\\), es donde se alcanza el valor mínimo de la suma los cuadrados de los residuos del modelo (\\(RSS\\)) y los contornos son combinaciones de valores de \\(\\beta_1\\) y \\(\\beta_2\\) que dan lugar al mismo valor de \\(RSS\\), pero que ya sería el mínimo. Las regiones de restricción son \\(|\\beta_1|+|\\beta_2|<s\\) (lasso) y \\(\\beta_1^2 +\\beta_2^2<s\\) (ridge). En el caso de la regresión ridge, el presupuesto es el radio del círculo y la regresión ridge busca el primer lugar en el que el contorno toca la región de restricción, pero, al ser un círculo, difícilmente uno u otro coeficiente va ser \\(0\\). En el caso de la regresión lasso, la región de restricción tiene forma de diamante y, por lo tanto, tiene vértices. Como puede apreciarse, en la Fig. 19.5 el contorno toca la región de restricción en el caso en que \\(\\beta_1=0\\).\nFigura 19.5: Contornos (rojo) de \\(RSS\\) y regiones de restricción (en azul) para la regresión lasso (izquierda) y ridge (derecha)\nSe vuelve al ejemplo del béisbol para mostrar la regresión lasso; en este caso el argumento \\(\\alpha\\) toma valor \\(1\\) (\\(0\\) en el caso de la regresión ridge.\nFigura 19.6: Valor de los parámetros estimados para distintos valores de la penalización (que depende del parámetros de penalización)\nEn la Fig. 19.6 se puede ver que, dependiendo del valor del parámetro de penalización, algunos de los coeficientes se hacen exactamente \\(0\\). Para elegir el valor de dicho parámetro y calcular el \\(MSE\\) resultante en el conjunto de test se procede como sigue:\nFigura 19.7: Valor del error cuadrático medio y su intervalo de confianza para distintos valores del parámetro de penalización\nEste valor es bastante más bajo que \\(MSE\\) en la muestra de test en el caso de mínimos cuadrados ordinarios \\((224.666,8\\)) y bastante parecido al obtenido con la regresión ridge cuando el parámetro de penalización se elige mediante validación cruzada: \\(139,856,6\\)). Sin embargo, la regresión lasso tiene una ventaja importante con respecto la regresión ridge ya que los coeficientes estimados son sparse. En los resultados que se muestran continuación, se puede observar que 10 de los 20 coeficientes estimados son \\(0\\). Por lo tanto, el modelo lasso con \\(\\lambda\\) elegido mediante validación cruzada contiene sólo nueve variables predictoras.","code":"\nlasso_mod <- glmnet(x[entreno, ], y[entreno], alpha = 1, lambda = grid)\nplot(lasso_mod)\nset.seed(1)\ncv_out <- cv.glmnet(x[entreno, ], y[entreno], alpha = 1)\nplot(cv_out)\nmejorlab <- cv_out$lambda.min\nlasso.pred <- predict(lasso_mod, s = mejorlab, newx = x[test, ])\nmean((lasso.pred - y_test)^2)\n#> [1] 143673.6\nout <- glmnet(x, y, alpha = 1)\nlasso_coef <- predict(out, type = \"coefficients\", s = mejorlab)[1:20, ]\nlasso_coef[lasso_coef != 0]\n#>   (Intercept)          Hits         Walks        CHmRun         CRuns \n#>   -3.04787656    2.02551572    2.26853781    0.01647106    0.21177390 \n#>          CRBI       LeagueN     DivisionW       PutOuts        Errors \n#>    0.41944632   20.48456551 -116.59062083    0.23718459   -0.94739923"},{"path":"cap-sparse.html","id":"elastic-net","chapter":"Capítulo 19 Modelos sparse y métodos penalizados de regresión","heading":"19.3.4 Elastic net ","text":"Uno de los problemas de la regresión lasso es cuando hay variables predictoras correladas entre sí, pues elegirá una de ellas (y los coeficientes de las demás los hará cero) sin un criterio objetivo. Además, supóngase que se está en una situación en la que el número de variables \\(p\\) es mayor que el número de observaciones \\(n\\); en este caso la regresión lasso elegiría como mucho \\(n\\) variables; mientras que la regresión ridge las utilizaría todas, aumentando la complejidad del modelo (esto en algunos casos puede ser lo deseable o ). Elastic net (Zou Hastie 2005) es una generalización de los métodos anteriores que combina las penalizaciones de las regresiones ridge y lasso:\n\\[\\sum_{=1}^n \\left ( y_i-\\beta_0-\\sum_{j=1}^p \\beta_jx_{ij}\\right )^2+\\lambda_1 \\sum_{j=1}^p \\beta_j^2+\\lambda_2 \\sum_{j=1}^p |\\beta_j|.\\]\nTambién aparece en muchas ocasiones de esta otra forma:\n\\[\\sum_{=1}^n \\left ( y_i-\\beta_0-\\sum_{j=1}^p \\beta_jx_{ij}\\right )^2+\\lambda \\left [ \\frac{1}{2} (1-\\alpha)\\sum_{j=1}^p \\beta_j^2+\\alpha \\sum_{j=1}^p |\\beta_j|\\right ],\\]\ndonde \\(\\alpha\\[0,1]\\). El parámetro \\(\\alpha\\) es que gobierna la combinación de las dos penalizaciones, mientras que \\(\\lambda\\) es el que controla la cantidad de penalización. Si \\(\\alpha=0\\) se está en el caso de la regresión ridge; \\(\\alpha=1\\) lleva la regresión lasso.La función glmnet() también sirve para ajustar elastic net, pero el parámetro \\(\\alpha\\) hay que elegirlo priori. Otra opción es utilizar el paquete caret para hacer validación cruzada sobre \\(\\alpha\\) y \\(\\lambda\\) simultáneamente:\nFigura 19.8: Valor de la raíz cuadrada del error cudrático medio para distintas combinaciones de \\(\\alpha\\) y \\(\\lambda\\)\nLa Fig. 19.8 muestra como la combinación de \\(\\alpha\\) y \\(\\lambda\\) da lugar diferentes \\(MSE\\) (en la figura aparece el \\(RMSE\\), o sea, su raíz cuadrada). Cada línea corresponde un valor de \\(\\lambda\\) distinto, y en el eje \\(x\\) se representan los valores de \\(\\alpha\\).continuación se calcula el valor del \\(MSE\\) en el conjunto de test para el modelo elastic net con \\(\\alpha=0,1\\) y \\(\\lambda=99,337\\), que son los valores de \\(\\alpha\\) y \\(\\lambda\\) que hacen mínimo dicho \\(MSE\\):Como puede comprobarse, es peor que el de la regresión ridge pero mejor que el de lasso.Por tanto, si se quiere hacer ningún tipo de selección debe estimar una regresión ridge y si se si se quiere reducir al máximo el número de variables predictoras se debe estimar una regresión lasso (costa de que aumente el \\(MSE\\) en el conjunto de test). El el equilibrio viene de la mano del modelo elastic-net, que hace selección de variables pero aumenta el \\(MSE\\) de predicción.Existen otros métodos de penalización que se derivan de estos, como el group lasso, el sparse group-lasso, etc. Se puede encontrar información sobre ellos en (Hastie Tibshirani 2015).","code":"\nset.seed(1)\nlibrary(\"caret\")\ncv_glmnet <- train(\n  x = x[entreno, ],\n  y = y[entreno],\n  method = \"glmnet\",\n  trControl = trainControl(method = \"cv\", number = 10),\n  tuneLength = 10\n)\n# modelo con el MSE más pequeño\ncv_glmnet$bestTune\n#>   alpha   lambda\n#> 9   0.1 99.12337\nggplot(cv_glmnet)\nelastic_mod <- glmnet(x[entreno, ], y[entreno], alpha = cv_glmnet$bestTune$alpha)\nelastic_pred <- predict(elastic_mod, newx = x[test, ], s = cv_glmnet$bestTune$lambda)\nmean((elastic_pred - y_test)^2)\n#> [1] 141626.1"},{"path":"cap-sparse.html","id":"resumen-17","chapter":"Capítulo 19 Modelos sparse y métodos penalizados de regresión","heading":"Resumen","text":"En este capítulo se introducen una serie de técnicas para mejorar la predicción y la interpretabilidad de los modelos de regresión. En particular:Se muestra el uso de la técnica de selección del mejor subconjunto de variables en el modelo, así como los métodos stepwise.Se muestra el uso de la técnica de selección del mejor subconjunto de variables en el modelo, así como los métodos stepwise.Se presentan 3 métodos tipo shrinkage: regresión ridge, lasso y elastic net, bien para la selección de variables, o para solventar problemas de multicolinealidad en el modelo.Se presentan 3 métodos tipo shrinkage: regresión ridge, lasso y elastic net, bien para la selección de variables, o para solventar problemas de multicolinealidad en el modelo.Se muestra cómo seleccionar el parámetro de penalización (o de conbinación de penalizaciones en el caso de la regresión elastic net) que controla la regresión penalizada.Se muestra cómo seleccionar el parámetro de penalización (o de conbinación de penalizaciones en el caso de la regresión elastic net) que controla la regresión penalizada.Se ilustra el uso de todas las metodologías propuestas en el capítulo mediante el análisis de un caso práctico.Se ilustra el uso de todas las metodologías propuestas en el capítulo mediante el análisis de un caso práctico.","code":""},{"path":"cap-series-temp.html","id":"cap-series-temp","chapter":"Capítulo 20 Modelización de series temporales","heading":"Capítulo 20 Modelización de series temporales","text":"Mª Carmen García CentenoUniversidad San Pablo-CEU, CEU Universities\n","code":""},{"path":"cap-series-temp.html","id":"conceptos","chapter":"Capítulo 20 Modelización de series temporales","heading":"20.1 Conceptos básicos","text":"El análisis de series temporales es muy útil para analizar el comportamiento de los datos con referencia temporal. Con dicho análisis, se trata de obtener modelos que expliquen su dinámica y puedan utilizarse para predecir valores futuros y tomar decisiones.Una serie temporal se puede definir como el conjunto de valores observados, en periodos consecutivos de tiempo de la misma amplitud, de una característica de interés en la que su valor en cada instante temporal es una variable aleatoria. Por consiguiente, una serie temporal es sino la realización de un proceso estocástico; por ejemplo: las precipitaciones diarias, la inflación mensual o el PIB trimestral.continuación se exponen algunos de los conceptos clave sobre los que se fundamenta el análisis de series temporales:Proceso estocástico. Es una sucesión de variables aleatorias \\(\\{Y_t\\}\\), donde \\(t= -\\infty,..., -2, -1, 0, 1,2,...,\\infty\\), que dependen de un parámetro. En el caso de las series temporales ese parámetro es el tiempo.145 Así, en cada momento del tiempo el proceso se particulariza en una variable aleatoria con una determinada distribución de probabilidad. Desde el punto de vista práctico es muy complicado, y veces imposible, caracterizar un proceso estocástico según su distribución de probabilidad conjunta. Por esta razón, se recurre caracterizarlo de una forma menos completa, pero más sencilla y práctica, basada en los dos primeros momentos de una distribución; en concreto: la media (\\(E(Y_t)=\\mu_t\\)), la varianza (\\(Var(Y_t)=E(Y_t-\\mu_t)^2=\\sigma_t^2\\)) y las covarianzas (\\(\\gamma(k)=Cov(Y_t,Y_{t-k})=E(Y_t-\\mu_t)(Y_{t-k}-\\mu_{t-k})\\)).Proceso estocástico. Es una sucesión de variables aleatorias \\(\\{Y_t\\}\\), donde \\(t= -\\infty,..., -2, -1, 0, 1,2,...,\\infty\\), que dependen de un parámetro. En el caso de las series temporales ese parámetro es el tiempo.145 Así, en cada momento del tiempo el proceso se particulariza en una variable aleatoria con una determinada distribución de probabilidad. Desde el punto de vista práctico es muy complicado, y veces imposible, caracterizar un proceso estocástico según su distribución de probabilidad conjunta. Por esta razón, se recurre caracterizarlo de una forma menos completa, pero más sencilla y práctica, basada en los dos primeros momentos de una distribución; en concreto: la media (\\(E(Y_t)=\\mu_t\\)), la varianza (\\(Var(Y_t)=E(Y_t-\\mu_t)^2=\\sigma_t^2\\)) y las covarianzas (\\(\\gamma(k)=Cov(Y_t,Y_{t-k})=E(Y_t-\\mu_t)(Y_{t-k}-\\mu_{t-k})\\)).Estacionariedad. Un proceso estocástico es estacionario, en sentido simple o amplio, si su media y la varianza se mantienen invariantes lo largo del tiempo y las covarianzas entre dos variables solo dependen del lapso de tiempo que transcurre entre ellas. Es decir:Estacionariedad. Un proceso estocástico es estacionario, en sentido simple o amplio, si su media y la varianza se mantienen invariantes lo largo del tiempo y las covarianzas entre dos variables solo dependen del lapso de tiempo que transcurre entre ellas. Es decir:Función de autocovarianzas. Está formada por el conjunto de autocovarianzas calculadas para distintos órdenes \\((k)\\). Tiene como finalidad determinar las relaciones de dependencia lineales que existen entre las variables del proceso con el fin de identificar el modelo que mejor explica su dinámica lo largo del tiempo.Función de autocovarianzas. Está formada por el conjunto de autocovarianzas calculadas para distintos órdenes \\((k)\\). Tiene como finalidad determinar las relaciones de dependencia lineales que existen entre las variables del proceso con el fin de identificar el modelo que mejor explica su dinámica lo largo del tiempo.Función de autocorrelación simple (ACF). Está formada por el conjunto de coeficientes de correlación lineales calculados para distintos órdenes \\((k)\\). De forma genérica, para medir la correlación lineal existente entre \\(Y_t\\) e \\(Y_{t-k}\\), el coeficiente de correlación de orden \\(k\\), \\(\\rho(k)\\), se calcula como el cociente entre la covarianza de y la varianza (ya que, como el proceso es estacionario, las desviaciones típicas de \\(Y_t\\) e \\(Y_{t-k}\\) son iguales).Función de autocorrelación simple (ACF). Está formada por el conjunto de coeficientes de correlación lineales calculados para distintos órdenes \\((k)\\). De forma genérica, para medir la correlación lineal existente entre \\(Y_t\\) e \\(Y_{t-k}\\), el coeficiente de correlación de orden \\(k\\), \\(\\rho(k)\\), se calcula como el cociente entre la covarianza de y la varianza (ya que, como el proceso es estacionario, las desviaciones típicas de \\(Y_t\\) e \\(Y_{t-k}\\) son iguales).\\[\\begin{equation}\n\\rho(k) = \\frac{Cov(Y_t,Y_{t-k})}{(\\sqrt{ Var(Y_t)})(\\sqrt {Var(Y_{t-k})})}=\\frac{\\gamma(k)}{\\gamma(0)}.\n\\end{equation}\\]La representación gráfica de los coeficientes de correlación \\(\\rho(k)\\) para \\(k=0,1,2,…\\) se conoce como .Función de autocorrelación parcial (PACF). Está formada por el conjunto de las correlaciones lineales parciales obtenidas para los distintos valores de \\(k\\). Así, para dos instantes de una serie temporal \\(t\\) y \\((t-k)\\), mide la correlación lineal existente entre las variables \\(Y_{t}\\) e \\(Y_{t-k}\\) asociadas ellos, ajustada de los valores que toma el proceso temporal en los periodos intermedios (\\(Y_{t-1},Y_{t-2},..., Y_{t-(k-1)}\\)).146Función de autocorrelación parcial (PACF). Está formada por el conjunto de las correlaciones lineales parciales obtenidas para los distintos valores de \\(k\\). Así, para dos instantes de una serie temporal \\(t\\) y \\((t-k)\\), mide la correlación lineal existente entre las variables \\(Y_{t}\\) e \\(Y_{t-k}\\) asociadas ellos, ajustada de los valores que toma el proceso temporal en los periodos intermedios (\\(Y_{t-1},Y_{t-2},..., Y_{t-(k-1)}\\)).146Ergodicidad. Un proceso estocástico es ergódico cuando, partir de un determinado desfase temporal entre las variables, la correlación lineal existente entre ellas tiende desaparecer. Esto implica que las covarianzas y el coeficiente de correlación tienden cero, es decir,Ergodicidad. Un proceso estocástico es ergódico cuando, partir de un determinado desfase temporal entre las variables, la correlación lineal existente entre ellas tiende desaparecer. Esto implica que las covarianzas y el coeficiente de correlación tienden cero, es decir,\\[\\lim\\limits_{k \\\\infty}\\gamma(k)=0 \\quad  y \\quad  \\lim\\limits_{k \\\\infty}\\rho(k)=0\\]Ruido blanco. Es un proceso puramente aleatorio que se puede expresar de la siguiente forma:\n\\[Y_t=a_t.\\]\nSe caracteriza por tener esperanza nula, varianza constante y covarianzas nulas. Es decir,\n\\[E(a_t)=0 \\quad\\forall t;  \\quad E(a_t^2)=\\sigma^2 \\quad\\forall t;\\quad  E(a_ta_s)=0 \\quad\\forall t\\neq s.\\]\nEsto implica que un ruido blanco siempre es estacionario.","code":""},{"path":"cap-series-temp.html","id":"modelos-arima","chapter":"Capítulo 20 Modelización de series temporales","heading":"20.2 Modelos ARIMA","text":"Para determinar las características del proceso estocástico subyacente la serie temporal, se van utilizar los modelos ARIMA (acrónimo del inglés AutoRegresive Integrated Moving Average). Estos modelos están formados por tres componentes: AR (autorregresivo), (integrado, es decir, número de diferencias necesarias para convertirlo en estacionario cuando lo es) y MA (medias móviles). Fueron propuestos por Box y Jenkins y son un caso paraticular de procesos estocásticos lineales, estacionarios, ergódicos y discretos. Entre este tipo de procesos lineales, los más frecuentes son:Los modelos ARIMA tratan de captar la dinámica y la dependencia existente en los datos de una serie temporal. Por lo tanto, son muy útiles para describir un valor como una combinación o función lineal de valores pasados y errores debidos al azar. Para detalles sobre la cuestión pueden verse, entre muchos otros, Hamilton (1994), Uriel Jiménez Peiro Giménez (2000), Cryer Chan (2010), Pemberton (2011), Mínguez Salido García Centeno (2011), Brockwell Davis (2016) y Shumway Stoffer (2017).En la modelización ARIMA se pueden destacar varias fases. La primera se centra en la identificación del modelo ARIMA que haya podido generar los datos de la serie temporal. En esta fase, es necesario decidir los órdenes del proceso (es decir, el desfase temporal entre el mayor y menor periodo de tiempo de las variables incluidas en el modelo) tanto de la parte regular como de la estacional.La ACF y la PACF desempeñan un papel clave en la identificación de estos órdenes. Antes de calcular la ACF y la PACF, es necesario comprobar que la serie es estacionaria, es decir, que deambula ni tiene tendencia creciente o decreciente. En el caso de que lo sea, se realizarán las transformaciones necesarias para convertirla en estacionaria, ya que la modelización ARIMA exige que los datos sean estacionarios. Las dos razones fundamentales por las cuales pueden ser estacionarios son: la constancia en el tiempo de la media o la existencia de fluctuaciones de diferente amplitud que hacen que la varianza se mantenga constante.Si la serie es estacionaria en varianza, de entre las transformacione Box-Cox, se puede utilizar una transformación logarítimica porque reduce el rango dinámico de la variable, corrige la asimetría positiva y acerca la normalidad la distribución de la variable; además, facilita la interpretación de las estimaciones de los coeficientes del modelo, ya que la diferencia del logaritmo de la serie es, aproximadamente, su tasa de variación porcentual.Si es estacionaria en media, puede ser debido la existencia de tendencia en el tiempo o de estacionalidad. Para eliminar la tendencia se calculan las diferencias regulares (una o dos como máximo según el tipo de tendencia lineal o lineal, es decir, \\(Y_t-Y_{t-1}=\\Delta Y_t\\) o \\(\\Delta^2 Y_t=\\Delta(\\Delta Y_t)\\), respectivamente). Para eliminar la estacionalidad, lo que se calcula es una diferencia estacional (es decir, \\(Y_t-Y_{t-s}=\\Delta_s Y_t\\)).Después de proponer el modelo, y tras dividir el conjunto de datos en dos subconjuntos (el de entrenamiento y el de test), se procede, con los datos del subconjunto apropiado, la estimación del modelo, su validación y la realización de predicciones. Primero se obtienen las estimaciones de los parámetros del modelo propuesto, así como sus desviaciones típicas, y los residuos del modelo. Posteriormente, se aborda la fase de validación o diagnóstico y se realizan los contrastes necesarios para determinar si el modelo propuesto es adecuado o , es decir si los parámetros estimados son estadísticamente significativos o ; si el modelo estimado es estacionario e invertible; y si sus residuos siguen un proceso ruido blanco o . En el caso de que lo sean, será necesario corregir el modelo con la información proporcionada por los residuos hasta obtener un modelo cuyos residuos sean ruido blanco. Finalmente, se procede la utilización del modelo para predecir.\n","code":""},{"path":"cap-series-temp.html","id":"análisis-de-series-temporales-con-r","chapter":"Capítulo 20 Modelización de series temporales","heading":"20.3 Análisis de series temporales con R","text":"Algunas de las librerías que normalmente se suelen utilizar en la modelización de series temporales con R son:La libreria tseries permite manipular datos de series temporales; astsa es adecuada para analizar series de tiempo en los dominios de frecuencia y tiempo; forecast lleva cabo (y analiza) predicciones con series temporales; lubridate facilita el trabajo con fechas y horas; foreign es esencial para crear objetos en R importando datos de casi cualquier formato conocido, como por ejemplo SAS, SPSS, Stata, etc.; quantmod ayuda, de forma cuantitativa, en el desarrollo de estrategias y modelización mediante la utilización de estadísticas;readxl facilita la exportación de datos de Excel R; y “ggplot2” es muy útil para la realización de gráficos.En el caso real que se propone, se utilizan datos mensuales del INE (www.ine.es) correspondientes al índice general de precios al consumo (IPC) en el periodo muestral comprendido entre enero de 2002 y marzo de 2022. Los datos están en el fichero ìpc del paquete CDR del libro.Para trabajar con series temporales, la fecha es un requisito imprescindible. En caso de los datos tuviesen fecha, habría que incluirla. Por ejemplo, para series mensuales, el código sería:En el caso de fichero ipc la fecha figura en la primera columna del fichero.La representación gráfica de la serie original puede ayudar saber si la serie ha sido generada por un proceso estacionario o . Para obtener dicha representación, se ejecuta el siguiente código:\nFigura 20.1: Evolución del IPC entre enero 2002 y marzo 2022.\nLa descomposición aditiva o multiplicativa en \\(()\\) tendencia, \\((ii)\\) componente estacional, \\((iii)\\) componente cíclico y $(iv) componente irregular de la serie del IPC, así como su representación gráfica, también puede ayudar determinar si la serie es estacionaria o . El código para la descomposición aditiva de la serie es el siguiente:\nFigura 20.2: Descomposición aditiva del IPC\nTanto en el gráfico de la serie original del IPC (Fig. 20.1) como en el de su descomposición en componentes (Fig. 20.2), se aprecia que la serie tiene tendencia y componente estacional (mensual). Por lo tanto, es estacionaria en media, es decir, la media es distinta para diferentes meses y años; tampoco lo es en varianza, ya que la dispersión respecto de la media cambia lo largo del tiempo.Si una serie es estacionaria ni en media ni en varianza es necesario empezar corrigiendo la estacionariedad en varianza y, posteriormente, la estacionariedad en media.Las transformaciones de Box-Cox son las más utilizadas para corregir el problema de estacionariedad en varianza. De todas estas transformaciones, la más habitual es el logaritmo. Para obtener la representación gráfica del logaritmo de \\(Y_t\\), se ejecuta el siguiente código:\nFigura 20.3: Logaritmo del IPC\nCorregido el problema de la estacionariedad en varianza, en la Fig. 20.3 se aprecia que sigue sin ser estacionaria, ya que al tener estacionalidad y tendencia creciente es estacionaria en media. Para conseguir que la serie sea estacionaria es necesario corregir tanto su tendencia como su estacionalidad.Es indiferente cuál de los dos problemas se resuelve primero, pues el resultado final de la transformación es el mismo. En este caso, se empieza corrigiendo la tendencia. Para ello, se calcula una diferencia regular del logaritmo del IPC, obteniéndose la serie \\((dlogipc_{t} = log(ipc_t) - log(ipc_{t-1})\\). Al ser datos mensuales, esta transformación representa la tasa de variación relativa mensual del IPC. El código para calcular esta diferencia regular y su representación gráfica (Fig. 20.4) es:\nFigura 20.4: Diferencia regular del logaritmo del IPC\nCorregida la tendencia, continuación se corrige su estacionalidad. Para ello, sobre la diferencia regular del logaritmo del IPC, se calculará una diferencia estacional mensual \\((d12dlogipc_{t})= log(ipc_t) - log(ipc_{t-12})\\). El código para calcular esta diferencia estacional y su representación gráfica (Fig. 20.5) es:\nFigura 20.5: Diferencia estacional de la diferencia regular del logaritmo del IPC\nPara contrastar si una serie es estacionaria en media o , también se puede utilizar un test de raíces unitarias^[Una raíz unitaria es una característica de los procesos que evolucionan lo largo del tiempo que implica problemas de estacionariedad en media y también al realizar inferencias. partir de la ecuación incial de un AR:\n\\[ \\Delta Y_t=\\alpha +(\\phi-1)Y_{t-1}+a_t,\\] las hipótesis del contraste son:\\[ H_0: (\\phi-1)=0 \\]\\[ H_1: (\\phi-1)<0. \\]\nEn esta ecuación se pueden añadir más retardos de la variable, una tendencia (en series que claramente tengan una tendencia decreciente o creciente) o eliminar la constante (en series con media nula).]\nEn este caso se utiliza el test de raíces unitarias de Dickey-Fuller, que es un test unilateral en el que la hipótesis nula es la existencia de raíces unitarias y, por lo tanto, que la serie es estacionaria (es decir, es (1)), mientras que la hipótesis alternativa es que sí es estacionaria (es decir, la serie es (0)). Si se realiza el test sobre la serie original (ipc_ts) y sobre la serie transformada del IPC (d12dLogIPC), se comprueba que esta última es la transformación estacionaria. El código utilizado para realizar este contraste es:la luz del p-valor para los valores originales del IPC, si se preestablece un nivel de significación del 10%, se rechaza la hipótesis nula (y, por tanto, la serie es estacionaria). Sin embargo, para lala serie resultante tras las transformaciones anteriores (d12dLogipc) se rechaza la \\(H_0\\). Por lo tanto, se puede concluir que dicha transformación la ha convertido en estacionaria.","code":"\nlibrary(\"tseries\")\nlibrary(\"astsa\")\nlibrary(\"forecast\")\nlibrary(\"lubridate\")\nlibrary(\"foreign\")\nlibrary(\"quantmod\")\nlibrary(\"ggplot2\")\nipc <- CDR::ipc\nipc_ts <- ts(ipc$ipc, start = c(2002, 1), end = c(2022, 3), frequency = 12)\nipc$Time <- as.Date(ipc$Time)\nggplot(\n  data = ipc,\n  aes(x = Time, y = ipc)\n) +\n  theme(\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 15)\n  ) +\n  geom_line(colour = \"blue\")\ncomponentes_ts <- decompose(ipc_ts)\nautoplot(componentes_ts)\nlogipc <- log10(ipc_ts)\nts_logipc <- data.frame(value = logipc, Time = time(logipc))\nggplot( \n  data = ts_logipc,\n  aes(x = Time, y = logipc)\n) +\n  theme(\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 15)\n  ) +\n  geom_line(colour = \"red\")\ndlogipc <- diff(logipc, differences = 1)\nts_dlogipc <- data.frame(value = dlogipc, Time = time(dlogipc))\nggplot(\n  data = ts_dlogipc,\n  aes(x = Time, y = dlogipc)\n) +\n  theme(\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 15)\n  ) +\n  geom_line(colour = \"blue\")\nd12dlogipc <- diff(dlogipc, 12)\nts_d12dlogipc <- data.frame(value = d12dlogipc, Time = time(d12dlogipc))\n\nggplot(\n  data = ts_d12dlogipc,\n  aes(x = Time, y = d12dlogipc)\n) +\n  theme(\n    axis.title.x = element_text(size = 15),\n    axis.title.y = element_text(size = 15)\n  ) +\n  geom_line(colour = \"purple\")\nadf.test(ipc_ts)\n#>\n#> Augmented Dickey-Fuller Test\n#>\n#> data: ipc_ts\n#> Dickey-Fuller = -1.8396, Lag order = 6, p-value = 0.6433\n#> alternative hypothesis: stationary\nadf.test(d12dlogipc)\n#>\n#> Augmented Dickey-Fuller Test\n#>\n#> data: d12dlogipc\n#> Dickey-Fuller = -3.3727, Lag order = 6, p-value = 0.06002\n#> alternative hypothesis: stationary"},{"path":"cap-series-temp.html","id":"identificación-o-especificación-del-modelo","chapter":"Capítulo 20 Modelización de series temporales","heading":"20.3.1 Identificación o especificación del modelo","text":"partir de la transformación estacionaria del modelo es necesario calcular la ACF y la PACF muestrales, con el fin de identificar el modelo ARIMA más adecuado, es decir, especificar tanto el modelo como su correspondiente orden. Antes de hacerlo para el caso concreto del IPC, se analizan brevemente los principales modelos teóricos que pueden explicar la dinámica de una serie temporal. Estos modelos, para la parte regular (es decir, de la modelización de la dependencia asociada observaciones consecutivas de tiempo) pueden ser: autorregresivos puros (AR(\\(p\\))), medias móviles (MA(\\(q\\))) o mixtos ARMA(\\(p\\),\\(q\\)), donde \\(p\\) y \\(q\\) representan sus correspondientes órdenes, respectivamente. En el caso de la parte estacional (\\(s\\)), es decir, de la modelización de la dependencia asociada observaciones que distan entre sí \\(s\\)-periodos de tiempo o múltiplos de \\(s\\), se tiene: sAR(\\(P\\)), sMA(\\(Q\\)) o sARMA(\\(P\\),\\(Q\\)), donde \\(P\\) y \\(Q\\) representan, respectivamente, los órdenes de los modelos correspondientes la parte estacional. Destacar que para identificar el modelo de la parte estacional es necesario analizar los coeficientes correspondientes los retardos estacionales. Así, por ejemplo, en el caso de una serie mensual, habría que fijarse en los coeficientes correspondientes los retardos 12, 24, 36, etc., para una serie trimestral en los retardos, 4, 8, 12, 16, etc., y así sucesivamente.Antes de identificar el modelo correspondiente al IPC en el periodo muestral analizado, se muestran algunos ejemplos de modelos ARIMA, su ecuación general y el comportamiento de su ACF y PACF.En el caso de MA(q) o sMA(Q):\nLa ACF se anula para órdenes superiores q (o Q en el caso estacional). Es decir, solo los q primeros coeficientes son significativos (o Q en el caso estacional). El resto de los coeficientes son nulos (o estadísticamente nulos en el caso muestral).\nLa PACF decrece de forma exponencial o sinusoidal hacia cero.\nEn el caso de MA(q) o sMA(Q):La ACF se anula para órdenes superiores q (o Q en el caso estacional). Es decir, solo los q primeros coeficientes son significativos (o Q en el caso estacional). El resto de los coeficientes son nulos (o estadísticamente nulos en el caso muestral).La ACF se anula para órdenes superiores q (o Q en el caso estacional). Es decir, solo los q primeros coeficientes son significativos (o Q en el caso estacional). El resto de los coeficientes son nulos (o estadísticamente nulos en el caso muestral).La PACF decrece de forma exponencial o sinusoidal hacia cero.La PACF decrece de forma exponencial o sinusoidal hacia cero.Ejemplo de las ACF y PACF teóricas de los procesos estacionarios MA(1).La ecuación que describe la dinámica de un modelo MA(1) o ARMA(0,1) es:\\[Y_t=a_t-\\theta a_{t-1},\\]o bien, en forma polinómica:\\[Y_t=(1-\\theta L)a_t , \\]donde \\(L\\) es el operador de retardos y \\(a_t\\) es un ruido blanco, es decir: \\(E(a_t)=0 \\; \\;\\forall t\\); \\(E(a_t^2)=\\sigma^2 \\; \\; \\forall t\\); \\(E(a_ta_s)=0 \\quad\\forall t\\neq s\\).Este modelo siempre es estacionario (ya que se obtiene como una combinación de procesos ruido blanco) y para que sea invertible (es decir, para que se pueda expresar en función del pasado de la variable), es necesario que las raíces del polinomio estén fuera del circulo unidad, o lo que es lo mismo, que \\(|\\theta|<1\\).Su ACF teórica es:\\[\n\\rho(k)=\\left\\{\n\\begin{array}{ll}\n\\frac{-\\theta}{(1+\\theta^2)} & \\text{si } k=1 \\\\\n0   &  \\text{}\\forall k>1,\n\\end{array}\n\\right.\n\\]\ny su PACF (también teórica) viene dada por:\n\\[\n\\phi_{kk}=\\frac{-\\theta^k(1-\\theta^2)}{1-\\theta^{2(k+1)}}\\quad \\text {para} \\quad k\\geq 1.\n\\]En un AR(p) o sAR(P):\nLa ACF decrece rápidamente de forma exponencial o sinusoidal hacia cero.\nLa PACF se anula para órdenes superiores p (o P, en el caso estacional). Es decir, sólo los p (o P en el caso estacional) primeros coeficientes son significativamente distintos de cero. El resto de los coeficientes son nulos (o estadísticamente nulos en el caso muestral).\nEn un AR(p) o sAR(P):La ACF decrece rápidamente de forma exponencial o sinusoidal hacia cero.La ACF decrece rápidamente de forma exponencial o sinusoidal hacia cero.La PACF se anula para órdenes superiores p (o P, en el caso estacional). Es decir, sólo los p (o P en el caso estacional) primeros coeficientes son significativamente distintos de cero. El resto de los coeficientes son nulos (o estadísticamente nulos en el caso muestral).La PACF se anula para órdenes superiores p (o P, en el caso estacional). Es decir, sólo los p (o P en el caso estacional) primeros coeficientes son significativamente distintos de cero. El resto de los coeficientes son nulos (o estadísticamente nulos en el caso muestral).Ejemplo de las ACF y PACF teóricas de los procesos estacionarios AR(1).La ecuación que describe la dinámica de un modelo AR(1) o ARMA(1,0) es:\\[Y_t=\\phi Y_{t-1}+ a_t,\\]o bien, en forma polinómica: \\(Y_t(1-\\phi L)=a_t\\), donde \\(L\\) es el operador de retardos y \\(a_t\\) es un ruido blanco.Este modelo siempre es invertible (ya que está expresado en función del pasado de la variable) y para que sea estacionario es necesario que las raíces del polinomio de retardos estén fuera del circulo unidad, o lo que es lo mismo, que \\(|\\phi|<1\\).Su ACF teórica es:\\[\n\\rho(k)=\\phi\\rho(k-1)=\\phi^k,\n\\]mientras que su PACF (teórica) viene dada por:\\[\n\\phi_{kk}=\\left\\{\n\\begin{array}{ll}\n\\rho_1=\\phi & \\text{si } k=1 \\\\\n0   &  \\text{}\\forall k>1.\n\\end{array}\n\\right.\n\\]En un ARMA(p,q) o sARMA(P,Q):\nLa ACF tiene un comportamiento irregular en los q primeros (o en los Q en el caso estacional) coeficientes. partir del orden q (o Q en el caso estacional), se comporta como la de un AR(p) (o un sAR(P) en el caso estacional).\nLa PACF tiene un comportamiento irregular en los p primeros coeficientes (o P en el caso estacional). partir del orden p (o P en el caso estacional), se comporta como la de un MA(q) (o de un sMA(Q) en el caso estacional).\nEn un ARMA(p,q) o sARMA(P,Q):La ACF tiene un comportamiento irregular en los q primeros (o en los Q en el caso estacional) coeficientes. partir del orden q (o Q en el caso estacional), se comporta como la de un AR(p) (o un sAR(P) en el caso estacional).La ACF tiene un comportamiento irregular en los q primeros (o en los Q en el caso estacional) coeficientes. partir del orden q (o Q en el caso estacional), se comporta como la de un AR(p) (o un sAR(P) en el caso estacional).La PACF tiene un comportamiento irregular en los p primeros coeficientes (o P en el caso estacional). partir del orden p (o P en el caso estacional), se comporta como la de un MA(q) (o de un sMA(Q) en el caso estacional).La PACF tiene un comportamiento irregular en los p primeros coeficientes (o P en el caso estacional). partir del orden p (o P en el caso estacional), se comporta como la de un MA(q) (o de un sMA(Q) en el caso estacional).Ejemplo de las ACF y PACF teóricas de los procesos estacionarios ARMA(1,1).La ecuación que describe la dinámica de un modelo ARMA(1,1) es:\\(Y_t=\\phi Y_{t-1}+ a_t-\\theta a_{t-1}\\), o bien, en forma polinómica, \\(Y_t(1-\\phi L)=(1-\\theta L)a_t\\), donde \\(L\\) es el operador de retardos y \\(a_t\\) es un ruido blanco.Para que el modelo sea estacionario, es necesario que las raíces del polinomio de la parte autorregresiva estén fuera del círculo unidad (es decir, que \\(|\\phi|<1\\)).Para que sea invertible es necesario que las raíces del polinomio de las medias móviles estén fuera del círculo unidad (o que \\(|\\theta |<1\\)).Además, es necesario que existan raíces comunes, es decir, \\(\\phi \\neq \\theta\\).Su ACF teórica es:\\[\n\\rho(k)=\\left\\{\n\\begin{array}{ll}\n\\frac{(1-\\phi\\theta)(\\phi-\\theta)}{1+\\theta^2-2\\phi\\theta} & \\text{si } k=1 \\\\\n\\phi\\rho(k-1)   &  \\text{}\\forall k>1.\n\\end{array}\n\\right.\n\\]Su PACF teórica se obtiene como:\\[\n\\begin{array}{l}\n\\phi_{11}=\\rho_1 \\\\\n\\\\\n\\phi_{22}=\\frac{\\begin{vmatrix} 1&\\rho_1 \\\\ \\rho_1 &\\rho_2\\end{vmatrix}}{{\\begin{vmatrix} 1&\\rho_1 \\\\ \\rho_1 &1\\end{vmatrix}}}=\\frac{\\rho_2-\\rho_1^2}{1-\\rho_1^2}\\\\\n\\\\\n\\phi_{33}=\\frac{\\begin{vmatrix} 1&\\rho_1&\\rho_1 \\\\ \\rho_1 &1&\\rho_2\\\\\\rho_2 &\\rho_1&\\rho_3\\end{vmatrix}}{{\\begin{vmatrix} 1&\\rho_1&\\rho_2 \\\\ \\rho_1 &1&\\rho_1\\\\\\rho_2 &\\rho_1&1 \\end{vmatrix}}}=\\frac{\\rho_1^3-\\rho_1\\rho_2(2-\\rho_2)+\\rho_3(1-\\rho_1^2)}{1-\\rho_2^2-2\\rho_1^2(1-\\rho_2)}\\\\\n\\\\\n\\vdots\n\\end{array}\n\\]Nota:La forma de obtener los diferentes coeficientes de la ACF y PACF de los modelos ARIMA estacionales es la misma que para los modelos ARIMA utilizados para la parte regular.La diferencia fundamental reside en que para la parte regular se calculan los coeficientes de correlación entre las variables que se encuentran en periodos consecutivos de tiempo, mientras que para la parte estacional se hace entre las variables que están separadas entre si \\(s\\) periodos o multiplos de \\(s\\) (donde, por ejemplo, \\(s\\)=12, si la serie es mensual; \\(s\\)=4 si la serie fuese trimestral, etc.). Por esta razón, sólo se muestra la forma de calcular la ACF y la PACF correspondiente la parte regular.Tendiendo en cuenta lo anterior, en el caso concreto del IPC, después de obtener su transformación estacionaria, la ACF y la PACF muestrales son muy útiles para identificar el modelo ARIMA más adecuado, así como su correspondiente orden en el periodo muestral estudiado.continuación se muestra el código R para ejecutar los comandos que permiten calcular y representar la ACF y la PACF muestrales de la transformación estacionaria del IPC (Fig. 20.6 y Fig. 20.7, respectivamente). El número de retardos (lags) se establece en 40 para, así, incluir al menos 3 retardos estacionales (12, 24 y 36):\nFigura 20.6: ACF. Función de autocorrelación muestral de d12dLogIPC\n\nFigura 20.7: PACF. Función de autocorrelación parcial muestral de d12dLogIPC\nObservando el comportamiento de estas funciones, para la parte regular se podría proponer un AR(1) y para la parte estacional un MA(1)12. Por lo tanto, ya que se ha calculado una diferencia regular y otra estacional para convertir la serie original en estacionaria, el modelo ARIMA para el IPC, en el periodo muestral analizado, sería un ARIMA(1,1,0)(0,1,1)12 o sARIMA(1,1,0)(0,1,1). Señalar que R permite la identificación automática de los órdenes del modelo.","code":"\nacf(ts(d12dlogipc, frequency = 1), lag.max = 40, main = \"\")\nknitr::include_graphics(\"img/acf1.png\")\npacf(ts(d12dlogipc, frequency = 1), lag.max = 40, main = \"\")"},{"path":"cap-series-temp.html","id":"estimación-del-modelo","chapter":"Capítulo 20 Modelización de series temporales","heading":"20.3.2 Estimación del modelo","text":"Los parámetros del modelo se estiman por el método de máxima verosimilitud. Dado que la función de máxima verosimilitud es lineal, para maximizarla se lleva cabo un procedimiento iterativo de estimación lineal. El código en R que permite obtener las estimaciones de los parámetros del modelo y sus correspondientes desviaciones típicas es:","code":"\nmodelo <- arima(ipc_ts, c(1, 1, 0), c(0, 1, 1))\nmodelo\n#>\n#> Call:\n#> arima(x = ipc_ts, order = c(1, 1, 0), seasonal = c(0, 1, 1))\n#>\n#> Coefficients:\n#> ar1 sma1\n#> 0.4665 -0.8302\n#> s.e. 0.0701 0.0860\n#>\n#> sigmaˆ2 estimated as 0.1082: log likelihood = -77.76, aic = 161.51"},{"path":"cap-series-temp.html","id":"validación-1","chapter":"Capítulo 20 Modelización de series temporales","heading":"20.3.3 Validación","text":"En la metodología ARIMA desarrollada por Box y Jenkins es necesario determinar si el modelo propuesto es correcto o , es decir, si se ajusta o correctamente los datos de la muestra. Para ello, en la fase de validación, fundamentalmente, es necesario comprobar que:Los parámetros del modelo estimado cumplen con las condiciones de estacionariedad e invertibilidad.El modelo estimado para el IPC es invertible y estacionario, ya que () la parte regular, al modelizarse mediante un AR(1), siempre va ser invertible; además, dado que la estimación de \\(\\phi\\) es, en valor absoluto, menor que uno, es decir, \\(|0.4665| <1\\), también es estacionario; (ii) la parte estacional sigue un esquema de medias móviles de primer orden (sMA(1)), que siempre es estacionario; como, adicionalmente, el valor absoluto de la estimación de \\(\\theta\\) es menor que uno (\\(|-0.8302|<1\\)), también es invertible.Si alguna de las raíces del polinomio de retardos de la parte autorregresiva estuviese próxima uno, entonces es posible que la serie original este subdiferenciada y, por lo tanto, sería estacionaria, lo que implicaría la necesidad de calcular alguna diferencia adicional. Si fuese alguna de las raíces del polinomio de retardos de las medias móviles la que estuviese próxima uno, entonces el modelo podría estar sobrediferenciado y habría que eliminar alguna diferencia. Además, si existiesen raíces comunes en ambos polinomios de retardos de la parte regular, sería necesario eliminar diferencias tanto en la parte autorregresiva como en la parte de medias móviles y el modelo correcto sería un ARIMA(p-1,d,q-1). Los comentarios anteriores son válidos para la parte estacional.Los parámetros estimados son estadísticamente significativos.Para comprobar si los parámetros son estadísticamente significativos o , (o en otros términos, para comprobar si se ha sobreparametrizado o el modelo), se realiza un contraste de significatividad individual para cada uno de ellos. Por ejemplo, para el parámetro del AR(1) la hipótesis nula y alternativa serían:\n\\[\n\\begin{array}{l}\nH_0:\\phi=0 \\\\\nH_1:\\phi\\neq 0.\n\\end{array}\n\\]Para realizar el contraste se utiliza el estadístico \\(t\\), que sigue una distribución t-Student con \\((n-k)\\) grados de libertad. Este estadístico \\(t\\) se calcula dividiendo el valor estimado del parámetro entre su correspondiente error estándar. En concreto, para los dos parámetros estimados de este modelo (el correspondiente al AR(1) de la parte regular y al sMA de la parte estacional) se tiene:\\[\nt=\\frac{0,4665}{0,0701} = 6,654 \\quad y\\quad |t|=|\\frac{-0,8302}{0,0860}|=9,653.\n\\]Como, para un nivel de significación del 5% y 248 grados de libertad, el valor crítico de la t-Student es aproximadamente 1,96, se rechaza \\(H_0\\), lo que implica que las estimaciones de los dos parámetros del modelo son estadísticamente distintas de cero.Los residuos del modelo son ruido blanco.Como se avanzó al final de la Sec. 20.1, uno de los supuestos de la modelización ARIMA es que el error del modelo (o perturbación aleatoria) tiene que ser ruido blanco. Como los errores son observables, las comprobaciones se llevan cabo sobre los residuos (diferencia entre los valores reales y los predichos por el modelo). Existen varias formas de comprobar si los residuos son ruido blanco o . Entre ellas, las más utilizadas son: el gráfico de la serie original de residuos, la ACF y la PACF estimadas y el contraste de Portmanteau (planteado inicialmente por Box-Pierce y actualizado posteriormente por Ljung-Box).En primer lugar, de forma intuitiva, el gráfico de los residuos puede mostrar si la media es constante e igual cero y si su varianza también es constante. Además, puede reflejar si existen valores atípicos u outliers (se consideran como tales aquellos que superen tres veces su desviación típica). El siguiente código R permite obtener los residuos y su representación gráfica (Fig. 20.8).\nFigura 20.8: Gráfico de los residuos\nEn la Fig. 20.8 se observa que la media es constante e igual cero y que partir de febrero de 2022, como consecuencia de las tensiones inflacionarias, los residuos son mayores que en el resto del periodo muestral.En segundo lugar, se pueden calcular la ACF y la PACF muestrales de los residuos para comprobar que están incorrelacionados, es decir, que los coeficientes de correlación calculados son estadísticamente nulos. Si éstos fuesen estadísticamente significativos, los residuos serían ruido blanco. Entonces, para la correcta modelización habría que identificar el proceso e incorporarlo al modelo inicial propuesto para volver estimarlo. Los correlogramas obtenidos (Fig. 20.9) con el código que se muestra continuación, indican que hay coeficientes de correlación estadísticamente significativos (ya que se encuentran dentro de los intervalos de confianza) y, por lo tanto, los residuos son ruido blanco.147\nFigura 20.9: ACF y PACF estimadas de los residuos\nTambién se puede utilizar el contraste de Portmanteau (Ljung-Box) para comprobar si los residuos están incorrelacionados y se comportan como un ruido blanco o . En este contraste global de significación, la hipótesis nula que se plantea, en el caso del IPC, es que los primeros 40 coeficientes de correlación son cero frente la hipótesis alternativa de que lo son. Para un nivel de significación del 5%, la evidencia empírica es suficiente para rechazar \\(H_0\\), ya que el p-valor correspondiente al estadístico del contraste (X-squared o Chi-cuadrado) es 0.5448, mayor que el nivel de significación del 5% y, por lo tanto, se concluye que los residuos están incorrelacionados. El código R para llevar cabo el test de Box-Ljung es:Análisis de la bondad del ajuste.Finalmente, se debe comprobar la capacidad de ajuste del modelo comparando los valores observados y los estimados. Una forma sencilla e intuitiva de hacerlo es través de su representación gráfica. El código R utilizado para ello es el siguiente:\nFigura 20.10: Ajuste con el modelo estimado.\nLa Fig. 20.10 muestra que el modelo estimado se ajusta bastante bien los valores observados y, por lo tanto, evidencia que el modelo sARIMA (1,1,0)(0,1,1) propuesto capta la dinámica del IPC en el periodo muestral analizado.Para comprobar la adecuación entre el modelo estimado y los valores observados suele ser adecuado utilizar el coeficiente de determinación o el coeficiente de determinación corregido, ya que si hay que calcular diferencias para convertir la serie en estacionaria, la variable dependiente cambia (es decir, es lo mismo \\(Y_t\\), que \\(dlog(Y_t)\\), que \\(d12dlog(Y_t)\\)). Si se quieren comparar diferentes modelos para elegir cuál es el que mejor se ajusta los datos, hay que utilizar otros métodos de información, como el criterio de información de Akaike (AIC), el criterio bayesiano o de Schwarz (BIC) o el de Hannan-Quinn (HQC). Si, por ejemplo, se eligiese el AIC será mejor el modelo con menor AIC.","code":"\nresiduos <- residuals(modelo)\nts_residuos <- data.frame(value = residuos, Time = time(residuos))\nggplot(\n  data = ts_residuos,\n  aes(\n    x = Time,\n    y = residuos\n  )\n) +\n  geom_line(colour = \"red\")\npar(mfrow = c(2, 1), mar = c(1, 1, 1, 1) + 0.1)\nacf(ts(residuos, frequency = 1), lag.max = 40)\npacf(ts(residuos, frequency = 1), lag.max = 40)\nBox.test(residuos, type = \"Ljung-Box\")\n#>\n#> Box-Ljung test\n#>\n#> data: residuos\n#> X-squared = 0.36682, df = 1, p-value = 0.5447\nplot(ipc_ts)\nlines(ipc_ts - modelo$residuals, col = \"red\")"},{"path":"cap-series-temp.html","id":"predicción-2","chapter":"Capítulo 20 Modelización de series temporales","heading":"20.3.4 Predicción","text":"Después de comprobar que el modelo ARIMA estimado es adecuado, se puede utilizar para obtener valores futuros de la variable objeto de análisis. Las predicciones que se obtienen pueden ser de dos tipos: puntuales o por intervalos. Las predicciones puntuales para un horizonte temporal h se obtienen calculando el valor esperado de la variable dependiente en \\(T+h\\) \\((Y_{T+h})\\)) condicionado al conjunto de información disponible hasta el momento actual (\\(T\\)). Para obtener las predicciones por intervalos es necesario sumar y restar la predicción puntual la desviación típica del error de predicción multiplicada por el valor crítico tabulado para el nivel de confianza fijado.Algunas características generales de las prediciones obtenidas con modelos ARIMA son:En el caso concreto del IPC, para obtener 12 predicciones puntuales y sus correspondientes intervalos de predicción al 80% y 95% de confianza, así como su representación gráfica (Fig. 20.11), el código R que hay utilizar es:\nFigura 20.11: Predicciones con el modelo ARIMA(1,1,0)(0,1,1)12 estimado\n","code":"\nforecast::forecast(modelo, h = 12)\n#>    Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n#> Apr 2022 109.7116 109.2900 110.1332 109.0668 110.3564\n#> May 2022 110.5926 109.8443 111.3410 109.4481 111.7371\n#> Jun 2022 111.1030 110.0714 112.1346 109.5253 112.6806\n#> Jul 2022 110.5518 109.2748 111.8289 108.5987 112.5050\n#> Aug 2022 110.7714 109.2787 112.2641 108.4885 113.0543\n#> Sep 2022 111.0288 109.3435 112.7140 108.4515 113.6060\n#> Oct 2022 111.9631 110.1034 113.8228 109.1189 114.8073\n#> Nov 2022 112.1740 110.1540 114.1939 109.0847 115.2632\n#> Dec 2022 112.3896 110.2209 114.5583 109.0728 115.7064\n#> Jan 2023 111.6049 109.2968 113.9130 108.0750 115.1349\n#> Feb 2023 111.6666 109.2270 114.1061 107.9355 115.3976\n#> Mar 2023 112.5012 109.9369 115.0656 108.5794 116.4231\n\npredicciones <- forecast::forecast(modelo, h = 12)\nautoplot(predicciones)"},{"path":"cap-series-temp.html","id":"resumen-18","chapter":"Capítulo 20 Modelización de series temporales","heading":"Resumen","text":"Los modelos univariantes ARIMA analizan las series de tiempo desde un punto de vista moderno y estocástico y son muy útiles para captar su dinámica. Es importante destacar que:Hay que tener en cuenta conceptos básicos que son fundamentales para llevar cabo una correcta modelización ARIMA. Entre ellos, se pueden destacar los siguientes: proceso estocástico, estacionalidad, estacionariedad, invertibilidad y ruido blanco.Hay que tener en cuenta conceptos básicos que son fundamentales para llevar cabo una correcta modelización ARIMA. Entre ellos, se pueden destacar los siguientes: proceso estocástico, estacionalidad, estacionariedad, invertibilidad y ruido blanco.Las principales fases en el análisis de un modelo ARIMA son: especificación, estimación, validación y predicción.Las principales fases en el análisis de un modelo ARIMA son: especificación, estimación, validación y predicción.Para identificar el modelo ARIMA que mejor capta la dinámica de la serie temporal objeto de análisis, es necesario que los datos sean estacionarios en media y en varianza.Para identificar el modelo ARIMA que mejor capta la dinámica de la serie temporal objeto de análisis, es necesario que los datos sean estacionarios en media y en varianza.La ACF y la PACF muestrales son muy útiles para identificar el modelo más adecuado para explicar la dinámica de los datos.La ACF y la PACF muestrales son muy útiles para identificar el modelo más adecuado para explicar la dinámica de los datos.Para comprobar que el modelo estimado es adecuado es necesario realizar un análisis de los parámetros estimados, un análisis de los residuos y medir la bondad del ajuste.Para comprobar que el modelo estimado es adecuado es necesario realizar un análisis de los parámetros estimados, un análisis de los residuos y medir la bondad del ajuste.Validado el modelo, se suele utilizar, fundamentalmente, para predecir y tomar decisiones.Validado el modelo, se suele utilizar, fundamentalmente, para predecir y tomar decisiones.","code":""},{"path":"cap-discriminante.html","id":"cap-discriminante","chapter":"Capítulo 21 Análisis discriminante","heading":"Capítulo 21 Análisis discriminante","text":"Mª Leticia Meseguer Santamaría\\(^{}\\) y Manuel Vargas Vargas\\(^{}\\)\\(^{}\\)Universidad de Castilla-La Mancha","code":""},{"path":"cap-discriminante.html","id":"introducción-10","chapter":"Capítulo 21 Análisis discriminante","heading":"21.1 Introducción","text":"El análisis discriminante (AD) es una técnica de dependencia orientada la clasificación de individuos en grupos (o poblaciones) preexistentes y con ciertas características conocidas, utilizando para ello la información proporcionada por un conjunto de variables clasificadoras.148 La clasificación se realiza mediante funciones discriminantes, combinaciones de las clasificadoras originales y que se emplean como criterio para asignar cada individuo un grupo o población.De esta forma, en el análisis discriminante se identifican dos finalidades: la descriptiva, caracterizando la separación entre grupos al proporcionar la contribución de cada variable clasificadora dicha separación; y la predictiva, estableciendo el criterio de clasificación de un individuo nuevo en alguno de los grupos partir de sus valores para las variables clasificadoras.El problema de la discriminación puede plantearse de diversas formas y aparece en numerosas áreas de investigación. El AD se incluye en los modelos denominados supervisados, puesto que se conoce priori que grupo o población está asignado cada individuo de la muestra, y se utiliza en campos tan diferentes como los sistemas automáticos de concesión de créditos bancarios (credit scoring), clasificación de pacientes en función de pruebas diagnósticas, atribución de obras literarias o pictóricas autores, o en control de calidad, cuando la información es muy costosa o requiera la destrucción de las unidades; entre otros. En el campo de la ingeniería, la discriminación se conoce como reconocimiento de patrones (pattern recognition) y es utilizada para el diseño de máquinas de clasificación automática (reconocimiento de billetes o monedas, sonidos, etc.).Aunque existen varios enfoques diferentes, en este capítulo se adoptará el enfoque clásico de Fisher, que asume la normalidad multivariante de las variables clasificadoras. Como punto de partida, para un AD se considera:Un conjunto de N individuos, de los que se conoce su grupo de pertenencia. Esta información se resume en una variable categórica \\(Y\\) cuyas categorías son los distintos grupos.Un conjunto de N individuos, de los que se conoce su grupo de pertenencia. Esta información se resume en una variable categórica \\(Y\\) cuyas categorías son los distintos grupos.Un conjunto de k grupos (\\(k \\geq 2\\)) con, al menos, dos individuos en cada uno de ellos.Un conjunto de k grupos (\\(k \\geq 2\\)) con, al menos, dos individuos en cada uno de ellos.Un conjunto de p variables, medidas en intervalo o razón. Estas variables deben presentar multicolinealidad, es decir, ninguna clasificadora puede ser combinación lineal de otras clasificadoras. Además, dado el enfoque adoptado, se asume que estas variables siguen una distribución normal multivariante.149Un conjunto de p variables, medidas en intervalo o razón. Estas variables deben presentar multicolinealidad, es decir, ninguna clasificadora puede ser combinación lineal de otras clasificadoras. Además, dado el enfoque adoptado, se asume que estas variables siguen una distribución normal multivariante.149El número de variables discriminantes debe ser inferior en más de dos al número de individuos (\\(p<N-2\\)) para poder identificar los parámetros de las funciones discriminantes. Además, en la práctica, es útil disponer de algún criterio o método que permita seleccionar qué variables se considerarán clasificadoras. Una alternativa pueden ser los métodos de jerarquización de variables desarrollados en análisis de regresión o la selección de variables (feature selection, véase Cap. 9). Como punto inicial, es frecuente que se considere que una variable puede ser clasificadora si presenta diferencias en su distribución entre los grupos, utilizando para ello un ANOVA.Así, el AD busca determinar un criterio o regla discriminante que clasifique cada individuo, \\(j, \\hspace{0,2cm}j=1,..., N\\), en uno de los \\(k\\) grupos conociendo las observaciones de cada una de las \\(p\\) variables \\(X_i\\), es decir, el vector \\({\\bf{x}}_j=(x_{1j},x_{2,j}, ... , x_{pj})'\\). Estas reglas discriminantes están basadas en la información muestral y en los supuestos que sobre ésta se hacen; en el planteamiento clásico de Fisher, al asumir la normalidad de las variables, se basan en el comportamiento, en los \\(k\\) grupos, de los vectores de medias y de las matrices de varianzas-covarianzas (véase Sec. ??). Por ello, se suelen distinguir varios casos que conducen distintos métodos de obtención de reglas discriminantes, por lo que reciben nombres diferentes:El caso más sencillo (e históricamente el más antiguo), además de la normalidad, supone que las matrices de varianzas-covarianzas son iguales en todos los grupos (supuesto de homocedasticidad). El método se conoce como análisis discriminante lineal (linear discriminant analysis o LDA). En este caso, detallado en la Sec. ??, la diferencia en la distribución de las variables entre los grupos se produce en los vectores de medias, y la función discriminante obtenida es una combinación lineal de las variables clasificadores que minimiza los errores de clasificación. El caso más sencillo (e históricamente el más antiguo), además de la normalidad, supone que las matrices de varianzas-covarianzas son iguales en todos los grupos (supuesto de homocedasticidad). El método se conoce como análisis discriminante lineal (linear discriminant analysis o LDA). En este caso, detallado en la Sec. ??, la diferencia en la distribución de las variables entre los grupos se produce en los vectores de medias, y la función discriminante obtenida es una combinación lineal de las variables clasificadores que minimiza los errores de clasificación. Otra posibilidad es que se asuma la normalidad pero que todos los grupos tengan la misma matriz de varianzas-covarianzas. En este caso, la función discriminante es una función cuadrática, por lo que el método se conoce como análisis discriminante cuadrático (quadatric discriminant analysis o QDA), detallado en la Sec. ??. Otra posibilidad es que se asuma la normalidad pero que todos los grupos tengan la misma matriz de varianzas-covarianzas. En este caso, la función discriminante es una función cuadrática, por lo que el método se conoce como análisis discriminante cuadrático (quadatric discriminant analysis o QDA), detallado en la Sec. ??. Sea cual sea el método elegido, las reglas discriminantes que se obtengan para clasificar un individuo en uno de los grupos deben determinarse minimizando los errores de clasificación, que pueden ser evaluados probabilísticamente al disponer de la distribución de probabilidad de las variables en cada grupo. Así, para cada individuo \\(j\\) y sus valores de las variables clasificadoras \\({\\bf{x}}_j=(x_{1j},x_{2,j}, ... , x_{pj})'\\), se dispone de las verosimilitudes para cada uno de los \\(k\\) grupos, \\(L_i\\left( {\\bf{x}}_j;{\\bf{\\theta}}_i \\right )\\ ,1\\leq \\leq k\\), donde en el vector \\({\\bf{\\theta}}_i\\) se recogen los parámetros de la distribución probabilísitica de las variables (en el caso de normalidad, dichos parámetros son la media y la desviación típica).Conociendo la probabilidad priori de pertenencia de un individuo cada grupo,150 \\(\\pi_i \\, \\ 1 \\leq \\leq k\\), se puede aplicar el teorema de Bayes (véase (12.3)) y calcular la probabilidad de que el individuo pertenezca cada uno de los grupos \\(G_i,\\hspace{0,2cm} =1,...,k\\). Por ejemplo, para el m-ésimo grupo:\\[\\begin{equation}\n\\tag{21.1}\nP(G_m/ {\\bf{x}}_j) = \\frac {L_m({\\bf{x}}_j;{\\bf{\\theta}} _m) \\pi _m} {\\sum_{=1}^{k} L_i ({\\bf{x}}_j;{\\bf{\\theta}} _i) \\pi _i}, \\hspace{0,5cm} m \\[1,...,k].\n\\end{equation}\\]partir de esta ecuación, la regla discriminante consiste en asignar el individuo al grupo más probable. Dado que el denominador de (21.1) es constante para todos los grupos, la regla equivale asignar el individuo al grupo donde sea ponderadamente más verosímil. Es decir, el j-ésimo individuo se clasifica en el m-ésimo grupo si:\\[\\begin{equation}\n\\tag{21.2}\nL_m({\\bold {x}}_j;{\\bold{\\theta}}_m) \\pi_m = \\underset {=1,...,k}{\\max} L_i({\\bf{x}}_j;{\\bold {\\theta}}_i) \\pi_i,\n\\end{equation}\\]ecuación que se simplifica en el caso de igual probabilidad priori, resultando la regla discriminante en asignar cada individuo al grupo más verosímil.En general, se pueden cometer dos tipos de error: clasificar al individuo en un grupo cuando realmente pertenece él; o clasificarlo en un grupo al que realmente pertenece. Si se conocen los costes de cometer dichos errores (o son iguales), afectan la regla discriminante; sin embargo, si son conocidos y han de ser tenidos en cuenta, la regla se modificaría, ponderando cada verosimilitud \\(L_i({\\bf{x}}_j;{\\bf{\\theta}}_i) ,\\hspace{0,2cm} =1,...,k,\\) por el coste de clasificar erróneamente un individuo perteneciente al -ésimo grupo.En las secciones siguientes se abordarán ambos modelos de AD que, aunque son los únicos, sí representan la gran mayoría de las aplicaciones prácticas.","code":""},{"path":"cap-discriminante.html","id":"id_150025lda","chapter":"Capítulo 21 Análisis discriminante","heading":"21.2 Análisis discriminante lineal","text":"Es un modelo de AD basado en los supuestos generales expuestos en el epígrafe anterior (\\(N\\) individuos, \\(k\\) grupos y \\(p\\) variables clasificadoras con distribución normal y sin multicolinealidad) y caracterizado por la igualdad de las matrices de varianza-covarianza de las variables en todos los grupos. Para la exposición de la metodología, se presenta el caso más sencillo, con sólo dos grupos y probabilidades priori iguales, para generalizarlo posteriormente al caso general de \\(k\\) grupos.Dos grupos y una variable clasificadora.Es el caso más simple posible, donde se han de clasificar \\(N\\) individuos en con dos grupos (y II) partir de la información de una única variable clasificadora, \\(X\\). En este caso, las distribuciones de probabilidad de \\(X\\) en los grupos y II solo difieren en la media, como se muestra en la Fig. 21.1.\nFigura 21.1: LDA: dos grupos y una variable clasificadora\nLa regla discriminante consistirá en asignar cada individuo al grupo con mayor verosimilitud (Eq. (21.2)). Como se aprecia, esta regla divide la recta real en dos partes, la izquierda y la derecha de \\(C\\), que es el el valor de la recta correspondiente al corte entre las funciones de densidad de los grupos y II:\\[\\begin{equation}\nC=\\frac{\\overline{x}_I+\\overline{x}_{II}}{2},\n\\end{equation}\\]quedando la asignación de cada individuo como sigue:151\\[\\begin{equation}\n\\text{si } x_j<C \\\\text{ Grupo y si } x_j>C \\\\text{ Grupo II}.\n\\end{equation}\\]Las probabilidades de los errores que se pueden cometer en la asignación corresponderían las áreas resaltadas en rojo (individuo asignado al grupo cuando realmente pertenece al grupo II) y en verde (individuo asignado al grupo II cuando realmente pertenece al grupo ), constituyendo la zona de error de clasificación.152Dos grupos y dos variables clasificadoras.Si, bajo los mismos supuestos, se dispone de dos variables clasificadoras, \\(X_1\\) y \\(X_2\\), se proyectan los elipsoides de ambos grupos sobre las dos variables y se obtiene la Fig. 21.2:\nFigura 21.2: LDA: dos grupos y dos variables clasificadoras\nSe obtienen, sobre cada variable, zonas de error de clasificación amplias (marcadas en amarillo) que conllevarán errores de clasificación grandes. Sin embargo, si se proyectan ambos elipsoides sobre un nuevo eje, obtenido como una combinación lineal de ambas variables clasificadoras (\\(w_1X_1+w_2X_2-D=0\\)), es posible reducir la zona de error de clasificación y, como consecuencia, la probabilidad de error de clasificación.El problema de la obtención de la combinación lineal que minimiza la probabilidad de error de clasificación fue resuelto por Fisher buscando la función discriminante que maximiza la separación entre ambos grupos, maximizando la distancia entre sus centroides y minimizando la variabilidad dentro de cada grupo. El procedimiento se detalla para el caso general de \\(p\\) variables.Dos grupos y p variables clasificadoras.El objetivo es encontrar una regla discriminante que permita separar ambos grupos. En otros términos, el objetivo es encontrar la función discriminante de Fisher, que se plantea como una combinación lineal de las \\(p\\) variables clasificadoras:\\[\\begin{equation}\nD=w_1X_1+w_2X_2+...+w_pX_p,\n\\end{equation}\\]que asigna al individuo j-ésimo una puntuación discriminante \\(D_j=w_1X_{1j}+w_2X_{2j}+...+w_pX_{pj}\\); expresando matricialmente estas puntuaciones en diferencias respecto las medias, se tiene que:\\[\\begin{equation}\n\\tag{21.3}\n\\begin{pmatrix} D_1 - \\bar{D} \\\\ D_2 - \\bar{D} \\\\ \\vdots \\\\ {D}_N - \\bar{D} \\end{pmatrix} = \\begin{pmatrix} X_{11} - \\bar{X}_1 & X_{21} - \\bar{X}_2 & \\cdots & X_{p1} - \\bar{X}_p \\\\ X_{12} - \\bar{X}_1 & X_{22} - \\bar{X}_2 & \\cdots & X_{p2} - \\bar{X}_p \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ X_{1N} - \\bar{X}_1 & X_{2N} - \\bar{X}_2 & \\cdots & X_{pN} - \\bar{X}_p \\\\ \\end{pmatrix} \\ \\begin{pmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_p \\\\ \\end{pmatrix}.\n\\end{equation}\\]donde \\(\\bar{D}= w_1\\bar{X}_1+ w_2\\bar{X}_2+...+w_p\\bar{X}_p\\), por ser \\(D\\) una combinación lineal de variables normales. En notación abreviada, la ecuación (21.3) se puede expresar como \\({\\bf{d}}^{*}={{\\bf{X}}^{*}}\\bf{w}\\).La suma de cuadrados de las desviaciones de la función discriminante respecto su media quedaría entonces como:\\[\\begin{equation}\n\\tag{21.4}\n{\\bf{d}}^{*\\prime} {\\bf{d}}^{*}= {\\bf{w}}^{\\prime} {\\bf{X}}^{*\\prime} \\bf{X}^{*}\\bf{w},\n\\end{equation}\\]donde \\({\\bf{X}}^{*\\prime} {\\bf{X}}^{*}\\) es la matriz simétrica de las desviaciones cuadráticas de las variables clasificadoras respecto sus medias (o matriz suma de cuadrados y productos cruzados, SCPC). Esta matriz se puede descomponer en la suma de dos matrices: la SCPC entregrupos o intergrupos, \\(\\bf{F}\\), y la SCPC residual o intragrupos, \\(\\bf{U}\\), por lo que la ecuación (21.4) se puede reexpresar como:\\[\\begin{equation}\n\\tag{21.5}\n{\\bf{d}}^{*\\prime} {\\bf{d}}^{*}={\\bf{w}}^{\\prime} \\bf{Fw}+ {\\bf{w}}^{\\prime} \\bf{Uw}.\n\\end{equation}\\]Fisher propuso determinar el vector de pesos, \\(\\bf{w}\\), buscando que se produzca la máxima discriminación entre los grupos, maximizando la variabilidad entre grupos respecto la variabilidad intragrupos, es decir:\\[\\begin{equation}\n\\tag{21.6}\n\\max \\frac{{\\bf{w}}^{\\prime} \\bf{Fw}} {{\\bf{w}}^{\\prime} \\bf{Uw}}.\n\\end{equation}\\]Como esta expresión es invariante frente cambios de escala, maximizar (21.6) es equivalente maximizar \\({\\bf{w}}^{\\prime} \\bf{Fw}\\) con la condición \\({\\bf{w}}^{\\prime} \\bf{Uw} =1\\) que, aplicando los multiplicadores de Lagrange, implica:\\[\\begin{equation}\n\\begin{array}{crl}\nL= {\\bf{w}}^{\\prime} {\\bf{Fw}} - \\lambda ({{\\bf{w}}^{\\prime} {\\bf{Uw}}}-{1}) \\Rightarrow \\frac{\\partial L}{\\partial {\\bf{w}}}=2{\\bf{Fw}}-2\\lambda\\bf{Uw}=\\bf{0} \\Rightarrow \\\\\n\\Rightarrow \\bf{Fw}=\\lambda \\bf{Uw} \\Rightarrow {\\bf{(U}}^{-1} \\bf{F)w}=\\lambda \\bf{w}. \\\\\n\\end{array}\n\\end{equation}\\]Así, el autovector asociado al mayor autovalor de la matriz \\(\\textbf{U}^{-1}\\textbf{F}\\) proporcionará los coeficientes de la función discriminante lineal de Fisher que mejor separa ambos grupos.El punto de corte (C) se obtiene evaluando la función discriminante en la media de cada grupo y calculando la media de las medias grupales ponderadas por el tamaño del grupo:\\[\\begin{equation}\n\\begin{array}{crl}\n\\bar{D}_I=w_1\\bar{X}_{1I}+w_2\\bar{X}_{2I}+...+w_p\\bar{X}_{pI} \\\\\n\\bar{D}_{II}=w_1\\bar{X}_{1II}+w_2\\bar{X}_{2II}+...+w_p\\bar{X}_{pII}, \\\\\n\\end{array}\n\\end{equation}\\]\\[\\begin{equation}\nC= \\frac{n_I \\bar{D}_I + n_{II} \\bar{D}_{II}}{N},\n\\end{equation}\\]y el criterio de asignación para el \\(j\\)-ésimo individuo, es:\\[\\begin{equation}\n\\text{si } \\ D_j<C \\\\text{ Grupo   y   si } D_j>C \\\\text{ Grupo II}.\n\\end{equation}\\]k grupos y p variables:En caso de existir más de dos grupos, la generalización del caso anterior es relativamente sencilla. Siguiendo la misma idea utilizada para dos grupos, se debe obtener un número de funciones discriminantes de Fisher suficiente para separar lo más posible los \\(k\\) grupos; este número es \\(T=\\min {(k-1,p)}\\).153Cada una de las \\(T\\) funciones discriminantes es una combinación lineal de las \\(p\\) variables clasificadoras:\\[\\begin{equation}\nD_t=w_{t1}X_1+w_{t2}X_2+...+w_{tp}X_p\\hspace{0,5cm} t=1,...T,\n\\end{equation}\\]donde se exige que el coeficiente de correlación lineal entre cada dos funciones discriminantes distintas sea nulo.La suma de cuadrados de las desviaciones de la matriz \\(\\textbf{D}\\) de funciones discriminantes respecto sus medias tendría una expresión equivalente la ecuación (21.4):\\[\\begin{equation}\n\\tag{21.7}\n\\bf{D}^{*\\prime} {\\bf{D}}^{*}= {\\bf{W}}^{\\prime} {\\bf{X}}^{* \\prime}{\\bf{XW}}.\n\\end{equation}\\]Para que las funciones discriminen lo máximo posible los \\(k\\) grupos, las combinaciones lineales han de maximizar la variabilidad entre los grupos respecto la variabilidad intragrupos, en un razonamiento análogo al expuesto en la ecuación (21.6):\\[\\begin{equation}\n\\tag{21.6}\n\\max \\frac{{\\bf{W}}^{\\prime} \\bf{FW}} {{\\bf{W}}^{\\prime} \\bf{UW}}.\n\\end{equation}\\]Al tratarse de una función homogénea, la maximización de (21.6) equivale maximizar \\({\\bf{W}}^{\\prime}\\bf{FW}\\) con la condición \\({{\\bf{W}}^{\\prime}\\bf{UW}}=1\\), que, aplicando los multiplicadores de Lagrange, implica:\\[\\begin{equation}\n\\begin{array}{crl}\nL= {\\bf{W}}^{\\prime} {\\bf{FW}} - \\lambda ({{\\bf{W}}^{\\prime} {\\bf{UW}}}-{1}) \\Rightarrow \\frac{\\partial L}{\\partial {\\bf{w}}}=2{\\bf{FW}}-2\\lambda\\bf{UW}=\\bf{0} \\Rightarrow \\\\\n\\Rightarrow \\bf{FW}=\\lambda {\\bf{UW}} \\Rightarrow {({\\bf{U}}^{-1} {\\bf{F}}}){W}=\\lambda \\bf{w}. \\\\\n\\end{array}\n\\end{equation}\\]Por tanto, el autovector asociado al mayor autovalor de la matriz \\(\\textbf{U}^{-1}\\textbf{F}\\) (generalmente simétrica) proporciona los coeficientes de la primera función discriminante lineal de Fisher, siendo el autovalor la proporción de la varianza total explicada por las \\(T\\) funciones discriminantes que recoge la primera función.Para obtener el resto de funciones discriminantes, basta con ir eligiendo los siguientes autovectores asociados los autovalores, ordenados decrecientemente. Como los autovectores son linealmente independientes, las funciones de discriminación están incorreladas.154De esta forma, la primera función discriminante, \\(D_1\\), es la que proporcione mayor discriminación entre los centroides de los grupos; \\(D_2\\), incorrelada con \\(D_1\\), es la que proporciona mayor discriminación, después de \\(D_1\\); y así sucesivamente: \\(D_t\\) es la que produce mayor discriminación entre los centroides de los grupos, después de las \\(t-1\\) anteriores, y está incorrelada con todas las anteriores.","code":""},{"path":"cap-discriminante.html","id":"discriminante-lineal-con-r-la-función-lda","chapter":"Capítulo 21 Análisis discriminante","heading":"21.2.1 Discriminante lineal con R: la función lda()","text":"continuación se ejemplifica la aplicación de un discriminante lineal con R. Para ello, se utilizará y cargará la base de datos iris, que consta de 150 observaciones y 5 variables, 4 numéricas, que serán las clasificadoras, y una categórica, sobre la que se realiza el análisis, con tres categorías: setosa, versicolor y virginica.Se clasificarán las flores iris, identificadas con la variable Species (especies de iris), utilizando como variables clasificadoras: Sepal.Length (longitud del sépalo), Sepal.Width (anchura del sépalo), Petal.Lenght (longitud del pétalo) y Petal.Width (anchura del pétalo).Para evaluar la capacidad predictiva del análisis discriminante, se divide el conjunto de datos en dos subconjuntos: el de entrenamiento o estimación (con el 80% de ellos) y el de test (con el 20% restante).155Las distribuciones univariadas deben ser normales; si fuera así, se podrían transformar utilizando las transformaciones log y root (distribuciones exponenciales) y Box-Cox (distribuciones sesgadas), como se muestra en la Sec. ??. Igualmente, es conveniente estandarizar las variables para evitar que la diferencia de escalas influya en la importancia relativa de cada variable clasificadora en las funciones discriminantes.Una inspección previa de los datos puede ayudar detectar si las variables clasificadoras pueden contribuir la discriminación entre los grupos. En este ejemplo, la Fig. 21.3 muestra la función de densidad de cada variable sobre cada grupo con los datos del subconjunto de entrenamiento:\nFigura 21.3: Función de densidad de cada variable clasificadora sobre los grupos\nIgualmente, los gráficos bivariantes pueden ayudar ver si hay “distancias” entre los centroides de los grupos para las variables clasificadoras, como muestra la Fig. 21.4:\nFigura 21.4: Diagramas bivariantes de dispersión de las variables clasificadoras.\nComo se observa en dichos gráficos, las variables clasificadoras pueden contribuir la discriminación entre las tres especies de flores iris.Para aplicar la función lda() se debe especificar la variable de clasificación (Species) y el conjunto de datos (entrenamiento_t); de forma opcional, se pueden especificar las probabilidades priori (prior, por defecto se usa proportions), el método de estimación de las medias y varianzas (method, por defecto moment) o el argumento CV para obtener los grupos pronosticados y las probabilidades posteriori (por defecto, CV=FALSE)La salida muestra las probabilidades previas (Prior probabilities groups) y los centroides de cada grupo (Group means). continuación muestra las funciones discriminantes de Fisher mediante los respectivos coeficientes \\(w_{}\\). En este caso, las dos funciones discriminantes son:\\(D_1=0.6795*SL+0.6565*SW-3,8365*PL-2,2722*PW\\)\\(D_2=0.0446*SL-1.0033*SW+1.4418*PL-1.9651*PW\\)con una proporción de discriminación de 0.9902 y 0.0098, respectivamente.La proyección de los individuos (en este caso flores) en el plano formado por las dos funciones discriminantes se recoge en la Fig. 21.5:\nFigura 21.5: Proyección de los individuos (flores) sobre las dos funciones discriminantes\nComo se aprecia, la primera función discriminante es la que mayor contribución tiene la separación entre los grupos, separando muy claramente la especie setosa y, en menor medida, las especies virginica y versicolor, grupos entre los que hay un pequeño grado de solapamiento. Por otro lado, la segunda función discriminante, con una proporción de discriminación de 0.0098, apenas contribuye la separación entre grupos.Por último, mediante la función partimat() del paquete klaR, se puede visualizar cómo quedan las regiones bivariantes que clasifican los individuos en cada clase (Fig. 21.6):\nFigura 21.6: Regiones bivariantes de clasificación en cada grupo (Centroides en rojo): setosa (celeste), versicolor (gris) y virginica (amarillo)\nPor último, aplicando las funciones discriminantes los datos reservados para estudiar la capacidad predictiva del modelo, se obtiene la tabla conocida como matriz de confusión, donde se compara el grupo real con el pronosticado por el modelo:En este caso, se clasifican correctamente 29 de las 30 “nuevas” flores, indicando un grado de ajuste del 96.9667%.","code":"\nlibrary(\"caret\")\nlibrary(\"MASS\")\nlibrary(\"klaR\")\ndata(\"iris\")\n# División de los datos: 80% para entrenamiento y 20% para test\nset.seed(123)\nmuestra <- iris$Species |>\n  createDataPartition(p = 0.8, list = FALSE)\nentrenamiento_d <- iris[muestra, ]\ntest_d <- iris[-muestra, ]\n# Estimación de los parámetros de preprocesamiento (estandarización)\npreproc_param <- entrenamiento_d |>\n  preProcess(method = c(\"center\", \"scale\"))\n# Transformación de los datos usando los parámetros estimados\nentrenamiento_t <- preproc_param |> predict(entrenamiento_d)\ntest_t <- preproc_param |> predict(test_d)\nlibrary(\"ggplot2\")\nlibrary(\"ggpubr\")\n\np1 <- ggplot(data = entrenamiento_t, aes(x = Sepal.Length, fill = Species, colour = Species)) +\n  geom_density(alpha = 0.3) +\n  theme_bw()\np2 <- ggplot(data = entrenamiento_t, aes(x = Sepal.Width, fill = Species, colour = Species)) +\n  geom_density(alpha = 0.3) +\n  theme_bw()\np3 <- ggplot(data = entrenamiento_t, aes(x = Petal.Length, fill = Species, colour = Species)) +\n  geom_density(alpha = 0.3) +\n  theme_bw()\np4 <- ggplot(data = entrenamiento_t, aes(x = Petal.Width, fill = Species, colour = Species)) +\n  geom_density(alpha = 0.3) +\n  theme_bw()\nggarrange(p1, p2, p3, p4, ncol = 2, nrow = 2, common.legend = TRUE, legend = \"bottom\")\npairs(x = entrenamiento_t[, -5], col = c(\"firebrick\", \"green3\", \"darkblue\")[entrenamiento_t$Species], pch = 20)\noptions(digits = 4)\nmodelo_lda <- lda(Species ~ ., data = entrenamiento_t)\nmodelo_lda\n#> Call:\n#> lda(Species ~ ., data = entrenamiento_t)\n#> \n#> Prior probabilities of groups:\n#>     setosa versicolor  virginica \n#>     0.3333     0.3333     0.3333 \n#> \n#> Group means:\n#>            Sepal.Length Sepal.Width Petal.Length Petal.Width\n#> setosa          -1.0113     0.78049      -1.2900     -1.2453\n#> versicolor       0.1014    -0.68675       0.2566      0.1473\n#> virginica        0.9099    -0.09374       1.0334      1.0981\n#> \n#> Coefficients of linear discriminants:\n#>                  LD1      LD2\n#> Sepal.Length  0.6795  0.04464\n#> Sepal.Width   0.6565 -1.00330\n#> Petal.Length -3.8365  1.44176\n#> Petal.Width  -2.2722 -1.96516\n#> \n#> Proportion of trace:\n#>    LD1    LD2 \n#> 0.9902 0.0098\ndatos_lda <- cbind(entrenamiento_t, predict(modelo_lda)$x)\nggplot(datos_lda, aes(LD1, LD2)) +\n  geom_point(aes(color = Species)) +\n  ggtitle(\"Gráfico LDA\")\npartimat(Species ~ ., data = entrenamiento_t, method = \"lda\", image.colors = c(\"skyblue\", \"lightgrey\", \"yellow\"), col.mean = \"red\")\npredicciones_lda <- modelo_lda |> predict(test_t)\ntable(test_t$Species, predicciones_lda$class, dnn = c(\"Grupo real\", \"Grupo pronosticado\"))\n#>             Grupo pronosticado\n#> Grupo real   setosa versicolor virginica\n#>   setosa         10          0         0\n#>   versicolor      0         10         0\n#>   virginica       0          1         9\nmean(predicciones_lda$class == test_t$Species)\n#> [1] 0.9667"},{"path":"cap-discriminante.html","id":"id_150025qda","chapter":"Capítulo 21 Análisis discriminante","heading":"21.3 Análisis discriminante cuadrático","text":"En el discriminante lineal visto anteriormente, se asume que las variables clasificadoras tienen idénticas matrices de varianzas-covarianzas en los distintos grupos, supuesto que garantiza que las funciones discriminantes son combinaciones lineales de las variables clasificadoras.Es posible eliminar esta restricción, permitiendo que las matrices de varianzas-covarianzas sean diferentes en los grupos, lo que introduce términos cuadráticos en las funciones discriminantes que conducen límites de decisión curvilíneos, por lo que el análisis discriminante cuadrático (QDA) puede aplicarse situaciones en las que la separación entre grupos es lineal.Denominando \\(\\pi_i\\) la probabilidad priori de pertenecer al grupo \\(G_i\\) y \\({\\bf{\\mu}}_i\\) y \\({\\bf{\\Sigma}}_t\\) al vector de medias y matriz de varianzas-covarianzas, respectivamente, en dicho grupo, partir del vector de observaciones \\(\\bf x\\), se puede obtener el discriminante cuadrático como:156\\[\\begin{equation}\n\\tag{21.8}\n\\begin{array}{crl}\nQ_{ij}\\left({\\bf{x}}\\right)=\\frac{1}{2}{\\bf{x}}^{\\prime}\\left({\\bf{\\Sigma}}_i^{-1}-{\\bf{\\Sigma}}_j^{-1}\\right){\\bf{x}}+{\\bf{x}}^{\\prime} \\left({\\bf{\\Sigma}}_i^{-1}{\\bf{\\mu}}_i-{\\bf{\\Sigma}}_j^{-1}{\\bf{\\mu}}_j\\right){\\bf{x}}+ \\\\\n+ \\frac{1}{2}{\\bf{\\mu}}_j^{\\prime}{\\bf{\\Sigma}}_j^{-1}{\\bf{\\mu}}_j-\\frac{1}{2}{\\bf{\\mu}}_i^{\\prime}{\\bf{\\Sigma}}_i^{-1}{\\bf{\\mu}}_i+\\frac{1}{2}\\log \\left(\\left|{\\bf{\\Sigma}}_j \\right|\\right)-\\frac{1}{2}\\log \\left(\\left|{\\bf{\\Sigma}}_i\\right|\\right) \\\\\n\\ \\forall \\neq j \\, \\ ,j=1,2,...,k. \\\\\n\\end{array}\n\\end{equation}\\]partir de aquí, la regla de clasificación para un individuo consiste en evaluar el discriminante cuadrático (21.8) para los diferentes grupos y, tras simplificaciones algebráicas, asignarlo al grupo \\(G_h\\) que verifique:157\\[\\begin{equation}\n\\tag{21.9}\nG_h=\\underset{}{\\operatorname{argmax}} \\log\\pi_i+\\frac{1}{2} \\log \\left |{\\bf{\\Sigma}}_i\\right|-\\frac{1}{2}\\left({\\bf{x}}-{\\bf{\\mu}}_{}\\right)^{\\prime} {\\bf{\\Sigma}}_i^{-1}\\left({\\bf{x}}-{\\bf{\\mu}}_i\\right)\n\\end{equation}\\]En este caso, los límites de la región de clasificación son ecuaciones cuadráticas del vector \\(\\bf{x}\\).Finalmente, señalar que el LDA es mucho más flexible que el QDA, y que tiene una varianza mucho menor, lo cual puede dar lugar mejores clasificaciones que con QDA. Sin embargo, si el supuesto de igualdad de matrices de varianzas-covarianzas en cada grupo dista mucho de cumplirse, entonces el LDA puede tener un sesgo importante. El LDA suele ser mejor opción que el QDA si el subconjunto de entrenamiento es pequeño y la reducción de la varianza se convierte en un objetivo importante. Si el conjunto de entrenamiento tiene un tamaño grande, la varianza del clasificador es un problema y el QDA sería la mejor opción; también lo es en caso de un incumplimiento significativo del supuesto de de igualdad de varianzas covarianzas del LDA.","code":""},{"path":"cap-discriminante.html","id":"discriminante-cuadrático-con-r-la-función-qda","chapter":"Capítulo 21 Análisis discriminante","heading":"21.3.1 Discriminante cuadrático con R: la función qda()","text":"Para ilustrar la realización de un análisis discriminante cuadrático en R, se aplica la función qda() los datos ìris utilizados en el caso lineal. La elección de la misma base de datos responde un planteamiento didáctico, para poder comparar los resultados de ambos métodos y las diferencias que produce asumir la igualdad de matrices de varianzas-covarianzas (método lineal) o asumirlas (método cuadrático).158La representación gráfica de las áreas por las que se clasifican los individuos se muestra en la Fig. 21.7.\nFigura 21.7: Regiones bivariantes de clasificación en cada grupo (centroides en rojo): setosa (celeste), versicolor (gris) y virginica (amarillo)\nComo se aprecia, ahora los contornos de las áreas son siempre lineales, sino que incluyen fronteras cuadráticas. Por último, aplicando el discriminante cuadrático los datos reservados para estudiar la capacidad predictiva del modelo, se obtiene la\nmatriz de confusión, donde se observa que se mejoran los resultados respecto al discriminante lineal.","code":"\noptions(digits = 4)\nmodelo_qda <- qda(Species ~ ., data = entrenamiento_t)\nmodelo_qda\n#> Call:\n#> qda(Species ~ ., data = entrenamiento_t)\n#> \n#> Prior probabilities of groups:\n#>     setosa versicolor  virginica \n#>     0.3333     0.3333     0.3333 \n#> \n#> Group means:\n#>            Sepal.Length Sepal.Width Petal.Length Petal.Width\n#> setosa          -1.0113     0.78049      -1.2900     -1.2453\n#> versicolor       0.1014    -0.68675       0.2566      0.1473\n#> virginica        0.9099    -0.09374       1.0334      1.0981\npartimat(Species ~ ., data = entrenamiento_t, method = \"qda\", image.colors = c(\"skyblue\", \"lightgrey\", \"yellow\"), col.mean = \"red\")\npredicciones_qda <- modelo_qda |> predict(test_t)\ntable(test_t$Species, predicciones_qda$class, dnn = c(\"Grupo real\", \"Grupo pronosticado\"))\n#>             Grupo pronosticado\n#> Grupo real   setosa versicolor virginica\n#>   setosa         10          0         0\n#>   versicolor      0         10         0\n#>   virginica       0          1         9\nmean(predicciones_qda$class == test_t$Species)\n#> [1] 0.9667"},{"path":"cap-discriminante.html","id":"resumen-19","chapter":"Capítulo 21 Análisis discriminante","heading":"Resumen","text":"El análisis discriminante permite clasificar individuos en distintos grupos preexistentes en relación una variable cualitativa, partir de las variables clasificadoras.La información se sintetiza en las funciones discriminantes. Su uso puede tener una finalidad descriptiva: identificar la separación entre grupos y la contribución de cada variable clasificadora; y una finalidad predictiva: clasificar un individuo nuevo.Los principales tipos son el lineal y el cuadrático, que se desarrollan en R con las funciones lda() y qda(), respectivamente.","code":""},{"path":"cap-conjunto.html","id":"cap-conjunto","chapter":"Capítulo 22 Análisis conjunto","heading":"Capítulo 22 Análisis conjunto","text":"Mª Leticia Meseguer SantamaríaUniversidad de Castilla-La Mancha","code":""},{"path":"cap-conjunto.html","id":"introducción-y-conceptos-clave","chapter":"Capítulo 22 Análisis conjunto","heading":"22.1 Introducción y conceptos clave","text":"El análisis conjunto (o conjoint analysis) estudia situaciones de elección múltiple. Funciona dividiendo un producto o servicio en sus componentes (atributos y niveles) y analizando las utilidades parciales de cada uno; después se realizan diferentes combinaciones de éstos para identificar las preferencias del consumidor. Permite conocer las preferencias del público ante el lanzamiento de nuevos productos o servicios, para adaptarlos ellas y maximizar el éxito. Evalúa la sensibilidad al precio u otras características del producto y predice su comportamiento en el mercado. Mediante este análisis se puede establecer qué atributo y qué categoría (nivel) son los más valorados y cuantificarlos de forma relativa.Los principales elementos de un análisis conjunto son:Atributos: características de un producto o servicio sobre las que se basará la elección.Niveles: valores que puede tomar cada atributo. El número de niveles tiene por qué ser igual en todos los atributos, pero es conveniente que sea similar para facilitar la elección del entrevistado.Diseño experimental: proceso estadístico por el que se confeccionan las opciones de las preguntas de la encuesta, y que realiza el investigador.Utilidades parciales: valoración numérica que representa el grado de preferencia por cada nivel de atributo. Se hace en referencia las otras opciones. También se conocen como “partworth utilities”, “niveles de preferencia”, “niveles relativos de preferencias” o “valoraciones relativas”.Importancia: valores numéricos que indican la preferencia por cada atributo.Perfil: combinación concreta de niveles de los atributos de un producto o servicio. También se denomina alternativa.Escenarios: conjunto de perfiles entre los que el entrevistado tiene que señalar sus preferencias.Pregunta: conjunto de opciones, normalmente entre 8 y 12.Probabilidad de elección: probabilidad de tomar una decisión determinada considerando las utilidades parciales.Modelo de comportamiento: modelo de decisión que se infiere partir de las probabilidades de elección, después del análisis de las preferencias (utilidades parciales) de los entrevistados.Análisis: estimación de las utilidades parciales.Valoración: impacto del nuevo producto o servicio, basado en el modelo de comportamiento que se haya establecido.Ejemplo: elección de gimnasio. Sea una muesta de gimnasios en la que se identifican 4 atributos (características) con distintos niveles. Se realiza una encuesta sobre qué gimnasio se prefiere. Los participantes eligen una opción entre las distintas combinaciones ofrecidas (preguntas). Mediante el análisis de los resultados de la encuesta se extrae el peso de cada atributo y nivel en las respuestas (utilidades parciales), que describen las preferencias medias de los encuestados; así mismo, se identifican los atributos y niveles más valorados y su importancia relativa.Por ejemplo, los resultados pueden indicar que el horario más demandado es el de 09:00 23:00 durante 7 días la semana.\nFigura 22.1: Elección de gimnasio. Atributos: horario, clases programadas, limpieza y precio\n","code":""},{"path":"cap-conjunto.html","id":"tipos-de-análisis-conjunto","chapter":"Capítulo 22 Análisis conjunto","heading":"22.2 Tipos de análisis conjunto","text":"En la literatura científica sobre el análisis conjunto se diferencian tres formas de abordar la investigación:El análisis tradicional de perfil completo (full profile)Este análisis, también llamado conjoint value analysis (CVA), muestra al entrevistado una selección de productos resultantes de la combinación de determinados niveles de una serie de atributos (es decir, una serie de perfiles) y se le pide que los valore según su preferencia en una escala numérica (utilidad). Las preferencias globales de cada individuo se descomponen en utilidades independientes y compatibles para cada atributo y nivel mediante métodos basados en regresión lineal múltiple (véase Cap. 15), que proporcionan las utilidades para los distintos niveles, partworth utilities o, simplemente, partworths.En el diseño original se detallan todos los perfiles hipotéticos (perfiles completos). Sin embargo, su número suele ser alto y se hace uso de técnicas de investigación que indiquen los más significativos. Está limitado un análisis con muy pocos atributos y niveles.El análisis conjunto adaptativo o adaptative conjoint analysis (ACA)La recolección de datos se hace en dos fases: en la primera, el encuestado señala la importancia que le da cada atributo; y en la segunda, asigna utilidades un número limitado de perfiles. Es decir, se le conduce través de una investigación sistemática por diferentes secciones, en las que se le presentan uno o pocos atributos, y que se va adaptando sus respuestas. Se obtienen las valoraciones de los niveles de interés (utilidades parciales).\nEstá limitado por su complejidad y por el uso de software especializado, aunque permite un gran número de atributos y niveles.El análisis conjunto basado en elecciones o choice-based conjoint (CBC)Pretende un mayor realismo, mostrando cada entrevistado un grupo de productos distintos y se le pide que elija cuál de ellos prefiere. Además, contempla la posibilidad de elegir ninguna alternativa. Las utilidades de los atributos-niveles se calculan mediante un modelo de regresión lineal conocido como modelo de elección discreta. Sus principales inconvenientes son: \\(()\\) tiene una mayor complejidad en su diseño y análisis, \\((ii)\\) requiere muestras muy grandes para tener validez estadística, y \\((iii)\\) el número máximo de atributos que admite es 10. Su mayor ventaja es que presenta un mejor sistema de elección que las alternativas anteriores.","code":""},{"path":"cap-conjunto.html","id":"etapas-de-la-realización-del-análisis-conjunto","chapter":"Capítulo 22 Análisis conjunto","heading":"22.3 Etapas de la realización del análisis conjunto","text":"Para la aplicación correcta de un análisis conjunto es conveniente seguir una serie de etapas que faciliten las elecciones metodológicas que han de hacerse, así como la interpretación de los resultados.Planteamiento del problema. Se analiza la oportunidad de aplicar esta técnica dados el objetivo y los datos del proyecto. Para ello, se debe especificar tanto el producto o servicio como los atributos que se quieren estudiar.Elección de la metodología conjoint aplicar. Ésta dependerá, básicamente, de las características y cantidad de atributos estudiados; también se debe considerar la forma en que se valorarán por parte de los individuos. modo de guía:Se opta por CVA cuando se analizan hasta 6 atributos, de productos o servicios habituales, de forma que el encuestado pueda hacer una elección rápida, sin demasiada reflexión.Se opta por CVA cuando se analizan hasta 6 atributos, de productos o servicios habituales, de forma que el encuestado pueda hacer una elección rápida, sin demasiada reflexión.Se elige ACA cuando se analizan más de 10 atributos, procediendo de manera que la elección sea rápida pero reflexiva, ya que se trata de un proceso adaptativo cuyo resultado final depende, en buena medida, de la idoneidad de las primeras valoraciones.Se elige ACA cuando se analizan más de 10 atributos, procediendo de manera que la elección sea rápida pero reflexiva, ya que se trata de un proceso adaptativo cuyo resultado final depende, en buena medida, de la idoneidad de las primeras valoraciones.Se usar CBC si se analizan entre 6 y 10 atributos y con elecciones reflexivas sobre productos, puesto que el encuestado asigna directamente utilidades, sino que se limita elegir una (o ninguna) de las opciones presentadas.Se usar CBC si se analizan entre 6 y 10 atributos y con elecciones reflexivas sobre productos, puesto que el encuestado asigna directamente utilidades, sino que se limita elegir una (o ninguna) de las opciones presentadas.La selección de elementos. Se escogen sólo los atributos que condicionan la elección, es decir, los que expliquen las preferencias de los individuos y permitan diferenciar bien entre los productos o servicios; deben estar lo más cercanos posible la independencia entre ellos. Para cada atributo se eligen sus niveles; deben ser mutuamente excluyentes y cubrir todo el rango de posibilidades. Por último, y dependiendo de la metodología utilizada, se determinan los perfiles y escenarios.Creación de estímulos. Se utilizarán diseños factoriales fraccionados ortogonales, que reducen el número de perfiles que se le muestran al entrevistado. Como el número de posibles perfiles es la multiplicación del número de niveles de todos los atributos, puede ser imposible en la práctica que el individuo indique sus preferencias entre todos ellos. Por ello, se seleccionan sólo algunos de los perfiles, que sean representativos del resto, es decir, que en los perfiles incluidos en los estímulos aparezca cada nivel de cada factor combinado con el resto de niveles de forma lo más proporcional posible.159 La selección se efectúa mediante diseño factorial de experimentos, fraccionado (que jerarquiza los efectos, permitiendo la reducción de perfiles) y ortogonal (equilibrado en los niveles).Forma de presentación. Dependerá de la metodología elegida. La Fig. 22.2 muestra algunos ejemplos.\nFigura 22.2: Formas de presentación\nCVA: matriz de comparaciones o trade-”, en la que el entrevistado valora la combinación de atributos y niveles (sólo es válida para dos atributos). Perfiles completos para ordenar, donde se elaboran perfiles de cada producto o servicio utilizando sólo un nivel de cada atributo y el encuestado los valora (ordena) según sus preferencias. Perfil determinado para valorar, combinación que el encuestado valora según sus preferencias.CVA: matriz de comparaciones o trade-”, en la que el entrevistado valora la combinación de atributos y niveles (sólo es válida para dos atributos). Perfiles completos para ordenar, donde se elaboran perfiles de cada producto o servicio utilizando sólo un nivel de cada atributo y el encuestado los valora (ordena) según sus preferencias. Perfil determinado para valorar, combinación que el encuestado valora según sus preferencias.ACA: comparaciones pareadas, en las que se comparan dos perfiles incompletos.ACA: comparaciones pareadas, en las que se comparan dos perfiles incompletos.CBC: elección de un perfil, en el que los encuestados señalan el perfil preferido entre el subconjunto que se les muestra, sin valorarlos ni ordenarlos.CBC: elección de un perfil, en el que los encuestados señalan el perfil preferido entre el subconjunto que se les muestra, sin valorarlos ni ordenarlos.Trabajo de campo y tratamiento de los datosCVA: la recogida de datos es en papel u ordenador, y el análisis en ordenador con software especializado.CVA: la recogida de datos es en papel u ordenador, y el análisis en ordenador con software especializado.ACA y CBC: la recogida y el análisis son con ordenador.ACA y CBC: la recogida y el análisis son con ordenador.Estimación de las utilidadesCon CVA se utiliza, por lo general, un modelo regresión por mínimos cuadrados ordinarios (OLS), mediante el cual se determinan las utilidades parciales o parthworths, que indican las preferencias del encuestado mediante un modelo aditivo lineal en relación los niveles de los atributos considerados como referencia.Sea la variable \\(X_{jk}^\\), que indica si el nivel \\(k\\) del j-ésimo atributo está o en el -ésimo perfil. Dicha variable toma sólo los valores \\(X_{jk}^=1\\) (si está) y \\(X_{jk}^=0\\) si lo está. Sea \\(Y^{}\\) la preferencia que tiene un individuo sobre el -ésimo perfil. Entonces la función de utilidad estimar es:\\[\\begin{equation}\n\\tag{22.1}\n  Y^{}=\\beta_0 + \\sum_{j,k} U_{jk} X_{jk}^{} + \\varepsilon^{},\n\\end{equation}\\]donde los coeficientes \\(U_{jk}\\) son las utilidades parciales, o partial partworths, que indican la utilidad que el individuo asigna cada nivel de cada atributo, y \\(\\varepsilon^{}\\) denota el término de error.En el modelo (22.1) habrá multicolinealidad160, ya que todos los atributos deben estar presentes en todos los perfiles, es decir \\(\\sum_k X_{jk}^{}=1\\). Para evitar las consecuencias indeseadas de la multicolinealidad en la estimación de las utilidades parciales, se elimina uno de los niveles de cada factor (sin pérdida de generalidad, el último, \\(K\\)), estimándose por OLS la función de utilidad (que ahora se denomina “restringida”):\\[\\begin{equation}\n\\tag{22.2}\n  Y^{}=\\delta + \\sum_{j,k, k \\neq K} \\gamma_{jk} X_{jk}^{} + \\varepsilon^{},\n\\end{equation}\\]donde \\(\\delta = \\beta_0 + \\sum_{j} U_{jK}\\) y \\(\\gamma_{jk}=U_{jk}-U_{jK}\\). partir de la estimación de (22.2), se pueden calcular los valores de la función de utilidad original en (22.1).161Si hay \\(Z\\) individuos, cada uno de ellos, \\(z\\), dará una valoración diferente los perfiles \\(\\), es decir los valores de \\(Y_{z}^{}\\) serán diferentes, produciendo una estimación diferente de la función de utilidad (22.1) y, por tanto, diferentes utilidades parciales \\(\\widehat{U}_{jk,z}\\). Para obtener las utilidades para el conjunto de individuos, se procede al cálculo de sus valores medios:\\[\\begin{equation}\n\\tag{22.3}\n  \\widehat{U}_{jk} = \\frac{\\sum_{z} \\widehat{U}_{jk,z}}{Z}\n\\end{equation}\\]Se obtienen, así, las utilidades de cada nivel \\(k\\) de cada atributo \\(j\\), reflejando la importancia que conceden los individuos cada uno de esos niveles.Para determinar la importancia de cada atributo \\(j\\) se utiliza la diferencia entre la utilidad más alta y más baja de sus niveles, es decir:\\[\\begin{equation}\n\\tag{22.4}\n  Imp_{j}=\\max_{k} \\lbrace {U_{jk}} \\rbrace - \\min_{k} \\lbrace {U_{jk}} \\rbrace\n\\end{equation}\\]En términos relativos, la importancia de cada atributo \\(j\\) respecto al conjunto de atributos, se puede expresar como:\\[\\begin{equation}\n\\tag{22.5}\n  ImpRel_{j}=\\frac{Imp_{j}}{\\sum_{t=1}^{J} Imp_{t}}.\n\\end{equation}\\]Por último, una vez estimadas las utilidades, la utilidad total de un perfil \\(\\) es la suma de las utilidades de los niveles de cada uno de los atributos que lo definen más la constante de regresión:162\\[\\begin{equation}\n\\tag{22.6}\n  \\widehat{U}_i=\\beta_0 + \\sum_{j=1}^J\\widehat{U}_{jk}\\hspace{0,2cm} \\text{para los niveles presentes en el perfil},\n\\end{equation}\\]donde: \\(\\widehat{U}_i\\) es la utilidad total del perfil \\(\\); \\(\\widehat{U}_{jk}\\) es la utilidad parcial asociada con el nivel \\(k\\) del atributo \\(j\\); y \\(\\beta_0\\) es la constante de regresión.En los casos de las metodologías ACA y CBC, la estimación de las utilidades parciales es más compleja, debiendo recurrir modelos de regresión lineal. Sin embargo, su interpretación es la misma que en el caso de la metodología CVA.Interpretación de resultados. Algunos de los resultados del análisis conjunto que frecuentemente se analizan son:La importancia de los atributos y niveles. La importancias relativa de cada atributo (\\(ImpRel_{j}\\)) refleja la opinión de los individuos sobre la relevancia (en porcentaje) de dichos atributos la hora de evaluar los perfiles. Igualmente, para cada atributo \\(j\\), las utilidades de sus niveles \\(U_{jk}\\) muestran la importancia relativa asignada dichos niveles en la valoración de cada atributo.La importancia de los atributos y niveles. La importancias relativa de cada atributo (\\(ImpRel_{j}\\)) refleja la opinión de los individuos sobre la relevancia (en porcentaje) de dichos atributos la hora de evaluar los perfiles. Igualmente, para cada atributo \\(j\\), las utilidades de sus niveles \\(U_{jk}\\) muestran la importancia relativa asignada dichos niveles en la valoración de cada atributo.Las utilidades totales de cada perfil, o la comparación relativa entre un conjunto determinado de perfiles, permiten detectar cuáles son los preferidos por los individuos. Esta información puede orientar en la determinación de la configuración final del producto o servicio analizado.Las utilidades totales de cada perfil, o la comparación relativa entre un conjunto determinado de perfiles, permiten detectar cuáles son los preferidos por los individuos. Esta información puede orientar en la determinación de la configuración final del producto o servicio analizado.La influencia de un determinado atributo sobre la elección de los individuos. Así, es posible estimar el cambios en la utilidad que se produciría por la modificación de sus niveles (cambio generalmente conocido como elasticidad); por ejemplo, si un atributo fuese el precio, se podría dar una aproximación la variación de utilidad producida por un incremento del precio.La influencia de un determinado atributo sobre la elección de los individuos. Así, es posible estimar el cambios en la utilidad que se produciría por la modificación de sus niveles (cambio generalmente conocido como elasticidad); por ejemplo, si un atributo fuese el precio, se podría dar una aproximación la variación de utilidad producida por un incremento del precio.Al disponer de las utilidades parciales de cada individuo, es posible estudiar si éstas son homogéneas para toda la muestra o si, por el contrario, existen grupos de individuos con utilidades parecidas entre ellos y diferenciadas del resto. Este planteamiento podría ayudar segmentar el mercado y ofrecer productos o servicios diferenciados cada nicho de mercado.Al disponer de las utilidades parciales de cada individuo, es posible estudiar si éstas son homogéneas para toda la muestra o si, por el contrario, existen grupos de individuos con utilidades parecidas entre ellos y diferenciadas del resto. Este planteamiento podría ayudar segmentar el mercado y ofrecer productos o servicios diferenciados cada nicho de mercado.","code":""},{"path":"cap-conjunto.html","id":"procedimiento-con-r-la-función-conjoint","chapter":"Capítulo 22 Análisis conjunto","heading":"22.4 Procedimiento con R: la función Conjoint()","text":"Para ejemplificar la puesta en práctica del análisis conjunto, con el método de perfil completo (CVA), se utiliza el paquete conjoint y su base de datos tea, compuesta por 5 listados: niveles (tlevn), perfiles (tprof), preferencias (tpref), preferencias en forma matricial (tprefm) y la simulación de perfiles (tsimp).El listado tprof recoge los 13 perfiles que se presentan los individuos, cada uno de ellos con una combinación de niveles de cada factor. El hecho de utilizar un diseño factorial fraccionario ortogonal ha permitido encontrar una muestra de perfiles representativa del total (habría 3x3x3x2=54 perfiles distintos) donde cada nivel de cada atributo está representado, aproximadamente, de forma proporcional.La matriz tprefm contiene las preferencias que cada uno de los 100 individuos asigna cada uno de los perfiles \\(\\) que se le presentan (estas preferencias se recogen ordenadas en el vector tpref). Por último, tsimp muestra cuatro simulaciones de perfiles distintos los mostrados los individuos.En este ejemplo, se desea conocer el por qué unos tés son preferidos otros. Para ello, en el contexto de un análisis tradicional con perfiles incompletos, se seleccionan 4 atributos, con sus correspondiente niveles (tlevn):Precio, distinguiendo entre bajo, medio y alto.Precio, distinguiendo entre bajo, medio y alto.Variedad, distinguiendo entre negro, verde y rojo.Variedad, distinguiendo entre negro, verde y rojo.Forma de presentación, distinguiendo entre bolsita, granulado y hojas.Forma de presentación, distinguiendo entre bolsita, granulado y hojas.Aroma, distinguiendo entre aromático y aromático.Aroma, distinguiendo entre aromático y aromático.De las 54 posibles combinaciones se seleccionan 13 perfiles, valorados de 0 10 según las preferencias de cada uno de los 100 encuestados.Para realizar el análisis conjunto propuesto, primeramente se carga tanto el paquete como los datos:La función Conjoint() devolverá las utilidades de los niveles, el vector de porcentajes de la importancia de los atributos y las gráficas correspondientes.163En la Tabla 22.1 se exponen otras funciones disponibles para obtener algunos resultados concretos:Tabla 22.1:  Funciones en R para cálculos sobre análisis conjuntoAsí, es posible obtener sólo la información relativa las utilidades, mediante la función caUtilities(), o sólo la correspondiente la importancia de los atributos, usando la función caImportance():Interpretación del resultado:La primera parte de la salida corresponde los coeficientes del modelo de regresión (22.2), donde se incluye el último nivel de cada atributo (para evitar la multicolinealidad); se señalan con asteriscos los que resultan estadísticamente significativos. También se indica el grado de ajuste del modelo. continuación, están las utilidades parciales para todos los niveles de todos los atributos:La primera parte de la salida corresponde los coeficientes del modelo de regresión (22.2), donde se incluye el último nivel de cada atributo (para evitar la multicolinealidad); se señalan con asteriscos los que resultan estadísticamente significativos. También se indica el grado de ajuste del modelo. continuación, están las utilidades parciales para todos los niveles de todos los atributos:El primer valor (3.5534) corresponde al término independiente del modelo, por lo que está asociado ningún atributo.El primer valor (3.5534) corresponde al término independiente del modelo, por lo que está asociado ningún atributo.Para el atributo precio, es el nivel bajo (low) el que recibe mayor preferencia (0.2402), siendo el nivel medio (medium) el menos preferido (-0.1431), algo por debajo de la preferencia del nivel alto (high), -0.0971.Para el atributo precio, es el nivel bajo (low) el que recibe mayor preferencia (0.2402), siendo el nivel medio (medium) el menos preferido (-0.1431), algo por debajo de la preferencia del nivel alto (high), -0.0971.Para el atributo variedad, el té negro (black) es el nivel con mayor utilidad (0.6149), seguido del té verde (0.0349), quedando el nivel de té rojo con una preferencia mucho menor (-0.6498).Para el atributo variedad, el té negro (black) es el nivel con mayor utilidad (0.6149), seguido del té verde (0.0349), quedando el nivel de té rojo con una preferencia mucho menor (-0.6498).En el caso del atributo presentación, en hojas (leafy) es el nivel más preferido (0.7529), seguido de la modalidad en bolsitas (bags), con una utilidad de 0.1369; el nivel granulado presenta la preferencia más baja (-0.8898).En el caso del atributo presentación, en hojas (leafy) es el nivel más preferido (0.7529), seguido de la modalidad en bolsitas (bags), con una utilidad de 0.1369; el nivel granulado presenta la preferencia más baja (-0.8898).Por último, para el atributo aroma, que sea aromático (yes) es el nivel más preferido, con una utilidad de 0.4108; al ser un atributo con solo dos niveles, la preferencia del otro nivel () es -0.4108.Por último, para el atributo aroma, que sea aromático (yes) es el nivel más preferido, con una utilidad de 0.4108; al ser un atributo con solo dos niveles, la preferencia del otro nivel () es -0.4108.La última parte de la salida recoge la importancia relativa de cada atributo, calculada partir de la importancia de cada atributo (22.4) pero expresada como proporción de la suma de importancias de todos ellos (22.5).En este ejemplo, el atributo con más peso es la variedad de té, con una importancia relativa del 32.22%; el siguiente es la presentación, con un 27.15%; después, el precio, con un 24.76%; y por último, el aroma, con un 15.88%. Es decir, el conjunto de 100 individuos, al valorar los perfiles que se les presentan, consideran principalmente el atributo variedad, seguido de forma de presentación y precio; por el contrario, que sea aromático o aporta relativamente poca valoración.De forma adicional, el análisis conjunto también permite profundizar en el conocimiento de los individuos en función de sus preferencias. Como ejemplo, se puede abordar cuestión de si existen grupos de individuos con preferencias similares entre ellos y diferentes de las del resto de grupos, cuestión conocida generalmente como segmentación. En este caso, el objetivo del proyecto sería identificar grupos de encuestados con preferencias similares, segmentando el mercado y permitiendo adaptar los atributos de cada producto o servicio las características concretas de ese nicho de mercado.Para ello, se puede utilizar la función caSegmentation(), que devuelve un análisis de conglomerados dividiendo los individuos en \\(k\\) clusters, usando el método k-means (véase Sec. 31). Como ejemplo, se consideran tres clusters.El vector de agrupación resume las características de los tres grupos formados.La salida contiene, en el apartado Cluster means, las utilidades medias de cada nivel (recogidas en las 13 columnas) para cada uno de los tres grupos (en filas), mostrando las diferencias entre ellas. La composición de cada cluster se muestra en el apartado Clustering vector, que recoge el cluster de pertenencia de cada uno de los cien individuos.ResumenEl análisis conjunto estudia situaciones de elección múltiple. Divide un producto o servicio en atributos y niveles y analiza las utilidades parciales de cada uno; después, se realizan diferentes combinaciones de éstos para identificar las preferencias del consumidor, estableciendo qué atributos y niveles son los más valorados, cuantificando dicha valoración de forma relativa. Permite evaluar las preferencias del público ante el lanzamiento de nuevos productos o su sensibilidad de alguna característica, como el precio, el formato, cambios en la imagen del producto, etc.","code":"\nlibrary(\"conjoint\")\ndata(\"tea\")\nConjoint(y=tpref, x=tprof, z=tlevn)\ncaUtilities(y=tprefm , x=tprof, z=tlevn)\n#> \n#> Call:\n#> lm(formula = frml)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -5,1888 -2,3761 -0,7512  2,2128  7,5134 \n#> \n#> Coefficients:\n#>                    Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)         3,55336    0,09068  39,184  < 2e-16 ***\n#> factor(x$price)1    0,24023    0,13245   1,814    0,070 .  \n#> factor(x$price)2   -0,14311    0,11485  -1,246    0,213    \n#> factor(x$variety)1  0,61489    0,11485   5,354 1,02e-07 ***\n#> factor(x$variety)2  0,03489    0,11485   0,304    0,761    \n#> factor(x$kind)1     0,13689    0,11485   1,192    0,234    \n#> factor(x$kind)2    -0,88977    0,13245  -6,718 2,76e-11 ***\n#> factor(x$aroma)1    0,41078    0,08492   4,837 1,48e-06 ***\n#> ---\n#> Signif. codes:  0 '***' 0,001 '**' 0,01 '*' 0,05 '.' 0,1 ' ' 1\n#> \n#> Residual standard error: 2,967 on 1292 degrees of freedom\n#> Multiple R-squared:  0,09003,    Adjusted R-squared:  0,0851 \n#> F-statistic: 18,26 on 7 and 1292 DF,  p-value: < 2,2e-16\n#>  [1]  3.55336207  0.24022989 -0.14311494 -0.09711494  0.61488506  0.03488506\n#>  [7] -0.64977011  0.13688506 -0.88977011  0.75288506  0.41077586 -0.41077586\ncaImportance(y=tprefm , x=tprof)\n#> [1] 24.76 32.22 27.15 15.88\nsegments<-caSegmentation(y=tprefm , x=tprof, c=3)\nsegments$segm\n#> K-means clustering with 3 clusters of sizes 40, 40, 20\n#> \n#> Cluster means:\n#>       [,1]   [,2]   [,3]     [,4]   [,5]     [,6]     [,7]     [,8]     [,9]\n#> 1 5.480275 2.9381 1.3681 4.540275 1.9731 3.782900 1.382900 0.965750 2.820750\n#> 2 4.754975 4.6918 3.6718 6.964975 6.6918 3.500525 4.385525 2.717225 3.062225\n#> 3 2.623500 6.6211 4.7511 2.933500 2.5211 4.189050 4.549050 5.066950 2.086950\n#>      [,10]    [,11]    [,12]    [,13]\n#> 1 0.111225 3.450750 0.442900 0.692900\n#> 2 1.840925 6.292225 6.595525 7.105525\n#> 3 5.312100 4.266950 4.859050 3.569050\n#> \n#> Clustering vector:\n#>   [1] 2 3 2 2 2 1 2 3 2 2 2 2 1 1 1 1 3 1 3 1 1 2 1 2 3 2 3 3 2 2 1 2 3 2 2 2 2\n#>  [38] 1 1 1 1 3 1 3 1 2 2 1 1 1 2 1 1 1 2 2 1 2 1 3 1 1 2 3 3 2 1 1 1 2 2 1 2 3\n#>  [75] 2 2 2 1 2 2 3 3 2 2 1 1 1 1 3 1 3 1 3 1 1 2 1 3 2 2\n#> \n#> Within cluster sum of squares by cluster:\n#> [1] 1605.654 2690.267 1131.293\n#>  (between_SS / total_SS =  41.4 %)\n#> \n#> Available components:\n#> \n#> [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n#> [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\""},{"path":"tablas-contingencia.html","id":"tablas-contingencia","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"Capítulo 23 Análisis de tablas de contingencia","text":"José-María MonteroUniversidad de Castilla-La Mancha","code":""},{"path":"tablas-contingencia.html","id":"motiv","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.1 Introducción","text":"Las tablas de contingencia analizan la relación existente entre variables categóricas, o susceptibles de categorizar, con un número de categorías finito. Dada su naturaleza, permiten el uso de las tradicionales operaciones aritméticas, con lo cual, en el ámbito de la Estadística Descriptiva su análisis suele basarse en diagramas de barras y porcentajes (véase Cap. ??), y en la esfera de la Inferencia Estadística (Cap. 13) se centra en los contrastes de hipótesis paramétricos y, básicamente, en el contraste de independencia entre dos o más de estas variables. Una pregunta que suele hacerse toda aquella persona que se acerca por primera vez al análisis de tablas de contingencia es el significado del término “contingencia”. Pues bien, este término fue acuñado por Pearson (Pearson 1904) al apuntar: “… Este resultado nos permite partir de la teoría matemática de la independencia probabilística, tal como se desarrolla en los libros de texto elementales, y construir partir de ella una teoría generalizada de la asociación o, como yo la llamo, contingencia.” El análisis de tablas de contingencia (o de asociación) permite dar respuesta, entre otras, preguntas como: los factores involucrados en una tabla de contingencia, ¿son independientes o están asociados? Si están asociados, ¿qué niveles de dichos factores son los que están asociados?, ¿cuál es la intensidad de dicha asociación?","code":""},{"path":"tablas-contingencia.html","id":"notac","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.1.1 Notación","text":" Sea una población (o una muestra) de N elementos sobre la que se pretende analizar, simultáneamente, dos (por simplicidad) atributos o factores (y B) con R y C niveles, modalidades o categorías, respectivamente. Sean {A1, A2, …, AR} y {B1, B2, …, BC} los niveles anteriormente aludidos. Sea nij el número de elementos que presentan la vez las modalidades y j de los factores y B, respectivamente. La tabla estadística que describe, conjuntamente, estos N elementos (en otros términos, que muestra las frecuencias conjuntas de los niveles de ambos factores) se denomina tabla de contingencia. modo de ejemplo, considérese una muestra de 80 ayuntamientos de una CC.AA., anotándose en la base ayuntam, incluida en el paquete CRD del libro, el signo político del equipo gubernamental (signo_gob) y si prestan o públicamente el servicio X (serv). Los resultados obtenidos fueron los siguientes:Del estudio de las distribuciones marginales de ambos factores se deduce que el 52,5% de los ayuntamientos de la CC.AA. están regidos por los Avanzados y el 47,5% por los Ilustrados. Y, más interesante, que en el 75% de los ayuntamientos prestan el servicio X.El análisis de la tabla de contingencia daría respuesta las siguientes preguntas: ¿La prestación pública del servicio X es independiente del signo político del ayuntamiento o depende de dicho signo? En este último caso: ¿Qué signo político está asociado con la prestación pública y cuál ?, ¿la asociación entre los factores “Signo político del equipo gubernamental” y “Prestación pública del servicio X” es muy intensa? Pero dicho análisis se abordará posteriormente.En función del número de factores involucrados en la tabla y del número de niveles de cada uno de ellos se tiene la siguiente tipología de tablas de contingencia:Tablas \\(R\\times C\\): 2 factores, el primero con R niveles y el segundo con C niveles.Tablas \\(R\\times C\\): 2 factores, el primero con R niveles y el segundo con C niveles.Tablas \\(R\\times C \\times M\\): 3 factores, con R, C y M niveles, respectivamente.Tablas \\(R\\times C \\times M\\): 3 factores, con R, C y M niveles, respectivamente.Y así sucesivamente.Y así sucesivamente.\nDentro de las tablas \\(R\\times C\\) se distinguen las tablas \\(2\\times 2\\) de las demás, por su especial interés en la realidad y por criterios pedagógicos, al ser las más sencillas.","code":"\nlibrary(\"CDR\")\ndata(\"ayuntam\")\n\nsummarytools::ctable(ayuntam$signo_gob, ayuntam$serv, headings = TRUE) |>\n  print()\n#> Cross-Tabulation, Row Proportions  \n#> signo_gob * serv  \n#> Data Frame: ayuntam  \n#> \n#> ------------ ------ ------------ ------------ -------------\n#>                serv           No           Sí         Total\n#>    signo_gob                                               \n#>    Avanzados          14 (33.3%)   28 (66.7%)   42 (100.0%)\n#>   Ilustrados           6 (15.8%)   32 (84.2%)   38 (100.0%)\n#>        Total          20 (25.0%)   60 (75.0%)   80 (100.0%)\n#> ------------ ------ ------------ ------------ -------------"},{"path":"tablas-contingencia.html","id":"prodecim","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.1.2 Diseños experimentales o procedimientos de muestreo que dan lugar a una tabla de contingencia","text":" Una cuestión la que se le da la suficiente importancia es la forma en la que se toma la información contenida en la tabla (el diseño del experimento o procedimiento de muestreo). Dada una determinada tabla de contingencia, ésta puede haber sido obtenida mediante uno u otro diseño de experimento o procedimiento de muestreo, y esta circunstancia es baladí, puesto que condiciona su análisis, sobre todo cuando el tamaño muestral es pequeño.Sin ánimo de exhaustividad, los diseños experimentales o procedimientos de muestreo más habituales que dan lugar una tabla de contingencia son los siguientes:164Tipo 1: se fijan los totales marginales de ambos factoresEjemplo: se desea investigar si la preferencia de la larva de gorgojo por el tipo de judía es independiente de la cubierta de la semilla o depende de ésta. Para ello se toman 22 judías de tipo y 18 de tipo B, que se introducen en un recipiente con 33 larvas. Dadas las condiciones de densidad, entrará más de una larva por judía. Pasado un tiempo prudencial para que las larvas entren en las judías, se cuentan las que han sido atacadas de cada tipo y las que .165Como puede apreciarse, los totales marginales de ambos factores han sido fijados en el diseño del experimento.Tipo 2: sólo se fijan los totales marginales de uno de los factoresEjemplo: en un municipio se desea investigar si el desempleo es o independiente del sexo del desempleado. Se seleccionan aleatoriamente 100 varones y 100 mujeres y se les pregunta por su situación laboral (trabajando; en paro).[^tablas_conting-marginales columnas][^tablas_conting-marginales columnas]: En este caso los totales marginales por columnas son variables.Tipo 3: únicamente se fija el tamaño muestralEjemplo: un estudio transversal sobre la prevalencia de osteoporosis y su relación con dietas pobres en calcio incluyó 400 mujeres entre 50 y 54 años. Cada una de ellas realizó una densiometría de columna y rellenó un cuestionario sobre sus antecedentes dietéticos para determinar si su dieta era o pobre en calcio.[^tablas_conting-marginales filas][^tablas_conting-marginales filas]: Ahora tanto los totales marginales por columnas como por filas son variables.","code":""},{"path":"tablas-contingencia.html","id":"contraste-de-independencia-en-tablas-2-times-2","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.2 Contraste de independencia en tablas \\(2 \\times 2\\)","text":"\nComo se avanzó en la Sec. 23.1, la primera pregunta la que debe dar respuesta el análisis de tablas de contingencia es si los factores involucrados en la tabla son independientes o, por el contrario, están asociados. La respuesta esta pregunta exige llevar cabo un contraste de independencia y, para ilustrarlo, se aborda, inicialmente, el caso de las tablas \\(2\\times 2\\). Dicho contraste se lleva cabo de tres formas: (\\(\\)) exacta, (\\(ii\\)) aproximada, y (\\(iii\\)) aproximada con corrección de continuidad.\n","code":""},{"path":"tablas-contingencia.html","id":"plantgen","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.2.1 Planteamiento general del contraste exacto de independencia","text":"Hipótesis:\n\\(H_0\\): los factores son independientes.\n\\(H_1\\): están asociados.166\nHipótesis:\\(H_0\\): los factores son independientes.\\(H_1\\): están asociados.166Filosofía del contraste: se trata de un contraste de significación. Por tanto, la tabla observada será “rara” (bajo \\(H_0\\)) si su probabilidad, más la probabilidad de obtener tablas más alejadas de H0 que ella, es inferior al nivel de significación, \\(\\alpha\\), prefijado para el contraste. En ese caso, se rechaza la hipótesis de independencia entre los factores involucrados en la tabla.Filosofía del contraste: se trata de un contraste de significación. Por tanto, la tabla observada será “rara” (bajo \\(H_0\\)) si su probabilidad, más la probabilidad de obtener tablas más alejadas de H0 que ella, es inferior al nivel de significación, \\(\\alpha\\), prefijado para el contraste. En ese caso, se rechaza la hipótesis de independencia entre los factores involucrados en la tabla.\n","code":""},{"path":"tablas-contingencia.html","id":"algoritmo","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.2.2 Algoritmo para la realización del contraste exacto de independencia","text":"De acuerdo con la filosofía de los contrastes de significación (Sec. 13.5), el algoritmo para la realización del contraste de independencia en tablas de contingencia es como sigue:Selección de la tablas del espacio muestral que se alejen de la hipótesis de independencia, en la dirección marcada por la hipótesis alternativa, tanto o más que la tabla observada, incluida esta última.Selección de la tablas del espacio muestral que se alejen de la hipótesis de independencia, en la dirección marcada por la hipótesis alternativa, tanto o más que la tabla observada, incluida esta última.Cálculo, bajo la hipótesis de independencia, de la probabilidad de ocurrencia de cada una de las tablas seleccionadas en el punto 1.Cálculo, bajo la hipótesis de independencia, de la probabilidad de ocurrencia de cada una de las tablas seleccionadas en el punto 1.Suma de dichas probabilidades y comparación con el \\(\\alpha\\) prefijado.Suma de dichas probabilidades y comparación con el \\(\\alpha\\) prefijado.Toma de la decisión relativa al rechazo o de la hipótesis de independencia.Toma de la decisión relativa al rechazo o de la hipótesis de independencia.Nótese que \\(()\\) los pasos 1 y 2 dependen del diseño del experimento o procedimiento de muestreo llevado cabo; \\((ii)\\) en ausencia del software adecuado, la realización de un test exacto es un procedimiento laborioso (veces un reto), con lo cual, si ese fuera el caso, los test aproximados de independencia son bienvenidos.continuación, se expone el contraste de independencia, en sus versiones exacta, aproximada y aproximada con corrección de continuidad, cuando el procedimiento de muestreo o diseño experimental es el de tipo 1. En la Sec. 23.2.4 se comentan algunas cuestiones de interés cuando el diseño de muestreo es de tipo 2 o tipo 3.","code":""},{"path":"tablas-contingencia.html","id":"contraste-de-independencia-diseño-tipo-1","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.2.3 Contraste de independencia: diseño tipo 1","text":"","code":""},{"path":"tablas-contingencia.html","id":"contraste-exacto-test-exacto-de-fisher","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.2.3.1 Contraste exacto (test exacto de Fisher)","text":"\nConsidérese el ejemplo del diseño tipo 1 expuesto en 23.1.2167. Supóngase que el resultado obtenido fue el siguiente:Según el algoritmo expuesto en la Sec. 23.2.2, el contraste es como sigue:1. Selección de las tablas que se alejan de \\(H_0\\) tanto o más que la observada.168 Como se señalaba en Pearson (1904), la teoría de la independencia probabilística indica que, bajo la hipótesis de independencia, el porcentaje de judías de tipo y de tipo B atacadas (o atacadas) por una larva de gorgojo tiene que ser el mismo. En otros términos, bajo la hipótesis de independencia, en cada una de las cuatro celdas se tiene que verificar que: \\(N_{ij} = \\frac{N_{.} N_{.j}}{N}, \\forall ,j\\), donde \\(\\frac{N_{.} N_{.j}}{N}= {E}_{ij}\\) se denomina frecuencia esperada bajo la hipótesis de independencia (en este caso, al estar los totales marginales fijos, \\({E}_{ij}=\\frac{n_{.} n_{.j}}{n}\\)). Denominando \\({D}_{ij}=N_{ij}-{E}_{ij}, \\hspace{0,2cm}\\forall ,j, \\hspace{0,2cm} =1,2,\\hspace{0,2cm} j=1,2\\), se puede que comprobar que en una tabla \\(2\\times 2\\), \\({D}_{11}={D} _{22}= -{D}_{12}= - {D}_{21}\\), con lo cual, tomando de referencia, por ejemplo, la celda {1,1}, las tablas que se alejan tanto o más que la observada de la hipótesis de independencia son aquellas que verifican, en valor absoluto, que \\({D}_{11}=N_{11}-{E}_{11}\\geq n_{11}-\\frac {n_{1.} n_{\\cdot1}}{n}\\).En el ejemplo que se considera, las \\({D}_{11}\\) son las siguientes (en negrita las de la tabla observada y aquellas otras que se alejan tanto o más que ella de \\(H_0\\)):T0: -3,85; T1: -2,85; T2: -1,85; T3: -0,85; T4: 0,15; T5: 1,15; T6: 2,15; T7: 3,15,donde el subíndice de T indica el valor de \\(N_{11}\\) en dicha tabla.\nNótese que el criterio anterior es otro que el criterio general de seleccionar las tablas en las que la diferencia de porcentajes, por ejemplo, por fila, en valor absoluto, sea superior la de la tabla observada, puesto que \\(\\left|\\frac {N_{11}}{n_{1.}} -\\frac {N_{21}}{n_{2.}} \\right|=|{D}_{11} | \\frac {n}{n_{1.} n_{2.} }\\).2. Cálculo, bajo la hipótesis de independencia, de la probabilidad de ocurrencia de cada una de las tablas seleccionadas en 1. La probabilidad de ocurrencia de una tabla de contingencia con los totales marginales fijos se puede obtener como el cociente entre el número de disposiciones de las frecuencias observadas favorables dicha tabla y el número de disposiciones posibles. El número de disposiciones favorables coincide con el coeficiente multinomial (maneras de que de \\(n\\) frecuencias observadas, \\(n_{11}\\) caigan en la celda {1,1}, \\(n_{12}\\) lo hagan en la celda {1,2}, \\(n_{21}\\) lo hagan en la celda {2,1} y \\(n_{22}\\) lo hagan en la celda {2,2}:\\(\\frac {n!}{n_{11}! n_{12}! n_{21}! n_{22}!}\\)).El número de disposiciones posibles, supuesta \\(H_0\\), es: \\(\\binom{n}{n_{1.}} \\binom{n}{n_{\\cdot1}} = \\frac{n!}{(n_{1.}! n_{2.}!)} \\frac {n!}{(n_{\\cdot1} ! n_{\\cdot2})}\\).Por tanto, el cociente entre ambas es: \\(P=\\frac {n_{1.} !n_{2.} !n_{\\cdot1} !n_{\\cdot2} !} {n!n_{11} !n_{12} !n_{21} !n_{22} !}\\).En consecuencia, las probabilidades de las tablas seleccionadas en el punto 1 son: \\(T_0: 0,0017; T_1: 0,0219; T_7: 0,0091\\).3. Suma de dichas probabilidades: 0,0327.4. Comparación con \\(\\alpha\\) y decisión sobre el rechazo o de la hipótesis de independencia: La decisión depende del valor de \\(\\alpha\\). Si fuera, por ejemplo, 0,05, se rechazaría la independencia entre el tipo de judía y si es o atacada por la larva de gorgojo.El código R necesario para tomar llevar cabo el test exacto de Fisher anterior es: Como puede apreciarse, se rechaza la hipótesis de independencia frente la de asociación (test bilateral). Esto significa que las larvas ataquen un tipo de judía y al otro. Atacan ambos tipos, ¡y bastante! Este es el primer hecho que se constata. Sin embargo, atacan más las judías de tipo (un 95% son atacadas) que las de tipo B (dos terceras partes son atacadas). Esa diferencia porcentual de judías atacadas se considera significativa bajo el supuesto de independencia y, en ese sentido, se dice que existe asociación entre el tipo de judía y la presencia o de larva atacante. La asociación sería -SI y B-. Sin embargo, ¡cuidado!, las larvas atacan siempre. La asociación anterior debe entenderse como “el porcentaje de ataque es muy grande en ambos casos, pero en (mucho) más que en B. Este es el segundo hecho importante que se constata: las larvas muestran una preferencia significativa por las judías tipo . Aunque ya se ha visto la dirección de la asociación (en el sentido de la diagonal ascendente), en la Sec. 23.4, dedicada las medidas de asociación en tablas 2x2, se cuantificará su intensidad.","code":"\ndatos_jud = c(1,6,21,12)\ntabla = cbind(expand.grid(list(Tipo_de_judía = c(\"A\",\"B\"), \n                               Presencia_larva = c(\"No\",\"Sí\"))), \n              count = datos_jud)\ntabla_jud <- ftable(xtabs(count~Tipo_de_judía+Presencia_larva, tabla))\n#Ho: Los factores son independientes. \n#H1: Los factores están asociados\nfisher <- fisher.test(tabla_jud, alternative = \"two.sided\") \nfisher$p.value\n#> [1] 0.0327607\n#Ho: Los factores son independientes. \n#H1: Existe asociación negativa.\nfisher_less <- fisher.test(tabla_jud, alternative = \"less\") \nfisher_less$p.value\n#> [1] 0.02361309\n#Ho: Los factores son independientes. \n#H1: Existe asociación positiva.\nfisher_greater <- fisher.test(tabla_jud, alternative = \"greater\") \nfisher_greater$p.value\n#> [1] 0.998293"},{"path":"tablas-contingencia.html","id":"contraste-aproximado","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.2.3.2 Contraste aproximado","text":"En este caso (tipo 1), bajo \\(H_0\\): independencia, la frecuencia conjunta de una celda, \\(N_{ij}\\), cualquiera que sea, se distribuye según una ley hipergeométrica con \\(E(N_{ij})=\\frac{N_{ij}} {n_{.}n_{\\cdot j}}\\) y \\(V(N_{ij})=\\frac{n_{.}n_{\\cdot j}(n-n_{.}) (n-n_{\\cdot j})}{n^2 (n-1)}\\). Por consiguiente,\n\\[P\\left(\\left( N_{11}-\\frac{n_{1.} n_{\\cdot1}} {n} \\right) ^2 \\geq \\left(n_{11} - \\frac{n_{1.} n_{\\cdot1}}{n}\\right)^2 \\right)= P\\left(\\frac{(N_{11}-\\frac{n_{1.} n_{\\cdot1}}\n{n})^2}{\\frac{n_{1.}n_{\\cdot1}n_{2.} n_{\\cdot2}}{n^2 (n-1)}}\n\\geq \\frac{(n_{11}-\\frac{n_{1.} n_{\\cdot1}}{n})^2}{\\frac{n_{1.}n_{\\cdot1}n_{2.} n_{\\cdot2})}{n^2 (n-1)}}\\right).\\]Y si ninguna \\(\\hat{E}_{ij}=\\frac{n_{ij}} {n_{.}n_{\\cdot j}}\\) es inferior 5, la probabilidad anterior puede aproximarse (teorema central del límite) por:\n\\[P\\left(\\chi^2_1 \\geq {\\frac{\\left( n_{11}-\\frac{n_{1.} n_{\\cdot1}}\n{n}\\right)^2} {\\frac{n_{1.}n_{\\cdot1}n_{2.} n_{\\cdot2}}{n^2 (n-1)}}}\\right)=P\\left( \\chi^2_1 \\geq {\\frac{ (n-1)(n_{11}n_{22}-n_{21}n_{12}\n)^2} {n_{1.}n_{\\cdot1}n_{2.} n_{\\cdot2}}}\\right),\\]\ndonde el estadístico \\(\\frac{ (n-1)(n_{11}n_{22}-n_{21}n_{12} )^2} {n_{1.}n_{\\cdot1}n_{2.} n_{.2}}\\)\nse denomina chi-cuadrado ajustado \\((\\chi^2_{ajd})\\) y es tal que\n\\(\\chi^2_{ajd}= \\frac {n-1} {n}\\frac{ n (n_{11}n_{22}-n_{21}n_{12} )^2} {n_{1.}n_{\\cdot1}n_{2.} n_{\\cdot2}}\\),\ndonde \\(\\frac{ n (n_{11}n_{22}-n_{21}n_{12} )^2} {n_{1.}n_{\\cdot1}n_{2.} n_{\\cdot2}}\\)\nes el estadístico chi-cuadrado (\\(\\chi^2\\)) que proporcionan todos los softwares de contraste de independencia en tablas de contingencia.En el ejemplo propuesto:Como \\(\\chi^2= 5,6828\\), entonces \\(\\chi^2_{ajd}=5,54073\\) y \\(P\\left(\\chi^2_{ajd} \\geq 5,54073\\right)=0,0186\\).169Nótese que la probabilidad exacta de obtener una tabla tan alejada o más de la hipótesis de independencia que la observada (incluida esta) es 0,0327, mientras que la probabilidad aproximada es 0,0186. La aproximación es muy buena, y ello se debe la existencia de frecuencias esperadas menores que 5.","code":"\nchisq.test(tabla_jud)$expected\n#>      [,1]  [,2]\n#> [1,] 3.85 18.15\n#> [2,] 3.15 14.85\nchisq.test(tabla_jud, correct=FALSE) \n#> \n#>  Pearson's Chi-squared test\n#> \n#> data:  tabla_jud\n#> X-squared = 5.6828, df = 1, p-value = 0.01713"},{"path":"tablas-contingencia.html","id":"contraste-aproximado-con-corrección-de-continuidad","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.2.3.3 Contraste aproximado con corrección de continuidad","text":"\nComo se vio en la subsección anterior, al aproximar la probabilidad de obtención de tablas tanto o más alejadas de \\(H_0\\) que la observada (que se calcula con una distribución hipergeométrica, que es discreta) mediante una distribución \\(\\chi^2_1\\) (que es continua), se comete un “error de continuización”. Dicho error se intenta corregir incluyendo en el contraste una corrección de continuidad. Hay varias correcciones que han tenido cierto éxito en la literatura. La más popular es la corrección de Yates, si bien sólo se recomienda cuando las \\({E}_{ij}\\) sean múltiplos de 0,5.En el contraste aproximado con corrección de Yates, se rechaza \\(H_0\\) si:\n\\[P\\left( \\chi^2_1 \\geq {\\frac{ (n-1)(|n_{11}n_{22}-n_{21}n_{12}\n|-0,5n)^2} {n_{1.}n_{\\cdot1}n_{2.} n_{\\cdot2}}}  \\right)\\leq \\alpha ,\\]\ndonde el estadístico \\({\\frac{ (n-1)(|n_{11}n_{22}-n_{21}n_{12}|-0,5n)^2} {n_{1.}n_{\\cdot1}n_{2.} n_{\\cdot2}}}\\) se denomina estadístico chi-cuadrado ajustado corregido de continuidad de Yates (\\(\\chi^2_{ajd,CCY}\\)).En el ejemplo propuesto, el test chi-cuadrado con corrección de continuidad de Yates se obtiene directamente con la función chisq.test() que, por defecto, incluye el argumento correct = TRUE.Como el estadístico Chi-cuadrado corregido de continuidad \\(\\chi^2_{CCY}\\) vale 3,8337, entonces \\(\\chi^2_{ajd,CCY}=\\frac{39}{40}\\times 3,8337=3,7636\\); y como \\(P\\left( \\chi^2_1 \\geq 3,7636\\right)=0,0524\\),\n\\(H_0\\) se rechazaría cuando \\(\\alpha > 0,0524\\). Nótese que si, por ejemplo, \\(\\alpha = 0,05\\), la decisión sobre el rechazo o de \\(H_0\\) es distinta con \\(\\chi^2_{CCY}\\) y \\(\\chi^2_{ajd,CCY}\\); de ahí la importancia de utilizar el estadístico ajustado.Por tanto, la corrección de Yates ha transformado la infraestimación de la probabilidad exacta en una sobreestimación de más o menos el mismo tamaño. Ello se debe que en la tabla observada, hay freecuencias esperadas (\\({E}_{ij}\\)) que distan mucho de ser multiplos de 0,5. La corrección de Yates es la que incluye la librería utilizada (stats). Otras correcciones pueden verse en Ruiz-Maya et al. (1995) y J. M. Montero (2002).","code":"\nchisq.test(tabla_jud) \n#> \n#>  Pearson's Chi-squared test with Yates' continuity correction\n#> \n#> data:  tabla_jud\n#> X-squared = 3.8637, df = 1, p-value = 0.04934"},{"path":"tablas-contingencia.html","id":"dise","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.2.4 Contraste de independencia: diseños tipo 2 y tipo 3","text":"\nEn el caso tipo 2, para la realización del test exacto, las tablas que se alejan tanto o más que la observada de la hipótesis de independencia son las que verifican: \\[\\left| \\frac {N_{11}}{n_{1.}}-\\frac{N_{21}}{n_{2.}}\\right|\n\\geq \\left|\\frac{n_{11}}{n_{1.}}-\\frac{n_{21}}{n_{2.}}\\right|,\\]\ny la probabilidad de ocurrencia de una tabla de contingencia viene dada por:El estadístico de contraste en el test aproximado viene dado por \\(\\chi^2=\\frac{ n (n_{11}n_{22}-n_{21}n_{12})^2} {n_{1.}n_{\\cdot1}n_{2.} n_{\\cdot2}}\\), y por \\(\\chi^2_{CC}=\\frac { n \\left( \\left|n_{11}n_{22}-n_{21}n_{12}\\right|-\\frac {f} {2}\\right)^{2}} {n_{1.}n_{\\cdot1}n_{2.} n_{\\cdot2}}\\) en el caso de estar corregido de continuidad, siendo f el mayor factor común de los tamaños muestrales fijados.En el caso tipo 3, las tablas que se alejan tanto o más que la observada de la hipótesis de independencia son las que verifican la condición expuesta en el tipo 2 (y tipo 1), siendo su probabilidad de ocurrencia:El test aproximado, en este caso, es un test razón de verosimilitudes donde el estadístico de contraste, \\(G=-2ln\\frac {n_{1.}^{n_{1.}} n_{2.}^{n_{2.}} n_{\\cdot1}^{n_{\\cdot1}} n_{\\cdot2}^{n_{\\cdot2}}}{n_{11}^{n_{11}} n_{21}^{n_{21}} n_{12}^{n_{12}} n_{22}^{n_{22}} n^n}\\), también se distribuye como una \\(\\chi^2_1\\) en caso de independencia. Apenas hay literatura sobre correcciones de continuidad en este modelo y la poca que hay sugiere la aplicación de la corrección de Yates.NotaEn el diseño de muestreo tipo 3 se recomienda usar ninguna corrección de continuidad, salvo que el tamaño muestral sea muy pequeño y sea imprescindible la realización del test.El código R para llevar cabo estos dos contrastes aproximados puede verse en la Sec. 23.3.1.","code":""},{"path":"tablas-contingencia.html","id":"contrasteRC","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.3 Contraste de independencia en tablas \\(R \\times C\\)","text":"El análisis de tablas \\(R\\times C\\) puede considerarse, en principio, una generalización del caso de tablas \\(2\\times 2\\). Ahora bien, en el caso \\(R\\times C\\) los test exactos, recomendados en el caso de que \\(E_{ij}\\leq5\\) en más del 20% de las celdas, [H. T. Reynolds (1984)]170 son un auténtico reto y aún están disponibles en el software convencional.Si se cumple el requisito anterior, una solución es agrupar categorías, con sentido común y coherencia.171 Si la agrupación de categorías pudiese hacerse, por carecer de sentido o cualquier otro motivo, lo más honesto sería realizar el contraste hasta disponer de una base de datos mejor.la luz de lo anteriormente expuesto, en el caso de tablas \\(R\\times C\\) la atención se centra en los tests aproximados.","code":""},{"path":"tablas-contingencia.html","id":"contaprox","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.3.1 Contrastes aproximados","text":"Cuando el procedimiento de muestreo o el diseño experimental es de tipo 1 o 2 el contraste aproximado de independencia es el denominado contraste Chi-cuadrado. La filosofía de dicho contraste es la siguiente: parece lógico que el contraste se base en las diferencias (cuadráticas, para que se compensen las negativas con las positivas) entre las frecuencias observadas y las esperadas bajo la hipótesis de independencia. Si los factores son independientes, dichas diferencias serán pequeñas y atribuibles fluctuaciones aleatorias. Si están asociados, serán grandes y atribuibles la asociación existente entre sus niveles. Pearson propuso el siguiente estadístico de contraste: \\[\\chi^2 =\\sum_{=1}^{R} \\sum_{j=1}^{C}\\frac {\\left(N_{ij}-{E}_{ij}\\right)^2} {{E}_{ij}},\\]\nque, si se incumple el requisito expuesto en las primeras lineas de la Sec. 23.3, y bajo la hipótesis de independencia, se distribuye como una \\(\\chi_{(R-1)(C-1)}^2\\). En caso de que \\(P\\left( \\chi_{(R-1)(C-1)}^2 \\geq \\sum_{=1}^{R} \\sum_{j=1}^{C}\\frac {\\left(n_{ij}-\\hat{E}_{ij}\\right)^2} {\\hat{E}_{ij}}\\right )\\) sea inferior al nivel de significación prefijado, se rechaza la hipótesis de independencia.172 Téngase en cuenta que en el caso \\(R\\times C\\) la hipótesis alternativa es “al menos un nivel de un factor está asociado con un nivel del otro factor”.La razón de que las diferencias\n\\(\\left(N_{ij}-\\hat{E}_{ij}\\right)^2\\) se dividan por \\(\\hat{E}_{ij}\\) en el estadístico chi-cuadrado de Pearson es la siguiente: la misma diferencia \\(N_{ij}-\\hat{E}_{ij}\\) puede significar cosas bien diferentes. Una diferencia de 5 es nada si \\(\\hat{E}_{ij}=1000\\); pero es muchísimo si \\(\\hat{E}_{ij}=2\\). Por eso la diferencia (cuadrática) se pone en relación con la frecuencia esperada.En el caso de que el procedimiento de muestreo sea del tipo 3, aunque puede aplicarse el contraste chi-cuadrado, es recomendable proceder con el contraste de independencia de razón de verosimilitudes, que compara por cociente las frecuencias esperadas bajo la hipótesis de independencia y las observadas. Se basa en la razón de la verosimilitud de la hipótesis de independencia la luz de la muestra obtenida y del máximo de la función de verosimilitud, \\(\\Lambda\\). Bajo el supuesto de independencia la razón será cercana la unidad, atribuyéndose la diferencia fluctuaciones aleatorias; el logaritmo neperiano de dicha razón (negativo) estará cercano cero. En caso contrario, el cociente de verosimilitudes (negativo) disminuye, tanto más cuanto más diferencia hay entre la verosimilitud de la hipótesis de independencia y el máximo de la función de verosimilitud. En Wilks (1935) se demostró que, cuando la hipótesis de independencia es cierta, \\(G=-2ln \\Lambda\\), con \\(\\Lambda= \\prod_{=1}^R \\prod_{j=1}^C \\left (\\frac {{E}_{ij}} {N_{ij}}\\right)^{N_{ij}}\\) se distribuye como una \\(\\chi_{(R-1)(C-1)}^2\\). Ambos estadísticos de contraste, \\(\\chi^2\\) y \\(G\\), son asintóticamente equivalentes.\nmodo de ejemplo, se quiere contrastar si en la Comunidad de Madrid la opinión\nsobre la presidente Dña. Isabel Díaz Ayuso depende de la zona geográfica o si\npor el contrario, es independiente de ella. Para ello se encuestan,\npor algún procedimiento aleatorio 2795 personas con derecho voto en la comunidad\ny se eliminan las respuestas “NS/NC/es indiferente”. Los resultados obtenidos fueron los siguientes (dataset ayuso del paquete CDR):Como puede verse, sea cual sea el estadístico de contraste, la hipótesis de independencia se rechaza para cualquiera de los valores de \\(\\alpha\\) utilizados en la práctica (1%, 2,5%, 5%, 10%).","code":"\ndata(\"ayuso\")\ntabla_ayuso<-table(ayuso)\ntabla_ayuso\n#>                opinion\n#> zona            n1_nefasta n2_mala n3_buena n4_excelente\n#>   n1_mad_muni           25     500       50         1000\n#>   n2_metropol           10     280       50          460\n#>   n3_extraradio          5     130       25          260\nchisq.test(tabla_ayuso)$expected\n#>                opinion\n#> zona            n1_nefasta  n2_mala n3_buena n4_excelente\n#>   n1_mad_muni    22.540250 512.7907 70.43828     969.2308\n#>   n2_metropol    11.449016 260.4651 35.77818     492.3077\n#>   n3_extraradio   6.010733 136.7442 18.78354     258.4615\nchisq.test(tabla_ayuso, correct=FALSE) \n#> \n#>  Pearson's Chi-squared test\n#> \n#> data:  tabla_ayuso\n#> X-squared = 19.486, df = 6, p-value = 0.003418\nlibrary(\"DescTools\")\nGTest(tabla_ayuso,correct = \"none\")\n#> \n#>  Log likelihood ratio (G-test) test of independence without correction\n#> \n#> data:  tabla_ayuso\n#> G = 19.357, X-squared df = 6, p-value = 0.003602"},{"path":"tablas-contingencia.html","id":"contraste-aproximado-con-corrección-de-continuidad-1","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.3.2 Contraste aproximado con corrección de continuidad","text":"Afortunadamente, en la mayoría de las ocasiones el tamaño muestral es grande y los totales marginales están muy desequilibrados, con lo que los estadísticos chi-cuadrado y chi-cuadrado corregido de continuidad son prácticamente iguales, sobre todo si el número de niveles de ambos factores es elevado. En caso de utilizar una corrección de continuidad, hay unanimidad en utilizar la de Yates, sea cual sea el procedimiento de muestreo y el test (chi-cuadrado o G), si bien dicha unanimidad tiene mucho que ver con que es la única que está programada en el software convencional sobre tablas de contingencia.","code":""},{"path":"tablas-contingencia.html","id":"medidas","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.4 Medidas de asociación en tablas \\(2\\times 2\\)","text":"\nSi se rechaza la hipótesis de independencia, el análisis de la tabla se puede dar por finalizado. En caso contrario, el nuevo objetivo es determinar la dirección de la asociación detectada (o las fuentes de asociación en el caso \\(R\\times C\\)) y su intensidad, y para ello se utilizan las denominadas medidas de asociación. Igual que en el contraste de independencia, se distinguirán los casos \\(2\\times 2\\) y \\(R\\times C\\), en esta ocasión tanto por motivos pedagógicos sino porque las situaciones son bien diferentes.En el caso \\(2\\times 2\\), los tipos de asociación en función de su dirección (positiva y negativa) ya se se definieron en la Sec. 23.2.1. Por lo que se refiere los límites de su intensidad, se dice que la asociación es perfecta cuando al menos uno de los niveles de uno de los factores queda determinado por un nivel del otro factor. La asociación perfecta puede ser estricta o implícita de tipo 2:173Estricta: dado el nivel de un factor, el nivel del otro queda inmediatamente determinado.Implícita de tipo 2: dado un nivel de un factor, el nivel del otro queda inmediatamente determinado; dado el otro nivel, queda determinado el nivel del otro factor.\n","code":""},{"path":"tablas-contingencia.html","id":"la-q-de-yule","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.4.1 La \\({Q}\\) de Yule","text":"En caso de independencia, las frecuencias observadas coinciden con las esperadas. medida que las primeras se separan de las segundas, se produce un alejamiento de dicha hipótesis y los niveles de los factores aumentan la intensidad de su asociación. Por consiguiente, las diferencias \\({D}_{ij}\\) entre las frecuencias observadas y las esperadas bajo el supuesto de independencia pueden ser la base de una magnífica medida de asociación. mayores diferencias, mayor asociación. Mas sencillo todavía: una única diferencia, por ejemplo la \\({D}_{11}\\), podría servir como medida de asociación porque, como bajo la hipótesis de independencia, \\({D}_{ij}=0\\) y \\({D}_{11}={D}_{22}= -{D}_{12}=-{D}_{21}\\), entonces se tiene que:En caso de independencia: \\({D}_{11}={D}_{22}= 0\\) y \\({D}_{12}={D}_{21}=0\\), o simplemente, \\({D}_{11}=0\\).En caso de independencia: \\({D}_{11}={D}_{22}= 0\\) y \\({D}_{12}={D}_{21}=0\\), o simplemente, \\({D}_{11}=0\\).En caso de asociación positiva: \\({D}_{11}={D}_{22} \\geq 0\\) y \\({D}_{12}={D}_{21}\\leq 0\\), o simplemente, \\({D}_{11}\\geq 0\\).En caso de asociación positiva: \\({D}_{11}={D}_{22} \\geq 0\\) y \\({D}_{12}={D}_{21}\\leq 0\\), o simplemente, \\({D}_{11}\\geq 0\\).En caso de asociación negativa: \\({D}_{11}={D}_{22} \\leq 0\\) y \\({D}_{12}={D}_{21}\\geq 0\\), o simplemente, \\({D}_{11}\\leq 0\\).En caso de asociación negativa: \\({D}_{11}={D}_{22} \\leq 0\\) y \\({D}_{12}={D}_{21}\\geq 0\\), o simplemente, \\({D}_{11}\\leq 0\\).Por tanto, \\({D}_{11}\\) determina muy fácilmente la dirección de la asociación. Sin embargo, en cuanto la intensidad de la misma, el campo de variación de \\({D}_{11}\\), \\(\\left[-\\frac {N_{12} N_{21}} {n};\\frac {N_{12} N_{21}} {n}\\right]\\), depende de los valores de las frecuencias observadas (esto es un problema la hora de la interpretación) y la máxima intensidad asociativa se da cuando la diagonal descendente o la diagonal ascendente sólo contienen ceros, es decir en caso de asociación perfecta estricta (negativa o positiva).Para solucionar el problema anterior, se define la \\({Q}\\) de Yule como:\\[{Q}=\\frac {n {D}_{11}} {N_{11}N_{22} - N_{12}N_{21}}=\\frac{N_{11}N_{22} - N_{12}N_{21}} {N_{11}N_{22}+ N_{12}N_{21}}.\\]El campo de variación de \\({Q}\\) es \\([-1;1]\\) y:En caso de independencia, \\({Q}=0\\).En caso de asociación positiva, \\({Q}< 0\\).En caso de asociación negativa, \\({Q}> 0\\).Por tanto, cuando se sustituyen los datos de la tabla observada en \\({Q}\\), obteniéndose su valor muestral u observado: \\(\\hat{Q}=\\frac{n_{11}n_{22} - n_{12}n_{21}} {n_{11}n_{22}+ n_{12}n_{21}}\\), se actúa como sigue:Cuando \\(\\hat{Q}=0\\) se dice que hay independencia.Cuando \\(\\hat{Q}< 0\\) se dice que hay asociación negativa.Cuando \\(\\hat{Q}> 0\\) se dice que hay asociación positiva.Lógicamente, mayor valor absoluto de \\(\\hat{Q}\\) mayor intensidad de la asociación.En el ejemplo utilizado para ilustrar el diseño experimental de tipo 1, el valor observado de \\(Q\\) es \\(\\hat{Q}=0,83\\):la luz del valor del valor de \\(\\hat{Q}\\) se concluye la existencia de una fuerte asociación negativa.","code":"\nYuleQ(tabla_jud)\n#> [1] -0.826087"},{"path":"tablas-contingencia.html","id":"otras-medidas-de-asociación-para-tablas-2times-2","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.4.2 Otras medidas de asociación para tablas \\(2\\times 2\\)","text":"","code":""},{"path":"tablas-contingencia.html","id":"cmc","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.4.2.1 Cuadrado medio de la contingencia de Pearson","text":"\nLa primera medida de asociación que se nos viene todos la cabeza es el propio estadístico de contraste \\(\\chi^2\\). Sin embargo, puede utilizarse como medida de asociación porque es siempre positivo y, sobre todo, porque su valor máximo, \\(n(k-1)\\), depende del tamaño muestral \\(N\\) y de \\(k\\), el número más pequeño de filas o columnas. En el caso \\(2\\times 2\\) depende únicamente de \\(n\\) porque \\(k-1=1\\). Para eliminar el efecto tamaño muestral, se define el cuadrado medio de la contingencia de Pearson como:\\[{\\phi^2}= \\frac{\\chi^2}{n}=\\frac{\\left(N_{11}N_{22} - N_{12}N_{21}\\right)^2} {N_{1.} N_{2.} N_{\\cdot1}N_{\\cdot2}},\\]\ny se estima como \\[\\hat{\\phi^2}=\\frac{\\left(n_{11}n_{22} - n_{12}n_{21}\\right)^2} {n_{1.} n_{2.} n_{\\cdot1}n_{\\cdot2}}.\\] partir de la tabla observada.Su campo de variación es \\([0;1]\\), tomando el valor 0 en caso de independencia y 1 cuando hay asociación perfecta y estricta. Cuanto mayor sea el valor del coeficiente, mayor es intensidad de la asociación.proporciona la dirección de la asociación, si bien se puede saber por el signo de \\(n_{11}n_{22}-n_{12}n_{21}\\). Otra consideración importante es que, si se codifican los niveles de los factores como (0;1), \\({\\phi}^2\\) coincide con el coeficiente de determinación lineal entre los factores. Por tanto, la asociación que mide es “lineal” (de ahí que su valor suela ser más bajo que el de \\({Q}\\)). Su raíz cuadrada es conocida como “la V de Cramer”. En el ejemplo utilizado se tiene que:","code":"\n(Phi(tabla_jud))^2\n#> [1] 0.1420701"},{"path":"tablas-contingencia.html","id":"odds-ratio-o-cociente-de-posibilidadestablas_conting-6","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.4.2.2 Odds ratio o cociente de posibilidades174","text":"\nSe define como \\(\\alpha=\\frac {P_{11}/P_{12}} {P_{21}/P_{22}}\\), donde \\(P_{ij}\\) es la probabilidad de que un elemento de la tabla pertenezca al -ésimo nivel de y al j-ésimo de B, y se estima como \\(\\hat{\\alpha}=\\frac {n_{11}/n_{12}} {n_{21}/n_{22}}=\\frac {n_{11}n_{22}} {n_{12}n_{21}}\\).Su campo de variación es \\([0;\\infty)\\), asimétrico, y por consiguiente difícil de interpretar. En todo caso:Si \\(\\alpha < 1\\), la probabilidad (aquí denominada posibilidad) de pertenecer al nivel 1 del factor B es menor en el nivel 1 del factor que en el 2.Si \\(\\alpha = 1\\), la probabilidad de pertenecer al nivel 1 del factor B es la misma en ambos niveles del factor .Si \\(\\alpha > 1\\), la probabilidad pertenecer de al nivel 1 del factor B es mayor en el nivel 1 del factor que en el 2.Una posible solución la dificultad de interpretación es definir \\(ln{\\alpha}\\), que es una medida simétrica en \\((-\\infty;+\\infty)\\). Sin embargo, su interpretación, dada la gran amplitud del campo de variación, continúa siendo muy difusa.Una ventaja que tiene respecto \\({\\alpha}\\) es que cambia si las filas se convierten en columnas y las columnas en filas.175 Por ello, la razón de posibilidades se puede utilizar sólo en estudios retrospectivos, sino también en aquéllos prospectivos y transversales. Finalmente, nótese que \\(()\\) la razón de posibilidades y el riesgo relativo (\\(P_1/P_2\\)) se relacionan como sigue: \\(\\alpha= \\frac {P_1 (1-P_2)} {P_2 (1-P_1)}\\); y que \\((ii)\\) ambos son similares cuando la probabilidad de éxito \\(P_i\\) está cerca de cero en ambos grupos.El código siguiente proporciona el valor muestral de \\(\\alpha\\) (\\(\\hat\\alpha\\)) en el ejemplo que nos ocupa, así como su intervalo de confianza del 95%:","code":"\nlibrary(\"epiR\")\nepi.2by2(tabla_jud, method = 'cohort.count')$massoc.summary[2,]\n#>          var       est      lower     upper\n#> 2 Odds ratio 0.0952381 0.01021365 0.8880565"},{"path":"tablas-contingencia.html","id":"medidas-rxc","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.5 Medidas de asociación en tablas \\(R\\times C\\)","text":"En caso de rechazo de la hipótesis de independencia en una tabla \\(R\\times C\\), se concluye que al menos un nivel de uno de los factores está asociado con uno del otro factor. En ese caso, se utilizarán las medidas de asociación para determinar la intensidad de la misma. Las asociaciones de determinados niveles del factor con determinados niveles del factor B que llevan al rechazo de la independencia de ambos se denominan fuentes de asociación, y se determinan mediante los residuos estandarizados ajustados.\n","code":""},{"path":"tablas-contingencia.html","id":"medidas-derivadas-del-estadístico-chi-cuadrado","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.5.1 Medidas derivadas del estadístico Chi-cuadrado","text":"Como se avanzó en la Sec. 23.4.2.1, el estadístico \\(\\chi^2\\) puede utilizarse como medida de asociación porque su valor máximo, \\(n(k-1)\\), siendo \\(n\\) el tamaño muestral y \\(k\\) el número más pequeño de filas o columnas, depende tanto de \\(N\\) como del número de niveles de los factores.El cuadrado medio de la contingencia, \\(\\phi^2\\), elimina el efecto “tamaño muestral”, pero el efecto “número de niveles de los factores”. Igual le ocurre al coeficiente de contingencia; y la T de Tschuprow, salvo en las tablas cuadradas. La única medida derivada del estadístico \\(\\chi^2\\) que corrige ambos efectos es la V de Cramer:\n\\[V=\\sqrt\\frac{\\chi^2}{kn},\\] con \\(k=min\\left(R-1; C-1\\right)\\). Su campo de variación es \\([0,1]\\) y alcanza su máximo en caso de asociación perfecta. En tablas cuadradas \\(V=T\\).\nEn el ejemplo utilizado en la Sec. 23.3.1:Aunque se rechaza la hipótesis de independencia, la asociación existente entre la opinión sobre la presidente y la zona geográfica es muy pequeña. Y ello porque, sea cual sea la zona geográfica, aunque hay ligerísimas variaciones, la opinión es muy favorable: para alrededor del 60% es excelente, para la tercera parte muy buena y tan solo para el 5% es mala (alrededor del 4%) o muy mala (apenas el 1%).","code":"\nCramerV(tabla_ayuso)\n#> [1] 0.0590406"},{"path":"tablas-contingencia.html","id":"medidas-basadas-en-la-reducción-proporcional-del-error-lambda-de-goodman-y-kruskal","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.5.2 Medidas basadas en la reducción proporcional del error: \\(\\lambda\\) de Goodman y Kruskal","text":"\nAl contrario que las medidas basadas en el estadístico Chi-cuadrado, exigen determinar cuál es el factor explicativo y cuál el factor explicar. Sea el factor explicativo y B el factor explicar: supóngase que se selecciona aleatoriamente uno de los elementos de la tabla. Este elemento pertenecerá un nivel “” de y un nivel “j” de B. Supóngase que se quiere predecir el nivel de B al que pertenece, \\(()\\) sin utilizar el hecho de saber que nivel de pertenece, y \\((ii)\\) utilizando dicho hecho. Lógicamente, tanto en el caso \\(()\\) como en el \\((ii)\\) se comete un error \\((P()\\) y \\(P(ii)\\), respectivamente). La probabilidad de error será la misma si los factores son independientes. Sin embargo, si están asociados, el conocimiento del nivel de al que pertenece el elemento seleccionado ayudará en la predicción del nivel de B al que pertenece (tanto más cuanto más asociados estén los factores) y la probabilidad de error disminuirá respecto al caso \\(()\\). La reducción proporcional que se opera en el error es:\\[\\lambda=\\frac{P()-P(ii)} {P()},\\]donde \\(P()\\) y \\(P(ii)\\) se estiman como sigue: \\(\\hat{P}()= n- \\max_{j} n_{\\cdot j}\\) y \\(\\hat{P}(ii)= n-\\sum_{j=1}^C \\max_{j} n_{\\cdot j}.\\)En el caso en que sea el factor explicar, \\(\\hat{P}()= n- \\max_{} n_{.}\\) y \\(\\hat{P}(ii)= n-\\sum_{=1}^R \\max_{} n_{.}\\).En caso de tener claro cuál es el factor explicar, se utiliza la media agregativa de las dos medidas anteriores:\\[\\hat\\lambda=\\frac{\\sum_{j=1}^C \\max_i n_{.}-max_i n_{.}+\\sum_{=1}^R \\max_j n_{\\cdot j}-\\max_j n_{\\cdot j} }{2n-\\max_i n_{.}\\max_j n_{\\cdot j}}.\\]Su campo de variación es \\([0,1]\\). En caso de independencia, \\(\\lambda=0\\). Ahora bien, que \\(\\hat\\lambda=0\\) implica necesariamente que y B tengan que ser independientes, puesto que \\(\\lambda\\) también toma el valor 0 cuando en uno de los niveles del factor explicar las frecuencias son superiores las de los demás niveles, y ello para todos los niveles del factor explicativo, aunque los factores sean independientes. En caso de asociación, \\(0< \\lambda \\leq 1\\), alcanzándose la unidad en caso de asociación perfecta.Una limitación de \\(\\lambda\\) (además de la anterior y de que exige determinar el factor explicativo y el factor explicar) es su sensibilidad totales marginales desequilibrados; en este caso, toma valores anormalmente bajos. Tal es el caso del ejemplo que nos ocupa, donde \\(\\hat\\lambda=0\\), con factor explicar la opinión, y, sin embargo, los factores son independientes. Y es que, sea cual sea la zona geográfica, las frecuencias de la categoría de opinión “excelente” son siempre las más elevadas.","code":"\nLambda(tabla_ayuso, direction = \"row\")\n#> [1] 0"},{"path":"tablas-contingencia.html","id":"determinación-de-las-fuentes-de-asociación","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.5.3 Determinación de las fuentes de asociación","text":"En el caso de tablas \\(R \\times C\\), el rechazo la hipótesis de independencia indica que cada nivel de uno de los factores esté asociado con uno de los niveles del otro factor, como en las tablas \\(2\\times 2\\). Lo que indica es que al menos uno de los niveles de uno de los factores está asociado con un nivel del otro. Por tanto, puede ser, y así es normalmente, que dicho rechazo se deba que algunos niveles de uno de los factores (incluso sólo uno) están asociados con alguno de los del otro factor. Ya hay dirección de la asociación. Hay fuentes de asociación.Para identificar las fuentes de asociación lo lógico es fijarse en cada celda en las diferencias entre la frecuencia observada y la esperada bajo el supuesto de independencia (tales diferencias juegan el papel de un término de error). Pero su interpretación depende del tamaño de la frecuencia esperada, y por ello se estandarizan, es decir, se ponen en relación la raíz cuadrada de las correspondientes frecuencias esperadas.\nComo para decidir si tales diferencias estandarizadas son significativamente grandes (asociación) o (independencia), se necesita conocer su distribución de probabilidad bajo la hipótesis de independencia, y para ello se dividen por su desviación típica porque de esta manera tienen aproximadamente una distribución N(0;1). Cuando estas diferencias estandarizadas divididas por sus desviaciones típicas se calculan (se estiman) partir de los resultados observados y dispuestos en la tabla, se denominan residuos estandarizados ajustados (o de Haberman), y son los que se utilizan para identificar las fuentes de asociación.Por tanto, la estimación de las diferencias (los residuos) son:\n\\[\\hat{R}_{ij}=n_{ij}-\\hat{E}_{ij},\\]\ny los de las diferencias estandarizadas (los residuos estandarizados) vienen dados por:\\[\\hat{R}_{ij}(est)=\\frac {n_{ij}-\\hat{E}_{ij}}{\\sqrt{\\hat{E}_{ij}}},\\]\nmientras que la siguiente expresión corresponde las estimaciones las dierencias estandarizadas ajustadas, que son otras que los denominados residuos estandarizados ajustados:\\[\\hat{R}_{ij}(est;ajd)=\\frac{\\hat{R}_{ij}(est)}{\\sqrt{\\left(1-\\frac {n_{.}} {N}\\right)\\left(1-\\frac {n_{\\cdot j}} {N}\\right)}}.\\]Habrá una fuente de asociación en cada celda {;j} que verifique: \\(|\\hat{R}_{ij}(est;ajd)|\\geq{k}\\), con \\(k= 2,33; 1,96; 1,64\\) para \\(\\alpha=0,01; 0,05; 0,10\\), respectivamente.En el ejemplo que nos ocupa, los residuos estandarizados ajustados son:Asumiendo \\(\\alpha=0,05\\), las fuentes de asociación son “Madrid municipio-excelente” costa de “buena”, y “Madrid metropolitano-buena” costa de “excelente”.","code":"\nlibrary(questionr)\nchisq.residuals(tabla_ayuso, digits = 2, std = TRUE)\n#>                opinion\n#> zona            n1_nefasta n2_mala n3_buena n4_excelente\n#>   n1_mad_muni         0.79   -1.04    -3.77         2.41\n#>   n2_metropol        -0.51    1.74     2.88        -2.78\n#>   n3_extraradio      -0.45   -0.76     1.59         0.17"},{"path":"tablas-contingencia.html","id":"contrastes-de-independencia-en-tablas-multidimensionales","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"23.6 Contrastes de independencia en tablas multidimensionales","text":"En tablas con más de dos factores (el objetivo aquí es el caso \\(R \\times C \\times M\\), por simplicidad), sólo se puede contrastar la hipótesis de independencia global sino que, en caso de ser rechazada, también se pueden contrastar las hipótesis de \\(()\\) independencia parcial: dos factores están asociados y el tercero es independiente de ellos, e \\((ii)\\) independencia condicional: dos de los factores son independientes para cada nivel del tercero pero están asociados con él.En los tres casos, el estadístico de contraste (contraste aproximado) es \\[\\chi^2=\\sum_{=1}^R \\sum_{j=1}^C \\sum_{m=1}^M \\frac{\\left (N_{ijm}-{E}_{ijm}\\right)^2}{{E}_{ijm})},\\]\ncon los siguientes grados de libertad (g.l.) y \\({E}_{ijm}\\) bajo la correspondiente \\(H_0\\):\nIndependencia global:\\(dl=(R\\times C\\times M)-(R-1)-(C-1)-(M-1)-1\\) y\n\\({E}_{ijm}=\\frac{N_{..}N_{.j.}N_{..m}}{n^2}\\)Independencia parcial:y B asociados entre sí pero independientes de C:\\(g.l.=(R\\times C\\times M)-(R\\times C-1)-(M-1)-1\\) y \\({E}_{ijm}=\\frac{N_{ij.}N_{..m}}{n}\\)y C asociados entre sí pero independientes de B:\\(g.l.=(R\\times C\\times M)-(R\\times M-1)-(C-1)-1\\) y \\({E}_{ijm}=\\frac{N_{.m}N_{.j.}}{n}\\)B y C asociados entre sí pero independientes de :\\(g.l.=(R\\times C\\times M)-(C\\times M-1)-(R-1)-1\\) y \\({E}_{ijm}=\\frac{N_{.jm}N_{..}}{n}\\)Independencia condicionalA y B son independientes entre sí, pero están asociados con C:\\(g.l.=(R\\times C\\times M)-(R\\times M-1)-(C\\times M-1)-1\\) y \\({E}_{ijm}=\\frac{N_{.m}N_{.jm}} {n}\\)y C son independientes entre sí, pero están asociados con B:\\(g.l.=(R\\times C\\times M)-(R\\times C-1)-(M\\times C-1)-1\\) y\n\\({E}_{ijm}=\\frac{N_{ij.}N_{.jm}}{n}\\)B y C son independientes entre sí, pero están asociados con :\\(g.l.=(R\\times C\\times M)-(C\\times R-1)-(M\\times R-1)-1\\) y\n\\({E}_{ij.}=\\frac{N_{.m}N_{.jm}}{n}\\)Para calcular el valor muestral del estadístico de contraste para las diferentes hipótesis, basta con sustituir las \\(N_{ij}\\) por las frecuencias observadas (las \\(n_{ij}\\)) y los totales marginales (\\(N_{.m}\\), \\(N_{.j.}\\), \\(N_{..}\\), etc.) por los totales marginales observados y que figuran en la tabla que surge de los datos en estudio (\\(n_{.m}\\), \\(n_{.j.}\\), \\(n_{..}\\), etc.).También son interesantes las relaciones de segundo orden o superior (por ejemplo, si la asociación entre dos de los factores difiere en dirección y/o intensidad para distintos niveles del tercero), pero se estudian mediante modelos logarítmico lineales.","code":""},{"path":"tablas-contingencia.html","id":"resumen-20","chapter":"Capítulo 23 Análisis de tablas de contingencia","heading":"Resumen","text":"Las tablas de contingencia analizan la relación entre variables categóricas. Su análisis responde preguntas como: los factores involucrados en la tabla, ¿son independientes o están asociados? Si están asociados, ¿qué niveles de dichos factores son los que están asociados?, ¿cuál es la intensidad de dicha asociación? Se aborda ampliamente el caso de tablas bifactoriales y se proponen test exactos y aproximados para el contraste de la hipótesis de independencia (para tres procedimientos de muestreo diferentes) y una selección de medidas de asociación. Finalmente, se hace una breve incursión en el ámbito de las tablas multidimensionales.","code":""},{"path":"cap-arboles.html","id":"cap-arboles","chapter":"Capítulo 24 Árboles de clasificación y regresión","heading":"Capítulo 24 Árboles de clasificación y regresión","text":"Ramón . Carrasco\\(^{}\\) e Itzcóatl Bueno\\(^{b,}\\)\\(^{}\\)Universidad Complutense de Madrid\n\\(^{b}\\)Instituto Nacional de Estadística","code":""},{"path":"cap-arboles.html","id":"intro_dectree","chapter":"Capítulo 24 Árboles de clasificación y regresión","heading":"24.1 Introducción","text":"Los árboles de decisión son modelos que se utilizan principalmente para\nla resolución de problemas de clasificación, en los que hay que predecir\nlas distintas categorías de la variable objetivo o dependiente, aunque\ntambién son aplicables la predicción de valores numéricos de dicha\nvariable objetivo, esto es, como modelos de regresión. De ahí que sean\nconocidos como árboles de clasificación y regresión (CART,\nClassification Regression Trees). Algunos ejemplos de árboles de\ndecisión son:Clasificación: en la medida que\nla variable objetivo debe ser categórica se podrían usar por ejemplo\npara tomar la decisión de qué empleados deberían de promocionar\n(variable con dos categorías: sí promocionar o promocionar) en\nbase sus méritos, capacidades, edad, etc. Otro ejemplo podría ser\nsu uso para decidir sí se juega o un partido de tenis en base \nla climatología prevista. Este ejemplo se muestra gráficamente en la\nFig. 24.1. En este último caso, el algoritmo que\nse utilice indicará la decisión tomar en base los registros\nclimatológicos de los partidos que ya se hayan jugado. Así, si un\ndeterminado día se quiere jugar al tenis, se deberán tomar como\nvariables de entrada las previsiones de Tipo de día (soleado,\nnublado o lluvioso), la fuerza del Viento y la Humedad. En caso de\nser un día nublado, el algoritmo sugerirá que se juegue. En caso de\nser soleado, comprobará el nivel de Humedad y, si es muy elevada,\nrecomendará que se juegue el partido. Lo mismo pasará si la\nprevisión es de lluvia pero la fuerza del Viento prevista es lo\nsuficientemente intensa como para impedir el normal desarrollo del\npartido.\nFigura 24.1: Ejemplo de árbol de decisión.\nRegresión: siguiendo con el ejemplo\ndel partido de tenis, también se puede aplicar un árbol de decisión\npara determinar cuántas horas jugar de acuerdo las condiciones\nclimatológicas. En la Fig. 24.1 se sustituirían la predicciones\ndicotómicas SI/por valores numéricos, como se muestra en la Fig. 24.2. Por ejemplo, el algoritmo puede sugerir jugar 5 horas si el día está soleado pero la Humedad\nes del 30% de vapor de agua por \\(m^3\\), y 3,5 horas si está soleado\npero la Humedad es del 80%. También puede decidir que si el día está\nnublado se jueguen 4 horas. O en caso de lluvia, podría decidir que\nel partido dure 0,75 horas si la fuerza del Viento es de 62km/h y\n1,15 horas si es de 27km/h.\nFigura 24.2: Ejemplo de árbol de regresión\n Como se ha comentado, CART es un término\ngenérico para describir este tipo de algoritmos de árbol y también un\nnombre específico para el algoritmo original de\n(Breiman et al. 1984) de construcción de árboles de clasificación\ny regresión. Sin embargo, existen otros como el ID3 (Induction Decission\nTrees), o el C4.5 que está basado en el ID3. En la Tabla\n24.1 se muestra una pequeña comparativa de estos tres\nalgoritmos:Tabla 24.1:  Características de los principales algoritmos de\nárboles de decisión.Los árboles de decisión tienen múltiples ventajas. Entre ellas destacan:Son fáciles de entender e interpretar. Su visualización clara\npermite interpretar la salida del modelo y entender su proceso como\nun conjunto de condicionantes.El mismo algoritmo incorporado en R (CART) es válido tanto para\nproblemas de clasificación como de regresión y, por tanto, la\nvariable objetivo puede ser continua o categórica. Respecto al resto\nde variables de entrada, las independientes, comentar que puede ser\ntanto categóricas como numéricas. Al contrario que ocurre con otros\nalgoritmos, este último tipo de variables requieren la\nestandarización, puesto que se basa en reglas y en el cálculo de\ndistancias entre observaciones.Tratan mejor que otros algoritmos el problema de la linealidad.Respecto los datos, hacen un tratamiento automático de valores\nausentes (en la mayoría de los árboles de clasificación) y se ven\nafectados con las observaciones atípicas.Sin embargo, también tienen ciertas desventajas:Son inestables ya que la inclusión de una nueva observación en la\nfase de entrenamiento obliga reconstruirlo, pudiendo modificar la\nestructura del árbol final.son recomendables en caso de grandes conjuntos de datos, puesto\nque el modelo entrenado puede estar sobreajustado. Este sobreajuste\nes el principal problema de los árboles de decisión, ya que modelos\ndemasiados complejos pueden ajustar muy bien los datos observados,\npero también pueden cometer muchos errores en la fase de predicción.\nCuando esta circunstancia se da, el modelo ha aprendido los datos de\nentrenamiento pero la generalidad del problema que es lo que\nnormalmente se pretende. El sobreajuste da lugar también una\nvarianza elevada.","code":""},{"path":"cap-arboles.html","id":"procedimiento-con-r-la-función-rpart","chapter":"Capítulo 24 Árboles de clasificación y regresión","heading":"24.2 Procedimiento con R: la función rpart()","text":"En el paquete rpart de R se encuentra la función rpart() que se\nutiliza para entrenar un árbol de decisión:formula: Refleja la relación entre la variable dependiente \\(Y\\) y\nlos predictores tal que \\(Y \\sim X_1 + ... + X_p\\).data: Conjunto de datos con el que entrenar el árbol de acuerdo \nla fórmula indicada.","code":"\nrpart(formula, data, ...)"},{"path":"cap-arboles.html","id":"árboles-de-clasificación","chapter":"Capítulo 24 Árboles de clasificación y regresión","heading":"24.3 Árboles de clasificación","text":"Formalmente, un árbol de decisión es un grafo acíclico (un grafo sin\nciclos, siendo un ciclo un circuito completo) que se inicia en un nodo\nraíz, el cual se divide en ramas,\ntambién conocidas como aristas. De las ramas salen las\nhojas, también denominadas nodos Estos\nnodos pueden ser nodos finales o puntos de\ndecisión (si de ellos salen nuevas ramas\ncon nuevos nodos) o (de ellos salen nuevas ramas con nuevas hojas o\nnodos) y así hasta que todos los nodos sean puntos de decisión. En el\nejemplo de la Fig. 24.1 el nodo raíz es la caja Tipo\nde día. Las ramas o aristas, son sus tres niveles o categorías:\nSoleado, Nublado o Lluvia. Cada una de estas ramas conecta con una\nnueva hoja o nodo: Humedad o Viento en los casos de soleado o\nlluvia, respectivamente. Sin embargo, en ese ejemplo, Nublado\nrepresenta un nodo terminal, puesto que, llegado ese punto, la salida\nque proporcionaría el árbol es “Jugar al tenis”. Este proceso se\nrepite utilizando el conjunto de datos disponible en cada hoja,\ngenerándose una clasificación final cuando una hoja tenga ramas\nnuevas, en cuyo caso recibe la denominación de nodo final. El objetivo\nes que el árbol sea lo más general y pequeño posible. Esto se consigue\nseleccionando, en cada paso, la variable que optimice la división de los\ndatos en conjuntos homogéneos, de tal forma que se prediga mejor la\nclase objetivo.","code":""},{"path":"cap-arboles.html","id":"cómo-se-va-formando-el-árbol-de-clasificación","chapter":"Capítulo 24 Árboles de clasificación y regresión","heading":"24.3.1 ¿Cómo se va formando el árbol de clasificación? ","text":"Como ya se ha mencionado, en la construcción de un árbol de decisión se\nva dividiendo en nuevas ramas de forma recursiva, es decir, cada división\nestá condicionada por las anteriores. El objetivo en cada hoja, es\nencontrar la variable más adecuada para dividir los datos de ese nodo en\ndos nuevas hojas, de tal forma que el error global entre la clase\nobservada y la predicha por el árbol se minimice. Para la construcción\nde árboles de clasificación, el algoritmo CART utiliza la medida de\nimpureza de Gini para generar las particiones, mientras que los\nalgoritmos ID3 y C4.5 están basados en las de entropía.","code":""},{"path":"cap-arboles.html","id":"impureza-de-gini","chapter":"Capítulo 24 Árboles de clasificación y regresión","heading":"24.3.1.1 Impureza de Gini","text":"La Impureza de Gini, utilizada por el algoritmo CART, es una medida\nde la frecuencia con la que una observación elegida aleatoriamente del\nconjunto se asignaría la clase errónea si se etiqueta al azar en una\nde las clases que se consideran. Formalmente, sea \\(X\\) un conjunto de\ndatos con \\(\\kappa\\) clases, y sea \\(p_i\\) la probabilidad de que una\nobservación pertenezca la clase \\(\\). La Impureza de Gini para \\(X\\) se\ndefine como:Como se ha comentado, la hora de construir el árbol se selecciona el\natributo con la menor impureza de Gini para dividir el conjunto de datos\nen el nodo en dos. Si un conjunto de datos \\(X\\) se divide en un atributo\n\\(\\varphi\\) en dos subconjuntos \\(X_1\\) y \\(X_2\\) con tamaños \\(n_1\\) y \\(n_2\\),\nrespectivamente, la impureza ponderada de Gini se define como:En el ejemplo de la Fig. 24.1 considérese la\nsiguiente situación:Tabla 24.2:  Datos para decidir si se juega el partidoPara el Tipo de día, los datos se agruparían como muestra la Tabla\n24.3, permitiendo el cálculo de la impureza de\nGini para cada una de sus categorías.Tabla 24.3:  Días que se juega o de acuerdo al Tipo de\ndía\\[\\begin{equation*}\nGini(Soleado) = 1 - \\Bigl(\\frac{2}{6}\\Bigr)^{2} - \\Bigl(\\frac{4}{6}\\Bigr)^{2} = 0,45\n\\end{equation*}\\]\n\\[\\begin{equation*}\nGini(Nublado) = 1 - \\Bigl(\\frac{4}{4}\\Bigr)^{2} = 0\n\\end{equation*}\\]\n\\[\\begin{equation*}\nGini(Lluvia) = 1 - \\Bigl(\\frac{4}{5}\\Bigr)^{2} - \\Bigl(\\frac{1}{5}\\Bigr)^{2} = 0,32\n\\end{equation*}\\]Ahora, se calcula la suma ponderada de la impureza de Gini para la\nvariable Tipo de día:Del mismo modo, se puede calcular la impureza de Gini para el resto de\nvariables. La Tabla 24.4 y la Tabla\n24.5 presentan los resultados para Humedad y\nViento, respectivamente.Tabla 24.4:  Impureza de Gini para las categorías de HumedadTabla 24.5:  Impureza de Gini para las categorías de VientoEn la Tabla 24.6 se puede ver que la impureza de\nGini para las tres variables incluidas en el ejemplo. La variable con la\nmenor impureza de Gini, el Tipo de día, es la elegida para ser el nodo\nraíz del árbol de clasificación.Tabla 24.6:  Impureza de Gini para las variables de\nentradaAl entrenar un árbol de decisión, se repite este proceso, y la hora de\ndividir cada nodo, se elige el atributo que proporcione el menor\n\\(Gini_{\\varphi}(X)\\).Para obtener la ganancia de información para una variable, las impurezas\nponderadas de los nodos hijos se restan de la impureza del nodo padre.\nLa ganancia de Gini para la variable X, \\(\\Delta Gini()\\), se calcula\nasí:Siguiendo el ejemplo del árbol de clasificación, para saber si se puede\njugar al tenis o , se tendría que obtener la impureza de Gini para el\nnodo Humedad o el nodo Viento. Repitiendo el proceso anteriormente\nmostrado, dado que el Tipo de día sea soleado, se obtienen los\nresultados de la Tabla 24.7.Tabla 24.7:  Impureza de Gini para las variables en días\nsoleadosEntonces, la ganancia de Gini para cada variable será:Puede observarse que la ganancia de información al dividir por Humedad\nes mayor que al hacerlo por Viento, por lo que el árbol se dividirá\nrespecto la Humedad, como se observó en la Fig.\n24.1.","code":""},{"path":"cap-arboles.html","id":"entropía","chapter":"Capítulo 24 Árboles de clasificación y regresión","heading":"24.3.1.2 Entropía ","text":"La entropía es un concepto matemático que mide la incertidumbre de una\nfuente de información, es decir, la varianza en los datos entre\ndiferentes clases. Para cada nodo y su partición, la entropía se calcula\ncomo:donde \\(p_1\\) y \\(p_2\\) representan la probabilidad de pertenecer cada una\nde las clases en ese nodo. En teoría de la información, la base\nlogarítmica varía dependiendo de la aplicación, y con ella varía la\nunidad de medida. En este caso, la ganancia de información se obtiene\ncomo:donde \\(E_\\varkappa\\) representa la entropía en el nodo padre, mientras\nque \\(E_{\\varkappa+1}\\) es la entropía en el nodo que resulta de dividir\nel nodo padre. Entonces, siguiendo el ejemplo basado en los datos de la\nTabla 24.3 se tendría que la entropía en origen\nes:Si se obtiene la entropía para cada variable, se determinará el nodo\nraíz para aquel que aporte una mayor ganancia de información. En el caso\nde la variable Tipo de día se calcula:\\[\\begin{equation*}\nE_{Soleado} = -\\frac{2}{6}\\log_2 \\Bigl(\\frac{2}{6}\\Bigr) - \\frac{4}{6}\\log_2 \\Bigl(\\frac{4}{6}\\Bigr) = 0,9183\n\\end{equation*}\\]\n\\[\\begin{equation*}\nE_{Nublado} = -\\frac{4}{4}\\log_2 \\Bigl(\\frac{4}{4}\\Bigr) - \\frac{0}{4}\\log_2 \\Bigl(\\frac{0}{4}\\Bigr) = 0\n\\end{equation*}\\]\n\\[\\begin{equation*}\nE_{Lluvia} = -\\frac{4}{5}\\log_2 \\Bigl(\\frac{4}{5}\\Bigr) - \\frac{1}{5}\\log_2 \\Bigl(\\frac{1}{5}\\Bigr) = 0,7219\n\\end{equation*}\\]Y por tanto:Repitiendo el mismo procedimiento con las variables Viento y Humedad\nse puede comprobar que \\(E(Viento) = 0,893\\) y \\(E(Humedad) = 0,809\\). \npartir de esto se puede obtener la ganancia de información como:\\[\\begin{equation*}\nIG_{\\text{Tipo de día}} = E - E_{\\text{Tipo de día}} = 0,918 - 0,608 = 0,31\n\\end{equation*}\\]\n\\[\\begin{equation*}\nIG_{Viento} = E - E_{Viento} = 0,918 - 0,893 = 0,025\n\\end{equation*}\\]\n\\[\\begin{equation*}\nIG_{Humedad} = E - E_{Humedad} = 0,918 - 0,809 = 0,109\n\\end{equation*}\\]Se puede comprobar que la disminución de la aleatoriedad, o la ganancia\nde información, es mayor para la variable Tipo de día y por tanto se\nelige para ser el nodo raíz. Repitiendo este proceso se va construyendo\nel árbol hasta alcanzar los nodos terminales.","code":""},{"path":"cap-arboles.html","id":"sobreajuste","chapter":"Capítulo 24 Árboles de clasificación y regresión","heading":"24.3.2 Sobreajuste ","text":"Ya se ha comentado en la Sec. @ref(intro_dectree) que una de las\nprincipales desventajas de los árboles de decisión es su propensión \nsobreajustar el modelo al conjunto de datos de entrenamiento y, por\ntanto, hay que prestar especial atención la complejidad del modelo.\nBasándose en las observaciones utilizadas en la fase de entrenamiento,\nun árbol de decisión puede extraer los patrones presentes en el conjunto\nde observaciones de entrenamiento y ser muy preciso en el ajuste de\ndichas observaciones. Sin embargo, puede ocurrir que el árbol resultante\nsea capaz de clasificar correctamente ni el conjunto de validación ni\nnuevas observaciones. Esta circunstancia puede ocurrir porque haya\npatrones observados en los datos de entrenamiento que el modelo es\ncapaz de detectar, o porque la división de los datos entre entrenamiento\ny validación se realizó correctamente siendo los datos de\nentrenamiento representativos del conjunto de datos completo.\nIntentando que el árbol entrenado tenga la capacidad de aprender\npatrones muy complejos, se puede producir este sobreajuste materializado\ncon árboles muy profundos. La forma de evitar el sobreajuste es\ncontrolar el crecimiento del árbol para evitar que se vuelva\nexcesivamente complejo.","code":""},{"path":"cap-arboles.html","id":"cuánto-debe-crecer-un-árbol-de-clasificación","chapter":"Capítulo 24 Árboles de clasificación y regresión","heading":"24.3.3 ¿Cuánto debe crecer un árbol de clasificación?","text":"En cada paso de construcción del árbol se determina la variable óptima\npara realizar la división de las observaciones de un nodo padre en sus\nnodos hijos. La pregunta es: ¿cuándo se detiene?, ¿cuál es el criterio\nde parada? Por ejemplo, se puede utilizar como criterio de parada que el\nárbol alcance un tamaño o profundidad determinado, para que sea\nexcesivamente complejo y así tengan lugar las consecuencias derivadas\ndel sobreajuste.En consecuencia, se debe llegar un equilibrio entre la profundidad y\ncomplejidad del árbol para optimizar la predicción de\nfuturas observaciones. Este equilibrio se puede lograr siguiendo alguno\nde los siguientes enfoques: la parada temprana o la poda.","code":""},{"path":"cap-arboles.html","id":"la-parada-temprana","chapter":"Capítulo 24 Árboles de clasificación y regresión","heading":"24.3.3.1 La parada temprana","text":"La parada temprana restringe el crecimiento del árbol, tanto de\nclasificación como de regresión, de forma explícita. Existen distintas\nmaneras de establecer esta restricción al árbol, pero dos de las\ntécnicas más populares son las de restringir la profundidad un cierto\nnivel o la de establecer un número mínimo de observaciones permitidas en\nun nodo terminal. En el primer caso, el árbol deja de dividirse al\nllegar cierta profundidad. Así, cuanto menos profundo sea el árbol,\nmenos variación habrá en las predicciones que proporcione. Sin embargo,\nexiste el riesgo de introducir mucho sesgo al modelo al ser capaz de\ncaptar interacciones y patrones complejos en los datos. El segundo\nenfoque lo que provoca es que se dividan nodos intermedios con pocas\nobservaciones. En el caso extremo, si se permite que un nodo terminal\nsólo contuviese una observación esta actuaría como predicción. De este\nmodo, los resultados probablemente serían generalizables y tendrían\nmucha variabilidad. En el otro extremo, si se exigen un gran número de\nobservaciones en el nodo terminal se reduce el número de divisiones y,\npor lo tanto, se reduce la varianza.","code":""},{"path":"cap-arboles.html","id":"la-poda","chapter":"Capítulo 24 Árboles de clasificación y regresión","heading":"24.3.3.2 La poda","text":"El otro enfoque es el de la poda que consiste en construir un árbol muy\nprofundo y complejo y después podarlo para encontrar el subárbol óptimo.\nEste subárbol se obtiene utilizando un hiperparámetro de complejidad\n\\((\\zeta)\\) que penaliza la función objetivo de la partición por el número\nde nodos terminales del árbol \\((\\tau)\\), es decir, se busca minimizar:Donde \\(R(\\tau)\\) es el error total de entrenamiento de los nodos,\n\\(|\\tau|\\) es el número total de nodos y \\(\\zeta\\) es el hiperparámetro de\ncomplejidad. medida que \\(\\zeta\\) aumenta, más ramas del árbol son\npodadas. Mientras que valores más bajos, los modelos producidos son\nmás complejos y en consecuencia más grandes. En conclusión, medida que\nun árbol crece, el error de entrenamiento debe tener una reducción mayor\nque la penalización por la complejidad.","code":""},{"path":"cap-arboles.html","id":"ejemplo-árbol-de-clasificación-para-determinar-la-intención-de-compra","chapter":"Capítulo 24 Árboles de clasificación y regresión","heading":"24.3.4 Ejemplo: Árbol de clasificación para determinar la intención de compra","text":"continuación se describe el caso que se va resolver mediante modelos\nde clasificación tanto en este como en los siguientes capítulos. Existen\ndiversas aserciones para definir Comercio Electrónico (CE). Entre ellas,\nla Organización para la Cooperación y el Desarrollo Económico (OCDE) lo\ndefine como el proceso de compra, venta o intercambio de bienes,\nservicios e información través de redes de comunicación, comúnmente\nInternet. La clasificación más básica del CE se hace en base al tipo de\nentes que se relacionan: empresas (businesses, B), consumidores\n(consumers, C) y entes públicos (governments, G). De esta forma, una\nempresa de CE convencional suele ser B2B si vende otras empresas, B2G\nsi su relación comercial es con administraciones o B2C si vende consumidores finales.En este caso, se puede considerar que la empresa “Beauty eSheep” lleva \ncabo un CE de tipo B2C. Su producto estrella es una crema hidratante\nunisex, denominada internamente como “Crema Luxury”, con mucho éxito\nentre su clientela. partir de este producto inicial, la empresa ha ido\nofreciendo un catálogo de productos tanto de tanto de belleza como de\nbienestar y salud.Hace tiempo la empresa instauró una estrategia relacional, centrada en\nel cliente, de tal manera que ha ido recabando diversos datos sobre los\nmismos, incluidas las distintas compras que han realizado.Basándose en los datos recopilados para cada cliente, la empresa quiere\nrealizar una campaña para impulsar la venta de tensiómetros digitales.\nLa empresa tiene acceso un stock muy flexible en fechas de envío de\nestos productos y el precio de los tensiómetros es muy bueno, por lo que\nse espera una buena rentabilidad en su venta.Por tanto, en este proyecto hay que identificar el público objetivo\nsusceptible de comprar dicho producto para ofrecérselo través de la\nplataforma de CE de la compañía, SMS y/o webmail durante el periodo que\ndura la campaña.La tabla con los datos integrados nivel de cliente, incluyendo el\nconsumo de los distintos productos de la empresa, es dp_ENTR,\nincluida en el paquete CDR, y que se resume en la Tabla\n24.8. Este ejemplo se va replicar en el resto de\ncapítulos de machine learning supervisado para clasificación.Tabla 24.8:  Descripción de las variables del conjunto de datos\ndp_entr.Se construye un árbol de clasificación\nutilizando el conjunto de entrenamiento, como se ha comentado, sin\ntransformar (en su escala original) mediante el algoritmo CART\nimplementado en el paquete rpart con Árboles de Regresión y Partición\nRecursiva (Recursive Partitioning Regression Trees, RPART) que se\npuede usar tanto para regresión como para clasificación.En primer lugar, se carga la librería necesaria para entrenar el modelo,\nasí como los datos de compras de los clientes. En este caso se usa el\nmétodo de remuestreo de validación cruzada con 10 folds, visto en el\nCap. 9. continuación, se determina la semilla\naleatoria para que los resultados sean replicables, y se entrena el\nmodelo.\nFigura 24.3: Resultados del modelo durante la validación cruzada.\nLos resultados de validación cruzada quedan recogidos en los boxplot,\npor lo que se puede ver los valores entre los que oscilan las\nprincipales medidas en los 10 folds del proceso de validación. Estas\nmedidas (ROC, sensibilidad y especificidad) se definieron en el Cap.\n9, y en el caso de árboles de clasificación se\nutilizan para medir la precisión del modelo. continuación se muestra\nel árbol generado. Se puede observar que este árbol es muy sencillo, y\npor tanto es fácil obtener su interpretación. En primer lugar decide si\nun cliente que compra el smartchwatch fitness comprará el nuevo\nproducto. En caso de comprar el smartchwatch fitness (\nind_pro15S=1), pero sí compra la depiladora eléctrica (Yes \nind_pro12S=1) sí comprará el tensiómetro digital. Si compra ninguno\nde esos dos productos comprará el nuevo producto.\nFigura 24.4: Árbol de clasificación sin ajuste automático de hiperparámetros.\nEste modelo se puede mejorar ajustando automáticamente\nel hiperparámetro\nincluido en rpart para el entrenamiento de árboles de decisión. Los\nhiperparámetros son los valores utilizadas durante el proceso de\nentrenamiento en la configuración del modelo. Por consiguiente, primero\nes necesario conocer el hiperparámetro optimizar en el algoritmo\nimplementado en R que estemos usando. Esto se consigue mediante la\nsiguiente instrucción incluida en el paquete caret:El hiperparámetro optimizar es la complejidad\ndel árbol, cp, que es un hiperparámetro que se aplica en la fase de\nparada durante la construcción del árbol. Según se ha comentado, esta\nfase tiene como función principal evitar desarrollar divisiones que \nvalgan la pena. Se puede entender cp como la mejora mínima necesaria\nen cada nodo del modelo. Es necesario definir los valores de cp que se\nquieren evaluar con el objetivo de obtener su valor óptimo.De forma automática se construyen diversos árboles para cada uno de los\nvalores explicitados del parámetro cp. Para cada uno de esos árboles\nse obtienen las correspondientes métricas de precisión: el área bajo la\ncurva (denotada como ROC, por las siglas en inglés de Receiver Operating\nCharacteristic), sensibilidad (Sens) y especificidad (Spec), todas ellas\ndefinidas en el Cap. 9. El valor ROC es el utilizado\npara la elección del valor óptimo de cp, por lo que se determina que\nfinalmente el óptimo es \\(cp=0,01\\) al maximizar el valor ROC alcanzando\nun 89,6%. Por tanto, ajustando el hiperparámetro se ha aumentado la\nprecisión del modelo en casi un 8% respecto al 81,7% que tenía el modelo\nsin ajustar automáticamente el valor de cp.En la Fig. 24.5 se puede ver el\nrendimiento de cada una de las métricas del árbol entrenado utilizando\nvalidación cruzada. Dicha figura se obtiene con la siguiente instrucción:\nFigura 24.5: Resultados del modelo con ajuste automático durante la validación cruzada\nEn la Fig. 24.6 se muestra el árbol generado.\nDicha visualización se ha obtenido con el siguiente código:\nFigura 24.6: Árbol de clasificación con ajuste automático.\nCon el objetivo de aumentar la generalidad del árbol y facilitar su\ninterpretación, se procede reducir su tamaño podándolo. Para ello se\nestablece que un nodo terminal tenga como mínimo 50 observaciones, dando\nlugar al árbol que se muestra en la Fig. 24.7.\nFigura 24.7: Árbol de clasificación con ajuste automático y podado.\nEl árbol ha reducido el número de nodos terminales, en tres de ellos el\nárbol predice que un cliente comprará el nuevo producto si:Compra el smartwatch fitness (ind_pro15 = S - Yes) y la\ndepiladora eléctrica (ind_pro12 = S - Yes).Compra el smartwatch fitness (ind_pro15 = S - Yes) y la\ndepiladora eléctrica (ind_pro12 = S - Yes).Compra el smartwatch fitness (ind_pro15 = S - Yes) y el\nestimulador muscular (ind_pro17 = S - Yes), pero la depiladora\neléctrica (ind_pro12 = S - ).Compra el smartwatch fitness (ind_pro15 = S - Yes) y el\nestimulador muscular (ind_pro17 = S - Yes), pero la depiladora\neléctrica (ind_pro12 = S - ).compra el smartwatch fitness (ind_pro15 = S - ), pero si la\ndepiladora eléctrica (ind_pro12 = S - Yes).compra el smartwatch fitness (ind_pro15 = S - ), pero si la\ndepiladora eléctrica (ind_pro12 = S - Yes).Sin embargo, dos nodos terminales predicen que el cliente compra el\nnuevo producto si:Compra el smartwatch fitness (ind_pro15 = S - Yes), pero la\ndepiladora eléctrica (ind_pro12 = S - ) ni el estimulador\nmuscular (ind_pro17 = S - ).Compra el smartwatch fitness (ind_pro15 = S - Yes), pero la\ndepiladora eléctrica (ind_pro12 = S - ) ni el estimulador\nmuscular (ind_pro17 = S - ).compra el smartwatch fitness (ind_pro15 = S - ) ni la\ndepiladora eléctrica (ind_pro12 = S - ).compra el smartwatch fitness (ind_pro15 = S - ) ni la\ndepiladora eléctrica (ind_pro12 = S - ).","code":"library(\"CDR\")\nlibrary(\"reshape\")\nlibrary(\"caret\")\nlibrary(\"rpart\")\nlibrary(\"rpart.plot\")\nlibrary(\"ggplot2\")\n\ndata(\"dp_entr\")\nhead(dp_entr)\n\n    ind_pro11 ind_pro12 ind_pro14 ind_pro15 ind_pro16 ind_pro17 importe_pro11\n1           S         N         S         S         S         N           157\n497         N         N         S         N         S         N             0\n265         N         N         S         S         S         S             0\n534         N         S         S         N         N         N             0\n415         N         S         S         N         S         N             0\n298         S         N         S         N         N         N           115\n    importe_pro12 importe_pro14 importe_pro15 importe_pro16 importe_pro17 edad\n1               0            40           200           180             0   49\n497             0           240             0           180             0   38\n265             0           425           200           180           300   61\n534           120            60             0             0             0   47\n415           120           133             0           180             0   34\n298             0           220             0             0             0   43\n    tamano_fam anos_exp ingresos_ano des_nivel_edu CLS_PRO_pro13\n1            4       24        30000         MEDIO             S\n497          2       12        53000         MEDIO             N\n265          4       37       172000        BASICO             S\n534          3       21        38000         MEDIO             N\n415          1       10        38000        BASICO             N\n298          2       18        60000          ALTO             N\n\ntrControl <- trainControl(\n  method = \"cv\",\n  number = 10,\n  classProbs = TRUE,\n  summaryFunction = twoClassSummary\n)\n\n# se fija una semilla aleatoria\nset.seed(101)\n\n# se entrena el modelo\nmodel <- train(CLS_PRO_pro13 ~ .,  # . equivale a incluir todas las variables\n             data=dp_entr,\n             method=\"rpart\",\n             metric=\"ROC\",\n             trControl=trControl)model\n\nCART\n\n558 samples\n 17 predictor\n  2 classes: 'S', 'N'\n\nNo pre-processing\nResampling: Cross-Validated (10 fold)\nSummary of sample sizes: 502, 502, 502, 503, 503, 502, ...\nResampling results across tuning parameters:\n\n  cp          ROC        Sens       Spec\n  0.05017921  0.8172123  0.9214286  0.7026455\n  0.10394265  0.7559406  0.8386243  0.6914021\n  0.51971326  0.6347222  0.8564815  0.4129630\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.05017921.\nggplot(melt(model$resample[,-4]), aes(x = variable, y = value, fill=variable)) +\n  geom_boxplot(show.legend=FALSE) +\n  xlab(NULL) + ylab(NULL)\n# Gráfico del árbol obtenido\nrpart.plot(model$finalModel)modelLookup(\"rpart\")\n\n  model parameter                label forReg forClass probModel\n1 rpart        cp Complexity Parameter   TRUE     TRUE      TRUE\n# Se especifica un rango de valores típicos para el hiperparámetro\ntuneGrid <- expand.grid(cp = seq(0.01,0.05,0.01))\n# se entrena el modelo\nset.seed(101)\n\nmodel <- train(CLS_PRO_pro13 ~ .,\n             data=dp_entr,\n             method=\"rpart\",\n             metric=\"ROC\",\n             trControl=trControl,\n             tuneGrid=tuneGrid)model\n\nCART\n\n558 samples\n 17 predictor\n  2 classes: 'S', 'N'\n\nNo pre-processing\nResampling: Cross-Validated (10 fold)\nSummary of sample sizes: 502, 502, 502, 503, 503, 502, ...\nResampling results across tuning parameters:\n\n  cp    ROC        Sens       Spec\n  0.01  0.8962254  0.8678571  0.8167989\n  0.02  0.8663454  0.9000000  0.7667989\n  0.03  0.8458097  0.9392857  0.7310847\n  0.04  0.8449381  0.9214286  0.7383598\n  0.05  0.8172123  0.9214286  0.7026455\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.01.\nggplot(melt(model$resample[,-4]), aes(x = variable, y = value, fill=variable)) +\n   geom_violin(show.legend=FALSE) +\n   xlab(NULL) + \n   ylab(NULL)\n# Gráfico del árbol obtenido\nrpart.plot(model$finalModel)\nset.seed(101)\nprunedtree <- rpart(CLS_PRO_pro13 ~ ., data=dp_entr,\n                    cp= 0.01, control = rpart.control(minbucket = 50))\n\nrpart.plot(prunedtree)"},{"path":"cap-arboles.html","id":"árboles-de-regresión","chapter":"Capítulo 24 Árboles de clasificación y regresión","heading":"24.4 Árboles de regresión ","text":"Como se ha comentado, los árboles de decisión también pueden ser usados\npara resolver problemas de regresión. En este caso, la idea es que la\npredicción dada en cada hoja sea un valor numérico en lugar de un valor\nde una categoría. En la Tabla 24.9 se muestran los\ndatos para un problema de regresión equivalente al presentado en\nsecciones anteriores para clasificación. Como ya se ha mencionado, la\nvariable objetivo (Horas jugadas) ahora es continua en lugar de\ncategórica, como ocurría en el ejemplo anterior con la variable\nDecisión.Tabla 24.9:  Datos de Horas jugadas dada la climatología del díaSe pueden calcular medidas descriptivas de la variable respuesta, Horas\njugadas, como la media, varianza, desviación típica y coeficiente de\nvariación siendo estas:","code":""},{"path":"cap-arboles.html","id":"cómo-se-va-formando-el-árbol-de-regresión","chapter":"Capítulo 24 Árboles de clasificación y regresión","heading":"24.4.1 ¿Cómo se va formando el árbol de regresión? ","text":"Mientras que en los árboles de clasificación se utilizaba la entropía o\nla impureza de Gini para medir la homogeneidad de un nodo, en los\nárboles de regresión se utiliza como métrica la desviación típica\n\\((\\sigma)\\). Por tanto, cuando se selecciona una variable para hacer la\ndivisión, se calcula la desviación típica para cada una de las ramas, y\nse obtiene una media ponderada en función del número de elementos de\ncada una de ellas. Esta media ponderada se calcula del siguiente\nmodo:Donde X es la variable de la cual se quiere obtener la desviación típica\ny \\(r\\) son las ramas que crecen desde este nodo. Para llevar cabo la\ndivisión, en primer lugar se debe calcular para cada nodo su desviación\ntípica. continuación, se seleccionan posibles variables para hacer la\ndivisión y se obtiene su desviación típica. Para cada una de estas\nvariables se calcula el decremento de la desviación, y se selecciona\naquel que introduzca la mayor reducción.Por ejemplo, para los datos mostrados en la Tabla\n24.9, la desviación típica es 0,50 Horas jugadas, como\nse calculó en la ecuación (24.5), y el árbol se construiría\ncomo se muestra continuación. En primer lugar, se selecciona Tipo de\ndía, Humedad y Viento como candidatos nodo raíz y se obtiene su\ndesviación típica tal que:Tabla 24.10:  Desviación típica en las ramas de la variable Tipo\nde díaTabla 24.11:  Desviación típica en las ramas de la variable\nHumedadTabla 24.12:  Desviación típica en las ramas de la variable\nVientoA partir de las desviaciones típicas en las ramas de cada variable, se\npuede obtener la desviación típica de cada variable de acuerdo la\necuación (24.7). Además, se calcula la reducción de\ndesviación típica como la diferencia entre la desviación de la variable\nrespuesta y la desviación si se divide el conjunto de datos en base \nalguna de las variables. Tanto la desviación típica de cada variable\ncomo el decremento en la desviación que producen se muestran en la Tabla\n24.13.Tabla 24.13:  Desviación típica y decremento de desviación de cada\nvariableDado que la variable Tipo de día es la que produce una mayor reducción\nen la desviación típica, resulta elegida como nodo raíz. El árbol\nseguiría creciendo repitiendo este proceso. Por ejemplo, se muestra cuál\nsería la siguiente división desde la rama soleado que crece desde nodo\nTipo de díaTabla 24.14:  Desviación típica en las ramas de la variable\nHumedad en días soleadosTabla 24.15:  Desviación típica en las ramas de la variable\nVientoEn la Tabla 24.16 se muestra la desviación típica para\ncada variable así como la reducción de desviación que produce. Por\ntanto, la siguiente división se realizaría con la variable Humedad.Tabla 24.16:  Desviación típica y decremento de desviación de cada\nvariable en la rama soleado","code":""},{"path":"cap-arboles.html","id":"cuánto-debe-crecer-el-árbol-de-regresión","chapter":"Capítulo 24 Árboles de clasificación y regresión","heading":"24.4.2 ¿Cuánto debe crecer el árbol de regresión? ","text":"Como ocurría en los árboles de clasificación, es necesario establecer\nreglas que pongan fin al proceso de crecimiento del árbol. Además de los\ncriterios de parada que se utilizan en árboles de clasificación (número\nde elementos mínimos en un nodo y nivel máximo del árbol), en árboles de\nregresión se detiene su crecimiento estableciendo un threshold (umbral\nde decisión) sobre el coeficiente de variación del nodo. En el ejemplo\nexpuesto sobre Horas jugadas, se puede ver qué nodos podrían seguir\ncreciendo si se establece que el árbol continúe creciendo en nodos con\nun coeficiente de variación de un 15% o más, y que tenga al menos 5\nobservaciones en el nodo.Tabla 24.17:  Medidas para decidir si el árbol sigue creciendoEn este ejemplo, el árbol seguiría creciendo por la rama Lluvia donde\nhabría que seleccionar la siguiente variable de división. En el resto de\nramas, se supera el número mínimo establecido de observaciones en el\nnodo, y en ocasiones tampoco se alcanza el coeficiente de variación\nmínimo. Por otra parte, en los árboles de regresión la poda se lleva \ncabo del mismo modo que para árboles de clasificación. En la ecuación\n(24.2) se mediría el error de entrenamiento través de la suma\nde los cuadrados de los errores (en inglés Sum Squared Estimate \nErrors, SSE), es decir:","code":""},{"path":"cap-arboles.html","id":"árbol-de-regresión-para-estimar-el-número-de-días-de-hospitalización","chapter":"Capítulo 24 Árboles de clasificación y regresión","heading":"24.4.3 Árbol de regresión para estimar el número de días de hospitalización","text":"En este ejemplo se utilizan los datos cleveland, contenidos en el\npaquete CDR, y que han sido utilizados en el Cap. 16 para\nestimar la variable dhosp. El conjunto de datos contiene información\nsobre pacientes que llegan un hospital con dolor de pecho y de los\ncuales se han recogido distintas características. Se pretende predecir\nel número de días de hospitalización que necesitará un paciente en base\nal resto de características observadas: si\nel paciente está diagnosticado de accidente coronario, su edad, su sexo,\nel tipo de dolor que padece y la depresión en el segmento ST inducida\npor ejercicio en relación al reposo.Se observa que para valores muy altos del hiperparámetro de complejidad,\nel SSE es muy elevado. Esto es, produce modelos muy sencillos pero con\nnula potencia predictiva. En el otro extremo, para \\(\\zeta=0,01\\) el SSE\nse minimiza hasta llegar \\(SSE=0,54\\), por lo que el árbol se poda de\nacuerdo la ecuación (24.8) con dicho valor de \\(\\zeta\\). El\nresultado del modelo se muestra en el árbol de la Fig.\n24.8. La interpretación de este árbol sería:Si el paciente tiene diagnóstico de accidente coronario, solo\nnecesitará un día de hospitalización.Si el paciente tiene diagnóstico de accidente coronario, solo\nnecesitará un día de hospitalización.En el caso de tener este diagnóstico, y una depresión mayor o igual\ndos en el segmento ST inducida por ejercicio en relación al\nreposo, necesitará 2,8 días de hospitalización.En el caso de tener este diagnóstico, y una depresión mayor o igual\ndos en el segmento ST inducida por ejercicio en relación al\nreposo, necesitará 2,8 días de hospitalización.En un último ejemplo, si la depresión en el segmento ST inducida por\nejercicio en relación al reposo está entre 0,35 y 2 entonces el\npaciente necesitará 3,8 días de hospitalización. Si por el\ncontrario, la depresión en el segmento ST inducida por ejercicio en\nrelación al reposo es menor 0,35, el número de días de\nhospitalización depende del sexo del paciente: los hombres\nnecesitarán 3,2 días y las mujeres tan solo 1,9 días.En un último ejemplo, si la depresión en el segmento ST inducida por\nejercicio en relación al reposo está entre 0,35 y 2 entonces el\npaciente necesitará 3,8 días de hospitalización. Si por el\ncontrario, la depresión en el segmento ST inducida por ejercicio en\nrelación al reposo es menor 0,35, el número de días de\nhospitalización depende del sexo del paciente: los hombres\nnecesitarán 3,2 días y las mujeres tan solo 1,9 días.\nFigura 24.8: Árbol de regresión para predecir el número de días de hospitalización.\n","code":"\n# se cargan los datos\ndata(\"cleveland\")\n\n# se entrena el modelo\nset.seed(101)\nmodel <- rpart(dhosp ~ diag + edad + sexo + tdolor + dep,\n               data=cleveland, method=\"anova\")model$cptable\n\n          CP nsplit rel error    xerror       xstd\n1 0.37275022      0 1.0000000 1.0128283 0.09213359\n2 0.01674747      1 0.6272498 0.6427926 0.06048143\n3 0.01132433      4 0.5770074 0.6788431 0.06681871\n4 0.01007684      6 0.5543587 0.6825792 0.06505426\n5 0.01000000      7 0.5442819 0.6843192 0.06514439\n# se pinta el árbol obtenido\nrpart.plot(model)"},{"path":"cap-arboles.html","id":"árbol-de-regresión-para-la-predicción-del-precio-unitario-de-la-vivienda-en-madrid","chapter":"Capítulo 24 Árboles de clasificación y regresión","heading":"24.4.4 Árbol de regresión para la predicción del precio unitario de la vivienda en Madrid","text":"En este ejemplo se va entrenar un árbol de regresión para predecir el\nprecio unitario de la vivienda en Madrid. Para ello, se van utilizar\nlos datos de viviendas la venta en Madrid publicadas en Idealista\ndurante el año 2018. Estos datos están incluidos en el paquete\nidealista18. Para facilitar la interpretación del modelo, sólo se van\nutilizar 8 de las variables incluidas en el conjunto de datos:\nsuperficie construida, número de dormitorios, número de baños, si tiene\nterraza, si tiene ascensor, si el precio incluye el parking, distancia\nal centro de Madrid y distancia una parada de metro.Como en el ejemplo anterior, para \\(\\zeta=0,01\\) el SSE se minimiza hasta\nllegar \\(SSE=0,56\\), por lo que el árbol se poda de acuerdo la\necuación (24.8) con dicho valor de \\(\\zeta\\). El resultado del\nmodelo se muestra en el árbol de la Fig. 24.9.\nLa interpretación de este árbol sería:Si una vivienda con ascensor se encuentra menos de 3,2km del\ncentro de Madrid y menos de 0,46km de una estación de Metro, el\nprecio unitario predicho para esa vivienda será de 5.248€/\\(m^{2}\\).Si una vivienda con ascensor se encuentra menos de 3,2km del\ncentro de Madrid y menos de 0,46km de una estación de Metro, el\nprecio unitario predicho para esa vivienda será de 5.248€/\\(m^{2}\\).Si una vivienda se encuentra más de 3,2km del centro de Madrid y\ntiene ascensor, el precio unitario predicho será de\n2.160€/\\(m^{2}\\).Si una vivienda se encuentra más de 3,2km del centro de Madrid y\ntiene ascensor, el precio unitario predicho será de\n2.160€/\\(m^{2}\\).Si una vivienda se encuentra menos de 3,2km del centro de Madrid y\nmás de 0,46km de una estación de Metro, el precio unitario\npredicho para esa vivienda será de 3.873€/\\(m^{2}\\).Si una vivienda se encuentra menos de 3,2km del centro de Madrid y\nmás de 0,46km de una estación de Metro, el precio unitario\npredicho para esa vivienda será de 3.873€/\\(m^{2}\\).\nFigura 24.9: Árbol de regresión para predecir el precio unitario de las viviendas en Madrid.\n","code":"library(\"idealista18\")\ndata(\"Madrid_Sale\")\n\nMadrid_Sale <- Madrid_Sale |>\n  dplyr::select(UNITPRICE, CONSTRUCTEDAREA, ROOMNUMBER, BATHNUMBER,\n         HASTERRACE,HASLIFT, ISPARKINGSPACEINCLUDEDINPRICE,\n         DISTANCE_TO_CITY_CENTER, DISTANCE_TO_METRO)\n\nhead(Madrid_Sale)\n\n  UNITPRICE CONSTRUCTEDAREA ROOMNUMBER BATHNUMBER HASTERRACE HASLIFT\n1  2680.851              47          1          1          0       1\n2  4351.852              54          1          1          0       0\n3  4973.333              75          2          1          0       0\n4  5916.667              48          1          1          0       1\n5  4560.000              50          0          1          0       0\n6  3921.260             127          3          2          0       1\n  ISPARKINGSPACEINCLUDEDINPRICE DISTANCE_TO_CITY_CENTER DISTANCE_TO_METRO\n1                             0               8.0584293         0.8720746\n2                             0               0.8763693         0.1163821\n3                             0               0.9074793         0.1391088\n4                             0               0.8454622         0.1442990\n5                             0               1.2502313         0.3370982\n6                             0               0.5417727         0.1614363\n# Se entrena el modelo\nlibrary(\"rpart\")\nset.seed(101)\nmodel <- rpart(UNITPRICE ~ ., Madrid_Sale, method = \"anova\")\n# se pinta el árbol obtenido\nrpart.plot(model)"},{"path":"cap-arboles.html","id":"resumen-21","chapter":"Capítulo 24 Árboles de clasificación y regresión","heading":"Resumen","text":"En este capítulo se introduce al lector en los árboles de decisión para\nclasificación y regresión, en particular:Se presenta la lógica para la construcción de árboles de decisión,\nya sean de regresión o clasificación.Se contemplan diferentes medidas con las que el árbol decide avanzar\nhacia un nuevo punto de decisión.Se presentan los conceptos de sobreajuste y complejidad del árbol,\nasí como la forma de controlarlos.Se muestra el uso de R para la clasificación de clases binarias y\npara la predicción de variables respuesta numéricas en casos\naplicados.","code":""},{"path":"cap-svm.html","id":"cap-svm","chapter":"Capítulo 25 Máquinas de vector soporte","heading":"Capítulo 25 Máquinas de vector soporte","text":"Ramón . Carrasco\\(^{}\\) e Itzcóatl Bueno\\(^{b,}\\)\\(^{}\\)Universidad Complutense de Madrid\n\\(^{b}\\)Instituto Nacional de Estadística","code":""},{"path":"cap-svm.html","id":"introducción-11","chapter":"Capítulo 25 Máquinas de vector soporte","heading":"25.1 Introducción","text":"Aunque las máquinas de vector soporte se desarrollaron en los años 90 dentro de la comunidad informática ((Boser, Guyon, Vapnik 1992), (Cortes Vapnik 1995)) como un método de clasificación binaria, su aplicación se ha extendido problemas de clasificación múltiple y regresión. Como técnica de clasificación, las máquinas de vector soporte (SVM por sus siglas inglés Support Vector Machines) son similares la regresión logística pero la SVM enfatiza en un margen de error aceptable en torno la frontera de decisión. \nFigura 25.1: SVM vs Regresión logística.\nEn la Fig. 25.1 se muestra que la regresión logística divide las observaciones en dos clases de tal forma que se minimice la distancia entre los puntos y la frontera de decisión (). Por otro lado, la frontera de decisión (B) de la SVM separa los datos en dos clases, pero maximizando la distancia entre esta y los puntos de ambas clases. El margen es la distancia entre la frontera de decisión y los puntos más cercanos. El margen es una parte clave del SVM, puesto que evita clasificaciones erróneas de casos futuros como podría pasar en el caso de la regresión logística y como se ilustra en la Fig. 25.2.\nFigura 25.2: Nueva observación clasificada en SVM vs Regresión logística.\nEn resumen, los nuevos datos pueden ser clasificados dentro del margen. Cuanto mayor sea este margen, mayor será la capacidad para clasificar correctamente estos puntos. Por tanto, para obtener una clasificación errónea en la SVM es necesario que una observación se clasifique aún más allá del margen que en cualquier otro discriminante lineal. En problemas reales, es difícil que los discriminantes lineales, vistos en el Cap. 21, logren una línea que divida perfectamente las categorías clasificar. Sin embargo, en la SVM, se incluye en la función objetivo (que mide la calidad del ajuste de los datos de entrenamiento) una penalización los puntos que queden del lado equivocado del límite de decisión. En caso de que los datos puedan ser divididos linealmente, se cometerá ninguna penalización y se maximizará el margen. Mientras que si los datos son linealmente separables, el mejor ajuste vendrá dado por el equilibrio entre una penalización del error total bajo y un margen de decisión grande. La penalización una observación mal clasificada es proporcional la distancia desde la frontera de decisión.Sin embargo, la SVM también tiene desventajas reseñables. En primer lugar, la SVM es adecuada en conjuntos de datos grandes porque la complejidad de entrenamiento es elevada. Además, la SVM funciona bien cuando los datos tienen mucho ruido, es decir, cuando las clases se superponen. Finalmente, si el conjunto de datos de entrenamiento tiene más variables que observaciones, el rendimiento del modelo disminuirá.","code":""},{"path":"cap-svm.html","id":"algoritmo-svm-para-clasificación-binaria","chapter":"Capítulo 25 Máquinas de vector soporte","heading":"25.2 Algoritmo SVM para clasificación binaria","text":"\nEl algoritmo por el que se obtiene un modelo SVM (Vapnik 1997) se basa en la ecuación del hiperplano compuesta por dos hiperparámetros: un vector de números reales \\(\\omega\\) de la misma dimensión que el vector de variables de entrada \\(x\\), y un número real \\(b\\) tal que:Donde \\(\\omega x\\) es \\(\\omega^{(1)}x^{(1)} + \\omega^{(2)}x^{(2)} + \\dots + \\omega^{(p)}x^{(p)}\\) siendo \\(p\\) el número de variables incluidas en \\(x\\). De este modo, la predicción para una instancia de \\(x\\) viene dada por:Siendo \\(sign\\) el operador que devuelve +1 para cualquier valor positivo y -1 para los valores negativos. Por tanto, el objetivo es ajustar los valores óptimos de \\(\\omega\\) y \\(b\\) para el algoritmo. Estos hiperparámetros se obtienen resolviendo un problema de optimización sujeto las siguientes restricciones:\\[\\begin{eqnarray}\n\\omega x_i - b\\geq 1 \\textrm{ si } y_i &= +1 \\textrm{ y } \\\\\n\\omega x_i - b\\leq 1 \\textrm{ si } y_i &= -1\n\\end{eqnarray}\\]Además, el objetivo del problema de optimización es maximizar el margen en torno la frontera de decisión. Para conseguir esto es necesario minimizar la norma euclídea, y, por tanto, el problema resolver es:","code":""},{"path":"cap-svm.html","id":"y-si-tengo-más-de-dos-clases","chapter":"Capítulo 25 Máquinas de vector soporte","heading":"25.3 ¿Y si tengo más de dos clases?","text":"Hasta ahora se ha presentado la SVM como un algoritmo solo aplicable la clasificación de dos clases pero ¿y si se tienen más de dos clases? En general, hay dos enfoques para resolver esto: uno contra todos (OVA, por One Vs ) y uno contra uno (OVO, por One Vs One). En el enfoque OVA, se ajusta una SVM para cada clase, es decir una clase contra las demás y se clasifica la clase para la cual el margen es mayor. En cambio, en el enfoque OVO se ajustan todas las SVM por pares y se clasifica la clase que gane las competiciones por pares.","code":""},{"path":"cap-svm.html","id":"truco-del-kernel-tratando-con-la-no-linealidad","chapter":"Capítulo 25 Máquinas de vector soporte","heading":"25.4 Truco del kernel: tratando con la no linealidad","text":"Las SVM funcionan muy bien si la separación entre clases es lineal. Sin embargo, si la separación es más compleja se intenta transformar el espacio en otro de mayor dimensionalidad donde las clases sí sean separables linealmente. Para ello, el modelo SVM se extiende incluyendo la función de pérdida (\\(\\ell\\))\n“hinge” ((Gentile Warmuth 1998),(Lee Lin 2013)) definida como:En machine learning, esta función de pérdida se utiliza para entrenar clasificadores, más concretamente para la clasificación por el margen máximo (métodos de clasificación binaria que se utiliza cuando hay una frontera lineal que separa perfectamente los datos de entrenamiento de una categoría de los de la otra), sobre todo para las SVM. La función de pérdida es cero cuando se cumplen las restricciones, es decir, si \\(\\omega x_i\\) es clasificado en el lado correcto de la frontera de decisión. Por otro lado, si un dato es mal clasificados, el valor obtenido con la función de pérdida es proporcional la distancia hasta la frontera de decisión. Por tanto, el objetivo es minimizar la función de coste:Donde \\(C\\) es un hiperparámetro que controla la compensación entre incrementar el tamaño de la frontera de decisión y asegurar que cada \\(x_i\\) sea clasificado en el lado correcto de la frontera de decisión.Un modelo SVM que optimiza la función de pérdida se denomina SVM soft-margin mientras que el modelo original es conocido como SVM hard-margin. La ecuación (25.1) muestra que para valores grandes de \\(C\\) el segundo término es despreciable, por lo que el algoritmo ignorará por completo la clasificación errónea y tratará de obtener el mayor margen posible. Si se reduce el valor de \\(C\\), se penaliza más cada error de clasificación, por lo que se cometerán menos errores sacrificando amplitud del margen.veces es posible separar los datos por un hiperplano en su espacio original. Sin embargo, el truco del kernel utiliza una función que implícitamente transforma el espacio original un espacio de mayor dimensión durante la optimización de la función de coste, como se muestra en la Fig. 25.3. Así, es posible transformar un espacio de datos bidimensional separable linealmente en un espacio de datos tridimensional linealmente separable usando un mapeo específico definido por \\(\\phi:x\\rightarrow\\phi(x)\\) donde \\(\\phi(x)\\) es un vector de mayor dimensión que \\(x\\). Sin embargo, se conoce la función de mapeo que funcionará en los datos. Si se prueban todas las transformaciones posibles, podría ser ineficiente y llegar la resolución del problema de clasificación planteado.\nFigura 25.3: Izquierda: Las dos clases en el espacio original (2-D). Derecha: Las dos clases en el espacio de sobredimensionado (3-D).\nSe puede trabajar eficientemente en espacios de mayor dimensión sin necesidad de hacer las transformaciones explícitamente. Utilizando el truco del kernel se puede evitar este proceso costoso de transformación de tal manera que se evita calcular el producto escalar reemplazándolo por una operación más simple con las variables originales que proporciona el mismo resultado. continuación se explican algunos de estos operadores especiales, llamados kernels, que permiten llevar cabo dicha transformación.","code":""},{"path":"cap-svm.html","id":"algunos-kernels-populares","chapter":"Capítulo 25 Máquinas de vector soporte","heading":"25.4.1 Algunos kernels populares","text":"Los kernels ((Schölkopf et al. 1997), (Scholkopf et al. 1997)) más populares en el entrenamiento de SVM están incluidos dentro de la función svm() del paquete e1071 donde se puede se especifican en el hiperparámetro kernel. Estos kernel son:lineal: \\(K(u,v) = \\langle u,v\\rangle\\)polinomial de grado \\(\\delta\\): \\(K(u,v) = \\gamma(k_1+\\langle u,v\\rangle)^\\delta\\)base radial: \\(K(u,v) = e^{\\gamma||u-v||^2}\\)sigmoidal: \\(K(u,v) = \\tanh(\\gamma\\langle u,v\\rangle+k_1)\\)Donde \\(\\langle u,v\\rangle = \\sum_{=1}^{n}{u_iv_i}\\) es el producto escalar. Cada uno de estos kernels tiene sus propios hiperparámetros, como \\(\\delta\\) o \\(\\gamma\\), que es necesario tunear para optimizar el rendimiento de la SVM. En machine learning, el término tunear, hace referencia al hecho de ajustar automáticamente tratando de optimizar los hiperparámetros del algoritmo. la hora de ajustar en R un modelo SVM se puede conocer los hiperparámetros ajustar utilizando la función modelLookup de caret. Por ejemplo, para una SVM con kernel de base lineal se usaría así:El hiperparámetro que se puede ajustar en un modelo SVM con kernel de base lineal en R es el coste (C), el cual representa la constante \\(C\\) en la ecuación (25.1).","code":"modelLookup(\"svmLinear\")\n      model parameter label forReg forClass probModel\n1 svmLinear         C  Cost   TRUE     TRUE      TRUE"},{"path":"cap-svm.html","id":"procedimiento-con-r-la-función-svm","chapter":"Capítulo 25 Máquinas de vector soporte","heading":"25.5 Procedimiento con R: la función svm()","text":"En el paquete e1071 de R se encuentra la función svm() que se utiliza para entrenar un modelo máquinas vector soporte:x: conjunto de datos de entrenamiento que contiene los predictoresy: vector respuesta con las clases o valores de la variable respuesta.scale: booleano que indica si es necesario escalar las variables.type: indica si se pretende resolver un problema de clasificación o de regresión.kernel: kernel utilizado durante el entrenamiento y la predicción.","code":"\nsvm(x, y, scale = TRUE, type = NULL, kernel = ..., ...)"},{"path":"cap-svm.html","id":"aplicación-de-un-modelo-svm-radial-con-ajuste-automático-en-r","chapter":"Capítulo 25 Máquinas de vector soporte","heading":"25.6 Aplicación de un modelo SVM Radial con ajuste automático en R","text":"Los datos utilizados para entrenar el modelo SVM en este capítulo se cargan desde la librería CDR. Además, para su entrenamiento se requieren las librerías caret y e1071.Se entrena un modelo SVM con kernel radial utilizando el conjunto de entrenamiento con todas las variables numéricas. Previamente, se aplica una normalización z-score, presentadas en el Cap. 9, al conjunto de entrenamiento. De este modo, las variables que inicialmente tenían distintas escalas de medida, ahora todas se miden en la misma escala. Además, se ajustan automáticamente los hiperparámetros de dicho algoritmo durante el proceso de entrenamiento.Se define como procedimiento de muestreo una validación cruzada, como la presentada en el Cap. 9, de 10 folds. Además, se le indica al modelo que debe calcular las probabilidades de clase en cada remuestreo en caso de estar entrenando un modelo de clasificación. Con el argumento summaryFunction = twoClassSummary se le indica al modelo que para resumir los resultados se calculen la sensibilidad, especificidad y el área bajo la curva ROC. Como se ha comentado, conviene estandarizar los datos, esto se le indica la función través del argumento preProcOptions con la opción center. su vez, se define una red de hiperparametros optimizar. través de la función train() se ajusta automáticamente el modelo con los hiperparametros óptimos.Los argumentos que requiere la función son la formula, es decir, indicar la variable respuesta y qué predictores intervienen en el modelo. Los datos que se van utilizar, así como el algoritmo entrenar, en este caso la SVM con kernel de base radial. Además, se indica una métrica para el rendimiento del modelo, en caso de indicarlo R asigna la más acorde de acuerdo la variable respuesta. Finalmente, se incluyen las opciones de entrenamiento y la red de hiperparámetros probar para determinar la combinación óptima. Los hiperparámetros del modelo entrenado son \\(\\sigma=0.1\\) y \\(C=0.1\\). Este resultado queda definido en la salida del modelo, pero también es representable como en la Fig. 25.4. En este gráfico el eje y mide el rendimiento del modelo para ciertos valores de los hiperparámetros. Cada línea representa un valor para el hiperparámetro sigma, y se mide su rendimiento variando distintos niveles del parámetro coste (C), que queda representado en el eje x. Así, se observa que la línea roja (sigma=0,1) alcanza el mayor nivel de precisión en el valor C=0,1.\nFigura 25.4: Optimización de los parámetros C y sigma de una SVM.\nLos boxplot de la Fig. 25.5 muestran un resumen del rendimiento del modelo en las distintas repeticiones del proceso de validación cruzada. De esta manera, se observa como la sensibilidad es superior al 75% y la especificidad supera valores del 70%, esto indica que el modelo entrenado es capaz de predecir correctamente tanto los clientes que van comprar el nuevo producto como los que lo van hacer.\nFigura 25.5: Resultados del modelo obtenidos durante la validación cruzada.\n","code":"\nlibrary(\"CDR\")\nlibrary(\"caret\")\nlibrary(\"e1071\")\nlibrary(\"reshape\")\nlibrary(\"ggplot2\")\n\ndata(dp_entr_NUM)\ntrControl <- trainControl(\n  method = \"cv\", \n  number = 10, \n  classProbs = TRUE,   \n  preProcOptions = list(\"center\"),\n  summaryFunction = twoClassSummary  \n)\n\n# Se especifica un rango de valores para los hiperparámetros\ntuneGrid <- expand.grid(sigma = seq(from=0.1, to=0.2, by=0.05),\n                        C = 10**(-2:4))# Se fija la semilla aleatoria\nset.seed(101)\n\n# Se entrena el modelo\nmodel <- train(CLS_PRO_pro13 ~ ., \n             data=dp_entr_NUM, \n             method=\"svmRadial\", \n             metric=\"ROC\",\n             trControl=trControl,\n             tuneGrid=tuneGrid)\n\nmodel\n\n\nSupport Vector Machines with Radial Basis Function Kernel \n\n558 samples\n 19 predictor\n  2 classes: 'S', 'N' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 502, 502, 502, 503, 503, 502, ... \nResampling results across tuning parameters:\n\n  sigma  C      ROC        Sens       Spec     \n  0.10   1e-02  0.9553241  0.8785714  0.7071429\n  0.10   1e-01  0.9566327  0.8924603  0.8247354\n  0.10   1e+00  0.9434902  0.8604497  0.8496032\n  0.10   1e+01  0.9227230  0.8460317  0.8423280\n  0.10   1e+02  0.8804894  0.8567460  0.8279101\n  0.10   1e+03  0.8645692  0.8674603  0.8206349\n  0.10   1e+04  0.8548469  0.8423280  0.8242063\n  0.15   1e-02  0.9527636  0.8535714  0.6642857\n  0.15   1e-01  0.9513653  0.9105820  0.8105820\n  0.15   1e+00  0.9310091  0.8783069  0.8494709\n  0.15   1e+01  0.8941421  0.8531746  0.8387566\n  0.15   1e+02  0.8602088  0.8781746  0.8242063\n  0.15   1e+03  0.8369331  0.8458995  0.8134921\n  0.15   1e+04  0.8369284  0.8637566  0.8064815\n  0.20   1e-02  0.9443925  0.8535714  0.6321429\n  0.20   1e-01  0.9440098  0.9250000  0.7384921\n  0.20   1e+00  0.9199310  0.8818783  0.8387566\n  0.20   1e+01  0.8752031  0.8674603  0.8207672\n  0.20   1e+02  0.8477324  0.8674603  0.8063492\n  0.20   1e+03  0.8308296  0.8638889  0.8134921\n  0.20   1e+04  0.8308296  0.8638889  0.8099206\n\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were sigma = 0.1 and C = 0.1.\nggplot(model)\nggplot(melt(model$resample[,-4]), aes(x = variable, y = value, fill=variable)) + \n  geom_boxplot(show.legend=FALSE) + \n  xlab(NULL) + ylab(NULL)"},{"path":"cap-svm.html","id":"importancia-de-las-variables","chapter":"Capítulo 25 Máquinas de vector soporte","heading":"25.6.1 Importancia de las variables","text":"En machine learning, muchos de los algoritmos de caja negra (definidos en el Cap. 4), proporcionan información sobre la importancia que tiene cada variable en el modelo. Este es el caso de las máquinas de vector soporte. Tanto para la SVM como para otros algoritmos, es posible cuantificar la importancia de cada variable utilizando paquetes de R como DALEX, iml o vip.Este último paquete incluye una función con el mismo nombre vip(). Para medir la importancia, se indica qué métrica se utilizó en el proceso de entrenamiento del modelo, en el caso de la SVM se indicará que fue el área bajo la curva (metric=\"auc\"). En el argumento pred_wrapper se indica una función de medida que contenga tanto los valores observados como los valores predichos. Dado que la SVM entrenada utiliza AUC para medir el rendimiento del modelo ajustado, la función de medida indicada en pred_wrapper deberá devolver la probabilidad de que el modelo asigne una observación la clase de referencia. En este ejemplo, la clase de referencia es “SI”, puesto que interesa saber si un cliente comprará el nuevo producto. Entonces, la función de predicción se define como:Ejecutando la función vip() con los argumentos mencionados se genera la Fig. 25.6. Este gráfico indica la importancia de cada variable en el modelo de más menos importante. En este caso, la variable más importante es el importe gastado en el smartchwatch fitness, seguida muy de cerca por la variable que indica si el cliente compra o el smartchwatch fitness. En el otro extremo, se observa que las variables que indican si el cliente tiene un nivel de educación básico o , son muy relevantes en la SVM entrenada.\nFigura 25.6: Importancia de las variables incluidas en la SVM.\npartir de la Fig. 25.6 se puede concluir que para predecir si un cliente comprará o el tensiómetro digital las variables que más importancia tienen: son el importe que gastó en el smartchwatch fitness, si compró o el smartchwatch fitness, el importe que gastó en la depiladora eléctrica y si compró o la depiladora eléctrica.De forma similar se podrían probar el resto de kernels disponibles para el algoritmo SVM.","code":"\nprob_si <- function(object, newdata) {\n  predict(object, newdata = newdata, type = \"prob\")[, \"S\"]\n}\nlibrary(\"vip\")\n\nset.seed(101)  \nvip(model, train = dp_entr_NUM, target = \"CLS_PRO_pro13\", metric = \"auc\",\n    reference_class = \"S\", pred_wrapper = prob_si, method=\"permute\",\n    aesthetics = list(color = \"steelblue2\", fill = \"steelblue2\"))"},{"path":"cap-svm.html","id":"resumen-22","chapter":"Capítulo 25 Máquinas de vector soporte","heading":"Resumen","text":"En este capítulo se introduce al lector en el algoritmo de máquinas vector soporte, en particular:Se presenta el concepto de margen de decisión, y las ventajas de la SVM respecto otras técnicas de clasificación.Se explica el truco del kernel cuando los datos son separables por un hiperplano en su espacio originalSe da un repaso los kernels más utilizados.Se presenta la aplicación de una SVM con kernel radial en R para la clasificación de datos con respuesta binaria, en el que se ajustan automáticamente los hiperparámetros.Se obtiene la importancia de las variables del modelo final.","code":""},{"path":"cap-knn.html","id":"cap-knn","chapter":"Capítulo 26 Clasificador k-vecinos más próximos","heading":"Capítulo 26 Clasificador k-vecinos más próximos","text":"Ramón . Carrasco\\(^{}\\) e Itzcóatl Bueno\\(^{b,}\\)\\(^{}\\)Universidad Complutense de Madrid\n\\(^{b}\\)Instituto Nacional de Estadística","code":""},{"path":"cap-knn.html","id":"introducción-12","chapter":"Capítulo 26 Clasificador k-vecinos más próximos","heading":"26.1 Introducción","text":"El k-vecinos más próximos (KNN, por sus siglas en inglés k-Nearest Neighbors) es un algoritmo de aprendizaje paramétrico. Un algoritmo paramétrico presupone la forma concreta del modelo entrenar, siendo más flexible. Sin embargo, esto se consigue costa de necesitar más datos de entrenamiento y siendo más lentos que los algoritmos paramétricos. Al contrario que otros algoritmos de aprendizaje que permiten deshacerse de los datos de entrenamiento una vez se entrena el modelo, el modelo KNN guarda las observaciones de entrenamiento en memoria. Esto es, al incorporar una nueva observación \\(x\\), el algoritmo KNN encuentra las \\(k\\) observaciones del conjunto de datos de entrenamiento más similares la nueva y proporciona la clase mayoritaria (en el caso de clasificación) o el valor medio (en el caso de regresión).El número de casos (\\(k\\)) utilizar para clasificar las nuevas observaciones es un parámetro crucial para este algoritmo (G. James et al. 2013). Si, por ejemplo, \\(k=3\\), el modelo KNN utilizará las tres observaciones más similares (vecinos) al nuevo caso para clasificarlo. Es recomendable probar distintos valores de \\(k\\) para conseguir el mejor ajuste del modelo, y por tanto, es conveniente evitar valores extremos de \\(k\\). Si se establece un valor muy bajo de \\(k\\) aumentará el sesgo y llevará clasificaciones erróneas. Mientras que valores muy elevados de \\(k\\) harán que el algoritmo sea computacionalmente costoso y además tampoco será un buen clasificador. Además, también se recomienda establecer valores impares de \\(k\\) para evitar puntos muertos estadísticos (empate entre categorías) y un resultado válido.\nFigura 26.1: Ejemplo de k-vecinos más próximos.\nLa escala de las variables puede impactar en el resultado del modelo KNN. Por ello, el conjunto de datos debe escalarse para que aquellas variables con unidades de medida grandes tengan más importancia en el cálculo que otras con magnitudes menores. Así se reduce la importancia de las variables debido sus unidades de medida y se estandariza la varianza.Pese que el modelo KNN es fácil de entender y generalmente preciso, almacenar el conjunto de datos de entrenamiento, así como calcular la distancia entre cada nueva observación clasificar y las observaciones del conjunto de datos, supone la necesidad de recursos computacionales altos. Esto implica que cuanto mayor es la cantidad de observaciones en el conjunto de datos, mayor es el tiempo para la ejecución de una sola predicción, y, por tanto, esto puede dar lugar tiempos de procesamiento lentos. Por este motivo, se recomienda el uso del algoritmo KNN cuando se dispone de conjuntos de datos muy grandes. Otra desventaja tener en cuenta es la dificultad de aplicar KNN conjuntos de datos con un gran número de variables, puesto que calcular las distancias entre observaciones con múltiples dimensiones también incrementará la necesidad de recursos computacionales y podría dificultar que se clasifique de forma precisa.","code":""},{"path":"cap-knn.html","id":"decisiones-a-tener-en-cuenta","chapter":"Capítulo 26 Clasificador k-vecinos más próximos","heading":"26.2 Decisiones a tener en cuenta","text":"La elección de la función de distancia, así como el número de vecinos \\(k\\) son decisiones que debe tomar el investigador antes de ejecutar el algoritmo. Siendo este último el hiperparámetro del modelo que la función modelLookup() de caret nos devuelve considerando que es primordial ajustarlo:","code":"modelLookup(\"knn\")\n  model parameter      label forReg forClass probModel\n1   knn         k #Neighbors   TRUE     TRUE      TRUE"},{"path":"cap-knn.html","id":"función-de-distancia-a-utilizar","chapter":"Capítulo 26 Clasificador k-vecinos más próximos","heading":"26.2.1 Función de distancia a utilizar","text":"El modelo KNN determina la cercanía entre dos observaciones través de una función de distancia. Generalmente, se utilizan la distancia euclídea, mostrada en la ecuación (26.1), y la distancia de Manhattan, mostrada en la ecuación (26.2). Otras funciones de distancia, como las presentadas en el Cap. ??, también pueden ser utilizadas para el entrenamiento de este algoritmo.\\[\\begin{equation}\nd(x_i,x_k)=\\sqrt{\\sum_{j=1}^{p}{(x_i^{(j)}-x_k^{(j)})^2}}\n\\tag{26.1}\n\\end{equation}\\]\\[\\begin{equation}\nd(x_i,x_k)=\\sum_{j=1}^{p}{|x_i^{(j)}-x_k^{(j)}|}\n\\tag{26.2}\n\\end{equation}\\]En el caso de querer incluir tanto variables cuantitativas como variables cualitativas en el cálculo de la distancia, el coeficiente de disimilitud de Gower es la función de distancia más popular para esta situación. El coeficiente de disimilitud de Gower se define como:\\[\\begin{equation}\nd(,j) = \\frac{\\sum^{p}_{k=1}{\\omega_k \\delta^{(k)}_{ij}d^(k)_{ij}}}{\\omega_k \\delta^{(k)}_{ij}}\n\\end{equation}\\]El coeficiente de Gower es una media ponderada de las distancias \\(d^{(k)}_{ij}\\) con ponderaciones \\(\\omega_{k}\\delta_{ij}^{(k)}\\).","code":""},{"path":"cap-knn.html","id":"número-de-vecinos-k-seleccionados","chapter":"Capítulo 26 Clasificador k-vecinos más próximos","heading":"26.2.2 Número de vecinos (k) seleccionados","text":"Como se ha reiterado, la elección de cuántos vecinos (\\(k\\)) intervienen en el ajuste del algoritmo es determinante para su rendimiento. Si se escogen demasiado pocos vecinos, se producirá sobreajuste en el modelo. En el extremo en el que sólo se utilizará un vecino (\\(k=1\\)), la predicción se basará en la observación con la menor distancia al elemento clasificar. Por otro lado, un número alto de vecinos (\\(k\\)) hace que el modelo ajuste bien al tener en cuenta un vecindario más grande. En este sentido, en el caso extremo de elegir todas las observaciones como vecinos más próximos (\\(k=n\\)), se obtendrá el valor medio (en el caso de la regresión) o la clase mayoritaria (en el caso de la clasificación) como valor predicho para todas las observaciones del conjunto de entrenamiento.existe una regla general para la elección óptima de \\(k\\), puesto que en gran medida dependerá del conjunto de datos utilizado. Cuando el conjunto de datos tiene pocas variables que aporten información, valores pequeños de \\(k\\) tienden funcionar mejor. Cuantas más variables sin importancia se incluyen en el conjunto de datos, mayor deberá ser el valor de \\(k\\) para suavizar su efecto.","code":""},{"path":"cap-knn.html","id":"procedimiento-con-r-la-función-knn","chapter":"Capítulo 26 Clasificador k-vecinos más próximos","heading":"26.3 Procedimiento con R: la función knn()","text":"En el paquete class de R se encuentra la función knn() que se utiliza para entrenar el modelo k-vecinos más próximos:train: conjunto de datos con las observaciones de entrenamiento.test: conjunto de datos con las observaciones de validación. Un vector se interpreta como una única observación validar.cl: clases de las observaciones de entrenamiento.k: número de vecinos considerar","code":"\nknn(train, test, cl, k = 1, ...)"},{"path":"cap-knn.html","id":"aplicación-del-modelo-knn-en-r","chapter":"Capítulo 26 Clasificador k-vecinos más próximos","heading":"26.4 Aplicación del modelo KNN en R","text":"En este ejemplo se entrena un modelo KNN para clasificar qué clientes comprarán el tensiómetro digital teniendo en cuenta sus características y el resto de sus compras. Este conjunto de datos está incluido en el paquete CDR con el nombre dp_entr_NUM. En este conjunto de datos todas las variables son cuantitativas (excepto la clase objetivo) pero dichas variables tienen distintas escalas de medida (euros, años, unidades, etc.) por lo que es necesario indicar en la función trainControl() que se haga un preprocesamiento para estandarizar las variables. Además, se define como método de remuestreo la validación cruzada, como la presentada en el Cap. 9, con 10 folds.Antes de obtener el modelo definitivo, es necesario la selección del número óptimo de vecinos \\(k\\) entrenando distintos modelos variando dicho hiperparámetro. Para facilitar este trabajo arduo, en el paquete caret de R se puede definir una red de posibles valores sobre los que evaluar el modelo KNN y que de forma automática se determine el valor que mejor rendimiento proporcione. continuación se definen los posibles valores de \\(k\\) que se quieren evaluar.Una vez se que se ha definido tanto el método de remuestreo como la red de posibles valores del hiperparámetro se puede entrenar el modelo:En la Fig. 26.2 se observa que el número óptimo de vecinos es \\(k=10\\), donde se alcanza el rendimiento óptimo del modelo.\nFigura 26.2: Número óptimo de vecinos (k).\nEl boxplot de los resultados obtenidos durante el proceso de validación cruzada muestra que el AUC del modelo oscila entre un 60% y un 85% aproximadamente.\nFigura 26.3: Resultados obtenidos durante el proceso de validación cruzada.\nEl modelo se resiente en su rendimiento al tener dificultades en predecir correctamente la clase positiva, más concretamente se puede observar en la Fig. 26.3 que la sensibilidad oscila entre el 40% y el 75%; resultados ligeramente peores que los que obtiene al predecir observaciones de la clase negativa, los cuales oscilan entre el 50% y el 85%.","code":"library(\"CDR\")\nlibrary(\"class\")\nlibrary(\"caret\")\nlibrary(\"reshape\")\nlibrary(\"ggplot2\")\n\ndata(dp_entr_NUM)\n\nhead(dp_entr_NUM)\n\n  ind_pro11 ind_pro12 ind_pro14 ind_pro15 ind_pro16 ind_pro17 des_nivel_edu.ALTO\n1         1         0         1         1         1         0                  0\n2         0         0         1         0         1         0                  0\n3         0         0         1         1         1         1                  0\n4         0         1         1         0         0         0                  0\n5         0         1         1         0         1         0                  0\n6         1         0         1         0         0         0                  1\n  des_nivel_edu.BASICO des_nivel_edu.MEDIO importe_pro11 importe_pro12 importe_pro14\n1                    0                   1           157             0            40\n2                    0                   1             0             0           240\n3                    1                   0             0             0           425\n4                    0                   1             0           120            60\n5                    1                   0             0           120           133\n6                    0                   0           115             0           220\n  importe_pro15 importe_pro16 importe_pro17 edad tamano_fam anos_exp ingresos_ano CLS_PRO_pro13\n1           200           180             0   49          4       24        30000             S\n2             0           180             0   38          2       12        53000             N\n3           200           180           300   61          4       37       172000             S\n4             0             0             0   47          3       21        38000             N\n5             0           180             0   34          1       10        38000             N\n6             0             0             0   43          2       18        60000             N\n\n\n# Definimos un método de remuestreo\ncv <- trainControl(\n  method = \"repeatedcv\",\n  number = 10,\n  repeats = 5,\n  classProbs = TRUE,\n  preProcOptions = list(\"center\"),\n  summaryFunction = twoClassSummary\n)\n# Definimos la red de posibles valores del hiperparámetro\nhyper_grid <- expand.grid(k = c(1:10,15,20,30,50,75,100))\nset.seed(101)\n# Se entrena el modelo ajustando el hiperparámetro óptimo\nmodel <- train(\n  CLS_PRO_pro13 ~ .,\n  data = dp_entr_NUM,\n  method = \"knn\",\n  trControl = cv,\n  tuneGrid = hyper_grid,\n  metric = \"ROC\"\n)\n\nmodel\n\nk-Nearest Neighbors\n\n558 samples\n 17 predictor\n  2 classes: 'S', 'N'\n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 5 times)\nSummary of sample sizes: 502, 502, 502, 503, 503, 502, ...\nResampling results across tuning parameters:\n\n  k    ROC        Sens       Spec\n    1  0.6584524  0.6466402  0.6702646\n    2  0.6769109  0.6179101  0.6131217\n    3  0.6828893  0.6216402  0.6496561\n    4  0.6851087  0.6404233  0.6394709\n    5  0.6951129  0.6540212  0.6666138\n    6  0.6914664  0.6216402  0.6543386\n    7  0.6982592  0.6252381  0.6953439\n    8  0.6974556  0.6281481  0.6960053\n    9  0.6992229  0.6159524  0.7117725\n   10  0.6994133  0.6037037  0.7052910\n   15  0.6875879  0.5749206  0.7232011\n   20  0.6731477  0.5722751  0.7010582\n   30  0.6752986  0.5529630  0.7024603\n   50  0.6890259  0.5163757  0.7605556\n   75  0.6852886  0.5092593  0.7670106\n  100  0.6719378  0.5049471  0.7820106\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was k = 10.\n\nggplot(model) + geom_vline(xintercept = unlist(model$bestTune),col=\"red\",linetype=\"dashed\") + theme_light()\nggplot(melt(model$resample[,-4]), aes(x = variable, y = value, fill=variable)) +\n   geom_boxplot(show.legend=FALSE) +\n  xlab(NULL) + ylab(NULL)"},{"path":"cap-knn.html","id":"resumen-23","chapter":"Capítulo 26 Clasificador k-vecinos más próximos","heading":"Resumen","text":"En este capítulo se introduce al lector en el algoritmo de aprendizaje supervisado conocido como k-vecinos más próximos, destacando:Las decisiones tener en cuenta antes de entrenar el modelo de k-vecinos más próximos.Se exponen algunas de las distancias más utilizadas para el entrenamiento de este modelo.Se especifican las ventajas y desventajas del número de vecinos tener en cuenta.","code":""},{"path":"cap-naive-bayes.html","id":"cap-naive-bayes","chapter":"Capítulo 27 Naive Bayes","heading":"Capítulo 27 Naive Bayes","text":"Ramón . Carrasco\\(^{}\\) e Itzcóatl Bueno\\(^{b,}\\)\\(^{}\\)Universidad Complutense de Madrid\n\\(^{b}\\)Instituto Nacional de Estadística","code":""},{"path":"cap-naive-bayes.html","id":"nb-intro","chapter":"Capítulo 27 Naive Bayes","heading":"27.1 Introducción","text":"Naive Bayes es un algoritmo de aprendizaje supervisado que se utiliza principalmente para la clasificación. Como otros algoritmos de aprendizaje supervisado, este algoritmo se entrena con variables de entrada y la categoría asociada cada observación y que el modelo debe predecir. Sin embargo, se denomina ‘naive’ dado que asume que las variables de entrada que se incluyen en el modelo son independientes entre sí. Por lo tanto, si se cambia una de las variables de entrada, las demás se verán afectadas.Aunque el algoritmo Naive Bayes es sencillo, destaca por su facilidad de implementación y su potencia predictiva. Su ventaja principal es que utiliza un enfoque probabilístico, lo que implica que todos los cálculos se realizan en tiempo real y, por tanto, los resultados se obtienen inmediatamente, como se detalla más adelante. Además, cuando el conjunto de datos tiene un gran número de observaciones, el algoritmo Naive Bayes es ventajoso respecto algoritmos como la SVM (Cap. 25) o el Random Forest (Cap. 28) debido su mejor tiempo de computación.Al utilizar un enfoque probabilístico, el algoritmo Naive Bayes está construido sobre conceptos de probabilidad, presentados en el Cap. 12, y en especial, este algoritmo hace uso del Teorema de Bayes. continuación se repasan los conceptos fundamentales en los que está basado el algoritmo.","code":""},{"path":"cap-naive-bayes.html","id":"teorema-de-bayes","chapter":"Capítulo 27 Naive Bayes","heading":"27.2 Teorema de Bayes","text":"Sean dos eventos y B definidos en un espacio muestral, se puede definir la probabilidad condicional de que ocurra el evento dado que previamente se haya observado B como:\\[\\begin{equation}\nP(|B) = \\frac{P(\\cap B)}{P(B)}\n\\end{equation}\\]Siempre que \\(P(B) \\neq 0\\) y donde \\(P(\\cap B)\\) es la probabilidad de que ocurran ambos eventos la vez. Los eventos son intercambiables de tal forma que \\(P(\\cap B) = P(B|)P()\\) y si se reemplaza en la primera ecuación tenemos:\\[\\begin{equation}\nP(|B) = \\frac{P(B|)\\cdot P()}{P(B)}\n\\end{equation}\\]Esta fórmula es la definición del teorema de Bayes. El algoritmo de clasificación Naive Bayes (NB) está basado en este teorema. Para ampliar los conceptos estadísticos aquí presentados pueden consultarse en más detalle en el Cap. 12.","code":""},{"path":"cap-naive-bayes.html","id":"el-algoritmo-naive-bayes","chapter":"Capítulo 27 Naive Bayes","heading":"27.3 El algoritmo naive Bayes","text":"Si se adapta el teorema de Bayes un problema de clasificación, se tendría:\\[\\begin{equation}\nP(C=c|\\ell)=\\frac{P(\\ell|C=c)\\cdot P(C=c)}{P(\\ell)}\n\\end{equation}\\]En este caso, \\(P(C=c|\\ell)\\) representa el objetivo de estimación en un problema de clasificación, es decir, la probabilidad de que un individuo pertenezca la clase \\(c\\) después de haber observado la evidencia \\(\\ell\\) (incluida en las variables del modelo). Esta es la denominada probabilidad posteriori. El resto de elementos de la fórmula, se definen como:\\(P(C=c)\\) es la probabilidad priori de pertenecer la clase \\(c\\), es decir, la probabilidad que un individuo tiene de ser asignado esa clase sin observar sus características previamente.\\(P(C=c)\\) es la probabilidad priori de pertenecer la clase \\(c\\), es decir, la probabilidad que un individuo tiene de ser asignado esa clase sin observar sus características previamente.\\(P(\\ell|C=c)\\) es la verosimilitud de observar una instancia particular de las variables incluidas en el modelo cuando el individuo pertenece la clase \\(c\\).\\(P(\\ell|C=c)\\) es la verosimilitud de observar una instancia particular de las variables incluidas en el modelo cuando el individuo pertenece la clase \\(c\\).\\(P(\\ell)\\) es la verosimilitud de observar una instancia particular de las variables incluidas en el modelo, independientemente de qué clase pertenezca el individuo.\\(P(\\ell)\\) es la verosimilitud de observar una instancia particular de las variables incluidas en el modelo, independientemente de qué clase pertenezca el individuo.Sin embargo, una gran dificultad para aplicar esta ecuación es la necesidad de conocer que \\(P(\\ell|c)\\) es igual \\(P(\\ell_1\\cap\\ell_2\\cap\\dots\\cap\\ell_\\kappa|c)\\). La existencia de un ejemplo concreto en el conjunto de datos de entrenamiento que coincida la perfección con \\(\\ell\\) es complicado, y en el caso de existir, se tendrían suficientes ejemplos para poder estimar una probabilidad de forma fiable. La forma de solucionar este problema es incluir una suposición de independencia particularmente fuerte, que como ya se mencionó en la Sec. 27.1, es lo que aporta la denominación de ‘naive’ al algoritmo.La independencia condicional implica que conocer un evento aporta información sobre otro evento. Esto es equivalente :\\[\\begin{equation}\nP(AB|C) = P(|C)\\cdot P(B|C)\n\\end{equation}\\]De este modo, el problema de clasificación en el que era difícil estimar \\(P(\\ell_1\\cap\\ell_2\\cap\\dots\\cap\\ell_\\kappa|c)\\), ahora se tendría:\\[\\begin{equation}\nP(\\ell|c)=P(\\ell_1|c)\\cdot P(\\ell_2|c)\\cdots P(\\ell_\\kappa|c)\n\\end{equation}\\]Y cada uno de los elementos \\(P(\\ell_i|c)\\) puede obtenerse directamente de los datos. Combinando este resultado con la regla de Bayes aplicada un problema de decisión, se obtiene la ecuación dada por el algoritmo Naive Bayes:\\[\\begin{equation}\nP(c|\\ell)=P(\\ell_1|c)\\cdot P(\\ell_2|c)\\cdots P(\\ell_\\kappa|c)P(c)\n\\end{equation}\\]El algoritmo Naive Bayes clasifica una nueva observación estimando la probabilidad de que pertenezca cada clase y asignándole aquella que tenga la mayor probabilidad.En definitiva, el clasificador Naive Bayes es muy eficiente en términos de espacio de almacenamiento necesario, así como tiempos de procesamiento. Además, pesar de ser muy simple, tiene en cuenta las características observadas. Otra de las ventajas de este clasificador es que es un modelo de aprendizaje incremental. Esto quiere decir que es una técnica de inducción que se actualiza con cada nueva observación de entrenamiento, es decir, es necesario volver procesar todo el conjunto de entrenamiento cuando se dispone de nuevas observaciones.El ejemplo presentado en el Cap. 24 en el que se buscaba predecir si se podría jugar o al tenis bajo unas condiciones meteorológicas determinadas, puede desarrollarse utilizando el modelo Naive Bayes. En este caso, el procedimiento puede resumirse en tres pasos:Resumir los datos en una tabla de frecuencias.Generar una tabla de verosimilitud obteniendo las probabilidades de las variables.Aplicar el teorema de Bayes para calcular la probabilidad posteriori.De este modo, las 15 observaciones registradas con el tipo de día (soleado, nublado, lluvioso) y si ese día se jugó, deben resumirse en una tabla de frecuencias como la Tabla 27.1. En este primer paso se tiene en cuenta la información sobre humedad o viento.Tabla 27.1:  Tabla de frecuencias - Tipo de día vs Jugar partidoEn un segundo paso, se obtienen las probabilidades de cada categoría partir de la Tabla 27.1 resultando en la Tabla 27.2.Tabla 27.2:  Tabla de verosimilitud - Tipo de día vs Jugar partidoA partir de la Tabla 27.2 se obtiene la probabilidad de cada tipo de día dado que con esa climatología se jugó o , es decir, \\(P(\\textrm{Tipo de día}|Jugar)\\). Obteniendo las probabilidades mostradas en la Tabla 27.3Tabla 27.3:  Probabilidad de Tipo de día sabiendo si se jugó el partidoEste proceso se repite de forma independiente para las variables viento y humedad obteniendo la Tabla 27.4 y la Tabla 27.5 respectivamente.Tabla 27.4:  Probabilidad fuerza del Viento sabiendo si se jugó el partidoTabla 27.5:  Probabilidad nivel de Humedad sabiendo si se jugó el partidoFinalmente, aplicando el teorema de Bayes se podría predecir si se juega o el partido ante la previsión de un nuevo día. Por ejemplo, ¿cuál es la probabilidad de jugar al tenis si el día se espera soleado, con fuertes rachas de viento y escasa humedad? Esto es, \\(\\ell\\)=[Soleado, Fuerte, Débil] y, de acuerdo al teorema de Bayes, esta pregunta se respondería través de:\\[\\begin{equation*}\nP(c|\\ell) = \\frac{P(\\ell|c)\\cdot P(c)}{P(\\ell)}\n\\end{equation*}\\]partir de las probabilidades previamente obtenidas y de la asunción de independencia entre las variables, se puede calcular la probabilidad de jugar como:\\[\\begin{equation}\nP(Si|\\ell) = P(Soleado|Si)\\cdot P(Fuerte|Si) \\cdot P (Débil|Si) \\cdot P(Si) = \\frac{2}{15}\\frac{4}{15}\\frac{2}{15}\\frac{10}{15} = 0,0032\n\\end{equation}\\]\\[\\begin{equation}\nP(|\\ell) = P(Soleado|)\\cdot P(Fuerte|) \\cdot P (Débil|) \\cdot P() = \\frac{4}{15}\\frac{3}{15}\\frac{1}{15}\\frac{5}{15} = 0,0012\n\\end{equation}\\]La probabilidad de jugar es superior la probabilidad de jugar y, por tanto, dado un día con esas condiciones climáticas se clasificará como un día en el que se puede jugar.","code":""},{"path":"cap-naive-bayes.html","id":"procedimiento-con-r-la-función-naive_bayes","chapter":"Capítulo 27 Naive Bayes","heading":"27.4 Procedimiento con R: la función naive_bayes()","text":"En el paquete naivebayes de R se encuentra la función naive_bayes() que se utiliza para entrenar un modelo Naive Bayes:formula: refleja la relación lineal entre la variable dependiente y los predictores \\(Y \\sim X_1 + ... + X_p\\).data: conjunto de datos con el que se entrena el modelo.prior: vector con las probabilidades priori de las clases.","code":"\nnaive_bayes(formula, data, prior = ..., ...)"},{"path":"cap-naive-bayes.html","id":"clasificación-de-clientes-utilizando-el-modelo-naive-bayes","chapter":"Capítulo 27 Naive Bayes","heading":"27.5 Clasificación de clientes utilizando el modelo Naive Bayes","text":"Como en los capítulos precedentes, en este ejemplo se pretende entrenar un modelo Naive Bayes utilizando el conjunto de datos de compras realizadas por clientes incluido en el paquete CDR. Este conjunto de datos cuenta con unas variables predictoras que indican qué productos han comprado los clientes, el importe que han gastado y otras características como su edad y su nivel educativo. Se utiliza el conjunto de datos sin transformar (dp_entr), es decir, en su escala original y con las variables categóricas sin codificar. La variable objetivo indica si un cliente comprará o el nuevo producto (tensiómetro digital).Los resultados del proceso de entrenamiento muestran que, en este caso, es indiferente indicar el argumento usekernel como FALSE o TRUE, los resultados de precisión son equivalentes. El resumen del modelo muestra que la precisión media obtenida durante la validación cruzada alcanza el 85,1%, lo cual indica que el modelo ajusta bastante bien la intención de compra de nuevos clientes.En la matriz de confusión del modelo se observa para cada celda el promedio porcentual entre remuestreos. Así, se observa que en media el modelo predice mejor cuando un cliente va comprar el nuevo producto que cuando sí lo hace, aunque con mucha diferencia (menos de un 2%). En ambos casos, las clasificaciones erróneas suponen ni el 10%.\nFigura 27.1: Resultados del modelo Naive Bayes obtenidos durante el proceso de validación cruzada.\nSe puede observar como la precisión oscila entre el 75% y el 95%, aunque en uno de los resultados se obtuvo un 96% de precisión, el cual se marca como un resultado atípico.","code":"\nlibrary(\"caret\")\nlibrary(\"naivebayes\")\nlibrary(\"reshape\")\nlibrary(\"ggplot2\")\nlibrary(\"CDR\")\n\ndata(\"dp_entr\")\n# se fija la semilla aleatoria\nset.seed(101)\n\n# se entrena el modelo\nmodel <- train(CLS_PRO_pro13 ~ .,\n            data=dp_entr,\n            method=\"nb\", \n            metric=\"Accuracy\",\n            trControl=trainControl(classProbs = TRUE,\n                                   method = \"cv\",\n                                   number = 10))\n# se muestra la salida del modelo\nmodelNaive Bayes \n\n558 samples\n 17 predictor\n  2 classes: 'S', 'N' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 502, 502, 502, 503, 503, 502, ... \nResampling results across tuning parameters:\n\n  usekernel  Accuracy   Kappa    \n  FALSE      0.8512662  0.7026716\n   TRUE      0.8512338  0.7025165\n\nTuning parameter 'fL' was held constant at a value of 0\nTuning parameter\n 'adjust' was held constant at a value of 1\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were fL = 0, usekernel = FALSE and adjust = 1.confusionMatrix(model)\nCross-Validated (10 fold) Confusion Matrix \n\n(entries are percentual average cell counts across resamples)\n \n          Reference\nPrediction    S    N\n         S 41.8  6.6\n         N  8.2 43.4\n                            \n Accuracy (average) : 0.8513\nggplot(melt(model$resample[,-4]), aes(x = variable, y = value, fill=variable)) + \n  geom_boxplot(show.legend=FALSE) + \n  xlab(NULL) + ylab(NULL)"},{"path":"cap-naive-bayes.html","id":"resumen-24","chapter":"Capítulo 27 Naive Bayes","heading":"Resumen","text":"En este capítulo se introduce al lector en el algoritmo de Naive Bayes, en concreto:Se presentan los fundamentos del algoritmo bayesiano, particularmente el Teorema de Bayes.Se explica el funcionamiento del algoritmo Naive Bayes y su relación con dicho Teorema de Bayes.Se demuestra su aplicabilidad un caso real de clasificación través de R.","code":""},{"path":"cap-bagg-rf.html","id":"cap-bagg-rf","chapter":"Capítulo 28 Métodos ensamblados: bagging y random forest","heading":"Capítulo 28 Métodos ensamblados: bagging y random forest","text":"Ramón . Carrasco\\(^{}\\) e Itzcóatl Bueno\\(^{b,}\\)\\(^{}\\)Universidad Complutense de Madrid\n\\(^{b}\\)Instituto Nacional de Estadística","code":""},{"path":"cap-bagg-rf.html","id":"introducción-a-los-métodos-ensamblados","chapter":"Capítulo 28 Métodos ensamblados: bagging y random forest","heading":"28.1 Introducción a los métodos ensamblados","text":"Puede ocurrir que ninguno de los algoritmos hasta ahora presentados (Caps. 24, 25, 26 y 27) proporcionen resultados convincentes para el problema que se quiere modelar. El aprendizaje ensamblado (Zhou 2012) es un paradigma que, como muestra la Fig. 28.1, en lugar de entrenar un modelo muy preciso, se centra en entrenar un gran número de modelos con menor precisión, y después combinar sus predicciones para obtener un metamodelo de una precisión más alta.\nFigura 28.1: Esquema de un metamodelo.\nlos modelos de menor precisión se les suele nombrar como algoritmos “débiles”, es decir, algoritmos con menor capacidad de aprender patrones complejos en los datos.Por tanto, generalmente, son rápidos tanto en tiempo de entrenamiento como de procesamiento. Existen dos paradigmas de aprendizaje ensamblado: el bagging y el boosting (Cap. 29).","code":""},{"path":"cap-bagg-rf.html","id":"bagging","chapter":"Capítulo 28 Métodos ensamblados: bagging y random forest","heading":"28.2 Bagging","text":"En lugar de buscar la división más eficiente en cada capa, como ocurre en el árbol de decisión, una alternativa sería construir un metamodelo combinando los resultados de múltiples árboles de decisión. Esta técnica se conoce como bagging y consiste en construir varios árboles utilizando una selección aleatoria de los datos que se utilizan para cada árbol y, finalmente, combinar la predicción de cada uno de ellos través de la media (en el caso de regresión) o mediante un sistema de votación (en el caso de un problema de clasificación).La principal característica del bagging es el llamado muestreo bootstrap. La idea básica del bootstrap es que la inferencia sobre una población se haga partir de una muestra, tomando el papel de población y se remuestree, permitiendo comparar valor poblacional y el valor muestral. En el bagging, el objetivo de este remuestreo es que cada árbol esté entrenado con una muestra única, y por tanto, generen respuestas únicas,esto es modelos débiles distintos. Para ello, debe existir aleatoriedad y variación en cada árbol que conforme el modelo final, puesto que tendría sentido construir varios árboles idénticos. Como se ha comentado, este problema queda resuelto por el muestreo bootstrap, el cual extrae una muestra aleatoria de los datos en cada ronda. En el caso del bagging, se extraen distintas muestras de datos para el entrenamiento de cada árbol. Aunque esto elimina la problemática del sobreajuste, los patrones presentes en el conjunto de datos aparecerán en la mayoría de los árboles entrenados y, por tanto, en la predicción final. Es por ello que el bagging es una técnica de gran eficacia para el tratamiento de los valores atípicos y para la reducción de la varianza que generalmente afecta un modelo compuesto por un único árbol de decisión.","code":""},{"path":"cap-bagg-rf.html","id":"rbagging","chapter":"Capítulo 28 Métodos ensamblados: bagging y random forest","heading":"28.2.1 Procedimiento con R: la función bagging()","text":"En el paquete ipred de R se encuentra la función bagging() que se utiliza para entrenar un modelo bagging:formula: Refleja la relación lineal entre la variable dependiente y los predictores \\(Y \\sim X_1 + ... + X_p\\).data: Conjunto de datos con el que se entrena el modelo.nbagg: Número de replicaciones bootstrap.coob: Indica si se debe calcular una estimación del ratio de error de predicción.","code":"\nbagging(formula, data, ...)"},{"path":"cap-bagg-rf.html","id":"implementando-bagging-en-r","chapter":"Capítulo 28 Métodos ensamblados: bagging y random forest","heading":"28.2.2 Implementando bagging en R","text":"Es posible la implementación de un modelo de predicción de agregación bootstrap en R. Para ello, se pueden utilizar múltiples funciones como la ya mencionada en la Sec. 28.2.1 bagging(). En este ejemplo se utilizan los datos sobre compras de clientes dp_entr del paquete CDR, cuyo objetivo es clasificar los clientes entre quienes comprarían un nuevo producto y quienes .El error de clasificación de este modelo es del 14,16%, o lo que es equivalente, el modelo tiene una precisión del 85,84%. Desafortunadamente, bagging() selecciona el número óptimo de replicaciones reduciendo el error de clasificación. Para seleccionar el número de replicaciones que minimice el error, se puede graficar la curva de error por número de replicaciones como en la Fig. 28.2. Se itera el modelo variando los valores del hiperparámetro nbagg (en este ejemplo entre 10 y 150, incrementándose de cinco en cinco). Se observa que el error mínimo (13,79%) se obtiene al establecer el hiperparámetro igual 60.\nFigura 28.2: Número de replicaciones vs Error de clasificación.\nLa función train() del paquete caret es otro método para entrenar un algoritmo de bagging en R. Para ello, el argumento method debe tomar el valor \"treebag\". Sin embargo, este algoritmo incluye hiperparámetros optimizar. Dado que se ha obtenido recursivamente el número óptimo de replicaciones, se puede entrenar el modelo con el valor obtenido y comprobar que el error dado coincide. Se observa que si se entrena un modelo bagging con 60 replicaciones, la precisión del modelo es del 86,93%. Esto es aproximadamente el resultado obtenido anteriormente en el que para 60 replicaciones el modelo tenía un error de clasificación del 13,79%.","code":"\nlibrary(\"CDR\")\nlibrary(\"ipred\")\nlibrary(\"caret\")\nlibrary(\"reshape\")\nlibrary(\"ggplot2\")\n\ndata(\"dp_entr\")\n# se fija la semilla aleatoria\nset.seed(101)\n\n# Se entrena el modelo\nbag_model <- bagging(\n  formula = CLS_PRO_pro13 ~ .,\n  data = dp_entr,\n  nbagg = 100,  \n  coob = TRUE,\n  control = rpart.control(minsplit = 2, cp = 0)\n)bag_model\n\nBagging classification trees with 100 bootstrap replications \n\nCall: bagging.data.frame(formula = CLS_PRO_pro13 ~ ., data = dp_entr, \n    nbagg = 100, coob = TRUE, control = rpart.control(minsplit = 2, \n        cp = 0))\n\nOut-of-bag estimate of misclassification error:  0.1416 \nmissclass <- c() # vector vacio para recopilar el error en cada iteración\nfor (n in seq(10,150,5)) { # valores a probar para nbagg\n  # se establece la semilla aleatoria\n  set.seed(101)\n  # se entrena el modelo\n  bag_model <- bagging(\n  formula = CLS_PRO_pro13 ~ .,\n  data = dp_entr,\n  nbagg = n,  \n  coob = TRUE,\n  control = rpart.control(minsplit = 2, cp = 0)\n  )\n  # se agrega el error de esta iteración\n  missclass <- c(missclass, bag_model$err) # se agrega el error de esta iteración\n}\nplot(seq(10,150,5),missclass,type = \"l\",xlab = \"Número de árboles\", ylab=\"Missclassification error\")\nset.seed(101)\nmodel_bag <- train(\n  CLS_PRO_pro13 ~ .,\n  data = dp_entr,\n  method = \"treebag\",\n  trControl = trainControl(method = \"cv\", number = 10),\n  nbagg = 60, \n  control = rpart.control(minsplit = 2, cp = 0)\n)model_bag\n\nBagged CART \n\n558 samples\n 17 predictor\n  2 classes: 'S', 'N' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 502, 502, 502, 503, 503, 502, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.8692532  0.7385449"},{"path":"cap-bagg-rf.html","id":"interpretación-de-variables-en-el-bagging","chapter":"Capítulo 28 Métodos ensamblados: bagging y random forest","heading":"28.2.3 Interpretación de variables en el bagging","text":"Una de las principales desventajas de los algoritmos ensamblados (incluido el bagging) es que mientras que los modelos base son interpretables, el metamodelo resultante lo es. Pese esto, aún es posible hacer inferencia de cómo cada una de las variables influye en el modelo entrenado. La manera de medir la importancia de las variables incluidas en un árbol es registrar para cada variable la reducción de la función de pérdida que se le atribuye en cada partición. Dado que una variable puede utilizarse varias veces para dividir el árbol, la importancia total de esa variable será la suma de la reducción de la función de pérdida que se le atribuya por todas las particiones en las que intervenga. Este proceso es similar para el bagging. En este caso, para cada árbol se calcula la reducción de la función de pérdida en todas las divisiones. Tras esto, se agrega esta medida en todos los árboles que forman el metamodelo. El paquete ipred, en el que se encuentra la función bagging(), captura la información requerida para calcular la importancia de las variables. Sin embargo, el paquete caret si lo hace y se puede construir un gráfico de importancia utilizando la función vip() del paquete vip.\nFigura 28.3: Importancia de las variables incluidas en el modelo bagging.\nLa Fig. 28.3 muestra que las variables más importantes en el modelo bagging entrenado para predecir si un cliente comprará o el tensiómetro digital son: si ha comprado la depiladora eléctrica, cuánto importe ha gastado en ese producto, si ha comprado el estimulador muscular y si ha comprado el smartchwatch fitness.","code":"\nlibrary(\"vip\")\nvip(model_bag, num_features = 15,\n    aesthetics = list(color = \"skyblue\", fill = \"skyblue\"))"},{"path":"cap-bagg-rf.html","id":"random-forest","chapter":"Capítulo 28 Métodos ensamblados: bagging y random forest","heading":"28.3 Random Forest","text":"El bagging es el paradigma tras el algoritmo de random forest. Este algoritmo fue desarrollado por primera vez por (T. K. Ho 1995). Sin embargo, fueron (Cutler Zhao 1999) y (Breiman 2001) quienes desarrollaron una versión extendida del modelo y registraron Random Forest como marca comercial. Este algoritmo básico de bagging funciona del siguiente modo: partir del conjunto de datos de entrenamiento se generan \\(K\\) muestras aleatorias \\(\\mathbb{S}_{k}\\), se entrena un modelo de árbol de decisión (\\(f_k\\)) utilizando la muestra \\(\\mathbb{S}_{k}\\) como conjunto de entrenamiento. Tras el entrenamiento, se dispone de \\(K\\) árboles de decisión, como se observa en la Fig. 28.4. La predicción de una nueva observación \\(x\\) se obtiene como la media de las \\(K\\) predicciones:\\[\\begin{equation}\ny\\leftarrow\\hat{f}(x)=\\frac{1}{K}\\sum^{K}_{k=1}f_{k}(x)\n\\end{equation}\\]En el caso de regresión, o por la votación por mayoría en el caso de clasificación.Tanto el bagging como el random forest desarrollan múltiples árboles y utilizan el muestreo bootstrap para la aleatorización de los datos. Sin embargo, el random forest establece una limitación artificial la selección de variables al considerar todas en cada árbol.\nFigura 28.4: Ejemplo de Random Forest.\nEl bagging considera las mismas variables para construir cada árbol con el objetivo de minimizar su entropía, y, por tanto, todos los árboles suelen tener un aspecto similar. Esto lleva que las predicciones dadas por los árboles estén altamente correlacionadas. El modelo random forest evita este problema estableciendo la obligación, en cada división, de utilizar un subconjunto de las variables. Esto proporciona algunas variables mayor probabilidad de ser seleccionadas, y al generar árboles únicos y correlacionados se consigue una estructura de decisión final más fiable.En general, es mejor que el random forest esté formado por una gran cantidad de árboles (por lo menos 100) para suavizar el impacto de valores atípicos. Sin embargo, la tasa de efectividad disminuye medida que se incorporan más árboles. Llegado cierto punto, los nuevos árboles aportan una mejora significativa al modelo, pero si incrementan los tiempos de procesamiento.El modelo random forest es rápido de entrenar y es una buena técnica para obtener un modelo de referencia. Aunque estos modelos funcionan bien en la interpretación de patrones complejos y son versátiles, otras técnicas, como por ejemplo el gradient boosting (Cap. 29), proporcionan una mayor precisión en las predicciones en muchos casos.Estos modelos se han vuelto populares porque tienden proporcionar un muy buen rendimiento con los parámetros predeterminados en las distintas implementaciones. En efecto, pesar de tener muchos hiperparámetros que pueden ser ajustados, los valores por defecto de dichos hiperparámetros tienden ofrecer buenos resultados en la predicción. Los hiperparámetros más importantes que hay que ajustar al entrenar un modelo random forest son: el número de árboles (\\(K\\)), el número de variables incluidos en el subconjunto aleatorio en cada división (mtry), la complejidad de cada árbol, el esquema de muestreo y la regla de división utilizar durante la construcción del árbol.","code":""},{"path":"cap-bagg-rf.html","id":"número-de-árboles-k","chapter":"Capítulo 28 Métodos ensamblados: bagging y random forest","heading":"28.3.1 Número de árboles (\\(K\\))","text":"El primer hiperparámetro ajustar es el número de árboles que componen el modelo de random forest. Su valor debe ser lo suficientemente grande como para que la tasa de error se estabilice. La regla general es que el valor mínimo de árboles sea igual 10 veces el número de variables incluidas en el modelo. Sin embargo, cuando se tienen en cuenta otros hiperparámetros para optimizar, es posible que el número de árboles se vea afectado. El tiempo de procesamiento aumenta linealmente con la cantidad de árboles incluidos, pero cuantos más se incluyan, se obtendrán estimaciones de error más estables.","code":""},{"path":"cap-bagg-rf.html","id":"número-de-variables-a-considerar-mtry","chapter":"Capítulo 28 Métodos ensamblados: bagging y random forest","heading":"28.3.2 Número de variables a considerar (mtry)","text":"mtry se refiere al hiperparámetro encargado de controlar la aleatorización de variables utilizadas para las particiones de los árboles. Este hiperparámetro ayuda equilibrar la baja correlación del árbol con los demás, y una razonable fuerza predictiva. Existe un valor predeterminado para este hiperparámetro el cual se puede utilizar en caso de querer o poder ajustarlo. En el caso de la regresión, se determina que \\(mtry=\\frac{p}{3}\\) siendo \\(p\\) el número de variables incluidas en el modelo. Y en problemas de clasificación, el valor predeterminado es \\(mtry=\\sqrt p\\). Cuando hay pocas variables relevantes, es decir, los datos son muy ruidosos, tiende funcionar mejor que el valor de mtry sea alto, pues hace que sea más probable seleccionar esas variables. En cambio, cuando muchas variables son importantes, funciona mejor un valor bajo de mtry.","code":""},{"path":"cap-bagg-rf.html","id":"complejidad-de-los-árboles","chapter":"Capítulo 28 Métodos ensamblados: bagging y random forest","heading":"28.3.3 Complejidad de los árboles","text":"Un modelo random forest se construye con árboles de decisión los que se les puede controlar su profundidad y su complejidad como se vio en el Cap. 24. Esto se puede hacer ajustando los hiperparámetros de profundidad máxima permitida, tamaño del nodo o la cantidad máxima de nodos terminales.El tamaño del nodo es el hiperparámetro más común para controlar la complejidad del árbol y la mayoría de las implementaciones usan los valores predeterminados de 1 para árboles de clasificación y 5 para los árboles de regresión, dado que estos valores tienden producir buenos resultados. Si se quiere controlar el tiempo de procesamiento, se pueden conseguir reducciones significativas del tiempo aumentando el tamaño del nodo impactando de manera marginal en la estimación del error.","code":""},{"path":"cap-bagg-rf.html","id":"esquema-de-muestreo","chapter":"Capítulo 28 Métodos ensamblados: bagging y random forest","heading":"28.3.4 Esquema de muestreo","text":"Por defecto, el random forest tiene como esquema de muestreo el bootstrapping, explicado anteriormente, en el cual todas las observaciones se muestrean con reemplazo. Todas las replicaciones de bootstrap tienen el mismo tamaño que el conjunto de datos de entrenamiento. Sin embargo, el esquema de muestreo se puede ajustar tanto en el tamaño de la muestra como en el diseño muestral (con o sin reposición). El hiperparámetro de tamaño de muestra determina cuántas observaciones se extraen para el entrenamiento de cada árbol. Cuanto menor sea el tamaño muestral, menor será la correlación entre los árboles, lo cual puede llevar mejores resultados de precisión en la predicción. La forma de determinar el tamaño muestral óptimo puede hallarse evaluando algunos valores que oscilen entre el 25% y el 100%, y en el caso de que haya variables balanceadas respecto los valores de las categóricas se puede intentar muestrear sin reposición.","code":""},{"path":"cap-bagg-rf.html","id":"regla-de-división","chapter":"Capítulo 28 Métodos ensamblados: bagging y random forest","heading":"28.3.5 Regla de división","text":"Por defecto, la regla de división que utilizan los árboles de decisión que conforman un random forest es la que se presentó en el Cap. 24. Esto es, en el caso de regresión seleccionar la división que minimiza la desviación típica \\((\\sigma)\\); y en el caso de clasificación la división que minimiza la impureza de Gini o la entropía.","code":""},{"path":"cap-bagg-rf.html","id":"procedimiento-con-r-la-función-randomforest","chapter":"Capítulo 28 Métodos ensamblados: bagging y random forest","heading":"28.3.6 Procedimiento con R: la función randomForest()","text":"En el paquete randomForest de R se encuentra la función randomForest() que se utiliza para entrenar un modelo de este tipo:formula: Refleja la relación entre la variable dependiente \\(Y\\) y los predictores tal que \\(Y \\sim X_1 + ... + X_p\\).data: Conjunto de datos con el que entrenar el árbol de acuerdo la fórmula indicada.x: Conjunto de datos de entrenamiento que contiene los predictoresy: Vector respuesta con las clases o valores de la variable respuesta.xtest: Conjunto de datos que contiene los predictores del conjunto de datos de validación.ytest: Variable respuesta del conjunto de datos de validación.ntree: Número de árboles construir en el modelo.mtry: Número de variables muestreadas aleatoriamente como candidatas en cada partición.","code":"\nrandomForest(formula, data=..., ...)\nrandomForest(x, y, xtest, ytest, ntree=500, mtry, ...)"},{"path":"cap-bagg-rf.html","id":"aplicación-del-modelo-random-forest-en-r","chapter":"Capítulo 28 Métodos ensamblados: bagging y random forest","heading":"28.3.7 Aplicación del modelo random forest en R","text":"En esta sección se aplica el modelo random forest al ejemplo de datos de retail incluido en el paquete CDR. Se carga el paquete y con ello, los datos dp_entr. Se busca predecir si un cliente va comprar o el nuevo producto de acuerdo los productos que ha consumido, el importe que gasta en ellos y otras características como, por ejemplo, su nivel educativo.Este algoritmo al estar basado en árboles de clasificación tiene los mismos requisitos para el entrenamiento que tenían dichos árboles, así se construye el modelo usando el conjunto de datos de entrenamiento.Los resultados de la validación cruzada se pueden ver en el siguiente boxplot. Se observa como la precisión oscila entre el 80% y el 95%. Además, se puede ver en el resultado del modelo que el hiperparámetro \\(mtry\\) se ha ajustado 10 variables.\nFigura 28.5: Resultados del modelo random forest durante el proceso de validación cruzada.\nFinalmente, aunque el random forest generado está compuesto por 500 árboles, se puede acceder cualquiera de ellos para estudiarlos en profundidad. Para ello, es necesario instalar el paquete reprtree desde el repositorio https://github.com/araastat/reprtree.Se pueden observar las decisiones que se toman en el árbol de forma tabulada, indicando qué variable se utiliza para la partición, cuál es el valor que decide la división, indicando si es un nodo terminal (-1) o (1) y la predicción del nodo, el cual es NA si es un nodo terminal.Este árbol se muestra en la Fig. 28.6.\nFigura 28.6: Árbol número 205 del random forest entrenado.\nSin embargo, el método por el que se representa gráficamente es muy claro y puede llevar confusión o dificultar la interpretación del árbol. Si se desea estudiar hasta cierto nivel del árbol, se puede incluir el argumento depth como en el ejemplo abajo mostrado, y que representa el mismo árbol con una profundidad de 5 ramas en la Fig. 28.7.\nFigura 28.7: Árbol número 205 del random forest entrenado hasta la capa 5.\n","code":"\nlibrary(\"CDR\")\nlibrary(\"randomForest\")\nlibrary(\"caret\")\nlibrary(\"reshape\")\nlibrary(\"ggplot2\")\ndata(dp_entr)\n# se fija la semilla aleatoria\nset.seed(101)\n\n# se entrena el modelo\nmodel <- train(CLS_PRO_pro13~., data=dp_entr_NUM, \n             method=\"rf\", metric=\"Accuracy\", ntree=500,\n             trControl=trainControl(method=\"cv\", \n                                    number=10, \n                                    classProbs = TRUE))model\n\nRandom Forest \n\n558 samples\n 19 predictor\n  2 classes: 'S', 'N' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 502, 502, 502, 503, 503, 502, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n   2    0.8602922  0.7206238\n  10    0.8620455  0.7241029\n  19    0.8620130  0.7240248\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 10.\nlibrary(\"devtools\")\nif(!('reprtree' %in% installed.packages())){\n  devtools::install_github('araastat/reprtree')\n}set.seed(101)\nrf <- randomForest(CLS_PRO_pro13~., data = dp_entr_NUM, ntree=500,\n                   mtry=unlist(model$bestTune))\n\n# se observa el árbol número 205\ntree205 <- getTree(rf, 205, labelVar=TRUE)\n\nhead(tree205[,-c(1,2)])\n      split var split point status prediction\n1 importe_pro15         100      1       <NA>\n2 importe_pro12          60      1       <NA>\n3 importe_pro16          90      1       <NA>\n4  ingresos_ano      156500      1       <NA>\n5 importe_pro17         150      1       <NA>\n6      anos_exp          33      1       <NA>\n  \ntail(tree205[,-c(1,2)])\n               split var split point status prediction\n120                 <NA>         0.0     -1          N\n121                 <NA>         0.0     -1          S\n122 des_nivel_edu.BASICO         0.5      1       <NA>\n123                 <NA>         0.0     -1          S\n124                 <NA>         0.0     -1          S\n125                 <NA>         0.0     -1          N\nlibrary(\"reprtree\")\nplot.getTree(rf, k=205)\nplot.getTree(rf, k=205, depth = 5)"},{"path":"cap-bagg-rf.html","id":"aplicación-del-modelo-random-forest-con-ajuste-automático","chapter":"Capítulo 28 Métodos ensamblados: bagging y random forest","heading":"28.3.7.1 Aplicación del modelo random forest con ajuste automático","text":"En este segundo ejemplo, se pretende mejorar la precisión del modelo anterior. Para ello, se ajusta de forma automática los hiperparámetros de dicho algoritmo. De los mencionados anteriormente, solo se va ajustar automáticamente mtry, que es el único incluido el método rf.Para ajustar el número de árboles y el resto de hiperparámetros, se puede iterar el modelo y probar distintos valores. En una red de opciones se incluyen los valores probar para el hiperparámetro mtry.continuación, se entrena el modelo para que se ajuste al valor de mtry que maximice el rendimiento predictivo del modelo.Mientras que en el ejemplo anterior el algoritmo sólo probó tres valores de mtry, esta vez se realiza una prueba exhaustiva de valores. En el primer ejemplo, el valor del hiperparámetro era mtry=10, pero ahora se ha reajustado mtry=2. Esto es equivalente decir que 2 variables seleccionadas en cada partición son suficientes, y que son necesarias 10 como en el ejemplo anterior. Finalmente, se puede observar en la Fig. 28.8 los resultados obtenidos durante la validación cruzada. Se observa cómo sólo la precisión es mayor que en el ejemplo anterior, sino que además los resultados tienen menos dispersión.\nFigura 28.8: Resultados obtenidos por el random forest con ajuste automático durante el proceso de validación cruzada.\n","code":"modelLookup(\"rf\")\n  model parameter                         label forReg forClass probModel\n1    rf      mtry #Randomly Selected Predictors   TRUE     TRUE      TRUE\n# Se especifica un rango de valores posibles de mtry\ntuneGrid <- expand.grid(mtry = 1:18)\n# se fija la semilla aleatoria\nset.seed(101)\n\n# se entrena el modelo\nmodel <- train(CLS_PRO_pro13 ~ ., data=dp_entr_NUM, \n               method = \"rf\", metric = \"Accuracy\",\n               tuneGrid = tuneGrid,\n               trControl = trainControl(classProbs = TRUE))model\n\nRandom Forest \n\n558 samples\n 19 predictor\n  2 classes: 'S', 'N' \n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 558, 558, 558, 558, 558, 558, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n   1    0.8641354  0.7283098\n   2    0.8650087  0.7298376\n   3    0.8629614  0.7256812\n   4    0.8635609  0.7268514\n   5    0.8639559  0.7276250\n   6    0.8612659  0.7222420\n   7    0.8604934  0.7206476\n   8    0.8610116  0.7216937\n   9    0.8590645  0.7177882\n  10    0.8589073  0.7174718\n  11    0.8607248  0.7211179\n  12    0.8583609  0.7163903\n  13    0.8587296  0.7170933\n  14    0.8587384  0.7171642\n  15    0.8583195  0.7163106\n  16    0.8585407  0.7167355\n  17    0.8573597  0.7144030\n  18    0.8581404  0.7159558\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.\nggplot(melt(model$resample[,-4]), aes(x = variable, y = value, fill=variable)) + \n  geom_boxplot(show.legend=FALSE) + \n  xlab(NULL) + ylab(NULL)"},{"path":"cap-bagg-rf.html","id":"resumen-25","chapter":"Capítulo 28 Métodos ensamblados: bagging y random forest","heading":"Resumen","text":"En este capítulo se introduce al lector en el bagging y el algoritmo de aprendizaje supervisado conocido como random forest, en concreto:Se presenta el concepto de aprendizaje ensamblado, y se profundiza en uno de sus paradigmas: el bagging.Se implementa el bagging en R través de un caso de clasificación binaria.Se expone cómo medir la importancia de las variables incluidas en un modelo bagging para facilitar su interpretación.Se explica el modelo random forest, fundamentado en los árboles decisión y en el bagging. Así como los hiperparámetros más importantes para ajustar el modelo de mayor precisión.Se presenta un ejemplo de clasificación binaria utilizando el modelo random forest en R.","code":""},{"path":"cap-boosting-xgboost.html","id":"cap-boosting-xgboost","chapter":"Capítulo 29 Boosting y el algoritmo XGBoost","heading":"Capítulo 29 Boosting y el algoritmo XGBoost","text":"Ramón . Carrasco\\(^{}\\) e Itzcóatl Bueno\\(^{b,}\\)\\(^{}\\)Universidad Complutense de Madrid\n\\(^{b}\\)Instituto Nacional de Estadística","code":""},{"path":"cap-boosting-xgboost.html","id":"métodos-ensamblados-bagging-vs-boosting","chapter":"Capítulo 29 Boosting y el algoritmo XGBoost","heading":"29.1 Métodos ensamblados: bagging vs boosting","text":"En el Cap. 28 se presentó la idea de métodos ensamblados. Una serie de modelos útiles cuando ningún modelo de aprendizaje supervisado es capaz de explicar bien la variable dependiente de interés. En el aprendizaje ensamblado se entrenan un gran número de modelos con menor precisión, y se combinan sus predicciones para obtener un metamodelo de una precisión más alta. Los dos modelos de aprendizaje ensamblado más populares son el bagging (Cap. 28) y el boosting.La principal diferencia entre ellos radica en cómo se combinan los modelos individuales para obtener una predicción final. Por ejemplo, si se quisiera organizar una fiesta y se tuviese que tomar una decisión sobre el tema para la decoración, se podría hacer tanto con bagging como con boosting. Si se utilizase el bagging, se pediría opinión distintos grupos de amigos. Después, se combinarán todas sus ideas para tomar una decisión final sobre la decoración de la fiesta. Por otro lado, si se utiliza boosting, en lugar de preguntarle diferentes grupos de amigos al mismo tiempo, se pediría un amigo en particular su opinión. Si su respuesta es convincente, entonces se busca la ayuda de otro amigo, quien se enfocará en mejorar la respuesta anterior. Este proceso continúa secuencialmente, solicitando la ayuda de diferentes amigos y construyendo sobre las respuestas anteriores para obtener una decisión final más precisa y refinada sobre la decoración de la fiesta.","code":""},{"path":"cap-boosting-xgboost.html","id":"qué-es-el-boosting","chapter":"Capítulo 29 Boosting y el algoritmo XGBoost","heading":"29.2 ¿Qué es el boosting?","text":"El boosting (Schapire Freund 2012) es el otro de los paradigmas de aprendizaje ensamblado, presentado en el Cap. 28. Como el bagging, el boosting agrega múltiples modelos con menor precisión (débiles) combinando sus predicciones para obtener un metamodelo con una precisión más alta. Los árboles de decisión son los modelos base o débiles que se usan más frecuentemente. En este caso, para llegar al metamodelo partir de los modelos base, es necesario introducir ponderaciones los árboles basándose en las clasificaciones erróneas del árbol entrenado previamente dicho árbol.El boosting reduce el problema del sobreajuste utilizando menos árboles que un modelo random forest. Mientras que agregar más árboles al random forest ayuda compensar el sobreajuste, también puede llevar un aumento del mismo y, por ello, hay que ser cauteloso la hora de agregar nuevos árboles. Sin embargo, el boosting aprende iterativamente de los errores en árboles anteriores, pudiendo llevar sobreajustar el modelo. Aunque este enfoque produce predicciones más precisas, muchas veces mejores la mayoría de algoritmos, puede llevar ajustar las observaciones atípicas. Es por esto que el random forest es una técnica más recomendada cuando se trabaja con conjuntos de datos muy complejos con un gran número de observaciones atípicas.Otra de las grandes desventajas del boosting es que su tiempo de procesamiento es muy elevado, puesto que su entrenamiento sigue una lógica secuencial. En el proceso de entrenamiento, un árbol debe esperar que el inmediatamente anterior sea entrenado, para iniciar su entrenamiento, y esto limita la escalabilidad del modelo. Mientras tanto, un random forest entrena los árboles en paralelo, lo que hace que su tiempo de procesamiento sea más rápido.Tanto los algoritmos de boosting como los de bagging presentan el inconveniente de la dificultad de interpretación que tienen respecto los árboles de decisión. En este aspecto estos algoritmos de ensamblado se pueden considerar como de caja negra.","code":""},{"path":"cap-boosting-xgboost.html","id":"gradient-boosting-gb","chapter":"Capítulo 29 Boosting y el algoritmo XGBoost","heading":"29.3 Gradient Boosting (GB)","text":"Uno de los algoritmos de boosting más conocidos es el gradient boosting . Mientras que el random forest seleccionaba combinaciones aleatorias de variables en cada proceso de construcción de un árbol, el gradient boosting selecciona variables que mejoren la precisión con cada nuevo árbol. Por lo tanto, la construcción del modelo es secuencial, puesto que cada nuevo árbol se construye utilizando información derivada del árbol anterior y, en consecuencia, la construcción de estos árboles son independientes. En cada iteración se registran los errores cometidos en los datos de entrenamiento y se tienen en cuenta para la siguiente ronda de entrenamiento. Además, se incorporan ponderaciones, como se observa en la Fig. 29.1 los datos basándose en los resultados de la iteración anterior. Las ponderaciones más altas se aplicarán las observaciones que fueron erróneamente clasificadas, y se dará tanta atención las bien clasificadas. Este proceso se repite hasta que se llega un nivel bajo de error. El resultado final se obtiene través de la media ponderada de las predicciones de los árboles de decisión.\nFigura 29.1: Ejemplo de Boosting.\nMatemáticamente, un algoritmo gradient boosting para clasificación sigue los pasos que continuación se detallan. Sea un problema de clasificación binaria y, asumiendo que se tienen \\(K\\) árboles de decisión de clasificación, la predicción del modelo ensamblado se obtiene utilizando la función sigmoidal, como en la regresión logística (Cap. 16), tal que:Donde \\(f(x)=\\sum_{\\kappa=1}^{K}{f_{\\kappa}(x)}\\) y \\(f_m\\) es un árbol de decisión. De nuevo, como en la regresión logística, se aplica el principio de máxima verosimilitud tratando de hallar una \\(f\\) que maximice \\(\\mathcal{L}_f = \\sum_{=1}^{N}{\\ln(P(y_i=1|x_i,f))}\\)El algoritmo, en origen, es un modelo constante de la forma \\(f=f_0=\\frac{p}{1-p}\\) donde \\(p=\\frac{1}{N}\\sum^{N}_{=1}\\). Tras cada iteración se añade un nuevo árbol \\(f_\\kappa\\) al modelo. Para encontrar el mejor árbol \\(f_\\kappa\\), la primera derivada parcial \\(g_i\\) del modelo actual se obtiene para \\(=1,\\dots,N\\):Donde \\(f\\) es el modelo de clasificación ensamblado construido en la iteración previa. Se necesita obtener las derivadas de \\(\\ln(P(y_i=1|x_i,f))\\) con respecto \\(f\\) para todo \\(\\) para poder calcular \\(g_i\\). Nótese que:Y, por tanto, la derivada respecto \\(f\\) es igual :Después, se reemplaza en el conjunto de entrenamiento la categoría original \\(y_i\\) por su correspondiente derivada parcial \\(g_i\\) y se construye un nuevo modelo \\(f_\\kappa\\) utilizando el conjunto de entrenamiento transformado. Tras esto, se obtiene la actualización óptima (\\(\\rho_\\kappa\\)) como:Al terminar la iteración \\(\\kappa\\), se actualiza el modelo ensamblado \\(f\\) añadiendo el nuevo árbol \\(f_\\kappa\\):Se itera hasta que \\(\\kappa=K\\), entonces el proceso se detiene y se obtiene el modelo ensamblado final \\(f\\).","code":""},{"path":"cap-boosting-xgboost.html","id":"hiperparámetros-del-modelo-gradient-boosting","chapter":"Capítulo 29 Boosting y el algoritmo XGBoost","heading":"29.3.1 Hiperparámetros del modelo gradient boosting","text":"Un modelo de gradient boosting tiene dos tipos de hiperparámetros:Hiperparámetros de boosting.Hiperparámetros del árbol.","code":""},{"path":"cap-boosting-xgboost.html","id":"hiperparámetros-de-boosting","chapter":"Capítulo 29 Boosting y el algoritmo XGBoost","heading":"29.3.1.1 Hiperparámetros de boosting","text":"Los hiperparámetros de boosting son principalmente dos: el número de árboles y la tasa de aprendizaje.El primero indica el número de árboles construir y que, como se ha comentado, es importante optimizar para evitar el sobreajuste del modelo. diferencia de los modelos random forest o bagging, en el boosting los árboles crecen en secuencia para que cada árbol corrija los errores del anterior. El número de árboles necesarios para que el modelo sea buen predictor puede verse incrementado en función de los valores que tomen los otros hiperparámetros.La tasa de aprendizaje es el hiperparámetro con el que se determina la contribución de cada árbol en el resultado final y controla la rapidez con la que el algoritmo avanza por el descenso del gradiente, es decir, la velocidad la que aprende. Este hiperparámetro toma valores entre 0 y 1, aunque los valores habituales oscilan entre 0,001 y 0,3. El modelo es más robusto las características específicas de cada árbol, permitiendo una buena generalización, cuando la tasa de aprendizaje toma valores bajos. Estos valores también facilitan la parada temprana antes del sobreajuste del modelo. Sin embargo, utilizar estos valores vuelve al modelo más exigente computacionalmente y dificulta alcanzar el modelo óptimo con un número fijo de árboles. En resumen, cuanto menor sea este valor, más preciso puede ser el modelo, pero también requerirá más árboles en la secuencia.","code":""},{"path":"cap-boosting-xgboost.html","id":"hiperparámetros-de-árbol","chapter":"Capítulo 29 Boosting y el algoritmo XGBoost","heading":"29.3.1.2 Hiperparámetros de árbol","text":"Los principales hiperparámetros de árbol son: la profundidad del árbol y el número mínimo de observaciones en nodos terminales, como se vio en el Cap. 24.El primer hiperparámetro controla la profundidad de los árboles individuales. Los valores habituales de profundidad oscilan entre 3 y 8. Los árboles de menor profundidad son eficientes computacionalmente, pero menos precisos. Sin embargo, los árboles de mayor profundidad permiten que el algoritmo capture interacciones únicas, aunque aumentan el riesgo de sobreajuste.El segundo hiperparámetro, además de controlar el número mínimo de observaciones en los nodos terminales, controla la complejidad de cada árbol. Los valores típicos de este hiperparámetro suelen estar entre 5 y 15. Los valores más altos ayudan evitar que un modelo aprenda relaciones que pueden ser muy específicas de la muestra particular seleccionada para entrenar el árbol, evitando así el sobreajuste. Sin embargo, los valores más pequeños pueden ayudar con clases desbalanceadas en problemas de clasificación.","code":""},{"path":"cap-boosting-xgboost.html","id":"estrategia-de-ajuste-de-hiperparámetros","chapter":"Capítulo 29 Boosting y el algoritmo XGBoost","heading":"29.3.2 Estrategia de ajuste de hiperparámetros","text":"diferencia del random forest, los modelos gradient boosting pueden variar mucho en su precisión de acuerdo su configuración de hiperparámetros. Por ello, el ajuste puede requerir seguir una estrategia. Un buen enfoque para esto es:Elegir una tasa de aprendizaje relativamente alta. El valor predeterminado es 0,1 y generalmente funciona. Sin embargo, para la mayoría de problemas funcionan valores entre 0,05 y 0,2.Determinar el número óptimo de árboles para la tasa de aprendizaje elegida.Ajustar los hiperparámetros del árbol y la tasa de aprendizaje y evaluar la velocidad frente al rendimiento.Ajustar los hiperparámetros específicos del árbol para determinar la tasa de aprendizaje.Una vez que se ajustan los parámetros específicos del árbol, se reduce la tasa de aprendizaje para evaluar cualquier mejora en la precisión.Utilizar la configuración final de hiperparámetros y aumentar los procedimientos de validación cruzada para obtener estimaciones más robustas. Si se utiliza validación cruzada en los pasos anteriores, entonces este paso es necesario.","code":""},{"path":"cap-boosting-xgboost.html","id":"procedimiento-con-r-la-función-gbm","chapter":"Capítulo 29 Boosting y el algoritmo XGBoost","heading":"29.3.3 Procedimiento con R: la función gbm()","text":"En el paquete gbm de R se encuentra la función con el mismo nombre gbm() que se utiliza para entrenar un modelo gradient boosting:formula: Refleja la relación entre la variable dependiente \\(Y\\) y los predictores tal que \\(Y \\sim X_1 + ... + X_p\\).data: Conjunto de datos con el que entrenar el árbol de acuerdo la fórmula indicada.","code":"\ngbm(formula,data=..., ...)"},{"path":"cap-boosting-xgboost.html","id":"aplicación-del-modelo-gradient-boosting-en-r","chapter":"Capítulo 29 Boosting y el algoritmo XGBoost","heading":"29.3.4 Aplicación del modelo gradient boosting en R","text":"través de los datos de compras dp_entr incluidos en el paquete CDR se va aplicar el modelo gradient boosting para clasificar qué clientes van comprar un nuevo producto (tensiómetro digital) y quienes . Se entrena el modelo utilizando el conjunto de datos de entrenamiento sin transformar (en su escala original). Así, en lugar de tener las variables categóricas transformadas mediante one-hot-encoding se usan en su escala original, como ocurre con el caso de la variable que mide el nivel educativo.El modelo resultante del proceso de entrenamiento es un gradient boosting que ha ajustado los hiperparámetros 150 árboles y una profundidad igual 3. Además, los valores tanto del número mínimo de observaciones en nodos como de la tasa de aprendizaje, toman los valores por defecto de 10 y 0,1, respectivamente. Los resultados en el proceso de validación cruzada se muestran en la Fig. 29.2, en el que se observa como la precisión oscila entre el 84% y el 93% en las iteraciones.\nFigura 29.2: Resultados del modelo GB obtenidos durante el proceso de validación cruzada.\n","code":"\nlibrary(\"CDR\")\nlibrary(\"caret\")\nlibrary(\"gbm\")\nlibrary(\"reshape\")\nlibrary(\"ggplot2\")\ndata(dp_entr)\n# se determina la semilla aleatoria\nset.seed(101)\n\n# se entrena el modelo\nmodel <- train(CLS_PRO_pro13 ~ ., \n            data=dp_entr, \n            method=\"gbm\", \n            metric=\"Accuracy\",\n            trControl = trainControl(classProbs = TRUE, \n                                     method = \"cv\", number = 10)\n            )model\n\nStochastic Gradient Boosting \n\n558 samples\n 17 predictor\n  2 classes: 'S', 'N' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 502, 502, 502, 503, 503, 502, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  Accuracy   Kappa    \n  1                   50      0.8564610  0.7130031\n  1                  100      0.8690909  0.7383556\n  1                  150      0.8762338  0.7526413\n  2                   50      0.8690909  0.7382344\n  2                  100      0.8762338  0.7526413\n  2                  150      0.8799026  0.7599227\n  3                   50      0.8763636  0.7528004\n  3                  100      0.8781494  0.7563575\n  3                  150      0.8835390  0.7671499\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\nTuning\n parameter 'n.minobsinnode' was held constant at a value of 10\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were n.trees = 150, interaction.depth =\n 3, shrinkage = 0.1 and n.minobsinnode = 10.\nggplot(melt(model$resample[,-4]), aes(x = variable, y = value, fill=variable)) + \n  geom_boxplot(show.legend=FALSE) + \n  xlab(NULL) + ylab(NULL)"},{"path":"cap-boosting-xgboost.html","id":"gradient-boosting-con-ajuste-automático","chapter":"Capítulo 29 Boosting y el algoritmo XGBoost","heading":"29.3.5 Gradient Boosting con ajuste automático","text":"Se repite el procedimiento para el ejemplo anterior. Sin embargo, en este ejemplo se ajustan de forma automática los hiperparámetros más relevantes de dicho algoritmo para mejorar los resultados respecto al modelo anterior. Se observa como los hiperparámetros ajustar para el método gbm son: el número de árboles, la profundidad, la tasa de aprendizaje y el número de observaciones en un nodo.Siguiendo la estrategia descrita se definen rangos de posibles valores para los principales hiperparámetros optimizar.Esta red de posibles valores para los hiperparámetros del modelo se incorporan la función de entrenamiento. Cuanto más exhaustivo sea el ajuste de estos valores, mayor será el tiempo de ajuste del modelo. La red presentada está formada por 81 combinaciones de los posibles cuatro hiperparámetros.El modelo que mejores resultados proporciona es aquel que ajusta los hiperparámetros los siguientes valores: 180 árboles, una profundidad igual 6, una tasa de aprendizaje de 0.05 y un tamaño mínimo de los nodos de 10 observaciones.En la Fig. 29.3 se muestran los resultados obtenidos durante el proceso de validación cruzada. Se puede ver que los resultados son similares los del modelo anterior, aunque hay diferencias importantes. En primer lugar, se alcanza un valor máximo de precisión mayor al anterior, pues en este caso la precisión oscila entre el 84% y el 95%. En segundo lugar, vemos que el valor mediano de la precisión ha subido del 87.5% del modelo anterior hasta el 90% de este modelo. Por último, que el rendimiento haya variado tan poco desde el modelo por defecto un modelo en el que se ha intentado ajustar los hiperparámetros, confirma lo ya expuesto sobre el buen rendimiento de un modelo de gradient boosting con los parámetros por defecto.\nFigura 29.3: Resultados del modelo GB con ajuste autmático obtenidos durante el proceso de validación cruzada.\n","code":"modelLookup(\"gbm\")\nmodel         parameter                   label forReg forClass probModel\n1   gbm           n.trees   # Boosting Iterations   TRUE     TRUE      TRUE\n2   gbm interaction.depth          Max Tree Depth   TRUE     TRUE      TRUE\n3   gbm         shrinkage               Shrinkage   TRUE     TRUE      TRUE\n4   gbm    n.minobsinnode Min. Terminal Node Size   TRUE     TRUE      TRUE\n# Se especifica un rango de valores posibles para los hiperparámetros\ntuneGrid <- expand.grid(interaction.depth = c(4,6,8),\n                        n.trees = c(10*ncol(dp_entr),300,500), \n                        shrinkage = c(0.05,0.1,0.2), \n                        n.minobsinnode = c(5,10,15))\n# se fija la semilla aleatoria\nset.seed(101)\n\n\n# se entrena el modelo\nmodel <- train(CLS_PRO_pro13~., \n            data=dp_entr, \n            method=\"gbm\", \n            metric=\"Accuracy\",\n            trControl=trainControl(classProbs = TRUE,\n                                   method=\"cv\", number=10),\n            tuneGrid=tuneGrid)model$bestTune\n   n.trees interaction.depth shrinkage n.minobsinnode\n13     180                 6      0.05             10\nggplot(melt(model$resample[,-4]), aes(x = variable, y = value, fill=variable)) + \n  geom_boxplot(show.legend=FALSE) + \n  xlab(NULL) + ylab(NULL)"},{"path":"cap-boosting-xgboost.html","id":"extreme-gradient-boosting-xgb","chapter":"Capítulo 29 Boosting y el algoritmo XGBoost","heading":"29.4 eXtreme Gradient Boosting (XGB)","text":"El eXtreme Gradient Boosting es una implementación eficiente y escalable del modelo gradient boosting. Este modelo, abreviado como XGBoost, es un paquete de código abierto en C++, Java, Python (Wade 2020), R, Julia, Perl y Scala. En R el modelo se incluye dentro del paquete xgboost (T. Chen et al. 2015). El paquete incluye un procedimiento para la solución eficiente de modelos lineales y un algoritmo de aprendizaje de árboles.El paquete es compatible con funciones objetivo de regresión, clasificación y ranking. Además, tiene varias características importantes:Velocidad: xgboost puede realizar automáticamente cálculos paralelos. Por lo general, es 10 veces más rápido que el modelo gradient boosting.Velocidad: xgboost puede realizar automáticamente cálculos paralelos. Por lo general, es 10 veces más rápido que el modelo gradient boosting.Tipo de entrada: xgboost toma varios tipos de datos de entrada en R:Tipo de entrada: xgboost toma varios tipos de datos de entrada en R:Matriz densa (matrix)Matriz dispersa (Matrix::dgCMatrix)Archivo de datos localesUn tipo de datos propio del paquete: xgb.DMatrixDispersión: xgboost acepta datos de entrada dispersos para los modelos incluidos.Dispersión: xgboost acepta datos de entrada dispersos para los modelos incluidos.Personalización: xgboost admite tanto funciones de objetivo y funciones de evaluación personalizadas.Personalización: xgboost admite tanto funciones de objetivo y funciones de evaluación personalizadas.Rendimiento: xgboost alcanza generalmente una mayor precisión.Rendimiento: xgboost alcanza generalmente una mayor precisión.","code":""},{"path":"cap-boosting-xgboost.html","id":"hiperparámetros-del-modelo-xgboost","chapter":"Capítulo 29 Boosting y el algoritmo XGBoost","heading":"29.4.1 Hiperparámetros del modelo XGBoost","text":"El modelo XGBoost proporciona los hiperparámetros que ya incluía el modelo gradient boosting referentes tanto al boosting como los árboles. Sin embargo, xgboost también proporciona hiperparámetros adicionales que pueden ayudar reducir las posibilidades de sobreajuste, lo que lleva una menor variabilidad de predicción y, por lo tanto, una mayor precisión. Estos hiperparámetros son: la regularización y el dropout.Los parámetros de regularización se incluyen para ayudar evitar el sobreajuste y reducir la complejidad del modelo. Existen tres hiperparámetros que tienen esta funcionalidad: gamma (\\(\\gamma\\)), alpha (\\(\\alpha\\)) y lambda (\\(\\lambda\\)). Gamma es un hiperparámetro de pseudo-regularización conocido como multiplicador Lagrangiano y controla la complejidad de un árbol dado. Este hiperparámetro establece que para hacer una partición adicional en un nodo es necesaria una reducción de pérdida mínima especificada por gamma. Al especificarlo, el modelo XGBoost hace crecer los árboles hasta una profundidad máxima establecida, pero en un paso de poda eliminará las divisiones que cumplan con la regularización \\(\\gamma\\). Este hiperparámetro toma valores entre 0 e infinito (\\(\\infty\\)), siguiendo la regla de que mayor valor, mayor será la regularización. Los otros hiperparámetros de regularización, \\(\\alpha\\) y \\(\\lambda\\), son más clásicos. Mientras que \\(\\alpha\\) proporciona una regularización \\(L_1\\), \\(\\lambda\\) proporciona una regularización \\(L_2\\). Estos parámetros de regularización establecen un límite cómo de extremos pueden llegar ser los pesos de los nodos en un árbol. Sus valores se encuentran, al igual que los de \\(\\gamma\\), entre 0 y \\(\\infty\\).El dropout es un enfoque alternativo para reducir el sobreajuste. Cuando se entrena un modelo de gradient boosting, los primeros árboles tienden dominar el rendimiento del modelo, mientras que los que se agregan después suelen mejorar la predicción solo para un pequeño grupo de variables. Esto puede llevar que se incremente el riesgo de sobreajuste. Con el dropout, se descartan árboles aleatoriamente en el proceso de entrenamiento.En su implementación en R, el modelo XGBoost incluye principalmente los siguientes parámetros para ser optimizados: número de iteraciones, profundidad máxima de los árboles, tasa de aprendizaje y la regularización \\(\\gamma\\).","code":"head(modelLookup(\"xgbTree\"),4)\n    model parameter                  label forReg forClass probModel\n1 xgbTree   nrounds  # Boosting Iterations   TRUE     TRUE      TRUE\n2 xgbTree max_depth         Max Tree Depth   TRUE     TRUE      TRUE\n3 xgbTree       eta              Shrinkage   TRUE     TRUE      TRUE\n4 xgbTree     gamma Minimum Loss Reduction   TRUE     TRUE      TRUE"},{"path":"cap-boosting-xgboost.html","id":"procedimiento-con-r-la-función-xgboost","chapter":"Capítulo 29 Boosting y el algoritmo XGBoost","heading":"29.4.2 Procedimiento con R: la función xgboost()","text":"En el paquete xgboost de R se encuentra la función xgboost() que se utiliza para entrenar un modelo extreme gradient boosting:data: Conjunto de datos con el que entrenar el modelo.label: Vector con la variable respuesta.","code":"\nxgboost(data = ..., label = ..., ...)"},{"path":"cap-boosting-xgboost.html","id":"aplicación-del-modelo-xgboost-en-r","chapter":"Capítulo 29 Boosting y el algoritmo XGBoost","heading":"29.4.3 Aplicación del modelo XGBoost en R","text":"Se entrena este modelo utilizando el conjunto de entrenamiento sin transformar (en su escala original). Se continúa así el ejemplo expuesto durante la aplicación del modelo gradient boosting sin y con ajuste automático de sus hiperparámetros. Se repite el procedimiento de entrenar el modelo para los hiperparámetros por defecto que proporciona R.Por defecto, el entrenamiento establece valores constantes para la regularización \\(\\gamma\\) (0) y para el tamaño mínimo del nodo (1). En cambio, ajusta los hiperparámetros del modelo dentro de los valores por defecto de la función. Así, el modelo XGBoost resultante tiene 50 iteraciones, una profundidad máxima igual 2 y una tasa de aprendizaje de 0,3. Los resultados de la validación cruzada muestran que la precisión obtenida oscila entre el 85% y el 95%, resultado similar al del gradient boosting con hiperparámetros ajustados. Sin embargo, el valor mediano de la precisión es del 88%, ligeramente inferior la observada en el modelo gradient boosting con ajuste automático.\nFigura 29.4: Resultados del modelo durante la validación cruzada.\n","code":"\n# se determina la semilla aleatoria\nset.seed(101)\n\n# se entrena el modelo\nmodel <- train(CLS_PRO_pro13~.,\n               data=dp_entr,\n               method=\"xgbTree\",\n               metric=\"Accuracy\",\n               trControl=trainControl(classProbs = TRUE,\n                                      method = \"cv\",\n                                      number=10))\nggplot(melt(model$resample[,-4]), aes(x = variable, y = value, fill=variable)) + \n  geom_boxplot(show.legend=FALSE) + \n  xlab(NULL) + ylab(NULL)"},{"path":"cap-boosting-xgboost.html","id":"xgboost-con-ajuste-automático","chapter":"Capítulo 29 Boosting y el algoritmo XGBoost","heading":"29.4.4 XGBoost con ajuste automático","text":"Se continúa el ejemplo aplicado los datos sobre compra de un nuevo producto por parte de los clientes utilizando un modelo XGBoost en R. Sin embargo, se quieren mejorar los resultados obtenidos, y por ello ajustan automáticamente los hiperparámetros más relevantes de dicho algoritmo generando una red de posibles valores para dichos hiperparámetros. Por motivos computacionales, ésta se hace excesivamente exhaustiva para evitar largos tiempos de entrenamiento. Si se dispone de tiempo suficiente para el entrenamiento, es aconsejable tratar de estudiar más valores para los hiperparámetros optimizar.El modelo resultante establece que se utilicen 100 iteraciones, que los árboles tengan una profundidad máxima de 4, una tasa de aprendizaje del 0,2 y que la regularización \\(\\gamma\\) tome el valor 5.\nFigura 29.5: Resultados del modelo durante la validación cruzada.\nLos resultados obtenidos durante la validación cruzada muestran que la precisión es muy similar la del modelo por defecto, al encontrarse entre el 85% y el 95%. Sin embargo, se observa en el valor mediano de la precisión una ligera mejoría, al aumentar hasta el 90%.","code":"\n# Se especifica un rango de valores típicos para los hiperparámetros\ntuneGrid <- expand.grid (nrounds=c(50,100,500), \n                         max_depth = c(3,4,8),\n                         eta =c(0.05,0.1,0.2,0.3),\n                         gamma=c(0,0.5,5),\n                         colsample_bytree=c(0.8),\n                         min_child_weight=c(5),\n                         subsample=c(0.5)) \n# se determina la semilla aleatoria\nset.seed(101)\n\n\n# se entrena el modelo\nmodel <- train(CLS_PRO_pro13~., \n               data=dp_entr, \n               method=\"xgbTree\", \n               metric=\"Accuracy\",\n               trControl=trainControl(classProbs = TRUE,\n                                      method = \"cv\",\n                                      number = 10),\n               tuneGrid=tuneGrid)model$bestTune[,1:4]\n   nrounds max_depth eta gamma\n71     100         4 0.2     5"},{"path":"cap-boosting-xgboost.html","id":"resumen-26","chapter":"Capítulo 29 Boosting y el algoritmo XGBoost","heading":"Resumen","text":"En este capítulo se introduce al lector en el algoritmo de aprendizaje supervisado conocido como gradient boosting, en concreto:Se presenta el otro paradigma principal de aprendizaje ensamblado: el boosting.Se explica el modelo basado en este paradigma, el gradient boosting, así como sus diferencias con el random forest (basado en bagging).Se exponen los hiperparámetros más relevantes la hora de optimizar un modelo de gradient boosting.Se presenta el eXtreme gradient boosting, una implementación eficiente y escalable del modelo gradient boosting. Así como los hiperparámetros de regularización y otros parámetros importantes en esta implementación.Se aplican ambos algoritmos en R en un caso práctico para la clasificación binaria de datos.","code":""},{"path":"jerarquico.html","id":"jerarquico","chapter":"Capítulo 30 Análisis cluster: clusterización jerárquica","heading":"Capítulo 30 Análisis cluster: clusterización jerárquica","text":"José-María Montero\\(^{}\\) y Gema Fernández-Avilés\\(^{}\\)\\(^{}\\)Universidad de Castilla-La Mancha","code":""},{"path":"jerarquico.html","id":"origen-cluster","chapter":"Capítulo 30 Análisis cluster: clusterización jerárquica","heading":"30.1 Introducción","text":"El origen de la actividad agrupatoria, hoy en día conocida como análisis cluster o de conglomerados (AC), taxonomía numérica o reconocimiento de patrones, entre otras denominaciones, se remonta tiempos de Aristóteles y su discípulo Teofrasto. Por tanto, tiene unas profundas raíces y hoy en día se aplica en todos los campos del saber. Se ha evitado la palabra “clasificación” porque existe una pequeña diferencia entre agrupación y clasificación. En la actividad clasificatoria se conoce el número de grupos y qué observaciones del conjunto de datos pertenecen cada uno, siendo el objetivo clasificar nuevas observaciones en los grupos ya existentes. En la actividad agrupatoria, el número de grupos puede ser conocido (normalmente lo es), pero las observaciones que pertenecen cada uno de ellos, siendo el objetivo la asignación de dichas observaciones diferentes grupos. Este y el siguiente capítulo se centran en este último problema, al cual se hará referencia por su denominación más popular: AC.\nEl AC está orientado la síntesis de la información contenida en un conjunto de datos, normalmente una muestra relativa objetos, individuos o, en general, elementos, definidos por una serie de características, con vistas establecer una agrupación de los mismos en función de su mayor o menor homogeneidad. En otros términos, el AC trata de agrupar dichos elementos en grupos mutuamente excluyentes, de tal forma que los elementos de cada grupo sean lo más parecidos posible entre sí y lo más diferentes posible de los pertenecientes otros grupos (Fig. 30.1).\nFigura 30.1: Datos simulados que presentan clusters\nPara llevar cabo un AC, se deben tomar una serie de decisiones:Selección de las variables en función de las cuales se van agrupar los elementos.Elección del tipo de distancia o medida de similitud que se va utilizar para medir la disimilitud entre los elementos objeto de clasificación.Elección de la técnica para formar los grupos o conglomerados.Determinación del número óptimo de clusters (si se determina priori).En este capítulo se abordarán la primera y, sobre todo, la segunda cuestión, dejando las otras dos para el capítulo siguiente.Como ilustración práctica, se utilizará la base de datos TIC2021 del paquete CDR, relativa las estadísticas de uso de las TIC en la Unión Europea en 2021.","code":""},{"path":"jerarquico.html","id":"selección-de-las-variables","chapter":"Capítulo 30 Análisis cluster: clusterización jerárquica","heading":"30.2 Selección de las variables","text":"La selección de las \\(p\\) variables o características, \\(\\{X_1, X_2, ..., X_p\\}\\), en función de las cuales se va proceder la agrupación de los \\(n\\) elementos disponibles es crucial, ya que determina la agrupación final, independientemente de los procedimientos técnicos utilizados. Una vez determinadas éstas, la información disponible, para los elementos objeto de agrupación, será:Tabla 30.1:  Información muestralEn definitiva, la información de partida es una matriz \\(\\bf X_{\\textit n\\times \\textit p}\\) donde cada elemento viene representado por un punto en el espacio \\(p\\)-dimensional de variables, es decir, una matriz que proporciona los valores de las variables para cada elemento176.Una cuestión tener en cuenta es el número de variables considerar en el AC. La exclusión de variables relevantes generará una agrupación deficiente. La inclusión de variables irrelevantes complicará el proceso de agrupamiento sin procurar ganancias sustantivas. Dado que el miedo del investigador vendrá por el lado de la exclusión de variables relevantes, tenderá incluir un número excesivo de variables (muchas de ellas correlacionadas). Por ello, se recomienda realizar previamente un ACP (véase Cap. 32), lo que reduce la dimensionalidad del problema, y llevar cabo el AC partir de las componentes principales retenidas (incorreladas, evitando así redundancias). La eliminación de información redundante es una cuestión importante en el proceso de clusterización, porque dicha información estaría sobreponderada en el resultado obtenido. Una solución menos drástica este problema es la utilización de la distancia de Mahalanobis, que, como se verá posteriormente, corrige estas redundancias.\nOtra cuestión importante en este momento es decidir si las variables (o componentes principales en su caso) seleccionadas se utilizarán estandarizadas o . existe consenso sobre la cuestión, si bien se suele recomendar su estandarización para evitar consecuencias deseadas derivadas de la distinta escala y/o unidades de medida. obstante, autores tan relevantes como Edelbrock (1979) y S. Brian (1993), están en contra y proponen las siguientes alternativas: (\\(\\)) recategorizar todas las variables en variables binarias, y aplicar éstas una distancia apropiada para ese tipo de medidas; \\((ii)\\) realizar distintos AC con grupos de variables homogéneas (en cuanto su métrica) y sintetizar después los diferentes resultados; y \\((iii)\\) utilizar la distancia de Gower, que es aplicable con cualquier tipo de métrica.","code":""},{"path":"jerarquico.html","id":"clusterdist","chapter":"Capítulo 30 Análisis cluster: clusterización jerárquica","heading":"30.3 Elección de la distancia entre elementos","text":"Una vez se dispone de la matriz de información \\(\\bf X_{\\textit n\\times \\textit p}\\) , la segunda etapa en el AC consiste en la creación de una nueva matriz \\(\\bf D_{\\textit n\\times \\textit n}\\) cuyos elementos \\(\\{d_{ij}\\}\\) sean las distancias o disimilaridades entre los elementos objeto de agrupamiento.\nEn caso de variables cuantitativas, la distancia entre dos elementos en un espacio de \\(p\\) dimensiones, \\(d({\\bf x}_i;{\\bf{x}}_{j})\\), se define como una función que cada dos puntos de \\(\\mathbb{R}^{p}\\) le asocia un número real y que verifica:177\\(d({\\bf x}_i;{\\bf{x}}_{j}) \\geq0\\),\\(d({\\bf x}_i;{\\bf{x}}_{j}) \\geq0\\),\\(d({\\bf x}_i;{\\bf{x}}_{j})=0\\) si y sólo si \\({\\bf{x}}_{}={\\bf x}_{j}\\),\\(d({\\bf x}_i;{\\bf{x}}_{j})=0\\) si y sólo si \\({\\bf{x}}_{}={\\bf x}_{j}\\),\\(d({\\bf x}_i;{\\bf{x}}_{j})=d({\\bf x}_j;{\\bf{x}}_{})\\),\\(d({\\bf x}_i;{\\bf{x}}_{j})=d({\\bf x}_j;{\\bf{x}}_{})\\),\\(d({\\bf x}_i;{\\bf{x}}_{j})+d({\\bf x}_j;{\\bf{x}}_{k}) \\geq d({\\bf x}_i;{\\bf{x}}_{k}), \\quad \\forall{\\bf{x}}_{k} \\\\mathbb{R}^{p}\\),\\(d({\\bf x}_i;{\\bf{x}}_{j})+d({\\bf x}_j;{\\bf{x}}_{k}) \\geq d({\\bf x}_i;{\\bf{x}}_{k}), \\quad \\forall{\\bf{x}}_{k} \\\\mathbb{R}^{p}\\),Con variables cualitativas, la similitud entre dos elementos, \\(s({\\bf x}_i;{\\bf{x}}_{j})\\), es una función que cada dos puntos de \\(\\mathbb{R}^{p}\\) le asocia un número real, y que verifica: \\(s({\\bf x}_i;{\\bf{x}}_{j}) \\leq s_0\\), donde \\(s_0\\) es un número real finito arbitrario (normalmente 1).\\(s({\\bf x}_i;{\\bf{x}}_{j})=s_0\\) si y sólo si \\({\\bf{x}}_{}={\\bf x}_{j}\\),\\(s({\\bf x}_i;{\\bf{x}}_{j})=s({\\bf x}_j;{\\bf{x}}_{})\\),\\(|s({\\bf x}_i;{\\bf{x}}_{j})+s({\\bf x}_j;{\\bf{x}}_{k})|s({\\bf x}_i;{\\bf{x}}_{k}) \\geq d({\\bf x}_i;{\\bf{x}}_{j})s({\\bf x}_j;{\\bf{x}}_{k}) \\quad \\forall{\\bf{x}}_{k}\\\\mathbb{R}^{p}\\).Son numerosas las formas de medir las distancias o similaridades entre dos elementos que satisfacen las condiciones expuestas. Las más populares son las siguientes:Variables cuantitativasDistancia euclídea. Se define como: \\[\\begin{equation}\nd_{e}({\\bf x}_i;{\\bf{x}}_{j})=\\sqrt{\\sum_{k=1}^{p}\\left(  x_{ik}-x_{jk}\\right)  ^{2}}.\n\\end{equation}\\]\nIgnora las unidades de medida de las variables y, en consecuencia, aunque es invariante los cambios de origen, lo es los cambios de escala. También ignora las relaciones entre ellas. Resulta de utilidad con variables cuantitativas incorreladas y medidas en las mismas unidades. El cuadrado de la distancia euclídea también suele utilizarse como distancia. Para el conjunto de datos TIC2021, la distancia euclídea se obtiene ejecutando el siguiente código:La Fig. 30.2 muestra un heatmap178 de distancias euclídeas entre los países de la UE27 partir de las estadísticas de uso de las TIC en 2021.\nFigura 30.2: \\(Heatmap\\) de distancias euclídeas: datos TIC2021 del paquete CDR\nDistancia Manhattan o city block. Se define como: \\[\\begin{equation}\nd_{MAN}({\\bf x}_i;{\\bf{x}}_{j})=\\sum_{k=1}^{p}\\left\\vert x_{ik}-x_{jk}\\right\\vert.\n\\end{equation}\\]\nViene afectada por los cambios de escala en alguna de las variables y es menos sensible que la distancia euclídea los valores extremos. Por ello, es recomendable cuando las variables son cuantitativas, con las mismas unidades de medida, sin relaciones entre ellas y con valores extremos.Distancia de Minkowski. Se define como: \\[\\begin{equation}\nd_{MIN}({\\bf x}_i;{\\bf{x}}_{j})=\\left(  \\sum_{k=1}^{p}\\left\\vert x_{ik}-x_{jk}\\right\\vert\n^{\\lambda}\\right)  ^{\\frac{1}{\\lambda}}.\n\\end{equation}\\]\nLas distancias euclídea y Manhattan son casos particulares de la distancia de Minkowski. En la distancia euclídea \\(\\lambda=2\\) y en la Manhattan \\(\\lambda=1.\\)Norma del supremo o distancia de Chebychev. Su expresión es: \\[\\begin{equation}\nd_{CHE}({\\bf x}_i;{\\bf{x}}_{j})=\\max_{1\\leq k\\leq p}\\sum_{k=1}^{p}\\left\\vert x_{ik}%\n-x_{jk}\\right\\vert.\n\\end{equation}\\]Únicamente influye en ella la variable con los valores más extremos y, en este sentido, es muy sensible los cambios de escala en una de las variables.Distancia de Mahalanobis. Se define como: \\[\\begin{equation}\nd_{MAH}=({\\bf x}_i;{\\bf{x}}_{j})=(\\mathbf{x}_{}-\\mathbf{x}_{j})^{\\prime}\\mathbf{S}^{-1} (\\mathbf{x}_{}-\\mathbf{x}_{j})\n\\end{equation}\\]Coincide con la distancia euclídea calculada sobre las componentes principales. Es invariante cambios de origen y de escala (por tanto, la matriz de covarianzas entre las variables agrupadoras, \\(\\bf S\\), se puede sustituir por su homónima de correlaciones, \\(\\bf R\\)). Además, tiene en cuenta, explícitamente, las correlaciones lineales que puedan existir entre las variables, corrigiendo así el efecto redundancia. Es, por tanto, una distancia apropiada cuando se trabaja con variables cuantitativas con relaciones aproximadamente lineales. Su principal desventaja es que \\(\\bf S\\) involucra, conjuntamente, todos los elementos, y únicamente, y de forma separada, los elementos de cada cluster.Coeficiente de correlación de Pearson. Se define como:\n\\[\\begin{equation}\nd_{P}({\\bf x}_i;{\\bf{x}}_{j})=\\frac{\\sum_{k=1}^{p}\\left(  x_{ik}-\\overline{x}_{}\\right)  \\left(  x_{jk}-\\overline{x}_{j}\\right)  }{\\sqrt{\\sum_{k=1}%\n^{p}\\left(  x_{ik}-\\overline{x}_{}\\right)  ^{2}\\sum_{k=1}^{p}\\left(\nx_{jk}-\\overline{x}_{j}\\right)  ^{2}}}.\n\\end{equation}\\]\nes una distancia sino un indicador de similitud. Por tanto, valores altos indican elementos similares y valores bajos elementos distintos.Su campo de variación es \\([-1,1]\\), por lo que se toma su valor absoluto. Cuando las variables están centradas, se denomina coeficiente de congruencia o distancia coseno, puesto que coincide con el coseno formado por los vectores representativos de cada pareja de elementos. Tiene un inconveniente importante: un valor unitario significa que los dos elementos sean iguales, puesto que también pueden obtenerse valores unitarios cuando los valores de las \\(p\\) variables en uno de los elementos sean combinación lineal de los valores de las \\(p\\) variables del otro.Se utiliza, en ocasiones, preferentemente con datos cuantitativos y con el algoritmo de distancia mínima. Los coeficientes de correlación por rangos de Kendall y Spearman se utilizan, también, en casos de variables ordinales.\nefectos prácticos, cambiando el argumento method de la función get_dist (euclidea, maximum, manhattan, minkowski, pearson, spearman, kendall) se obtienen distintas matrices de distancias entre los elementos.Variables cualitativas (dicotómicas)En este caso, se pueden establecer distintas medidas de similaridad en base la siguiente tabla de contingencia \\(2\\times 2\\): partir de la tabla anterior, la similaridad entre dos elementos se puede medir partir de las coincidencias, ya sea de presencias y ausencias como de solo presencias.Entre las medidas de similaridad que involucran tanto presencias como ausencias comunes están:El coeficiente de coincidencias simple: \\(c_{cs}=\\frac{(n_{11} + n_{22})} {2}\\)El coeficiente de Rogers-Tanimoto: \\(c_{RT}=\\frac{(n_{11} + n_{22})} {2 (n_{11} + n_{22})+ n_{12}+n_{21}}\\)\nEstos dos coeficientes tienen una relación monotónica (si la distancia entre dos elementos es igual o superior la distancia entre otros dos con una de las medidas, también lo es con la otra). Esto es importante, dado que algunos procedimientos de agrupación se ven afectados por la medida utilizada siempre y cuando el ordenamiento establecido por ellas sea el mismo.Entre aquellas que identifican similaridad con presencias destacan:El coeficiente de Jacard: \\(c_J=\\frac{n_{11}} {n_{11} + n_{12}+ n_{21}}\\)El coeficiente de Jacard: \\(c_J=\\frac{n_{11}} {n_{11} + n_{12}+ n_{21}}\\)El coeficiente de Czekanowski: \\(c_{C}=\\frac{2 n_{11}} {2n_{11} + n_{12}+ n_{21}}\\)El coeficiente de Czekanowski: \\(c_{C}=\\frac{2 n_{11}} {2n_{11} + n_{12}+ n_{21}}\\)El coeficiente de Sokal y Sneath: \\(c_{SS}=\\frac{n_{11} } {n_{11} + 2(n_{12}+n_{21})}\\)El coeficiente de Sokal y Sneath: \\(c_{SS}=\\frac{n_{11} } {n_{11} + 2(n_{12}+n_{21})}\\)El coeficiente de Russell y Rao: \\(c_{RR}=\\frac {n_{11}}{p}\\)El coeficiente de Russell y Rao: \\(c_{RR}=\\frac {n_{11}}{p}\\)\nLos tres primeros coeficientes disfutan de la relación de monotonicidad en el sentido anteriormente apuntado, siendo las dos primeros las más utilizados en la práctica.También se usan como indicadores de similitud las medidas de asociación para tablas \\(2\\times2\\), sobre todo \\(Q\\) y \\(\\phi\\) (Sec. ref(medidas)).Variables cualitativas (politómicas)Cuando todas las variables sean cualitativas y alguna sea politómica, se generan para estas ultimas tantas variables dicotómicas como categorías tienen, denotando con 1 la presencia y con 0 la ausencia.Variables cuantitativas y cualitativasSi las variables son del mismo tipo, se utiliza la medida de similaridad de Gower: \\[\\begin{equation}\n    S_{ij}({\\bf x}_i;{\\bf{x}}_{j})=\\frac{\\sum_{k=1}^{p}s_{ij}}{\\sum_{k=1}^{p}w_{ij}}\n    \\end{equation}\\]donde \\(w_{ij}\\) vale siempre la unidad, salvo para variables binarias si los dos elementos presentan el valor cero. En cuanto al valor de \\(S_{ij}\\), se distinguen tres casos:Variables cualitativas de más de dos niveles: 1 si ambos elementos son iguales en la k-ésima variable; 0 si son diferentes.Variables dicotómicas: 1 si la variable considerada está presente en ambos elementos; 0 en los demás casos.Variables cuantitativas: \\(1-\\frac {|x_{ik}-x_{jk}|}{R_k}\\), donde \\(R\\) es el rango de la variable k.es recomendable cuando las variables cuantitativas sean muy asimétricas. En este caso, hay dos procedimientos aproximados: \\(()\\) calcular medidas separadas para las variables cuantitativas y cualitativas y combinarlas estableciendo algún tipo de de ponderación; \\((ii)\\) pasar las variables cuantitativas cualitativas y utilizar las medidas propuestas para este tipo de variables.","code":"\nlibrary(\"CDR\")\ndata(\"TIC2021\")\nlibrary(\"factoextra\")\n\ntic <- scale(TIC2021) # estandariza las variables\nd_euclidea <- get_dist(x = tic, method = \"euclidea\")\nas.matrix(d_euclidea)[1:5, 1:5]\n#>          BE       BG       CZ       DK       DE\n#> BE 0.000000 6.421631 2.417212 1.870962 2.304686\n#> BG 6.421631 0.000000 4.616177 7.988106 4.871235\n#> CZ 2.417212 4.616177 0.000000 3.765714 1.366011\n#> DK 1.870962 7.988106 3.765714 0.000000 3.607589\n#> DE 2.304686 4.871235 1.366011 3.607589 0.000000\nfviz_dist(dist.obj = d_euclidea, lab_size = 10)"},{"path":"jerarquico.html","id":"técnicas-de-agrupación-jerárquicas","chapter":"Capítulo 30 Análisis cluster: clusterización jerárquica","heading":"30.4 Técnicas de agrupación jerárquicas","text":" ","code":""},{"path":"jerarquico.html","id":"introac","chapter":"Capítulo 30 Análisis cluster: clusterización jerárquica","heading":"30.4.1 Introducción","text":"Una vez se han seleccionado las variables en función de las cuales se van agrupar los elementos disponibles en clusters o conglomerados, así como se ha decidido qué distancia utilizar para tal propósito, el siguiente paso del AC es la selección de un criterio o técnica de agrupamiento para formar los conglomerados. Dichas técnicas se pueden clasificar en \\(()\\) jerárquicas y \\((ii)\\) jerárquicas.TÉCNICAS DE CLUSTERIZACIÓN:Jerárquicas:\nAglomerativas:\nVecino más cercano o encadenamiento simple\nVecino más lejano o encadenamiento completo\nMétodo de la distancia media\nMétodo de la distancia entre centroides\nMétodo de la mediana\nMétodo de Ward\nEncadenamiento intra-grupos\nMétodo flexible de Lance y Williams\n\nDivisivas:\nVecino más cercano o encadenamiento simple\nVecino más lejano o encadenamiento completo\nMétodo de la distancia media\nMétodo de la distancia entre centroides\nMétodo de la mediana\nMétodo de Ward\nEncadenamiento intra-grupos\nAnálisis de la asociación\nDetector automático de interacciones\n\nAglomerativas:\nVecino más cercano o encadenamiento simple\nVecino más lejano o encadenamiento completo\nMétodo de la distancia media\nMétodo de la distancia entre centroides\nMétodo de la mediana\nMétodo de Ward\nEncadenamiento intra-grupos\nMétodo flexible de Lance y Williams\nVecino más cercano o encadenamiento simpleVecino más lejano o encadenamiento completoMétodo de la distancia mediaMétodo de la distancia entre centroidesMétodo de la medianaMétodo de WardEncadenamiento intra-gruposMétodo flexible de Lance y WilliamsDivisivas:\nVecino más cercano o encadenamiento simple\nVecino más lejano o encadenamiento completo\nMétodo de la distancia media\nMétodo de la distancia entre centroides\nMétodo de la mediana\nMétodo de Ward\nEncadenamiento intra-grupos\nAnálisis de la asociación\nDetector automático de interacciones\nVecino más cercano o encadenamiento simpleVecino más lejano o encadenamiento completoMétodo de la distancia mediaMétodo de la distancia entre centroidesMétodo de la medianaMétodo de WardEncadenamiento intra-gruposAnálisis de la asociaciónDetector automático de interaccionesNo jerárquicas:\nTécnicas de reasignación:\nBasadas en centroides: Método de Forgy, \\(k\\)-medias\nBasadas en medoides: \\(k\\)-medoides, PAM, CLARA, CLARANS\nBasadas en medianas: \\(k\\)-medianas\n\nTécnicas basadas en la densidad de elementos (mode-seeking):\nAproximación tipológica: Análisis modal, métodos TaxMap, de Fortin, de Gitman y Levine, de Catel y Coulter\nAproximación probabilística: método de Wolf\nDBSCAN\n\nOtras técnicas jerárquicas\nMétodos directos: \\(block\\)-; bi; co-; \\(two-mode\\) \\(clustering\\)\nMétodos de reducción de la dimensionalidad: modelos\n\\(Q\\)- y \\(R\\)-factorial\nClustering difuso\nMétodos basados en mixturas de modelos\n\nTécnicas de reasignación:\nBasadas en centroides: Método de Forgy, \\(k\\)-medias\nBasadas en medoides: \\(k\\)-medoides, PAM, CLARA, CLARANS\nBasadas en medianas: \\(k\\)-medianas\nBasadas en centroides: Método de Forgy, \\(k\\)-mediasBasadas en medoides: \\(k\\)-medoides, PAM, CLARA, CLARANSBasadas en medianas: \\(k\\)-medianasTécnicas basadas en la densidad de elementos (mode-seeking):\nAproximación tipológica: Análisis modal, métodos TaxMap, de Fortin, de Gitman y Levine, de Catel y Coulter\nAproximación probabilística: método de Wolf\nDBSCAN\nAproximación tipológica: Análisis modal, métodos TaxMap, de Fortin, de Gitman y Levine, de Catel y CoulterAproximación probabilística: método de WolfDBSCANOtras técnicas jerárquicas\nMétodos directos: \\(block\\)-; bi; co-; \\(two-mode\\) \\(clustering\\)\nMétodos de reducción de la dimensionalidad: modelos\n\\(Q\\)- y \\(R\\)-factorial\nClustering difuso\nMétodos basados en mixturas de modelos\nMétodos directos: \\(block\\)-; bi; co-; \\(two-mode\\) \\(clustering\\)Métodos de reducción de la dimensionalidad: modelos\n\\(Q\\)- y \\(R\\)-factorialClustering difusoMétodos basados en mixturas de modelosLos procedimientos jerárquicos particionan el conjunto de elementos de una sola vez, sino que realizan particiones sucesivas distintos niveles de agrupamiento; es decir, establecen una jerarquía de clusters, de ahí su nombre. Forman los conglomerados, bien agrupando los elementos en grupos cada vez más grandes, fusionando grupos en cada paso, (jerárquicos aglomerativos), o bien desagregándolos en conglomerados cada vez más pequeños (jerárquicos divisivos).\nLas técnicas jerárquicas se caracterizan porque \\(()\\) el número de clusters se suele determinar priori; \\((ii)\\) utilizan directamente los datos originales, si necesidad de calcular una matriz de distancias o similaridades; y \\((iii)\\) los clusters resultantes están anidados unos en otros, sino que están separados. La caja informativa proporciona un detalle mayor de la tipología de técnicas de agrupación que aborda el presente capítulo179. En lo que sigue, el objetivo son las técnicas jerárquicas, abordando las jerárquicas en el Cap. 31.","code":""},{"path":"jerarquico.html","id":"técnicas-jerárquicas-aglomerativas","chapter":"Capítulo 30 Análisis cluster: clusterización jerárquica","heading":"30.4.2 Técnicas jerárquicas aglomerativas","text":"Las técnicas jerárquicas aglomerativas, de amplia utilización, parten de tantos conglomerados como elementos y llegan un único conglomerado final.Se parte de un conglomerado constituido por los dos elementos más próximos, de tal manera que en la segunda etapa el conglomerado formado actuará modo de elemento (como si se tuviesen \\(n-1\\) elementos). En la segunda etapa, de nuevo se agrupan de nuevo los dos elementos más cercanos, que pueden ser dos elementos simples o uno simple y otro compuesto (el conglomerado anterior); en el primer caso, se tendrían dos conglomerados (cada uno de ellos formado por dos elementos) y en el segundo, un conglomerado con tres elementos y otro con uno. Sea cual sea el caso, al final de la segunda etapa se tienen \\(n-2\\) elementos, dos de los cuales son conglomerados. En las etapas siguientes se procede de idéntica manera: agrupación de los dos elementos (sean elementos simples o conglomerados formados en las etapas anteriores) más cercanos, y así sucesivamente hasta formar un único conglomerado integrado por todos los elementos. Es importante resaltar que un elemento, una vez forma parte de un conglomerado, ya sale de él.La pregunta que surge en este momento es: en el proceso de agrupamiento descrito, ¿cómo se mide la distancia de un elemento un conglomerado, o entre dos conglomerados?180 Los métodos más populares son los siguientes:Método del encadenamiento simple o vecino más cercano. Utiliza el criterio de “la distancia mas cercana”. Por tanto, \\(()\\) la distancia entre un elemento y un conglomerado es la menor de las distancias entre dicho elemento y cada uno de los elementos del conglomerado; \\((ii)\\) la distancia entre dos conglomerados viene dada por la distancia entre sus dos elementos más cercanos. Una vez computada la matriz de distancias se seleccionan los conglomerados más cercanos.Método del encadenamiento completo o vecino más lejano. Funciona igual que el anterior, pero ahora el criterio es “la distancia más lejana”.Nótese que, mientras que con el método del vecino más cercano la distancia entre los elementos más próximos de un cluster es siempre menor que la distancia entre elementos de distintos clusters, con el criterio del vecino más lejano la distancia entre los dos elementos más alejados de un cluster es siempre menor que la distancia entre cualquiera de sus elementos y los elementos más alejados de los demás clusters. Nótese también que, mientras que el método del vecino más cercano tiende separar los individuos en menor medida que la indicada por sus disimilaridades iniciales (es espacio-contractivo), el criterio del vecino más lejano es espacio-dilatante, es decir, tiende separar los individuos en mayor medida que la indicada por sus disimilaridades iniciales (Gallardo-San Salvador Vera-Vera 2004).Método de la distancia media. Surge como una solución la constricción o dilatación del espacio que provocan los dos métodos anteriores (por eso se dice que es espacio-conservativo y es muy utilizado), utilizando “la distancia promedio”, es decir, la distancia entre un elemento y un conglomerado es la media aritmética de las distancias de dicho elemento cada uno de los elementos del conglomerado. En caso de dos conglomerados, la distancia entre ellos viene dada por el promedio aritmético de las distancias, dos dos, tomándose un elemento de cada conglomerado. Igual que los dos métodos precedentes, es invariante transformaciones monótonas de la distancia utilizada.En la Fig. 30.3 se puede ver la constricción, dilatación y conservación del espacio que producen los métodos del vecino más cercano, más lejano y de la distancia media, respectivamente. En este caso se utiliza como representación gráfica el dendrograma (diagrama de árbol). En figuras posteriores se utilizarán otras alternativas al dendrograma, con el objetivo de mostrar las más populares. \nFigura 30.3: Clusterización jerárquica con distancias euclídeas (dendrograma): métodos del vecino más cercano, vecino más lejano y distancia media\nMétodo de la distancia entre centroides. Según este método, la distancia entre dos grupos o conglomerados es la distancia entre sus centroides, entendiendo por centroide del grupo \\(g\\): \\(c_{g}=\\left(\\overline{x}_{1g},\\overline{x}_{2g},...,\\overline{x}_{pg}\\right),\\) donde \\(\\overline{x}_{jg}\\) es la media de la \\(j\\)-ésima variable en dicho grupo.Igual que el método de la media, este método es también espacio-conservativo. Sin embargo, tiene la limitación de que cuando se agrupan dos conglomerados de diferente tamaño, el conglomerado resultante queda más cerca del conglomerado mayor y más alejado del menor, de forma proporcional la diferencia de tamaños, lo que lleva que lo largo del proceso de clusterización se vayan perdiendo las propiedades de los conglomerados pequeños (Gallardo San-Salvador 2022).Método de la mediana. Viene superar la limitación del método del centroide. Para ello, la estrategia natural es suponer que los grupos son de igual tamaño. Dicha estrategia se plasma en suponer que la distancia entre un elemento (o un conglomerado, \\(k\\)) y el conglomerado formado por la agrupación de los conglomerados \\(\\) y \\(j\\) viene dada por la mediana del triángulo formado por sus centroides (de ahí su nombre). Se trata de un método espacio conservativo, pero, igual que el método del centroide, es invariante transformaciones monótonas de la distancia utilizada.La Fig. 30.4, un tanglegrama o diagrama de laberinto, muestra las agrupaciones producidas por los métodos del centroide y la mediana. En ella se puede observar como el método de la mediana corrije la limitación del método del centroide. index{método! de la mediana} index{método! del centroide}\nFigura 30.4: Clusterización jerárquica con distancias euclídeas (tanglegrama): método del centroide vs. método de la mediana\nMétodo de Ward. El método de Ward agrupa, en cada etapa, los dos clusters que producen el menor incremento de la varianza total intra-cluster: \\(W=\\sum_g\\sum_{\\g} (x_{ig}- \\bar{x}_g)^{\\prime} (x_{ig}- \\bar{x}_g)\\), donde \\(\\bar{x}_g\\) es el centroide del grupo \\(g\\). Así, los grupos formados distorsionan los datos originales.181 Es muy utilizado en la práctica, dado que tiene casi todas las ventajas del método de la media y suele ser más discriminatorio en la determinación de los niveles de agrupación. También suele crear conglomerados muy compactos de tamaño similar. Dado que el menor incremento de \\(W\\) es proporcional la distancia euclídea al cuadrado entre los centroides de los grupos fusionados, \\(W\\) es decreciente, solventándose los problemas de los otros métodos basados en centroides.La Fig. 30.5, muestra el filograma, diagrama filético en forma de árbol filogenético, generado por la librería igraph con el método de agrupación de Ward. \nFigura 30.5: Clusterización jerárquica con distancias euclídeas al cuadrado (filograma): método de Ward\nMétodo del encadenamiento intra-grupos. Según el método de la distancia promedio (o vinculación entre grupos) la distancia entre dos conglomerados se obtenía calculando las distancias de cada elemento de uno de los grupos con todos los del otro y computando, posteriormente, la media aritmética de dichas distancias. Con el método de la vinculación intra-grupos se computa la distancia media entre la totalidad de los elementos de los conglomerados susceptibles de agrupación, con independencia de si pertenecen al mismo conglomerado inicial o distinto conglomerado. Por ejemplo: si un conglomerado está formado por los elementos \\(\\) y \\(b\\), y otro por los elementos \\(c\\) y \\(d\\), la distancia inter-grupos entre los dos conglomerados es:\\[d_{inter-grupos}=\\frac{d_{(;c)}+d_{(;d)}+d_{(b;c)}+d_{(b;d)}}{4}\\] mientras que la distancia intra-grupos vendrá dada por la media de las distancias entre los elementos \\(,b,c\\) y \\(d\\): \\[d_{intra-grupos}=\\frac{d_{(;b)}+d_{(;c)}+d_{(;d)}+d_{(b;c)}+d_{(b;d)}\n     +d_{(-c;d)}}{6}\\]Método flexible de Lance y Williams. Calcula la distancia entre dos conglomerados (el primero formado por la unión de otros dos en la etapa previa) partir de la siguiente expresión: \\[d_{\\left(  g_{1}\\cup g_{2}\\right); g_{3}}=\\alpha_{1}d_{(g_{1};g_{3})}\n+\\alpha_{2}d_{(g_{2};g_{3})}+\\beta d_{(g_{1};g_{2})}+\\gamma\\left\\vert\nd_{(g_{1};g_{2})}-d_{(g_{2};g_{2})}\\right\\vert,\\] donde \\(\\alpha_{1}+\\alpha_{2}+\\beta=1; \\alpha_{1}=\\alpha_{2};\\beta<1;\\gamma=0\\), si bien Lance y Williams sugieren adicionalmente un pequeño valor negativo de \\(\\beta\\). Por ejemplo \\(\\beta =-0,25\\).Los métodos anteriormente expuestos son casos particulares de éste. Denominando \\(n_1\\), \\(n_2\\) y \\(n_3\\) los tamaños de los grupos \\(g_1\\), \\(g_2\\) y \\(g_3\\), respectivamente, se tiene:Valores de \\(\\alpha_1\\), \\(\\alpha_2\\), \\(\\beta\\) y \\(\\gamma\\) para distintos procedimientos de agrupación","code":"\nhc_simple <- hcut(tic, k = 3, hc_method = \"single\")\nhc_completo <- hcut(tic, k = 3, hc_method = \"complete\")\nhc_promedio <- hcut(tic, k = 3, hc_method = \"average\")\n\nlibrary(\"patchwork\")\nd1 <- fviz_dend(hc_simple, cex = 0.5, k = 3, main = \"Vecino más cercano\")\nd2 <- fviz_dend(hc_completo, cex = 0.5, k = 3, main = \"Vecino más lejano\")\nd3 <- fviz_dend(hc_promedio, cex = 0.5, k = 3, main = \"Distancia promedio\")\n\nd1 + d2 + d3\nlibrary(\"dendextend\")\nlibrary(\"cluster\")\nhc_cent_dend <- as.dendrogram(hclust(d_euclidea, method = \"centroid\"))\nhc_med_dend <- as.dendrogram(hclust(d_euclidea, method = \"median\"))\ntanglegram(hc_cent_dend, hc_med_dend)\nlibrary(\"igraph\")\nset.seed(5665)\nhc_ward <- hcut(tic, k = 3, hc_method = \"ward.D2\")\nfviz_dend(\n  x = hc_ward,\n  k = 3,\n  type = \"phylogenic\"\n)"},{"path":"jerarquico.html","id":"técnicas-jerárquicas-divisivas","chapter":"Capítulo 30 Análisis cluster: clusterización jerárquica","heading":"30.4.3 Técnicas jerárquicas divisivas ","text":"En este caso, la secuencia de acontecimientos es justo la inversa. Se parte de un único conglomerado formado por todos los elementos y se llega \\(n\\) conglomerados formados, cada uno de ellos, por un único elemento (veces el proceso termina cuando se llega un número de grupos preestablecido). Ahora bien, dado que ahora se trata de subividir conglomerados, es decir, de identificar los elementos más distantes, o menos similares, para separarlos del resto del conglomerado, la estrategia seguir estará basada en maximizar las distancias (o minimizar las similitudes). En el proceso disociativo surge una cuestión importante: cuándo debe dejar de dividirse un cluster determinado y pasar dividir otro, cuestión que se resuelve por el procedimiento propuesto por MacNaughton-Smith et al. (1964). Las técnicas divisivas (también llamadas partitivas o disociativas), pueden ser monotéticas o politéticas. En el primer caso, las divisiones se basan en una sola característica o atributo. En el segundo, se tienen en cuenta todas. Las técnicas divisivas son menos populares que las aglomerativas. Sin embargo, la probabilidad de que lleven decisiones equivocadas (debido la variabilidad estadística de los datos) en las etapas iniciales del proceso, lo cual distorsionaría el resultado final del mismo, es menor que en las aglomerativas. En este sentido, los métodos partitivos, al partir del total de elementos, se consideran más seguros que los aglomerativos. Los métodos disociativos más populares son los siguientes:Método de la distancia promedio Dentro de las técnicas politéticas, entre las que se cuentan todas las vistas en la clusterización jerárquica aglomerativa, quizás la más popular es la que utiliza para la partición el método de la distancia promedio. Para ilustrarla, supóngase que se tienen 5 elementos y que su matriz de distancias es la siguiente:\\[\\bf X=\\left(\\begin{matrix} .&.&.&.&.\\\\\n8&.&.&.&.\\\\\n7&4&.&.&.\\\\\n6&1&4&.&.\\\\\n3&4&5&4&.\n\\end {matrix}\\right)\\]En la primera etapa hay que dividir el grupo de cinco elementos en dos conglomerados. Hay \\(2^{2n-1}-1\\) posibilidades, pero según el método de la distancia promedio, se calcula la distancia de cada elemento los demás y se promedia, desgajándose el elemento con distancia promedio máxima. En este caso, se desgajaría el primer elemento, y en la segunda etapa se partiría de dos grupos: \\(\\{e_1 \\}\\) y \\(\\{e_2, e_3, e_4, e_5\\}\\).partir de la segunda etapa, se procede como sigue (véase Tabla 30.2):\\(()\\) Se calculan las (4) distancias promedio de cada elemento del conglomerado principal al elemento desgajado;\\((ii)\\) Se calculan las (4) distancias promedio de cada elemento del conglomerado principal al resto de elementos del mismo;\\((iii)\\) Se computan las diferencias \\(()-(ii)\\) para cada uno de los 4 elementos del conglomerado principal;\\((iv)\\) De entre aquellos elementos del grupo principal en los que \\(()-(ii)<0\\) se selecciona aquel para el cual es máxima. Tras esta segunda etapa los conglomerados son \\(\\{e_1, e_5\\}\\) y \\(\\{e_2, e_3, e_4\\}\\).Tabla 30.2:  Distancias entre conglomerados: segunda etapaEn las siguientes etapas se procede de igual manera, hasta que todas las diferencias sean positivas (en el caso que se considera, esto ocurre en la tercera etapa; véase Tabla 30.3.Tabla 30.3:  Distancias entre conglomerados: tercera etapaCuando esto ocurre, es decir, cuando todos los elementos del conglomerado principal están más cerca de los demás que lo componen que de los del conglomerado disociado, se vuelve iniciar el algoritmo, pero esta vez para cada uno de los dos conglomerados generados (MacNaughton-Smith et al. 1964). En caso que nos ocupa, en \\(\\{e_1, e_5\\}\\) la única partición posible es \\(\\{e_1\\}\\), \\(\\{e_5\\}\\). En \\(\\{e_2, e_3, e_4\\}\\) se desgaja el elemento con mayor distancia promedio los demás del grupo. Como \\(\\frac{d_{(2,3)}+ d_{(2,4)}}{2}=2,5\\), \\(\\frac{d_{(3,2)}+ d_{(3,4)}}{2}=4\\) y \\(\\frac {d_{(4,2)} + d_{(4,3)}} {2}=2,5\\), se desgaja \\(\\{e_3\\}\\).continuación se aplica el algoritmo anteriormente expuesto cada elemento del grupo principal \\(\\{e_2, e_4\\}\\) y \\(\\{e_3\\}\\) (Tabla 30.4) y, como todas las distancias son positivas, se divide \\(\\{e_2, e_4\\}\\) en \\(\\{e_2 \\}\\) y \\(\\{e_4\\}\\).Tabla 30.4:  Distancia entre conglomerados: etapa finalEl algoritmo DIvisive ANAlysis (DIANA) permite llevar cabo la partición anterior utilizando el diámetro de los clusters para decidir el orden de partición clusters cuando se tienen varios con más de un elemento (véase capítulo 6 de Kaufman Rousseeuw (1990)). Proporciona \\((\\)) el coeficiente divisivo (véase ?diana.object), que mide la cantidad de estructura de agrupamiento encontrada; y \\((ii\\)) la pancarta, una novedosa presentación gráfica (véase ?plot.diana). Para el ejemplo de los datos TIC, DIANA proporciona el coeficiente divisivo (valores cercanos 1 sugieren una estructura de agrupación fuerte), y el dendrograma, en este caso circular, representado en la Fig. 30.6.\nFigura 30.6: Clusterización jerárquica divisiva con DIANA\nAnálisis de la asociación En caso de que los elementos vengan caracterizados por variables cualitativas o factores dicotómicos, \\(F_1,F_2,..., F_n\\) (si alguno fuese politómico, cada una de sus categorías se consideraría como un factor dicotómico), el método del análisis de la asociación (o suma de estadísticos chi-cuadrado) es una técnica monotética muy utilizada que procede como sigue: \\(()\\) Considérese \\(F_1\\) y divídase el conjunto de elementos en dos grupos o categorías: uno con los elementos en los que \\(F_1\\) esté presente y otro con aquellos en los que esté ausente. Hágase lo mismo con los demás factores.\\((ii)\\) Constrúyanse las \\(n\\times(n-1)\\) tablas de contingencia \\(2\\times2\\) que cruzan cada factor con cada uno de los demás (véase Sec. 23.1.1).\ndonde \\(\\neq j\\).\\((iii)\\) Calcúlese el estadístico chi-cuadrado (\\(\\chi_{ij}^2=\\frac {n(n_{11}n_{22}-n_{12}n_{21})^2}{n_{1\\cdot} n_{2\\cdot}n_{\\cdot1}n_{\\cdot2}}\\) para una de dichas tablas (véase eígrafe 23.2.4) y compútese \\(\\sum_{\\neq j}\\chi_{ij}^2\\).\\((iii)\\) Desgájese del conglomerado inicial en dos: uno con los elementos que contienen el factor con la máxima \\(\\sum_{\\neq j}\\chi_{ij}^2\\) y otro con el resto de los elementos (donde dicho factor está ausente).\\((iv)\\) Procédase así iterativamente.Método del detector automático de interacciones (AID) es propiamente un método de AC, sino de la esfera de los modelos lineales de rango completo. Sin embargo, se menciona, siquiera mínimamente, porque se utiliza en algunas ocasiones para combinar categorías de los factores utilizados con la finalidad de generar grupos que difieran lo más posible entre sí respecto de los valores de una variable dependiente medida en una escala métrica (con una escala proporcional o de intervalo) o ficticia (dicotómica con valores 0 y 1).\nEspecíficamente, el AID realiza divisiones secuenciales dicotómicas de la variable explicar mediante un ANOVA, dividiendo inicialmente el conjunto de elementos objeto de agrupación en dos grupos según la variable que mejor explica las diferencias en el comportamiento estudiar (en cada etapa se busca la partición que maximiza la varianza inter-grupos y minimiza la varianza intra-grupos); cada uno de los dos grupos formados se vuelve subdividir de acuerdo con la variable que mejor explica las diferencias entre ellos; este proceso continúa hasta que el tamaño de los grupos dicotómicos alcanza un mínimo pre-establecido o hasta que las diferencias entre los valores medios de los grupos sean significativas.En este algoritmo, el proceso de subdivisión del conjunto de elementos en grupos dicotómicos continúa hasta que se verifica algún criterio de parada.Las limitaciones más importantes del AID son las siguientes:Tiende seleccionar como más explicativas las variables con mayor número de categorías. Por eso conviene utilizarlo cuando las variables explicativas difieran mucho en el número de categorías.Las particiones resultantes dependen de la variable elegida en primer lugar, condicionando las sucesivas particiones.Su naturaleza exclusivamente dicotómica también es una limitación importante. Si se llevasen cabo particiones con tres o más ramas producirían una mayor reducción de la varianza residual y, además, permitirían una mejor selección de otras variables.El AID basado en tablas de contingencia y el estadístico chi-cuadrado (CHAID) corrige la mayoría de estas limitaciones. Aunque inicialmente fue diseñado para variables categóricas, posteriormente se incluyó la posibilidad de trabajar con variables categóricas nominales, categóricas ordinales y continuas, permitiendo generar tanto árboles de decisión, para resolver problemas de clasificación, como árboles de regresión. Además, los nodos se pueden dividir en más de dos ramas.","code":"\nhc_diana <- diana(tic, metric = \"euclidea\")\nhc_diana$dc\n#> [1] 0.8043393\nfviz_dend(\n  x = hc_diana,\n  k = 3,\n  type = \"circular\",\n  ggtheme = theme_minimal()\n)"},{"path":"jerarquico.html","id":"calidad-de-la-agrupación-y-número-de-clusters","chapter":"Capítulo 30 Análisis cluster: clusterización jerárquica","heading":"30.5 Calidad de la agrupación y número de clusters","text":"","code":""},{"path":"jerarquico.html","id":"el-coeficiente-de-correlación-lineal-cofenético","chapter":"Capítulo 30 Análisis cluster: clusterización jerárquica","heading":"30.5.1 El coeficiente de correlación lineal cofenético ","text":"Dado que las técnicas jerárquicas imponen una estructura sobre los datos y pueden producir distorsiones significativas en las relaciones entre los datos originales, una vez realizada la jerarquización de los elementos objeto de clusterización, surge la siguiente pregunta: ¿en qué medida la estructura final obtenida representa las similitudes o diferencias entre dichos objetos? En otros términos, ¿en qué medida el dendrograma representa la matriz de distancias o similitudes original?El coeficiente de correlación lineal cofenético da respuesta dichas preguntas. Se define como el coeficiente de correlación lineal entre los \\(n(n-1)\\) elementos del triangulo superior de la matriz de distancias o similitudes y sus homónimos en la matriz cofenética, \\(\\bf C\\), cuyos elementos \\(\\{c_{ij}\\}\\) son las distancias o similitudes entre los elementos \\((,j)\\) tras la aplicación de la técnica de jerarquización. Obviamente, se utilizará la técnica jerárquica que origine el mayor coeficiente.En el ejemplo TIC, el mayor coeficiente cofenético corresponde al método del promedio o del centroide, si bien el de las otras técnicas de agregación es bastante parecido.","code":"\n# comparación con la distancia euclidea: d_euclidea\ncof_simp <- cophenetic(hc_simple)\ncof_comp <- cophenetic(hc_completo)\ncof_prom <- cophenetic(hc_promedio)\ncof_ward <- cophenetic(hc_ward)\ncof_dia <- cophenetic(hc_diana)\ncoef_cofeneticos <- cbind(d_euclidea, cof_simp, cof_comp, cof_prom, cof_ward, cof_dia)\n\nround(cor(coef_cofeneticos)[1, ], 2)\n#> d_euclidea   cof_simp   cof_comp   cof_prom   cof_ward    cof_dia \n#>       1.00       0.71       0.61       0.77       0.60       0.65"},{"path":"jerarquico.html","id":"número-óptimo-de-clusters","chapter":"Capítulo 30 Análisis cluster: clusterización jerárquica","heading":"30.5.2 Número óptimo de clusters ","text":"Acabado el procedimiento de clusterización de los \\(n\\) elementos disponibles, sea por un procedimiento jerárquico aglomerativo o divisivo, hay que tomar una decisión sobre el número de óptimo de clusters, \\(k\\). Esta decisión es ardua y requiere un delicado equilibrio. Valores grandes de \\(k\\) pueden mejorar la homogeneidad de los clusters; sin embargo, se corre el riesgo de sobreajuste. Lo contrario ocurre con un \\(k\\) pequeño.Para tomar esta decisión, además del sentido común y el conocimiento que se tenga del fenómeno en estudio, se puede echar mano de distintos procedimientos heurísticos:El primero se basa en el dendrograma y, en concreto, en la representación de las distintas etapas del algoritmo y las distancias la que se producen las agrupaciones o particiones de los clusters. Para cada distancia, el dendrograma produce un número determinado de clusters que aumenta (o disminuye) con la misma. Por tanto, el número de clusters dependerá de la distancia la que se corte el dendrograma (eje de ordenadas del dendrograma, height). Dicha distancia debería elegirse de tal forma que los conglomerados estuviesen bien determinados y fuesen interpretables. En las primeras etapas del proceso las distancias varían mucho, pero en las etapas intermedias y, sobre todo, finales, las distancias aumentan mucho entre dos etapas consecutivas. Por ello, se suele cortar el dendrograma la distancia la cual las distancias entre dos etapas consecutivas del proceso empiecen ser muy grandes, indicador de que los grupos empiezan ser muy distintos.El primero se basa en el dendrograma y, en concreto, en la representación de las distintas etapas del algoritmo y las distancias la que se producen las agrupaciones o particiones de los clusters. Para cada distancia, el dendrograma produce un número determinado de clusters que aumenta (o disminuye) con la misma. Por tanto, el número de clusters dependerá de la distancia la que se corte el dendrograma (eje de ordenadas del dendrograma, height). Dicha distancia debería elegirse de tal forma que los conglomerados estuviesen bien determinados y fuesen interpretables. En las primeras etapas del proceso las distancias varían mucho, pero en las etapas intermedias y, sobre todo, finales, las distancias aumentan mucho entre dos etapas consecutivas. Por ello, se suele cortar el dendrograma la distancia la cual las distancias entre dos etapas consecutivas del proceso empiecen ser muy grandes, indicador de que los grupos empiezan ser muy distintos.Otra posibilidad es utilizar el gráfico de sedimentación (Sec. 32.4), que relaciona la variablidad entre clusters (eje de ordenadas) con el el número de clusters (eje de abscisas). Normalmente, decrece bruscamente al principio, y posteriormente más despacio, hasta llegar la parte de sedimentación (el codo del gráfico), donde el decrecimiento es muy lento. Pues bien, el número óptimo de conglomerados es el correspondiente al codo o comienzo del área de sedimentación del gráfico.\nEl algoritmo del gráfico de sedimentación es como sigue:\nClusterícese variando el número de grupos, \\(k\\), por ejemplo, de 1 10.\nPara cada valor de \\(k\\), compútese la suma de cuadrados intra-grupo (WSS).\nTrácese la gráfica de WSS vs. \\(k\\).\nDetermínese el número óptimo de grupos.\nCon conjuntos de datos de tamaño pequeño moderado, este proceso se puede realizar convenientemente con factoextra::fviz_nbclust().Otra posibilidad es utilizar el gráfico de sedimentación (Sec. 32.4), que relaciona la variablidad entre clusters (eje de ordenadas) con el el número de clusters (eje de abscisas). Normalmente, decrece bruscamente al principio, y posteriormente más despacio, hasta llegar la parte de sedimentación (el codo del gráfico), donde el decrecimiento es muy lento. Pues bien, el número óptimo de conglomerados es el correspondiente al codo o comienzo del área de sedimentación del gráfico.El algoritmo del gráfico de sedimentación es como sigue:Clusterícese variando el número de grupos, \\(k\\), por ejemplo, de 1 10.Para cada valor de \\(k\\), compútese la suma de cuadrados intra-grupo (WSS).Trácese la gráfica de WSS vs. \\(k\\).Determínese el número óptimo de grupos.Con conjuntos de datos de tamaño pequeño moderado, este proceso se puede realizar convenientemente con factoextra::fviz_nbclust().Otra opción es el ancho de silueta promedio. El coeficiente o ancho de silueta compara, por cociente, la distancia media elementos en el mismo grupo con la distancia media elementos en otros grupos. \nEste método calcula el ancho de silueta promedio (avg.sil.wid.) de los elementos objeto de agrupación para diferentes valores de \\(k\\). Como un valor alto del ancho promedio indica una buena agrupación, el número óptimo de conglomerados es el que lo maximiza. El campo de variación del ancho de silueta es [-1, 1], donde 1 significa que los elementos están muy cerca de su propio cluster y lejos de otros clusters, mientras que -1 indica que están cerca de los clusters vecinos.Otra opción es el ancho de silueta promedio. El coeficiente o ancho de silueta compara, por cociente, la distancia media elementos en el mismo grupo con la distancia media elementos en otros grupos. Este método calcula el ancho de silueta promedio (avg.sil.wid.) de los elementos objeto de agrupación para diferentes valores de \\(k\\). Como un valor alto del ancho promedio indica una buena agrupación, el número óptimo de conglomerados es el que lo maximiza. El campo de variación del ancho de silueta es [-1, 1], donde 1 significa que los elementos están muy cerca de su propio cluster y lejos de otros clusters, mientras que -1 indica que están cerca de los clusters vecinos.\n\n\n\nEl criterio del gap (brecha), similar al método del codo, tiene como finalidad encontrar la mayor diferencia o distancia entre los diferentes grupos de elementos que se van formando en el proceso de clusterización y que se representan normalmente en un dendrograma. Se computan las distancias de cada uno de los enlaces que forman el dendrograma y se observa cuál es la mayor de ellas. El máximo del gráfico de estas diferencias vs. el número de clusters indica el número óptimo de clusters. \nFigura 30.7: Métodos heurísticos para la determinación del número óptimo de clusters\nFinalmente, el índice de Dunn es el cociente entre la mínima distancia inter-grupos y la máxima distancia intra-grupos. mayor índice, mayor calidad de clusterización. En el ejemplo TIC, el gráfico de sedimentación y criterio del gap indican un número óptimo de clusters de 3. El ancho de silueta alcanza su máximo con dos clusters, si bien la altura del gráfico para tres clusters es prácticamente la misma. Por ello, se opta por 3 clusters pesar de que el índice de Dunn también se decanta por dos. El primero lo forman Rumanía, Bulgaria y Grecia, la franja sudeste de la UE27, que se caracteriza por tener los peores guarismos en dotación y uso de las TIC, tanto nivel de hogar como de empresa. El segundo lo integran el resto de la franja este más las tres primeras economías de la Unión y Portugal. Tienen unos elevados porcentajes en todas las variables, pero los mayores, que corresponden los demás países de la UE27, el tercer conglomerado.Además de los procedimientos anteriores, hay otros, tan populares, \\(()\\) basados en el contraste de hipótesis, suponiendo que los datos siguen alguna distribución multivariante (casi siempre la normal) o \\((ii)\\) procedentes de la abstracción de procedimientos inherentes al análisis multivariante paramétrico; los detalles pueden verse en Gallardo San-Salvador (2022). El paquete NbClust de R contiene la función NbClust(), que calcula 30 índices para valorar el número óptimo de clusters.","code":"\np1 <- fviz_nbclust(tic,\n  FUN = hcut, method = \"wss\",\n  k.max = 10\n) +\n  ggtitle(\"Elbow\")\np2 <- fviz_nbclust(tic,\n  FUN = hcut, method = \"silhouette\",\n  k.max = 10\n) +\n  ggtitle(\"Silhouette\")\np3 <- fviz_nbclust(tic,\n  FUN = hcut, method = \"gap_stat\",\n  k.max = 10\n) +\n  ggtitle(\"Gap\")\n\np1 + p2 + p3\nlibrary(\"clValid\")\ncut2_hc_prom <- cutree(hc_promedio, k = 2)\ncut3_hc_prom <- cutree(hc_promedio, k = 3)\ncut4_hc_prom <- cutree(hc_promedio, k = 4)\ncut5_hc_prom <- cutree(hc_promedio, k = 5)\n\ndunn(d_euclidea, cut2_hc_prom)\n#> [1] 0.4465593\ndunn(d_euclidea, cut3_hc_prom)\n#> [1] 0.3751942\ndunn(d_euclidea, cut4_hc_prom)\n#> [1] 0.4074884\ndunn(d_euclidea, cut5_hc_prom)\n#> [1] 0.4366356"},{"path":"jerarquico.html","id":"resumen-27","chapter":"Capítulo 30 Análisis cluster: clusterización jerárquica","heading":"Resumen","text":"El análisis cluster está orientado la agrupación de un conjunto de elementos en grupos, en función de una serie de características, tal que los elementos de cada grupo sean lo más parecidos posible entre sí y lo más diferentes posible de los de otros grupos. Este proceso implica \\(()\\) la selección de las variables en función de las cuales se van agrupar; \\((ii)\\) la elección de la distancia o medida de similitud entre ellos; \\((iii)\\) la elección de la técnica para formar los grupos; y \\((iv)\\) la determinación del número óptimo de clusters, cuando sea menester. Estas son las cuestiones que se estudian en este capítulo, si bien, por cuestiones de espacio, en \\((iii)\\) solo se abordan las técnicas de clusterización jerárquicas, estudiándose las jerárquicas en el siguiente capítulo.","code":""},{"path":"no-jerarquico.html","id":"no-jerarquico","chapter":"Capítulo 31 Análisis cluster: clusterización no jerárquica","heading":"Capítulo 31 Análisis cluster: clusterización no jerárquica","text":"José-María Montero\\(^{}\\) y Gema Fernández-Avilés\\(^{}\\)\\(^{}\\)Universidad de Castilla-La ManchaComo se avanzó en 30.4.1, aunque las técnicas de agrupación jerárquicas son muy utilizadas, existen otras, también muy populares, que se aglutinan bajo la denominación de jerárquicas y que se pueden clasificar, sin ánimo de exhaustividad, en \\(()\\) de optimización o reasignación; \\((ii)\\) basadas en la densidad de elementos; y \\((iii)\\) otras, como los métodos directos (por ejemplo, el block-; bi-; co-; two-mode cluster), los de reducción de la dimensionalidad (como el Q- y el R-factorial), los métodos de clusterización difusa, o los basados en mixturas de modelos.\nLas técnicas jerárquicas proceden con el criterio de la inercia, maximizando la varianza inter-grupos y minimizando la intra-grupos. Se caracterizan porque:\nEl número de clusters se suele determinar priori.Utilizan directamente los datos originales, si necesidad de computo de una matriz de distancias o similaridades.Los elementos pueden cambiar de cluster.Los clusters resultantes están anidados unos en otros.","code":""},{"path":"no-jerarquico.html","id":"métodos-de-reasignación","chapter":"Capítulo 31 Análisis cluster: clusterización no jerárquica","heading":"31.1 Métodos de reasignación","text":"Los métodos de reasignación permiten que un elemento asignado un grupo en una determinada etapa del proceso de clusterización sea reasignado otro grupo, en una etapa posterior, si dicha reasignación implica la optimización del criterio de selección. El proceso finaliza cuando hay ningún elemento cuya reasignación permita optimizar el resultado conseguido. Estas técnicas suelen asumir un número determinado de clusters priori y se diferencian entre sí en la manera de obtener la partición inicial y en la medida optimizar en el proceso. Respecto esta última cuestión, los procedimientos más populares son: \\(()\\) la minimización de la traza de la matriz de covarianzas intra-grupos; \\((ii)\\) la minimización de su determinante; \\((iii)\\) la maximización de la traza del producto de las matrices de covarianzas inter-grupos e intra-grupos; \\((iv)\\) medidas de información o de estabilidad.","code":""},{"path":"no-jerarquico.html","id":"técnicas-basadas-en-centroides-métodos-de-forgy-y-bf-k-medias","chapter":"Capítulo 31 Análisis cluster: clusterización no jerárquica","heading":"31.1.1 Técnicas basadas en centroides: métodos de Forgy y \\(\\bf k\\)-medias","text":" Los algoritmos de reasignación más populares son el de Forgy y, sobre todo, el \\(k\\)-medias. La literatura sobre este tipo de técnicas es clara y, frecuentemente, se confunden el método de Forgy y el \\(k\\)-medias, así como el \\(k\\)-medias con algunas de sus otras denominaciones (dándose entender que son técnicas distintas). Sin embargo, la historia es la siguiente: originalmente, Forgy (1965) propuso un algoritmo consistente en la iteración sucesiva, hasta obtener convergencia, de las dos operaciones siguientes:\\(()\\) representación de los grupos por sus centroides; y \\((ii)\\) asignación de los elementos al grupo con el centroide más cercano. Posteriormente, Diday (1971), Diday (1973), Anderberg (1973), Bock (1974) y Späth (1975) desarrollaron una variante del método de Forgy, que solo se diferencia de él en que los centroides se recalculan después de asignar cada elemento (con la técnica de Forgy primero se llevan cabo todas las asignaciones y posteriormente se recalculan los centroides). Diday la llamó método de las nubes dinámicas o clusters dinámicos, Anderberg se refirió ella como el criterio de inclusión en el grupo del centroide más cercano, Bock la denominó particionamiento iterativo basado en la mínima distancia, y Späth la llamó HMEANS, una versión por lotes del procedimiento de los autores anteriores. Sin embargo, fue MacQueen (1967) quien previamente acuñó la denominación de “\\(k\\)-medias” que se usa hasta la fecha.\\(K\\)-medias182 requiere la especificación previa del número de grupos, \\(k\\), en los que se va dividir el conjunto de elementos. El algoritmo \\(()\\) selecciona \\(k\\) elementos por algún procedimiento; \\((ii)\\) asigna los restantes elementos al elemento más cercano de los previamente seleccionados; \\((iii)\\) sustituye los elementos seleccionados en \\(()\\) por los centroides de los grupos que se han formado; \\((iv)\\) asigna el conjunto de elementos al centroide más cercano del punto \\((iii)\\); \\((v)\\) repite iterativamente los dos últimos pasos hasta que la asignación de elementos los centroides cambia. Los grupos entonces formados maximizan la distancia inter-grupos y minimizan la distancia intra-grupos.\nRecuérdese que con el método de Forgy la etapa \\((iii)\\) comienza hasta que se hayan asignado todos los elementos un cluster en la etapa \\((ii)\\), mientras que en “\\(k\\)-medias” los centroides se recomputan cada vez que un elemento es asignado un grupo.La partición que se obtiene es un óptimo local (pequeños cambios en la reasignación de elementos lo mejoran), pero se puede asegurar que sea el global, pues se trata de un método heurístico. Sí se puede asegurar que la partición es de calidad.\\(K\\)-medias es eficiente y sencillo de implementar, pero tiene algunas desventajas: \\(()\\) necesita conocer priori el número de grupos; \\((ii)\\) la agrupación resultante puede depender de la asignación inicial (normalmente aleatoria) de los centroides, pudiendo converger mínimos locales, por lo que se recomienda repetir la clusterización 25-50 veces y seleccionar la que tenga menor varianza intra-grupos; \\((iii)\\) es robusto valores extremos; y \\((iv)\\) trabaja con datos nominales. En el ejemplo TIC, se ha usado el algoritmo 136 de J. . Hartigan Wong (1979), una versión eficiente del de John . Hartigan (1975) que busca óptimos locales (varianza intra-grupos mínima en cada grupo), sino soluciones tales que ninguna reasignación de elementos reduzca la varianza (global) intra-grupos (véase Fig. 31.1).\nFigura 31.1: Clusterización jerárquica con \\(k\\)-medias\nAlgunas versiones del \\(k\\)-medias como el \\(k\\)-medias difuso, el \\(k\\)-medias recortadas, el \\(k\\)-medias armónicas, el \\(k\\)-medias sparse y el \\(k\\)-medias sparse robusto pueden verse en Carrasco-Oberto (2020).\n","code":"\nset.seed(123)\nkmeans_tic <- eclust(tic, \"kmeans\", k = 3)"},{"path":"no-jerarquico.html","id":"técnicas-basadas-en-medoides","chapter":"Capítulo 31 Análisis cluster: clusterización no jerárquica","heading":"31.1.2 Técnicas basadas en medoides","text":"","code":""},{"path":"no-jerarquico.html","id":"bf-k-medoides-pam","chapter":"Capítulo 31 Análisis cluster: clusterización no jerárquica","heading":"31.1.2.1 \\(\\bf K\\)-medoides (PAM) ","text":"Es un método de clusterización similar al \\(k\\)-medias que también requiere la especificación priori del número de grupos. La diferencia es que, en \\(k\\)-medoides, cada grupo está representado por uno de sus elementos, denominados medoides (o centrotipos)183. En el \\(k\\)-medias están representados por sus centroides, que tienen por qué coincidir con ninguno de los elementos agrupar. Se trata pues, de formar grupos particionando el conjunto de elementos alrededor de los medoides (PAM).El algoritmo \\(k\\)-medoides es más robusto al ruido y valores grandes de los datos (de hecho es invariante los outliers) que el \\(k\\)-medias, ya que minimiza la suma de diferencias por\nparejas (utiliza la distancia Manhattan) en lugar de la suma de los cuadrados de las distancias euclídeas184.\nAdemás, sus agrupaciones dependen del orden en que han sido introducidos 1os elementos, cosa que puede ocurrir con otras técnicas -jerárquicas, y, como se avanzó anteriormente, propone como centro del cluster un elemento del mismo.PAM funciona muy bien con conjuntos de datos pequeños (por ejemplo 100 elementos en 5 grupos) y permite un análisis detallado de la partición realizada, puesto que proporciona las características del agrupamiento y un gráfico de silueta, así como un índice de validez propio para determinar el número óptimo de clusters. El algoritmo PAM puede verse al completo en Kaufman Rousseeuw (1990); para un muy buen resumen véase Amat Rodrigo (2017). \nFigura 31.2: Clusterización jerárquica con PAM\n","code":"\nset.seed(123)\npam_tic <- eclust(tic, \"pam\", k = 3)"},{"path":"no-jerarquico.html","id":"clara","chapter":"Capítulo 31 Análisis cluster: clusterización no jerárquica","heading":"31.1.2.2 CLARA ","text":"La ineficiencia de PAM para bases de datos grandes, junto con su complejidad computacional, llevó al desarrollo de\nCLARA (clustering Large Applications)185La diferencia entre PAM y CLARA es que el segundo se basa en muestreos. Solo una pequeña porción de los datos totales es seleccionada como representativa de los datos y\nlos medoides son escogidos (en la muestra) usando PAM.\nCLARA, pues, combina la idea de \\(k\\)-medoides con el remuestreo para que pueda aplicarse grandes volúmenes de datos. De acuerdo con Amat Rodrigo (2017) (una descripción completa puede verse en Kaufman Rousseeuw (1990)), CLARA selecciona una muestra aleatoria y le aplica el algoritmo de PAM para encontrar los clusters óptimos dada esa muestra. Alrededor de esos medoides se agrupan los elementos de todo el conjunto de datos. La calidad de los medoides resultantes se cuantifica con la suma total de distancias intra-grupos. CLARA repite este proceso un número predeterminado de veces con el objetivo de reducir el sesgo de muestreo. Por último, se seleccionan como clusters finales los obtenidos con los medoides que minimizaron la suma total de distancias intra-grupo. \nFigura 31.3: Clusterización jerárquica con CLARA\n","code":"\nset.seed(123)\nclara_tic <- eclust(tic, \"clara\", k = 3)"},{"path":"no-jerarquico.html","id":"clarans","chapter":"Capítulo 31 Análisis cluster: clusterización no jerárquica","heading":"31.1.2.3 CLARANS ","text":"CLARANS (Clustering Large Applications based upon Randomized Search) es una mezcla de PAM y CLARA.\nComo CLARA puede dar lugar una mala clusterización si uno de los medoides de la muestra está lejos de los mejores medoides, CLARANS trata de superar esta limitación. El algoritmo puede verse en Ng Han (2002).","code":""},{"path":"no-jerarquico.html","id":"técnicas-basadas-en-medianas-bf-k-medianas","chapter":"Capítulo 31 Análisis cluster: clusterización no jerárquica","heading":"31.1.3 Técnicas basadas en medianas: \\(\\bf k\\)-medianas ","text":"Igual que el \\(k\\)-medoides, es una variante del \\(k\\)-medias que utiliza como centros las medianas, para que le afecten ni el ruido ni los valores atípicos. La diferencia con el \\(k\\)-medoides es que la mediana de un grupo tiene por qué ser una de las observaciones. \\(K\\)-medianas utiliza la distancia Manhattan. Volviendo al ejemplo TIC, como se ha podido comprobar, la clusterización de los países de la UE27 en función del uso del las TIC es prácticamente la misma con técnicas jerárquicas aglomerativas como el vecino más lejano, el método de Ward, el del centroide, o algoritmos divisivos como DIANA, que con las técnicas jerárquicas con pre-selección de 3 grupos (\\(k\\)-medias, PAM y CLARA).","code":""},{"path":"no-jerarquico.html","id":"métodos-basados-en-la-densidad-de-elementos","chapter":"Capítulo 31 Análisis cluster: clusterización no jerárquica","heading":"31.2 Métodos basados en la densidad de elementos","text":"Utilizan indicadores de frecuencia, construyendo grupos mediante la detección de aquellas zonas del espacio de las variables (que caracterizan los elementos) densamente pobladas (clusters naturales) y de aquellas otras con un escasa densidad de elementos. Los elementos que forman parte de un conglomerado se consideran ruido. Emulan, pues, el funcionamiento del cerebro humano.La identificación de los grupos (y los parámetros que los caracterizan, cuando se manejan modelos probabilísticos) se lleva cabo haciéndolos crecer hasta que la densidad del grupo más próximo sobrepase un cierto umbral. Por tanto, imponen reglas para evitar el problema de obtener un solo grupo cuando existen puntos intermedios. Se suele suponer que la densidad de elementos en los grupos es Gaussiana si las variables son cuantitativas, y Multinomial si son cualitativas.Se suelen clasificar en: \\(()\\) Las que tienen un enfoque tipológico: los grupos se construyen buscando las zonas con mayor concentración de elementos. Pertenecen este tipo el análisis modal de Wishart, que supone clusters esféricos y dada la complejidad de su algoritmo tuvo mucho éxito, el método TaxMap, que introduce un valor de corte en caso de que los grupos estén claramente aislados (ello lleva que los resultados tengan un cierto grado de subjetividad), y el método de Fortin, también con muy escasa difusión en la literatura.\\((ii)\\) Las que tienen un enfoque probabilístico: las variables que caracterizan los elementos siguen una distribución de probabilidad cuyos parámetros cambian de un grupo otro. Se trata, pues, de agrupar los elementos que pertenecen la misma distribución. Un ejemplo es el método de las combinaciones de Wolf. obstante, estos algoritmos, y otros como, por ejemplo, los de Gitman y Levine, y Catel y Coulter, aunque muy citados en la literatura en español, tuvieron poco éxito.Mayor éxito han tenido otros algoritmos como expectation-maximization (EM), model based clustering (MCLUST), density-based spatial clustering applications noise (DBSCAN), ordering point identify clustering structure clustering (OPTICS), que es una generalización de DBSCAN, wavelet-based cluster (WAVECLUSTER) y density-based clustering (DENCLUE), entre otros. DBSCAN186 es, quizás, el más popular. Incluso ha recibido premios por sus numerosísimas aplicaciones lo largo del tiempo. Soluciona los problemas de los métodos de reasignación, que son buenos para clusters con forma esférica o convexa que tengan demasiados outliers o ruido, pero que fallan cuando los clusters tienen formas arbitrarias. De acuerdo con Amat Rodrigo (2017), DBSCAN evita este problema siguiendo la idea de que, \\(()\\) para que una observación forme parte de un cluster, tiene que haber un mínimo de observaciones vecinas dentro de un radio de proximidad y \\((ii)\\) que los clusters están separados por regiones vacías o con pocas observaciones. Consecuentemente, DBSCAN necesita dos parámetros: el radio (\\(\\epsilon\\)) que define la región vecina una observación (\\(\\epsilon\\)-neighborhood); y el número mínimo de puntos (minPts) u observaciones en ella.187 Los elementos objeto de agrupación se pueden clasificar, en función de su \\(\\epsilon\\)-neighborhood y minPts, como: \\(()\\) elementos centrales, si el número de elementos en su \\(\\epsilon\\)-neighborhood es igual o mayor que minPts; \\((ii)\\) elementos frontera, si son elementos centrales pero pertenecen al \\(\\epsilon\\)-neighborhood de otro elemento que sí es central; y \\((iii)\\) elementos atípicos o de ruido, si verifican ni \\(()\\) ni \\((ii)\\).partir de la clasificación anterior, y para \\(\\epsilon\\)-neighborhood y minPts dados, se origina otra:\n\\(()\\) un elemento \\(Q\\) es denso-alcanzable directamente desde el elemento \\(P\\) si \\(Q\\) está en el \\(\\epsilon\\)-neighborhood de \\(P\\) y \\(P\\) es un elemento central; \\(Q\\) es denso-alcanzable desde \\(P\\) si existe una cadena de objetos \\(\\{Q_{1}=P, Q_2, Q_3,..., Q_{n}\\}\\) tal que \\(Q_{+1}\\) es denso-alcanzable directamente desde \\(Q_{}\\), \\(\\forall 1 \\leq \\leq n\\); \\((iii)\\) \\(Q\\) está denso-conectado con \\(P\\) si hay un elemento \\(R\\) desde el cual \\(P\\) y \\(Q\\) son denso-alcanzables. Los pasos del algoritmo DBSCAN son los siguientes (Amat Rodrigo (2017)): Para cada elemento u observación \\(x_i\\) calcúlese su distancia con el resto de observaciones. Márquese como central si lo es y como visitado si lo es.Para cada elemento u observación \\(x_i\\) calcúlese su distancia con el resto de observaciones. Márquese como central si lo es y como visitado si lo es.Para cada observación marcada como elemento central, si aún ha sido asignada ningún grupo, créese un grupo nuevo y asígnesele él. Búsquense, recursivamente, todas las observaciones denso-conectadas con ella y asígnense al mismo grupo.Para cada observación marcada como elemento central, si aún ha sido asignada ningún grupo, créese un grupo nuevo y asígnesele él. Búsquense, recursivamente, todas las observaciones denso-conectadas con ella y asígnense al mismo grupo.Itérese el mismo proceso para todas todas las observaciones visitadas.Itérese el mismo proceso para todas todas las observaciones visitadas.Aquellas observaciones que tras haber sido visitadas pertenecen ningún cluster se marcan como outliers.Aquellas observaciones que tras haber sido visitadas pertenecen ningún cluster se marcan como outliers.Como resultado del algoritmo DBSCAN se generan clusters que verifican: \\(()\\) todos los elementos que forman parte de un mismo cluster están denso conectados entre ellos; y \\((ii)\\) si un elemento \\(P\\) es denso-alcanzable desde cualquier otro de un cluster, entonces \\(P\\) también pertenece al cluster.El éxito de DBSCAN se debe sus importantes ventajas. De nuevo siguiendo Amat Rodrigo (2017), requiere la especificación previa del número de clusters; require esfericidad (ni ninguna forma determinada) en los grupos; y puede identificar valores atípicos, por lo que la clusterización resultante vendrá influenciada por ellos. También tiene algunas desventajas, como que es un método totalmente determinista puesto que \\(()\\) los puntos frontera que son denso-alcanzables desde más de un cluster pueden asignarse uno u otro dependiendo del orden en el que se procesen los datos; y \\((ii)\\) genera buenos resultados cuando la densidad de los grupos es muy distinta, ya que es posible encontrar un \\(\\epsilon\\)-neighborhood y un minPts válidos para todos la vez.Dado el escaso número de datos de la base de datos TIC2021 del paquete CDR se puede utilizar DBSCAN. Sin embargo, para ilustrar su utilización, la Fig. 31.4 muestra la agrupación en 5 clusters de la base de datos multishapes de la librería factoextra mediante DBSCAN (función dbscan) y \\(k\\)-medias. Se trata de una base de datos que contiene observaciones pertenecientes 5 grupos distintos y con cierto ruido (outliers); en consecuencia, los grupos deberían ser esféricos y DBSCAN sería un algoritmo de agrupación adecuado.\nFigura 31.4: Comparación entre los algoritmos \\(k\\)-means y DBSCAN para el conjunto de datos simulado multishapes\n","code":""},{"path":"no-jerarquico.html","id":"otros-métodos","chapter":"Capítulo 31 Análisis cluster: clusterización no jerárquica","heading":"31.3 Otros métodos","text":"Por último, en el cajón de sastre de otras técnicas de clusterización jerárquicas, merece la pena siquiera mencionar los métodos directos, los de reducción de la dimensionalidad, los de clustering difuso y la clusterización basada en modelos.Los métodos directos agrupan simultáneamente los elementos y las variables. El más conocido es el cluster por bloques (\\(block\\)-; bi-; co-; o two mode \\(clustering\\)). El paquete biclust proporciona varios algoritmos para encontrar clusters en dos dimensiones. Además, es muy recomendable para el pre-procesamiento de los datos y para la visualización y validación de los resultados.\nLas técnicas de reducción de la dimensionalidad buscan factores en el espacio de los elementos (modelo \\(Q\\)-factorial) o de las variables (modelo R-factorial) haciendo corresponder un cluster cada factor. Centrándonos en el modelo \\(Q\\)-factorial, el método parte de la matriz de correlaciones entre los elementos y rota ortogonalmente los factores encontrados. Dado que los elementos pueden pertenecer varios clusters y, por tanto, los clusters pueden solaparse, su interpretación se hace muy difícil.\nLas técnicas de clustering difuso precisamente permiten la pertenencia de un elemento varios clusters, estableciendo un grado de pertenencia cada uno de ellos. El algoritmo de clustering difuso más popular es fuzzy c-medias, muy similar al k-medias pero que calcula los centroides como una media ponderada (la ponderación es la probabilidad de pertenencia) y, lógicamente, proporciona la probabilidad de pertenencia cada grupo. La clusterización basada en modelos tiene un enfoque estadístico y consiste en la utilización de una mixtura finita de modelos estocásticos para la construcción de los grupos. -\nUn vector aleatorio \\(\\bf X\\) procede de una mixtura finita de distribuciones paramétricas si, \\(\\forall \\bf x \\subset \\bf X\\) su función de densidad conjunta se puede escribir como\n\\(f{(\\bf {x} | \\bf {\\psi})}=\\sum_{g=1}^{G} {\\pi_g} f_g {(\\bf {x}|\\bf {\\theta_g})},\\) donde \\(\\pi_g\\) son las proporciones asignadas cada grupo en la mixtura, tal que \\(\\sum_{g=1}^{G} \\pi_g =1\\); \\(f_g(\\bf x|\\theta_g)\\) es la función de densidad correspondiente al \\(g\\)-ésimo grupo y \\(\\bf \\psi=(\\pi_1, \\pi_2,..., \\pi_G, \\bf \\theta_1,\\bf \\theta_2,...\\bf \\theta_G\\)). Las funciones de densidad \\(f_g {(\\bf {x}|\\bf {\\theta_g})}\\) suelen ser idénticas para todos los grupos.En términos menos formales, el clustering basado en modelos considera que los datos observados (multivariantes) han sido generados partir de una combinación finita de modelos componentes (distribuciones de probabilidad, normalmente paramétricas). modo de ejemplo, en un modelo resultante de una mixtura de normales multivariantes (el caso habitual), cada componente (cluster) es una normal multivariante y el componente responsable de la generación de una observación específica determina el grupo al que pertenece dicha observación. Para la estimación de la media y matriz de covarianzas, se suele recurrir al algoritmo expectation-maximization, una extensión del \\(k\\)-medias188. El paquete mclust utiliza la estimación máximo verosímil para estimar dichos modelos con distintos número de clusters, utilizando el Bayesian information criterion (BIC) para la selección del mejor. Sus limitaciones fundamentales son (\\(\\))considerar que las características de los elementos son independientes y (\\(ii\\)) que es recomendable para grandes bases de datos o distribuciones de probabilidad que impliquen un elevado coste computacional.Una revisión de la evolución de la clusterización basada en modelos desde sus orígenes en 1965 puede verse en McNicholas (2016). Para una idea intuitiva, véase Amat Rodrigo (2017).","code":""},{"path":"no-jerarquico.html","id":"nota-final","chapter":"Capítulo 31 Análisis cluster: clusterización no jerárquica","heading":"31.4 Nota final","text":"La elección de que técnica de clusterización, jerárquica o , es una decisión del investigador, y dependerá de cómo quiera realizar la agrupación, la métrica de las variables y la distancia o medida de similaridad elegida. obstante, ambos tipos de técnicas tienen sus ventajas y desventajas, y deberán ser tenidas en cuenta la hora de decidir. Las jerárquicas adolecen de cierta inestabilidad, lo que plantea dudas sobre la fiabilidad de sus resultados. Además, veces es difícil decidir cuántos grupos deben seleccionarse. Suelen recomendarse en caso de conjuntos de datos pequeños. En caso de grandes conjuntos de datos, la literatura suele recomendar las jerárquicas; además, tienen una gran fiabilidad, ya que al permitir la reasignación de los elementos, una incorrecta asignación puede ser corregida posteriomente.","code":""},{"path":"no-jerarquico.html","id":"resumen-28","chapter":"Capítulo 31 Análisis cluster: clusterización no jerárquica","heading":"Resumen","text":"En este capítulo se pasa revista las principales técnicas y algoritmos de agrupación jerárquicas. Primeramente, se abordan los principales métodos de reasignación, y en particular los basados en centroides (método de Forgy y \\(k\\)-medias), medoides (\\(k\\)-medoides, PAM, CLARA, CLARANS) y medianas (\\(k\\)-medianas). Posteriormente, se exponen las técnicas basadas en la densidad de puntos desde las perspectivas tipológica (análisis modal, métodos TaxMap, de Fortin, de Gitman y Levine, y de Catel y Coulter) y probabilistica (método de Wolf), así como se estudia el DBSCAN. Finalmente, se muestran otras técnicas de agrupación jerárquicas como los métodos directos (\\(block\\)-; bi; co-; two mode-$ \\(clustering\\)), los de reducción de la dimensionalidad (modelos \\(Q\\)- y \\(R\\)-factorial), el clustering difuso y los métodos basados en mixturas de modelos.","code":""},{"path":"acp.html","id":"acp","chapter":"Capítulo 32 Análisis de componentes principales","heading":"Capítulo 32 Análisis de componentes principales","text":"José-María Montero\\(^{}\\) y José Luis Alfaro Navarro\\(^{}\\)\\(^{}\\)Universidad de Castilla-La Mancha","code":""},{"path":"acp.html","id":"introducción-13","chapter":"Capítulo 32 Análisis de componentes principales","heading":"32.1 Introducción","text":"En el estudio de cualquier problema de interés, lo ideal es tomar información del mayor número de variables posible, lo cual, actualmente, es un impedimento. Sin embargo, trabajar con muchas variables es incómodo (por ejemplo, si fueran 30 y se estuviese interesado en su correlación dos dos, habría que calcular 435 coeficientes). Además, tener muchas variables implica necesariamente tener mucha información. Si están correlacionadas entre ellas (que suele ser el caso en la realidad), parte de la información que proporcionan es redundante. Por consiguiente, el reto es reducir la dimensionalidad del problema sin reducir la cantidad de información proporcionada por las variables originales, midiéndose dicha cantidad de información través de su variabilidad, en consonancia con el concepto de entropía. En concreto, se adopta como medida de la variabilidad de las variables originales la suma de sus varianzas.\nPues bien, el análisis de componentes principales (ACP, perteneciente al ámbito del aprendizaje supervisado) es una técnica de reducción de la dimensionalidad, un problema importante en ciencia de datos, tanto en el aprendizaje supervisado como supervisado. ACP opera sustituyendo las variables originales por un número reducido de combinaciones lineales de ellas, incorreladas, denominadas componentes principales (c.p.), que capturan un elevado porcentaje de la variabilidad de las variables originales (Hothorn Everitt 2014; B. Boehmke B. y Greenwell 2020). ACP es el primer intento de reducción de la dimensionalidad y el único utilizado tal fin hasta el advenimiento del escalamiento multidimensional (aunque es su función principal) y otras técnicas más complicadas pertenecientes al ámbito del aprendizaje múltiple (manifold learning).\nLa reducción de la dimensionalidad solo es útil en el estudio de fenómenos complejos con un elevado número de dimensiones, sino también para facilitar la implementación de otros métodos de aprendizaje supervisado, como el análisis cluster189 (reduciendo el número de dimensiones utilizar para configurar los clusters), o supervisado, como, por ejemplo, la regresión (reduciendo el número de regresores y haciéndolos incorrelados, evitando así información redundante y la multicolinealidad); o la técnica de partial least squares (PLS, similar la regresión con c.p. pero que, en vez de ignorar la variable respuesta en la determinación las combinaciones lineales, busca aquellas que, además de explicar la varianza de las variables originales, predicen la variable respuesta lo mejor posible).190 También es muy útil para representar gráficamente relaciones multivariantes. En R hay varias opciones para la realización de un ACP: princomp(), prcomp() y PCA(), de la librería FactoMineR (Lê, Josse, Husson 2008), entre otras. Se ha optado por la última porque \\(()\\) incorpora notables mejoras gráficas; \\((ii)\\) permite el ACP con missing values, imputando dichos valores (paquete missMDA); \\((iii)\\) proporciona una descripción e interpretación automática de los resultados, seleccionando los mejores gráficos, mediante el paquete FactoInvestigate; \\((iv)\\) permite la implementación de técnicas híbridas (por ejemplo, clusterización con c.p.); y \\((vi)\\) posibilita la predicción de las coordenadas de individuos y variables adicionales utilizando únicamente inputs del ACP previo.Como ilustración práctica del ACP, se abordará la reducción de la dimensionalidad de un problema del ámbito de la sociedad de la información en la UE-27, en 2021. Se dispone, para 2021 y nivel de país, de información sobre 7 variables: 4 relacionadas con el uso de las TIC por parte de las empresas y 3 relativas al uso de dichas tecnologías por parte de las personas y la equipación TIC de los hogares. Dicha información, así como la descripción de las variables, puede consultarse en la base de datos TIC2021 del paquete CDR.","code":""},{"path":"acp.html","id":"obtención-de-las-componentes-principales","chapter":"Capítulo 32 Análisis de componentes principales","heading":"32.2 Obtención de las componentes principales","text":"","code":""},{"path":"acp.html","id":"descripción-formal-del-proceso","chapter":"Capítulo 32 Análisis de componentes principales","heading":"32.2.1 Descripción formal del proceso","text":"Sea \\(\\mathbf{X^\\prime}=(X_{1},\\dotsc,X_{p})\\) un vector \\(p\\)-dimensional de variables aleatorias con vector de medias \\(\\boldsymbol{\\mu}\\) y matriz de covarianzas conocida \\(\\boldsymbol{\\Sigma}\\). Puesto que los cambios de origen afectan la covarianza, las variables originales se consideran centradas, de tal manera que \\(\\boldsymbol{\\mu}=\\mathbf {0}\\) y \\(\\boldsymbol{\\Sigma}= E\\left (\\mathbf{X^\\prime} \\mathbf{X}\\right)\\). Se trata de encontrar un conjunto de \\(p\\) combinaciones lineales incorreladas de dichas variables, \\(Y_{j}=a_{1j}X_{1}+a_{2j}X_{2}+\\dotsb+a_{pj}X_{p}=\\mathbf{}_{j}^{\\prime}\\mathbf{X}, \\hspace{0,1 cm}{j=1,2,\\dotsc,p}\\), denominadas c.p., que recojan la variabilidad existente en los datos. La idea es ordenar las c.p. tal que \\(V(Y_1)> V(Y_2)>...> V(Y_p)\\), y seleccionar \\(m\\) de ellas (las \\(m\\) primeras), \\(m<p\\), que capturen un elevado porcentaje de la variabilidad de los datos.NotaGeométricamente, las c.p. representan un nuevo sistema de coordenadas obtenido mediante la rotación de los ejes originales. Los nuevos ejes representan las direcciones de máxima variabilidad y proporcionan una descripción más simple de la estructura de covarianza.La varianza de cada componente y la covarianza entre ellas vienen dadas por:\n\\[\\begin{equation}\n\\begin{split}\nVar(Y_{j})=\\mathbf{}_{j}^{\\prime}\\mathbf{\\Sigma}\\mathbf{}_{j},    \\quad{\\forall j=1,2,\\dotsc,p}, \\\\\nCov(Y_{j}, Y_{k})=\\mathbf{}_{j}^{\\prime}\\mathbf{\\Sigma}\\mathbf{}_{k},      \\quad{\\forall j, k \\{j\\neq k\\} =1,2,\\dotsc,p}.\n\\end{split}\n\\tag{32.1}\n\\end{equation}\\]Obtención de la primera componente principalLa primera c.p., \\(Y_1\\), se obtiene seleccionando el vector \\(\\mathbf{}_{1}\\) que maximice su varianza. Sin embargo, dado que la varianza de cada c.p. puede incrementarse arbitrariamente multiplicando \\(\\mathbf{}_{1}\\) por una constante, se impone la condición \\(\\mathbf{}_{1}^{\\prime}\\mathbf{}_{1}=1\\); es decir, se normalizan los vectores, de tal forma que tengan longitud unitaria. Por tanto, se trata de encontrar el vector \\(\\mathbf{}_{1}\\) que maximiza \\(Var(Y_{1})=\\mathbf{}_{1}^{\\prime}\\mathbf{\\Sigma}\\mathbf{}_{1}\\) sujeto que \\(\\mathbf{}_{1}^{\\prime}\\mathbf{}_{1}=1\\). En otros términos, se selecciona el vector \\(\\mathbf{}_{1}\\) que maximiza el lagrangiano: \\[\\begin{equation}\n\\mathcal{L}(\\mathbf {\\mathbf }_{1})=\\mathbf{}_{1}^{\\prime}\\mathbf{\\Sigma}\\mathbf{}_{1}-\\lambda (\\mathbf{}_{1}^{\\prime}\\mathbf{}_{1}-1).\n(\\#eq:ecuacion2acp)\n\\end{equation}\\]\nPara ello, se deriva respecto \\(\\mathbf{}_{1}\\) y \\(\\lambda\\), y se igualan cero dichas derivadas: \\[\\begin{equation}\n\\begin{split}\n\\frac{\\partial\\mathcal{L} (\\mathbf{}_{1})}{\\partial\\mathbf{}_{1}}=2\\mathbf{\\Sigma}\\mathbf{}_{1}-2\\lambda\\mathbf{}_{1}=(\\mathbf{\\Sigma}-\\lambda\\mathbf{})\\mathbf{}_{1}= \\mathbf{0}, \\\\\n\\frac{\\partial \\mathcal{L}(\\mathbf{}_{1})}{\\partial\\lambda}=\\mathbf{}_{1}^{\\prime}\\mathbf{}_{1}-1=0.\n\\end{split}\n(\\#eq:ecuacion3acp)\n\\end{equation}\\] La primera ecuación tendrá solución distinta del vector nulo cuando \\((\\mathbf{\\Sigma}-\\lambda\\mathbf{})\\) sea singular. Es decir, cuando \\(|\\mathbf{\\Sigma}-\\lambda\\mathbf{}|=0\\), o en otros términos, cuando \\(\\lambda\\) sea un autovalor de \\(\\mathbf{\\Sigma}\\). Dado que,\\(\\boldsymbol{\\Sigma}\\) es semidefinida positiva y, en general, tendrá p autovalores negativos,y que en el proceso de optimización, premultiplicando \\((\\mathbf{\\Sigma}-\\lambda\\mathbf{})\\mathbf{}_{1}= \\mathbf{0}\\) por \\(\\mathbf{}^{\\prime}_{1}\\) y teniendo en cuenta que \\(\\mathbf{}_{1}^{\\prime}\\mathbf{}_{1}=1\\), resulta que \\(\\lambda= \\mathbf{}_{1}^{\\prime}\\mathbf{\\Sigma}\\mathbf{}_{1}= V({Y_1})\\),191se seleccionará el mayor de los autovalores de \\(\\boldsymbol{\\Sigma}\\), obteniéndose el autovector \\(\\mathbf{}_{1}\\) de tal forma que cumpla la condición \\(\\mathbf{}_{1}^{\\prime}\\mathbf{}_{1}=1\\).\nObtención de la segunda componente principal\\(Y_{2} = \\mathbf{}_{2}^{\\prime}\\bf{X}\\) se obtiene igual que \\(Y_{1}\\), pero añadiendo la condicion de incorrelación con \\(Y_{1}\\): \\(Cov(Y_{1}, Y_{2}) = \\mathbf{}_{2}^{\\prime} \\mathbf{\\Sigma} \\mathbf{}_{1} = 0,\\) o equivalentemente, \\(\\mathbf{}_{2}^{\\prime}\\mathbf{}_{1}=0\\) (\\(\\mathbf a_1\\) y \\(\\mathbf a_2\\) ortogonales).192Por tanto, el lagrangiano maximizar es:Derivando respecto \\(\\bf{}_{2}\\) e igualando cero:Premultiplicando por \\(\\mathbf{}_{1}^{\\prime}\\) y considerando la condición de ortogonalidad, se tiene que \\(\\gamma=2 Cov (Y_1,Y_2)=0\\), con lo que \\(\\frac{\\partial \\mathcal{L}(\\mathbf {}_{2})}{\\partial\\mathbf{}_{2}}= 2{\\mathbf \\Sigma}{\\mathbf {}_2} - 2 \\lambda \\mathbf a_2 = 0\\), que implica que \\((\\mathbf \\Sigma -\\lambda \\mathbf )\\mathbf a_2=0\\).Siguiendo el mismo razonamiento que en la obtención de la primera componente, se elige el segundo mayor autovalor de \\(\\mathbf\\Sigma\\)193, \\(\\lambda_{2}\\), siendo \\({\\bf{}}_{2}\\) el autovector asociado él.Obtención del resto de las componentes principales\nRepitiendo este procedimiento, se obtienen las p c.p., siendo los coeficientes de la j-ésima los componentes del autovector asociado al j-ésimo mayor autovalor de \\(\\mathbf \\Sigma\\).El vector de c.p. se puede expresar como \\(\\mathbf{Y}=\\mathbf{}^{\\prime}\\mathbf{X}\\), donde \\(\\mathbf{} = [{\\bf{}}_{1}, {\\bf{}}_{2},\\dotsc,{\\bf{}}_{p}]\\) es la matriz de autovectores (ortogonales) obtenidos.","code":""},{"path":"acp.html","id":"cuestiones-importantes-en-el-análisis-de-componentes-principales","chapter":"Capítulo 32 Análisis de componentes principales","heading":"32.2.2 Cuestiones importantes en el análisis de componentes principales","text":"","code":""},{"path":"acp.html","id":"varianza-de-las-variables-originales-y-las-componentes-principales","chapter":"Capítulo 32 Análisis de componentes principales","heading":"32.2.2.1 Varianza de las variables originales y las componentes principales","text":"La matriz de covarianzas de las c.p, \\(\\mathbf V(\\mathbf Y)=\\mathbf ^\\prime \\mathbf \\Sigma \\mathbf \\), coincide con \\(\\mathbf{\\Lambda}\\), que es una matriz diagonal, puesto que las c.p. están incorreladas y sus varianzas (los valores de la diagonal principal) son los autovalores de \\(\\boldsymbol{\\Sigma}\\) . En consecuencia: \\[\\begin{equation}\n\\begin{split}\n\\sum_{=1}^{p}Var(Y_{})= tr (\\mathbf{\\Lambda})= tr (\\mathbf{}^{\\prime} \\mathbf{\\Sigma} \\mathbf{}) = tr (\\mathbf{\\Sigma} \\mathbf{} \\mathbf{}^{\\prime})\n= tr (\\mathbf{\\Sigma}) = \\sum_{=1}^{p}Var(X_{}),\n\\end{split}\n(\\#eq:ecuacion6)\n\\end{equation}\\] pudiéndose comprobar que la suma de las varianzas de las variables originales194 coincide con la suma de las varianzas de las c.p.Por tanto, la j-ésima c.p. captura un porcentaje de la variabilidad de las variables originales cifrado en \\(\\frac{\\lambda_{j}}{\\sum_{j=1}^{p} \\lambda_{j}} 100\\), siendo \\(\\frac{\\sum_{j=1}^m\\lambda_{j}}{\\sum_{j=1}^{p} \\lambda_{j}} 100\\) la proporción capturada por las \\(m\\) primeras componentes.","code":""},{"path":"acp.html","id":"componentes-principales-a-partir-de-variables-estandarizadas","chapter":"Capítulo 32 Análisis de componentes principales","heading":"32.2.2.2 Componentes principales a partir de variables estandarizadas","text":"\nmenudo, solo se centran las variables originales sino que también se estandarizan, para que tengan varianza unitaria.195 La razón es que, si las variables originales presentan grandes diferencias en sus escalas de medida o en los rangos de las unidades de medida (edad en años, altura en metros, longitud en kilómetros…), sus combinaciones lineales tendrán poco significado, porque las variables que las conforman son “igualmente importantes” y en la primera componente tendrá un gran peso la variable original con mayor magnitud (Chatfield Collins 1980). Si fuera el caso, es mejor partir de \\(\\bf\\Sigma\\); además, la teoría muestral de las c.p. es mucho más compleja cuando las variables están estandarizadas que cuando lo están (Morrison 1976).El mecanismo de obtención de las c.p. cambia en absoluto, pero su punto de arranque ya es \\(\\boldsymbol{\\Sigma}\\) sino \\(\\bf{P}\\), la matriz de correlaciones de dichas variables. Los autovectores de \\(\\bf{P}\\) son, en general, distintos los de \\(\\boldsymbol{\\Sigma}\\). Además, la suma de los autovalores, como coincide con la suma de las varianzas de las variables originales, es p, luego el porcentaje de la variación total capturada por la componente j-ésima es \\(\\frac{\\lambda_{j}}{p} 100\\), siendo \\(\\frac{\\sum_{j=1}^m\\lambda_{j}}{p} 100\\) la proporción capturada por las \\(m\\) primeras componentes .","code":""},{"path":"acp.html","id":"correlación-entre-las-variables-originales-y-las-componentes-principales","chapter":"Capítulo 32 Análisis de componentes principales","heading":"32.2.2.3 Correlación entre las variables originales y las componentes principales","text":"Considérese la variable original \\(X_{}\\) y la c.p. \\(Y_{j}=a_{1j}X_{1}+a_{2j}X_{2}+\\dotsb+a_{pj}X_{p}=\\mathbf{}_{j}^{\\prime}\\mathbf{X}\\). Dado que \\(\\mathbf{X}^{\\prime} = [X_{1},\\dotsb,X_{p}]\\), entonces \\(X_{}=\\mathbf{e}_{}^{\\prime}\\mathbf{X}\\), donde \\(\\mathbf{e}_{}\\) es un vector de ceros excepto un 1 en la -ésima posición.Entonces, como \\(({\\bf \\Sigma}-{\\lambda_{j}} {\\bf )}{\\bf {}}_j=0\\), se tiene que \\({\\bf\\Sigma} {\\bf }_j={\\lambda}_j {\\bf {}}_j\\) y que:donde \\(\\sigma_{ii}\\) es el elemento -ésimo de la diagonal principal de \\(\\boldsymbol{\\Sigma}\\).Si se parte de variables estandarizadas, entonces se tiene que \\(r_{Z_{},Y_{j}}=\\sqrt{\\lambda_{j}}a_{ij}\\), donde ahora \\(\\lambda_{j}\\) es el j-ésimo autovalor de \\(\\bf{P}\\) y \\(a_{ij}\\) es el elemento -ésimo de su autovector asociado. Sin embargo, el coeficiente de correlación lineal varía por el hecho de haber estandarizado las variables originales.Como se verá posteriormente, estos dos coeficientes, \\(r_{X_{},Y_{j}}\\) y \\(r_{Z_{},Y_{j}}\\), serán de gran utilidad en la interpretación de las c.p. Además, como \\(r_{X_{},Y_{j}}\\) coincide con el coseno del ángulo que forma \\(X_{}\\) con \\(Y_{j}\\) (que es la proyección o coordenada de \\(X_{}\\) en el eje de \\(Y_{j}\\)), resulta de gran ayuda para representar las variables originales en el espacio de las componentes y, por consiguiente, para la interpretación de estas últimas. mayor coseno, mayor correlación lineal entre \\(X_{}\\) e \\(Y_{j}\\). Matricialmente, y denominando \\({\\mathbf }^*\\) la matriz de coeficientes de correlación lineal entre las variables originales estandarizadas y las c.p., se tiene que \\({\\mathbf }^{*}= {\\bf } {\\bf\\Lambda}^{\\frac {1}{2}}\\). \\(\\mathbf{}^*\\) (imprescindible en la interpretación de las c.p.) cambia por el hecho de estandarizar también las c.p.Los cuadrados de los elementos de \\(\\bf ^*\\) expresan la proporción de varianza de la variable \\(X_i\\) explicada por la componente \\(Y_j\\). Por tanto, la suma de los cuadrados de las filas de \\(\\bf ^*\\) será la unidad. Se denomina contribución (individual) de \\(X_{}\\) la componente \\(Y_{j}\\) la cantidad \\(\\frac{r_{X_i,Y_j}^2} {\\sum_{=1}^{p} r_{{X_{},Y_{j}}}^2}=\\frac{cos(X_i,Y_j)}{\\sum_{=1}^{p}{cos(X_i,Y_j)}}\\).Otras dos expresiones interesantes que involucran \\(\\bf ^*\\) son \\(\\bf ^*\\bf ^*{^\\prime}=\\bf {R}\\) y \\(\\bf ^*{^\\prime}\\bf ^*=\\bf {\\Lambda}.\\)","code":""},{"path":"acp.html","id":"estimación-de-las-componentes-principales","chapter":"Capítulo 32 Análisis de componentes principales","heading":"32.3 Estimación de las componentes principales","text":"\nHasta el momento, se han derivado las c.p. suponiendo conocida la matriz de covarianzas poblacional \\(\\boldsymbol{\\Sigma}\\) (o la de correlaciones \\(\\bf P\\)). Sin embargo, este suele ser el caso en la práctica, por lo que se sustituyen por sus homónimas muestrales \\({\\bf{S}}=\\frac {1}{N}\\bf{X}^\\prime\\bf{X}\\) (o \\(\\bf{R}\\)). Nada cambia en el proceso de obtención de las c.p., salvo que el punto de partida es \\(\\bf{S}\\) (o \\(\\bf{R}\\)) y que los valores de los autovalores y autovectores asociados son estimaciones.En el ejemplo que nos ocupa, \\(\\bf R\\) puede verse en la Fig. 32.1. Puede apreciarse que la correlación entre las variables es notable en la mayoría de los casos, lo que invita analizar el problema con menos variables e incorreladas, es decir, mediante ACP.\nFigura 32.1: Matriz de correlaciones\n","code":"\nlibrary(\"CDR\")\ndata(\"TIC2021\")\nTIC <- TIC2021\n\nlibrary(\"corrplot\")\ncorrplot.mixed(cor(TIC))\nlibrary(\"FactoMineR\")\nacp <- PCA(TIC, ncp = 7, graph = FALSE)"},{"path":"acp.html","id":"numcomp","chapter":"Capítulo 32 Análisis de componentes principales","heading":"32.4 Número de componentes a retener","text":"Dado que la finalidad de la técnica de componentes principales es la reducción de la dimensionalidad, una decisión clave es el número \\(m\\) de componentes retener. Los criterios más populares para tomar esta decisión son:Seleccionar un número de componentes que capturen, entre todas, un porcentaje de la variabilidad total determinadoDicho porcentaje suele estar alrededor del 80%, si bien, si el número de c.p. es elevado, su interpretación es muy difícil.Criterio de la media aritmética o criterio de Kaiser Dado que la variabilidad total coincide con la suma de los autovalores, se seleccionan aquéllas c.p. cuya varianza exceda la varianza media. Es decir, se selecciona la componente j-ésima si \\(\\lambda_{j}> \\bar{\\lambda}\\) (si se parte de \\(\\mathbf\\Sigma\\)) o si \\(\\lambda_{j}> 1\\) (si se parte de \\(\\mathbf R\\)). En caso de valores anómalos (outliers) es recomendable utilizar la media geométrica en vez de la aritmética. Criterio de Catell Se basa en la representación gráfica de los autovalores vs. su número de orden, que se denomina gráfico de sedimentación porque se asemeja la ladera de una montaña con su correspondiente zona de sedimentación. Se seleccionan las c.p. asociadas los autovalores previos la zona de sedimentación. En general, el criterio de Catell tiende incluir demasiadas c.p., al contrario que el de la media, que tiende incluir demasiado pocas (sobre todo si \\(p<20\\)) (Mardia, Kent, Bibby 1979a).Otros criteriosOtros criterios menos populares son la validación cruzada, el test de esfericidad o igualdad de autovalores de Anderson (requiere normalidad multivariante) y el criterio del bastón roto (véase Cuadras (2007) para los dos últimos). Para grandes conjuntos de datos, (jobson1992?) propone un criterio basado en la partición de la muestra en submuestras mutuamente excluyentes, similar la validación cruzada.La Fig. 32.2 muestra el gráfico de sedimentación en el ejemplo que nos ocupa. Puede apreciarse que con tan solo las dos primeras c.p. se captura el 82,07% de la variabilidad total de las siete variables originales.\nFigura 32.2: Gráfico de sedimentación\n\n","code":"\nround(acp$eig[1:7, 1:2], 3)\n#>        eigenvalue percentage of variance\n#> comp 1      4.644                 66.341\n#> comp 2      1.101                 15.731\n#> comp 3      0.547                  7.814\n#> comp 4      0.328                  4.679\n#> comp 5      0.191                  2.731\n#> comp 6      0.124                  1.768\n#> comp 7      0.066                  0.937\nlibrary(\"factoextra\")\nfviz_eig(acp, addlables = TRUE)"},{"path":"acp.html","id":"interpretación-de-las-componentes-principales","chapter":"Capítulo 32 Análisis de componentes principales","heading":"32.5 Interpretación de las componentes principales","text":"\nUna primera vía consiste en analizar el signo y la magnitud de los coeficientes (cargas o loadings) de cada variable original en cada componente.Una segunda vía es el análisis de los \\(r_{X_{},Y_{j}}, \\forall {,j}\\).\nFigura 32.3: Gráfico de cosenos o coeficientes de correlación variables-componentes\nSe puede utilizar cualquiera de las dos vías, pues los resultados serán contradictorios. Sin embargo, algunos autores recomiendan utilizar sólo la segunda, pues los \\(r_{X_{},Y_{j}}\\) sólo tienen en cuenta la variable original considerada y el resto; es decir, se estarían interpretando las componentes desde una perspectiva univariante.NotaTambién es de interés la siguiente consideración: cuando las variables originales están correlacionadas positivamente, la primera c.p. tiene todas sus coordenadas del mismo signo y puede interpretarse como un promedio ponderado de todas ellas.\nEn la matriz de cargas (o loadings) se aprecia que la primera c.p. es una media ponderada (con ponderaciones similares) de las variables originales, mientras que esales, hbroad y hiacc cargan fuertemente en la segunda (esales positivamente y las otras dos de forma negativa). La interpretación desde la perspectiva univariante de los coeficientes de correlación lineal es prácticamente la misma. Por ello, cabe interpretar la primera c.p. como un indicador general del uso de las TIC, mientras que la segunda, positivamente relacionada con la dotación TIC de las empresas pero con una fuerte relación negativa con la de los individuos y hogares, pudiera estar relacionada con las ayudas públicas la implantación de TICs en el tejido empresarial.\nFigura 32.4: Contribución de las variables originales las componentes retenidas\n","code":"\nloadings <- sweep(acp$var$coord, 2, sqrt(acp$eig[1:7, 1]))\nround(loadings, 3)\n#>            Dim.1  Dim.2  Dim.3  Dim.4  Dim.5  Dim.6  Dim.7\n#> ebroad    -1.410 -0.854 -1.358 -0.561 -0.303 -0.271 -0.259\n#> esales    -1.604 -0.318 -0.411 -0.404 -0.310 -0.262 -0.225\n#> esocmedia -1.317 -0.858 -0.645 -1.062 -0.536 -0.311 -0.244\n#> eweb      -1.264 -0.865 -0.825 -0.356 -0.773 -0.422 -0.284\n#> hbroad    -1.343 -1.550 -0.570 -0.495 -0.413 -0.159 -0.386\n#> hiacc     -1.290 -1.496 -0.675 -0.495 -0.413 -0.348 -0.053\n#> iuse      -1.217 -1.135 -0.652 -0.587 -0.255 -0.608 -0.331\nround(acp$var$cor, 3)\n#>           Dim.1  Dim.2  Dim.3  Dim.4  Dim.5  Dim.6  Dim.7\n#> ebroad    0.745  0.195 -0.618  0.012  0.134  0.081 -0.003\n#> esales    0.551  0.731  0.328  0.169  0.128  0.090  0.031\n#> esocmedia 0.838  0.191  0.095 -0.490 -0.099  0.040  0.012\n#> eweb      0.891  0.185 -0.085  0.217 -0.336 -0.070 -0.028\n#> hbroad    0.812 -0.501  0.170  0.077  0.024  0.193 -0.129\n#> hiacc     0.865 -0.446  0.065  0.077  0.024  0.003  0.203\n#> iuse      0.938 -0.086  0.087 -0.014  0.183 -0.256 -0.075\nfviz_pca_var(acp,\n  col.var = \"contrib\",\n  gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n  repel = TRUE\n)\nacp1 <- fviz_contrib(acp, choice = \"var\", axes = 1, top = 10)\nacp2 <- fviz_contrib(acp, choice = \"var\", axes = 2, top = 10)\nlibrary(patchwork)\nacp1 + acp2"},{"path":"acp.html","id":"reproducción-de-los-datos-tipificados-y-de-la-matriz-de","chapter":"Capítulo 32 Análisis de componentes principales","heading":"32.6 Reproducción de los datos tipificados y de la matriz de","text":"\nEn la práctica, el punto de partida del ACP es la matriz \\(\\bf R\\), y en tal caso se suelen estandarizar también las c.p. Pues bien, se tiene que:\n\\[{\\bf Y^*}= {\\bf X } {\\bf \\Lambda}^{-\\frac {1}{2}}= \\bf X\\bf \\bf{\\Lambda^{\\frac {1}{2}}} \\bf\\Lambda^{-1}=\\bf X\\bf ^*\\bf\\Lambda^{-1}=\\bf X\\bf F,\\]\ndonde \\(\\bf F\\) es la matriz de puntuaciones de las c.p. La expresión anterior proporciona las coordenadas de los \\(N\\) elementos en el espacio de las c.p. y, por tanto, sirve de ayuda en la interpretación de éstas. La estandarización de las c.p. asegura que los \\(m\\) ejes (componentes) tengan una métrica homogénea que facilitará la visualización e interpretación. Dichas coordenadas también constituyen el input de técnicas híbridas como, por ejemplo, regresión con c.p., cluster o PLS.En este caso:De la expresión anterior se deduce que \\({\\bf X}={\\bf Y} {\\bf \\Lambda}^{-\\frac{1}{2}} {\\bf\\Lambda}^{\\frac{1}{2}}{\\bf }^\\prime={\\bf Y^*}_{N \\times m}{\\bf ^*}^\\prime_{m \\times p}\\), expresión que permite reproducir la matriz \\(\\bf X\\) partir de las \\(m\\) primeras c.p. estandarizadas. En consecuencia, la reproducción de \\(\\bf R\\) partir de las \\(m\\) primeras c.p. estandarizadas se lleva cabo como sigue:\n\\[\\begin{eqnarray}\n{\\bf R}_{p \\times p} = \\frac {1}{N}  {\\bf X}^{\\prime}_{p \\times N} {\\bf X}_{N \\times p}= {\\bf }^{*}_{p \\times m} \\frac {1} {N} {\\bf Y}^{*^{\\prime}}_{m \\times N} {\\bf Y}^{*}_{N \\times m} {\\bf }^{*^{\\prime}}_{m \\times p} \\nonumber\\\\\n={\\bf }^{*}_{p \\times m} {\\bf }_{m\\times m}{\\bf }^{*^{\\prime}}_{m \\times p}={\\bf }^{*}_{p \\times m}{\\bf }^{*^{\\prime}}_{m \\times p}.\n\\end{eqnarray}\\]Debajo se muestran las tres primeras filas de la reproducción de \\(\\bf R\\) partir de las dos primeras c.p. De la comparación de sus valores con los de la verdadera \\(\\bf R\\) (Fig. 32.1) se concluye que se trata de una buena reproducción.196","code":"\npuntuaciones <- acp$ind$coord\nround(puntuaciones[1:5, ], 3)\n#>     Dim.1  Dim.2  Dim.3  Dim.4  Dim.5  Dim.6  Dim.7\n#> BE  1.651  1.053  0.310 -0.238 -0.196  0.296 -0.196\n#> BG -4.759 -0.127 -0.128 -0.223  0.023 -0.013 -0.106\n#> CZ -0.324  0.875 -0.564  0.870 -0.082 -0.032 -0.345\n#> DK  3.188  1.331  0.497  0.262  0.343 -0.300  0.319\n#> DE  0.024  0.144 -0.183  0.560 -0.871 -0.485  0.115\nmatrix <- acp$var$coord[, 1:2] %*% t(acp$var$coord)[1:2, ]\nround(matrix[1:3, ], 3)\n#>           ebroad esales esocmedia  eweb hbroad hiacc  iuse\n#> ebroad     0.593  0.553     0.662 0.700  0.507 0.557 0.682\n#> esales     0.553  0.839     0.602 0.626  0.081 0.151 0.455\n#> esocmedia  0.662  0.602     0.740 0.782  0.585 0.640 0.770"},{"path":"acp.html","id":"limitaciones-del-análisis-de-componentes-principales","chapter":"Capítulo 32 Análisis de componentes principales","heading":"32.7 Limitaciones del análisis de componentes principales","text":"\nUna primera limitación es que su implementación sólo es posible si todas las variables se trabajan bajo un nivel de análisis numérico. Otra limitación importante es el supuesto subyacente de que los datos observados son combinación lineal de una cierta base. Es decir, sólo se consideran las combinaciones lineales de las variables originales. Otros métodos de reducción de la dimensionalidad como, por ejemplo, el t-distributed stochastic neighbor embedding (t-SNE), o la versión Kernel de la técnica, que también funcionan con linealidad, superan esta limitación.\nAdemás, el hecho que todas las c.p. sean combinaciones lineales de todas las variables originales dificulta su interpretación. Para superar esta limitación, han surgido algunas alternativas, como el sparse PCA, que obtiene las c.p. como un problema minimización del error de reconstrucción forzando que los autovectores tengan una gran parte de sus componentes nula.El t-SNE es la única alternativa lineal procedente de la comunidad de machine learning. Otras, denominadas actualmente “aprendizaje múltiple” (manifold learning) , incluyen el Sammon’s mapping, el curvilinear component analysis (CCA) y sus variantes: los Laplacian eigenmaps y el maximum variance unfolding (MVU); véase Wismüller et al. (2010).Finalmente, señalar que el ACP es una técnica matemática que requiere que las variables originales sigan una distribución normal multivariante, aunque, si así fuera, se podría dar una interpretación más profunda de las c.p.ResumenEl ACP es una técnica de reducción de la dimensionalidad que captura un gran porcentaje de la variabilidad de un conjunto de variables correladas partir de un número mucho menor de componentes latentes (las componentes principales) incorreladas. La piedra angular de la construcción de estas componentes son los autovalores de la matriz de covarianzas (o de correlaciones) de las variables originales. En el ACP son cuestiones importantes, entre otras, \\(()\\) la determinación del número de componentes retener, \\((ii)\\) su interpretación y \\((iii)\\) la cuantificación del valor de las componentes para cada observación (puntuaciones), que constituyen el input de técnicas híbridas como, por ejemplo, regresión con componentes principales, cluster o partial least squares. ","code":""},{"path":"análisis-factorial.html","id":"análisis-factorial","chapter":"Capítulo 33 Análisis factorial","heading":"Capítulo 33 Análisis factorial","text":"José-María Montero\\(^{}\\) y José Luis Alfaro Navarro\\(^{}\\)\\(^{}\\)Universidad de Castilla-La Mancha","code":""},{"path":"análisis-factorial.html","id":"introaf","chapter":"Capítulo 33 Análisis factorial","heading":"33.1 Introducción","text":"Según el trabajo pionero de Harman (1976), el objeto del Análisis Factorial (AF) es la representación de una variable \\(X_j\\) en términos de varios factores subyacentes observables197. En el marco lineal, y considerando \\(p\\) variables198, hay varias alternativas dependiendo del objetivo que se pretenda:La captura de la mayor cantidad posible de la varianza de dichas variables (o “explicación” de su varianza).La mejor reproducción (o “explicación”) de sus correlaciones observadas.modo introductorio, supónganse dos variables politómicas que surgen de las respuestas de \\(N\\) futbolistas profesionales dos preguntas: (1) ¿Está usted gusto en el club? y (2) ¿Se quedaría en el club la siguiente temporada? Las posibles respuestas son: 1, 2, 3, 4, 5 (1 “en total desacuerdo” y 5 “totalmente de acuerdo”).Cada variable tiene su varianza (nula si todos los futbolistas opinasen igual y máxima si la mitad marcase el 1 y la otra mitad el 5). Esta varianza puede ser común o compartida por las dos variables, o . Lo normal es que cuanto más gusto estén los futbolistas en su club mayor sea su deseo de permanecer en él la siguiente temporada, por lo que gran parte de la variabilidad de cada una de las variables sería compartida (ya que la relación -lineal- entre ellas es positiva). El resto de la variabilidad sería específica de cada variable (puede que un futbolista esté muy bien en el club, pero quiera ir otro más prestigioso; o que esté mal, pero su familia le encante la ciudad) o residual (normalmente debida factores de medida). El porcentaje de varianza compartida se mide través del coeficiente de determinación lineal, \\(r^2\\). El resto, hasta la varianza unidad, o el 100%, es varianza única de cada variable, que incluye tanto la específica como la residual.\nDe acuerdo con De la Fuente (2011), en el AF caben dos enfoques:199El análisis de toda la varianza (común y común).El análisis, únicamente, de la varianza común. Ambos caben bajo el paraguas genérico del AF; ambos se basan en las relaciones entre las variables para identificar grupos de ellas asociadas entre sí. Sin embargo, del primero se ocupa el ACP (Cap. 32) y, si se parte de la matriz de correlaciones (cuyas entradas fuera de la diagonal principal, al cuadrado, indican la proporción de varianza compartida por las variables que se cruzan en dicha entrada), ésta lleva unos en la diagonal principal. Al segundo se le aplica la denominación de AF y en la matriz de correlaciones se sustituyen los unos de la diagonal principal por la varianza que cada variable comparte con las demás (su comunalidad). Por eso se dice que el objetivo del AF es la explicación de la varianza compartida o común de las variables en estudio mediante una serie de factores comunes latentes.200\nEl AF puede ser exploratorio o confirmatorio. En el primero se establecen consideraciones priori sobre el número de factores comunes extraer, sino que éste se determina lo largo del análisis. Por el contrario, en el segundo se trata de contrastar hipótesis relativas al número de factores comunes, así como sobre qué variables serán agrupadas o tendrán más peso en cada factor. Una práctica habitual es validar mediante el análisis factorial confirmatorio los modelos teóricos basados en los resultados del análisis factorial exploratorio. Sin embargo, Pérez-Gil, Chacón, Moreno (2000) alertan de los peligros de esta práctica. Este capítulo considera la versión exploratoria del AF.\nefectos prácticos, se utilizará la base de datos TIC2021 del paquete CDR, ya trabajada en Cap. 32 para el ACP, relativa al uso (por empresas e individuos) y equipación (de los hogares) de las TIC en los países de la UE-27, así como la librería psych (Revelle 2022) de R.","code":"\nlibrary(\"psych\")\nlibrary(\"CDR\")\ndata(\"TIC2021\")"},{"path":"análisis-factorial.html","id":"elementos-teóricos-del-análisis-factorial","chapter":"Capítulo 33 Análisis factorial","heading":"33.2 Elementos teóricos del análisis factorial","text":"","code":""},{"path":"análisis-factorial.html","id":"modelobasicoaf","chapter":"Capítulo 33 Análisis factorial","heading":"33.2.1 Modelo básico y terminología","text":"Considérense \\(p\\) variables \\(\\{X_1, X_2,..., X_p\\}\\) y \\(N\\) elementos, objetos o individuos, siendo las matrices de datos, \\(\\bf X\\), y datos estandarizados, \\(\\bf Z\\), las siguientes:\\[\\bf X=\\left(\\begin{matrix} x_{11} & x_{12} & \\cdots &x_{1N}\\\\\nx_{21}&x_{22}&\\cdots&x_{2N}\\\\\n\\vdots&\\vdots&\\ddots     &\\vdots\\\\\nx_{p1}&x_{p2}&\\cdots&x_{pN}\\\\\n\\end {matrix}\\right),\\quad\n\\bf Z=\\left(\\begin{matrix} z_{11} & z_{12} & \\cdots &z_{1N}\\\\\nz_{21}&z_{22}&\\cdots&z_{2N}\\\\\n\\vdots&\\vdots&\\ddots     &\\vdots\\\\\nz_{p1}&z_{p2}&\\cdots&z_{pN}\\\\\n\\end {matrix}\\right),\\]donde el primer subíndice indica la variable y el segundo el elemento.Mientras que el enfoque de componentes principales está representado por:\n\\[\\begin{equation} Z_{j}=a_{j1}F_1+a_{j2}F_2+ \\cdots +a_{jp}F_p, \\quad j=1,2,\\cdots,p,\n\\tag{33.1}\n\\end{equation}\\]\nen el enfoque AF clásico el modelo teórico es:\n\\[\\begin{equation}\nZ_{j}=a_{j1}F_1+a_{j2}F_2+ \\cdots +a_{jk}F_k + b_jSP_j+c_jE_j, \\quad j=1,2,\\cdots,p,\n\\tag{33.2}\n\\end{equation}\\]\ndonde \\(Z_{j}, \\hspace{0,1cm} j=1,2,\\cdots, p\\), se modeliza, linealmente, en términos de \\(()\\) \\(k\\ll p\\) factores comunes, \\(F_m,\\hspace{0,1cm} m=1,2,\\cdots,k\\), que dan cuenta de la correlaciones entre las variables \\(Z_{j}, \\hspace{0,1cm} j=1,2,\\cdots, p\\), y \\((ii)\\) un factor específico, \\(SP_j, \\hspace{0,1cm} j=1,2,\\cdots,p\\), y un término de error, \\(E_j, \\hspace{0,1cm} j=1,2,\\cdots,p,\\) que dan cuenta de la varianza compartida (específica y residual, respectivamente). Los coeficientes \\(a_{jm}\\) se denominan cargas factoriales y, aunque su notación es igual que en el modelo de componentes principales, tienen por qué coincidir; el problema básico del AF es precisamente la estimación de dichas cargas. En lo que sigue, se aunarán el factor específico y el término de error de \\(Z_{j}\\) en un factor único, \\(U_{j}\\), con lo que:\n\\[\\begin{equation}\nZ_{j}=a_{j1}F_1+a_{j2}F_2+ \\cdots +a_{jk}F_k + d_jU_j, \\quad j=1,2,\\cdots,p,\n\\tag{33.3}\n\\end{equation}\\]Los supuestos del modelo (33.3) son los siguientes:Como en la práctica los factores comunes y únicos son desconocidos, sin pérdida de generalidad pueden suponerse con media cero y varianza unitaria;Los factores únicos se suponen independientes entre sí y de los factores comunes; Y dado que los factores involucrados en el modelo se consideran variables aleatorias, si se asume normalidad, e independencia de los factores comunes, \\(\\{F_{1},F_{2},\\cdots, F_k\\}\\) sigue una distribución normal multivariante y \\(Z_{j},\\hspace{0,1cm} j=1,2,\\cdots,p,\\) una distribución normal.En términos de valores observados, el modelo AF (33.3) viene dado por:201\n\\[\\begin{equation}\nz_{ji}=a_{j1}f_{1i}+a_{j2}f_{2i}+ \\cdots +a_{jk}f_{ki} + d_ju_{ji}, \\quad =1,2,\\cdots,N; \\hspace{0.1cm} j=1,2,\\cdots,p,\n\\tag{33.4}\n\\end{equation}\\]\nEl modelo AF es muy parecido al de regresión lineal: una variable se describe como una combinación lineal de otro conjunto de variables más un residuo. Sin embargo, en el análisis de regresión las variables son observables, mientras que en el AF son construcciones hipotéticas que sólo pueden estimarse partir de los datos observados. Los propios factores se estiman en una etapa posterior del análisis.En términos matriciales, y considerando:\n\\[\\bf z=\\left(\\begin{matrix}Z{1}\\\\\nZ_{2}\\\\\n\\vdots\\\\\nZ_{p}\\\\\n\\end{matrix}\\right),\\quad \\bf f=\\left(\\begin{matrix}\nF_{1}\\\\\nF_{2}\\\\\n\\vdots\\\\\nF_{k}\\\\\n\\end{matrix}\\right),\\quad \\bf u=\\left(\\begin{matrix} U_{1}\\\\\nU_{2}\\\\\n\\vdots\\\\\nU_{p}\\\\\n\\end{matrix}\\right),\\]\n\\[\\bf =\\left(\\begin{matrix} a_{11} & a_{12} & \\cdots &a_{1k}\\\\\na_{21}&a_{22}&\\cdots&a_{2k}\\\\\n\\vdots&\\vdots&\\ddots     &\\vdots\\\\\na_{p1}&a_{p2}&\\cdots&a_{pk}\\\\\n\\end {matrix}\\right),\\quad \\bf D=\\left(\\begin{matrix} d_{1} & 0 & \\cdots &0\\\\\n0&d_{2}&\\cdots&0\\\\\n\\vdots&\\vdots&\\ddots     &\\vdots\\\\\n0&0&\\cdots&d_{p}\\\\\n\\end {matrix}\\right),\\]\nel modelo (33.3) puede expresarse como \\(\\bf z=\\bf \\bf f +\\bf D \\bf u\\).Centrándonos en el modelo (33.3), la varianza de \\(Z_j\\) viene dada por:\n\\[\\begin{equation}\nV(Z_j)=1= \\sum_{m=1}^{k} a_{jm}^2+\n2\\sum_{m< q }^{k} a_{jm} a_{jq} r_{(F_{mi},F_{qi})}  +d_j^2,\n\\tag{33.5}\n\\end{equation}\\]\ny si los factores comunes están incorrelados, \\(V(Z_j)=1= \\sum_{m=1}^{k} a_{jm}^2+ d_j^2\\).De la expresión (33.5) surgen las siguientes definiciones:\\(a_{jm}^2\\) es la contribución de \\(F_m\\) la varianza de \\(Z_j\\).\\(V_m=\\sum_{j=1}^{p}a_{jm}^2\\) es la contribución de \\(F_m\\) suma de las varianzas de todas las variables \\(Z_j,\\hspace{0,1cm} j=1,2,\\cdots,p\\).\\(V=\\sum_{m=1}^{k}V_m\\) es la contribución de todos los factores comunes la varianza de todas las variables \\(Z_j,\\hspace{0,1cm} j=1,2,\\cdots,p\\).\\(\\frac{V} {p}\\) es un indicador de la completitud del análisis factorial. \\(h_j^2=a_{j1}^2+a_{j2}^2+\\cdots+a_{jk}^2\\) es la comunalidad de \\(Z_j,\\hspace{0,1cm} j=1,2,\\cdots,p\\), es decir la contribución de los factores comunes la variabilidad de \\(Z_j\\).\\(d_j^2\\) es la unicidad (o varianza única) de \\(Z_j,\\hspace{0,1cm} j=1,2,\\cdots,p\\), o contribución de \\(U_j\\) la varianza de \\(Z_j\\). Es un indicador de la medida en la que los factores comunes fracasan la hora de representar la varianza (unitaria) de \\(Z_j\\). Cuando se descompone el factor único en sus dos componentes (modelo (33.2)), \\(b_j^2\\) se denomina especificidad (o varianza específica) de \\(Z_j\\) y es la varianza de \\(Z_j\\) debida la particular selección de las variables en el estudio, mientras que \\(c_j^2\\) es la que se debe al error (normalmente de medida), que mide la “falta de fiabilidad”.\n","code":""},{"path":"análisis-factorial.html","id":"patrón-y-estructura-factoriales","chapter":"Capítulo 33 Análisis factorial","heading":"33.2.2 Patrón y estructura factoriales","text":"Se denomina patrón factorial la siguiente expansión del modelo (33.3),\n\\[\\begin{equation}\n\\begin{split}\nZ_{1}= a_{11}F_{1}+ a_{12}F_{2}+ \\dotsb + a_{1k}F_{k}+ d_1U_1\\\\\nZ_{2}= a_{21}F_{1} + a_{22}F_{2}+ \\dotsb + a_{2k}F_{k}+d_2U_2 \\\\\n\\ \\ \\ \\ \\vdots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vdots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ddots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vdots \\ \\ \\ \\ \\ \\vdots \\ \\ \\ \\\\\nZ_{p}= a_{p1}F_{1}+ a_{p2}F_{2}+ \\dotsb + a_{pk}F_{k}+ d_pU_p\\\\\n\\end{split}\n\\tag{33.6}\n\\end{equation}\\]\no simplemente la tabla, o matriz, con los coeficientes \\(a_{jm}\\) y \\(d_j\\) (o únicamente, caso habitual, la matriz de cargas \\(\\bf \\)). \\(k\\) determina la “complejidad del modelo”.Se denomina estructura factorial al siguiente conglomerado de \\(k+1\\) conjuntos de \\(p\\) ecuaciones lineales, en \\(\\{a_{jm}\\}\\), los \\(k\\) primeros, y en \\(\\{d_{j}\\}\\), el último, \\(\\hspace{0,1cm} j=1,2,\\cdots, p; \\hspace{0,1cm} m=1,2,\\cdots, k\\):202\\[\\begin{equation}\n\\begin{split}\nr_{Z_jF_1} & = a_{j1}r_{F_1F_1}+ a_{j2}r_{F_1F_2}+ \\dotsb +a_{jm}r_{F_1F_m}+\\dotsb + a_{jk}r_{F_1F_k}\\\\\n\\ \\ \\ & \\vdots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vdots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vdots \\ \\ \\ \\ \\ \\ \\ \\ddots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vdots \\ \\ \\ \\ \\ \\ \\ \\ddots\\ \\ \\ \\ \\ \\ \\ \\ \\vdots \\ \\ \\ \\\\\nr_{Z_jF_m} & = a_{j1}r_{F_mF_1}+ a_{j2}r_{F_mF_2}+ \\dotsb +a_{jm}r_{F_mF_m}+\\dotsb + a_{jk}r_{F_mF_k}\\\\\n\\ \\ \\ &  \\vdots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vdots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vdots \\ \\ \\ \\ \\ \\ \\ \\ddots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vdots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ddots\\ \\ \\ \\ \\ \\ \\ \\vdots \\ \\ \\ \\\\\nr_{Z_jF_k} & = a_{j1}r_{F_kF_1}+ a_{j2}r_{F_kF_2}+ \\dotsb +a_{jm}r_{F_kF_m}+\\dotsb + a_{jk}r_{F_kF_k}\\\\ \\\\\n\\ \\ \\ &  \\vdots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vdots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vdots \\ \\ \\ \\ \\ \\ \\ \\ddots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vdots \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ddots \\ \\ \\ \\ \\ \\ \\ \\vdots \\ \\ \\ \\\\\nr_{Z_jU_j} & = d_j\\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ & \\vdots\n\\end{split}\n\\tag{33.7}\n\\end{equation}\\]\nEn la práctica, viene dada por una tabla, o matriz, \\(\\bf \\Gamma\\), con los coeficientes \\(\\{r_{jm}\\}\\). Cuando todos los factores están incorrelados el patrón y la estructura coinciden.El conjunto patrón factorial más estructura factorial se denomina solución factorial completa. El patrón factorial muestra la relación lineal de las variables en términos de los factores, como si de una regresión lineal se tratase, y puede usarse para reproducir la correlación entre las variables (y, por tanto, para determinar la bondad de la solución). La estructura factorial es útil para la identificación de los factores y la posterior estimación de las puntuaciones factoriales. En términos matriciales, denominando\n\\[\\bf F=\\left(\\begin{matrix} f_{11} & f_{12} & \\cdots &f_{1N}\\\\\nf_{21}&f_{22}&\\cdots&f_{2N}\\\\\n\\vdots&\\vdots&\\ddots     &\\vdots\\\\\nf_{k1}&f_{k2}&\\cdots&f_{kN}\\\\\n\\end {matrix}\\right),\\quad \\bf \\Gamma=\\left(\\begin{matrix} r_{Z_1F_1} & r_{Z_1F_2} & \\cdots &r_{Z_1F_k}\\\\\nr_{Z_2F_1}&r_{Z_2F_2}&\\cdots&r_{Z_2F_k}\\\\\n\\vdots&\\vdots&\\ddots     &\\vdots\\\\\nr_{Z_pF_1}&r_{X_p^*Z_pF_2}&\\cdots&r_{Z_pF_k}\\\\\n\\end {matrix}\\right),\\]\nel patrón factorial viene dado por \\(\\bf Z=\\bf \\bf F + \\bf D \\bf U\\). Multiplicando por \\(\\bf F^{\\prime}\\) y realizando simples operaciones se tiene que \\(\\bf \\Gamma = \\bf \\bf \\Phi\\), donde \\(\\bf \\Phi\\) es la matriz de correlaciones entre los factores comunes. Si los factores comunes están incorrelados, \\(\\bf \\Gamma=\\bf \\). Por último, resaltar que el AF es indeterminado, es decir, dado un conjunto de correlaciones, los coeficientes del patrón factorial son únicos (dado \\(\\bf R\\), se pueden encontrar infinitos sistemas de factores incorrelados u ortogonales)203 consistentes con ella. Por ello, normalmente, tras obtener una solución que ajuste bien los datos originales, se lleva cabo una rotación de la misma (que ajusta igual de bien dichos datos) que facilite la interpretación de los factores.204 ","code":""},{"path":"análisis-factorial.html","id":"el-análisis-factorial-en-la-práctica","chapter":"Capítulo 33 Análisis factorial","heading":"33.3 El análisis factorial en la práctica","text":"","code":""},{"path":"análisis-factorial.html","id":"pre-análisis-factorial","chapter":"Capítulo 33 Análisis factorial","heading":"33.3.1 Pre-análisis factorial","text":"","code":""},{"path":"análisis-factorial.html","id":"procede-la-realización-de-un-análisis-factorial","chapter":"Capítulo 33 Análisis factorial","heading":"33.3.1.1 ¿Procede la realización de un análisis factorial?","text":"Antes de comenzar con el AF, conviene determinar si procede o ; es decir, si las variables se encuentran fuertemente intercorrelacionadas o . En caso negativo, el AF tendría sentido. Para ello, se pueden utilizar procedimientos sencillos como observar si el determinante de \\(\\bf R\\) es bajo (correlaciones altas) o elevado (correlaciones bajas); o calcular la matriz de correlaciones anti-imagen, cuyos elementos son los coeficientes de correlación parcial cambiados de signo. En la diagonal muestra la medida de adecuación muestral para esa variable, \\(MSA_j\\). Para que se den las condiciones de realización del AF, la mayoría de los elementos diagonales deben ser pequeños y los diagonales deben estar próximos la unidad.\nOtras alternativas más sofisticadas incluyen las dos siguientes:Contraste de esfericidad de BartlettExige normalidad multivariante. Contrasta la incorrelación de las variables, es decir, \\(H_0:\\bf R=\\bf \\) frente \\(H_1:\\bf R\\neq \\bf \\) (o \\(H_{0}:|\\bf{\\bf R}|=1\\) frente \\(H_{1}:|\\bf{\\bf R}|=1\\)). El estadístico de contraste es \\(d_{\\bf R}= - \\left( N-1-\\frac{1}{6} (2p+5)\\right) ln|\\mathbf{R}|\\) y, bajo \\(H_0\\), sigue una \\(\\chi^2_{\\frac{p(p-1)}{2}}\\), siendo nulo en caso de incorrelación.Medida de adecuación muestral de Kaiser, Meyer y Olkin (KMO)\nSe basa en la idea de que, entre cada par de variables, el coeficiente de correlación parcial (que mide la correlación existente entre cada par de ellas eliminando el efecto que el resto de variables tiene sobre las dos consideradas), debe ser cercano cero, puesto que es una estimación de la correlación entre sus factores específicos, que se suponen incorrelados. Por tanto, si el número de coeficientes de correlación parcial nulos es elevado, la solución factorial es compatible con los datos.En otros términos, cuando las variables incluidas en el análisis comparten gran cantidad de información debido la presencia de factores comunes, la correlación parcial entre cualquier par de variables debe ser reducida. Por el contrario, cuando dos variables comparten gran cantidad de información entre ellas, pero la comparten con las restantes variables (ni, consecuentemente, con los factores comunes), la correlación parcial entre ellas será elevada, lo cual es un mal síntoma de cara la idoneidad del AF. El índice KMO se define como \\(KMO=\\frac{\\displaystyle\\sum_{j}\\displaystyle\\sum_{\\neq j} r_{ji}^2}{\\displaystyle\\sum_{j}\\displaystyle\\sum_{\\neq j} r_{ji}^2+\\displaystyle\\sum_{j}\\displaystyle\\sum_{\\neq j}r_{ji}^{*2}}\\), donde \\(r_{ji}^{*}\\) es el coeficiente de correlación parcial entre las variables \\(Z_j\\) y \\(Z_i\\). Se considera que valores por encima 0,9 implican elevadísismas correlaciones en \\(\\bf R\\); entre 0,5 y 0,9 permiten el AF; y por debajo de 0,5 resultan inaceptables para el AF.Las \\(MSA_j\\) mencionadas anteriormente, son la versión del índice KMO limitado cada variable: \\(MSA_{j}= \\frac{\\displaystyle\\sum_{\\neq j} r_{ji}^2}{\\displaystyle\\sum_{\\neq j} r_{ji}^2+\\displaystyle\\sum_{\\neq j} r_{ji}^{*2}}.\\)La interpretación es similar la de KMO, pero mide la adecuación de cada variable para realizar un AF, lo que permite considerar aquellas variables con menor MSA de cara mejorar la KMO. obstante, para eliminar una variable del estudio es aconsejable tener en cuenta también las comunalidades de cada variable, los residuos del modelo e interpretar los factores obtenidos.Como puede apreciarse, en nuestro ejemplo TIC, tanto el test de Barlett como el índice \\(KMO\\) y las \\(MSA_j\\) indican que el AF se puede llevar cabo con garantías.","code":"\nn <- dim(TIC2021)[1] \ncortest.bartlett(cor(TIC2021),n)$chisq\n#> [1] 149.7113\ncortest.bartlett(cor(TIC2021),n)$p.value\n#> [1] 1.992514e-21\nround(KMO(TIC2021)$MSA,3)\n#> [1] 0.83\nround(KMO(TIC2021)$MSAi,3)\n#>    ebroad    esales esocmedia      eweb    hbroad     hiacc      iuse \n#>     0.850     0.671     0.934     0.856     0.808     0.764     0.875"},{"path":"análisis-factorial.html","id":"el-problema-de-la-comunalidad-yo-del-número-de-factores-comunes","chapter":"Capítulo 33 Análisis factorial","heading":"33.3.1.2 El problema de la comunalidad y/o del número de factores comunes","text":"\nEl objetivo del AF es encontrar una matriz de correlaciones reproducida partir de los resultados obtenidos, \\({\\bf R}^{rep}\\), con menor rango que la original, \\(\\bf R\\), tal que su diferencia, la matriz de correlaciones de los residuos , \\({\\bf R}^{res}\\), se atribuya únicamente errores muestrales. \\(\\bf R\\) es una matriz gramiana: simétrica de números reales y de diagonal principal dominante, con lo cual es semidefinida positiva y sus autovalores son nulos o positivos. Por tanto, el número de factores comunes será igual al de autovalores positivos (\\(k\\leq p\\)). Si el punto de partida en el análisis es \\(\\bf R\\), rara vez se obtienen menos factores comunes que variables originales, con lo cual el AF realmente es un ACP. Ahora bien, como el número de factores comunes coincide con el rango de \\({\\bf R}^{rep}\\), y éste se ve afectado por los valores de la diagonal principal, al sustituir los unos por las estimaciones de las comunalidades (en este caso se está realizando un AF), \\({\\bf R}^{rep}\\) será, en general, gramiana y \\(k< p\\). En conclusión: como la solución factorial (\\(k< p\\)) pasa por el conocimiento del rango de \\(\\bf R\\) o de las comunalidades, o se hipotetiza sobre dicho rango o se hipotetizan o estiman las comunalidades. Normalmente se sigue uno de estos dos caminos:Se parte de un \\(k\\) prefijado, se lleva cabo el AF y se contrasta la hipótesis \\(H_0\\): número de factores comunes = \\(k\\).\nSe parte de un \\(k\\) prefijado, se lleva cabo el AF y se contrasta la hipótesis \\(H_0\\): número de factores comunes = \\(k\\).Se estiman las comunalidades y se obtienen los factores comunes .\nSe estiman las comunalidades y se obtienen los factores comunes .En cuanto prefijar un número \\(k\\) de factores, se pueden seguir los criterios expuestos en el Cap. 32 para determinar el número de componentes principales retener (criterio de Kaiser , gráfico de sedimentación, porcentaje mínimo de varianza explicada, …). En cuanto la estimación de las comunalidades, de las múltiples posibilidades existentes, las siguientes son interesantes por su sencillez y buenos resultados: Una de las más sencillas, si el número de variables es grande, es aproximar la comunalidad de una variable por su correlación más alta con las demás variables: \\(\\hat {h}_j^2=max(r_{j1},r_{j2},\\cdots,r_{j(j-1)},r_{j(j+1)},\\cdots, r_{jp})\\).Otra posibilidad es \\(\\hat{h}_j^2=\\frac{r_{js}r_{jt}}{r_{ts}}\\), donde \\(Z_{s}\\) y \\(Z_{t}\\) son, por este orden, las dos variables más correlacionadas con \\(Z_{j}\\). Este procedimiento modera el efecto que tendría en el anterior una correlación excepcionalmente elevada.En la misma línea, otra posibilidad es el promedio de los coeficientes de correlación entre la variable en cuestión y las restantes: \\(\\hat{h}_j^2=\\frac{\\sum_{j \\neq s}r_{js}}{p-1}\\).Otra alternativa es realizar un ACP y tomar como comunalidad de cada variable la varianza explicada por los factores retenidos con el criterio de autovalor mayor que la unidad.También se puede utilizar el coeficiente de determinación lineal múltiple de cada variable con las demás como estimación de la cota inferior de sus comunalidades: \\(\\hat {h}_j^2 \\geq r^2_{{Z_j};(Z_{j1},\\cdots,Z_{2},\\cdots Z_{j-1},Z_{j+1},\\cdots, Z_{p})}=1-\\frac{1}{r^{jj}}\\), donde \\(r^{jj}\\) es el \\(j\\)-ésimo elemento de la diagonal de \\({\\bf R}^{-1}\\).Un valor alto de la comunalidad, próximo \\(V(X_j)\\), significa que dicha variable está bien representada en el espacio de factores.","code":""},{"path":"análisis-factorial.html","id":"análisis-factorial-1","chapter":"Capítulo 33 Análisis factorial","heading":"33.3.2 Análisis factorial","text":"","code":""},{"path":"análisis-factorial.html","id":"metodosdeextraccion","chapter":"Capítulo 33 Análisis factorial","heading":"33.3.2.1 Métodos de extracción de los factores","text":"Método de componentes principales Su objetivo es el análisis de toda la varianza, común y común, (modelo (33.1)). Por consiguiente, las entradas de la diagonal de \\(\\bf R\\)205 son unitarias y se requiere la estimación priori de las comunalidades ; tampoco se requiere la estimación priori del numero de factores comunes, que se determinan posteriori. Para la exposición del método, así como para su ejemplificación con la base de datos TIC2021 del paquete CDR, se remite al lector al Cap. 32. Aunque en el Cap. 32 se utilizó la función PCA de la librería FactoMineR, también se puede utilizar la función principal de la librería psych. Este método tiene la ventaja de que siempre proporciona una solución. Sin embargo, al estar basado en el modelo (33.3), puede dar estimaciones de las cargas factoriales muy sesgadas, sobre todo cuando hay variables con comunalidades pequeñas.Método de los factores principalesEs la aplicación del método de componentes principales la matriz de correlaciones reducida, \\({\\bf R}^*\\), es decir, con comunalidades en la diagonal en vez de unos. Exige, por tanto, la estimación previa de las comunalidades y su objetivo es el análisis de la varianza compartida por todas las variables (modelo (33.3)). Se trata de un procedimiento iterativo que consta de las siguientes etapas:1.- Cálculo de la matriz de correlaciones muestrales.2.- Estimación inicial de las comunalidades utilizando el coeficiente de determinación lineal múltiple de cada variable con las demás.2063.- Cálculo de la matriz de correlaciones reducida:\\[\\mathbf{R}^*\n=\\begin{pmatrix}\n\\hat{h}_{1(0)}^2 & r_{12} & \\dotsb & r_{1p}\\\\\nr_{21} & \\hat{h}_{2(0)}^2 & \\dotsb & r_{2p}\\\\\n\\vdots & \\vdots & \\dotsb & \\vdots \\\\\nr_{p1} & r_{p2} & \\dotsb & \\hat{h}_{p(0)}^2\\\\\n\\end{pmatrix}.\\]4.- Cálculo de los autovalores y autovectores asociados \\(\\mathbf{R}^*\\) (matriz necesariamente semidefinida positiva) y, partir de ellos, obtención de las estimaciones de la matriz de cargas factoriales \\(\\bf{}_{(0)}\\). En este paso hay que determinar el número de factores utilizando los criterios del ACP.5.- partir de la estimación de \\(\\bf{}_{(0)}\\), obtención de una nueva estimación de las comunalidades: \\(\\hat{h}_{j(1)}^2= \\hat{}_{j1(1)}^2+\\hat{}_{j2(1)}^2+ \\dotsb +\\hat{}_{jk(1)}^2\\) y, por tanto, de una nueva estimación de la varianza única (o unicidad) \\(\\hat{d}_{j(1)}^2 =1 - \\hat{h}_{j(1)}^2\\).\n6.- Comparación de \\(\\hat{h}_{j(1)}^2\\) con \\(\\hat{h}_{j(0)}^2\\), \\(j=1,2,\\cdots,p\\). Si hay diferencia significativa se vuelve al paso 3, y si la discrepancia supera una cantidad prefijada se aceptan como válidas las últimas estimaciones disponibles.En el software R, el método de los factores principales se implementa con la función fa de la librería psych, que parte de \\({\\bf R}^*\\) (véase Harman (1976) para el procedimiento iterativo y Revelle (2022) para los detalles sobre la manera cómo fa parte de \\({\\bf R}^*\\) y lleva cabo la extracción de los factores).En la salida anterior, \\(SS\\) \\(loadings\\) son los autovalores de \\({\\bf R}^*\\), que coinciden con la suma de los cuadrados de las cargas de las variables en cada factor (suma de las cargas al cuadrado por columnas). \\(h2\\) son las comunalidades (suma de las cargas al cuadrado por filas; sólo se muestran las cargas para los dos primeros factores puesto que entre ambos ya acumulan una varianza explicada de más del 75%). \\(u2\\) son las varianzas de los factores únicos; finalmente, \\(com_j\\) (en la salida \\(com\\)) es el número de factores comunes necesarios para explicar la variable \\(Z_j\\): \\(com_j=\\frac{\\left( \\sum_{j=1}^{p} a_{jm}^{2} \\right)^2}{\\sum_{j=1}^{p} a_{jm}^4}\\). Cuanto mayor es \\(com_j\\) mejor es la calidad de la variable para participar en la extracción factorial. El promedio de los \\(com_j\\) se denomina índice de complejidad de Hoffmann. Una solución de estructura simple perfecta tiene una complejidad de uno (cada variable carga solo en un factor); una solución con elementos distribuidos uniformemente tiene una complejidad mayor que 1. Interesa que la estructura sea simple y perfecta porque entonces tendría sentido la reducción dimensional. Por tanto, el índice de Hofmann deberá ser superior la unidad. Las comunalidades y unicidades son:\nNótese que con el método de los factores principales, al aplicar ACP sobre \\({\\bf R}^*\\), los factores obtenidos están incorrelados y la estructura coincide con el patrón.\nLos resultados son, en signo, aunque tanto en valor, similares los obtenidos por el método de componentes principales. Además, como era de esperar, permiten una interpretación clara de los factores comunes. Para facilitar dicha interpretación, dichos factores deberán ser rotados (véase Sec. 33.3.2.2). Método de máxima verosimilitud Exige normalidad multivariante y la determinación priori del número de factores comunes, pero la estimación de las comunalidades. Obedece al modelo (33.3) y consiste en obtener las estimaciones máximo verosímiles de \\(\\bf \\) y \\(\\bf D\\). Dado que cualquier transformación ortogonal de \\(\\bf{}\\) puede ser una solución, se impone la condición de que \\(\\bf ^{\\prime}(DD^{\\prime})^{-1}\\bf \\) sea diagonal. La log-verosimilitud viene dada por \\(l=-\\frac{N}{2}\\left( log|2\\pi{\\bf {\\Sigma}}|+ tr{\\bf {\\Sigma}}^{-1}\\bf S \\right)\\), donde \\(\\bf\\Sigma=\\bf \\bf ^{\\prime}+\\bf D \\bf D^{\\prime}\\) y \\(\\bf S\\) son las matrices de covarianzas poblacional y muestral, respectivamente, de las variables \\(X_j\\), \\(j= 1,2,\\cdots,p\\). \\(\\hspace{0,1cm} \\bf D \\bf D^{\\prime}\\) es la matriz de covarianzas (diagonal) de los factores únicos, donde los elementos de la diagonal representan la parte de la varianza única de cada variable, y en la literatura sobre AF es conocida como \\(\\bf \\Psi\\) (por ello, \\(d_j^2=\\psi_{jj})\\).La decisión sobre el número de factores comunes , \\(k\\), que en este método debe hacerse al principio, es muy importante, pues dos soluciones, una con \\(k\\) factores y otra con \\(k+1\\), pueden tener matrices de cargas factoriales muy diferentes, al contrario que en el método de componentes principales, donde los primeros \\(k\\) componentes son siempre iguales. Pues bien, una ventaja del método de máxima verosimilitud es que lleva asociado un test estadístico secuencial para determinar el número de factores (véase Sec. 33.3.3). Otra ventaja es que las estimaciones máximo verosímiles son invariantes ante cambios de escala; en consecuencia, las matrices de covarianzas teórica y muestral de la log-verosimilitud pueden ser sustituidas por sus homónimas de correlación sin variación alguna en los resultados. Una desventaja es que puede haber un problema de grados de libertad; o, en otros términos, el número de factores \\(k\\) debe ser compatible con un número de grados de libertad positivo.El método máximo verosímil se puede implementar en R con la librería pysch y la función fa (con fm=ml). Otra posibilidad es utilizar la función factanal. En ambos casos hay comprobar el cumplimiento de la hipótesis de normalidad. En el ejemplo TIC procede su implementación al cumplirse tal hipótesis.Otros métodosRazones de espacio impiden comentar otros procedimientos de extracción de los factores. obstante, hay que señalar que la función fa de la librería psych también permite implementar los métodos \\(()\\) minres (mínimo residuo), que estima las cargas factoriales minimizando (sin ponderaciones) los cuadrados de los residuos diagonales de \\({\\bf R}^{res}\\); parte de una estimación de \\(k\\) y, como el método máximo verosímil, precisa estimar las comunalidades, que se obtienen como subproducto tras la estimación de las cargas; y \\((ii)\\) alpha, que maximiza el alpha de Cronbach para los factores. Aunque fa también proporciona el método del centroide o la descomposición triangular (que exigen la estimación de las comunalidades, o el análisis imagen (que requiere el número de factores), en la actualidad están en desuso. Otros métodos de extracción de los factores son los métodos de mínimos cuadrados ponderados y mínimos cuadrados generalizados, que minimizan la suma de las diferencias cuadráticas entre las matrices de correlación observada y reproducida, en el último caso ponderando los coeficientes de correlación inversamente la unicidad de las variables (alta unicidad supone baja comunalidad). Ambos son proporcionados por fa y FAiR, que también es una librería muy recomendable.","code":"\naf_facprin <- fa(cor(TIC2021), nfactors=2, rotate=\"none\", fm='pa')\nprint(af_facprin, digits=3)\n#> Factor Analysis using method = pa\n#> Call: fa(r = cor(TIC2021), nfactors = 2, rotate = \"none\", fm = \"pa\")\n#> Standardized loadings (pattern matrix) based upon correlation matrix\n#>              PA1     PA2    h2    u2   com\n#> ebroad     0.678   0.189 0.495 0.5050 1.16\n#> esales     0.503   0.547 0.553 0.4474 1.99\n#> esocmedia  0.796   0.212 0.678 0.3218 1.14\n#> eweb       0.872   0.239 0.818 0.1822 1.15\n#> hbroad     0.816  -0.452 0.869 0.1306 1.56\n#> hiacc      0.888  -0.439 0.982 0.0181 1.46\n#> iuse       0.935  -0.023 0.875 0.1248 1.00\n#>\n#>                  PA1   PA2\n#> SS loadings    4.435 0.835\n#> Proportion Var 0.634 0.119\n#> Cumulative Var 0.634 0.753\nround(af_facprin2$communality,3)\n#> ebroad esales esocmedia    eweb hbroad hiacc iuse \n#>  0.495 0.553      0.678  0.818   0.869 0.982 0.875 \nround(af_facprin2$uniquenesses,3)\n#> ebroad esales esocmedia    eweb hbroad hiacc iuse \n#>  0.505 0.447      0.322   0.182 0.131  0.018 0.125 "},{"path":"análisis-factorial.html","id":"rotaciones","chapter":"Capítulo 33 Análisis factorial","heading":"33.3.2.2 Rotaciones en el modelo de análisis factorial","text":"La interpretación de los factores se lleva cabo través de la estructura factorial , que, si los factores comunes están incorrelados, coincide con el patrón factorial. Sin embargo, aunque el modelo obtenido sea representativo de la realidad, en ocasiones la interpretación de los factores es harto dificultosa, porque presentan correlaciones similares con un gran número de variables. Como la solución AF es única (si \\(\\bf{}\\) es una solución factorial, también lo es cualquier transformación ortogonal), con la rotación se trata de que cada variable tenga una correlación próxima 1 con un factor y 0 con el resto, facilitando la interpretación de los factores.Geométricamente, la \\(j\\)-ésima fila de la matriz de cargas contiene las coordenadas de un punto (elemento, observación) en el espacio de las cargas correspondientes \\(X_j\\). Al realizar la rotación, se obtienen las coordenadas respecto unos nuevos ejes, siendo el objetivo situarlos lo más cerca posible del mayor número de puntos. Esto asociaría cada grupo de variables con un sólo factor, haciendo la interpretación más objetiva y sencilla.Sea \\(\\bf T\\) una matriz ortogonal (\\(\\bf T^{\\prime} \\bf T=\\bf T\\bf T^{\\prime}=\\bf \\)), denominada matriz de transformación. Entonces, el modelo (33.3) puede escribirse como \\(\\bf Z=\\bf \\bf T\\bf T^{\\prime}\\bf F+ \\bf U=\\bf B \\bf T^{\\prime}\\bf F+ \\bf U\\). Se trata de llegar una estructura simple, que se caracteriza porque en \\(\\bf B\\):Cada fila tiene al menos un cero.Cada columna tiene, al menos, tantos ceros como factores comunes (\\(k\\)).Cada par de columnas debe ser tal que, para varias variables, una tenga cargas despreciables y la otra .Si \\(k\\geq 4\\), cada par de columnas debe tener un número elevado de variables cuyas cargas sean nulas en ambas variables.Para cada par de columnas, el número de variables con cargas nulas en ambas columnas debe ser muy pequeño.Como se avanzó, se trata de que las variables se aglomeren lo más posible en\ntorno los factores comunes, y de la manera más discriminatoria posible. Así mejora la interpretación de éstos y, por lo general, aumenta su significado teórico. Las rotaciones pueden ser ortogonales u oblicuas, dependiendo de si los nuevos factores siguen estando incorrelacionados (ejes perpendiculares) o (ejes oblicuos).","code":""},{"path":"análisis-factorial.html","id":"rotaciones-ortogonales","chapter":"Capítulo 33 Análisis factorial","heading":"33.3.2.2.1 Rotaciones ortogonales","text":"Preservan la perpendicularidad de los ejes y varían las comunalidades , pues \\({\\bf{B}}{\\bf{B}}^{\\prime}= {\\bf{}} \\bf{T} \\bf{T}^{\\prime} {\\bf{}}^{\\prime}= \\bf{} \\bf{}^{\\prime}\\). Tampoco modifican los cuadrados de las comunalidades ni, por tanto, la suma de sus cuadrados (para todas las variables): \\(\\sum_{j=1}^p\\sum_{m=1}^k b_{jm}^4+2\\sum_{m<r=1}^k\\sum_{m=1}^k b_{jm}^2b_{jr}^2\\). Y como esta expresión se mantiene invariante, minimizar el segundo término implica maximizar el primero.Las rotaciones ortogonales más usadas son:Rotación VARIMAXSe define simplicidad del factor \\(m\\)-ésimo como la varianza de los cuadrados de las cargas factoriales (rotadas) \\(b_{ji}\\), \\(j=1,2,\\cdots,p\\):\n\\({SMPL}_{m} = \\frac{\\sum_{j=1}^{p} {b}_{jm}^4}{p}- \\left(\\frac {\\sum_{j=1}^{p}b_{jm}^2}{p}\\right)^2\\). Cuanto mayor es la simplicidad de los factores, más sencilla es su interpretación. Por ello, el objetivo es que \\(\\bf{T}\\) sea tal que se maximice la varianza del cuadrado de las cargas en cada columna del patrón factorial, es decir, en cada factor.Dicho lo anterior, la rotación VARIMAX consiste en la obtención de una \\(\\bf{T}\\) que maximice la suma de las simplicidades de todos los factores, \\(V=\\sum_{m=1}^{k}{SMPL}_{m}\\).207Sin embargo, las variables con mayor comunalidad, y por tanto con mayores cargas factoriales, tendrán mayor influencia en la solución final, porque la comunalidad se ve afectada por la rotación ortogonal. Para evitar esto, Kaiser propuso la rotación VARIMAX normalizada208, donde las cargas se dividen entre la raíz cuadrada de la comunalidad de la variable correspondiente. Los valores obtenidos son los elementos de \\(\\bf B\\).El procedimiento de cálculo de las cargas de los factores rotados es iterativo, rotándose los factores por parejas hasta que se consigue maximizar la suma de simplicidades normalizadas.La rotación VARIMAX es muy popular por la robustez de sus resultados, si bien se recomienda para un número muy elevado de factores comunes .Rotación QUARTIMAXSu objetivo es maximizar la varianza de los cuadrados de todas las cargas factoriales, es decir, maximizar \\(Q=\\frac{\\sum_{j=1}^{p}\\sum_{m=1}^{k}{b}_{jm}^4}{pk}-\\left( \\frac {\\sum_{j=1}^{p}\\sum_{m=1}^{k}{b}_{jm}^2}{pk} \\right)^2\\). Nótese que, como la rotación ortogonal modifica las comunalidades , \\({h}_{j}^2=\\sum_{m=1}^{k} b_{jm}^2\\), el segundo término de la expresión anterior se verá modificado, por lo que el criterio anterior equivale maximizar \\(\\frac{\\sum_{j=1}^{p}\\sum_{m=1}^{k}{b}_{jm}^4}{pk}\\).QUARTIMAX es recomendable cuando el número de factores es elevado. Tiende generar un factor general, el primero, sobre el que la mayor parte de las variables tienen cargas elevadas, lo cual contradice los objetivos que persigue la rotación.Rotación ORTOMAXEs una clase general de los métodos de rotación ortogonal que se construye como una composición ponderada de las dos rotaciones anteriores: \\(\\alpha Q+ \\beta V\\), donde \\(V\\) se multiplica por \\(p\\) por conveniencia, ya que una constante multiplicativa afecta la solución. Por tanto, su objetivo es maximizar la expresión: \\(ORT=\\sum_{m=1}^{k} \\left({\\sum_{j=1}^{p} {b}_{ji}^4 - \\left( \\frac{\\theta}{p}\\sum_{j=1}^{p} b_{ji}^2 \\right)^2}\\right),\\hspace{0,1cm} 0< \\theta=\\frac{\\alpha}{\\alpha+\\beta}< 1\\).Si \\(\\theta=1\\), se tiene el criterio VARIMAX; si \\(\\theta=0\\), se tiene el criterio QUARTIMAX; si \\(\\theta=0,5\\), se tiene un criterio igualmente ponderado denominado BIQUARTIMAX; y si \\(\\theta=\\frac{k}{2}\\), se tiene el criterio EQUAMAX, recomendado por parte de la literatura. Nótese que QUARTIMAX pone el énfasis en la simplificación de la descripción por filas (variables) de la matriz factorial, mientras que VARIMAX lo pone en la simplificación por columnas (factores), para satisfacer los requisitos de estructura simple; así, aunque se pueda conseguir la simplicidad de cada variable y que, la vez, las cargas respecto del mismo factor sean grandes, tal factor queda excluido por la restricción impuesta por la simplificación sobre cada factor (Harman 1976).","code":""},{"path":"análisis-factorial.html","id":"rotaciones-oblicuas","chapter":"Capítulo 33 Análisis factorial","heading":"33.3.2.2.2 Rotaciones oblicuas","text":"Superan la incorrelación u ortogonalidad de los factores y se suelen aplicar cuando: \\(()\\) se sospecha que, en la población, los factores tienen una fuerte correlación y/o \\((ii)\\) cierta correlación entre los factores permite una gran ganancia en la interpretación de los mismos. Podrían aplicarse siempre, como norma general, puesto que, en realidad, la ortogonalidad es un caso particular de la oblicuidad.Los procedimientos que proporcionan soluciones con estructura simple oblicua emanan de los mismos criterios objetivos que los que proporcionan soluciones con estructura simple ortogonal. De hecho, si se relajan las condiciones de ortogonalidad, algunos procedimientos de rotación ortogonal pueden adaptarse al caso oblicuo (tal es el caso, por ejemplo, del método OBLIMAX, partir del criterio QUARTIMAX). Por otra parte, los métodos de rotación oblicua solo son directos, sino que también pueden introducir los principios de estructura simple que se requieren para la solución factorial primaria de forma indirecta (métodos indirectos). Las rotaciones oblicuas exigen nuevos conceptos y nueva nomenclatura: Factores de referencia, \\({G}_m\\), \\(m=1, 2,\\cdots, k\\): para cada factor rotado se puede encontrar un nuevo factor incorrelado con los rotados. esos nuevos factores de les llama factores de referencia. En caso de rotación ortogonal, los factores de referencia coinciden con los primeros. Factores de referencia, \\({G}_m\\), \\(m=1, 2,\\cdots, k\\): para cada factor rotado se puede encontrar un nuevo factor incorrelado con los rotados. esos nuevos factores de les llama factores de referencia. En caso de rotación ortogonal, los factores de referencia coinciden con los primeros. Estructura factorial de referencia: hasta ahora, se denominaba estructura factorial la matriz de correlaciones entre las variables \\(Z_j\\), \\({j=1,2, \\cdots, p}\\) y los factores rotados, que en el caso ortogonal coincide con la matriz de cargas factoriales rotadas. Pues bien, se denomina estructura factorial de referencia la matriz de correlaciones entre las variables \\(Z_j\\) y los factores de referencia. Si la rotación es ortogonal, coincide con la estructura factorial. Estructura factorial de referencia: hasta ahora, se denominaba estructura factorial la matriz de correlaciones entre las variables \\(Z_j\\), \\({j=1,2, \\cdots, p}\\) y los factores rotados, que en el caso ortogonal coincide con la matriz de cargas factoriales rotadas. Pues bien, se denomina estructura factorial de referencia la matriz de correlaciones entre las variables \\(Z_j\\) y los factores de referencia. Si la rotación es ortogonal, coincide con la estructura factorial. Matriz de transformación: en el caso oblicuo se representa por \\(\\bf\\Lambda\\).Matriz de transformación: en el caso oblicuo se representa por \\(\\bf\\Lambda\\).Estructura factorial oblicua: \\(\\bf V\\), tal que \\(\\bf V= \\bf \\bf \\Lambda\\); sus elementos son \\(v_{jm}\\). Estructura factorial oblicua: \\(\\bf V\\), tal que \\(\\bf V= \\bf \\bf \\Lambda\\); sus elementos son \\(v_{jm}\\). Cargas: en el caso oblicuo el término “carga” se utiliza para denotar la correlación de la variable con el eje de referencia: \\(v_{jm}=r_{Z_j;\\Lambda_m}\\).Cargas: en el caso oblicuo el término “carga” se utiliza para denotar la correlación de la variable con el eje de referencia: \\(v_{jm}=r_{Z_j;\\Lambda_m}\\).Mientras las rotaciones ortogonales intentan encontrar la estructura factorial más simple, las oblicuas hacen lo mismo pero con la estructura de referencia.El método (directo) OBLIMAX maximiza la expresión \\(K=\\frac{\\sum_{j=1}^{p}\\sum_{m=1}^{k}v_{jm}^4}{\\left(\\sum_{j=1}^{p}\\sum_{m=1}^{k}v_{jm}^{2}\\right)^2}\\). Nótese que se trata del criterio QUARTIMAX ortogonal, pero incorporando el denominador, puesto que en la rotación oblicua ya es constante. El QUARTIMIN directo, también derivado del QUARTIMAX ortogonal, minimiza el criterio \\(N=\\sum_{j=1}^{p}\\sum_{m\\leq q=1}^{k}v_{jm}^2v_{jq}^2\\), y recibe este nombre por minimizar términos de cuarto grado. La generalización del criterio “minimizar \\(H=\\sum_{j=1}^{p}\\sum _{m<q=1}^k b_{jm}^2b_{jq}^2\\)” para factores oblicuos se denomina OBLIMIN , y da lugar métodos indirectos. Entre ellos, destaca el COVARIMIN, que se obtiene relajando la condición de ortogonalidad en el VARIMAX , minimizando las covarianzas de los cuadrados de los elementos de \\(\\bf V\\): \\(C^*=\\sum_{m\\leq q=1}^{k}\\left(p\\sum_{j=1}^{p} v_{jm}^2v_{jq}^2-\\sum_{j=1}^ {p}v_{jm}^2\\sum_{j=1}^ {p}v_{jq}^2\\right)\\). La versión COVARIMIN normalizada minimiza \\(C=\\sum_{m\\leq q=1}^{k}\\left(p\\sum_{j=1}^{p} \\frac{v_{jm}^2}{h_j^2}\\frac{v_{jq}^2}{h_j^2}-\\sum_{j=1}^ {p}\\frac {v_{jm}^2}{h_j^2}\\sum_{j=1}^ {p}\\frac{v_{jq}^2}{h_j^2}\\right)\\).Se ha comprobado empíricamente que QUARTIMIN tiende ser demasiado oblicuo y COVARIMIN demasiado ortogonal. Una solución intermedia es la rotación BIQUARTIMIN, que consiste en minimizar \\(B^*=H+\\frac{C^*}{p}\\), donde \\(\\frac{C^*}{p}\\) es la expresión completa del COVARIMIN. Una generalización de la rotación BIQUARTIMIN es \\(B^*=\\alpha H+\\beta \\frac{C^*}{p}\\). Sencillas operaciones aritméticas llevan \\(B^*=\\sum_{m< q=1}^{k}\\left(p \\sum_{j=1}^{p} v_{jm}^2 v_{jq}^2-\\gamma \\sum _{j=1}^{p}v_{jm}^2 \\sum_{j=1}^{p}v_{jq}^2\\right)\\), con \\(\\gamma= \\frac {\\beta}{\\alpha + \\beta}\\). La rotación QUARTIMIN se obtiene con \\(\\gamma=0\\), la BIQUARTIMIN con \\(\\gamma=0,5\\) y la COVARIMIN con \\(\\gamma=1\\). También se pueden obtener versiones normalizadas sin más que normalizar las cargas (dividirlas por \\(h_{jm}^2\\)). El criterio BINORMALMIN (normalizado) es una alternativa al BIQUARTIMIN para corregir el sesgo de oblicuidad del criterio COVARIMIN. Minimiza \\(D=\\sum_{m< q=1}^{k}\\left( \\frac{\\sum_{j=1}^{p} \\frac {v_{jm}^2}{h_j^2} \\frac {v_{jq}^2}{h_j^2}} {\\sum _{j=1}^{p} \\frac{v_{jm}^2}{h_j^2}\\sum _{j=1}^{p} \\frac{v_{jq}^2}{h_j^2}}\\right)\\). BINORMALMIN suele ser mejor con datos muy simples o muy complejos; BIQUARTIMIN es más recomendable con datos moderadamente complejos.El método de rotación OBLIMIN directo, en vez de proceder como \\(B^*\\), que depende de los valores de la estructura, minimiza directamente una función de la matriz del patrón factorial primario: \\(F{(\\bf{})}= \\sum_{m< q=1}^{k}\\left(\\sum_{j=1}^p a_{jm}^2 a_{jq}^2-\\frac{\\delta}{p}\\sum_{j=1}^pa_{jm}^2\\sum_{j=1}^pa_{jq}^2\\right)\\). Cuando \\(\\delta=0\\), se tiene el QUARTIMIN directo.Hay otros tipos de transformaciones oblicuas, pero únicamente se mencionarán \\(()\\) la ORTOBLICUA, que llega la solución oblicua mediante una serie de transformaciones ortogonales intermedias; y \\((ii)\\) el la PROMAX, muy popular, que actúa alterando los resultados de una rotación ortogonal (concretamente elevando las cargas de la rotación ortogonal una potencia entre 2 y 4) hasta crear una solución con cargas factoriales lo más próximas la estructura ideal. Cuanto mayor es esta potencia más oblicua es la solución obtenida.","code":""},{"path":"análisis-factorial.html","id":"rotaciones-ortogonales-u-oblicuas","chapter":"Capítulo 33 Análisis factorial","heading":"33.3.2.2.3 ¿Rotaciones ortogonales u oblicuas?","text":"\nLa selección del método de rotación, ortogonal u oblicua, depende del objetivo perseguido. Si se pretende reducir el número de variables originales un conjunto mucho menor de variables incorrelacionadas para su uso posterior en otra técnica, por ejemplo regresión, la rotación debe ser ortogonal. Si el objetivo es obtener unos factores teóricos significativos, puede resultar apropiada la aplicación de una rotación oblicua.En R es muy sencillo implementar una rotación ortogonal u oblicua. Basta, por ejemplo, con utilizar la librería GPArotation (Bernaards Jennrich 2005) e indicarlo en el argumento rotate de la función fa. modo de ejemplo, extrayendo los factores por el método de los factores principales y utilizando una rotación VARIMAX normalizada, sería:Nótese que la salida por defecto es la normalizada. También se puede utilizar la libería stats indicando T o F en el argumento normalize, dependiendo de que se quiera o , respectivamente, una rotación VARIMAX (u otra) normalizada .En el ejemplo del uso las TIC en los países de la UE-27, la rotación VARIMAX ha conseguido facilitar la interpretación de los factores comunes , ya que, tras la rotación, las variables relacionadas con el uso de las TIC escala individual y de hogar cargan en el primer factor, mientras que las relacionadas con el uso de las TIC nivel empresarial cargan en el segundo. Por tanto, ambos factores pueden considerarse indicadores de la dotación y uso de las TIC en los ámbitos familiar y empresarial, respectivamente. El lector puede probar (y comparar) con otras rotaciones sin más que incluirlas en el argumento rotate.","code":"\nlibrary(\"GPArotation\")\naf_facprin2 <- fa(cor(TIC2021), nfactors=2, rotate=\"varimax\", fm=\"pa\", digits=3)\naf_facprin2 # el objeto contiene información adicional no relevante en estos momentos\n#> Factor Analysis using method = pa\n#> Standardized loadings (pattern matrix) based upon correlation matrix \n#>              PA1 PA2   h2    u2 com\n#> ebroad     0.38 0.59 0.50 0.505 1.7\n#> esales     0.02 0.74 0.55 0.447 1.0\n#> esocmedia  0.46 0.68 0.68 0.322 1.7\n#> eweb       0.50 0.75 0.82 0.182 1.7\n#> hbroad     0.91 0.20 0.87 0.131 1.1\n#> hiacc      0.96 0.26 0.98 0.018 1.1\n#> iuse       0.72 0.60 0.88 0.125 1.9\n#>\n#>                 PA1  PA2\n#> SS loadings    2.87 2.40\n#> Proportion Var 0.41 0.34\n#> Cumulative Var 0.41 0.75\nlibrary(stats)\nvarimax(loadings(af_facprin),normalize=T)"},{"path":"análisis-factorial.html","id":"postanalisis","chapter":"Capítulo 33 Análisis factorial","heading":"33.3.3 Post-análisis factorial","text":"Realizado el AF, los siguientes procedimientos permiten comprobar la bondad del modelo obtenido:Análisis de las correlaciones residualesSe entiende por bondad de la solución factorial la medida del grado en que los factores del modelo explican las correlaciones entre las variables. Por ello, parece natural que tal medida se base en la comparación entre las correlaciones observadas y las que se derivan del modelo factorial (reproducidas) o, en términos matriciales, en la magnitud de las entradas de la matriz de correlaciones residuales \\({\\bf R}^{res}={\\bf R} - {\\bf R}^{rep}\\), donde \\({\\bf R}=\\frac{1}{N} \\bf Z \\bf Z^{\\prime}\\) y \\({\\bf R}^{rep}=\\bf \\bf \\Phi\\bf ^{\\prime}=\\bf \\Gamma \\bf ^{\\prime}\\) (relación fundamental entre el patrón y la estructura factorial; en caso de incorrelación entre los factores, \\(\\bf \\Phi=\\bf \\) y \\({\\bf R}^{rep}=\\bf {\\bf }^{\\prime}\\). La matriz \\({\\bf R}^{rep}\\) se obtiene sin más que sustituir \\(\\bf Z\\) por \\(\\bf \\bf F\\) en la expresión de \\(\\bf R\\). Ahora bien, ¿cuál es el criterio apropiado para concluir si una solución factorial es aceptable o ? Para que sea aceptable, los elementos (los residuos) de \\({\\bf R}^{res}\\) deben ser cercanos cero, y como todos los factores comunes han sido considerados, se supone que existen más vínculos entre las variables y que la distribución de dichos residuos debe ser como la de correlación cero en una muestra del mismo tamaño. Por tanto, como \\(\\sigma_{r=0}=\\frac{1}{\\sqrt{N-1}}\\), entonces \\(S_{r_{res}}\\leq\\frac{1}{\\sqrt{N-1}}:\\)209Si \\(S_{r_{res}}\\gg\\frac{1}{\\sqrt{N-1}}\\), es razonable pensar que existen relaciones adicionales significativas entre las variables y hay que modificar la solución factorial. Si \\(S_{r_{res}}\\ll\\frac{1}{\\sqrt{N-1}}\\), es razonable pensar que la solución factorial incluye relaciones que están justificadas.Si \\(S_{r_{res}}\\leq pero \\hspace{0,07cm} \\ll \\frac{1}{\\sqrt{N-1}}\\), la solución es aceptable.Otra posibilidad, también muy sencilla, propuesta por Revelle (2022), es utilizar \\(fit= 1-\\frac{\\sum \\left (\\bf R-{\\bf FF}^{\\prime}\\right)^2}{\\sum (\\bf R)^2}\\), que indica la reducción proporcional en la matriz de correlación debida al modelo factorial. Nótese que esta medida es sensible al tamaño de las correlaciones originales. Es decir, si los residuos son pequeños, pero las correlaciones son pequeñas, el ajuste es malo. Las medidas clásicas como el RMSE (raíz cuadrada del error cuadrático medio), o similares, también son susceptibles de uso.En el ejemplo TIC seguido en este capítulo el ajuste realizado es muy bueno:NOTA IMPORTANTEComo se avanzó en la introducción, el AF está enfocado al ajuste de las correlaciones entre las variables observadas mediante el patrón factorial correspondiente al modelo (33.2) (con los factores comunes y el factor único). Pues bien, si en el proceso reproductivo se utiliza el modelo sólo con los factores comunes, la matriz de correlaciones que se reproduce es \\(\\bf R\\), lo que implica el modelo ACP (modelo (33.1)). Si se incluye también el factor específico, la matriz de correlaciones que se reproduce es \\({\\bf R}^*\\) (modelo AF). Si en dicha reproducción se utilizasen los factores comunes y el término de error, se reproduciría \\(\\bf R\\) con una diagonal principal cuyas entradas serían la unidad menos las estimaciones de las comunalidades. Test de bondad de ajusteSe trata de un contraste de razón de verosimilitudes que se puede llevar cabo cuando se extraigan los factores por el método de máxima verosimilitud. La hipótesis nula es la suficiencia de \\(k\\) factores comunes para explicación de las correlaciones entre las variables originales y de la varianza que comparten. El estadístico del contraste es \\(-2ln\\lambda=np(\\hat{} - ln \\hat{g} -1]\\), donde \\(\\hat{}\\) y \\(\\hat{g}\\) son las medias aritmética y geométrica, respectivamente, de los autovalores de la matriz \\(\\hat{\\boldsymbol{\\Sigma}}_{H_{0}}^{-1} \\mathbf{S}\\). Bajo \\(H_0\\), se distribuye asintóticamente como una \\(\\chi_{df}^2\\), con \\(df= \\left( p+\\frac{p(p+1)}{2}\\right) - \\left( p+pk+p-\\frac{k(k-1)}{2}\\right)= \\frac{1}{2} (p-k)^2- \\frac{1}{2}(p+k)\\).210Este test se aplica de manera secuencial: se formula como hipótesis nula \\(k=0\\). Si se rechaza, hay factores comunes subyacentes. Si se rechaza, se sigue con \\(k=1\\). Si se rechaza \\(k=1\\), se concluye que el modelo con un factor es una adecuada representación de la realidad; si se rechaza, se formula la hipótesis nula de que \\(k=2\\), y el proceso continúa hasta que se rechace la hipótesis nula, siempre que el valor de \\(k\\) sea compatible con un número de grados de libertad positivo.","code":"\nround(af_facprin2$residual, 3)\n#>           ebroad esales esocmedia   eweb hbroad  hiacc   iuse\n#> ebroad     0.505 -0.068     0.008  0.068 -0.045  0.002  0.003\n#> esales    -0.068  0.447     0.026  0.015  0.004 -0.012  0.021\n#> esocmedia  0.008  0.026     0.322 -0.047  0.014 -0.005  0.017\n#> eweb       0.068  0.015    -0.047  0.182  0.012  0.015 -0.042\n#> hbroad    -0.045  0.004     0.014  0.012  0.131 -0.005  0.010\n#> hiacc      0.002 -0.012    -0.005  0.015 -0.005  0.018  0.002\n#> iuse       0.003  0.021     0.017 -0.042  0.010  0.002  0.125\naf_facprin2$rms\n#> [1] 0.02907475\naf_facprin2$fit\n#> [1] 0.9715865"},{"path":"análisis-factorial.html","id":"puntuaciones-factoriales","chapter":"Capítulo 33 Análisis factorial","heading":"33.3.4 Puntuaciones factoriales ","text":"Las puntuaciones factoriales son las estimaciones de los valores de los factores aleatorios observados, es decir, de los elementos de \\({\\bf F}_{mxm}\\). Así, \\(\\hat{f}_{im}\\) será la estimación del valor del \\(m\\)-ésimo factor para la \\(\\)-ésima observación (elemento, individuo, objeto…). Cuando se extraen los factores por componentes principales las puntuaciones son exactas.Estas estimaciones pueden ser usadas como inputs para posteriores análisis (regresión, cluster, etc.) en los que se trabaje con los mismos elementos o individuos, sustituyendo las variables originales por los nuevos factores obtenidos. La cuestión es: ¿cómo calcular estas puntuaciones?, porque tanto los factores como los errores son observables sino aleatorios.Los métodos más populares para obtener la estimación de las puntuaciones factoriales son:El de regresión por mínimos cuadrados ordinarios (MCO), donde \\(\\hat{\\bf F}=\\left (\\bf ^{\\prime} \\bf \\right)^{-1}\\bf ^{\\prime}\\bf Z\\).El de regresión por mínimos cuadrados ordinarios (MCO), donde \\(\\hat{\\bf F}=\\left (\\bf ^{\\prime} \\bf \\right)^{-1}\\bf ^{\\prime}\\bf Z\\).El de Bartlett, basado en el método de estimación por mínimos cuadrados generalizados (MCG), con \\(\\hat{\\bf F}=\\left (\\bf ^{\\prime} \\bf \\Psi ^{-1}\\bf \\right)^{-1}\\bf ^{\\prime}\\Psi ^{-1}\\bf Z\\). El mismo estimador se puede obtener por máxima verosimilitud asumiendo normalidad multivariante.El de Bartlett, basado en el método de estimación por mínimos cuadrados generalizados (MCG), con \\(\\hat{\\bf F}=\\left (\\bf ^{\\prime} \\bf \\Psi ^{-1}\\bf \\right)^{-1}\\bf ^{\\prime}\\Psi ^{-1}\\bf Z\\). El mismo estimador se puede obtener por máxima verosimilitud asumiendo normalidad multivariante.El de Thompson (con un enfoque bayesiano), donde \\(\\hat{\\bf F}=\\left (\\bf +\\bf ^{\\prime} \\bf \\Psi ^{-1}\\bf \\right)^{-1}\\bf ^{\\prime}\\Psi ^{-1}\\bf Z\\). El de Thompson (con un enfoque bayesiano), donde \\(\\hat{\\bf F}=\\left (\\bf +\\bf ^{\\prime} \\bf \\Psi ^{-1}\\bf \\right)^{-1}\\bf ^{\\prime}\\Psi ^{-1}\\bf Z\\). El de Anderson-Rubin (que obtiene estimaciones MCG imponiendo la condición \\(\\bf F^{\\prime}F =\\) (\\(\\hat{\\bf F}=\\left (\\bf ^{\\prime} \\bf \\Psi ^{-1}\\bf R \\bf \\Psi ^{-1}\\bf \\right)^{-1}\\bf ^{\\prime}\\Psi ^{-1}\\bf Z\\)). El de Anderson-Rubin (que obtiene estimaciones MCG imponiendo la condición \\(\\bf F^{\\prime}F =\\) (\\(\\hat{\\bf F}=\\left (\\bf ^{\\prime} \\bf \\Psi ^{-1}\\bf R \\bf \\Psi ^{-1}\\bf \\right)^{-1}\\bf ^{\\prime}\\Psi ^{-1}\\bf Z\\)). Las ventajas y desventajas de cada uno de ellos pueden verse en Mardia, Kent, Bibby (1979b) y De la Fuente (2011).En el ejemplo de las TIC, las puntuaciones de los dos factores extraídos con el método de los factores principales y rotados con VARIMAX (la rotación afecta las puntuaciones), calculadas por el método de regresión, para los países de la UE-27 (se muestran los de Bélgica, Bulgaria y la República Checa), se obtienen en R como sigue:","code":"\naf_facprin3 <- fa(cor(TIC2021), nfactors=2, rotate=\"VARIMAX\", fm=\"pa\", scores=\"regression\") \nfactor.scores(TIC2021, af_facprin3)$scores[1:3,]\n#>           PA1         PA2\n#> BE  0.6256359  1.01289866\n#> BG -2.1820404 -0.03439974\n#> CZ -0.2189723  1.08635525"},{"path":"análisis-factorial.html","id":"relaciones-y-diferencias-entre-el-af-y-el-acp","chapter":"Capítulo 33 Análisis factorial","heading":"33.4 Relaciones y diferencias entre el AF y el ACP","text":"ACF y AF son aparentemente muy similares, pero en realidad son muy diferentes.\nTanto ACP como AF son técnicas de reducción de la dimensionalidad que aparecen juntas en los paquetes estadísticos y persiguen objetivos muy similares, lo cual, en determinadas ocasiones, lleva al lector pensar que son intercambiables entre sí, cuando ello es cierto. Por ello, este capítulo finaliza con un breve comentario sobre las diferencias más relevantes entre ambos enfoques.La primera es que ACP es una mera transformación de los datos en la que se hace ningún supuesto sobre la matriz de covarianzas o de correlaciones. Sin embargo, AF asume que los datos proceden de un modelo bien definido, el modelo (33.3), en el que los factores subyacentes satisfacen unos supuestos bien definidos.En segundo lugar, en ACP el énfasis se pone en el paso desde las variables observadas las componentes principales, mientras que en AF se pone en el paso desde los factores latentes las variables observadas. Es cierto que en ACP se pueden retener \\(k\\) componentes y partir de ellas aproximar (reproducir) las variables observadas; sin embargo, esta manera de proceder parece menos natural que la aproximación de las variables observadas en términos de los factores comunes y, además, al tener en cuenta la unicidad de las variables, sobrestima las cargas factoriales y la dimensionalidad del conjunto de variables originales.Una tercera diferencia es que, mientras que ACP obtiene componentes en función de las variables originales (los valores de las variables pueden ser estimados posteriori en función de dichas componentes o factores), en AF las variables son, ellas mismas, combinaciones lineales de factores desconocidos. Es decir, mientras que en ACP la solución viene de la mano de la descomposición en valores singulares, en AF requiere procedimientos de estimación, normalmente iterativos.La cuarta es que ACF es un procedimiento cerrado mientras que AF es abierto, en el sentido de que explica la varianza común y toda la varianza.Finalmente, como pudo verse en 33.3.2.1, cuando las varianzas de los factores únicos son prácticamente nulas, el método de los factores principales es equivalente ACP, y cuando son pequeñas ambos dan resultados similares. Sin embargo, cuando son grandes, en ACP las componentes principales (tanto las retenidas como las que se retienen) las absorben, mientras que el AF las considera y les da su lugar.RESUMENEl Análisis Factorial es una técnica de reducción de la dimensionalidad que trata de dar una explicación de la varianza compartida, o común, de las variables objeto de estudio (de toda la varianza, como hace el análisis de componentes principales) mediante un número mucho menor de factores comunes latentes. Por consiguiente, solo tiene sentido implementarlo si dichas variables se encuentran fuertemente correlacionadas. Tras introducir al lector en los principales elementos teóricos del Análisis Factorial (el modelo básico y la solución factorial completa), se abordan las distintas etapas del procedimiento en su vertiente práctica: \\(()\\) el pre-análisis factorial, que responde la pregunta de si procede o llevarlo cabo; \\((ii)\\) el análisis factorial propiamente dicho, prestando especial atención los métodos de extracción de los factores y las rotaciones de los mismos para facilitar su interpretación; y \\((iii)\\) el post-análisis factorial, que incluye una serie de procedimientos para determinar si la solución factorial obtenida es o aceptable. Posteriormente, se aborda la cuestión de cómo estimar los valores de los factores obtenidos para cada elemento o individuo involucrado en el análisis, pues estas estimaciones pueden usarse como inputs en análisis posteriores (regresión, cluster, etc.) sustituyendo las variables originales por los factores obtenidos. El capítulo finaliza con algunos comentarios sobre las diferencias entre el análisis factorial y el de componentes principales, aparentemente muy similares, pero en realidad muy diferentes.","code":""},{"path":"escalamiento-multidimensional.html","id":"escalamiento-multidimensional","chapter":"Capítulo 34 Escalamiento multidimensional","heading":"Capítulo 34 Escalamiento multidimensional","text":"José Luis Alfaro Navarro \\(^{}\\) y Manuel Vargas Vargas \\(^{}\\)\\(^{}\\) Universidad de Castilla-La Mancha","code":""},{"path":"escalamiento-multidimensional.html","id":"introducción-14","chapter":"Capítulo 34 Escalamiento multidimensional","heading":"34.1 Introducción","text":"El escalado multidimensional (EMD) fue propuesto por primera vez la Universidad de Princeton por Warren S. Torgerson principios de la década de 1950 siendo un investigador importante en este campo Joseph Bernard Kruskal. El EMD engloba una variedad de técnicas multivariables cuya finalidad es obtener la estructura (factores o dimensiones) de los individuos (o variables) subyacente una matriz de datos empíricos, lo que se consigue al representar dicha estructura en una forma geométrica bi o tridimensional.Por tanto, la idea del EMD es representar los datos en baja dimensión (usualmente 2 dimensiones) utilizando la información proporcionada por las distancias entre los datos. Esta técnica surge ya que cada vez con más frecuencia los datos particulares de los que se dispone y el objetivo del análisis hacen difícil su tratamiento con las medidas clásicas, por lo que se han ido diseñando nuevas medidas de distancia entre datos. Estas medidas se pueden utilizar para diferentes tareas: agrupamiento de casos, clasificación, detección de patrones o dimensiones subyacentes, recuperación de información, etc. Por lo tanto, EMD aborda algunas problemáticas que pueden ser analizadas con otras técnicas como, por ejemplo, análisis de componentes principales o factorial cuando el objetivo es representar muchas variables en pocas dimensiones mediante la identificación de la estructura interna de los datos, dimensiones o factores en base la matriz de correlaciones como medida de proximidad entre las variables o el análisis cluster cuando el objetivo es analizar la proximidad entre los objetos, personas, productos, etc. estudiados.El EMD analiza matrices de proximidad (similitud, disimililitud o distancia), por ello, es una alternativa más flexible que otros métodos multivariantes con los que comparte objetivos, ya que sólo requiere de una matriz con las proximidades entre los datos, que pueden representar valoraciones personales, grado de acuerdo entre juicios, parecido entre objetos, frecuencias de aparición de rasgos, diferencias entre tratamientos, etc. La idea central es que las distancias que median entre los puntos se corresponden con las proximidades entre los objetos por medio de una función de ajuste resultante de un proceso iterativo de optimización, pudiendose describir las relaciones entre los objetos sobre la base de las proximidades observadas (López-Gónzalez Hidalgo-Sánchez 2010)En R, existen distintas funciones para desarrollar el EMD, desde las clásicas funciones cmdscale() del paquete base e isoMDS() del paquete MASS hasta el enfoque más actual, usado en este documento, recogido en el paquete smacof (de Leeuw Mair 2009; Mair, Groenen, de Leeuw 2022) que proporciona al usuario una gran flexibilidad para especificar EMD. Utiliza siempre matrices de disimilaridad y, desde la primera versión, se han implementado varios enfoques adicionales de EMD y despliegue, así como varias extensiones y funciones de utilidad.modo de ejemplo se va usar la información relacionada con 7 variables de la sociedad de la información disponibles para 27 países europeos en la base de datos TIC2021, cuatro relacionadas con el uso de las TIC por parte de las empresas y tres de aspectos relacionados con el uso por parte de las personas y la equipación de los hogares. Dicha información, así como la descripción de las variables, puede consultarse en la base de datos TIC2021 del paquete CDR.","code":""},{"path":"escalamiento-multidimensional.html","id":"medición-de-distancias-y-similitudes","chapter":"Capítulo 34 Escalamiento multidimensional","heading":"34.2 Medición de distancias y similitudes","text":"Tanto para el EMD como para muchas otras técnicas multivariantes, el concepto de distancia , entendida como medida de diferenciación entre objetos, constituye la base fundamental de la obtención y presentación de sus resultados. También son frecuentes los conceptos de “disimilaridad”, muy parecido al de distancia, o de “similaridad”, dual en su sentido al de distancia. Se nombre como se nombre, la característica que hay que tener siempre presente es si la medida indica “alejamiento” entre los objetos (distancia o disimilaridad) o “cercanía” (similaridad o proximidad) .Básicamente, se considera una medida de distancia una función que asigna cada par de objetos (\\(o_i\\) y \\(o_j\\)), que pueden contener mediciones de variables x e y, un número real, \\(d(o_i, o_j)=\\delta_{ij}\\), que debe cumplir las siguientes condiciones (para un análisis más detallado véase Sec. 30.3):negatividad \\(\\delta_{ij} \\geq 0\\)Simetría, \\(\\delta_{ij} = \\delta_{ji}\\)Identificación del objeto, \\(\\delta_{ii}=0\\)Si además es semidefinida positiva y cumple la desigualdad triangular se dice que \\(\\delta\\) es una distancia métrica.Aunque se va profundizar en ello, existen diferencias matemáticas en los requisitos que debe cumplir una medida para ser considerada una distancia o una distancia métrica, así como las condiciones para ser considerada una similaridad . Básicamente, se considera una medida de similaridad una aplicación que asigna cada par de objetos (\\(o_i\\) y \\(o_j\\)) un número real, \\(s_{ij}\\), que cumple las mismas condiciones que la distancia salvo la condición c para la que tiene que cumplir que \\(s_{ij} \\leq s_{ii}\\).Esta condición es más díficil de cumplir por lo que se emplean mucho más las medidas de distancia al ser más sencillo formular la propiedad c pues simplifica mucho el poder atribuir un valor de referencia cero para definir la distancia de un individuo sí mismo. La similitud carece de este valor de referencia, siendo posible que la similitud de un individuo sí mismo sea diferente de unos otros. pesar de esta dificultad, las medidas de similitud surgen de modo natural en muchos problemas relacionados con valoraciones subjetivas de similitud.Para un conjunto finito de objetos, la matriz de similaridad es:\\[\\begin{equation}\n\\textbf{S}= \\begin{pmatrix}\ns_{11} & s_{12} & \\dotsb & s_{1n}\\\\\ns_{21} & s_{22} & \\dotsb & s_{2n}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\ns_{n1} & s_{n2} & \\dotsb & s_{nn}\\\\\n\\end{pmatrix}\n\\tag{34.1}\n\\end{equation}\\]El paso de una medida de similaridad (\\(s_{ji}\\)) una distancia (\\(\\delta_{ij}\\)) se puede hacer de diversas formas. Las más usuales son:\\(\\delta_{ij} = 1-s_{ij}\\)\\(\\delta_{ij} = 1-s_{ij}\\)\\(\\delta_{ij} = \\sqrt{1-s_{ij}}\\)\\(\\delta_{ij} = \\sqrt{1-s_{ij}}\\)Si los valores de la diagonal de S son la unidad, \\(\\delta_{ij} = \\sqrt{s_{ii}+s_{jj}-2s_{ij}}\\)Si los valores de la diagonal de S son la unidad, \\(\\delta_{ij} = \\sqrt{s_{ii}+s_{jj}-2s_{ij}}\\)En general, cuando las características que se miden sobre los objetos son variables cuantitativas p-dimensionales, las distancias más usadas son la euclídea, la city-block, la de Minkowski o la de Mahalanobis (véase Sec. 30.3).Cuando las variables son binarias (0 y 1), los coeficientes de similaridad más utilizados son el coeficiente de Jaccard o el de Sokal-Sneath; por último, en el caso más general en el que existan variables cuantitativas, binarias y/o cualitativas, se suele utilizar la distancia de Gower (véase Sec. 30.3).Como se aprecia, las características de los datos que se quieren analizar influyen determinantemente en qué tipo de medida de proximidad utilizar. su vez, la elección de una medida concreta puede modificar la configuración de los datos y, consecuentemente, los resultados de los análisis que se hagan partir de ellos.","code":""},{"path":"escalamiento-multidimensional.html","id":"modelo-de-escalamiento-multidimensional","chapter":"Capítulo 34 Escalamiento multidimensional","heading":"34.3 Modelo de escalamiento multidimensional","text":"El EMD parte de una matriz de proximidades entre \\(n\\) objetos:\\[\\begin{equation}\n\\boldsymbol{\\Delta} _{nxn}=\n\\begin{pmatrix}\n\\delta_{11} & \\delta_{12} & \\dotsb & \\delta_{1n}\\\\\n\\delta_{21} & \\delta_{22} & \\dotsb & \\delta_{2n}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\delta_{n1} & \\delta_{n2} & \\dotsb & \\delta_{nn}\\\\\n\\end{pmatrix}\n\\tag{34.2}\n\\end{equation}\\]y busca una representación de los \\(n\\) objetos en un espacio de menor dimensión, \\(m\\), donde \\(x_{ij}\\) es la coordenada del objeto \\(\\) en la dimensión \\(j\\):\\[\\begin{equation}\n\\mathbf{X}_{nxm}=\n\\begin{pmatrix}\nx_{11} & x_{12} & \\dotsb & x_{1m}\\\\\nx_{21} & x_{22} & \\dotsb & x_{2m}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nx_{n1} & x_{n2} & \\dotsb & x_{nm}\\\\\n\\end{pmatrix}\n\\tag{34.3}\n\\end{equation}\\]de forma que se puede calcular la distancia euclídea entre cada par de objetos:\\[\\begin{equation}\nd_{ij}=\\sqrt{\\displaystyle\\sum_{t=1}^{m}{(x_{} - x_{jt})^2}}\n\\tag{34.4}\n\\end{equation}\\]y, construir una matriz de distancias “reproducidas” \\[\\begin{equation}\n\\mathbf{D}_{nxn}=\n\\begin{pmatrix}\nd_{11} & d_{12} & \\dotsb & d_{1n}\\\\\nd_{21} & d_{22} & \\dotsb & d_{2n}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nd_{n1} & d_{n2} & \\dotsb & d_{nn}\\\\\n\\end{pmatrix}\n\\tag{34.5}\n\\end{equation}\\]que aproxime, en la medida de lo posible, la matriz de proximidades, \\(\\boldsymbol{\\Delta}\\).El concepto básico del EMD es que las distancias entre los objetos en la configuración X, \\(d_{ij}\\) deben corresponder las proximidades originales, \\(\\delta_{ij}\\) mediante una transformación, \\(d_{ij}=f(\\delta_{ij})\\), donde f es de algún tipo determinado.En la práctica, se suele encontrar un ajuste perfecto, por lo que existe un cierto grado de error. Por ello, se define el Stress de Kruskal como una medida de la bondad de ajuste del modelo:\\[\\begin{equation}\nStress= \\sqrt{\\frac {\\sum{(f(\\delta_{ij})-d_{ij})^2}}{\\sum d_{ij}^2}}\n\\tag{34.6}\n\\end{equation}\\]En caso de un ajuste perfecto, el Stress sería 0, aumentando conforme más grandes sean los errores (diferencias entre las distancias “reproducidas” y las originales). Así, la solución proporcionada por el EMD será “mejor” cuanto más pequeña sea la medida del Stress. También es frecuente utilizar una variante, llamada S-Stress, definida como:\\[\\begin{equation}\nS-Stress= \\sqrt{\\frac {\\sum{(f(\\delta_{ij})^2-d_{ij}^2)^2}}{\\sum (d_{ij}^2)^2}}\n\\tag{34.7}\n\\end{equation}\\]Otra medida que se suele utilizar como grado de ajuste es el coeficiente de correlación al cuadrado (RSQ), que indica la proporción de variabilidad de los datos explicada por el modelo:\\[\\begin{equation}\nRSQ=\\frac{[\\sum{(d_{ij}-d_{..})(f(\\delta_{ij})-f(\\delta_{..}))}]^2}{[\\sum{(d_{ij}-d_{..})^2][\\sum{(f(\\delta_{ij})-f(\\delta_{..}))}^2]}}\n\\tag{34.8}\n\\end{equation}\\]Este valor oscila entre 0 y 1, y se intrepreta de forma contraria las medidas de Stress: mientras mayor sea el RSQ, mejor ajuste del modelo.","code":""},{"path":"escalamiento-multidimensional.html","id":"tipos-de-escalamiento-multidimensional","chapter":"Capítulo 34 Escalamiento multidimensional","heading":"34.4 Tipos de escalamiento multidimensional","text":"La elección de la función \\(f\\) que relaciona las proximidades originales y las distancias “reproducidas” produce dos tipos básicos de EMD, el EMD métrico (o clásico) y el EMD métrico . En el primero, se considera que los datos están medidos en escala de intervalo o de razón y existe una relación funcional entre las distancias originales y las reproducidas; mientras que el segundo se suele aplicar cuando los datos están en escala ordinal o se asume una relación funcional entre las distancias originales y las reproducidas, sino que sólo se conserva su ordenación.","code":""},{"path":"escalamiento-multidimensional.html","id":"escalado-multidimensional-métrico","chapter":"Capítulo 34 Escalamiento multidimensional","heading":"34.4.1 Escalado multidimensional métrico","text":"En el modelo de escalamiento métrico se asume que la relación entre las proximidades y las distancias es de tipo lineal, \\(d_{ij}=+b \\delta_{ij}\\). De esta forma, se conserva la métrica de distancia original, entre puntos, lo mejor posible, siendo adecuado para el caso de variables cuantitativas. También se conoce como EMD clásico o como análisis de coordenadas principales.En el ejemplo introductorio, se va aplicar un EMD métrico con un doble objetivo: por un lado se usa la matriz de correlaciones entre la variables con el objetivo de analizar la similitud entre las mismas y, por otro lado, se determina la distancia entre observaciones con el objetivo de analizar la similitud exitente entre observaciones, en este caso los países europeos.En el primer caso, los “objetos” son las siete variables de la base de datos TIC2021 y se ha utilizado como medida de proximidad (similitud) el coeficiente de correlación entre las variables, por lo que se plantea un EMD métrico. Los pasos seguir son:• Calcular la matriz de disimilaridades sobre la que actúa smacof(). En este ejemplo, se usa la matriz de correlaciones que se debe convertir en matriz de disimilaridades, mediante la función sim2diss().La conversión de similidades (correlaciones) en disimilaridades se ha hecho por el método “corr”, que utiliza la expresión general \\(\\delta_{ij} = 1-s_{ij}\\). Existen otros métodos en la función sim2diss() para cuando la matriz de proximidades sea de correlaciones. El argumento .dist=TRUE permite convertir el resultado en un objeto de la clase dist.• Una vez que disponemos de la matriz de disimilaridades, aplicamos el EMD métrico mediante la función mds(), versión equivalente la función smacofSym().La Fig. 34.1 que representa los objetos según sus distancias “reproducidas” muestra la configuración final de los siete objetos:\nFigura 34.1: Gráfico de objetos en el plano de las distancias reproducidas.\nLa información numérica detallada se podria obtener con la información de la salidad dada por: res$conf que mostraría las coordenadas de los objetos en las dos dimensiones; res$confdist que muestra la matriz de distancias reproducidas; res$stress para obtener la medida de stress de Krukal y res$spp con la contribución porcentual de cada objeto al stress.En este caso los resultados muestran que la medida de stress es razonablemente baja con un valor de 0.16, indicando una buena “reproducción” de las proximidades originales. Además, la “contribución” relativa al stress de cada uno de los objetos (delitos) es bastante homogénea, siendo la variable porcentaje de empresas con banda ancha (EBROAD) la que más contribuyen al stress y el nivel de acceso internet de los hogares (HIACC), la que menos. Para ver gráficamente el grado de ajuste, se usa el gráfico de Shepard (Fig. 34.2) que compara las proximidades originales y las distancias obtenidas: \nFigura 34.2: Gráfico de Shepard con método ratio.\nEl diagrama de Shepard incluye las proximidades originales entre pares de objetos (en gris claro) y las obtenidas por el EMD (en negro). También representa el método elegido; en este ejemplo, al usar el argumento method=“ratio” estamos imponiendo una relación proporcional entre ambos tipos de similitudes, por lo que aparece una recta (que pasa por el origen). Dadas las diferencias que se ven en el gráfico, quizás la elección del método ratio (opción por defecto), sea la más adecuada. Probando con el método interval, que impone una relación lineal entre ambos tipos de similitudes (recta que tiene que pasar necesariamente por el origen), se obtendría la Fig. 34.3:\nFigura 34.3: Gráfico de Shepard con método interval.\nLa medida de stress se ha reducido la mitad, 0.087, indicando mejor ajuste, y el gráfico de Shepard muestra más concordancia entre las proximidades originales de los pares de objetos y las distancias reproducidas. Otra opción sería usar el método mspline, pero en este caso las diferencias son menores. Los tres métodos son métricos, puesto que se fija una forma funcional para relacionar las proximidades originales y las disimilitudes del modelo (un ratio, una función lineal o una función spline).• Por último, para “interpretar” el sentido de las dimensiones en las que se representan los objetos, se recurre ver cuáles están en los extremos. En la parte izquierda de la dimensión 1 están las variables relacionadas con el uso en las empresas mientras que en la parte derecha están las relacionadas con los hogares y las personas; se podría decir, entonces, que es una dimensión relacionada con el ámbito de uso de las TIC. En la dimensión 2, con menos distancias, y una interpretación menos clara, aparecen en la parte superior las variables relacionadas con el tipo de conexión y la existencia de web en las empresas y en la parte inferior las relacionadas con las redes sociales, las ventas o la frecuencia de uso de internet por parte de los individuos; se podría decir, que es una dimensión asociada al uso dado las TIC.Los pasos para desarrollar el mismo análisis agrupando países, observaciones en lugar de variables, serían similares pero usando la matriz de distancias en lugar de la matriz de correlaciones, por lo tanto:\nFigura 34.4: Gráfico de países sobre el plano de las distancias reproducidas.\nEn la Fig. 34.4, la interpretación de la dimensión 1 muestra la existencia de una diferencia clara entre los países del norte y los de última adhesión la Unión Europea mientras que en la dimensión 2, con unas diferencias menores, aparecen en la parte superior los países de Chipre y Luxemburgo y en la parte inferior Lituanía, Croacia, Irlanda y la República Checa.","code":"\nlibrary('smacof')\nlibrary('CDR')\ncorrelacion<- cor(TIC2021)\nround(correlacion[1:3,],3)\n#>           ebroad esales esocmedia  eweb hbroad hiacc  iuse\n#> ebroad     1.000  0.377     0.587 0.704  0.422 0.521 0.632\n#> esales     0.377  1.000     0.542 0.585  0.167 0.195 0.479\n#> esocmedia  0.587  0.542     1.000 0.698  0.567 0.609 0.756\ndatos<-sim2diss(correlacion,method=\"corr\",to.dist=TRUE)\nres<-mds(datos,ndim=2,type=\"ratio\")\nres\n#> \n#> Call:\n#> mds(delta = datos, ndim = 2, type = \"ratio\")\n#> \n#> Model: Symmetric SMACOF \n#> Number of objects: 7 \n#> Stress-1 value: 0.161 \n#> Number of iterations: 42\nplot(res)\nplot(res,plot.type=\"Shepard\")\nres2<-mds(datos,type=\"interval\")\nplot(res2,plot.type=\"Shepard\")\nlibrary('factoextra')\nd_euclidea <- get_dist(x = TIC2021, method = \"euclidea\")\nres3<-mds(d_euclidea,ndim=2,type=\"ratio\")\nres3\n#> \n#> Call:\n#> mds(delta = d_euclidea, ndim = 2, type = \"ratio\")\n#> \n#> Model: Symmetric SMACOF \n#> Number of objects: 27 \n#> Stress-1 value: 0.112 \n#> Number of iterations: 77\nplot(res3)"},{"path":"escalamiento-multidimensional.html","id":"escalado-multidimensional-no-métrico","chapter":"Capítulo 34 Escalamiento multidimensional","heading":"34.4.2 Escalado multidimensional no métrico","text":"En el modelo de escalamiento métrico (también conocido como EMD ordinal) se asume ninguna fórmula métrica que relacione las proximidades originales con las distancias reproducidas, sino que sólo describe un patrón creciente (o decreciente) entre ellas. es significativo el valor de distancia, sino su relación con las distancias entre otros pares de objetos, por lo que construye distancias ajustadas que están en el mismo orden de rango que la proximidad original. Por ejemplo, si la distancia de los objetos separados y B ocupa el tercer lugar en los datos de proximidades originales ordenadas, entonces también debería ocupar el tercer lugar en los datos de distancias reproducidas ordenadas. Así, el EMD métrico busca conservar, tanto los valores de las proximidades originales, sino la ordenación de los objetos en función de dichas proximidades; es, por tanto, un modelo que se ajusta mejor datos cualitativos que el métrico, aunque también se utiliza cuando se busca mayor flexibilidad en el ajuste.Se va aplicar un MDS métrico la matriz de correlaciones de las tasas anuales de siete delitos en 50 estados de EE.UU. (asesinatos, violaciones, robos, asaltos, allanamiento de morada, hurtos y sustracciones de vehículos). Los datos están disponibles en la librería smacof bajo el nombre de crimes:En este ejemplo, los “objetos” son los siete tipos de delito, y se ha utilizado como medida de proximidad (similitud) el coeficiente de correlación entre las tasas de delito (variables cuantitativas).El primer paso consiste en calcular la matriz de disimilaridades sobre la que actúa smacof, utilizando la función sim2diss().La conversión de similidades (correlaciones) en disimilaridades se ha hecho por el método corr, que utiliza la expresión general \\(\\delta_{ij} = \\sqrt {1-s_{ij}}\\). Existen otros métodos en la función sim2diss() para cuando la matriz de proximidades sea de correlaciones. El argumento .dist=TRUE permite convertir el resultado en un objeto de la clase dist si fuese necesario.Frente los métodos métricos, que fijan una forma funcional para relacionar las proximidades originales y las disimilitudes del modelo (un ratio, una función lineal o una función spline), una alternativa más flexible es asumir una relación métrica, donde sólo se conserve la ordenación de las proximidades originales. En este caso, se busca que las distancias reproducidas ordenen los pares de objetos de forma idéntica la original.Para ello, se utiliza el método ordinal dentro de la función mds():Como se aprecia, la medida de stress es muy baja (0.002), indicando un muy buen ajuste, que resulta significativo como indica la Fig. 34.5 basada en la función permtest():\nFigura 34.5: Test de permutaciones para evaluación de la significatividad de la medida de stress.\nEl gráfico de Shepard, representado en la Fig. 34.6, incluye las proximidades originales entre pares de objetos (en gris claro) y las obtenidas por el MDS (en negro), mostrando una alta concordancia entre las ordenaciones original y reproducida:\nFigura 34.6: Gráfico de Shepard: concordancia entre las ordenaciones original y reproducida.\nLa contribución porcentual de cada delito la medida de stress y las coordenadas bidimensionales reproducidas serían:El delito de robo (39.27%) y el de sustracción de vehículos (26.48%) son responsables del 65.75% del stress, seguidos muy de lejos por los otros cinco tipos de delito.La Fig. 34.7 muestra la representación bidimensional de la configuración final de los siete delitos según sus distancias “reproducidas”:\nFigura 34.7: Representación bidimensional de las disimilitudes entre los siete tipos de delito.\nPor último, para “interpretar” el sentido de las dimensiones en las que se representan los objetos, se recurre ver cuáles están en los extremos. En la parte izquierda de la dimensión 1 están los delitos de asesinato y asalto, mientras que en la parte derecha están los de hurto, sustracción de vehículos y allanamiento; se podría decir, entonces, que es una dimensión relacionada con el grado de “personalización” del delito: los que afectan personas directamente frente los que . La dimensión 2, con menos distancias (véasen las escalas) y de más difícil interpretación, contrapone en la parte superior los delitos que más “beneficios” económicos producen (robos, sustracción de vehículos) frente los que menos (violación o hurto); se podría decir, entonces, que es una dimensión asociada la repercusión económica del delito.Por último, destacar que puede ser interesante analizar la estabilidad de las soluciones del EMD (bien mediante jackknife, bootstrap, o elipses de pseudo-confianza). También puede interesar plantear un modelo de EMD que permita valorar las diferencias individuales (abordable con la función smacofIndDiff()). Otra alternativa puede ser abordar un desplegamiento multidimensional, que representa conjuntamente objetos e individuos.ResumenLa aplicación del análisis EMD implica tres pasos consecutivos:• La determinación de las proximidades originales entre los objetos. Esta fase depende de las características de los objetos y del tipo de relación que se quiera/pueda establecer entre ellos. Actualmente, R dispone de paquetes que permiten estimar las matrices de proximidad partir de los datos en bruto.• La conversión de las proximidades en similaridades (si fuese necesario) y el ajuste entre las originales y las reproducidas por el EMD. Se debe elegir el tipo de EMD utilizar, que depende de la función de ajuste: se puede optar por funciones de tipo ratio, interval o mspline (EMD métricos) o ordinal (EMD métrico) . La elección estará relacionada con el tipo de datos usados y el grado de ajuste (stress) de los modelos.• La interpretación de los resultados partir de la configuración obtenida, tanto del significado de las dimensiones como de la estructura de los objetos (cuáles se parecen, si existen grupos, etc.)","code":"\ndata(\"crimes\")\noptions(digits=3)\ndata<-sim2diss(crimes,method=\"corr\",to.dist=FALSE)\ndata\n#>            Murder  Rape Robbery Assault Burglary Larceny Auto.Theft\n#> Murder      0.000 0.693   0.812   0.436    0.849   0.970      0.943\n#> Rape        0.693 0.000   0.671   0.548    0.566   0.632      0.748\n#> Robbery     0.812 0.671   0.000   0.663    0.616   0.748      0.616\n#> Assault     0.436 0.548   0.663   0.000    0.693   0.825      0.819\n#> Burglary    0.849 0.566   0.616   0.693    0.000   0.447      0.548\n#> Larceny     0.970 0.632   0.748   0.825    0.447   0.000      0.671\n#> Auto.Theft  0.943 0.748   0.616   0.819    0.548   0.671      0.000\nres4<-mds(data,ndim=2,type=\"ordinal\")\nres4\n#> \n#> Call:\n#> mds(delta = data, ndim = 2, type = \"ordinal\")\n#> \n#> Model: Symmetric SMACOF \n#> Number of objects: 7 \n#> Stress-1 value: 0.002 \n#> Number of iterations: 15\nptestnm<-permtest(res4,nrep=50)\nplot(ptestnm)\nplot(res4,plot.type=\"Shepard\")\nres4$spp #Contribución porcentual de cada objeto al stress\n#>     Murder       Rape    Robbery    Assault   Burglary    Larceny Auto.Theft \n#>      8.064      6.443     39.270      0.104     10.618      9.023     26.478\nres4$conf #Coordenadas de los objetos en dos dimensiones\n#>                 D1       D2\n#> Murder     -0.9673  0.01136\n#> Rape       -0.1225 -0.37592\n#> Robbery     0.0496  0.53424\n#> Assault    -0.5630 -0.00483\n#> Burglary    0.3925 -0.11326\n#> Larceny     0.6015 -0.47400\n#> Auto.Theft  0.6093  0.42240\nplot(res4)"},{"path":"correspondencias.html","id":"correspondencias","chapter":"Capítulo 35 Análisis de correspondencias","heading":"Capítulo 35 Análisis de correspondencias","text":"Román Mínguez Salido\\(^{}\\) y Manuel Vargas Vargas\\(^{}\\)\\(^{}\\) Universidad de Castilla-La Mancha","code":""},{"path":"correspondencias.html","id":"introducción-15","chapter":"Capítulo 35 Análisis de correspondencias","heading":"35.1 Introducción","text":"El análisis de correspondencias es un método gráfico descriptivo de reducción de la dimensión incluido entre los algoritmos de aprendizaje supervisado. La idea principal es equivalente al método de componentes principales, pero aplicado variables cualitativas. El objetivo es representar los valores (niveles en R) de variables cualitativas (factores en R) en ejes cuantitativos cuyas coordenadas representen la cercanía o lejanía entre los niveles de los factores. Es decir, es un método de reducción de la dimensionalidad para factores representables en pocas dimensiones.Por sencillez, el punto de partida será una tabla de contingencia RxC, \\(T\\), (véase Cap. 23) que recoge la frecuencia de cada par de niveles \\(A_1,A_2,...,A_R\\) del factor \\(\\) y \\(B_1,B_2,...,B_C\\) del factor \\(B\\):Tabla 35.1:  Ejemplo de tabla de contingencia RxCCada fila representa el perfil condicional del nivel \\(A_i\\), siendo la última el perfil marginal del factor \\(\\). Igualmente, cada columna representa el perfil condicional del nivel \\(B_j\\), siendo la última el perfil marginal del factor \\(B\\).Como se vió en el Cap. 23, si los factores fueran independientes, el valor esperado en cada casilla sería \\(E_{ij}=\\frac{n_{.}n_{.j}}{N}\\), por lo que la diferencia tipificada, \\(r_{ij}=\\frac{n_{ij}- E_{ij}}{\\sqrt{E_{ij}}}\\) es una medida de asociación entre las modalidades \\(A_i\\) y \\(B_j\\). La matriz formada por estos “residuos estandarizados” (véase sección 23.5.4), \\(R=\\lbrace r_{ij} \\rbrace\\) resume la asociación entre los atributos, y es el objetivo básico del análisis de correspondencias; básicamente, se realiza una proyección de las filas y columnas de la tabla de frecuencias relativas (transformadas) para obtener las coordenadas en ejes cuantitativos, representables en la forma habitual como diagramas de puntos.Para un estudio en profundidad de esta técnica pueden consultarse Greenacre (2008) (en español) o Beh Lombardo (2014). En el resto del capítulo se hará una breve exposición de la metodología y se ejemplificará con el análisis de dos tablas de contingencia.","code":""},{"path":"correspondencias.html","id":"metodología-del-análisis-de-correspondencias","chapter":"Capítulo 35 Análisis de correspondencias","heading":"35.2 Metodología del análisis de correspondencias","text":"Dada una tabla de contingencia \\(T\\), partir de las frecuencias observadas \\(n_{ij}\\), se definen las distancias entre los perfiles:para los perfiles fila, \\(d_{ii`}= \\sum_{k=1}^C \\frac {1}{n_{.k}} \\left( \\frac {n_{ik}}{n_{.}} - \\frac {n_{`k}}{n_{`.}} \\right)^2\\)para los perfiles fila, \\(d_{ii`}= \\sum_{k=1}^C \\frac {1}{n_{.k}} \\left( \\frac {n_{ik}}{n_{.}} - \\frac {n_{`k}}{n_{`.}} \\right)^2\\)para los perfiles columna, \\(d_{jj`}= \\sum_{k=1}^R \\frac {1}{n_{k.}} \\left( \\frac {n_{kj}}{n_{.j}} - \\frac {n_{kj`}}{n_{.j`}} \\right)^2\\)para los perfiles columna, \\(d_{jj`}= \\sum_{k=1}^R \\frac {1}{n_{k.}} \\left( \\frac {n_{kj}}{n_{.j}} - \\frac {n_{kj`}}{n_{.j`}} \\right)^2\\)Estas distancias aumentan cuanto más se “diferencien” unos perfiles de otros. El análisis de correspondencias busca construir dimensiones (habitualmente, dos) y obtener las coordenadas de los niveles de ambos factores en dichas dimensiones\\[\\begin{equation}\n\\textbf{} = \\begin{pmatrix} \\textbf{'}_1\\\\ \\vdots\\\\ \\textbf{'}_R\\end{pmatrix} \\text{,    con } \\textbf{}_i= (a_{i1} \\ a_{i2})' \\text{,    y } \\ \\textbf{B} = \\begin{pmatrix} \\textbf{b'}_1\\\\ \\vdots\\\\ \\textbf{b'}_C\\end{pmatrix} \\text{,   con } \\textbf{b}_j= (b_{j1} \\ b_{j2})'\n\\end{equation}\\]siendo \\(\\textbf{}_i\\) las coordenadas del nivel fila \\(A_i\\) y \\(\\textbf{b}_j\\) las del nivel columna \\(B_j\\) en el plano, de forma que “reproduzcan” las distancias entre perfiles fila y columna y los residuos estandarizados (asociaciones):\\[\\begin{equation}\n\\begin{array}{crl}\n{d(\\textbf{}_i , \\textbf{}_{'})= \\sqrt {(a_{i1}-a_{'1})^2+(a_{i2}-a_{'2})^2} \\approx d_{ii'}} \\\\ {d(\\textbf{b}_j , \\textbf{b}_{j'})= \\sqrt {(b_{j1}-b_{j'1})^2+(b_{j2}-b_{j'2})^2} \\approx d_{jj'}} \\\\ \\textbf{'}_i * \\textbf{b}_j \\approx r_{ij} \\end{array}\n\\end{equation}\\]Con las coordenadas contenidas en las matrices y B, es posible “visualizar” la posición relativa de cada factor en las nuevas dimensiones. Esta estructura permite ver, tanto las “distancias” que hay entre los niveles de cada factor (mediante la distancia de representación en el plano), como las “asociaciones” entre niveles de ambos factores (ya que mientras más asociación haya, más cerca se representarán en el plano).Para resolver el problema de estimación de las matrices y B, se busca una descomposición de la matriz de \\(\\textbf{R}= \\lbrace r_{ij} \\rbrace\\) en valores singulares. Según la importancia que se de al ajuste de uno de los perfiles o la matriz de residuos, se tienen diferentes métodos de selección, llamados normalizaciones, que pueden consultarse en Greenacre (2008).","code":""},{"path":"correspondencias.html","id":"proyecciones-fila-columna-y-simétrica","chapter":"Capítulo 35 Análisis de correspondencias","heading":"35.2.1 Proyecciones fila, columna y simétrica","text":"El punto de partida es la matriz de frecuencias relativas \\(\\textbf{P}\\) cuyas entradas son $ n_{ij}/N$, también llamada matriz de correspondencias. Definiendo el vector de unos \\(\\mathbf{1}\\), con la dimensión adecuada, las masas, o frecuencias marginales, de filas y columnas, \\(r_i = \\sum_{j=1}^C p_{ij}\\) y \\(c_j = \\sum_{=1}^R p_{ij}\\), respectivamente, se pueden expresar matricialmente como \\(\\textbf{r}=\\textbf{P} \\mathbf{1}\\) y \\(c=\\textbf{P'} \\mathbf{1}\\) o, en forma de matrices diagonales,\n\\[\\textbf{D}_R=diag(r) \\equiv diag(f_{1.},...,f_{R.}) \\text{  y } \\ \\textbf{D}_C=diag(c) \\equiv diag(f_{.1},...,f_{.C})\\]\nSe calcula la matriz de residuos estandarizados (véase Sec. @(contaprox) como\\[\\begin{equation}\n\\textbf{S}=\\textbf{D}_R^{-\\frac {1}{2}} (\\textbf{P-rc'}) \\textbf{D}_C^{-\\frac {1}{2}}\n\\end{equation}\\]La matriz S se descompone en valores singulares, calculando matrices las U, D y V tales que:\\[\\begin{equation}\n\\begin{array}{crl}\n{\\textbf{S=UDV'}} \\\\ {\\textbf{UU'=V'V=} \\ \\text{ , } \\ \\textbf{U}_{(RxK)}} \\ \\text{ , } \\ \\textbf{V}_{(CxK)} \\ \\text{ , } \\ K=min(R-1, \\ C-1) \\\\ {\\textbf{D}=diag(\\mu_1,...,\\mu_K)} \\end{array}\n\\end{equation}\\]donde los \\(\\mu_i\\) son los llamados valores singulares, estando ordenados de forma decreciente \\(\\mu_1 \\geq \\mu_2 \\geq ... \\geq \\mu_K\\).partir de esta descomposición se pueden obtener:las coordenadas estándar de las filas, \\(\\Phi=\\textbf{D}_R^{-\\frac {1}{2}}\\textbf{U}\\), y sus coordenadas principales, \\(\\textbf{F}=\\Phi \\textbf{D}\\).las coordenadas estándar de las filas, \\(\\Phi=\\textbf{D}_R^{-\\frac {1}{2}}\\textbf{U}\\), y sus coordenadas principales, \\(\\textbf{F}=\\Phi \\textbf{D}\\).las coordenadas estándar de las columnas, \\(\\Gamma=\\textbf{D}_C^{-\\frac {1}{2}}\\textbf{V}\\), y sus coordenadas principales, \\(\\textbf{G}=\\Gamma \\textbf{D}\\).las coordenadas estándar de las columnas, \\(\\Gamma=\\textbf{D}_C^{-\\frac {1}{2}}\\textbf{V}\\), y sus coordenadas principales, \\(\\textbf{G}=\\Gamma \\textbf{D}\\).las inercias principales, \\(\\lambda_i=\\mu_i^2\\).las inercias principales, \\(\\lambda_i=\\mu_i^2\\).Las coordenadas principales son las utilizadas para definir las proyecciones fila y proyecciones columna, que representan, en menor dimensión, los perfiles correspondientes, formando los llamados mapas asimétricos.Por último, las matrices \\(\\textbf{=D}_R^{-\\frac {1}{2}} \\textbf{UD}\\) y \\(\\textbf{B=D}_C^{-\\frac {1}{2}} \\textbf{VD}\\) representan las coordenadas de ambos perfiles en un espacio común, llamado mapa simétrico.","code":""},{"path":"correspondencias.html","id":"procedimiento-con-r-la-función-ca","chapter":"Capítulo 35 Análisis de correspondencias","heading":"35.3 Procedimiento con R: la función ca()","text":"Para realizar un análisis de correspondencias simple con R se puede utilizar el paquete ca, que contiene la función ca(). Esta función acepta como argumento de entrada o bien directamente una tabla de contingencia, o bien los datos originales como objeto matriz o data-frame. Incluso, el argumento puede ser una fórmula del tipo ~ F1 + F2 donde F1 y F2 son factores. Entre los argumentos adicionales se pueden incluir el número de dimensiones en el output así como filas o columnas suplementarias.","code":""},{"path":"correspondencias.html","id":"caso-práctico-1-tareas-del-hogar.","chapter":"Capítulo 35 Análisis de correspondencias","heading":"35.3.1 Caso práctico 1: tareas del hogar.","text":"Como primer ejemplo, se van utilizar los datos housetasks, contenidos en el paquete factoextra, que representan una tabla de contingencia con la frecuencia de ejecución de 13 tareas del hogar por los miembros de la pareja.En primer lugar, la aplicación del test \\(\\chi^2\\) de independencia (véase 23) permite contrastar si los factores son independientes o, por el contrario, están asociados:Con un valor de \\(\\chi^2 =1944.5\\) y un p-valor de 2.2e-16, hay suficiente evidencia como para rechazar la hipótesis nula de independencia, indicando asociación entre ambos factores, por lo que tiene sentido analizar más en profundidad la estructura de dicha asociación.La función ca() proporciona los valores singulares y, tanto para filas como para columnas, las masas (valores “Mass”); las distancias chi-cuadrado, que representan las distancias en esa métrica de cada fila respecto la fila centroide (dada por la masa de las columnas, promedio de los vectores fila); las inercias explicadas, que representan la distancia cuadrática \\(\\chi^2\\) respecto al perfil promedio (sin calcular raíces), ponderada por la masa (de la fila o columna) correspondiente; así como las coordenadas en el espacio proyectado:Las dos primeras dimensiones explican el 48.69% y 39.91% de la inercia respectivamente, por lo que la representación en un plano engloba al 88.6% de la inercia global.Las distancias chi-cuadrado indican lo cerca o lejos que está cada fila respecto al centroide de las mismas. En este ejemplo, la fila más distante del centroide de filas es “Repairs” (1.819), mientras que la columna más distante respecto del centroide de columnas es “Husband” (1.321).Como las inercias miden la variabilidad de los perfiles, en este ejemplo, respecto las filas, el nivel que mayor contribuye es “Repairs” (0.312874) mientras que por columnas es “Husband” (0.381373). Esto es sorprendente ya que ambos niveles eran los más alejados del centro.Con las coordenadas de las dimensiones se puede realizar un gráfico de las mismas utilizando la función plot(), pudiéndose optar por la proyección sólo de las filas (usando los argumentos map=“rowprincipal”, =c(“”,“none”)) o de las columnas (map= “colprincipal”, =c(“none”,“”)), tal como se muestra en la Fig. 35.1:\nFigura 35.1: Proyecciones de los perfiles fila y columna\nRespecto las filas, se aprecian varios grupos: el compuesto por “Breakfast”, “Dinner”, “Main_meal” y “Laundry”; otro por “Shopping”, “Dishes” y “Tidying”; uno tercero por “Insurance” y “Finance”; y el compuesto por “Driving” y “Official”. Los niveles “Holiday” y “Repairs” están alejados del resto.Las coordenadas simétricas permiten la representación de ambos factores la vez (map= “symmetric”, =c(“”,“”)), como se muestra en la Fig. ??.\nFigura 35.2: Proyección simétrica de ambos factores\nEl gráfico conjunto permite observar qué niveles de filas y columnas pueden estar más cercanos (aproximación la asociación entre ellos). El grupo de “Driving” y “Repairs” está cercano “Husband”; el grupo de “Dinner”, ”Breakfast”, “Laundry” y “Main_meal” está cercano “Wife”; mientras que el nivel “Jointly” parece estar asociado “Holidays”, “Finance”, e “Insurance”.","code":"\nlibrary('ca')\nlibrary('factoextra')\ndata('housetasks')\nchisq.test(housetasks)\n#> \n#>  Pearson's Chi-squared test\n#> \n#> data:  housetasks\n#> X-squared = 1944.5, df = 36, p-value < 2.2e-16\noptions(digits = 2)\nca_house <- ca(housetasks, nd = 2)\nca_house\n#> \n#>  Principal inertias (eigenvalues):\n#>            1        2        3       \n#> Value      0.542889 0.445003 0.127048\n#> Percentage 48.69%   39.91%   11.4%   \n#> \n#> \n#>  Rows:\n#>         Laundry Main_meal Dinner Breakfeast Tidying Dishes Shopping Official\n#> Mass       0.10     0.088  0.062      0.080   0.070  0.065    0.069    0.055\n#> ChiDist    1.15     1.017  0.786      0.716   0.594  0.550    0.466    0.984\n#> Inertia    0.13     0.091  0.038      0.041   0.025  0.020    0.015    0.053\n#> Dim. 1    -1.35    -1.188 -0.940     -0.690  -0.534 -0.256   -0.160    0.308\n#> Dim. 2    -0.74    -0.735 -0.462     -0.679   0.651  0.663    0.605   -0.380\n#>         Driving Finances Insurance Repairs Holidays\n#> Mass       0.08    0.065     0.080   0.095    0.092\n#> ChiDist    1.13    0.675     0.853   1.819    1.463\n#> Inertia    0.10    0.030     0.058   0.313    0.196\n#> Dim. 1     1.01    0.367     0.878   2.075    0.343\n#> Dim. 2    -0.98    0.926     0.710  -1.296    2.151\n#> \n#> \n#>  Columns:\n#>          Wife Alternating Husband Jointly\n#> Mass     0.34       0.146    0.22    0.29\n#> ChiDist  0.94       0.899    1.32    1.04\n#> Inertia  0.30       0.118    0.38    0.31\n#> Dim. 1  -1.14      -0.084    1.58    0.20\n#> Dim. 2  -0.55      -0.437   -0.90    1.54\npar(mfrow = c(1, 2))\nplot(ca_house, map = \"rowprincipal\", what = c(\"all\", \"none\"), xlab = \"Perfiles fila\")\nplot(ca_house, map = \"colprincipal\", what = c(\"none\", \"all\"), xlab = \"Perfiles columna\")\nplot(ca_house, map = \"symmetric\", what = c(\"all\", \"all\"), xlab = \"Proyección común de ambos factores\")"},{"path":"correspondencias.html","id":"caso-práctico-2-accidentes-2020.","chapter":"Capítulo 35 Análisis de correspondencias","heading":"35.3.2 Caso práctico 2: accidentes 2020.","text":"Como segundo ejemplo, se van utilizar los datos accidentes2020_data, contenidos en el paquete CDR, en concreto, la información sobre “tipo_accidente” y “estado_meteorológico”. Para evitar pares de niveles con frecuencia nula, se eliminan los niveles “Atropello animal”, “Despeñamiento” y “Otro” del factor “tipo_accidente” y los niveles “Granizando”, “Nevando”, “NULL” y “Se desconoce” del factor “estado_meteorológico”.Se comprueba que existe asociación y se obtienen los resultados del análisis de correspondencias:Las dos primeras dimensiones explican el 87.97% y 11.4% respectivamente, por lo que la representación en un plano engloba al 99.37% de la inercia.La representación gráfica de la proyección simétrica se muestra en la Fig. 35.3:\nFigura 35.3: Proyección simétrica de ambos factores\nSe observa que “Lluvia intensa” está especialmente asociada ningún tipo de accidente; “Lluvia débil” con “Colisión múltiple” y “Choque contra obstáculo fijo”; “Despejado” con “Colisión fronto-lateral”, “Colisión frontal” y “Alcance”; y “Nublado” con “Colisión lateral”.ResumenDada una tabla de contingencia, el análisis de correspondencias reproduce: () las distancias entre niveles de cada factor en un espacio de menor dimensión, permitiendo la comparación gráfica entre ellos; (ii) la representación de los niveles de ambos factores en un espacio común.En el primer caso, permite una visualización de la composición interna de cada factor, identificando los niveles que más se distancian del centroide. En el segundo, permite la representación de la asociación entre niveles de cada uno de los factores.","code":"\nlibrary('CDR')\nlibrary('dplyr')\ndata('accidentes2020_data')\ndatos <- data.frame(\n  V1 = as.factor(accidentes2020_data$tipo_accidente),\n  V2 = as.factor(accidentes2020_data$estado_meteorológico)\n)\nlevelsV1 <- c(\"Alcance\", \"Choque contra obstáculo fijo\", \"Colisión frontal\", \"Colisión fronto-lateral\", \"Colisión lateral\", \"Colisión múltiple\")\nlevelsV2 <- c(\"Despejado\", \"Lluvia débil\", \"LLuvia intensa\", \"Nublado\")\ndatos_depu <- droplevels(filter(datos, (V1 %in% levelsV1) & (V2 %in% levelsV2)))\ntable(datos_depu)\n#>                               V2\n#> V1                             Despejado Lluvia débil LLuvia intensa Nublado\n#>   Alcance                           5525          403             84     449\n#>   Choque contra obstáculo fijo      3258          308             43     224\n#>   Colisión frontal                   711           42              5      48\n#>   Colisión fronto-lateral           6359          398             51     494\n#>   Colisión lateral                  3241          169             30     277\n#>   Colisión múltiple                 1619          173             29     111\ntabla <- table(datos_depu)\nchisq.test(tabla)\n#> \n#>  Pearson's Chi-squared test\n#> \n#> data:  tabla\n#> X-squared = 104, df = 15, p-value = 2e-15\nca_tabla <- ca(tabla, k = 2)\nca_tabla\n#> \n#>  Principal inertias (eigenvalues):\n#>            1        2        3      \n#> Value      0.003804 0.000493 2.7e-05\n#> Percentage 87.97%   11.4%    0.62%  \n#> \n#> \n#>  Rows:\n#>          Alcance Choque contra obstáculo fijo Colisión frontal\n#> Mass     0.26864                       0.1594          0.03351\n#> ChiDist  0.03200                       0.0817          0.06591\n#> Inertia  0.00028                       0.0011          0.00015\n#> Dim. 1  -0.16016                      -1.2818          0.72969\n#> Dim. 2   1.36554                      -0.9207         -1.84795\n#>         Colisión fronto-lateral Colisión lateral Colisión múltiple\n#> Mass                     0.3036          0.15455            0.0803\n#> ChiDist                  0.0446          0.07682            0.1284\n#> Inertia                  0.0006          0.00091            0.0013\n#> Dim. 1                   0.6586          1.22996           -2.0813\n#> Dim. 2                  -0.8221          0.52856            0.1210\n#> \n#> \n#>  Columns:\n#>         Despejado Lluvia débil LLuvia intensa Nublado\n#> Mass      0.86121       0.0621        0.01006 0.06665\n#> ChiDist   0.01344       0.2145        0.28954 0.08391\n#> Inertia   0.00016       0.0029        0.00084 0.00047\n#> Dim. 1    0.20552      -3.4619       -3.67116 1.12291\n#> Dim. 2   -0.19007      -0.8381        8.05783 2.02007\nplot(ca_tabla, map = \"symmetric\", what = c(\"all\", \"all\"), xlab = \"Proyección común de ambos factores\")"},{"path":"capNN.html","id":"capNN","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"Capítulo 36 Redes neuronales artificiales","text":"Noelia Vállez Enano\\(^{}\\) y José Luis Espinosa Aranda\\(^{}\\)\\(^{}\\)Universidad de Castilla-La Mancha","code":""},{"path":"capNN.html","id":"qué-es-el-deep-learning","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.1 ¿Qué es el deep learning?","text":"La inteligencia artificial es una disciplina científica que se ocupa de crear programas informáticos que ejecutan operaciones comparables las que realiza la mente humana, como el aprendizaje o el razonamiento lógico (Real Academia Española 2023). Entre otros ejemplos se pueden encontrar en la actualidad tanto robots que son capaces de realizar tareas de manera similar un humano en una fábrica, las denominadas como casas inteligentes o los vehículos autónomos.Dentro de las técnicas utilizadas para la inteligencia artificial, se encuentran las técnicas clásicas de machine learning, ya explicadas en capítulos anteriores de este libro, las cuales tienen la habilidad de aprender sin haber sido explícitamente programadas para una tarea en particular, pudiendo ser utilizadas para varios fines y aplicaciones.su vez, dentro de estos algoritmos, se pueden enmarcar como un subconjunto de las mismas las técnicas de deep learning, las cuales intentan simular tanto la arquitectura como el comportamiento del sistema nervioso humano, en particular, de las redes de neuronas que componen el encéfalo y que se encargan de realizar tareas específicas (Fig. 36.1). Para ello, estas técnicas se basan en el concepto de redes neuronales, que intentan emular la forma de aprendizaje de los humanos (Goodfellow, Bengio, Courville 2016).\nFigura 36.1: Inteligencia Artificial vs Machine learning vs Deep Learning\n","code":""},{"path":"capNN.html","id":"diferencias-entre-las-técnicas-de-machine-learning-tradicional-y-el-deep-learning","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.1.1 Diferencias entre las técnicas de machine learning tradicional y el deep learning","text":"Como se vio en el Cap. 2, la metodologías de ciencia de datos tienen una etapa llamada preparación de datos que incluye la tarea de elección de variables, la cual ha sido tratada ampliamente en el Cap. 9, para realizar una selección de las mejores características que representen el problema resolver, y que puedan ser comprendidas por el algoritmo de machine learning seleccionado de tal forma que sea capaz de solucionar el problema planteado.Por ejemplo, en el caso de querer detectar una cara dentro de una imagen, sería necesario definir qué tipo de características servirían para detectar la misma, como podrían ser, bajo nivel, determinados tipos de bordes de la imagen (Fig. 36.2). Estas características proporcionarían la base para detectar nivel medio elementos de la cara como ojos, narices, orejas, etc. y, definitivamente, alto nivel, reconocer donde hay una cara dentro de la imagen.\nFigura 36.2: Detección de bordes de una imagen mediante el método de Scharr\nEsta elección de características requiere en muchas ocasiones de la intervención humana, por lo que puede llevar mucho tiempo y diversos experimentos de prueba y error hasta poder encontrar una combinación de características que permita resolver el problema planteado.Las técnicas de deep learning, diferencia de las técnicas de machine learning tradicional, son capaces de aprender cuales son las mejores características que permitirán representar el problema que se quiere resolver sin necesidad de la interacción humana la misma vez que buscan la solución al mismo.Continuando con el ejemplo anterior de la detección de caras, mientras que en las técnicas de machine learning sería necesario introducirle al algoritmo qué características base componen una cara para que sea capaz de reconocerlas, al utilizar deep learning únicamente sería necesario mostrarle suficientes imágenes de caras para conseguir que el algoritmo sea capaz de aprender identificar una cara por sí mismo, identificando de forma automática las características más importantes de una cara.Esta capacidad de aprender las mejores características necesarias por sí mismo hace que nivel teórico las técnicas de deep learning puedan llegar ser más potentes que el machine learning clásico, pero debido la mayor complejidad del problema y, por consiguiente, al proceso de entrenamiento, también lleva que que sean necesarios muchos más datos y una mayor potencia de cómputo para entrenarlas.Este hecho explica que, aunque las bases de las técnicas de deep learning como el algoritmo del descenso del gradiente (Kiefer Wolfowitz 1952), el perceptrón (Rosenblatt 1958), los algoritmos de retropropagación y el perceptrón multicapa (Rumelhart, Hinton, Williams 1986) y la primera red neuronal convolucional (LeCun, Bengio, et al. 1995), datan de varios años atrás, sea hasta hace relativamente poco tiempo, cuando se ha podido empezar utilizar estas técnicas. Esto se debe diversos factores:La evolución en el hardware de procesamiento. En particular, debido la mejora de la capacidad de paralelismo masivo durante el cómputo que proporcionaron las nuevas tarjetas gráficas (GPU) al incorporar una gran cantidad de microprocesadores específicos, han podido ser utilizadas para las técnicas de deep learning. Originalmente su principal uso era representar modelos complejos 3D en los monitores, pero su utilización para técnicas de deep learning ha llevado recientemente al desarrollo de tarjetas específicas para este fin. Además, es posible disponer bajo demanda de estos recursos de computación como servicios través de Internet. Esto es lo que se conoce como cloud computing.La evolución en el hardware de procesamiento. En particular, debido la mejora de la capacidad de paralelismo masivo durante el cómputo que proporcionaron las nuevas tarjetas gráficas (GPU) al incorporar una gran cantidad de microprocesadores específicos, han podido ser utilizadas para las técnicas de deep learning. Originalmente su principal uso era representar modelos complejos 3D en los monitores, pero su utilización para técnicas de deep learning ha llevado recientemente al desarrollo de tarjetas específicas para este fin. Además, es posible disponer bajo demanda de estos recursos de computación como servicios través de Internet. Esto es lo que se conoce como cloud computing.El Big data. La gran cantidad de datos que se generan y almacenan en la actualidad, así como la mayor facilidad la hora de trabajar con esos conjuntos de datos (gracias las nuevas herramientas disponibles), han permitido cubrir la necesidad del gran volumen de datos iniciales necesarios.El Big data. La gran cantidad de datos que se generan y almacenan en la actualidad, así como la mayor facilidad la hora de trabajar con esos conjuntos de datos (gracias las nuevas herramientas disponibles), han permitido cubrir la necesidad del gran volumen de datos iniciales necesarios.La evolución del sofware. Recientemente ha habido un amplio interés tanto en buscar nuevos modelos para resolver todo tipo de problemas, como para mejorar las técnicas utilizadas para entrenar dichas redes neuronales. Esto ha llevado la creación y mejora de diversos frameworks, librerías y aplicaciones relacionadas con el entrenamiento y despliegue de redes neuronales. Entre ellos, serían destacables Keras, Tensorflow, Pytorch, Caffe2, Matlab y OpenVINO.La evolución del sofware. Recientemente ha habido un amplio interés tanto en buscar nuevos modelos para resolver todo tipo de problemas, como para mejorar las técnicas utilizadas para entrenar dichas redes neuronales. Esto ha llevado la creación y mejora de diversos frameworks, librerías y aplicaciones relacionadas con el entrenamiento y despliegue de redes neuronales. Entre ellos, serían destacables Keras, Tensorflow, Pytorch, Caffe2, Matlab y OpenVINO.","code":""},{"path":"capNN.html","id":"aplicaciones-del-deep-learning","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.2 Aplicaciones del deep learning","text":"Las posibles aplicaciones de las técnicas de deep learning son muy diversas y, gracias la continua investigación desarrollada en el área en la actualidad, hacen más que aumentar. continuación se comentan algunas de ellas:Clasificación de imágenes. Aunque la clasificación de imágenes dentro del área de la visión por computador o artificial es una realidad hace muchos años, es con las técnicas de deep learning con las que se han logrado los mayores avances, en particular, utilizando las redes neuronales convolucionales. Estas redes permiten determinar qué clase, perteneciente al conjunto de categorías utilizado para entrenar, se corresponde una determinada imagen.Clasificación de imágenes. Aunque la clasificación de imágenes dentro del área de la visión por computador o artificial es una realidad hace muchos años, es con las técnicas de deep learning con las que se han logrado los mayores avances, en particular, utilizando las redes neuronales convolucionales. Estas redes permiten determinar qué clase, perteneciente al conjunto de categorías utilizado para entrenar, se corresponde una determinada imagen.Detección de objetos. Permite localizar los objetos contenidos en una imagen mediante un rectángulo, clasificándolo su vez por su tipología. Por ejemplo, utilizando una cámara de seguridad instalada en una calle con este tipo de modelos sería posible localizar y diferenciar entre peatones y vehículos (Fig. 36.3).Detección de objetos. Permite localizar los objetos contenidos en una imagen mediante un rectángulo, clasificándolo su vez por su tipología. Por ejemplo, utilizando una cámara de seguridad instalada en una calle con este tipo de modelos sería posible localizar y diferenciar entre peatones y vehículos (Fig. 36.3).\nFigura 36.3: Detección de peatones y vehículos utilizando una cámara térmica y técnicas de deep learning\nSegmentación semántica/de instancias. De forma similar la detección de objetos, la segmentación semántica permite localizar objetos contenidos en una imagen, además de su tipología, pero en este caso se marcan utilizando una máscara nivel de píxel. La segmentación de instancias además es capaz de diferenciar entre diferentes instancias de una misma clase aun cuando se encuentren situadas de forma contigua.Segmentación semántica/de instancias. De forma similar la detección de objetos, la segmentación semántica permite localizar objetos contenidos en una imagen, además de su tipología, pero en este caso se marcan utilizando una máscara nivel de píxel. La segmentación de instancias además es capaz de diferenciar entre diferentes instancias de una misma clase aun cuando se encuentren situadas de forma contigua.Reconocimiento del habla. Permite un computador procesar y comprender el habla humana. En la actualidad existen varios asistentes inteligentes basados en esta tecnología que además son capaces de interpretar órdenes o instrucciones sencillas y actuar en consecuencia.Reconocimiento del habla. Permite un computador procesar y comprender el habla humana. En la actualidad existen varios asistentes inteligentes basados en esta tecnología que además son capaces de interpretar órdenes o instrucciones sencillas y actuar en consecuencia.Traducción automática. Consiste en utilizar las técnicas de deep learning para traducir un texto automáticamente de una lengua otra sin la necesidad de intervención humana. En la actualidad, se limita únicamente la traducción literal, palabra por palabra, del texto, si que también tiene en cuenta el significado que tendría en el idioma original para adaptarlo al idioma destino (Fig. 36.4).Traducción automática. Consiste en utilizar las técnicas de deep learning para traducir un texto automáticamente de una lengua otra sin la necesidad de intervención humana. En la actualidad, se limita únicamente la traducción literal, palabra por palabra, del texto, si que también tiene en cuenta el significado que tendría en el idioma original para adaptarlo al idioma destino (Fig. 36.4).\nFigura 36.4: Traductor automático basado en Deep Learning\nGeneración automática de imágenes/texto. Permite obtener desde una imagen un texto descriptivo que indique el contenido de la imagen, o al contrario, partir de un texto descriptivo generar una imagen basada en dicha descripción. Un ejemplo de este último caso sería Dall-E (Borji 2022) (Fig. 36.5).\nFigura 36.5: Algunas salidas posibles del generador de imágentes partir de texto Dall-E, para el texto \\(``\\)cat glasses studying computer vision space Earth background\\(\"\\)\nAutomóvil autónomo. Las técnicas de deep learning están siendo claves para el desarrollo del vehículo autónomo, capaz de circular sin la necesidad de la interacción de un conductor humano. Para lograr definitivamente un vehículo con estas características, es necesario que sea capaz de ver, tomar decisiones y conducir al mismo tiempo. Esto se consigue en la actualidad integrando la información de gran cantidad de sensores que obtienen datos en tiempo real sobre el entorno, como serían cámaras, LIDAR, radares o ultrasónicos entre otros, y que son procesados por varias redes neuronales con el fin de que sea capaz de tomar una decisión en cuestión de milisegundos (Fig. 36.3).Automóvil autónomo. Las técnicas de deep learning están siendo claves para el desarrollo del vehículo autónomo, capaz de circular sin la necesidad de la interacción de un conductor humano. Para lograr definitivamente un vehículo con estas características, es necesario que sea capaz de ver, tomar decisiones y conducir al mismo tiempo. Esto se consigue en la actualidad integrando la información de gran cantidad de sensores que obtienen datos en tiempo real sobre el entorno, como serían cámaras, LIDAR, radares o ultrasónicos entre otros, y que son procesados por varias redes neuronales con el fin de que sea capaz de tomar una decisión en cuestión de milisegundos (Fig. 36.3).Chatbots con inteligencia artificial. Son aplicaciones software que, utilizando la inteligencia artificial conversacional, son capaces de conversar mediante un chat escrito como si fueran un ser humano. Caben destacar los asistentes virtuales existentes en diversas páginas web y el reciente ChatGPT (OpenAI 2022), el cual es capaz de mantener conversaciones con el usuario, resolver problemas sencillos, generar textos y resúmenes sobre cualquier tema o generar código en diversos lenguajes de programación partir de una petición realizada mediante lenguaje natural.Chatbots con inteligencia artificial. Son aplicaciones software que, utilizando la inteligencia artificial conversacional, son capaces de conversar mediante un chat escrito como si fueran un ser humano. Caben destacar los asistentes virtuales existentes en diversas páginas web y el reciente ChatGPT (OpenAI 2022), el cual es capaz de mantener conversaciones con el usuario, resolver problemas sencillos, generar textos y resúmenes sobre cualquier tema o generar código en diversos lenguajes de programación partir de una petición realizada mediante lenguaje natural.","code":""},{"path":"capNN.html","id":"redes-neuronales","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.3 Redes neuronales","text":"Las redes neuronales artificiales (en inglés Artificial Neural Network (ANN)) tienen su origen en la definición de neurona artificial de (McCulloch Pitts 1943) y en el diseño del perceptrón por parte de Frank Rosenblatt (Rosenblatt 1958). Cada ANN está formada por un conjunto de elementos conocidos como ``neuronas” cuya organización está inspirada en la que siguen las redes neuronales de los seres vivos. Entre dos neuronas adyacentes existe una serie de conexiones través de las cuales se envía la información como si de pulsos eléctricos se tratase. De forma aislada, cada neurona procesa la información recibida para producir un resultado que será utilizado por las siguientes neuronas con las que está conectada.Cada ANN tiene como objetivo resolver una tarea concreta. Por ejemplo, una ANN podría estar diseñada para reconocer un dígito o una letra partir de una imagen. Para conseguir resolver dicha tarea, la red sigue un proceso de aprendizaje automático. Este proceso se conoce como ``entrenamiento” y requiere que se disponga de un conjunto de datos representativos de la tarea resolver.","code":""},{"path":"capNN.html","id":"perceptrón-o-neurona","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.4 Perceptrón o neurona","text":"El elemento básico de toda ANN es la neurona artificial, inspirada en la neuronas biológicas. Cada neurona tiene una serie de entradas y produce una única salida. Las entradas pueden ser variables extraídas de la tarea que se debe resolver o salidas de otras neuronas de la red.Para calcular la salida, cada neurona realiza una suma ponderada de sus entradas utilizando una serie de pesos, \\(\\boldsymbol w\\) donde \\(w_i\\\\mathbb{R}\\), y añade un término constante,\\(w_0\\\\mathbb{R}\\). Por tanto, cada neurona actúa como un clasificador lineal que puede separar dos conjuntos diferentes dependiendo de si la salida es positiva o negativa (Figura 36.6).\nFigura 36.6: Estructura del perceptrón o neurona\nPara cada vector de entrada, \\(\\boldsymbol x\\), la neurona aplicará los pesos, \\(\\boldsymbol w\\), como el producto escalar de ambos vectores:\\[\\begin{equation}\n\\boldsymbol w^{\\prime}    \\boldsymbol x  = w_0\\cdot 1+w_1 \\cdot x_1+w_2 \\cdot x_2+\\dots+w_n \\cdot x_n .\n\\end{equation}\\]Una vez obtenida la suma ponderada, típicamente se puede separar las entradas en dos conjuntos, obteniéndose como salida final un valor binario, siguiendo la fórmula:\\[\\begin{equation}\nf (\\boldsymbol w^{\\prime}   \\boldsymbol x) = \\begin{cases}\n1 & \\text{si $\\boldsymbol w^{\\prime}   \\boldsymbol x>0$}\\\\\n0 & \\text{en otro caso}\n\\end{cases} .\n\\end{equation}\\]","code":""},{"path":"capNN.html","id":"aprendizaje","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.4.1 Aprendizaje","text":"Durante el proceso de aprendizaje, el perceptrón busca el ajuste automático de los valores de los pesos. Éstos deben seleccionarse de forma que minimicen el error de clasificación cometido sobre un conjunto de entrenamiento. El conjunto de entrenamiento estará compuesto por un conjunto de muestras del que se conoce su clase:\\[\\begin{equation}\nD = \\{ (\\boldsymbol x_1 , y_1 ), (\\boldsymbol x_2 , y_2 ), \\dots, (\\boldsymbol x_m , y_m ) \\},\n\\end{equation}\\]donde cada muestra, \\(\\boldsymbol x_i = (x_{i1},x_{i2},\\dots,x_{})\\), pertenece una de las dos clases, \\(y_i = \\{ 0,1 \\}\\) .El primer paso del aprendizaje o entrenamiento consiste en la inicialización de cada peso \\(w_j\\) 0 o algún otro valor aleatorio.Tras ello, se calcula la clase estimada, \\(\\hat y\\), en un momento determinado, \\(t\\), para cada muestra \\(\\boldsymbol x_i\\) del conjunto de datos:\n\\[\\begin{equation}\n\\hat y_i(t) = f(\\boldsymbol w(t)^{T}  \\boldsymbol x_i) = f(w_0(t) + w_1(t) \\cdot x_{i1} + \\dots + w_n(t) \\cdot x_{}) .\n\\end{equation}\\]Tras obtener la salida para todas las muestras de entrenamiento, cada uno de los pesos, \\(w_j\\), de la neurona se actualiza siguiendo la fórmula:\n\\[\\begin{equation}\nw_j(t+1) = w_j(t) + \\lambda \\cdot |y_i-\\hat y_i(t)|\\cdot x_{ij} .\n\\end{equation}\\]donde \\(|y_i-\\hat y_i(t)|\\) será 0 cuando la clase predicha coincida con la clase real de la muestra y \\(\\lambda\\) es la tasa de aprendizaje. La tasa de aprendizaje debe seleccionarse de antemano y controla la variación de los pesos entre iteraciones. En algunos casos el valor de \\(\\lambda\\) es 0 o varía durante el proceso de entrenamiento.Los dos pasos anteriores se repiten hasta que el error de clasificación es menor que un cierto umbral o el número de iteraciones alcanza un cierto valor fijado. Normalmente se suele utilizar el número de iteraciones como criterio de paro puesto que siempre es posible alcanzar una tasa de error más baja que la deseada.","code":""},{"path":"capNN.html","id":"convergencia","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.4.2 Convergencia","text":"El teorema de la convergencia del perceptrón dice que, en los problemas en los que haya dos clases linealmente separables, es siempre posible encontrar unos pesos que realicen la separación en un número finito de iteraciones (Novikoff 1962). Sin embargo, en la mayoría de los casos, se está ante problemas linealmente separables, esto es, es posible obtener un conjunto de variables que separen perfectamente las muestras de ambas clases. Por ello, es necesario el uso de ciertas estrategias que solucionen el problema de convergencia en estos casos. Algunas de las estrategias más utilizadas son:Algoritmo Pocket: Guarda la mejor solución obtenida hasta el final del entrenamiento.Algoritmo Pocket: Guarda la mejor solución obtenida hasta el final del entrenamiento.Algoritmo Maxover: Halla el margen de separación máximo permitiendo clasificaciones incorrectas.Algoritmo Maxover: Halla el margen de separación máximo permitiendo clasificaciones incorrectas.Algoritmo de Voto: Se utilizan múltiples perceptrones combinando sus salidas.Algoritmo de Voto: Se utilizan múltiples perceptrones combinando sus salidas.","code":""},{"path":"capNN.html","id":"perceptrón-multiclase","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.5 Perceptrón multiclase","text":"Una extensión lógica del uso del perceptrón es su empleo en la resolución de tareas de clasificación donde existan más de dos clases (Haykin 1999). En ese caso se tendrá un conjunto de entrenamiento, \\(D\\), de \\(m\\) muestras:\\[\\begin{equation}\nD = \\{ (\\boldsymbol x_1 , y_1 ), (\\boldsymbol x_2 , y_2 ), \\dots, (\\boldsymbol x_m , y_m ) \\},\n\\end{equation}\\]donde cada muestra \\(\\boldsymbol x_i = (x_{i1},x_{i2},\\dots,x_{})\\) pertenezca una de las \\(c\\) clases posibles:\\[\\begin{equation}\ny_i = \\{ 0,1,\\dots,c-1 \\} .\n\\end{equation}\\]diferencia del problema binario, en su versión multiclase lo que se definen son varios modelos, \\(F\\), uno para cada una de las \\(c\\) clases:\\[\\begin{equation}\nF=\\{f_0,f_1,\\dots,f_{c-1}\\}\\\\\nf_j: \\mathbb{R}^n \\rightarrow \\mathbb{R} .\n\\end{equation}\\]En este caso la salida se selecciona en función de si el valor obtenido es positivo o negativo, sino que se asigna la clase del modelo que obtenga el valor más alto tras aplicar los pesos la muestra. Esta estrategia recibe el nombre de ``uno contra todos”:\\[\\begin{equation}\n\\hat y_i = argmax_j(f_j(\\boldsymbol x_i))\\\\\nj\\\\{0,1,\\dots ,c-1\\} .\n\\end{equation}\\]En muchas ocasiones lo que se obtiene es un único valor con la clase asignada como salida, sino que se obtiene un vector con las salidas binarias de cada uno de los modelos empleados. En ese caso, el vector contendrá un 1 en la posición de la clase asignada y un 0 en el resto de clases. Por ejemplo, el vector \\([0,1,0,0,0]\\) representaría que una muestra ha sido asignada la segunda clase en un problema de clasificación donde existen 5 clases posibles:\\[\\begin{equation}\n[(f_1(\\boldsymbol x_i)),(f_2(\\boldsymbol x_i)),\\dots,(f_c(\\boldsymbol x_i))] .\n\\end{equation}\\]","code":""},{"path":"capNN.html","id":"funciones-de-activación","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.6 Funciones de activación","text":"Además de los pesos, toda neurona tiene asociada una función de activación. Esta función se encarga de transformar la suma ponderada de las entradas en el resultado final. En las secciones anteriores se ha utilizado una función de activación con umbral 0, pero existen muchas otras. Algunas de las más utilizadas se enumeran continuación.Para algunas de ellas, se ha implementado una función, plot_activation_function(), que permite dibujarlas en R, y que se puede ver continuación:Función lineal. Se trata de una función identidad donde la salida tiene el mismo valor que la entrada. Normalmente se aplica en problemas de regresión lineal. Por ejemplo, si se quiere predecir el número de días que lloverá en un mes determinado.\\[\\begin{equation}\nf(x)=x\n\\end{equation}\\]   Y se representa gráficamente de la siguiente forma:Función umbral. Esta función recibe también el nombre de función escalón. Si el valor de entrada es menor que el umbral la salida será 0. En caso contrario, la salida será 1. Si el umbral es 0, la función se reduce mirar el signo del valor analizado.\\[\\begin{equation}\nf(x)=\\begin{cases}\n0 & \\text{si $x<u$}\\\\\n1 & \\text{en otro caso}\n\\end{cases}\n\\end{equation}\\]   Se representa gráficamente mediante el siguiente código, el cual se corresponde con una modificación de la función plot_activation_function, ya que la versión original mostraría de forma correcta la gráfica al requerir representar dos valores en la posición 0, el valor 0 y el valor 1 del escalón:Función sigmoide. También conocida como función logística, se trata de una de las funciones más utilizadas para asignar una clase. Si el punto de evaluación de la función es un valor negativo muy bajo, la función dará como resultado un valor muy cercano 0, si se evalúa en 0, el resultado es 0,5 y si se evalúa en un valor positivo alto el resultado será aproximadamente 1.\\[\\begin{equation}\nf(x)=\\frac{1}{1-e^{-x}}\n\\end{equation}\\]   Representándose gráficamente de la siguiente forma:\n- Función tangente hiperbólica. El rango de valores de salida es [-1, 1], donde los valores altos tienden de manera asintótica 1 y los valores muy bajos tienden de manera asintótica -1 de forma similar la sigmoide.\\[\\begin{equation}\nf(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}\n\\end{equation}\\]   Siendo su representación gráfica de la siguiente forma:\n- Función ReLU. Se trata de la unidad lineal rectificada (del inglés Rectified Linear Unit). Es posiblemente la función de activación más utilizada actualmente en deep learning (Nair Hinton 2010).\\[\\begin{equation}\nf(x)=\\begin{cases}\n0 & \\text{si $x\\leq 0$}\\\\\nx & \\text{en otro caso}\n\\end{cases}\n\\end{equation}\\]   Y se representaría gráficamente de la siguiente manera:","code":"\nrequire(ggplot2)\nplot_activation_function <- function(f, title, range){\n  ggplot(data.frame(x=range), mapping=aes(x=x)) +\n    geom_hline(yintercept=0, color='black', alpha=3/4) +\n    geom_vline(xintercept=0, color='black', alpha=3/4) +\n    stat_function(fun=f, colour = \"red\") +\n    ggtitle(title) +\n    scale_x_continuous(name='x') +\n    scale_y_continuous(name='f(x)') +\n    theme(plot.title = element_text(hjust = 0.5))\n}\nf <- function(x){ x }\nplot_activation_function(f, 'Lineal', c(-4,4))\ndf <- data.frame(x=c(-4, -3, -2, -1, 0, 1, 2, 3, 4), f=c(0,0,0,0,1,1,1,1,1))\nggplot(data=df, aes(x=x, y=f, group=1)) +\n    theme(plot.title = element_text(hjust = 0.5)) +\n    ggtitle(\"Umbral\")+\n    scale_y_continuous(name='f(x)')+\n    geom_hline(yintercept=0, color='black', alpha=3/4) +\n    geom_vline(xintercept=0, color='black', alpha=3/4) +\n    geom_step(color='red')\nf <- function(x){1 / (1 + exp(-x))}\nplot_activation_function(f, 'Sigmoide', c(-4,4))\ntanh_func <- function(x){tanh(x)}\nplot_activation_function(tanh_func, 'Tangente Hiperbólica', c(-4,4))\nrec_lu_func <- function(x){ ifelse(x < 0 , 0, x )}\nplot_activation_function(rec_lu_func, 'ReLU', c(-4,4))"},{"path":"capNN.html","id":"perceptrón-multicapa","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.7 Perceptrón multicapa","text":"Aunque el perceptrón puede aprender muchos tipos de lógica, es posible que aprenda la operación XOR (exclusivo) que se diferencia del en que asigna un 1 la salida cuando las dos entradas son distintas (Minsky Papert 1969). El perceptrón multicapa o, en inglés, Multilayer Perceptron (MLP) surge para dar una solución este problema que es un paradigma de los problemas linealmente separables, que realmente son la mayoría en el mundo real.Un MLP está compuesto por varias capas con neuronas. La primera capa será la de entrada, que recibirá las variables que representan los elementos del problema resolver. Por otro lado, la última capa representará las clases de salida (en las que hay que clasificar las entradas), esto es, la salida del MLP. Entre ambas capas existirán una o más capas ``ocultas”. Las neuronas de una capa intermedia tienen como entrada la salida de la capa anterior y su salida es la entrada de las neuronas de la siguiente capa (Figura 36.7). Este tipo de capas también son llamadas densas o totalmente conectadas.\nFigura 36.7: Estructura del perceptrón multicapa (MLP)\n","code":""},{"path":"capNN.html","id":"aprendizaje-1","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.7.1 Aprendizaje","text":"El MLP entra en la categoría de los algoritmos de propagación hacia adelante o feedforward ya que las entradas de las neuronas de una capa se combinan mediante la suma ponderada, pasan por una función de activación y el resultado es propagado las neuronas de la capa siguiente. Este proceso se lleva cabo desde la capa de entrada hasta la capa de salida.Dado un conjunto de muestras de entrenamiento \\(\\{(\\boldsymbol x_1, y_1), (\\boldsymbol x_2, y_2), \\ldots, (\\boldsymbol x_n, y_n)\\}\\) donde cada \\(\\boldsymbol x_i \\\\mathbb{R}^d\\) e \\(y_i \\\\{0, 1\\}\\) (siendo \\(d\\) el número de características), la salida de la primera capa, \\(\\boldsymbol z_1\\), para una entrada \\(\\boldsymbol x\\) vendrá dada por la expresión:\\[\\begin{equation}\n\\boldsymbol z_1 = \\boldsymbol W_{(1)}^{\\prime}   \\boldsymbol x + \\boldsymbol b_1 ,\n\\end{equation}\\]donde \\(\\boldsymbol b_1 \\\\mathbb{R}^{h}\\) es un vector con las constantes de la primera capa, siendo \\(h\\) el número de varibles de cada capa, y \\(\\boldsymbol{W}_{(1)} \\\\mathbb{R}^{h \\times d}\\) son los pesos de la capa. Tras aplicar la función de activación, \\(g(\\cdot)\\), al vector intermedio, \\(\\boldsymbol{z}\\\\mathbb{R}^h\\), se obtiene:\\[\\begin{equation}\n\\boldsymbol{h_1}= g(\\mathbf{z_1}) .\n\\end{equation}\\]La salida de una capa intermedia, \\(\\boldsymbol{h_i}\\\\mathbb{R}^h\\), también está formada por variables intermedias que sirven de entrada la siguiente capa. La función calcular en la siguiente capa será por tanto:\\[\\begin{equation}\n\\boldsymbol h_2 = g ( \\boldsymbol W_{(2)}^{\\prime}   \\boldsymbol h_1 + \\boldsymbol b_2) .\n\\end{equation}\\]Siguiendo el mismo razonamiento, la salida de la última capa, \\(\\hat y\\), y por tanto de la red, vendrá dada por:\\[\\begin{equation}\n\\hat y = g ( \\boldsymbol W_{(n)}^{\\prime}   \\boldsymbol h_{n-1} + \\boldsymbol b_n ) .\n\\end{equation}\\]Por ejemplo, si se tiene una red de tres capas la salida podrá calcularse como:\\[\\begin{equation}\n\\hat y = g ( \\boldsymbol W_{(3)}^{\\prime}   \\boldsymbol g ( \\boldsymbol W_{(2)}^{\\prime}   \\boldsymbol g ( \\boldsymbol W_{(1)}^{\\prime}   \\boldsymbol x + \\boldsymbol b_1)+ \\boldsymbol b_2 )+ \\boldsymbol b_3  ) .\n\\end{equation}\\]Para entrenar y ajustar los pesos de este tipo de redes es necesario realizar el ajuste de la combinación de todos los pesos de la red. De forma similar la búsqueda de los pesos de una sola neurona, será necesario encontrar la combinación de valores que clasifiquen bien todas las muestras del conjunto de entrenamiento o, en su defecto, que fallen en el menor número de muestras posible o minimicen alguna otra función de coste. En este punto es donde entra en juego la propagación hacia atrás o backpropagation.La propagación hacia atrás es el mecanismo por el que el MLP ajusta de forma iterativa los pesos de la red con el objetivo de minimizar una función de coste que mide lo bueno o malo que es el resultado obtenido en un momento determinado (Rumelhart, Hinton, Williams 1986). Su único requisito de aplicación es que todas las operaciones de la red (incluidas las funciones de activación) sean diferenciables ya que se utiliza el algoritmo del descenso del gradiente para optimizar la función de coste.El MLP utiliza diferentes funciones de coste o pérdida según el tipo de problema resolver. Para los problemas de clasificación, la función de coste más utilizada es la Entropía Cruzada Media (en inglés Average Cross-Entropy). Para un problema binario esta función de coste se calcula como;\\[\\begin{equation}\nC(\\hat{y},y,\\boldsymbol W) = -\\dfrac{1}{n}\\sum_{=0}^n(y_i \\ln {\\hat{y_i}} + (1-y_i) \\ln{(1-\\hat{y_i})}) + \\dfrac{\\alpha}{2n} ||\\boldsymbol W||_2^2 ,\n\\end{equation}\\]donde \\(\\alpha ||W||_2^2\\) con \\(\\alpha > 0\\) es un término de regularización, L2, también conocido como penalización ya que penaliza los modelos complejos. \\(\\alpha\\) es un hiperparámetro cuyo valor se establece manualmente.Para los problemas de regresión, la función de coste se basa en el Error Cuadrático Medio (Mean Squared Error):\\[\\begin{equation}\nC(\\hat{y},y,\\boldsymbol W) = \\frac{1}{2n}\\sum_{=0}^n||\\hat{y}_i - y_i ||_2^2 + \\frac{\\alpha}{2n} ||\\boldsymbol W||_2^2 .\n\\end{equation}\\]Cada iteración en el proceso de aprendizaje estará compuesta entonces por dos etapas, una de propagación hacia adelante y otra de propagación hacia atrás. En la primera etapa se introducen los valores de entrada la red y se propagan las operaciones y los resultados hasta obtener la salida final de la red. En la segunda, el gradiente de la función de coste es propagado hacia atrás para actualizar los valores de los pesos de todas las capas y acercarse más los valores que minimizan la función de coste.En el algoritmo del descenso del gradiente, \\(\\nabla C_{\\boldsymbol W}\\) se calcula y deduce de \\(\\boldsymbol W\\).\nFormalmente esto puede expresarse como:\\[\\begin{equation}\n\\boldsymbol W^{t+1} = \\boldsymbol W^{\\prime}   - \\lambda \\nabla {C}_{\\boldsymbol W}^{t} ,\n\\end{equation}\\]donde \\(t\\) es el estado de la red en una iteración determinada y \\(\\lambda\\) es la tasa de aprendizaje cuyo valor debe ser superior 0.Al igual que en el caso del perceptrón único, el entrenamiento terminará cuando se alcance un número máximo de iteraciones o la mejora en la función de coste entre dos iteraciones consecutivas supere cierto umbral.Durante el proceso de aprendizaje, es necesario guardar en memoria los resultados de cada una de las muestras del conjunto de entrenamiento. Si el número de muestras o el tamaño de la red son grandes, es posible que se disponga del suficiente espacio. Para resolver este problema, en una iteración se utiliza todo el conjunto de entrenamiento, sino que se utiliza un subconjunto del mismo llamado batch. El conjunto de entrenamiento se divide en cada iteración, por tanto, en un número de batches disjuntos con un número de muestras por batch. Atendiendo esta división, es posible definir una serie de hiperparámetros:Tamaño del batch. Número de muestras utilizadas en cada iteración para actualizar los pesos.Número de épocas. Número de pasadas completas sobre el conjunto de entrenamiento hasta terminar el proceso de aprendizaje.Número de iteraciones por época. Será el resultado de dividir el número total de muestras por el tamaño del batch.Por ejemplo, si se tiene un conjunto de 55000 muestras y el tamaño del batch es de 100, cada época tendrá 550 iteraciones.","code":""},{"path":"capNN.html","id":"instalación-de-librerías-de-deep-learning-en-r-tensorflowkeras","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.8 Instalación de librerías de deep learning en R: Tensorflow/Keras","text":"El framework que se va utilizar en este libro para trabajar con técnicas de deep learning será Tensorflow/Keras, debido que es uno de los más completos en la actualidad, permitiendo realizar una configuración completa del proceso de entrenamiento y trabajar con diversos tipos de redes neuronales.Para poder utilizar Tensorflow/Keras en R, es necesario realizar la instalación de la librería fuera de R. Por ello, si ya se dispone de una instalación del mismo sería posible utilizarla. obstante, se recomienda seguir los pasos indicados continuación para tener una instalación nativa de Tensorflow/Keras asociada directamente R.Paso 1 - Librería de Tensorflow en REl primer paso será instalar el paquete de tensorflow en R [].continuación, será necesario tener una instalación de Conda en el sistema. Los usuarios tanto de Windows como de Linux/Mac podrán realizar directamente la instalación de una versión de Conda denominada Mini-Conda en el instalador del siguiente paso, la cual sería la opción recomendada para tener que realizar una instalación externa de manera adicional.NOTAOtra manera disponible para los usuarios de Windows, pero recomendada por los autores de este libro salvo que ya se disponga de Anaconda instalado, sería la de utilizar el programa y la librería directamente dentro de Anaconda, instalando una versión de R directamente en el sistema través del siguiente link:https://docs.anaconda.com/anaconda/install/windows/Paso 2 - Instalación de tensorflow y kerasPara continuar la instalación se activará la librería de Tensorflow y se ejecutará la función install_tensorflowAl ejecutar esta función, los usuarios deberán marcar “Y” para aceptar la instalación de Mini-Conda, descartando aceptar la utilización de cualquier otro sistema Conda que pueda estar instalado previamente.También se puede ejecutar la función install_keras del paquete keras para instalar Tensorflow.Paso 3 - Confirmar la instalaciónPara confirmar la instalación, se puede comprobar con los siguientes comandos (la salida puede variar según el equipo, pero la línea final tiene que ser similar la indicada):","code":"\ninstall.packages(\"tensorflow\")\nlibrary(tensorflow)\ninstall_tensorflow()\ninstall.packages(\"keras\")\nlibrary(keras)\ninstall_keras()\nlibrary(tensorflow)\ntf$constant(\"Hellow Tensorflow\")tf.Tensor(b'Hellow Tensorflow', shape=(), dtype=string)"},{"path":"capNN.html","id":"ejemplo-de-red-para-clasificación-en-r","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.9 Ejemplo de red para clasificación en R","text":"En esta sección se entrena una red neuronal artificial para reconocer o clasificar los dígitos manuscritos del conjunto de datos MNIST (https://en.wikipedia.org/wiki/MNIST_database). Cada una de las imágenes de este conjunto de datos tiene un tamaño de \\(28\\times28\\) píxeles en escala de grises. En vez de extraer una serie de variables partir de cada imagen, en este caso se utilizan cada uno de los \\(28\\times28=784\\) píxeles como variable de entrada (Figura 36.8).\nFigura 36.8: MLP para reconocimiento de dígitos manuscritos\n","code":""},{"path":"capNN.html","id":"carga-y-visualización-de-los-datos","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.9.1 Carga y visualización de los datos","text":"El primer paso será cargar la librería keras que permite crear redes neuronales y conjunto de imágenes que se encuentra disponible públicamente:continuación, se puede ver el contenido de las variables generadas, donde cabe destacar que el conjunto de datos MNIST ya viene separado en dos subconjuntos, uno para entrenamiento y otro para test, compuestos por 60000 y 10000 imágenes respectivamente. En ambos casos, estos datos se almacenan en la variable de nombre x.Además, las imágenes de cada subconjunto vienen acompañadas de la clase la que pertenecen (dígito contenido en la imagen). En ambos casos, esta etiqueta se almacena en la variable con nombre y. continuación se muestra un pequeño ejemplo que permitirá visualizar alguna de las imágenes contenidas en el conjunto de datos de entrenamiento junto con la etiqueta representando el dígito contenido:\nFigura 36.9: Algunas imágenes del conjunto de entrenamiento\n","code":"\nlibrary(keras)\nmnist <- dataset_mnist()\nnames(mnist)\n#> [1] \"train\" \"test\"\ndim(mnist$train$x)\n#> [1] 60000    28    28\ndim(mnist$train$y)\n#> [1] 60000\ndim(mnist$test$x)\n#> [1] 10000    28    28\ndim(mnist$test$y)\n#> [1] 10000\npar(mfcol=c(4, 4))\npar(mar=c(0, 0, 3, 0), xaxs='i', yaxs='i')\nfor (j in 1:16) {\n    im <- mnist$train$x[j, , ]\n    im <- t(apply(im, 2, rev))\n    image(x=1:28, y=1:28, z=im, col=gray((0:255)/255),\n          xaxt='n', main=paste(mnist$train$y[j]))\n}"},{"path":"capNN.html","id":"preprocesamiento","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.9.2 Preprocesamiento","text":"Una vez cargados los datos y comprobado su contenido, es posible realizar algún tipo de preprocesado. Dependiendo del tipo de problema se podrán realizar unas operaciones u otras. Por ejemplo, cuando se trabaja con imágenes es muy típico estandarizar los valores de color de las imágenes para mitigar las diferencias producidas por las diferentes condiciones de iluminación.En este caso, solo se va transformar los valores originales de la imagen (en rango de 0 255) valores entre 0 y 1 dividiendo cada valor por el máximo, 255:","code":"\nmnist$train$x <- mnist$train$x/255\nmnist$test$x <- mnist$test$x/255"},{"path":"capNN.html","id":"nngen","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.9.3 Generación de la red neuronal","text":"El siguiente paso consiste en la generación de la red neuronal. Para ello, se define primero la estructura utilizando la interfaz sequential proporcionada por Tensorflow/Keras través de la función keras_model_sequential:Como se puede observar, la red definida está compuesta por una capa de tipo flatten que se encarga de transformar los 28x28 valores un vector de 784 elementos, para que continuación una capa oculta dense de 15 neuronas con activación relu se encargue de realizar las primeras operaciones con esos datos. Al final, una última capa dense se encarga de obtener la probabilidad de que la imagen represente cada una de las posibles clases mediante una activación softmax211:Finalmente, es necesario compilar el modelo, indicando algunos de los parámetros de configuración necesarios para el proceso de entrenamiento, como la función de coste o pérdida, el optimizador utilizar y las métricas obtener:","code":"\nmodel <- keras_model_sequential() |>\n  layer_flatten(input_shape = c(28, 28)) |>\n  layer_dense(units = 15, activation = \"relu\") |>\n  layer_dense(10, activation = \"softmax\")\nsummary(model, line_length=64)\n#> Model: \"sequential\"\n#> ____________________________________________________________________\n#>  Layer (type)              Output Shape               Param #    \n#> ====================================================================\n#>  flatten (Flatten)         (None, 784)                0          \n#>  dense_1 (Dense)           (None, 15)                 11775      \n#>  dense (Dense)             (None, 10)                 160        \n#> ====================================================================\n#> Total params: 11,935\n#> Trainable params: 11,935\n#> Non-trainable params: 0\n#> ____________________________________________________________________\nmodel |>\n  compile(\n    loss = \"sparse_categorical_crossentropy\", # función utilizada para problemas de clasificación con varias clases\n    optimizer = \"sgd\", # stochastic gradient descent\n    metrics = \"accuracy\" # Precisión\n  )"},{"path":"capNN.html","id":"nntrain","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.9.4 Entrenamiento","text":"Una vez generada la estructura de la red neuronal y definida la anterior configuración, es posible entrenarla mediante la función fit(). Para ello, se le debe indicar el conjunto de imágenes de entrenamiento, x, que debe utilizar y sus clases correspondientes, y. Además de otros parámetros, se podrá configurar el número de épocas, epochs, entrenar (pasadas sobre el conjunto completo de entrenamiento), el tamaño del batch que se utilizará en cada iteración con batch_size (número de imágenes por iteración), qué porcentaje de elementos del conjunto de datos se utilizarán para validar el modelo con validation_split (imágenes utilizadas durante el entrenamiento pero solo para obtener una estimación real del error cometido) o la tasa de aprendizaje, learning_rate.Tras el entrenamiento es posible ver su evolución mediante las gráficas de coste/pérdida y precisión:\nFigura 36.10: Evolución durante el entrenamiento de la función de precisión y de coste/pérdida de los conjuntos de entrenamiento y validación\nComo se puede observar, la red entrenada tiene alrededor de un 90% de precisión (porcentaje de aciertos al clasificar las imágenes) para las imágenes en los conjuntos de entrenamiento y validación. En el caso de la función de pérdida o coste, que mide el error cometido al realizar la clasificación, podemos ver como se reduce conforme la precisión del modelo aumenta.","code":"\ntraining_evolution <- model |>\n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    epochs = 10, batch_size = 128,\n    validation_split = 0.2,\n    learning_rate = 0.1,\n    verbose = 2\n  )\n#> Epoch 1/10\n#> 375/375 - 2s - loss: 1.6313 - accuracy: 0.5266 - val_loss: 1.0455 - val_accuracy: 0.7510 - 2s/epoch - 6ms/step\n#> Epoch 2/10\n#> 375/375 - 1s - loss: 0.8433 - accuracy: 0.7881 - val_loss: 0.6409 - val_accuracy: 0.8434 - 1s/epoch - 3ms/step\n#> Epoch 3/10\n#> 375/375 - 1s - loss: 0.6022 - accuracy: 0.8427 - val_loss: 0.5031 - val_accuracy: 0.8712 - 1s/epoch - 3ms/step\n#> Epoch 4/10\n#> 375/375 - 1s - loss: 0.5047 - accuracy: 0.8656 - val_loss: 0.4381 - val_accuracy: 0.8830 - 1s/epoch - 3ms/step\n#> Epoch 5/10\n#> 375/375 - 1s - loss: 0.4526 - accuracy: 0.8767 - val_loss: 0.4019 - val_accuracy: 0.8909 - 1s/epoch - 3ms/step\n#> Epoch 6/10\n#> 375/375 - 1s - loss: 0.4201 - accuracy: 0.8854 - val_loss: 0.3764 - val_accuracy: 0.8959 - 1s/epoch - 3ms/step\n#> Epoch 7/10\n#> 375/375 - 1s - loss: 0.3976 - accuracy: 0.8896 - val_loss: 0.3593 - val_accuracy: 0.8996 - 1s/epoch - 3ms/step\n#> Epoch 8/10\n#> 375/375 - 1s - loss: 0.3809 - accuracy: 0.8939 - val_loss: 0.3463 - val_accuracy: 0.9022 - 1s/epoch - 3ms/step\n#> Epoch 9/10\n#> 375/375 - 1s - loss: 0.3678 - accuracy: 0.8975 - val_loss: 0.3359 - val_accuracy: 0.9050 - 1s/epoch - 3ms/step\n#> Epoch 10/10\n#> 375/375 - 1s - loss: 0.3571 - accuracy: 0.8997 - val_loss: 0.3289 - val_accuracy: 0.9064 - 1s/epoch - 3ms/step\nplot(training_evolution)"},{"path":"capNN.html","id":"test","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.9.5 Test","text":"Una vez entrenado el modelo, es posible aplicarlo sobre el conjunto de test. Para ello, se puede realizar la predicción sobre cualquiera de las imágenes mediante la función predict, obteniendo la probabilidad de que pertenezca una determinada clase:También se puede utilizar la función evaluate para calcular tanto el coste o pérdida como la precisión de la red neuronal sobre el conjunto de test. Como se puede observar, se obtienen valores muy similares los obtenidos durante el entrenamiento:Con la función predict se puede también generar la matriz de confusión de la red para evaluar aciertos y fallos para cada clase:En la diagonal principal podemos observar el número de aciertos que obtiene el modelo entrenado para el conjunto de test, mientras que el resto de valores indican en cuantas ocasiones una clase es clasificada de manera incorrecta como otra diferente. partir de esta matriz de confusión se puede calcular el valor de accuracy calculado mediante la función evaluate previa.","code":"\npredictions <- predict(model, mnist$test$x)\nhead(round(predictions, digits=3), 5)#>       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\n#> [1,] 0.000 0.000 0.000 0.003 0.000 0.000 0.000 0.995 0.000 0.002\n#> [2,] 0.009 0.000 0.836 0.024 0.000 0.009 0.119 0.000 0.003 0.000\n#> [3,] 0.000 0.962 0.013 0.006 0.001 0.001 0.003 0.002 0.010 0.002\n#> [4,] 0.999 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n#> [5,] 0.001 0.000 0.007 0.000 0.836 0.004 0.011 0.012 0.017 0.111\nmodel |>\n  evaluate(mnist$test$x, mnist$test$y, verbose = 0)#>      loss  accuracy \n#> 0.3310305 0.9045000\nprediction_matrix <- model |> predict(mnist$test$x) |> k_argmax()\nconfusion_matrix <- table(as.array(prediction_matrix), mnist$test$y)\nconfusion_matrix#>    \n#>        0    1    2    3    4    5    6    7    8    9\n#>   0  953    0   11    4    2   16   16    3    8    7\n#>   1    0 1108   10    2    6    1    3   21   10    5\n#>   2    4    3  901   27    5   11   14   27   13    6\n#>   3    2    2   16  903    0   46    1    4   29   10\n#>   4    1    0   16    0  899   16   12    9   11   43\n#>   5    6    1    1   29    1  726    8    1   24   13\n#>   6    9    4   19    3   10   21  902    0   10    0\n#>   7    2    2   12   17    2   10    0  916   11   18\n#>   8    3   15   35   20   10   38    2    3  839    9\n#>   9    0    0   11    5   47    7    0   44   19  898"},{"path":"capNN.html","id":"guardado-y-reutilización-del-modelo","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.9.6 Guardado y reutilización del modelo","text":"Finalmente, es posible almacenar el modelo entrenado mediante la función save_model_tf, que genera una carpeta con la red que se puede cargar y reutilizar mediante la función load_model_tf.","code":"\nsave_model_tf(object = model, filepath = \"model\")\nreloaded_model <- load_model_tf(\"model\")\nround(predict(reloaded_model, mnist$test$x[1,1:28,1:28]), digits=4)#>       [,1] [,2]  [,3]   [,4] [,5]  [,6] [,7]   [,8] [,9] [,10]\n#> [1,] 2e-04    0 1e-04 0.0028    0 1e-04    0 0.9948    0 0.002"},{"path":"capNN.html","id":"ejemplo-de-red-para-regresión-en-r","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.10 Ejemplo de red para regresión en R","text":"En esta sección se entrena una red neuronal artificial para predecir el precio de la vivienda según sus características en Madrid. Para ello se usará el dataset de Madrid_Sale disponibles en el paquete de R Idealista18, con datos inmobiliaros del año 2018 y que fue utilizado en el Cap. 9. Para ello, se tomarán las siguientes 7 variables que se usarán para realizar la estimación:CONSTRUCTEDAREA: metros cuadrados construidos.ROOMNUMBER: número de habitaciones.BATHNUMBER: número de baños.HASLIFT: si tiene ascensor.DISTANCE_TO_CITY_CENTER: distancia al centro de la ciudad.DISTANCE_TO_METRO: distancia la parada de metro más cercana.DISTANCE_TO_CASTELLANA: distancia la Castellana.","code":""},{"path":"capNN.html","id":"carga-y-visualización-de-los-datos-1","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.10.1 Carga y visualización de los datos","text":"Considerando que ya se ha cargado previamente la librería keras, se carga el conjunto de datos indicando las variables considerar:El conjunto de datos contiene un total de 94815 elementos, que se dividirán en un 90% para entrenamiento y un 10% para test:","code":"\nlibrary(idealista18)\ndata(\"Madrid_Sale\")\n\nvariables <- c(\"CONSTRUCTEDAREA\",\"ROOMNUMBER\",\"BATHNUMBER\",\n               \"HASLIFT\",\"DISTANCE_TO_CITY_CENTER\",\"DISTANCE_TO_METRO\",\n               \"DISTANCE_TO_CASTELLANA\")\nx_madrid <- Madrid_Sale[variables]\nx_madrid_mat <- unname(data.matrix(x_madrid))\ny_madrid <- Madrid_Sale$PRICE\ny_madrid_mat <- matrix(y_madrid,nrow = length(y_madrid),byrow = TRUE)\nind <- sample(c(TRUE, FALSE), length(y_madrid), replace=TRUE, prob=c(0.9, 0.1))\nmadrid_dat_train_x <- x_madrid_mat[ind, ]\nmadrid_dat_test_x <- x_madrid_mat[!ind, ]\nmadrid_dat_train_y <- y_madrid_mat[ind, ]\nmadrid_dat_test_y <- y_madrid_mat[!ind, ]"},{"path":"capNN.html","id":"preprocesamiento-1","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.10.2 Preprocesamiento","text":"Una vez cargados los datos y comprobado su contenido, es recomendable la normalización de las variables contenidas en el conjunto de datos debido su heterogeneidad. Aunque sería posible para la red neuronal el adaptarse esta situación, ciertamente puede complicar el proceso de entrenamiento haciéndola más imprecisa. Para ello, se utilizará la función scale() sobre las variables predictoras y se dividirá la variable del precio entre 100000 para reducir su escala:","code":"\nmadrid_dat_train_x <- scale(madrid_dat_train_x)\nmadrid_dat_test_x <- scale(madrid_dat_test_x)\nmadrid_dat_train_y <- madrid_dat_train_y/100000\nmadrid_dat_test_y <- madrid_dat_test_y/100000"},{"path":"capNN.html","id":"generación-de-la-red-neuronal","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.10.3 Generación de la red neuronal","text":"El siguiente paso consiste en la generación de la red neuronal. Para ello, al igual que en la sección 36.9.3, se define primero la estructura utilizando la interfaz sequential proporcionada por Tensorflow/Keras través de la función keras_model_sequential():Como se puede observar, la red está compuesta por varias capas ocultas tipo dense, en las que las tres primeras tienen una activación relu. Al final, una última capa dense se encarga de obtener el valor de la estimación y, al contrario que en el ejemplo previo, incluye ningún tipo de función de activación debido que el valor de la misma ya es comprensible tanto para el modelo como para su interpretación. Esto sería equivalente utilizar la función de activación lineal.Finalmente, se compila el modelo indicando los parámetros de configuración necesarios para el proceso de entrenamiento. En este caso la función de coste o pérdida se corresponderá con el Error Medio Cuadrático y la métrica con el error medio absoluto:","code":"\nmodel <- keras_model_sequential() |>\n  layer_dense(units=128, activation=\"relu\", input_shape=7) |>\n  layer_dense(units=64, activation=\"relu\") |>\n  layer_dense(units=16, activation=\"relu\") |>\n  layer_dense(units=1)\nsummary(model, line_length=64)\n#> Model: \"sequential_1\"\n#> ________________________________________________________________\n#>  Layer (type)               Output Shape              Param #   \n#> ================================================================\n#>  dense_5 (Dense)            (None, 128)               1024      \n#>  dense_4 (Dense)            (None, 64)                8256      \n#>  dense_3 (Dense)            (None, 16)                1040      \n#>  dense_2 (Dense)            (None, 1)                 17        \n#> ================================================================\n#> Total params: 10,337\n#> Trainable params: 10,337\n#> Non-trainable params: 0\n#> ________________________________________________________________\nmodel |>\n  compile(\n    loss = \"mse\", # mean squared error\n    optimizer = \"sgd\", # stochastic gradient descent\n    metrics = \"mae\" # mean average error\n  )"},{"path":"capNN.html","id":"entrenamiento","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.10.4 Entrenamiento","text":"Una vez generada la estructura de la red neuronal y definida la anterior configuración, se entrena la misma utilizando la función fit(), configurando el resto de parámetros de forma similar como se vio en la sección 36.9.4:Tras el entrenamiento es posible ver su evolución mediante las gráficas de coste/pérdida y error:\nFigura 36.11: Evolución durante el entrenamiento de la precisión y la pérdida de los conjuntos de entrenamiento y validación\nComo se puede observar, en este caso el modelo tiene aún posibilidad de mejora, ya que la pérdida sigue siendo alta y se ha estancado, por lo que incrementando el número de épocas y el tiempo de entrenamiento se podría obtener un mejor resultado.","code":"\ntraining_evolution <- model |>\n  fit(\n    x = madrid_dat_train_x, y = madrid_dat_train_y,\n    epochs = 50, batch_size = 512,\n    validation_split = 0.2,\n    learning_rate = 0.1,\n    verbose = 2\n  )\nplot(training_evolution)"},{"path":"capNN.html","id":"test-1","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"36.10.5 Test","text":"Una vez entrenado el modelo, es posible aplicarlo sobre el conjunto de test mediante la función predict(), obteniendo la estimación para cada una de las viviendas:Y mediante la función evaluate() se calcula tanto el coste o pérdida como el error de la red neuronal sobre el conjunto de test, el cual tendremos que multiplicar por 100000 para obtener el resultado en la escala original del conjunto de datos:","code":"\npredictions <- predict(model, madrid_dat_test_x)\nhead(predictions, 5)\n#> [,1]\n#> [1,] 6.669374\n#> [2,] 5.895504\n#> [3,] 3.887646\n#> [4,] 6.390513\n#> [5,] 5.721725\nmodel |>\n  evaluate(madrid_dat_test_x, madrid_dat_test_y, verbose = 0)\n#> loss mae\n#> 2.4195166 0.9227165"},{"path":"capNN.html","id":"resumen-29","chapter":"Capítulo 36 Redes neuronales artificiales","heading":"Resumen","text":"En este capítulo se ha explicado en detalle el concepto de redes neuronales artificiales, incluyendo los elementos que la componen, desde el perceptrón o neurona básica hasta el perceptrón multicapa, pasando el perceptron multiclase, junto al proceso de aprendizaje de los mismos.En este capítulo se ha explicado en detalle el concepto de redes neuronales artificiales, incluyendo los elementos que la componen, desde el perceptrón o neurona básica hasta el perceptrón multicapa, pasando el perceptron multiclase, junto al proceso de aprendizaje de los mismos.Además, se han definido las funciones de activación clásicas utilizadas en las redes neuronales artificiales, las cuales se encargan de transformar la suma ponderada de las entradas en el resultado final de la capa.Además, se han definido las funciones de activación clásicas utilizadas en las redes neuronales artificiales, las cuales se encargan de transformar la suma ponderada de las entradas en el resultado final de la capa.Finalmente, se han explicado los pasos necesarios para poder entrenar una red neuronal artificial utilizando la librería Tensorflow/Keras en R, resolviendo el problema de clasificación de dígitos manuscritos representado en el conjunto de datos MNIST y un problema de regresión para estimar el precio de viviendas según sus características representado en el conjunto de datos de Idealista18.Finalmente, se han explicado los pasos necesarios para poder entrenar una red neuronal artificial utilizando la librería Tensorflow/Keras en R, resolviendo el problema de clasificación de dígitos manuscritos representado en el conjunto de datos MNIST y un problema de regresión para estimar el precio de viviendas según sus características representado en el conjunto de datos de Idealista18.","code":""},{"path":"cap-redes-convol.html","id":"cap-redes-convol","chapter":"Capítulo 37 Redes neuronales convolucionales","heading":"Capítulo 37 Redes neuronales convolucionales","text":"Noelia Vállez Enano\\(^{}\\) y José Luis Espinosa Aranda\\(^{}\\)\\(^{}\\)Universidad de Castilla-La Mancha","code":""},{"path":"cap-redes-convol.html","id":"introducción-16","chapter":"Capítulo 37 Redes neuronales convolucionales","heading":"37.1 Introducción","text":"Las redes neuronales convolucionales (en inglés Convolutional Neural Network, CNN) son una extensión de las redes neuronales artificiales en las que se incluyen capas convolucionales para aprender extraer, de forma automática, las características de los datos de entrenamiento al inicio de la arquitectura (Fig. 37.1).\nFigura 37.1: Estructura general de una CNN\nLas primeras capas convolucionales de la red aprenden extraer características generales de los datos de entrada mientras que las últimas capas convolucionales extraen características mucho más específicas. Cuanto más larga es la red (o más profunda) mayor cantidad de detalles podrá aprender distinguir. Esto es lo que ha propiciado la aparición del término ``aprendizaje profundo” (Goodfellow, Bengio, Courville 2016).Tras las capas convolucionales suelen encontrarse las capas densas o totalmente conectadas de la misma tipología de las vistas en el Cap. 36. Esta parte de la red será la encargada de realizar la clasificación de las muestras según los valores de las características extraídas en la parte convolucional. Por tanto, se dice que este tipo de redes tiene dos partes: una parte de extracción de características (realizada por la red convolucional) y una parte de clasificación o regresión (como las vistas en el Cap. 36).","code":""},{"path":"cap-redes-convol.html","id":"convolución","chapter":"Capítulo 37 Redes neuronales convolucionales","heading":"37.2 Convolución","text":"Aunque las redes neuronales artificiales (ANN) pueden utilizarse con los valores de color de una imagen como variables para reconocer qué hay en ella (ver Cap. 36), es posible extraer información espacial de esta forma. Para lidiar con este problema, las CNN incorporan capas convolucionales para extraer características de las muestras de entrada, incluyendo información de la estructura espacial (LeCun, Bengio, et al. 1995).Las convoluciones realizan una tarea similar al sistema visual humano, de hecho, se inspiran en cómo el ser humano percibe y procesa las características de los objetos. Aunque se diseñaron principalmente para ayudar resolver tareas de visión por computador donde la entrada de la red es una imagen, es posible utilizarlas también con entradas vectoriales o series temporales.Una convolución aplica un filtro sobre la entrada siguiendo un proceso de ventana deslizante. El filtro (o kernel) es otra cosa que una matriz con unos pesos que se centrará en cada uno de los valores de la entrada para realizar una media ponderada de los valores de la entrada por los valores del filtro (Garcia et al. 2015). El tamaño de los filtros suele ser, por tanto, impar.La Figura 37.2 muestra el resultado de aplicar la operación de convolución con un filtro de tamaño 3x3 sobre una matriz de entrada de tamaño 5x5. La salida será una matriz, \\(\\boldsymbol M\\), de tamaño 3x3, donde cada elemento será la suma ponderada de multiplicar los elementos del filtro centrado en esa posición de la entrada por los valores de la entrada.\nFigura 37.2: Ejemplo de convolución. De izquierda derecha: entrada (negro = 0, blanco = 1), filtro y salida.\nEn general, una convolución en dos dimensiones se define como:\\[\\begin{equation}\n\\boldsymbol M[x,y]=\\sum_{s=-}^{}\\sum_{t=-b}^{b}\\boldsymbol F[s,t]\\boldsymbol [x-s,y-t] ,\n\\end{equation}\\]donde \\(\\boldsymbol F\\) es el filtro aplicar, \\(\\boldsymbol \\) es la matriz de entrada, \\(\\boldsymbol M\\) es la matriz de resultado que recibe también el nombre de ``mapa de características” y \\(\\) y \\(b\\) son los desplazamientos desde el centro del filtro cualquier otro valor.Por tanto, cada valor del ejemplo de la Figura 37.2 se obtiene como:\\[\\begin{gather}\n\\begin{aligned}\nM_{1,1} = 1\\cdot0+ 0\\cdot 0+1 \\cdot 0+0 \\cdot 0 + & 1 \\cdot 1 + 0 \\cdot 1+ 1\\cdot 0 +0\\cdot 1 +1 \\cdot 1=2 \\\\\nM_{1,2} = 1\\cdot0+ 0\\cdot 0+1 \\cdot 0+0 \\cdot 1 + & 1 \\cdot 1 + 0 \\cdot 1+ 1\\cdot 1 +0\\cdot 1 +1 \\cdot 1=3 \\\\\n& \\cdots \\\\\nM_{2,2} = 1\\cdot1+ 0\\cdot 1+1 \\cdot 1+0 \\cdot 1 + & 1 \\cdot 1 + 0 \\cdot 1+ 1\\cdot 1 +0\\cdot 1 +1 \\cdot 1=5 \\\\\n& \\cdots \\\\\nM_{3,3} = 1\\cdot1+ 0\\cdot 1+1 \\cdot 0+0 \\cdot 1 + & 1 \\cdot 1 + 0 \\cdot 0+ 1\\cdot 0 +0\\cdot 0 +1 \\cdot 0=2\n\\end{aligned}\n\\end{gather}\\]La elección de los valores del filtro obtendrá matrices de salida que realcen o suavicen ciertas partes de la entrada. Por ejemplo, si la entrada es una imagen, es posible definir filtros que realcen los bordes, que los suavicen o incluso que detecten dichos bordes y cómo de marcados están. La Figura 37.3 muestra el resultado de aplicar distintos filtros una imagen de entrada.\nFigura 37.3: Resultado de aplicar diferentes filtros de convolución sobre una imagen dada.\nLos valores (o pesos) de los filtros, que se ajustaban tradicionalmente, en un inicio, de forma manual según el problema resolver. En los frameworks actuales estos filtros se ajustan durante el proceso de entrenamiento de la CNN junto al resto de pesos de la red. Esto permite encontrar valores que maximicen la precisión final de la red.","code":""},{"path":"cap-redes-convol.html","id":"neuronas-convolucionales","chapter":"Capítulo 37 Redes neuronales convolucionales","heading":"37.3 Neuronas convolucionales","text":"Las capas convolucionales de la CNN estarán compuestas por perceptrones, sino por neuronas convolucionales que realizan las operaciones comentadas. Estas neuronas cuentan con matrices de pesos y con vectores de pesos como lo hace el perceptrón. En este caso, tanto la entrada como la salida de la neurona son matrices. Para una neurona \\(j\\), la salida \\(\\boldsymbol Y_j\\) se calcula como la combinación lineal de las salidas de las neuronas de la capa anterior \\(\\boldsymbol Yi\\) operando cada una de ellas con el filtro \\(\\boldsymbol F_{ij}\\) correspondiente esa conexión de forma que:\\[\\begin{equation}\n\\boldsymbol Y_j = g ( \\boldsymbol B_j + \\sum \\boldsymbol F_{ij} \\otimes + \\boldsymbol Y_i ) ,\n\\end{equation}\\]donde \\(\\boldsymbol B_j\\) y \\(g\\) representan el bias y la función de activación respectivamente. La mayoría de las CNN utilizan la ReLU como activación o alguna variante de ésta. Esta activación funciona muy bien con el método del descenso del gradiente utilizado para encontrar los pesos.Cada neurona dará lugar un mapa de activaciones. La salida de una capa convolucional será entonces un conjunto de estos mapas (Fig. 37.4)\nFigura 37.4: Conjunto de mapas de activaciones de una determinada capa. Cada filtro de la capa da lugar un mapa diferente.\nEn el caso de que la entrada sea una matriz 2D sino que sea una matriz 3D como, por ejemplo, una imagen, los filtros contarán con una tercera dimensión. La Figura 37.5 muestra algunos de los rellenos más empleados.\nFigura 37.5: Resultado de aplicar un filtro 3D una imagen antes y después de pasar por el filtro de activación\n","code":""},{"path":"cap-redes-convol.html","id":"relleno-del-borde","chapter":"Capítulo 37 Redes neuronales convolucionales","heading":"37.4 Relleno del borde","text":"Si se aplica el filtro convolucional una entrada, la salida será algo más pequeña al poder centrar el filtro en los bordes de la matriz. Para poder hacerlo, se suele incrementar la entrada de la capa con un relleno (en inglés padding). El relleno se puede realizar con ceros, con algún valor, con el valor más cercano del borde, etc. La Figura 37.6 muestra algunos de los rellenos más empleados.\nFigura 37.6: Distintos tipos de relleno del borde\n","code":""},{"path":"cap-redes-convol.html","id":"desplazamiento","chapter":"Capítulo 37 Redes neuronales convolucionales","heading":"37.4.1 Desplazamiento","text":"El desplazamiento (en inglés stride) básico con el que se aplica un filtro convolucional es de 1. Sin embargo, la aplicación de muchos filtros repartidos en capas lo largo de la red hace que sea especialmente difícil mantener todos los datos generados en un momento determinado del entrenamiento. Para reducir este volumen de datos, se suelen aplicar las convoluciones con un desplazamiento mayor que 1. Esto reduce el tamaño del mapa de activaciones obtenido por una determinada capa (Fig. 37.7).\nFigura 37.7: Desplazamiento 2x2 del filtro. El punto es el centro de la zona en la que se aplica el filtro en cada momento.\n","code":""},{"path":"cap-redes-convol.html","id":"capas-de-agrupación","chapter":"Capítulo 37 Redes neuronales convolucionales","heading":"37.5 Capas de agrupación","text":"La ejecución en secuencia de varias capas convolucionales es muy efectiva la hora de decidir si ciertas características están o presentes en la entrada. Sin embargo, una de sus ventajas y la vez limitaciones es que mantiene la localización espacial de las características. Aunque es necesario cierta información espacial como, por ejemplo, el que hubiera unos bigotes cerca de una boca sería característico de una imagen que contuviese un gato, pequeños movimientos del contenido de la imagen producirían mapas de características diferentes.Una forma de mitigar este problema es usar capas de agrupación (en inglés pooling). Estas capas agrupan un número de valores adyacentes de los mapas de características obteniendo un nuevo conjunto de mapas más pequeños. Es posible emplear distintos tipos de operaciones con las que realizar la agrupación. Los más empleados suelen ser el max pooling y el average pooling (Goodfellow, Bengio, Courville 2016) que seleccionan el máximo de los valores u obtienen su media respectivamente (Fig. 37.8). El tamaño más típico es 2x2.\nFigura 37.8: Resultado de emplear dos métodos de agrupación diferentes para reducir la dimentsión de los datos\n","code":""},{"path":"cap-redes-convol.html","id":"desvanecimiento-del-gradiente","chapter":"Capítulo 37 Redes neuronales convolucionales","heading":"37.6 Desvanecimiento del gradiente","text":"La primera red convolucional fue propuesta en 1982 (Fukushima Miyake 1982). Esta arquitectura recibió el nombre de Neocognitron y ya constaba de capas convolucionales y capas de pooling. Siguiendo la misma idea, en 1998 se diseñó otra CNN para resolver el problema de reconocimiento de dígitos manuscritos, MNIST (LeCun et al. 1998). esta arquitectura de CNN se la conoce con el nombre de LeNet y es una de las arquitecturas más pequeñas que podemos definir para resolver un problema de clasificación (Fig. 37.9). El extractor de características, consta de dos capas convolucionales alternadas con 2 capas de pooling que obtienen un total de 400 variables. La parte final con el clasificador está compuesta por 3 capas densas de 120, 84 y 10 neuronas.\nFigura 37.9: Arquitectura LeNet\npesar de los buenos resultados obtenidos por la arquitectura, el uso de estos métodos para resolver problemas reales estaba aún lejos debido la carga computacional requerida para su entrenamiento. fue hasta el año 2012, cuando los ganadores del concurso ImageNet Challenge presentaron una nueva arquitectura llamada AlexNet, que las CNN volvieron estar en el punto de mira de los investigadores (Deng et al. 2012). partir de ese momento, y teniendo en cuenta los grandes avances computacionales de las tarjetas gráficas (GPU) que permitían ejecutar operaciones matriciales de forma eficiente, se empezaron desarrollar cada vez más arquitecturas diferentes.Durante los primeros años, las arquitecturas desarrolladas tenían cada vez más capas y más filtros en cada una de ellas para extraer la mayor cantidad de información posible de la entrada. Sin embargo, las arquitecturas más profundas se encontraron con un problema: el desvanecimiento del gradiente.Ciertas funciones de activación como, por ejemplo, la sigmoide, comprimen el espacio de entrada entre 0 y 1. Esto hace que grandes cambios en la entrada produzcan cambios muy pequeños en la salida, haciendo que la derivada sea pequeña. Como los gradientes de la red se calculan durante la propagación hacia atrás capa capa siguiendo la regla de la cadena, si los valores son muy cercanos 0, la multiplicación de muchos de estos valores hará que el gradiente de la red caiga rápidamente. Un gradiente muy pequeño hará que los pesos de las capas iniciales apenas se modifiquen con cada iteración y lleguen obtener valores adecuados durante el entrenamiento.Algunas soluciones este problema son:El uso de activaciones tipo ReLU que obtienen valores muy pequeños en su derivada.El uso de activaciones tipo ReLU que obtienen valores muy pequeños en su derivada.Capas de normalización. Si se normalizan los datos de entrada ya habrá grandes cambios entre ellos y los valores estarán lejos de los extremos de la sigmoide.Capas de normalización. Si se normalizan los datos de entrada ya habrá grandes cambios entre ellos y los valores estarán lejos de los extremos de la sigmoide.Uso de bloques con conexiones residuales que suman el valor de la entrada del bloque su salida.Uso de bloques con conexiones residuales que suman el valor de la entrada del bloque su salida.","code":""},{"path":"cap-redes-convol.html","id":"sobreajuste-1","chapter":"Capítulo 37 Redes neuronales convolucionales","heading":"37.7 Sobreajuste","text":"Cuanto mayor es el número de parámetros de la red, mayor probabilidad hay de que ``memorice” los datos de entrenamiento. Esto se debe la cantidad de características que la red es capaz de extraer y medir. Si la red es muy profunda, aprenderá cosas muy concretas del conjunto de entrenamiento, lo que dará lugar modelos que generalizan bien con nuevos datos (Tetko, Livingstone, Luik 1995).Además de esto, la linealidad que añaden las funciones de activación puede hacer que se encuentren fronteras de decisión que modelen datos que son linealmente separables, pero también facilita que se produzca el sobreajuste (Fig. 37.10).\nFigura 37.10: Tipos de ajuste del modelo los datos\nPara evitar que se produzca el sobreajuste se suelen emplear técnicas de regularización. Se trata de técnicas que impiden que los modelos sean demasiado complejos mejorando su capacidad de generalización. Algunas de estas técnicas son:Dropout. Durante el entrenamiento, algunas activaciones se ponen 0 de forma aleatoria (entre el 10% y el 50%). Esto hace que una capa de la red dependa siempre de los mismos nodos anteriores.Early Stopping. Se trata de parar el entrenamiento antes de que se produzca el sobreajuste y seleccionar ese modelo como final. Para ello se utilizan dos conjuntos: uno de entrenamiento y otro de validación. Cuando las curvas de pérdida de ambos conjuntos comienzan diverger, se para el entrenamiento y se selecciona el modelo resultante del momento anterior al comienzo de la divergencia (Fig. 37.11).\nFigura 37.11: Selección del modelo antes del sobreajuste\nRegularización L1. Penaliza los pesos grandes por lo que fuerzan los pesos tener valores cercanos 0 (sin ser 0). Añade un término de penalización la función de coste sumando todos los pesos de la matriz y multiplicado por un valor \\(\\alpha\\) que es otro hiperparámetro que debe ser seleccionado manualmente:\\[\\begin{equation}\n\\alpha||\\boldsymbol W||_1 = \\alpha\\sum_i\\sum_j|w_{ij}| .\n\\end{equation}\\]Regularización L2 o weight decay. Parecida la regularización L2 pero con una expresión algo diferente:\\[\\begin{equation}\n\\frac{\\alpha}{2} ||\\boldsymbol W||_2^2 = \\frac{\\alpha}{2}\\sum_i\\sum_j w^2_{ij} .\n\\end{equation}\\]","code":""},{"path":"cap-redes-convol.html","id":"generación-de-datos-de-entrenamiento-artificiales","chapter":"Capítulo 37 Redes neuronales convolucionales","heading":"37.8 Generación de datos de entrenamiento artificiales","text":"Como se ha comentado anteriormente, las técnicas de deep learning suelen requerir de gran cantidad de datos para su correcto funcionamiento. En muchas situaciones, se dispone de un conjunto limitado para poder entrenar los modelos de forma correcta, por lo que para tratar de suplir la falta de datos se recurre la generación de datos artificiales, técnica conocida con el nombre de, con la expresión en inglés, data augmentation (Shorten Khoshgoftaar 2019).Esta técnica realiza pequeñas variaciones en los datos del conjunto de entrenamiento del que se dispone para obtener nuevos, manteniendo el significado semántico de los mismos. Esto también permite mejorar la generalización de los modelos. Por ejemplo, si se tienen imágenes donde una de ellas contiene un elemento de la clase gato, las modificaciones que se realicen deben permitir poder reconocer esa misma clase partir de las imágenes modificadas.Algunos ejemplos de técnicas de data augmentation en imagen pueden ser: la realización de rotaciones, modificación del contraste o cambios en la iluminación, reescalados, adición/eliminación de ruido o cambio en las proyecciones de las mismas.\nFigura 37.12: Ejemplos de técnicas de generación de datos artificiales\nPara agregar diferentes tipologías de esta técnica de data augmentation en R, se pueden incluir capas de preprocesado en el modelo secuencial, que serán ejecutadas de manera aleatoria únicamente durante el entrenamiento. En el siguiente ejemplo se realizarán rotaciones aleatorias, volteados horizontales y acercamientos la imagen:continuación se muestran los diferentes tipos de preprocesado disponibles para imagen y redes neuronales convolucionales:NOTAOtros tipos de data augmentation disponibles en keras y R para otro tipo de datos pueden consultarse enhttps://tensorflow.rstudio.com/guides/keras/preprocessing_layers","code":"\ndata_augmentation <-\n  keras_model_sequential() |>\n  layer_random_rotation(0.1) |>\n  layer_random_flip(\"horizontal\") |>\n  layer_random_zoom(0.1)\nlayer_random_crop()\nlayer_random_flip()\nlayer_random_flip()\nlayer_random_translation()\nlayer_random_rotation()\nlayer_random_zoom()\nlayer_random_height()\nlayer_random_width()\nlayer_random_contrast()"},{"path":"cap-redes-convol.html","id":"ejemplo-en-r-para-el-conjunto-de-datos-cifar10","chapter":"Capítulo 37 Redes neuronales convolucionales","heading":"37.9 Ejemplo en R para el conjunto de datos CIFAR10","text":"En esta sección se verá cómo entrenar una red neuronal convolucional para ser capaces de clasificar las clases contenidas en el conjunto de datos CIFAR10. La descarga debe hacerse través del siguiente enlace https://drive.google.com/file/d/1-pFGg-bkooss1fNp5UNYR0-hLUmDP_XO/view?usp=sharing y el conjunto tiene que guardarse en una carpeta data dentro del proyecto de trabajo para asegurar la reproducibilidad del capítulo. Cada una de las imágenes contenidas en el mismo contiene un único elemento que puede ser clasificado como avión, coche, pájaro, gato, ciervo, perro, rana, caballo, barco o camión.NotaExiste otra versión del conjunto de datos denominada como CIFAR100, en la cual se definen un total de 100 posibles categorías en las que las imágenes contenidas pueden ser clasificadas. El ejemplo continuación puede ser replicado con este mismo conjunto de datos.https://www.rdocumentation.org/packages/keras/versions/2.9.0/topics/dataset_cifar100Cada una de las imágenes contenidas en el conjunto tiene un tamaño de 32x32 píxeles en color, representándose mediante los 3 canales RGB, siendo diferente al ejemplo del capítulo 36, en el cual se trabaja con imágenes en escala de grises y, por tanto, un único canal.continuación, se verán los pasos seguidos, siendo de forma general muy similares los ya descritos en el Cap. 36, pero adaptando la red al tipo de dato utilizado.","code":""},{"path":"cap-redes-convol.html","id":"visualizacion","chapter":"Capítulo 37 Redes neuronales convolucionales","heading":"37.9.1 Carga y visualización de los datos","text":"El primer paso será cargar la librería keras, para así poder crear las redes neuronales necesarias y también para cargar el conjunto de imágenes CIFAR10 que se encuentra disponible públicamente:continuación, se puede ver el contenido de las variables generadas, donde se puede observar como el conjunto de datos CIFAR10 ya viene separado en dos subconjuntos que pueden ser utilizados para entrenamiento y para test. Además se puede ver que el conjunto de entrenamiento está compuesto por 50000 imágenes, mientras que el conjunto de test por 10000. En ambos casos, estas imágenes se almacenan en la variable x.Además, las imágenes de cada subconjunto tienen definida la clase la que pertenecen, en este caso, cualquiera de las 10 clases indicadas anteriormente. En ambos casos, esta etiqueta se almacena en la variable y. continuación, se muestra un pequeño ejemplo que permitirá mostrar alguna de las imágenes contenidas en el conjunto de datos de entrenamiento junto con su etiqueta:","code":"\nlibrary(keras)\nload(\"data/cifar10.RData\")\nnames(cifar)\n#> [1] \"train\" \"test\"\ndim(cifar$train$x)\n#> [1] 50000    32    32     3\ndim(cifar$train$y)\n#> [1] 50000     1\ndim(cifar$test$x)\n#> [1] 10000    32    32     3\ndim(cifar$test$y)\n#> [1] 10000     1\nclass_names <- c('avion', 'coche', 'pajaro', 'gato', 'ciervo',\n               'perro', 'rana', 'caballo', 'barco', 'camion')\n\nindex <- 1:30\n\npar(mfcol = c(5,6), mar = rep(1, 4), oma = rep(0.2, 4))\ncifar$train$x[index,,,] |> \n  purrr::array_tree(1) |>\n  purrr::set_names(class_names[cifar$train$y[index] + 1]) |> \n  purrr::map(as.raster, max = 255) |>\n  purrr::iwalk(~{plot(.x); title(.y)})"},{"path":"cap-redes-convol.html","id":"preprocesamiento-2","chapter":"Capítulo 37 Redes neuronales convolucionales","heading":"37.9.2 Preprocesamiento","text":"Una vez cargados los datos y comprobado su contenido, al igual que se explicó en el Cap. 36, es posible realizar algún tipo de preprocesado. Al estar trabajando con imágenes, es muy típico estandarizar los valores de color de las imágenes para mitigar las diferencias producidas por las diferentes condiciones de iluminación.En este caso, al igual que en el Cap. 36, se va transformar los valores originales de la imagen (en rango de 0 255) valores entre 0 y 1 dividiendo cada valor por el máximo, 255:","code":"\ncifar$train$x <- cifar$train$x/255\ncifar$test$x <- cifar$test$x/255"},{"path":"cap-redes-convol.html","id":"generación-de-la-red-neuronal-1","chapter":"Capítulo 37 Redes neuronales convolucionales","heading":"37.9.3 Generación de la red neuronal","text":"En esta ocasión se creará la red neuronal convolucional en dos pasos, para además mostrar cómo se pueden utilizar las funciones proporcionadas por la librería keras para definir una CNN en varias partes, combinándolas poco poco.En el primero, se definirá, utilizando la interfaz sequential proporcionada por Tensorflow/Keras través de la función keras_model_sequential, la base convolucional de la red combinando varias capas Conv2d con MaxPooling2D. Esta es la parte de la red que se encargará de aprender las características necesarias que permitirán representar el contenido de la imagen. Otra de las diferencias principales que se puede observar en esta red es que, al aceptar imágenes de 3 canales, RGB, en vez de imágenes en escala de grises, el tamaño de la entrada de la primera capa tiene que reflejar esto input_shape = c(32,32,3).Como se puede observar, en esta parte de la red se reduce la dimensión de la información de manera paulatina en cada capa, obteniendo las características representativas del objeto contenido en cada imagen hasta llegar un tamaño de \\(4\\times 4\\times 64\\).Ahora, será necesario añadir capas que permitan transformar los resultados de la parte convolucional de la red implementada un valor de probabilidad de que la imagen represente cada una de las posibles clases de la imagen.Para ello, primero se inserta una capa de tipo flatten que se encarga de transformar la salida de la última capa convolucional 4x4x64 un vector de 1024 elementos. continuación, una capa oculta dense de 64 neuronas con activación relu se encarga de realizar las primeras operaciones con esos datos y de reducir la dimensionalidad. Finalmente, una última capa dense con activación softmax se encarga de obtener la probabilidad de que la imagen represente cada una de las 10 posibles clases:continuación, se puede observar como quedaría la estructura final del modelo implementado:NOTAUn detalle tener en cuenta con respecto al ejemplo del Cap. 36 es el parámetro Total params. Este valor indica el número de parámetros que contiene nuestra red neuronal y, en cierta manera, el tamaño de la misma. Se puede observar que en este caso tiene un mayor tamaño al contar con un total de 122570 parámetros con respecto los 11935 del ejemplo previo.Finalmente, es necesario compilar el modelo, indicando algunos de los parámetros de configuración necesarios para el proceso de entrenamiento, como serían el optimizador utilizar, la función de coste y las métricas calcular para poder evaluar la red entrenada:","code":"\nmodel <- keras_model_sequential() |> \n  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = \"relu\", \n                input_shape = c(32,32,3)) |> \n  layer_max_pooling_2d(pool_size = c(2,2)) |> \n  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = \"relu\") |> \n  layer_max_pooling_2d(pool_size = c(2,2)) |> \n  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = \"relu\")\nsummary(model, line_length=74)\n#> Model: \"sequential_2\"\n#> __________________________________________________________________________\n#>  Layer (type)                    Output Shape                 Param #     \n#> ==========================================================================\n#>  conv2d_2 (Conv2D)               (None, 30, 30, 32)           896         \n#>  max_pooling2d_1 (MaxPooling2D)  (None, 15, 15, 32)           0           \n#>  conv2d_1 (Conv2D)               (None, 13, 13, 64)           18496       \n#>  max_pooling2d (MaxPooling2D)    (None, 6, 6, 64)             0           \n#>  conv2d (Conv2D)                 (None, 4, 4, 64)             36928       \n#> ==========================================================================\n#> Total params: 56,320\n#> Trainable params: 56,320\n#> Non-trainable params: 0\n#> __________________________________________________________________________\nmodel |> \n  layer_flatten() |> \n  layer_dense(units = 64, activation = \"relu\") |> \n  layer_dense(units = 10, activation = \"softmax\")\nsummary(model, line_length=74)\n#> Model: \"sequential_2\"\n#> __________________________________________________________________________\n#>  Layer (type)                    Output Shape                 Param #     \n#> ==========================================================================\n#>  conv2d_2 (Conv2D)               (None, 30, 30, 32)           896         \n#>  max_pooling2d_1 (MaxPooling2D)  (None, 15, 15, 32)           0           \n#>  conv2d_1 (Conv2D)               (None, 13, 13, 64)           18496       \n#>  max_pooling2d (MaxPooling2D)    (None, 6, 6, 64)             0           \n#>  conv2d (Conv2D)                 (None, 4, 4, 64)             36928       \n#>  flatten_1 (Flatten)             (None, 1024)                 0           \n#>  dense_7 (Dense)                 (None, 64)                   65600       \n#>  dense_6 (Dense)                 (None, 10)                   650         \n#> ==========================================================================\n#> Total params: 122,570\n#> Trainable params: 122,570\n#> Non-trainable params: 0\n#> __________________________________________________________________________\nmodel |> compile(\n  optimizer = \"sgd\", # stochastic gradient descent\n  loss = \"sparse_categorical_crossentropy\", # función utilizada para problemas de clasificación con varias clases\n  metrics = \"accuracy\" # Precisión\n)"},{"path":"cap-redes-convol.html","id":"entrenamiento-1","chapter":"Capítulo 37 Redes neuronales convolucionales","heading":"37.9.4 Entrenamiento","text":"Una vez generada la estructura de la red neuronal convolucional, es posible entrenarla para resolver el problema de clasificación mediante la función fit. Para ello, se le debe indicar el conjunto de imágenes de entrenamiento, x, que debe utilizar y sus etiquetas correspondientes, y. Además de otros parámetros, se podrá configurar el número de epochs entrenar (pasadas sobre el conjunto completo de entrenamiento), el tamaño del batch que se utilizará en cada iteración con batch_size (número de imágenes por iteración), qué porcentaje de elementos del conjunto de datos se utilizarán para validar el modelo con validation_split (imágenes utilizadas durante el entrenamiento pero solo para obtener una estimación real del error cometido) o la tasa de aprendizaje, learning_rate, entre otros.NOTAComo se puede observar, el batch_size configurado es menor que el del Cap. 36 (32 vs 128). Esto es debido que, el número máximo de imágenes que un mismo equipo utilizado para entrenar podrá procesar en una iteración vendrá determinado por el tamaño de la red neuronal, es decir, por la variable Total params indicada en la Nota anterior. Cuanto mayor sea el tamaño de la red, menor será el número máximo de imágenes que podrá tener el batch.Tras el entrenamiento, se puede observar la evolución del mismo mediante las gráficas de coste/perdida y precisión.\nFigura 37.13: Evolución durante el entrenamiento de la precisión y la pérdida de los conjuntos de entrenamiento y validación\nComo se puede observar, la red entrenada es capaz de alcanzar un 60% de precisión tanto en los conjuntos de entrenamiento como los de validación","code":"\ntraining_evolution <- model |> \n  fit(\n    x = cifar$train$x, y = cifar$train$y,\n    epochs = 10, batch_size = 32,\n    validation_split = 0.2,\n    learning_rate = 0.1,\n    verbose = 2\n  )\n#> Epoch 1/10\n#> 1250/1250 - 12s - loss: 2.1097 - accuracy: 0.2316 - val_loss: 1.9339 - val_accuracy: 0.2958 - 12s/epoch - 9ms/step\n#> Epoch 2/10\n#> 1250/1250 - 8s - loss: 1.7478 - accuracy: 0.3667 - val_loss: 1.6987 - val_accuracy: 0.3965 - 8s/epoch - 6ms/step\n#> Epoch 3/10\n#> 1250/1250 - 8s - loss: 1.5464 - accuracy: 0.4399 - val_loss: 1.4731 - val_accuracy: 0.4707 - 8s/epoch - 7ms/step\n#> Epoch 4/10\n#> 1250/1250 - 9s - loss: 1.4304 - accuracy: 0.4866 - val_loss: 1.3653 - val_accuracy: 0.5149 - 9s/epoch - 7ms/step\n#> Epoch 5/10\n#> 1250/1250 - 8s - loss: 1.3477 - accuracy: 0.5199 - val_loss: 1.3407 - val_accuracy: 0.5257 - 8s/epoch - 6ms/step\n#> Epoch 6/10\n#> 1250/1250 - 7s - loss: 1.2784 - accuracy: 0.5437 - val_loss: 1.2563 - val_accuracy: 0.5564 - 7s/epoch - 6ms/step\n#> Epoch 7/10\n#> 1250/1250 - 7s - loss: 1.2118 - accuracy: 0.5705 - val_loss: 1.2331 - val_accuracy: 0.5720 - 7s/epoch - 6ms/step\n#> Epoch 8/10\n#> 1250/1250 - 8s - loss: 1.1539 - accuracy: 0.5954 - val_loss: 1.1807 - val_accuracy: 0.5882 - 8s/epoch - 6ms/step\n#> Epoch 9/10\n#> 1250/1250 - 7s - loss: 1.1015 - accuracy: 0.6135 - val_loss: 1.1516 - val_accuracy: 0.5935 - 7s/epoch - 6ms/step\n#> Epoch 10/10\n#> 1250/1250 - 7s - loss: 1.0526 - accuracy: 0.6286 - val_loss: 1.1014 - val_accuracy: 0.6128 - 7s/epoch - 6ms/step\nplot(training_evolution)"},{"path":"cap-redes-convol.html","id":"test-2","chapter":"Capítulo 37 Redes neuronales convolucionales","heading":"37.9.5 Test","text":"Una vez entrenado el modelo, es posible aplicarlo sobre el conjunto de test proporcionado. Para ello, se puede realizar la predicción sobre cualquiera de las imágenes mediante la función predict(), obteniendo la probabilidad de que pertenezca una determinada clase:También se puede utilizar la función evaluate() para calcular tanto el coste o pérdida como la precisión de la red neuronal sobre el conjunto de test. Como se puede observar, se obtienen valores muy similares los obtenidos durante el entrenamiento:Con la función predict se puede también generar la matriz de confusión de la red para evaluar qué pares de clases está cometiendo un mayor número de errores:","code":"\npredictions <- predict(model, cifar$test$x)\nhead(round(predictions, digits=2), 5)#>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n#> [1,] 0.03 0.00 0.18 0.47 0.01 0.14 0.16    0 0.00  0.00\n#> [2,] 0.04 0.07 0.00 0.00 0.00 0.00 0.00    0 0.89  0.00\n#> [3,] 0.08 0.28 0.00 0.00 0.00 0.00 0.00    0 0.55  0.07\n#> [4,] 0.82 0.01 0.04 0.00 0.00 0.00 0.00    0 0.11  0.00\n#> [5,] 0.00 0.00 0.05 0.11 0.11 0.03 0.69    0 0.00  0.00\nevaluate(model, cifar$test$x, cifar$test$y, verbose = 0)#>     loss accuracy \n#> 1.094381 0.611100\nprediction_matrix <- model |> predict(cifar$test$x) |> k_argmax()\nconfusion_matrix <- table(as.array(prediction_matrix), cifar$test$y)\nconfusion_matrix#>    \n#>       0   1   2   3   4   5   6   7   8   9\n#>   0 650  25  57  12  39  12   3  21  82  35\n#>   1  43 790  15  18  13  11  16  12  43 179\n#>   2 119  20 676 180 325 170 112  94  33  26\n#>   3  23  15  57 427  76 184  54  48  18  18\n#>   4   1   0  13  20 236  12   5  21   3   2\n#>   5   6   7  52 145  41 488  24  82   5  10\n#>   6  14   5  71 110 119  42 755  13   4  14\n#>   7   7   6  30  46 126  60  16 676   9  18\n#>   8 114  58  15  19  20  13   6   5 777  62\n#>   9  23  74  14  23   5   8   9  28  26 636"},{"path":"cap-redes-convol.html","id":"otros-ejemplos-de-interés","chapter":"Capítulo 37 Redes neuronales convolucionales","heading":"37.9.6 Otros ejemplos de interés","text":"Transfer learning fine tuning. Explicación de estas técnicas para clasificar imágenes que contienen perros y gatos.\nhttps://tensorflow.rstudio.com/guides/keras/transfer_learningTransfer learning fine tuning. Explicación de estas técnicas para clasificar imágenes que contienen perros y gatos.\nhttps://tensorflow.rstudio.com/guides/keras/transfer_learninghttps://tensorflow.rstudio.com/guides/https://tensorflow.rstudio.com/guides/https://tensorflow.rstudio.com/examples/https://tensorflow.rstudio.com/examples/","code":""},{"path":"cap-redes-convol.html","id":"resumen-30","chapter":"Capítulo 37 Redes neuronales convolucionales","heading":"Resumen","text":"Este capítulo presenta las características de las redes neuronales convolucionales y sus diferencias con el perceptrón multicapa.Este capítulo presenta las características de las redes neuronales convolucionales y sus diferencias con el perceptrón multicapa.Además, se exponen los principales problemas la hora de diseñar este tipo de redes profundas y sus posibles soluciones.Además, se exponen los principales problemas la hora de diseñar este tipo de redes profundas y sus posibles soluciones.Finalmente, se han explicado los pasos necesarios para poder entrenar una red neuronal convolucional utilizando la librería Tensorflow/Keras en R, resolviendo el problema de clasificación de las 10 clases del conjunto de datos CIFAR10.Finalmente, se han explicado los pasos necesarios para poder entrenar una red neuronal convolucional utilizando la librería Tensorflow/Keras en R, resolviendo el problema de clasificación de las 10 clases del conjunto de datos CIFAR10.","code":""},{"path":"mineria-textos.html","id":"mineria-textos","chapter":"Capítulo 38 Minería de textos","heading":"Capítulo 38 Minería de textos","text":"Víctor Casero-Alonso\\(^{}\\),\nÁngela Celis\\(^{}\\) y\nMaría Lozano Zahonero\\(^{b}\\)\\(^{}\\)Universidad de Castilla-La Mancha y\n\\(^{b}\\) Università degli Studi di Roma Tor Vergata.","code":""},{"path":"mineria-textos.html","id":"introducción-17","chapter":"Capítulo 38 Minería de textos","heading":"38.1 Introducción","text":"En la actualidad, entre el 80 % y el 90 % de los datos que se generan diariamente son datos estructurados (vistos en el Cap. \\(\\ref{datos--sql}\\)). Un ejemplo típico de datos estructurados son los textos, desde los comentarios o mensajes de las redes sociales, reseñas, blogs y microblogs, chats o whatsapp hasta las noticias periodísticas, los discursos políticos o las obras literarias. En consecuencia, aprender procesar y analizar datos exige aprender procesar y analizar textos.Los textos precisan, sin embargo, un tratamiento especial. diferencia de la mayoría de los datos que se tratan en este libro, que son datos estructurados, los datos textuales requieren que se les otorgue un orden y estructura para su manejo y análisis con el software R. Además, al utilizar un lenguaje natural –es decir, un idioma como, por ejemplo, el español, el chino o el inglés–, los textos pueden ser procesados directamente por un ordenador. Es preciso “traducirlos” antes un lenguaje formal que los ordenadores puedan entender.La minería de textos (en inglés, text mining), también conocida como análisis de textos (en inglés, text analysis), puede definirse como el proceso para detectar, extraer, clasificar, analizar y visualizar la información explícita que contienen los textos, transformando los datos textuales en datos estructurados y el lenguaje natural en lenguaje formal fin de determinar, después, de manera automática, patrones recurrentes y desviaciones de los mismos. La minería de textos utiliza muchas técnicas y métodos diferentes, la mayor parte de los cuales proceden del procesamiento del lenguaje natural (PLN), un ámbito de la inteligencia artificial que se ocupa de la comunicación entre los seres humanos y las máquinas mediante el tratamiento computacional del lenguaje humano.Este capítulo constituye una primera aproximación la minería de textos con R. Su objetivo es proporcionar un marco teórico y aplicado básico de este ámbito. Para ello, en la Sec. 38.2, se presentan los conceptos y fases fundamentales de la minería de textos. La Sec. 38.3 está dedicada al análisis de sentimientos, que constituye uno de los campos de la minería de textos de mayor desarrollo en la actualidad. La Sec. 38.4 indica paquetes de R que permiten realizar análisis textuales de distintos tipos.\nCierra el capítulo un ejemplo, en el que se aplica y se amplía lo estudiado anteriormente. Dos referencias útiles sobre el tema son Fradejas Rueda (2022) y Jockers (2014).","code":""},{"path":"mineria-textos.html","id":"secCONCEPTOS","chapter":"Capítulo 38 Minería de textos","heading":"38.2 Conceptos y tareas fundamentales","text":"Lo primero que se necesita para hacer un análisis de textos son los textos. Esta afirmación podría parecer banal, pero lo es. El volumen de textos en circulación es ingente, pero, en la mayor parte de los casos, es necesario realizar una serie de operaciones complejas para poder extraer y recopilar los datos textuales que se quiere analizar. Es también difícil muchas veces acceder después estos datos, ya que los textos pueden presentar formatos muy heterogéneos, siempre interpretables o fáciles de convertir en un formato interpretable. Baste pensar, por ejemplo, en una nota escrita mano. Dado que este capítulo es una primera aproximación la minería de textos, se parte del supuesto de que el texto o los textos están disponibles ya en un fichero, denominado corpus, legible por R. En este contexto, corpus es la colección de textos con el mismo origen, por ejemplo, el corpus de las obras de un autor, que para poder manejarse requiere metadatos con detalles adicionales.","code":""},{"path":"mineria-textos.html","id":"secPREPARA","chapter":"Capítulo 38 Minería de textos","heading":"38.2.1 Preparación de los datos","text":"Una vez constituido el corpus, la primera fase es la preparación de los datos. Los textos suelen contener un cierto grado de “suciedad”, es decir, elementos que alteran o impiden el análisis. De una buena “limpieza” inicial, dependerá en gran parte la validez de los resultados que se obtengan. Entre las operaciones de “limpieza” generales figuran una serie de transformaciones cuya finalidad es evitar el recuento incorrecto de palabras, como el cambio de mayúsculas por minúsculas y la eliminación de los signos de puntuación, los números y los espacios en blanco en exceso.La siguiente operación de preparación, que tiene un importante peso en el análisis, es la eliminación de las palabras vacías (en inglés, stopwords). En la lengua todas las palabras tienen el mismo tipo de significado. Las palabras con significado léxico, como mesa o corpus, son palabras las que corresponde un concepto que se puede definir o explicar. Otras palabras, sin embargo, son palabras funcionales, cuyo contenido es puramente gramatical. Son palabras como el artículo el, la preposición de o la conjunción o: se puede explicar cómo se usan, pero definirlas asociándolas un concepto porque carecen de contenido léxico-semántico.Las palabras vacías son, con gran diferencia respecto de las palabras léxicas, las más frecuentes de la lengua, pero, dado su escaso o nulo significado léxico, en los análisis de tipo semántico, como el análisis de sentimientos o el modelado de temas, carecen de valor informativo, por lo que es conveniente eliminarlas. es aconsejable eliminarlas, sin embargo, en otros tipos de análisis, como los análisis estilométricos, donde tienen un importante valor informativo como se verá en la Sec. 38.2.4. Las palabras vacías pertenecen clases cerradas, es decir, clases de palabras con un número de elementos limitado, finito. Es posible confeccionar, por tanto, listas de palabras vacías para facilitar su eliminación. En el ejemplo de aplicación que se verá en la Sec. 38.5, se aprenderá usar estas listas y se podrá apreciar con detalle la diferente información que proporciona una tabla de frecuencias con y sin palabras vacías.","code":""},{"path":"mineria-textos.html","id":"secTOKEN","chapter":"Capítulo 38 Minería de textos","heading":"38.2.2 Segmentación del texto: tokenización","text":"La segunda fase de la minería de textos consiste en la segmentación del texto, denominada también tokenización. El texto se divide en tokens, secuencias de texto con valor informativo. De esta manera, se pasa del lenguaje natural un lenguaje formal comprensible por el software, dándole formato de ‘vector’ o ‘tabla’. Así se pueden aplicar algunas de las herramientas que se utilizan con datos numéricos para manejar el texto y obtener resúmenes y visualizaciones que muestren la información explícita contenida en él en forma de patrones recurrentes.Generalmente, los tokens son palabras, es decir, secuencias de caracteres entre dos espacios en blanco y/o signos de puntuación, pero pueden ser también oraciones, líneas, párrafos o \\(n\\)-gramas. Como se verá en el ejemplo de aplicación, un primer análisis del significado consiste en eliminar las palabras vacías y obtener las frecuencias212 de las palabras con valor informativo para responder la pregunta “¿Qué se dice?” (Silge Robinson 2017).NotaTambién puede ser útil obtener la tasa de riqueza léxica (TTR, del inglés type-token ratio). Esta mide la relación entre el número de palabras diferentes que contiene un texto (types) dividido entre las palabras totales de ese texto (tokens)213.\n\\[TTR=\\dfrac{\\text{Types}}{\\text{Tokens}}\\]","code":""},{"path":"mineria-textos.html","id":"n-gramas","chapter":"Capítulo 38 Minería de textos","heading":"38.2.2.1 \\(N\\)-gramas","text":"El análisis puede proseguir estudiando la frecuencia de los \\(n\\)-gramas, secuencias de n palabras consecutivas en el mismo orden. Se tienen así bigramas o 2-gramas (secuencias de dos palabras), trigramas o 3-gramas (secuencias de tres palabras), etc. El estudio de los \\(n\\)-gramas responde al principio de Firth: “shall know word company keeps” (Firth 1957, 11). Este principio es el fundamento del llamado análisis de colocaciones: para conocer el significado de una palabra es preciso conocer las palabras con las que aparece, el contexto relevante. En un sentido amplio, el análisis de colocaciones consiste en examinar los contextos izquierdo y/o derecho de una palabra. La segmentación en \\(n\\)-gramas permite tener en cuenta este contexto relevante que indicará, por ejemplo, que banco es, con toda probabilidad, un asiento en las secuencias banco de madera o banco en la terraza, pero lo es en secuencias como banco de peces, banco de arena, banco de inversiones, banco de datos o banco de pruebas. La división en \\(n\\)-gramas permitirá también considerar en el análisis, al menos hasta cierto punto, el peso de la ambigüedad, la negación o el distinto significado que pueden tener las palabras según el ámbito temático. Por ejemplo, la forma larga tiene el mismo significado en los bigramas falda larga, mano larga y cara larga, ni tiene tampoco el mismo valor informativo en es larga / es larga o en de larga experiencia (valor positivo) y en se hizo larga (valor negativo). En el ejemplo de aplicación (Sec. 38.5), se verá la segmentación en \\(n\\)-gramas en la práctica, y cómo la visualización de redes contribuye complementar el análisis.","code":""},{"path":"mineria-textos.html","id":"stemming-y-lematización","chapter":"Capítulo 38 Minería de textos","heading":"38.2.3 Stemming y lematización","text":"La tokenización se puede refinar mediante el stemming, o reducción de las palabras “flexionadas” su raíz, y la lematización, o extracción del lema de cada palabra. Un ejemplo de stemming sería reducir las palabras texto, textos, textual y textuales, que R cuenta como cuatro palabras diferentes, la raíz “text”. El stemming puede proporcionar un recuento más preciso en algunos casos, pero en otros, al eliminar los sufijos de las palabras, puede crear confusión. Además, como en el ejemplo anterior, las raíces pueden coincidir con palabras existentes, lo que hace que sean difíciles de interpretar y resulten extrañas si se visualizan en nubes de palabras. Con la lematización se reducen las formas flexionadas de una misma palabra al lema, que es la forma que encabeza la entrada de la palabra en el diccionario. Por ejemplo, si se quiere buscar el significado de la palabra niñas se encontrará como tal sino bajo el lema niño y si se quiere buscar iremos se tendrá que buscar el lema ir. En el caso anterior, la lematización reduciría las formas texto, textos, textual y textuales dos lemas: texto y textual. La lematización evita la dispersión de significado en varias formas, pero veces es compleja y puede conducir la pérdida de información pertinente.","code":""},{"path":"mineria-textos.html","id":"secESTILOM","chapter":"Capítulo 38 Minería de textos","heading":"38.2.4 Campos de aplicación de la minería de textos","text":"La minería de textos tiene varios campos de aplicación. Entre ellos destacan tres:El análisis de sentimientos se tratará con detalle en la Sec. 38.3 y en el ejemplo de aplicación (Sec. 38.5.4).El análisis de sentimientos se tratará con detalle en la Sec. 38.3 y en el ejemplo de aplicación (Sec. 38.5.4).El modelado de temas o tópicos (en inglés, topic modelling), como su propio nombre indica, tiene por objeto identificar los temas principales sobre los que versa el texto haciendo uso de técnicas de clasificación supervisada del campo del aprendizaje automático, como por ejemplo LDA (Latent Dirichlet Allocation). Se ilustrará en el Cap. \\(\\ref{nlp-textil}\\).El modelado de temas o tópicos (en inglés, topic modelling), como su propio nombre indica, tiene por objeto identificar los temas principales sobre los que versa el texto haciendo uso de técnicas de clasificación supervisada del campo del aprendizaje automático, como por ejemplo LDA (Latent Dirichlet Allocation). Se ilustrará en el Cap. \\(\\ref{nlp-textil}\\).La estilometría o análisis estilométrico es una aplicación de la minería de textos cuya finalidad consiste en determinar las relaciones existentes entre el estilo de los textos y los metadatos incluidos en ellos. Se utiliza principalmente en la atribución de autoría. El concepto base es el de huella lingüística, constituida por el conjunto de rasgos lingüísticos que caracterizan el estilo de un autor como un estilo individual y único y permiten identificarlo. Un punto clave es que, contrariamente lo que podría pensarse, los rasgos que conforman en mayor medida la huella lingüística son los que tienen un mayor índice de frecuencia. La mayor parte de los enfoques utilizan el vector de las “palabras más frecuentes” (MFW, por sus siglas en inglés), que son, como se ha visto antes, las palabras vacías y las palabras con significado léxico, para determinar el estilo de un autor. Esto es debido fundamentalmente que las palabras vacías se usan de manera involuntaria e inconsciente, configurando de esta manera, sin ningún tipo de filtros racionales, una clave estilística idiosincrásica (Lozano Zahonero 2020). De lo anterior se deduce fácilmente que en este tipo de análisis deben eliminarse las palabras vacías.La estilometría o análisis estilométrico es una aplicación de la minería de textos cuya finalidad consiste en determinar las relaciones existentes entre el estilo de los textos y los metadatos incluidos en ellos. Se utiliza principalmente en la atribución de autoría. El concepto base es el de huella lingüística, constituida por el conjunto de rasgos lingüísticos que caracterizan el estilo de un autor como un estilo individual y único y permiten identificarlo. Un punto clave es que, contrariamente lo que podría pensarse, los rasgos que conforman en mayor medida la huella lingüística son los que tienen un mayor índice de frecuencia. La mayor parte de los enfoques utilizan el vector de las “palabras más frecuentes” (MFW, por sus siglas en inglés), que son, como se ha visto antes, las palabras vacías y las palabras con significado léxico, para determinar el estilo de un autor. Esto es debido fundamentalmente que las palabras vacías se usan de manera involuntaria e inconsciente, configurando de esta manera, sin ningún tipo de filtros racionales, una clave estilística idiosincrásica (Lozano Zahonero 2020). De lo anterior se deduce fácilmente que en este tipo de análisis deben eliminarse las palabras vacías.En la actualidad, el análisis estilométrico se usa en ámbitos muy dispares: desde la criminología o los servicios de inteligencia para identificar los autores de mensajes o notas en casos de asesinatos, terrorismo, secuestro o acoso, por ejemplo, hasta el derecho civil o la literatura en cuestiones de derechos de autor o detección de plagio, entre muchas otras cuestiones.","code":""},{"path":"mineria-textos.html","id":"secSENTIM","chapter":"Capítulo 38 Minería de textos","heading":"38.3 Análisis de sentimientos","text":"El análisis de sentimientos (en inglés, sentiment analysis) es una aplicación de la minería de textos que tiene como finalidad la detección, extracción, clasificación, análisis y visualización de la dimensión subjetiva asociada los temas o tópicos presentes en los textos. La dimensión subjetiva comprende solo los sentimientos, sino también las emociones, sensaciones y estados afectivos y anímicos, así como las opiniones, creencias, percepciones, puntos de vista, actitudes, juicios y valoraciones. De ahí que reciba también el nombre de minería de opinión (en inglés, opinion mining) (Lozano Zahonero 2020).El análisis de sentimientos asigna esta dimensión subjetiva una polaridad, que puede ser positiva o negativa (Pang Lee 2008). Algunas técnicas añaden además una polaridad neutra. En algunos casos, el análisis de sentimientos se refina hasta llegar las emociones básicas: este subcampo del análisis de sentimientos se conoce como detección de emociones.La primera aplicación del análisis de sentimientos fue la investigación de mercados. partir del año 2000, se registra un crecimiento exponencial de textos como reseñas, chats, foros, blogs, microblogs o comentarios y mensajes de las redes sociales, en los que predomina la expresión de emociones y opiniones personales. Mediante el análisis de sentimientos se extrae de ellos información que permite conocer los gustos del consumidor y diseñar productos su medida. Esta idea se extenderá después otros ámbitos, en especial aquellos en los que predomina la comunicación persuasiva como las campañas publicitarias o políticas. Recientemente, ha empezado utilizarse también con fines predictivos y preventivos en muchas esferas: desde cuáles son los políticos, las empresas, las películas, canciones u obras literarias que obtendrán un mayor rendimiento, mejores resultados o más votos o ventas hasta cómo detectar y prevenir, por ejemplo, conductas suicidas mediante el análisis de mensajes en las redes sociales.En el análisis de sentimientos y la detección de emociones existen dos enfoques principales: el enfoque basado en el aprendizaje automático (machine learning), en el que se usan algoritmos de aprendizaje supervisado, y el enfoque semántico basado en diccionarios o lexicones. Este último enfoque es el que se verá en detalle en el ejemplo de aplicación.En R están implementados varios lexicones para el análisis de sentimientos. Dos de los más utilizados son bing, de Bing Liu y colaboradores (B. Liu 2015), y NRC, de Saif Mohammad y Peter Turney, ambos incluidos tanto en el paquete tidytext como en syuzhet (Jockers 2017). Estos lexicones tienen en común que están basados en unigramas, es decir, en palabras sueltas, y que tienen como idioma original el inglés, si bien hay versiones traducidas automáticamente distintas lenguas. La diferencia principal entre los dos lexicones es que bing clasifica las palabras de forma binaria en polaridad positiva/negativa, mientras que NRC además de la polaridad positiva/negativa permite detectar también ocho emociones básicas (ira, miedo, anticipación, confianza, sorpresa, tristeza, alegría, asco). En el ejemplo de aplicación se compararán ambos diccionarios. Como se verá, los resultados del análisis dependerán en buena medida del lexicón elegido, así como del idioma del texto y de si el lexicón se elaboró originalmente en ese idioma o es una versión traducida automáticamente de otra lengua.","code":""},{"path":"mineria-textos.html","id":"secPACKAGEStext","chapter":"Capítulo 38 Minería de textos","heading":"38.4 Minería de textos en R","text":"En R existen diversos paquetes y funciones que facilitan la minería de textos, entre los que destacan:tidytext: con la filosofía del tidyverse, puede combinarse con los conocidos paquetes dplyr, broom, ggplot2, etc. Se puede destacar la función unnest_tokens(), que automatiza el proceso de tokenización y el almacenamiento en formato tidy en un único paso.tidytext: con la filosofía del tidyverse, puede combinarse con los conocidos paquetes dplyr, broom, ggplot2, etc. Se puede destacar la función unnest_tokens(), que automatiza el proceso de tokenización y el almacenamiento en formato tidy en un único paso.tm: destaca por tener soporte back-end de base de datos integrada, gestión avanzada de metadatos y soporte nativo para leer en varios formatos de archivo.tm: destaca por tener soporte back-end de base de datos integrada, gestión avanzada de metadatos y soporte nativo para leer en varios formatos de archivo.tokenizers: incluye tokenizadores de palabras, oraciones, párrafos, \\(n\\)-gramas, tweets, expresiones regulares, así como funciones para contar caracteres, palabras y oraciones, y para dividir textos más largos en documentos separados, cada uno con el mismo número de palabras.tokenizers: incluye tokenizadores de palabras, oraciones, párrafos, \\(n\\)-gramas, tweets, expresiones regulares, así como funciones para contar caracteres, palabras y oraciones, y para dividir textos más largos en documentos separados, cada uno con el mismo número de palabras.wordcloud: permite visualizar nubes de palabras. Las palabras más frecuentes aparecen en mayor tamaño permitiendo de un vistazo obtener las palabras clave del texto.wordcloud: permite visualizar nubes de palabras. Las palabras más frecuentes aparecen en mayor tamaño permitiendo de un vistazo obtener las palabras clave del texto.quanteda: maneja matrices de documentos-términos y destaca en tareas cuantitativas como recuento de palabras o sílabas.quanteda: maneja matrices de documentos-términos y destaca en tareas cuantitativas como recuento de palabras o sílabas.syuzhet: incluye distintas funciones que facilitan el análisis de textos, en particular el análisis de sentimientos de textos literarios.syuzhet: incluye distintas funciones que facilitan el análisis de textos, en particular el análisis de sentimientos de textos literarios.gutenbergr: almacena las obras del proyecto Gutenberg214; muy útil si se quieren analizar textos literarios.gutenbergr: almacena las obras del proyecto Gutenberg214; muy útil si se quieren analizar textos literarios.","code":""},{"path":"mineria-textos.html","id":"secEJEMPLO","chapter":"Capítulo 38 Minería de textos","heading":"38.5 Ejemplo de aplicación","text":"","code":""},{"path":"mineria-textos.html","id":"declaración-institucional-del-estado-de-alarma-2020","chapter":"Capítulo 38 Minería de textos","heading":"38.5.1 Declaración institucional del Estado de Alarma 2020","text":"La “Declaración institucional del presidente del Gobierno anunciando el Estado de Alarma en la crisis del coronavirus” (en adelante, “la Declaración”), dada en La Moncloa el 13 de marzo de 2020 es el objeto de análisis. Esta se puede encontrar en el paquete CDR que acompaña este libro.\nSe le van aplicar las operaciones y técnicas mencionadas en la Sec. 38.2.","code":"\nlibrary(\"CDR\")\ndata(\"declaracion\")"},{"path":"mineria-textos.html","id":"segmentación-en-palabras-y-oraciones","chapter":"Capítulo 38 Minería de textos","heading":"38.5.2 Segmentación en palabras y oraciones","text":"Las primeras tareas del análisis son la preparación, limpieza y segmentación o tokenización de los textos, como se vió en las Sec. 38.2.1 y 38.2.2. continuación, se verá una segmentación en palabras individuales. La función tokenize_words() del paquete tokenizers prepara el texto convirtiéndolo minúsculas, elimina todos los signos de puntuación y finalmente segmenta el texto en palabras.Con la última sentencia se obtiene la longitud de la Declaración, el número de palabras utilizadas: 922.La frecuencia de cada palabra se puede obtener y presentar con el código de abajo.\nLa primera sentencia crea la tabla de frecuencias,\nla tercera la transforma en el tipo tibble, creando la columna recuento,\ny ordena la tabla de forma descendente, de mayor menor frecuencia.En la primera fila de la salida se indican las dimensiones de la tibble, por lo que se puede ver que en esta Declaración hay 390 “palabras” distintas (considera los números como palabras).El resultado son las palabras más utilizadas en el texto, que, como puede apreciarse, son palabras vacías. Esto debería sorprender porque, como ya se ha visto, estas palabras son las más frecuentes. En la siguiente Sección, se verá cómo eliminarlas para obtener datos con valor informativo.Para otras formas de segmentar el texto (oraciones, párrafos, tweets, etc.): véase ?tokenize_words. Por ejemplo, para segmentar en oraciones:Las tres primeras oraciones y la última se obtienen con el siguiente código.También podría medirse la longitud de cada oración, en número de palabras, normalmente para comparaciones con otros textos. Para ello hay que separar cada oración en palabras y obtener la longitud de cada oración, con la función sapply(), que puede verse en la Fig. 38.1.\nFigura 38.1: Número de palabras en cada oración de la Declaración\n","code":"\nlibrary(\"tokenizers\")\npalabras <- tokenize_words(declaracion)\ntokenizers::count_words(declaracion)\n#> [1] 922\nlibrary(\"tidyverse\")\ntabla <- table(palabras[[1]])\n( tabla <- tibble(palabra = names(tabla), \n                  recuento = as.numeric(tabla)) |>\n    arrange(desc(recuento)) ) \n#> # A tibble: 390 × 2\n#>    palabra recuento\n#>    <chr>      <dbl>\n#>  1 de            43\n#>  2 y             41\n#>  3 la            35\n#>  4 a             31\n#>  5 los           26\n#>  6 en            22\n#>  7 que           20\n#>  8 el            17\n#>  9 al            14\n#> 10 para          14\n#> # … with 380 more rows\noraciones <- tokenize_sentences(declaracion)\ncount_sentences(declaracion)\n#> [1] 44\noraciones[[1]][1:3] # primeras 3 oraciones\n#> [1] \"Buenas tardes.\"                                                                                                                                                                                                                  \n#> [2] \"Estimados compatriotas.\"                                                                                                                                                                                                         \n#> [3] \"En el día de hoy, acabo de comunicar al Jefe del Estado la celebración, mañana, de un Consejo de Ministros extraordinario, para decretar el Estado de Alarma en todo nuestro país, en toda España, durante los próximos 15 días.\"\noraciones[[1]][count_sentences(declaracion)] # última oración\n#> [1] \"Buenas tardes.\"\npalabras_oracion <- tokenize_words(oraciones[[1]])\nlongitud_o <- sapply(palabras_oracion, length)\nhead(longitud_o)\n#> [1]  2  2 39 33 33 32"},{"path":"mineria-textos.html","id":"análisis-exploratorio","chapter":"Capítulo 38 Minería de textos","heading":"38.5.3 Análisis exploratorio","text":"","code":""},{"path":"mineria-textos.html","id":"eliminación-de-palabras-vacías","chapter":"Capítulo 38 Minería de textos","heading":"38.5.3.1 Eliminación de palabras vacías","text":"Se llevará cabo con el paquete stopwords, que contiene listas de palabras vacías en diferentes idiomas.\nPara el ejemplo, se define una tabla con la misma estructura que la tabla de la Declaración con las 308 palabras vacías españolas que tiene el paquete:La siguiente sentencia ‘limpia’ la tabla de la Declaración quitando las palabras vacías españolas. Además, se hace uso de la función kable() para una visualización más sofisticada de la tabla (con la longitud que se desee):Tabla 38.1: Palabras más frecuentes (sin palabras vacías)El resultado, Tabla 38.1, se puede considerar el primer análisis léxico con valor informativo: la palabra más frecuente es virus, seguida de recursos y social. Se podría ver que en total hay 319 palabras vacías distintas.El método de eliminar palabras con el paquete stopwords es perfecto. Por ejemplo, va y cada (posiciones 9 y 10 de la tabla) son muy informativas. En estos casos, como se ha visto antes, se pueden utilizar listas de palabras vacías de otros paquetes como, por ejemplo tidytext o tokenizers o el listado en español propuesto por Fradejas Rueda (2022), o pueden confeccionarse listas ad hoc.","code":"\nlibrary(\"stopwords\")\ntabla_stopwords <- tibble(palabra = stopwords(\"es\")) \ntabla <- tabla  |> anti_join(tabla_stopwords)\nknitr::kable(tabla[1:10,], \n             caption = \"Palabras más frecuentes (sin palabras vacías)\")"},{"path":"mineria-textos.html","id":"nubes-de-palabras","chapter":"Capítulo 38 Minería de textos","heading":"38.5.3.2 Nubes de palabras","text":"Una manera habitual de mostrar la información de forma visual es con las denominadas nubes de palabras, acudiendo la función wordcloud() del paquete con el mismo nombre. Al contener dicha función un componente aleatorio, se fija con set.seed() (para la reproducibilidad del gráfico por parte del lector).\nFigura 38.2: Nube de palabras más frecuentes de la Declaración\nEl resultado se muestra en la Fig. 38.2. Como se puede observar, el tamaño de letra de la palabra, y en este caso también el color, están relacionados con su frecuencia.","code":"\nset.seed(12)\nlibrary(\"wordcloud\")\nwordcloud(tabla$palabra, tabla$recuento,\n          max.words = 50, colors = rainbow(3))"},{"path":"mineria-textos.html","id":"secSENTYEMO","chapter":"Capítulo 38 Minería de textos","heading":"38.5.4 Análisis de sentimientos y detección de emociones","text":"","code":""},{"path":"mineria-textos.html","id":"lexicón-bing","chapter":"Capítulo 38 Minería de textos","heading":"38.5.4.1 Lexicón bing","text":"El lexicón bing, como se ha visto en la Sec. 38.3, es uno de los repertorios léxicos para el análisis de sentimientos que se pueden encontrar en R. Es un diccionario de polaridad (positiva/negativa) cuyo idioma original es el inglés. Se puede obtener con la función get_sentiments() del paquete tidytext. Contiene 2005 palabras positivas y 4781 palabras negativas, por lo que hay un marcado sesgo hacia la polaridad negativa.Para ilustrar el uso de bing, se ha traducido al inglés (automáticamente) la Declaración. continuación se carga el texto y se genera el objeto tabla, replicando el procedimiento descrito arriba de preparación, limpieza, segmentación en palabras, eliminación de palabras vacías (obviamente, en idioma inglés).Los sentimientos positivos de la Declaración se pueden obtener con:Análogamente, se pueden obtener los sentimientos negativos. Las siete palabras más frecuentes de cada tipo que aparecen en la Declaración se presentan conjuntamente en la Tabla 38.2.","code":"\ndata(\"EN_declaracion\")\ntabla <- table(tokenize_words(EN_declaracion)[[1]])\ntabla <- tibble(word = names(tabla), \n                recuento = as.numeric(tabla))\ntabla <- tabla |> anti_join(tibble(word=stopwords(\"en\"))) |>\n  arrange(desc(recuento))\nlibrary(\"tidytext\")\npos <- get_sentiments(\"bing\") |> \n  dplyr::filter(sentiment==\"positive\")\npos_EN <- tabla |> semi_join(pos)\nknitr::kable(pos_EN)"},{"path":"mineria-textos.html","id":"lexicón-nrc","chapter":"Capítulo 38 Minería de textos","heading":"38.5.4.2 Lexicón NRC","text":"Para poder observar las similitudes y diferencias en el análisis según el lexicón elegido, se aplica también NRC la Declaración (véase la Tabla 38.2).Tabla 38.2:  Palabras más frecuentes de la Declaración utilizando bing y NRCCon el léxico NRC pueden detectarse emociones. La misma palabra puede tener asociada distintas emociones/sentimientos. En la Fig. 38.3 se puede observar la dispar frecuencia de palabras de cada tipo:\nFigura 38.3: Gráfico de barras con la frecuencia de las emociones del lexicón NRC\nEl análisis de sentimientos y la detección de emociones de la Declaración mediante NRC se puede realizar con el siguiente código, mediante el cual se obtiene la tabla de frecuencias por emociones y sentimientos:Como se ha mencionado antes, algunas palabras tienen asociados distintos sentimientos, por ejemplo, resources. La información de la tabla se puede visualizar bien con un gráfico de barras (Fig. 38.4) bien con nubes de palabras (Fig. 38.5).\nFigura 38.4: Frecuencia de emociones de la Declaración utilizando NRC\nEntre las distintas opciones para dibujar nubes de palabras para el análisis de sentimientos es interesante la que se obtiene con el paquete syuzhet dado que permite visualizar las palabras agrupadas por emociones. Su obtención requiere distintos pasos en los que primero las palabras se agrupan por emoción y después se organizan en una matriz de documentos con la función TermDocumentMatrix() del paquete tm. Finalmente la función comparison.cloud() permite visualizar el gráfico (tiene distintos argumentos opcionales que admiten distintas posibilidades). En el ejemplo que figura continuación solo se han escogido tres emociones215:\nFigura 38.5: Nube de palabras de tres emociones NRC seleccionadas\n","code":"\nemo <- get_sentiments(\"nrc\")\nemo |> ggplot(aes(sentiment)) +\n  geom_bar(aes(fill=sentiment), show.legend = FALSE)\nemo_tab <- tabla |> inner_join(emo)\nhead(emo_tab, n=7)\n#> # A tibble: 7 × 3\n#>   word          recuento sentiment\n#>   <chr>            <dbl> <chr>    \n#> 1 virus                9 negative \n#> 2 resources            7 joy      \n#> 3 resources            7 positive \n#> 4 resources            7 trust    \n#> 5 extraordinary        6 positive \n#> 6 alarm                4 fear     \n#> 7 alarm                4 negative\nemo_tab |> \n  dplyr::count(sentiment) |> \n  ggplot(aes(x=sentiment, y=n)) +\n  geom_bar(stat = \"identity\", aes(fill=sentiment), show.legend = FALSE) +\n  geom_text(aes(label = n), vjust=-0.25)\nlibrary(\"syuzhet\")\npalabras_EN2 <- get_tokens(EN_declaracion)\nemo_tab2 <- get_nrc_sentiment(palabras_EN2, lang = \"english\" )\nemo_vec <- c(\n  paste(palabras_EN2[emo_tab2$anger> 0], collapse = \" \"),\n  paste(palabras_EN2[emo_tab2$anticipation > 0], collapse = \" \"),\n  paste(palabras_EN2[emo_tab2$disgust > 0], collapse = \" \"))\nlibrary(\"tm\")\ncorpus <- Corpus(VectorSource(emo_vec))\nTDM <- as.matrix(TermDocumentMatrix(corpus))\ncolnames(TDM) <- c('anger', 'anticipation', 'disgust')\nset.seed(1)\ncomparison.cloud(TDM, random.order = FALSE,\n                 colors = c(\"firebrick\", \"forestgreen\", \"orange3\"),\n                 title.size = 1.5, scale = c(3.5, 1), rot.per = 0)"},{"path":"mineria-textos.html","id":"n-gramas-1","chapter":"Capítulo 38 Minería de textos","heading":"38.5.5 \\(N\\)-gramas","text":"El siguiente código muestra la obtención de \\(n\\)-gramas con tokenizers.Se ha procedido eliminar de los bigramas y trigramas aquellas combinaciones con al menos una palabra vacía (stopword).Se procede ahora obtener los bigramas con tidytext. Para el resto de \\(n\\)-gramas el procedimiento es análogo, haciendo las modificaciones oportunas.\nEn el último paso se ordenan por frecuencia (de mayor menor):Una forma de eliminar las palabras vacías es:","code":"\nbigramas <- tokenize_ngrams(declaracion, n = 2, \n                            stopwords = tabla_stopwords$palabra)\nhead(bigramas[[1]], n = 3)\n#> [1] \"buenas tardes\"          \"tardes estimados\"       \"estimados compatriotas\"\ntrigramas <- tokenize_ngrams(declaracion, n = 3, \n                             stopwords = tabla_stopwords$palabra)\nhead(trigramas[[1]], n = 3)\n#> [1] \"buenas tardes estimados\"       \"tardes estimados compatriotas\"\n#> [3] \"estimados compatriotas día\"\ndeclara2 <- tibble(texto = declaracion)\nbigramas <- declara2 |>\n  unnest_tokens(bigram, texto, token = \"ngrams\", n = 2) |> \n  dplyr::count(bigram, sort = TRUE)\nbigramas[1:5, ]\n#> # A tibble: 5 × 2\n#>   bigram         n\n#>   <chr>      <int>\n#> 1 todos los      6\n#> 2 de la          5\n#> 3 de los         5\n#> 4 del estado     5\n#> 5 estado de      5\nbigramas_limpios <- bigramas |>\n  tidyr::separate(bigram, c(\"word1\", \"word2\"), sep = \" \") |> \n  dplyr::filter(!word1 %in% tabla_stopwords$palabra) |> \n  dplyr::filter(!word2 %in% tabla_stopwords$palabra) |> \n  tidyr::unite(bigram, word1, word2, sep = \" \")\nbigramas_limpios[1:5, ]\n#> # A tibble: 5 × 2\n#>   bigram                       n\n#>   <chr>                    <int>\n#> 1 autoridades sanitarias       2\n#> 2 buenas tardes                2\n#> 3 disciplina social            2\n#> 4 haga falta                   2\n#> 5 ministros extraordinario     2"},{"path":"mineria-textos.html","id":"significado-y-contexto","chapter":"Capítulo 38 Minería de textos","heading":"38.5.5.1 Significado y contexto","text":"Como se ha visto en la Sec. 38.2.2, con los \\(n\\)-gramas se puede hacer un análisis de colocaciones para extraer los distintos significados y valores informativos partir del contexto. En este caso, se puede ver cómo la palabra atender cambia de sentido cuando va precedida de o sin. continuación, se filtran los bigramas cuya primera palabra es :Estos resultados se pueden utilizar para el análisis de sentimientos y la detección de emociones.","code":"\nbigramas_no <- bigramas |>\n  tidyr::separate(bigram, c(\"word1\", \"word2\"), sep = \" \") |> \n  dplyr::filter(word1 == \"no\") |> \n  dplyr::count(word1, word2, sort = TRUE)\nbigramas_no\n#> # A tibble: 3 × 3\n#>   word1 word2       n\n#>   <chr> <chr>   <int>\n#> 1 no    atiende     1\n#> 2 no    cabe        1\n#> 3 no    es          1"},{"path":"mineria-textos.html","id":"análisis-de-redes","chapter":"Capítulo 38 Minería de textos","heading":"38.5.6 Análisis de redes","text":"En esta Sección se proporcionan las instrucciones para realizar un análisis básico de redes (ver Cap. 39), utilizando los paquetes igraph y ggraph. Dada la corta extensión de la Declaración es posible obtener conclusiones.\nEn la Fig. 38.6 se pueden ver los gráficos de redes de bigramas, tanto sin palabras vacías como con ellas.\nFigura 38.6: Redes de bigramas sin palabras vacías y con ellas\n","code":"\nlibrary(\"igraph\")\nlibrary(\"ggraph\")\nset.seed(1)\ngraf_bigramas_l <- bigramas_limpios |>  \n  tidyr::separate(bigram, c(\"first\", \"second\"), sep = \" \") |> \n  dplyr::filter(n > 1) |>\n  graph_from_data_frame()\ng1 <- ggraph(graf_bigramas_l, layout = \"fr\") +\n  geom_edge_link(arrow = arrow(length = unit(4, 'mm'))) +\n  geom_node_point(size=0) +\n  geom_node_text(aes(label = name)) \ngraf_bigramas <- bigramas |>  \n  tidyr::separate(bigram, c(\"first\", \"second\"), sep = \" \") |> \n  dplyr::filter(n > 2) |>\n  graph_from_data_frame()\ng2 <- ggraph(graf_bigramas, layout = 'fr') +\n  geom_edge_link0() +\n  geom_node_point(size=0) +\n  geom_node_label(aes(label = name)) \nlibrary(\"patchwork\")\ng1+g2"},{"path":"mineria-textos.html","id":"resumen-31","chapter":"Capítulo 38 Minería de textos","heading":"Resumen","text":"En este capítulo se introduce al lector en la minería de textos, en particular:Se presentan los conceptos y tareas fundamentales de este ámbito, así como sus principales campos de aplicación.\nSe pone de relieve la importancia de la preparación de los datos y su segmentación (distintos niveles) para obtener buenos resultados, acordes con el objetivo de la investigación.Se muestra el uso de R para el análisis de textos y de sentimientos.Se presenta un ejemplo de aplicación para ilustrar las técnicas de minería de textos.Se mencionan otros análisis plausibles de minería de textos, como la estilometría o el modelado de temas (véase el Cap. \\(\\ref{nlp-textil}\\)).","code":""},{"path":"grafos.html","id":"grafos","chapter":"Capítulo 39 Análisis de grafos y redes sociales","heading":"Capítulo 39 Análisis de grafos y redes sociales","text":"José J. GalánUniversidad Complutense de Madrid\n","code":""},{"path":"grafos.html","id":"introducción-18","chapter":"Capítulo 39 Análisis de grafos y redes sociales","heading":"39.1 Introducción","text":"El origen de la teoría de grafos se debe al problema de los siete puentes de Königsberg (Paul Euler 1736), que es considerado el primer artículo sobre teoría de grafos. El problema se centra en la ciudad Königsberg en Prusia, ahora Kaliningrado (Rusia), donde existen varios puentes y el problema plantea trazar una ruta que cruce todos los puentes una única vez (ver Fig. \\(\\ref{fig:puentes}\\)). Euler mediante el uso de grafos demostró que era posible.\nFigura 39.1: Siete puentes de Königsberg, Euler (1736).\nPero, ¿qué relación tiene un concepto creado en 1736, el de grafo, con algo tan reciente como las redes sociales?. Informalmente se puede hablar de las redes sociales (RRSS) como las relaciones existentes entre personas, un hilo invisible que une las personas en relación con algo que tienen en común. En algunos casos es muy evidente porque se crean grupos específicos de personas que comparten una afición y en otros casos es menos evidente porque, por ejemplo, pueden compartir un amigo en común sin saberlo. Estos hilos “invisibles” se unen y forman una red que se puede representar como un grafo, el mismo concepto de grafo que describió Euler, y que permite establecer diferentes caminos para unir las personas que forman la red.\n","code":""},{"path":"grafos.html","id":"teoría-de-grafos","chapter":"Capítulo 39 Análisis de grafos y redes sociales","heading":"39.2 Teoría de grafos","text":"Informalmente se puede decir que un grafo es un conjunto de nodos (vértices) que pueden estar unidos por aristas (enlaces).Si se piensa en cada nodo como una persona y en cada arista como la relación que los une, entonces se podría representar mediante grafos una red social (ver Fig. \\(\\ref{fig:grafo}\\)).Antes de ver el primer ejemplo se muestran las librerías necesarias en este capítulo.En este primer ejemplo en R se puede observar cómo se obtienen los datos de una red social. El conjunto datos_facebook está incluido en el paquete CDR. continuación, se representan las relaciones de los miembros que la componen mediante un grafo.Más formalmente una red social puede modelizarse con una estructura de red invisible (relación familiar, amistad, trabajo …) que une mediante relaciones distintos actores través de sus intereses o valores comunes, estableciendo una relación personal entre individuos o grupos de individuos conectados.Existen distintos grafos dependiendo de las características de la red social representada, algunos ejemplos de estos grafos son:El grafo de amistad (ver Fig. @ref{fig:grafo-comunidades2}), donde cada nodo representa una persona y la arista conecta dos personas si dentro de la red social son amigos.El grafo de comunidades (ver Fig. @ref{fig:grafo-comunidades2}), donde también cada nodo representa una persona y la arista les conecta si dentro de la red social pertenecen la misma comunidad, entendiendo por comunidad un grupo de individuos que comparten intereses o características en común.\nFigura 39.2: Grafo de amistades y comunidades.\n","code":"\nlibrary(\"igraph\") \nlibrary(\"CDR\") \ngrafo_facebook <- graph.data.frame(datos_facebook, directed = F)\nplot(grafo_facebook, vertex.label = NA, vertex.size = 8)\namistades <- data.frame(\n  persona = c(\"A\", \"B\", \"C\", \"D\", \"E\"),  amigo = c(\"B\", \"C\", \"A\", \"E\", \"A\") )\ngrafo_amistades <- graph_from_data_frame(amistades)\ncomunidades <- data.frame(\n  persona = c(\"A\", \"B\", \"C\", \"D\", \"E\"), comunidad = c(\"1\", \"2\", \"1\", \"2\", \"1\") )\ngrafo_comunidades <- graph_from_data_frame(comunidades)\npar(mfrow=c(1,2))\nplot(grafo_amistades, vertex.label = V(grafo_amistades)$name, main=\"Grafo amistades\")\nplot(grafo_comunidades, vertex.label = V(grafo_comunidades)$name, main=\"Grafo comunidades\")"},{"path":"grafos.html","id":"elementos-de-un-grafo","chapter":"Capítulo 39 Análisis de grafos y redes sociales","heading":"39.3 Elementos de un grafo","text":"El análisis de RRSS mediante la teoría de grafos requiere conocer previamente una serie de conceptos básicos (Perez Sola 2021) que se enumeran continuación.Los vértices representan nodos que se unen mediante aristas. En una red social cada vértice representa una de las personas de dicha red, unidas en ocasiones por intereses comunes otras.Los vértices representan nodos que se unen mediante aristas. En una red social cada vértice representa una de las personas de dicha red, unidas en ocasiones por intereses comunes otras.Las aristas son las relaciones que unen los nodos. Son dirigidas (Fig.\\(\\ref{fig:grafo-dirigido-nodirigido-sencillo}\\)) si tienen un sentido definido y dirigidas (Fig. \\(\\ref{fig:grafo-dirigido-nodirigido-sencillo}\\)) en caso contrario.Las aristas son las relaciones que unen los nodos. Son dirigidas (Fig.\\(\\ref{fig:grafo-dirigido-nodirigido-sencillo}\\)) si tienen un sentido definido y dirigidas (Fig. \\(\\ref{fig:grafo-dirigido-nodirigido-sencillo}\\)) en caso contrario.El siguiente código computa un grafo dirigido:Para un grafo dirigido, basta con especificar directed = F en la función graph.data.frame():En otras RRSS, como LinkedIn, las aristas podrían representar la relación que une las personas. Las personas forman parte de un grupo con intereses comunes, formando un grafo dirigido. Pero también se pueden seguir alguien sin necesariamente ser seguido; en ese caso las relaciones se pueden representar como un grafo dirigido.Un grafo es un conjunto de vértices y aristas que se puede representar mediante \\(G = (V, E)\\), donde \\(V\\) es el conjunto de nodos o vértices del grafo y \\(E\\) es un conjunto de pares de vértices llamado arista, arco o edge.Se presenta un grafo “sencillo” en la Fig. \\(\\ref{fig:grafo-dirigido-nodirigido-sencillo}\\) (sólo se indican las aristas y R es capaz de interpretar los vértices), sobre el cual se explicará la matriz de adyacencia, grado y camino:\nFigura 39.3: Grafo dirigido, dirigido y sencillo.\nLa información recogida en un grafo también se puede expresar mediante números, organizados en una matriz denominada matriz de adyacencia, \\(A_{n\\times n}\\), lo que facilita los cálculos computacionales en grandes redes. Cada entrada de la matriz, \\(a_{ij}\\), indica el número de aristas que comparten los vértices (o nodos) -ésimo y j-ésimo (si existe relación entre ellos, entonces \\(a_{ij}=0\\)); cada fila de la matriz indica el número de aristas que comparte el vértice -ésimo con cada uno de los otros vértices. La suma de todas las entradas \\(a_{ij}\\) de una fila es el grado del vértice correspondiente.\nEn los grafos dirigidos \\(A_{n \\times n}\\) es simétrica, ya que si el vértice o nodo \\(1\\) conecta con el \\(2\\), entonces el \\(2\\) también conecta con el \\(1\\). En los grafos dirigidos, donde cada arista tiene una orientación, esto tiene por qué ocurrir: el vértice o nodo \\(1\\) puede conectar con el \\(1\\) pero al revés. En este tipo de grafos la matriz de adyacencia es simétrica.El grado o valencia de un nodo x es el numero de aristas que concurren en dicho nodo, y se representa mediante grado(x), g(x) o gr(x), lo cual en R se calcula con la función degree, siendo un vértice de grado 0 un vértice aislado. En un grafo \\(G\\) hay un grado máximo \\(\\Delta (G)\\) y un grado mínimo \\(\\delta (G)\\); el grado del grafo, g(G), es la suma de los grados de todos sus vértices. En una red social representa el número de relaciones que existen; en una red social como Facebook podría significar conocer cuántos amigos tiene cada persona.continuación se muestra la matriz de adyacencia del grafo visto previamente:Ahora muestra el grado del mismo grafo:Un camino es un conjunto de aristas recursivas. Entre dos vértices puede existir más de un camino, además puede haber varios y se puede incluir el mismo vértice en el camino más de una vez. Evidentemente, siempre habrá un camino más corto: que será aquel que menos aristas ha recorrido. Si entre todos los pares de vértices existe un camino, entonces el grafo se denomina conexo.El siguiente código se utiliza para mostrar el camino más corto entre los vértices 2 y 5 de grafo utilizado de ejemplo.","code":"\ngrafodirigido <- graph.data.frame(datos_grafos, directed = T)\ngrafonodirigido <- graph.data.frame(datos_grafos, directed = F)\ngrafo <- graph(edges = c(1, 2, 1, 3, 1, 4, 2, 4, 3, 5, 4, 5))\npar(mfrow=c(1,3))\nplot(grafodirigido, vertex.label = V(grafodirigido)$name, main=\"Grafo dirigido\")\nplot(grafonodirigido, vertex.label = V(grafonodirigido)$name, main=\"Grafo no dirigido\")\nplot(grafo,  main=\"Grafo \\\"sencillo\\\"\")\nmatriz_adyacencia <- get.adjacency(grafo, sparse = FALSE)\nmatriz_adyacencia\n#>      [,1] [,2] [,3] [,4] [,5]\n#> [1,]    0    1    1    1    0\n#> [2,]    0    0    0    1    0\n#> [3,]    0    0    0    0    1\n#> [4,]    0    0    0    0    1\n#> [5,]    0    0    0    0    0\ndegree(grafo)\n#> [1] 3 2 2 3 2\n# Camino más corto entre el nodo 2 y el 5\ncaminos <- get.shortest.paths(grafo, from = \"2\", to = \"5\")\nV(grafo)[caminos$vpath[[1]]]\n#> + 3/5 vertices, from 2a6bb3c:\n#> [1] 2 4 5"},{"path":"grafos.html","id":"procedimiento-con-r-el-paquete-igraph","chapter":"Capítulo 39 Análisis de grafos y redes sociales","heading":"39.4 Procedimiento con R: el paquete igraph","text":"Existen diversos paquetes en R para representar grafos, pero el más utilizado y popularizado, por sencillez y eficacia, es igraph (Csardi Nepusz 2006). Se trata de un paquete que permite crear y manipular grafos para analizar redes en R de forma muy sencilla (Fig. @ref{fig:grafobasico}).continuación se muestra un sencillo ejemplo sobre cómo crear un grafo dirigido con la librería igraph\nFigura 39.4: Ejemplo de grafo con igraph\nPara crear un grafo, Fig. @ref{fig:grafobasico}, partir de un data frame se ha usado la función graph_from_data_frame() con los siguientes argumentos:graph_from_data_frame(edges, directed = TRUE, vertices = nodes) donde:edges: es un data frame donde las dos primeras columnas representan una lista de aristas.edges: es un data frame donde las dos primeras columnas representan una lista de aristas.directed: es un valor lógico que indica si es un grafo dirigido o dirigido.directed: es un valor lógico que indica si es un grafo dirigido o dirigido.vertices es un data frame con los valores de los vértices o NULL.vertices es un data frame con los valores de los vértices o NULL.El siguiente código muestra las relaciones entre los actores de dos películas.\nnodes contiene el nombre de cada actor y su descripción, es imprescindible que los nombres que más adelante se introducen en edges existan en nodes. Al mismo tiempo es obligatorio declarar los nodos ya que pueden ser extraídos de las relaciones.\nedges contiene la relaciones, y , además de la película donde coinciden. Siendo este último dato descriptivo y necesario.\nFigura 39.5: Grafo representativo de la relacion de actores respecto películas\nEn la Fig. @ref{fig:grafoactores} se puede observar como el actor Jim Carrey tuvo relación con todos los actores de la red propuesta, mientras que la actriz Cameron Diaz solo participó con uno de ellos (el propio Jim Carrey).","code":"\nnodes <- data.frame(\"nodos\" = c(\"A\", \"B\", \"C\", \"D\", \"E\"))\nedges <- data.frame(\n  \"from\" = c(\"A\", \"C\", \"B\", \"A\", \"A\", \"A\"), \n  \"to\" = c(\"B\", \"D\", \"C\", \"C\", \"D\", \"E\"))\nred <- graph_from_data_frame(edges, directed = TRUE, vertices = nodes)\nplot(red, vertex.size = 50)\nnodes <- data.frame(\"actores\" = c(\n  \"Jim Carrey\", \"Arnold Swarzenneger\", \"George Cloney\",\n  \"Cameron Diaz\"\n), \"descripcion\" = c(\"actor\", \"actor\", \"actor\", \"actriz\"))\nedges <- data.frame(\n  \"from\" = c(\"Jim Carrey\", \"Jim Carrey\", \"George Cloney\", \"Jim Carrey\"),\n  \"to\" = c(\n    \"Arnold Swarzenneger\", \"George Cloney\", \"Arnold Swarzenneger\",\n    \"Cameron Diaz\"\n  ), \"pelicula\" = c(\n    \"Batman y Robin\", \"Batman y Robin\",\n    \"Batman y Robin\", \"La mascara\")\n)\nred <- graph_from_data_frame(edges, directed = F, vertices = nodes)\nplot(red, vertex.size = 50)"},{"path":"grafos.html","id":"análisis-de-influencia-en-un-grafo-aplicado-a-rrss","chapter":"Capítulo 39 Análisis de grafos y redes sociales","heading":"39.5 Análisis de influencia en un grafo aplicado a RRSS","text":"Existen paquetes para obtener información de distintas RRSS; por ejemplo, en R se puede utilizar el paquete Rfacebook para conectarse Facebook y obtener información de los contactos existentes. Para ello será necesario activar la API, Interfaz de Programación de Aplicaciones, desde https://developers.facebook.com. La información necesaria se puede encontrar en su página web https://developers.facebook.com/docs. Para ilustrar un ejemplo didáctico, sin que el lector necesite conocimientos de desarrollo para descargar los datos, se ha generado un fichero Excel que simula la relación entre amigos de una red social como podría ser Facebook generando un grafo dirigido, tipo de grafo habitual en este tipo de redes.Se incorporan los datos y se muestra su cabecera.En primer lugar, se utiliza el siguiente código para recoger los datos de un fichero CSV, con dos columnas, separadas por un espacio. Cada columna mediante un número identificador representa una persona, la unión de estas dos personas es el resultado de una relación, estas relaciones pueden visualizarse con el siguiente código.En esta ocasión se utiliza la función graph.data.frame() del paquete igraph para crear un objeto de tipo grafo, dirigido en este caso, partir de un data frame en R. Seguidamente, mediante plot() se muestra el grafo al mismo tiempo que se establecen sus propiedades. Ver Fig. \\(\\ref{fig:grafofacebook1}\\). Nótese que la estructura de los datos de entrada para construir el grafo es diferente la función graph_from_data_frame() vista en el apartado anterior, pero ambas cumple el objetivo de construir un grafo y por ello se presentan ambas opciones.\nFigura 39.6: Aplicación de igraph RRSS.\nSiguiendo con el ejemplo, se ven las relaciones de la red social, se puede observar que el número de aristas que concurren en cada vértice indica el número de personas con las que se relaciona el individuo, representado por dicho vértice. Indica, por tanto, el número de relaciones que mantiene dicho individuo.Ahora sobre el mismo ejemplo se personalizan los datos. Las RRSS son enormes y, por tanto,es útil centrarse en una subred para estudios concretos. modo de ejemplo, para focalizar este caso de estudio el ejemplo se centrará en aquéllos cuyo grado sea igual o superior 26, asignándoles su nombre.\nFigura 39.7: Aplicación de Igraph RRSS.\nEn la Fig. \\(\\ref{fig:grafofacebook3}\\) se pueden ver las relaciones entre las personas incluidas en el grafo (quién siguen y por quiénes son seguidas). Por ejemplo, Gema la sigue nadie. Leonor, otro caso extremo igual que Gema, es seguida por Aurora, Ángeles y Gabriel, pero ella sigue nadie.","code":"\ndatos_faceboook <- graph.data.frame(datos_facebook, directed = F)\n#datos_faceboook # descomentar para ver las relaciones\ngrafo_facebook <- graph.data.frame(datos_facebook, directed = T)\nplot.igraph(grafo_facebook,\n  layout = layout.fruchterman.reingold,\n  vertex.label = NA, vertex.label.cex = 1, vertex.size = 3, edge.curved = TRUE\n)\ntable(degree(grafo_facebook))\n#> \n#>  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 \n#>  1  2  4  4  7  4  8 14 19 15 14 19 22 15 11 13  9  5  3  3  4  1  4\nbad_network <- V(grafo_facebook)[degree(grafo_facebook) <= 26]\ngrafo_facebook <- delete.vertices(grafo_facebook, bad_network)\nV(grafo_facebook)$name <- c(\n  \"Gema\", \"Patricia\", \"Ramon\", \"José\", \"Maria\", \"Ángeles\",\n  \"Gabriel\", \"Javier\", \"Victor\", \"Leonor\", \"Ana\", \"Isabel\", \"Cristóbal\", \"Rosa\", \"Aurora\"\n)\nplot(grafo_facebook, vertex.size = 20)"},{"path":"grafos.html","id":"centralidad","chapter":"Capítulo 39 Análisis de grafos y redes sociales","heading":"39.5.1 Centralidad","text":"Un grafo tiene una centralidad real porque tiene coordenadas (Easley 2010), pero existen distintas medidas de centralidad que, en una red/grafo social, permitirán identificar el networking social de cada individuo, es decir, su influencia.En teoría de grafos y análisis de RRSS, el concepto de centralidad refiere la importancia o prominencia de los vértices o nodos en un determinado grafo o red social. En el caso de una red social de amigos, como, por ejemplo, Facebook, la centralidad de un nodo (persona) representa el número de amigos que tiene.Son innumerables las medidas de centralidad (generalmente normalizadas o estandarizadas) que pueden encontrarse en la literatura sobre la cuestión para determinar y comparar, de forma cuantitativa, la importancia relativa de un nodo en el conjunto de la red.\nLa centralidad es un atributo intrínseco de los nodos, sino un atributo estructural: un valor que depende de las relaciones de dicho nodo con los demás de la red. Generalmente, el nodo central suele tener la mayor centralidad, mientras que la mínima suele corresponder los nodos periféricosEn los grafos dirigidos, cuantas más aristas reciba un nodo (persona) más personas estan intentando interactuar con ella y más prestigio tendrá en la red. Pero si la interacción hacia esta persona es directa y se realiza través de un camino más largo pasando por más nodos quiere decir que su influencia es elevada, ya que más personas han recibido esa influencia.Existen diversas técnicas de obtener la centralidad (Wasserman 1995), entre las que destacamos la centralidad por intermediación y la de vector propio por ser dos conceptos diferentes en el análisis de redes. Ahora se vera más en detalle como la primera mide la influencia de un nodo en la transmisión de información, mientras que la segunda es una medida mas amplia de la centralidad global de un nodo en un grafo.La técnica de centralidad de intermediación (betweenness) se basa en el número de caminos mínimos (camino más corto entre dos vértices en un grafo ponderado) en los que un nodo esta involucrado. Por lo tanto, en una red social una persona tendrá mayor influencia cuanto mayor betweenness tenga, porque comunicará mucha información través de los nodos de la red. Si puede llegar un grupo grande, aunque sea través de un nodo quien nadie sigue, como Gema, puede alcanzar un gran nivel de viralización porque su información llegara hasta muchas personas.El siguiente código muestra la centralidad mediante intermediación correspondiente los nodos del ejemplo actualMaria es el nodo con mayor porcentaje, ello quiere decir que es quien tiene un mayor número de enlaces o conexiones con otros nodos, siendo el nodo de mayor importancia en términos de conectividad.La Centralidad de vector propio (eigenvector). Se basa en la centralidad de los nodos con los que se relaciona. En concreto la centralidad de valor propio es proporcional la suma de las centralidades de sus nodos vecinos.\nSe representa mediante \\(c_{} = \\lambda \\sum_{j} a_{ij}c_{j}\\), donde \\(\\lambda\\) es la constante de proporcionalidad y \\(a_{ij}\\) es el valor de la fila \\(\\) y la columna \\(j\\) de la matriz de adyacencia de la red social.El siguiente código muestra la centralidad mediante vector propio correspondiente los nodos del ejemplo actualObservamos que según esta centralidad, basada en el número de enlaces y conexiones con otros nodos importantes, el mayor porcentaje es obtenido por el nodo Victor. Este nodo es por tanto central en términos de conexión con otros nodos importantes de la red siendo identificado como el nodo líder o de mayor influencia de la red.","code":"\nbetweenness_centrality <- betweenness(grafo_facebook)\nbetweenness_centrality\n#>      Gema  Patricia     Ramon      José     Maria   Ángeles   Gabriel    Javier \n#>  0.000000 33.500000 30.200000 10.500000 53.300000  7.000000 37.800000 27.200000 \n#>    Victor    Leonor       Ana    Isabel Cristóbal      Rosa    Aurora \n#> 42.700000  0.000000  9.333333  0.000000  6.000000  4.666667 23.800000\neigencentrality <- eigen_centrality(grafo_facebook)$vector\neigencentrality\n#>      Gema  Patricia     Ramon      José     Maria   Ángeles   Gabriel    Javier \n#> 0.1618269 0.6090993 0.7768737 0.3148866 0.9257523 0.6787138 0.7180398 0.4714266 \n#>    Victor    Leonor       Ana    Isabel Cristóbal      Rosa    Aurora \n#> 1.0000000 0.4725741 0.7663131 0.2086397 0.7620632 0.3831565 0.7000984"},{"path":"grafos.html","id":"detección-de-comunidades","chapter":"Capítulo 39 Análisis de grafos y redes sociales","heading":"39.5.2 Detección de comunidades","text":"\nEn el análisis de una red social es importante detectar las distintas comunidades que la componen, entendiendo por comunidad un grupo de personas afines, gracias las comunidades podremos estudiar los distintos grupos que lo forman en función de sus relaciones y afinidad (Missaouri 2015).Existen diversos algoritmos en R para detectar comunidades (Borgatti 2022), pero ninguno ha demostrado que pueda actuar la perfección con todos los grafos debido la gran tipología que existe. Así, según el ejemplo anterior podemos detectar las siguientes comunidades:La detección de comunidades mediante betweenness tiene implementado en el paquete igraph un algoritmo para detectar comunidades por lo que continuación se expone.La detección de comunidades mediante walktrap es también muy utilizada en RRSS, se basa en el concepto de que las caminatas aleatorias cortas permanecen la misma comunidad y esta basado en una medida de centralidad.continuación, se presenta el código del algoritmo walktrap, mediante el cual se detectan dos comunidades:","code":"\ncommunities <- cluster_edge_betweenness(grafo_facebook)\n# head(communities, 10) # descomentar para ver las comunidades\ncommunities <- walktrap.community(grafo_facebook)"},{"path":"grafos.html","id":"representación-con-grafos","chapter":"Capítulo 39 Análisis de grafos y redes sociales","heading":"39.5.3 Representación con grafos","text":"Ahora que se ha observado la detección de comunidades se realiza su representación final mediante grafos.\n- El siguiente código muestra la red mediante un grafo de tipo Eigenvector donde se han detectado mediante este algoritmo tres comunidades. Según este método, es tan importante que tengas muchos amigos, lo importante es que tus amigos sean muy influyentes (ver Fig. \\(\\ref{fig:grafofacebook8}\\))\nFigura 39.8: Aplicación de Igraph RRSS\n\n- continuación el código que muestra la red mediante el grafo de tipo betweenness donde en la Fig.\\(\\ref{fig:grafo-Betweenness-Walktrap}\\) se observan siete comunidades.\n- Ahora se puede observar el código para obtener el grafo de tipo Walktrap. Se pueden observar dos comunidades detectadas con Walktrap, ver Fig.\\(\\ref{fig:grafo-Betweenness-Walktrap}\\)\nFigura 39.9: Grafo Betweenness y Walktrap.\n","code":"\ncl_g_network <- leading.eigenvector.community(grafo_facebook)\nplot(cl_g_network, grafo_facebook,\n  edge.arrow.size = 0.25, main =\n    \"Leading Eigenvector Community\", vertex.size = 50\n)\nbetweenness_grafo <- edge.betweenness.community(grafo_facebook)\nWalktrap_grafo <- walktrap.community(grafo_facebook, steps = 5, modularity = TRUE)\n#Debe indicarse el largo de la caminata aleatoria, se recomienda usar 5 caminatas.\npar(mfrow=c(1,2))\nplot(betweenness_grafo, grafo_facebook, edge.arrow.size = 0.25, vertex.size = 50, main=\"Grafo Betweenness\")\nplot(Walktrap_grafo, grafo_facebook, edge.arrow.size = 0.25,\nvertex.label = (grafo_facebook)$name, vertex.size = 50, main=\"Grafo Walktrap\")"},{"path":"grafos.html","id":"entorno-social-en-el-universo-cinematográfico-marvel","chapter":"Capítulo 39 Análisis de grafos y redes sociales","heading":"39.6 Entorno social en el universo cinematográfico Marvel","text":"Se va analizar el Universo Cinematográfico de Marvel como una red social donde cada héroe tiene un grado de relación con otro. Se realiza este estudio utilizando los datos marvel_edges del paquete CDR que contiene dos columnas formateadas para representar la red social del Universo Marvel, donde la primera columna es el nombre de un personaje del Universo cinematográfico Marvel y la segunda el nombre de otro con el que coincide en alguna película, representado cada relación una arista entre dos nodos. Con estos datos se forma el grafo correspondiente su red social, donde las coincidencias en la misma película entre héroes representan relaciones y cada héroe un nodo. En el siguiente código cargamos el fichero y formamos el grafo_marvel.\nFigura 39.10: Grafo original sobre el Universo cinematografico Marvel\nSon muchas las relaciones mostradas en la \\(\\ref{fig:marvel1}\\), por lo que continuación se muestran aquellos héroes que tienen dos o más relaciones. Esto crea un grafo más visible, incluido un subgrafo de dos nodos (véase \\(\\ref{fig:grafo-relaciones-comunidades}\\)).continuación, se presenta el número de relaciones que tiene cada nodo, cada héroe, mediante la centralidad de grado en R usando la función degree() del paquete igraph. Se puede apreciar como Iron Man y Capitán América son quienes mayor número de relaciones tiene y por lo tanto quienes más gozan de popularidad e influencia.Ahora, se analizan las comunidades que tiene, identificando cada una con un color distinto. En esta ocasión se utiliza el algoritmo Louvain, se considera\nel algoritmo más popular por su fácil interpretación, flexibilidad, alta calidad de las comunidades y eficiencia en tiempo de ejecución. El resultado podemos verlos en la \\(\\ref{fig:grafo-relaciones-comunidades}\\).\nFigura 39.11: Grafo de relaciones y comunidades de héroes.\n\nFigura 39.12: Distintas comunidades\ncontinuación, el análisis se centra en la comunidad con más relaciones (Fig. @ref{fig:grafo-relaciones-color}). Para ello, se identifica la comunidad más grande y se genera un subgrafo, mediante la función induced_subgraph() del paquete igraph, para su visualización.La Fig. @ref{fig:grafo-relaciones-color} muestra el tamaño de los héroes según el número de relaciones y con un color distinto por cada héroe, lo que hace que el gráfico final sea más llamativo y fácil de interpretar.\nFigura 39.13: Grafo estandar con más relaciones () y grafo adaptado con color y tamaño (b)\nLas comunidades pueden ser representadas por otros algoritmos. continuación se representan los algoritmos ya comentados, Betweenness, el cual es útil cuando los nodos que conectan distintas comunidades son los más importantes y se quiere asegurar que se incorporen en una comunidad y el algoritmo Walktrap, el cual detecta eficazmente comunidades de tamaños similares.\nFigura 39.14: Comunidades según el algoritmo utilizado\n","code":"\ngrafo_marvel <- graph.data.frame(marvel_edges, directed = F)\nplot(grafo_marvel)\nnodos_poca_realacion <- which(degree(grafo_marvel) < 2)\ngrafo <- delete.vertices(grafo_marvel, nodos_poca_realacion)\ngrado_nodos <- degree(grafo)\n#sort(grado_nodos)# descomentar para ver la centralidad de grado de los héroes.\ncomunidades <- cluster_louvain(grafo)\npar(mfrow=c(1,2))\nplot(grafo, vertex.label = V(grafo)$name, main=\"Relaciones de héroes\")\nplot(grafo, vertex.color = comunidades$membership, vertex.label = V(grafo)$name, main=\"Comunidades de héroes\")\ncomunidades <- cluster_louvain(grafo)\nnum_comunidades <- length(unique(comunidades$membership))\n\nfor (i in 1:num_comunidades) {\n  nodos_comunidad <- which(comunidades$membership == i)\n  subgrafo <- induced_subgraph(grafo, nodos_comunidad)\n}\ncomunidades <- cluster_louvain(grafo)\ntamanos_comunidades <- table(comunidades$membership)\nindice_comunidad_max <- which.max(tamanos_comunidades)\nnodos_comunidad_max <- which(comunidades$membership == indice_comunidad_max)\n\nsubgrafo <- induced_subgraph(grafo, nodos_comunidad_max)\n# Calcular el grado de cada nodo\ngrados <- degree(subgrafo)\n\n# Ajustar el tamaño de los nodos proporcionalmente a su grado\ntamaños <- 80 * grados / max(grados)\n\n# Generar un vector de colores aleatorios\ncolores <- sample(colors(), vcount(subgrafo), replace = TRUE)\npar(mfrow=c(1,2))\nplot(subgrafo, main=\"(a)\")\nplot(subgrafo, vertex.color = colores, vertex.size = tamaños, main=\"(b)\")\n# edge_betweenness\ncomunidades <- cluster_edge_betweenness(grafo)\n# walktrap\ncomunidades <- cluster_walktrap(grafo)"},{"path":"grafos.html","id":"reumen","chapter":"Capítulo 39 Análisis de grafos y redes sociales","heading":"Reumen","text":"Este capítulo ha introducido la teoría de grafos y su relación con las RRSS, destacando:Los conceptos elementales de la teoría de grafos: vértice, arista, gráfico dirigido y dirigido, grado y camino entre otros.Los conceptos elementales de la teoría de grafos: vértice, arista, gráfico dirigido y dirigido, grado y camino entre otros.El procedimiento con R para el análisis de grafos través del paquete igraph, mostrando la estructura necesaria para componer un grafo.El procedimiento con R para el análisis de grafos través del paquete igraph, mostrando la estructura necesaria para componer un grafo.El análisis de influencia en un grafo aplicado RRSS, introduciendo los conceptos de centralidad y comunidad.El análisis de influencia en un grafo aplicado RRSS, introduciendo los conceptos de centralidad y comunidad.El análisis del entorno social de los personajes del universo cinematográfico Marvel, aplicando los conocimientos teoricos presentados lo largo del capítulo.El análisis del entorno social de los personajes del universo cinematográfico Marvel, aplicando los conocimientos teoricos presentados lo largo del capítulo.","code":""},{"path":"datos-espaciales.html","id":"datos-espaciales","chapter":"Capítulo 40 Trabajando con datos espaciales","heading":"Capítulo 40 Trabajando con datos espaciales","text":"Gema Fernández-Avilés216Universidad de Castilla-La Mancha\n","code":""},{"path":"datos-espaciales.html","id":"introducción-19","chapter":"Capítulo 40 Trabajando con datos espaciales","heading":"40.1 Introducción","text":"Los datos espaciales, también conocidos como datos geográficos o datos geo-referenciados, son aquellos datos relacionados o que contienen información de una localización o área geográfica de la superficie de la Tierra. El primer análisis de datos geoespaciales fue hecho por el médico John Snow en 1854. Éste produjo un famoso mapa que muestra las muertes causadas por un brote de cólera (que mató 127 personas en 3 días) en Soho, Londres así como la ubicación de las bombas de agua en el área (Fig. 40.1). Snow descubrió que había un agrupamiento significativo de muertes alrededor de una determinada bomba, y al quitar la manija de la bomba se detuvo el brote. Los datos con los que trabajó Snow y aquellos que contienen coordenadas son considerados datos espaciales.\nFigura 40.1: Mapa de cólera en Londres según Snow. Fuente: Wikipedia\nEl análisis espacial de Snow es considerado el antecedente más antiguo conocido de la ciencia de datos (Baumer, Kaplan, Horton (2021)): () la información clave se obtuvo mediante la combinación de tres fuentes de datos (las muertes por cólera, las ubicaciones de las bombas de agua y el mapa de calles de Londres); (ii) se puede crear un modelo espacial directamente partir de los datos y (iii) el problema solo se resolvió cuando la evidencia basada en datos se combinó con un modelo plausible que explicaba el fenómeno físico. Es decir, Snow era médico y su conocimiento sobre la transmisión de enfermedades fue suficiente para convencer sus colegas de que el cólera se transmitía por el aire.","code":""},{"path":"datos-espaciales.html","id":"estadística-para-datos-espaciales","chapter":"Capítulo 40 Trabajando con datos espaciales","heading":"40.1.1 Estadística para datos espaciales","text":"El área que se encarga de estudiar y analizar los datos espaciales es la estadística espacial o la estadística para datos espaciales (N. . C. Cressie (1993), José-Marı́Montero, Fernández-Avilés, Mateu (2015)).Debido que los datos espaciales surgen en una gran variedad de campos y aplicaciones, también hay una gran variedad de tipos de datos espaciales, estructuras y escenarios (Schabenberger Gotway 2005, 6). La Fig. 40.2 representa la clasificación de datos espaciales proporcionada por N. . C. Cressie (1993) basada en la naturaleza del dominio espacial en estudio. Cressie distingue tres tipos de datos espaciales:datos geoestadísticos,datos geoestadísticos,datos de patrones de puntos ydatos de patrones de puntos ydatos lattice o reticulares. datos lattice o reticulares. El estudio de los datos geoestadíticos se aborda en el Cap.41, el análisis de los datos lattice se lleva cabo en el Cap. 42 dedicado la Econometría espacial y los datos de patrones de puntos se analizan en el Cap. 43.\nFigura 40.2: Clasificación de datos espaciales propuesta por Cressie (1993)\n","code":""},{"path":"datos-espaciales.html","id":"conceptos-clave","chapter":"Capítulo 40 Trabajando con datos espaciales","heading":"40.2 Conceptos clave","text":"Visto el contexto original de los datos espaciales y antes de entrar en detalle en su análisis, se debe tener en cuenta una serie de conceptos clave. La Fig. 40.3, representa la localización de los accidentes de tráfico registrados en la ciudad de Madrid durante el año 2020. Sin embargo, tal representación aporta información útil para su análisis. Por ejemplo, sería interesante añadir un mapa de carreteras junto con la localización de los accidentes.\nFigura 40.3: Accidentes de Tráfico en Madrid (2020)\nAdemas de las coordenadas, en la representación de geodatos es importante el marco o contexto espacial, así como el conocimiento del () Sistema de referencia de coordenadas o Coordinate reference system (CRS) en el que están goe-referenciadas o proyectadas las coordenadas y (ii) el tipo de datos con el que se está trabajando: vectores o ráster.\nFigura 40.4: Accidentes de tráfico en Madrid proyectados y con mapa de carreteras (2020)\nLa Fig. 40.4 permite observar ciertos patrones en la ocurrencia de accidentes. Por ejemplo, apenas se producen accidentes en la Casa de Campo o en el Monte del Pardo, y parece observarse cierta concentración en la ciudad y en las autopistas de salida de la ciudad.","code":"\nlibrary(\"CDR\")\nlibrary(\"tidyverse\")\nggplot(data = accidentes2020_data,\n       aes(x = coordenada_x_utm, y = coordenada_y_utm)) + \n  geom_point(col=\"blue\", size = 0.1, alpha = 0.3) +\n  coord_fixed()\nlibrary(\"sf\")\naccidentes2020_sf <- st_as_sf(accidentes2020_data,\n  coords = c(\"coordenada_x_utm\", \"coordenada_y_utm\"),\n  crs = 25830 # proyección ETRS89/ UTM zone 30N. Área de uso: Europa  \n  )  \n\nlibrary(\"mapSpain\")\nmadrid <- esp_get_munic(munic = \"^Madrid$\") |>\n  st_transform(25830) \n\n# descara imagen de un de mapa estático de las carreteas de Madrid\ntile <- esp_getTiles(madrid, \"IDErioja\", zoommin = 2)  \n\nggplot() +\n  tidyterra::geom_spatraster_rgb(data = tile) +\n  geom_sf(data = accidentes2020_sf, \n    col = \"blue\", size = 0.1, alpha = 0.3) +\n  coord_sf(expand = FALSE)"},{"path":"datos-espaciales.html","id":"CRS","chapter":"Capítulo 40 Trabajando con datos espaciales","heading":"40.2.1 Sistema de referencia de coordenadas","text":"Los CRS permiten identificar con exactitud la posición de los datos sobre el globo terráqueo. Cuando se trabaja con datos espaciales provenientes de distintas fuentes de información es necesario comprobar que dichos datos se encuentran definidos en el mismo CRS. Ésto se consigue transformando (o proyectando) los datos un CRS común. Una buena referencia para profundizar este tema es el Cap. 2 de E. Pebesma Bivand (2022b).En la Fig. 40.5 se muestran los puertos en un mapa mundial. Todos los vienen representados por el punto rojo. ¿qué se debe? que los datos están en distintos CRS.\nFigura 40.5: Localización de los puertos en el mapamundi (distinto CRS en los puertos y el mapa)\nLos dos tipos de CRS que existen se describen continuación:Geográficos: aquellos en los que los parámetros empleados para localizar una posición espacial son la latitud (Norte-Sur [-90º,90º]) y la longitud (Este-Oeste [-180º,180º]). Están basadod en la geometría esférica. En este caso las distancias entre dos puntos son distancias angulares.Geográficos: aquellos en los que los parámetros empleados para localizar una posición espacial son la latitud (Norte-Sur [-90º,90º]) y la longitud (Este-Oeste [-180º,180º]). Están basadod en la geometría esférica. En este caso las distancias entre dos puntos son distancias angulares.Proyectados: permiten reducir la superficie de la esfera terrestre (3D) un sistema cartesiano (2D). Para ello, es necesario transformar las coordenadas longitud y latitud en coordenadas cartesianas \\(X\\) e \\(Y\\). La unidad de distancia, habitualmente, es el metro.Proyectados: permiten reducir la superficie de la esfera terrestre (3D) un sistema cartesiano (2D). Para ello, es necesario transformar las coordenadas longitud y latitud en coordenadas cartesianas \\(X\\) e \\(Y\\). La unidad de distancia, habitualmente, es el metro.Tras proyectar los puertos al mismo CRS que el mapamundi utilizando la proyección de Robinson (la proyección cartográficas más convencional para mapamundis), la Fig. 40.6 muestra adecuadamente el mapa de la Fig. 40.5.\nFigura 40.6: Localización de los puertos en el mapa mundi (mismo CRS puertos y mapa)\n¿Qué proyección uso? El CRS adecuado para cada análisis depende de la localización y el rango espacial de los datos. El paquete crsuggest (Walker 2022) facilita la labor, sugiriendo la escala de estudio o el tipo de análisis más adecuado para cada zona.","code":"\nlibrary(\"giscoR\")\n\npaises <- gisco_get_countries()\npuertos <- gisco_get_ports()\npaises_robin <- st_transform(paises, st_crs(\"ESRI:54030\")) #Proyección Robinson\n\nplot(st_geometry(paises_robin), main = \" \")\nplot(st_geometry(puertos), add = TRUE, col=\"2\", pch=20, lwd=2.5)\nst_crs(puertos) == st_crs(paises_robin) # Comprueba CRS\n#> [1] FALSE\npuertos_robin <- st_transform(puertos, st_crs(paises_robin))\nplot(st_geometry(paises_robin), main = \" \")\nplot(st_geometry(puertos_robin), add = TRUE, col=4, pch=20)"},{"path":"datos-espaciales.html","id":"formatos","chapter":"Capítulo 40 Trabajando con datos espaciales","heading":"40.2.2 Formatos de datos espaciales","text":"En el ámbito del análisis espacial, el formato de datos espaciales se puede clasificar en función del modelo de datos. Se pueden distinguir dos tipos de modelos de datos (Lovelace, Nowosad, Münchow 2019): vectoriales y ráster217.","code":""},{"path":"datos-espaciales.html","id":"vec","chapter":"Capítulo 40 Trabajando con datos espaciales","heading":"40.2.2.1 Datos de vectores","text":"Este modelo está basado en puntos georeferenciados. Los puntos pueden representar localizaciones específicas, como la localización de los Hospitales y Centros de Salud de la ciudad de Toledo (Fig. 40.7).\nFigura 40.7: Hospitales y Centros de Salud en Toledo\nLos puntos también pueden estar conectados entre sí, de manera que formen geometrías más complejas, como líneas y polígonos.En la Fig. 40.8, el río Tajo está representado como una línea (tajo, sucesión de puntos unidos entre sí) y la ciudad de Toledo como un polígono (toledo, línea de puntos cerrada formando un continuo).\nFigura 40.8: Datos vector: Puntos, líneas y polígonos\nLas extensiones más habituales de los archivos que contienen datos de vectores se muestran continuación:Ficheros con datos vectorESRI Shapefile surgió como uno de los primeros formatos de intercambio de datos geográficos y en la actualidad es, quizá, el formato más empleado. Sin embargo, tiene una serie de limitaciones: es un formato multiarchivo y el CRS es opcional.","code":"\nggplot() +\n  geom_sf(data = hosp_toledo, \n          aes(fill = \"Hospitales y Centros Sanitarios\"), \n          color = \"blue\") +\n  labs(title = NULL, fill = NULL) +\n  theme_minimal() +\n  theme(legend.position = \"right\")\nggplot(toledo) +\n  geom_sf(fill = \"cornsilk2\") +\n  geom_sf(data = tajo, col = \"lightblue2\", lwd = 2, alpha = 0.7) +\n  geom_sf(data = hosp_toledo, col = \"blue\") +\n  coord_sf(xlim = c(-4.2, -3.8), ylim = c(39.8, 39.95)) +\n  theme_minimal()"},{"path":"datos-espaciales.html","id":"raster","chapter":"Capítulo 40 Trabajando con datos espaciales","heading":"40.2.2.2 Datos ráster","text":"Los datos raster son datos proporcionados en una rejilla de píxeles (regulares o ) denominada matriz. El caso más popular de un ráster es una fotografía, donde la imagen se representa como una serie de celdas, determinadas por la resolución de la imagen, es decir, el tamaño del píxel (por ejemplo, 5 x 5 unidades, si es regular, 5 x 10 unidades, si es irregular) y el valore del pixel (RGB, por ejemplo) que determina el color que presenta cada uno de estos píxeles. En el ámbito de los datos espaciales, un archivo ráster está formado por una malla de píxeles georreferenciada, tal y como muestra la Fig. 40.9. Aquí se visualiza el conjunto de datos elev dentro del paquete CDR que representa los datos de la altitud de la provincia de Toledo en metros.\nFigura 40.9: Datos ráster. Altitud de la provincia de Toledo\nEn la Fig. 40.9, el objeto ráster elev\ntiene únicamente una capa. Eso implica que cada píxel tiene asociado un único valor, en este caso, la altitud media del terreno observado. Las extensiones más habituales de los archivos que contienen datos ráster se muestran continuación:Ficheros con datos raster","code":"\nlibrary(\"terra\")\nelev <- rast(system.file(\"external/Toledo_DEM.asc\", package = \"CDR\"))\nplot(elev, main = \" \")\npols <- as.polygons(elev, dissolve=FALSE)\nplot(pols, add = TRUE, border = \"grey90\")\nplot(st_geometry(Tol_prov), add = TRUE)"},{"path":"datos-espaciales.html","id":"primer-mapa","chapter":"Capítulo 40 Trabajando con datos espaciales","heading":"40.3 Mi primer mapa","text":"Definidos los elementos clave de los datos espaciales se llevará cabo la representación en un mapa de la distribución municipal de la renta neta per cápita (renta_municipio_data) por municipio (municipios) en el periodo 2019 en España218.\nLos datos están incluidos en el paquete CDR.Ambos conjuntos deben tener, al menos, un campo en común, codigo_ine en este caso, para su unión.\nFigura 40.10: Distribución de la renta neta media por persona (€) en 2019\nLa Fig. 40.10 presenta un mapa temático o de coropletas, es decir, una visualización sencilla de cómo varía la distribución de una variable (en este caso la renta neta media por persona) en un área geográfica (España). Adicionalmente, una serie de elementos gráficos característicos de los objetos espaciales puede verse en la información contenida en el objeto munis_renta: los datos son de tipo vector, el tipo de geometría es MULTIPOLYGON, el CRS es ETRS89 y una leyenda explica el significado de la variable.","code":"\nlibrary(\"CDR\")\nlibrary(\"sf\")\nmunis_renta <- municipios |>\n  left_join(renta_municipio_data) |>   # une datasets\n  select(name, cpro, cmun, `2019`)     # selecciona variables\n#> Joining, by = \"codigo_ine\"\nggplot(munis_renta) +\n  geom_sf(aes(fill = `2019`), color = NA) +\n  scale_fill_continuous(\n    labels = scales::label_number(\n      big.mark = \".\", decimal.mark = \",\", suffix = \" €\" )) +\n  theme_minimal()\nhead(munis_renta)[1:3, ]\n#> Simple feature collection with 3 features and 4 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension: XY\n#> Bounding box: xmin: -3.140179 ymin: 36.73817 xmax: -2.741701 ymax: 37.24562\n#> Geodetic CRS: ETRS89\n#> name cpro cmun 2019 geom\n#> 1 Abla 04 001 10192 MULTIPOLYGON (((-2.775594 3...\n#> Abrucena   04  002 10021 MULTIPOLYGON (((-2.787566 3...\n#> Adra   04  003  8192 MULTIPOLYGON (((-3.051988 3..."},{"path":"datos-espaciales.html","id":"no-mentir","chapter":"Capítulo 40 Trabajando con datos espaciales","heading":"40.4 ¿Cómo (no) mentir con la visualización?","text":"Si se realiza un mapa de coropletas como el de la Fig. 40.10, puede que la información aparezca distorsionada. Algunas consideraciones básicas en visualización son:La escala de color.La escala de color.La distribución de los datos.La distribución de los datos.La definición de intervalos.La definición de intervalos.¿Cómo es la distribución de la variable renta? La variable sigue una distribución Normal (la renta sigue una distribución Gamma219), y, si el objetivo es mostrar patrones espaciales de la variable, para una mejor representación será necesario dividir los datos en clases con el paquete classInt (Bivand 2020). De entre las distintas posibilidades que ofrece la función classIntervals(), se utiliza el método de Fisher-Jenks, que consiste en un mapa de cortes naturales que utiliza un algoritmo lineal para agrupar observaciones de modo que se maximice la homogeneidad dentro del grupo. Este algorímo está desarrollado específicamente para la clasificación de datos espaciales y su visualización en mapas. Además, se eliminan los municipios sin datos (sombreados en color gris) y se elige una escala de color adecuada. El mapa de la Fig. 40.11 proporciona ahora una visualización adecuada de la variable renta.\nFigura 40.11: Renta neta per cápita (€) por tramos según Fisher-Jenks\n","code":"\nmunis_renta_clean <- munis_renta |>\n  filter(!is.na(`2019`))\n\n# crea Fisher-Jenks clases\nlibrary(classInt)\nfisher <- classIntervals(munis_renta_clean$`2019`,\nstyle = \"fisher\", n = 10\n)\n\nggplot(munis_renta_clean) +\n  geom_sf(aes(fill = cut(`2019`, fisher$brks)), color = NA) +\n            scale_fill_viridis_d(option= \"A\" , \n                                 labels= scales::label_number(suffix= \"€\")) +\nguides(fill = guide_colorsteps()) +\n  labs(fill= \"Fisher-Jenks\") +\ntheme_minimal()"},{"path":"datos-espaciales.html","id":"mapas-espacio-temporales","chapter":"Capítulo 40 Trabajando con datos espaciales","heading":"40.5 Mapas espacio-temporales","text":"La dimensión temporal es cada vez más importante en el ámbito espacial, por ello, es importante representar en el tiempo los procesos espaciales. La Fig. 40.13 representa la temperatura mínima registrada en España del 6 al 10 de Enero de 2021, CDR::tempmin_data, durante la Borrasca Filomena.La primera pregunta se debe formular es: ¿tengo el CRS de las estaciones de monitoreo en la misma proyección que el contorno de España?Comprobado el CRS, es habitual representar las coordenadas con las que se trabaja. La Fig. 40.12 muestra la localización de las estaciones de monitoreo en España que registran la temperatura.\nFigura 40.12: Estaciones de AEMET en la Península Ibérica\nPor último, se representa el mapa espacio-temporal con la función gglot() indicando en el argumento facet_wrap() la dimension tempporal.NotaLos paquetes tmap (Tennekes 2018) y mapsf (Giraud 2022) son referentes para mapas temáticos y pueden utilizarse como alternativa.\nFigura 40.13: Temperatura mínima en España (6-10 enero 2021)\n","code":"\ntmin_sf <- st_as_sf(tempmin_data, \n  coords = c(\"longitud\", \"latitud\"),\n  crs = 4326 # coordenadas geográficas longitud/latitud WGS84\n  ) \n\nesp <- esp_get_ccaa() |> # sf objeto, contorno de España\n        filter(ine.ccaa.name != \"Canarias\") # excluye Canarias\nst_crs(tmin_sf) == st_crs(esp)\n#> [1] FALSE\nesp2 <- st_transform(esp, st_crs(tmin_sf))\nst_crs(tmin_sf) == st_crs(esp2)\n#> [1] TRUE\nggplot(esp2) +\n  geom_sf() +   \n  geom_sf(data = tmin_sf) +\n  theme_light()\n# definición de intervalos\ncortes <- c(-Inf, seq(-20, 20, 2.5), Inf)\ncolores <- hcl.colors(15, \"PuOr\", rev = TRUE)\n\ntmin_sf_sptem <- tmin_sf |>\n  arrange(fecha, desc(tmin))\n\nggplot() +\n  geom_sf(data = esp2, fill = \"grey95\") +\n  geom_sf(data = tmin_sf, aes(color = tmin), size=3, alpha= .7) +\n  facet_wrap(vars(fecha),ncol = 3) + \n  labs(color = \"Temp. mím\") +\n  scale_color_gradientn(\n    colours = colores,\n    breaks = cortes,\n    labels = ~str_c(. , \"º\"),\n    guide = \"legend\")"},{"path":"datos-espaciales.html","id":"mapas-interactivos","chapter":"Capítulo 40 Trabajando con datos espaciales","heading":"40.6 Mapas interactivos","text":"\nEl desarrollo de la informática ha propiciado también el desarrollo de la geocomputación, que está relacionada con los desarollos webs, y permite, entre otras cosas, la representación de mapas interactivos.modo de ejemplo, el mapa de la Fig. 40.14 representa el mapa Fig. 40.1 de forma interactiva con la librería leaflef. Estos mapas dinámicos, ampliables y desplazables, son más informativos que los mapas estáticos y, además, son una alternativa que pueden proporcionar una experiencia diferente y una mayor interacción al usuario.\nFigura 40.14: Mapa interactivo de las muertes por cólera en Londres según Snow en 1854\n","code":"\nlibrary(\"leaflet\")\nlibrary(\"isdas\")\ndata(\"snow_deaths\")\ndata(\"snow_pumps\")\n\n## crea mapa interactivo\nsnow_map <- leaflet() |>\n  setView(lng = -0.136, lat = 51.513, zoom = 16) |>\n  addTiles() |>\n  addMarkers( data = snow_deaths, ~long, ~lat,\n    clusterOptions = markerClusterOptions(),\n    group = \"Deaths\" ) |>\n  addMarkers(data = snow_pumps, ~long, ~lat,\n    group = \"Pumps\" )\nsnow_map"},{"path":"datos-espaciales.html","id":"reumen-1","chapter":"Capítulo 40 Trabajando con datos espaciales","heading":"Reumen","text":"Los datos espaciales son aquellos que contienen información de una zona geográfica de la tierra. Vienen definidos por coordenadas y por un sistema de referencia de coordenadas que debe tenerse en cuenta para su representación.Existen dos tipos de formatos de datos: vector y ráster.Los datos espaciales pueden clasificarse en: geoestadísticos, reticulares y puntuales.","code":""},{"path":"geo.html","id":"geo","chapter":"Capítulo 41 Geoestadística","heading":"Capítulo 41 Geoestadística","text":"Gema Fernández-Avilés\\(^{}\\) y José-María Montero\\(^{}\\)\\(^{}\\)Universidad de Castilla-La Mancha","code":""},{"path":"geo.html","id":"introducción-20","chapter":"Capítulo 41 Geoestadística","heading":"41.1 Introducción","text":"El término “geoestadística” apareció por primera vez en Matheron (1962), y en él “geo” enfatiza la referencia las Ciencias de la Tierra,\nextendiendo el ámbito de la estadística tradicional, cuyo objetivo es el uso de métodos probabilísticos-inferenciales220, con la incorporación del componente geográfico. La geoestadística estudia los fenómenos regionalizados, que son aquellos que: Se extienden en el espacio, siendo el dominio espacial, \\(D\\), continuo (se puede observar en cualquiera de sus puntos) y fijo (las ubicaciones observadas son estocásticas; se seleccionan, por el procedimiento que sea, juicio del investigador)221.Presentan una organización o estructura debida la dependencia espacial existente. El objetivo fundamental de la geoestadística es sacar provecho de la dependencia espacial existente para llevar cabo predicciones (interpolaciones) óptimas en ubicaciones o áreas de interés (en este sentido se habla de predicciones puntuales o por bloques, respectivamente), o la realización de mappings sobre todo el dominio o parte de él. Al ser \\(D\\) continuo, se puede hacer una representación exhaustiva del fenómeno, pero sí se puede reconstruir partir de las observaciones disponibles. Las consecuencias de utilizar la estadística clásica, que considera la dependencia espacial, cuando la hay, son muy graves y pueden verse en José-Marı́Montero, Fernández-Avilés, Mateu (2015).El ámbito de aplicación de la geoestadística es enorme: minería, industria petrolífera, geología, meteorología, control de la calidad del aire, ecología, epidemiología, salud pública, criminología, economía, etc. Así, por ejemplo, en el ámbito del control de la calidad del aire en las grandes urbes, la concentración de ozono en aire se mide en una serie de estaciones de seguimiento, y partir de dichas mediciones se reproduce el comportamiento del proceso sobre toda la urbe.En conclusión, las dos partes del análisis geoestadístico son: el análisis estructural de la dependencia espacial y la predicción (que se suele acompañar del calificativo “krigeada”). Pero antes de estudiarlas, detengámonos en algunos preliminares.\n","code":""},{"path":"geo.html","id":"preliminares-geo","chapter":"Capítulo 41 Geoestadística","heading":"41.2 Preliminares","text":"Dado que los procedimientos geoestadísticos pueden ser aplicados directamente sobre los fenómenos regionalizados como tales, porque son realidades físicas, se necesita una descripción matemática de los mismos la que puedan ser aplicados: la variable regionalizada (v.r.) o regionalización, definida en un espacio geográfico, y que se supone que mide y representa correctamente dicho fenómeno.\nFormalmente, cuando \\(\\mathbf{s}\\) recorre \\(D\\), el conjunto \\(z(\\mathbf{s}), \\mathbf{s}\\D,\\) se denomina v.r., siendo \\(z(\\mathbf{s_i}), =1,2,3,...\\) una colección de valores regionalizados. Desde la perspectiva probabilística, cada uno de los valores que toma v.r. puede interpretarse como el resultado de un mecanismo aleatorio, la variable aleatoria, v.. (). Si se tomasen valores regionalizados en todos los puntos del dominio, \\(D\\), es decir, si se considerase v.r., ésta podría ser vista como un conjunto infinitamente grande de v.., una en cada punto de \\(D\\), que se conoce como función aleatoria (f..), proceso estocástico o campo aleatorio espacial, \\(Z(\\mathbf{s}), \\mathbf{s}\\D\\), donde \\(Z\\) representa el fenómeno de interés. Pues bien, v.r. se interpreta como una realización de una f.. espacial, y esta es una decisión metodológica clave en geoestadística.\nEs importante tener en cuenta que, \\(()\\) frecuentemente, v.r. es muy irregular escala local, lo que impide su representación mediante una función determinista; y \\((ii)\\) muestra cierta organización o estructura espacial. La interpretación de v.r. como una realización de una f.. espacial permite considerar estos dos aspectos:En cada localización \\(\\mathbf{s}\\), \\(Z(\\mathbf{s})\\) es una v.. (de ahí el aspecto errático).En cada localización \\(\\mathbf{s}\\), \\(Z(\\mathbf{s})\\) es una v.. (de ahí el aspecto errático).Para un conjunto de puntos dado, \\({\\mathbf{s}_1},{\\mathbf{s}_2}, ..., {\\mathbf{s}_k}\\), las v.. \\(Z({\\mathbf{s}_1}), Z({\\mathbf{s}_2}), ..., Z({\\mathbf{s}_k})\\) están ligadas por una red de correlaciones espaciales que son las responsables de la similitud en los valores que toman (de ahí el aspecto estructurado).Para un conjunto de puntos dado, \\({\\mathbf{s}_1},{\\mathbf{s}_2}, ..., {\\mathbf{s}_k}\\), las v.. \\(Z({\\mathbf{s}_1}), Z({\\mathbf{s}_2}), ..., Z({\\mathbf{s}_k})\\) están ligadas por una red de correlaciones espaciales que son las responsables de la similitud en los valores que toman (de ahí el aspecto estructurado).Las f.. \\(Z(\\mathbf{s})\\) pueden ser estacionarias (en sentido estricto o de segundo orden), intrínsecamente estacionarias o estacionarias, y el hecho de que tengan uno u otro tipo de estacionariedad determina el análisis geoestadístico.\nUna f.. espacial es estrictamente estacionaria si las familias de v.. \\(Z(\\mathbf{s}_1),Z(\\mathbf{s}_2),...,Z(\\mathbf{s}_k)\\), tienen la misma distribución de probabilidad conjunta que \\(Z(\\mathbf{s}_1+\\mathbf{h}),Z(\\mathbf{s}_2+\\mathbf{h}),...,Z(\\mathbf{s}_k+\\mathbf{h})\\), \\(\\forall k\\), \\(\\forall \\mathbf{s}_1, \\mathbf{s}_2,...,\\mathbf{s}_k\\) y \\(\\forall \\mathbf{h} \\\\mathbb{R}^d\\) (donde \\(\\mathbf{h}\\) es un vector de traslación), siempre que \\(\\mathbf{s}_1+\\mathbf{h}, \\mathbf{s}_2+\\mathbf{h},...,\\mathbf{s}_k+\\mathbf{h}\\D\\).\nEs decir, la distribución de probabilidad conjunta de \\(Z(\\mathbf{s}_1+\\mathbf{h}),Z(\\mathbf{s}_2+\\mathbf{h}),...,Z(\\mathbf{s}_k+\\mathbf{h})\\) se ve afectada por una traslación \\(\\mathbf{h}\\), y por tanto, ni ella, ni las funciones de densidad de dimensión inferior \\(k\\), dependen de las localizaciones consideradas.La estacionariedad estricta es una condición muy restrictiva. Por ello, en la practica lo que se suele asumir es la estacionariedad de segundo orden, que limita la estacionariedad los dos primeros momentos de la f..222Si una f.. es estrictamente estacionaria, también es estacionaria de segundo orden. Sin embargo, la relación inversa tiene por qué ser cierta.La estacionariedad de segundo orden implica la existencia de la varianza de la f.., y deja fuera los fenómenos con infinita capacidad de variación. En este caso, si las diferencias \\(Z(\\mathbf{s}+\\mathbf{h})-Z(\\mathbf{s})\\) son estacionarias de segundo orden, se dice que la f.. es intrínsecamente estacionaria.Aquellas f.. cuya esperanza y/o varianza dependan de la localización (son invariantes las traslaciones) se denominan estacionarias.Salvo indicación de lo contrario, se asumirá la estacionariedad de segundo orden.Finalmente, unos breves comentarios sobre la importancia de la estacionariedad. Es imposible inferir la ley de probabilidad que gobierna la f.. espacial partir de una sola realización de la misma (una sola regionalización), pues sería como tener una muestra de tamaño 1. Pero en la práctica ese será el caso. Bueno, ni siquiera eso. Solo se dispondrá de una parte de la regionalización: la correspondiente las localizaciones observadas. La solución tan importante limitación es adoptar la hipótesis de estacionariedad u homogeneidad espacial. Es decir, sustituir la repetición de realizaciones de la f.. espacial por repeticiones en el espacio; dicho de otra forma, suponer que los valores observados en distintas localizaciones de \\(D\\) tienen las mismas características estadísticas y pueden ser considerados, en términos estadísticos, como realizaciones de la misma f..223 Por tanto, la hipótesis de estacionariedad significa que la ley espacial que gobierna f.., o parte de ella, es invariante traslaciones; depende de las localizaciones específicas observadas sino solo de \\(\\mathbf{h}\\).La hipótesis de estacionariedad permitirá actuar como si todas las v.. que conforman la f.. tuviesen la misma distribución de probabilidad (o los mismos momentos), haciendo posible el proceso inferencial. Por eso se le da tanta importancia que la f.. sea estacionaria, del tipo que sea.","code":""},{"path":"geo.html","id":"ana-estructural","chapter":"Capítulo 41 Geoestadística","heading":"41.3 Análisis estructural de la dependencia espacial","text":"","code":""},{"path":"geo.html","id":"semivariograma","chapter":"Capítulo 41 Geoestadística","heading":"41.3.1 Semivariograma","text":"La estadística espacial se basa en la suposición de que las unidades georeferenciadas cercanas están relacionadas (son dependientes) de alguna manera (Getis 1999a), y tanto más cuanto más cercanas estén (W. R. Tobler 1970).Los procesos con dependencia espacial se reconocen, visualmente, porque muestran un patrón en el espacio; en los que la tienen, el patrón es el de la aleatoriedad. La Fig. 41.1 muestra una simulación de una f.. con dependencia espacial (panel izquierdo) frente unos datos totalmente aleatorios (panel derecho).\nFigura 41.1: Dependencia espacial frente aleatoriedad\nPasando del terreno de las simulaciones la realidad, la Fig. 41.2 muestra la temperatura máxima en España el 6 de agosto de 2022224, en plena ola de calor (este es el ejemplo real que se utilizará lo largo del capítulo). En ella puede observarse claramente una estructura de dependencia espacial, con máximas cercanas 40 grados en la meseta central y Extremadura, de 30 grados o menos en la cordillera cantábrica y las costas atlántica, cantábrica y andaluza, y entre 30 y 35 grados en el resto del país (básicamente Murcia, Comunidad Valenciana y Cataluña).\nFigura 41.2: Temperatura máxima en España peninsular, 6 de agosto de 2022\nAhora bien, para poder llevar cabo predicciones geoestadísticas es necesario representar, previamente, los patrones de dependencia espacial observados mediante funciones que indiquen cuál es la estructura de dicha dependencia espacial. Dichas funciones son los semivariogramas. Dado que la identificación de la estructura de la dependencia espacial existente en el fenómeno de interés es la clave del éxito del proceso predictivo, al semivariograma se le considera la piedra angular de la predicción geoestadística (José-Marı́Montero, Fernández-Avilés, Mateu 2015). Un semivariograma se define como la semivarianza de los incrementos de la f..: \\[\\begin{equation} (\\#eq:vario)\n\\gamma(\\mathbf{s}_i - \\mathbf{s}_j) = \\frac{1}{2} V[Z(\\mathbf{s}_i)-Z(\\mathbf{s}_j)] , \\forall \\mathbf{s}_i, \\mathbf{s}_j\\D.\n\\end{equation}\\]\nque, en el caso habitual de f.. estacionarias de segundo orden o intrínsecamente estacionarias (sin deriva), se transforma en:\n\\[\\begin{equation}\n\\gamma (\\mathbf{h}) = \\frac{1}{2} V \\left( Z(\\mathbf{s} + \\mathbf{h}) - Z(\\mathbf{s}) \\right) =\\frac{1}{2}E\\left( {\\left( {Z(\\mathbf{s} + \\mathbf{h}) - Z(\\mathbf{s})} \\right)^2} \\right),\n\\end{equation}\\]Nótese que:Si hay dependencia espacial (positiva225, es lo normal), la diferencia entre los valores de la f.. en los puntos separados por una pequeña distancia será poca y más o menos la misma, es decir, dichas diferencias serán poco variables, y el valor del semivariograma, pequeñas distancias, será pequeño.Si aumenta la distancia, la dependencia espacial se reduce y la diferencia entre los valores de la f.en los puntos separados por distancias intermedias y grandes será tan parecida como en el caso anterior, sino mayor; y variará más: Es decir, el valor del semivariograma aumenta con la distancia.Si aumenta la distancia, la dependencia espacial se reduce y la diferencia entre los valores de la f.en los puntos separados por distancias intermedias y grandes será tan parecida como en el caso anterior, sino mayor; y variará más: Es decir, el valor del semivariograma aumenta con la distancia.Si la distancia aumenta lo suficiente como para que ya haya dependencia espacial, las diferencias entre los valores de la f.. separados por tal distancia alcanzarán la variabilidad de la f.en estudio, y si ésta es estacionaria de segundo orden, el semivariograma se estabilizará en torno ella.Si la distancia aumenta lo suficiente como para que ya haya dependencia espacial, las diferencias entre los valores de la f.. separados por tal distancia alcanzarán la variabilidad de la f.en estudio, y si ésta es estacionaria de segundo orden, el semivariograma se estabilizará en torno ella.En el caso estacionario, las funciones de covarianza, \\(C(\\bf h)\\), también pueden ser utilizadas para representar la estructura de la dependencia espacial, si bien se prefiere el semivariograma porque requiere el conocimiento de la media de la f.. en estudio. Además, el semivariograma cubre un espectro más amplio de fenómenos regionalizados que la función de covarianza, ya que ésta puede definirse en el caso de estacionariedad intrínseca. Los detalles pueden verse en José-Marı́Montero, Fernández-Avilés, Mateu (2015) y J. M. Montero Larraz (2008).\nCuando el semivariograma depende tanto de la dirección como de la longitud del vector \\(\\mathbf{h}\\) que une las localizaciones \\(\\mathbf{s}\\) y \\(\\mathbf{s}+ \\mathbf{h}\\), se denomina anisotrópico. Cuando solo depende de la distancia (\\(\\lvert \\mathbf{h} \\rvert=|| \\mathbf{h} ||\\), porque en el espacio euclídeo el módulo y la norma coinciden) se denomina isotrópico.\nUn semivariograma puede ser cualquier función. Tiene que ser nulo en el origen, negativo, verificar que \\(\\gamma(\\mathbf{h})=\\gamma(-\\mathbf{h})\\), debe ser una función condicionalmente definida negativa y tener un ritmo de crecimiento inferior \\(|\\mathbf h|^2\\), es decir, \\(lim_{|\\mathbf h|\\\\infty} \\frac {\\gamma(\\mathbf{h})}{|\\mathbf h|^2}=0\\) cuando el proceso es estacionario (sin deriva), siendo finito en caso de procesos estacionarios de segundo orden.El análisis del comportamiento de un semivariograma pequeñas, medias y grandes distancias es de sumo interés, como se verá continuación.En general, distancias medias y grandes, los semivariogramas asociados f.. estacionarias de segundo orden crecen, monótonamente, desde el origen con la distancia, hasta alcanzar un valor límite, la varianza priori de la f.. (o covarianza para \\(\\mathbf {h}=0\\), \\(C(\\mathbf{0})\\)), bien de forma exacta o asintóticamente. Dicho valor límite se denomina meseta, \\(m\\), y la distancia la cual se alcanza se conoce como alcance o rango, \\(\\). Por tanto, el rango es la distancia partir de la cual ya hay dependencia espacial. Cuando \\(m\\) se alcanza asintóticamente, el alcance queda perfectamente definido y se toma como alcance, efectos prácticos, \\(^\\prime\\), la distancia la cual el semivariograma toma el valor \\(0,95m\\). En el caso estacionario (por ejemplo, si hay deriva) o intrínsecamente estacionario el semivariograma tiene meseta.El comportamiento pequeñas distancias, sobre todo cerca del origen, que es donde más dependencia espacial hay, está muy relacionado con el grado de continuidad y regularidad de f.. Cuanto más continua y regular sea, más suaves y estructuradas serán las realizaciones que produzca, y más regular será el comportamiento del semivariograma cerca del origen (Fig. 41.3, panel izquierdo; representación bidimensional).Los semivariogramas con un comportamiento lineal cerca del origen son típicos de v.r. continuas, al menos por partes, pero diferenciables. Su representación gráfica tridemensional está llena de picos. La amplitud de las fluctuaciones aumenta con la distancia entre localizaciones y es proporcional la pendiente de la tangente en el origen (Fig. 41.3, panel derecho; representación bidimensional).\nFigura 41.3: Representacion bidimensional de dos \\(f..\\): con semivariograma parabólico en el origen (izquierda); con semivariograma lineal en el origen (derecha)\nLas v.r. regulares (aquellas cuya gráfica tridimensional tiene picos) se identifican con un comportamiento semivariográfico parabólico en el origen. Si dicho comportamiento persiste largas distancias, puede que exista una fuerte deriva.Las discontinuidades en el origen (que teóricamente pueden darse) son frecuentes en la práctica. Su amplitud se denomina “efecto pepita” (nugget effect) y son típicas de variables regionalizadas muy irregulares y, quizás, discontinuas. Las causas más frecuentes del “efecto pepita” son la existencia de una estructura con alcance inferior la distancia más corta entre localizaciones y los errores de posicionamiento o de medida (véase Chilès Delfiner (1999) para más detalle). El caso límite del efecto pepita es el “efecto pepita puro”. En ese caso, el semivariograma es constante cualquiera que sea la distancia, indicando ausencia de dependencia espacial.La Fig. 41.4 muestra gráficamente los principales elementos de un semivariograma.\nFigura 41.4: Elementos del semivariograma (meseta unitaria)\n ","code":"\nlibrary(geoR)\nlibrary(fields)\npar(mfrow = c(1, 2))\nset.seed(728)\n\nsim_dep <- grf(401, grid = \"reg\", cov.pars = c(1, 0.8), messages = FALSE)\npoints.geodata(sim_dep,\n  main = \"Dependencia espacial\",\n  col = tim.colors(), cex.max = 2\n)\n\nsim_indep <- grf(401, grid = \"reg\", cov.pars = c(0.01, 0), messages = FALSE)\npoints.geodata(sim_indep,\n  main = \"Aleatoriedad\",\n  col = tim.colors(), cex.max = 2\n)\nlibrary(CDR)\n#summary(CDR::tempmax_data)\n\n# renombra objetos por simplicidad en el análisis\nESP <- tempmax_data$ESP\nESP_utm <- tempmax_data$ESP_utm\ngrd_sf <- tempmax_data$grd_sf\ngrd_sp <- tempmax_data$grd_sp\ntemp_max_utm_sf <- tempmax_data$temp_max_utm_sf\ntemp_max_utm_sp <- tempmax_data$temp_max_utm_sp\nlibrary(ggplot2)\nbr_paper <- c(-Inf, seq(17.5, 45, 2.5), Inf)\npal_paper <- hcl.colors(15, \"YlOrRd\", rev = TRUE)\n\nggplot(ESP_utm) +\n  geom_sf() +\n  geom_sf(data = temp_max_utm_sf, aes(col = tmax), size = 4) + # temp_max_utm\n  theme_light() +\n  scale_color_gradientn(colours = pal_paper)\nlibrary(fields)\nlibrary(geoR)\npar(mfrow = c(1, 2))\nset.seed(123)\n\nfa_gauss <- grf(1225, grid = \"reg\", cov.pars = c(1, .25), cov.model = \"gaussian\")\nimage(fa_gauss, col = tim.colors())\n\nfa_sph <- grf(1225, grid = \"reg\", cov.pars = c(1, .25), cov.model = \"spherical\")\nimage(fa_sph, col = tim.colors())\nlibrary(geoR)\nsemivar <- function(x, ...) {\n  1 - cov.spatial(x, ...)\n}\ncurve(semivar(x, cov.pars = c(0.8, 0.4), cov.model = \"sph\"), 0.0, 1,\n  xlab = \"Distancia\",\n  ylab = expression(bold(gamma(\"|h|\"))), lwd = 4, lty = 1, col = \"4\", xlim = c(0.03, 1), ylim = c(0, 1)\n)\nabline(v = 0.4, col = 2, lty = 2, lwd = 2) # alcance\nabline(h = 1, col = 3, lty = 2, lwd = 2) # meseta\nlegend(-0.05, 0.15, \"Efecto pepita\")\nlegend(0, 0.95, \"Meseta\")\nlegend(0.25, 0.5, \"Alcance\")\nlegend(0.5, 0.75, \"Ausencia de dependencia\")\nknitr::include_graphics(\"img/semivar-parts.png\")"},{"path":"geo.html","id":"modelos-de-semivariogramas-válidos","chapter":"Capítulo 41 Geoestadística","heading":"41.3.2 Modelos de semivariogramas válidos ","text":"Las funciones que verifican las condiciones que debe cumplir un semivariograma (véase 41.3.1) se conocen como semivariogramas válidos. El incumplimiento de alguna de ellas tiene perversas consecuencias en el proceso predictivo (por ejemplo, varianzas de los errores de predicción negativas). Su tipología, siguiendo el enfoque sugerido en Journel Huijbregts (1978) y José-Marı́Montero, Fernández-Avilés, Mateu (2015), es la siguiente:226","code":""},{"path":"geo.html","id":"semivariogramas-con-meseta","chapter":"Capítulo 41 Geoestadística","heading":"41.3.2.1 Semivariogramas con meseta ","text":"Están asociados f.. estacionarias de segundo orden. Los más utilizados son:Esférico. Válido en \\(\\mathbb{R}^1\\), \\(\\mathbb{R}^2\\) y\n\\(\\mathbb{R}^3\\), y viene dado por:\n\\[\\begin{equation}\n\\gamma(|\\mathbf{h}|)= \\left\\{\n\\begin{array}{ll}\nm \\left( 1.5\\frac{|\\mathbf{h}|}{}-0.5\\left(\\frac{|\\mathbf{h}|}{}\\right)^{3} \\right) & \\mbox{si $|\\mathbf{h}|\\leq $} \\\\\nm & \\mbox{si $|\\mathbf{h}| > .$}\n\\end{array}\n\\right.\n\\end{equation}\\]Tiene un comportamiento lineal cerca del origen, indicando continuidad y cierto grado de irregularidad en la f.. grandes distancias alcanza la meseta cuando \\(|\\mathbf{h}|=\\). Estas dos características son propias de muchas regionalizaciones observadas en la realidad; de ahí su popularidad.Exponencial.  Válido en \\(\\mathbb{R}^d, d\\geq1\\) y viene dado por:\n\\[\\begin{equation}\n\\gamma (|\\mathbf{h}|)= m \\left( 1-\\exp \\left( -\\frac{|\\mathbf{h}|}{}\n\\right) \\right).\n\\end{equation}\\]Igual que el esférico, cerca del origen exhibe un comportamiento lineal, siendo menor la pendiente. diferencia de él, solo alcanza la meseta asintóticamente. efectos prácticos, se toma como alcance la distancia para la cual el semivariograma alcanza el valor del 95% de la meseta, \\(^\\prime \\approx 3a\\).Gausiano. Válido en \\(\\mathbb{R}^d, d \\geq1\\). Está definido por:\n\\[\\begin{equation}   \n\\gamma (|\\mathbf{h}|)=m\\left( 1-\\exp \\left(-\\frac{|\\mathbf{h}|^{2}}{^{2}}\\right) \\right).\n\\end{equation}\\]diferencia del esférico y el exponencial, tiene un comportamiento parabólico cerca del origen. Por consiguiente, está asociado con f.. estacionarias de segundo orden infinitamente diferenciables y, en consecuencia, muy regulares. Igual que el modelo exponencial, alcanza la meseta sólo asintóticamente, con \\(^\\prime \\approx \\sqrt 3\\).Efecto pepita puro. Refleja la ausencia de dependencia espacial:\\[\\begin{equation}\n\\gamma(|\\mathbf{h}|) = \\left\\{ {\\begin{array}{l}\nm\\;\\;\\mbox{si}\\;|\\mathbf{h}| = 0 \\\\\n0\\;\\;\\mbox{ si}\\;|\\mathbf{h}| > 0 \\\\\n\\end{array}}, \\right.\\; \\;\\;\\; m> 0.\n\\end{equation}\\]K-Bessel. Válido \\(\\mathbb{R}^d, d\\geq1\\). Su expresión es:\n\\[\\begin{equation}\n\\gamma(|\\mathbf{h}|)=m \\left(1- \\frac{1}{2^{\\alpha-1} \\Gamma(\\alpha)}\n\\left(\\frac{|\\mathbf{h}|}{} \\right)^\\alpha K_\\alpha \\left(\\frac{|\\mathbf{h}|}{} \\right) \\right), \\quad \\alpha >0,\n\\end{equation}\\]donde \\(K_\\alpha\\) es la función de segunda especie de orden \\(\\alpha\\). Este modelo puede representar cualquier tipo de comportamiento cerca del origen. Por ejemplo, para \\(\\alpha=0,5\\) se obtiene el modelo exponencial.","code":""},{"path":"geo.html","id":"semivariogramas-con-meseta-y-efecto-hoyo","chapter":"Capítulo 41 Geoestadística","heading":"41.3.2.2 Semivariogramas con meseta y efecto hoyo ","text":"J-Bessel. Un semivariograma tiene por qué ser necesariamente una función monótona decreciente, sino que puede tener “ondas” (efecto hoyo). Tal es el caso del modelo J-Bessel, que puede ser utilizado en presencia de dependencia espacial negativa o, específicamente, en caso de alternancia entre dependencia positiva y negativa. Válido en \\(\\mathbb{R}^d\\), \\(d\\leq 2(\\alpha +1)\\), su expresión viene dada por:\n\\[\\begin{equation}\n\\gamma(|\\mathbf{h}|)=m \\left(1- \\left(\\frac{2a}{|\\mathbf{h}|}\\right)^{\\alpha} \\Gamma(\\alpha + 1)\nJ_\\alpha \\left(\\frac{|\\mathbf{h}|}{} \\right) \\right),\n\\end{equation}\\]donde \\(\\alpha\\) es un parámetro de forma, \\(\\) es un parámetro de escala, \\(\\Gamma\\) es la función de Euler que interpola el factorial y \\(J_{\\alpha}\\) es la función J-Bessel de primera especie de orden \\(\\alpha\\).La Fig. 41.5 muestra una representación gráfica de los anteriores semivariogramas.\nFigura 41.5: Representación de semivariogramas con meseta válidos (meseta y alcance efectivo unitarios; excepción del efecto pepita puro)\n","code":"\nlibrary(gstat)\nshow.vgms(models = c(\"Sph\", \"Exp\", \"Gau\", \"Nug\", \"Bes\", \"Wav\"))"},{"path":"geo.html","id":"semivariogramas-sin-meseta","chapter":"Capítulo 41 Geoestadística","heading":"41.3.2.3 Semivariogramas sin meseta ","text":"Estos modelos van más allá de la hipótesis estacionaria de segundo\norden y corresponden f.. con una\ncapacidad ilimitada de dispersión espacial, es decir, f.. intrínsecamente estacionarias, pero estacionarias de segundo orden.Potencial. Válido en \\(\\mathbb{R}^d, d\\geq1\\) y definido por:\n\\[\\begin{equation}\n\\gamma (|\\mathbf{h}|)=  (|\\mathbf{h}|)^{\\alpha }, \\quad \\mbox  {con } 0<\\alpha <2,\n\\end{equation}\\]Potencial. Válido en \\(\\mathbb{R}^d, d\\geq1\\) y definido por:\n\\[\\begin{equation}\n\\gamma (|\\mathbf{h}|)=  (|\\mathbf{h}|)^{\\alpha }, \\quad \\mbox  {con } 0<\\alpha <2,\n\\end{equation}\\]Logarítmico. Válido en \\(\\mathbb{R}^d, d\\geq1\\) y con expresión:\n\\[\\begin{equation}\n\\gamma (|\\mathbf{h}|) = b\\log |\\mathbf{h}|\\; \\;\\mbox{si}\\ |\\mathbf{h}| \\ge 0,\n\\end{equation}\\]Logarítmico. Válido en \\(\\mathbb{R}^d, d\\geq1\\) y con expresión:\n\\[\\begin{equation}\n\\gamma (|\\mathbf{h}|) = b\\log |\\mathbf{h}|\\; \\;\\mbox{si}\\ |\\mathbf{h}| \\ge 0,\n\\end{equation}\\]donde \\(b\\) es una constante.Una representación gráfica de ambos semivariogramas puede verse en la Fig. 41.6.\nFigura 41.6: Representación de semivariogramas sin meseta válidos\n","code":"\nlibrary(gstat)\nshow.vgms(models = c(\"Pow\", \"Log\"), sill = 1, range = c(2, 1), nugget = 0)"},{"path":"geo.html","id":"semivariograma-empírico","chapter":"Capítulo 41 Geoestadística","heading":"41.3.3 Semivariograma empírico","text":"Dado que la única información de la que se dispone es una realización observada de la f.objeto de estudio, en la práctica la estructura de la dependencia espacial se estima mediante el semivariograma empírico. En el marco de la estacionariedad intrínseca (que incluye la estacionariedead estricta y de segundo orden), y en \\(\\mathbb{R}^d, d\\geq1\\), se estiman (insesgadamente) los valores semivariográficos para un número determinado de distancias, por el método de los momentos:\n\\[\\begin{equation}\n\\hat{\\gamma} (\\mathbf{h}) = \\frac{1}{2\\#N(\\mathbf{h})}\\sum\\limits_{N(\\mathbf{h})} {\\left( {Z(\\mathbf{s}_i + \\mathbf{h}) - Z({\\mathbf{s}}_i )} \\right)^2},\n\\end{equation}\\]\ndonde \\(\\#N(\\mathbf{h})\\) es el número de parejas de localizaciones separadas por el vector \\(\\mathbf{h}\\).La función que mejor ajusta las estimaciones de los valores semivariograficos anteriormente referidos se denomina semivariograma empírico, y también se suele denotar por \\(\\hat{\\gamma} (\\mathbf{h})\\).Los valores semivariográficos se suelen computar para distancias inferiores la mitad del diametro de D, porque, para distancias superiores, el número de parejas de localizaciones suele ser pequeño para proporcionar estimaciones fiables. En la práctica, como en muchas de las direcciones hay un numero de parejas suficiente para calcular el semivariograma con cierta fiabilidad, lo habitual es construir un semivariograma empírico omnidireccional, es decir, que depende solo de la distancia (longitud del vector h) y de la dirección. Para ello se crean regiones de tolerancia, que se solapen, basadas en intervalos de distancia (normalmente de la misma longitud) y un angulo de tolerancia. En concreto, la tolerancia se especifica en el módulo de \\(\\mathbf h\\) (\\(\\pm\\Delta |\\mathbf h|\\)) y su dirección (\\(\\pm\\Delta\\theta\\)). Para más detalles y ejemplos, véase José-Marı́Montero, Fernández-Avilés, Mateu (2015). La Fig. 41.7 muestra la ubicación de los puntos semivariográficos, indicando el número de parejas cada distancia, en el caso de las temperaturas máximas en España (06/08/2022).\nFigura 41.7: Valores semivariográficos. Temperaturas máximas (06/08/2022)\nEn el ejemplo ilustrado, las distancia mínima entre dos estaciones meteorológicas es 1.125m y la máxima 1.027.597m. Sin embargo, dada la geometría del mapa de España (aunque de Huelva Gerona hay 987 km en linea recta, más de dos terceras partes de las ciudades españolas están separadas más de 500 km.), se consideró 250.000m (1/4 de la distancia máxima) como distancia máxima la hora de calcular los valores semivariográficos, ya que partir de dicha distancia el número de parejas es lo suficientemente grande como para obtener resultados fiables. Por convenio, gstat divide la distancia en 15 intervalos (geoR se divide en 13 porque los autores lo hicieron un viernes 13). ","code":"\nvgm_tmax <- variogram(tmax ~ 1, temp_max_utm_sf,\n  cutoff = 250000 # 250 km\n)\nplot(vgm_tmax, plot.numbers = TRUE, pch = \"+\", lwd = 2, cex = 2)"},{"path":"geo.html","id":"ajuste-semivariográfico","chapter":"Capítulo 41 Geoestadística","heading":"41.3.4 Ajuste semivariográfico ","text":"Cualquier función que dependa de una distancia y una dirección es necesariamente un semivariograma, pues para ello tienen que cumplir los requisitos especificados en 41.3.1. Esta es la razón por la que el semivariograma empírico puede utilizarse directamente para realizar predicciones geoestadísticas. Por ello, los valores semivariográficos estimados se les ajusta una función que represente un semivariograma válido. Sin embargo, esta tarea, clave para el éxito del posterior proceso predictivo, es sencilla ni existe consenso en torno ella.\nEl ajuste puede ser manual, utilizando métodos visuales y gráficos, o automático, que usa procedimientos estadísticos. Una combinación de ambos es muy recomendable.El ajuste manual pudiera parecer “muy científico” pero, dado que lo más importante la hora del ajuste es tanto la bondad del ajuste para todos los puntos semivariográficos sino lo bien que un semivariograma válido representa las principales características del fenómeno, especialmente el tipo de estacionariedad (comportamiento largas distancias) y, sobre todo, el tipo de continuidad (comportamiento cerca del origen), resulta ser un procedimiento muy práctico si se guía por las anteriores consideraciones. En este sentido, cualquier conocimiento sobre el fenómeno en estudio es bienvenido.El ajuste automatizado mediante procedimientos estadísticos incluye los métodos de mínimos cuadrados (tanto ordinarios como generalizados y ponderados ), que son los más populares en la práctica, y los métodos basados en máxima verosimilitud, que incluyen, entre otros, el tradicional método máximo verosímil, la máxima verosimilitud restringida y el método de la verosimilitud compuesta. La Fig. 41.8 muestra el semivariograma empírico correspondiente los puntos semivariográficos de la Fig. 41.7. De todos los modelos con meseta, el semivariograma ajustado ha sido un exponencial con alcance 76.404,64 metros y meseta 13,74.\nFigura 41.8: Semivariograma empírico. Temperaturas máximas (06/08/2022)\nEl método de ajuste ha sido el que figura por defecto en la función vgm : mínimos cuadrados ponderados con ponderaciones \\(\\frac {N_{\\bf |h|}} {|\\bf h|^2}\\), que funciona bien en la práctica y selecciona el semivariograma que mejor ajuste cuando el número de parejas es elevado y la distancia pequeña, que es la parte del semivariograma que hay que ajustar bien porque pequeñas distancias es cuando más dependencia espacial hay. Respecto los parámetros iniciales, aunque el investigador puede especificar los que considere convenientes, se recomienda utilizar los que tiene la función por defecto: \\(()\\) alcance igual 1/3 de la distancia máxima en la muestra; \\((ii)\\) como efecto pepita se toma la media de los tres primeros valores semivariográficos; y \\((iii)\\) como meseta parcial (meseta menos efecto pepita), la media de los cinco últimos valores semivariográficos. ","code":"\nfit_vgm_tmax <- fit.variogram(vgm_tmax,\n                              model = vgm(model = c(\"Sph\", \"Exp\", \"Gau\", \"Nug\", \"Bes\", \"Wav\")), fit.sills = TRUE, fit.ranges = TRUE, fit.kappa = TRUE, fit.method = 7)\nfit_vgm_tmax\n#>   model    psill    range\n#>1   Exp 13.74102 76404.64\nattr(fit_vgm_tmax, \"SSErr\") \n#> [1] 6.200657e-07\nplot(vgm_tmax, fit_vgm_tmax, lwd = 2, col = \"2\", pch = \"*\", cex = 3)"},{"path":"geo.html","id":"kriging","chapter":"Capítulo 41 Geoestadística","heading":"41.4 Kriging","text":"Seleccionado el semivariograma válido que mejor se ajusta los puntos semivariográficos, se aborda el proceso predictivo. El método predictivo que usa la geoestadística es conocido como kriging en honor al ingeniero de minas D.G. Krige. El kriging tiene como objetivo predecir el valor de una f.., \\(Z(\\mathbf{s})\\), en uno o más puntos (o bloques) observados, partir de la regionalización observada (pueden ser puntos o bloques) en un\ndominio , y proporciona el mejor predictor lineal insesgado\nde la v.r. de interés en tales puntos o bloques observados227. La limitación la clase de predictores lineales\nobedece que, bajo estacionariedad de segundo orden, solo se requiere el conocimiento de los momentos de segundo orden de la f.. Con más información estructural, pueden definirse predictores lineales.Las principales ventajas del kriging sobre los métodos de interpolación espacial deterministas (método de la distancia inversa, splines, regresión polinomial, etc.), es que (\\(\\)) considera la estructura de la dependencia espacial (dando lugar mejores predicciones), (\\(ii\\)) proporciona, junto con la predicción, la varianza del error de predicción y (\\(iii\\)) es un interpolador exacto.Dependiendo del tipo de estacionariedad que se considere en la f.. el kriging puede ser: universal (estacionariedad en media) u ordinario (estacionariedad de segundo orden o intrínseca). Nos centraremos en el kriging ordinario (KO). La generalización al caso universal (hay deriva: la media depende de las localizaciones en vez de ser constante) puede verse en José-Marı́Montero, Fernández-Avilés, Mateu (2015).En términos formales, KO se plantea como sigue: Sea \\(Z =\\left\\{ Z(\\mathbf{s}),\\,\\, \\mathbf{s} \\D\\right\\}\\) una f.. con estacionariedad de segundo orden o intrínseca y con media desconocida (cuando se conoce, KO se denomina kriging simple). Sea el predictor lineal krigeado \\(Z^{\\ast} (\\mathbf{s}_0 ) = \\sum\\limits_{= 1}^n {\\lambda _i } Z(\\mathbf{s}_i )\\), donde las ponderaciones \\(\\lambda _i, =1,2,...,n\\), se obtienen imponiendo al error de predicción las condiciones de esperanza nula y mínima varianza.\nEl sistema de ecuaciones que proporciona dichas ponderaciones óptimas es:\n\\[\\begin{equation}  \n\\left\\{ {\\begin{array}{l}\n\\sum\\limits_{j = 1}^n {\\lambda _j \\gamma (\\mathbf{s}_i - \\mathbf{s}_j)\n+ \\alpha = \\gamma (\\mathbf{s}_i - \\mathbf{s}_0 ), \\quad \\ = 1,...,n} \\\\\n\\sum\\limits_{= 1}^n {\\lambda _i = 1}, \\; \\\\\n\\end{array}} \\right.\n\\end{equation}\\]\nsiendo la varianza del error de predicción:\n\\(\\sigma_{OK}^2 (\\mathbf{s}_0) = \\sum\\limits_{= 1}^n {\\lambda _i \\gamma (\\mathbf{s}_i - \\mathbf{s}_0 ) + \\alpha }\\), donde \\(\\alpha\\) es el multiplicador de Lagrange involucrado en el proceso de optimización.Retomando el ejemplo de las temperaturas máximas en la España peninsular el 6 de agosto de 2022, continuación se muestra el código necesario para la creación de un mapping de predicción de dichas temperaturas.\nFigura 41.9: \\(Mapping\\) de temperaturas máximas (06/08/2022).\nEl mapping de la Fig. 41.9 tiene poco valor si se acompaña de otro que muestre la desviación típica de los errores de predicción. \nFigura 41.10: Desviaciones típicas del error de predicción\nComo se aprecia en la Fig. 41.10, cuanto mayor es el número de localizaciones observadas alrededor del punto de predicción, menor es la desviación típica del error de predicción.\nRESUMEN. La geoestadística estudia de fenómenos regionalizados, que son aquellos que se extienden en el espacio y presentan una organización o estructura debida la dependencia espacial existente. Su objetivo es sacar provecho de dicha dependencia espacial para llevar cabo predicciones (interpolaciones) óptimas en ubicaciones o áreas de interés, o la realización de mappings sobre todo el dominio o parte de él. Las dos partes del análisis geoestadístico son: \\(()\\) el análisis estructural de la dependencia espacial y \\((ii)\\) la predicción “krigeada”. La estructura de dependencia espacial se representa mediante un semivariograma. La elección del semivariograma entre el elenco de funciones semivariográficas válidas es la clave del éxito de la predicción geoestadística, y por ello al semivariograma se le considera la piedra angular de la geoestadística. La técnica que utiliza la geoestadística para predecir se denomina kriging, y presenta un buen número de ventajas sobre los tradicionales métodos de interpolación espacial deterministas al considerar la estructura espacial de las observaciones.","code":"\nkriged_tmax <- krige(tmax ~ 1,\n  temp_max_utm_sp,\n  grd_sp,\n  model = fit_vgm_tmax\n)\n#> [using ordinary kriging]\nkriged_df <- as.data.frame(kriged_tmax, xy = T, na.rm = T)\n\nggplot() +\n  geom_tile(data = kriged_df,\n    aes(x = coords.x1, y = coords.x2, fill = var1.pred)\n  ) +\n  geom_sf(data = ESP_utm, col = \"black\", fill = NA) +\n  scale_fill_gradientn(colours = pal_paper,\n    breaks = br_paper,\n    labels = function(x) {\n      paste0(x, \"º\")\n    },\n    guide = guide_legend(reverse = TRUE, title = \"Temp. max.\")\n  ) +\n  theme_light() +\n  theme(panel.background = element_blank(),\n    panel.border = element_blank(),\n    axis.title = element_blank(),\n  )\nggplot(kriged_df) +\n  geom_contour_filled(aes(coords.x1, coords.x2, z = sqrt(var1.var)),\n    breaks = c(0, 2, 2.5, 3, 3.5, 4, max(sqrt(kriged_df$var1.var)))\n  ) +\n  geom_sf(data = ESP_utm, col = \"black\", fill = NA) +\n  geom_sf(data = temp_max_utm_sf, col = \"blue\", shape = 4) +\n  scale_fill_manual( # paleta colores\n    values = c(\"springgreen\", hcl.colors(8, \"PuRd\", rev = TRUE)),\n    guide = guide_legend(title = \"Desv. típica\\n error predicción\")\n  ) +\n  theme_light() +\n  theme(panel.background = element_blank(),\n    panel.border = element_blank(),\n    axis.title = element_blank(),\n  )"},{"path":"cap-econom-esp.html","id":"cap-econom-esp","chapter":"Capítulo 42 Modelos econométricos espaciales","heading":"Capítulo 42 Modelos econométricos espaciales","text":"Andrés Vallone\\(^{}\\) y Coro Chasco\\(^{b,c}\\)\\(^{}\\) Escuela de Ciencias Empresariales-Instituto de Políticas Publicas Universidad Católica del Norte\n\\(^{b}\\) Departamento de Economía Aplicada, Universidad Autónoma de Madrid\n\\(^{c}\\) Universidad de Nebrija","code":""},{"path":"cap-econom-esp.html","id":"la-dependencia-espacial","chapter":"Capítulo 42 Modelos econométricos espaciales","heading":"42.1 La dependencia espacial","text":"En muchas ocasiones, los fenómenos de estudio son independientes del espacio geográfico en el cual se producen. Esto se refleja en la primera ley de la Geografía enunciada por Waldo R. Tobler (1970) “Todas las cosas están relacionadas entre sí, pero las cosas más próximas en el espacio tienen una relación mayor que las distantes” (Waldo R. Tobler 1970, p 236). Esta situación, produce una violación del supuesto básico de independencia de las variables aleatorias requerido por el método de estimación de minimos cuadrados ordinarios (MCO).En este contexto, los MCO ya son óptimos y, por tanto, los estadísticos de contraste \\(t\\) y \\(F\\) pueden llevar conclusiones erróneas(Anselin 1988). Por ello es necesario encontrar la manera de incorporar el espacio geográfico en los procesos de modelación. En este capítulo se abordará esta cuestión, mostrando primero los métodos de exploración de datos espaciales, para luego presentar las formas de modelización del espacio y los métodos de estimación.Los modelos de econometría espacial se centran en manejar las situaciones de dependencia espacial. Existe dependencia espacial cuando lo que sucede en una locación \\(\\) esta influenciado por lo que sucede en una locación \\(j\\) y viceversa (Anselin 2013). La dependencia espacial se traduce en que los valores de la variable en las locaciones \\(\\) y \\(j\\) con \\(\\neq j\\) están correlacionados entre sí, hecho que se conoce como autocorrelación espacial (Anselin 2013). La autocorrelación espacial puede ser positiva, cuando las locaciones con valores similares tienden estar juntos (altos con altos, bajos con bajos) o negativa, cuando las unidades espaciales tienden estar rodeadas de vecinos con valores diferentes. Los patrones espaciales formados por la existencia de autocorrelación se muestran en la Figura 42.1. La ausencia de algún tipo de autocorrelación es lo que se entiende como aleatoriedad espacial (Anselin 2013).\nFigura 42.1: Patrones de autocorrelación espacial\n","code":""},{"path":"cap-econom-esp.html","id":"modelización-del-espacio","chapter":"Capítulo 42 Modelos econométricos espaciales","heading":"42.1.1 Modelización del espacio","text":"El espacio puede jugar un rol importante en la determinación de los procesos modelizar. Por ello, resulta relevante encontrar una forma de incorporar el espacio en los procesos de estimación. Para modelizar la interacción de una variable consigo misma es natural pensar en el concepto de autocorrelación. obstante, diferencia de la autocorrelación temporal, que es unidireccional (sólo el pasado puede afectar el presente), en el caso del espacio la influencia es multidireccional en el entorno o vecindario de la localidad de análisis y, por tanto, es crucial definir el vecindario.La matriz de vecindad o contigüidad \\(\\mathbf{W}_{n \\times n}\\) muestra la relación entre las \\(n\\) localidades analizadas, y por tanto la interacción existente entre ellas. Es una matriz simétrica y binaria, de forma que \\(w_{ij} = 1\\) si las localidades \\(\\) y \\(j\\) son vecinas y cero si lo son. Por tanto, \\(w_{ii}=0\\) puesto que una localidad puede ser vecina de sí misma. Existen distintos criterios de definición de vecindad dependiendo del proceso que se desee modelizar y las características de los datos. Si se cuenta con un mapa de polígonos, entonces podemos utilizar alguno de los criterios que se presentan en la Figura 42.2 para configurar la matriz \\(\\mathbf{W}\\).\nFigura 42.2: Criterios de vecindad\nLas matrices \\(\\mathbf{W}\\) generadas bajo el criterio lineal consideran como vecinas la localidad \\(\\) todas aquellas localidades que compartan un borde situadas en la misma dirección cardinal, norte sur o este oeste, de esta localidad El resto de los criterios de contigüidad siguen los movimientos de las piezas del ajedrez para definir la vecindad de la localidad \\(\\). La construcción de una matriz de vecindad bajo el criterio de la torre implica considerar como vecinos de la localidad \\(\\) aquellas localidades situadas al norte, sur, este u oeste y que compartan un borde en común con dicha localidad. El uso del criterio de alfil considera como vecindad de la localidad \\(\\) aquellas localidades situadas al noreste, noroeste, sureste o suroeste de la localidad \\(\\) y que tengan, al menos, un punto en común. El criterio de la reina considera como vecindario de la unidad espacial \\(\\) las localidades en todas las direcciones cardinales y que tengan al menos un punto en común con ella (Martori, Hoberg, Madariaga 2008).Dependiendo del fenómeno que se analice, la matriz de contigüidad puede ser construida considerando un vecindario más amplio; por ejemplo, considerando como vecinos de la localidad \\(\\) los vecinos de los vecinos de dicha localidad, en este caso se dice que la matriz de vecindad es de orden 2 (los vecinos y los vecinos de los vecinos). Las matrices \\(\\mathbf{W}\\), se utilizan para capturar el efecto del vecindario partir de medias ponderadas basadas en la cercanía de las unidades espaciales. Es por ello que las matrices de vecindad se estandarizan por filas. Los elementos de la matriz estandarizada se obtienen de la siguiente manera:En palabras simples, se divide cada elemento de una fila de la matriz \\(\\mathbf{W}\\) por la suma de dicha fila. Esto asegura que cada elemento de la matriz \\(\\mathbf{W}\\) estandarizada se encuentre entre 0 y 1, y que la suma de cada una de sus filas sea siempre 1. Las matrices de vecindad estandarizadas llevan el nombre de matrices de pesos espaciales. partir de ahora, cuando se haga relación la matriz \\(\\mathbf{W}\\) se estará haciendo referencia una matriz de pesos espaciales.Para el cálculo de las matrices de vecindad se utilizará el paquete spedep (Bivand 2022). La función poly2nb() construye la relación de vecindad partir de los polígonos de un objeto espacial según el criterio y el orden que se indique; la función nb2listw() transforma la relación de vecindad en una lista de pesos espaciales. Para el ejemplificar la construcción de la matriz \\(\\mathbf{W}\\), se utilizará el conjunto de datos del estudio de Guerry (1833) utilizados en (Anselin 2017). Esta base de datos contiene información respecto estadísticas morales, criminales y sociales de las distintas provincias de Francia en 1830228.Para construir una matriz de contigüidad de la torre de orden 1 se utilizan las mismas funciones, cambiando el parámetro queen de la función poly2nb()Cuando se trabaja con datos nivel puntual, o cuando existen situaciones geográficas de contigüidad como, por ejemplo, una isla, la construcción de la matriz de contigüidad es tan evidente. En estos casos, resulta más oportuno definir la matriz de vecindad partir de criterios de distancia. Las matrices de \\(\\mathbf{W}\\) basadas en distancias pueden tener configuraciones continuas de la matriz respecto la distancia \\(d\\) entre las localidades \\(\\) y \\(j\\) de tal manera que \\(w_{ij}=1/d_{ij}\\) o \\(w_{ij}=1/d_{ij}^2\\) y \\(w_{ii}=0\\). Otras configuraciones implican considerar un numero \\(k\\) de vecinos más cercanos cada localidad de tal manera que:donde \\(d_i(k)\\) es la k-esima menor distancia entre las localidades \\(\\) y \\(j\\). Utilizando funciones de Bivand (2022) y E. Pebesma (2022) y los datos de Vallone Chasco (2020) se calculará la matriz de 5 vecinos más próximos de las áreas urbanas chilenas con más de 2000 habitantes.El uso de matrices de \\(k\\) vecinos puede forzar la vecindad entre localidades, considerando vecinas localidades que estén muy distantes entre ellas. Para evitar este problema se puede usar una configuración de vecindad basada en una distancia limite \\(d_{max}\\), de tal manera que:El problema de este criterio de vecindad es la posibilidad de generar unidades espaciales aisladas cuando \\(d_{max}\\) se fija en un valor muy bajo. Este problema se se evita fijando la distancia máxima (\\(d_{max}\\)) de tal manera que se asegure que todas las unidades espaciales tengan al menos un vecino.Debe considerarse que la configuración basada en \\(k\\) vecinos y en la distancia censurada dan lugar matrices binarias, mientras que las matrices \\(\\mathbf{W}\\) basadas en distancias, . Para realizar el cálculo de la matriz \\(\\mathbf{W}\\) basado en la distancia inversa (\\(1/d_{ij}\\)) se utilizará en mismo conjunto de datos que en las matrices \\(\\mathbf{W}\\) basadas en distancias. Dos elementos deben considerarse: utilizar una función decreciente respecto distancia (en este caso una hipérbola) para satisfacer la ley de Tobler (Waldo R. Tobler 1970) y segundo, dado que la incidencia que puede tener una localidad \\(j\\) que se encuentre muy lejana la localidad \\(\\) tiende cero, (Waldo R. Tobler 1970), la matriz suele censurarse una distancia máxima \\(d_{max}\\) partir de la cual la incidencia entre unidades espaciales es nula, el criterio para la fijación de \\(d_{max}\\) es el mismo que el utilizado para evitar la existencia de unidades espaciales aisladas.Se ha indicado anteriormente que la matriz \\(\\mathbf{W}\\) se utiliza para capturar los efectos del espacio partir de medias ponderadas, es decir mediante la matriz \\(\\mathbf{W}\\) se puede de construir el retardo espacial. El retardo espacial \\(\\mathbf{Wy}\\) de la variable \\(\\mathbf{y}\\) se obtiene al multiplicar dicha variable por la matriz \\(\\mathbf{W}\\); por tanto, cada elemento del retardo espacial puede ser interpretado como la media ponderada de las observaciones de la variable \\(\\mathbf{y}\\) en el vecindario de cada localidad \\(\\).","code":"\nlibrary(\"spdep\")\nlibrary(\"CDR\")\n\nreina<-poly2nb(guerry, queen=TRUE)\nw_reina<-nb2listw(reina, style=\"W\", zero.policy=TRUE)\nw_reina\n#> Characteristics of weights list object:\n#> Neighbour list object:\n#> Number of regions: 85\n#> Number of nonzero links: 420\n#> Percentage nonzero weights: 5.813149\n#> Average number of links: 4.941176\n#>\n#> Weights style: W\n#> Weights constants summary:\n#>    n  nn  S0     S1        S2\n#> W 85 7225 85 37.2761 347.6683\ntorre<-poly2nb(guerry, queen=FALSE)\nw_torre<-nb2listw(torre, style=\"W\", zero.policy=TRUE)\nlibrary(\"sf\")\n#Se extraen las coordenadas de las ciudades\ncoord <- st_coordinates(cities) \n# Calcula la vecindad de 5 vecinos más cercanos\nw5knn <- knearneigh(coord, k=5, longlat= T) |> knn2nb() \n#Calcula la k=1 matriz W\nknn1 <- knearneigh(coord) |> knn2nb()\n# Obtiene la distancia critica\ndistancia_critica <- max(unlist(nbdists(knn1,coord))) \n#genera la matriz de vencindad de distancia w_ij < d_max\nk1 <- dnearneigh(coord, 0, distancia_critica) \nw_dist <- nb2listw(k1)\nknn1 <- knearneigh(coord) |> knn2nb() \ndistancia_critica <- max(unlist(nbdists(knn1,coord))) \nk1 <- dnearneigh(coord, 0, distancia_critica)\n#Calcula la distancia entre los vecinos\ndist_list<- nbdists(k1, st_coordinates(cities)) \n#Calcula la distancia inversa\ni_dist_list <- lapply(dist_list, function(x) 1/x) \n#Crea la matriz W\nw_dist_i <- nb2listw(k1, glist=i_dist_list, style=\"W\") "},{"path":"cap-econom-esp.html","id":"medidas-de-autocorrelación-espacial","chapter":"Capítulo 42 Modelos econométricos espaciales","heading":"42.2 Medidas de autocorrelación espacial ","text":"Una buena herramienta para entender y comprender las medidas de autocorrelación espacial es el diagrama de Moran (Anselin 1996). El diagrama de Moran relaciona una variable con lo que sucede en su entorno mediante su retardo espacial en una gráfico de puntos. La Fig. 42.3 presenta un diagrama de Moran para la base clergy del conjunto de datos Guerry, esta variable muestra el ratio de sacerdotes católicos sobre la problación de cada provincia francesa. La linea horizontal discontinua en la Figura 42.3 muestra el promedio del retardo espacial, mientras que la linea vertical discontinua indica el promedio de la variable clergy. El dividir el diagrama partir de dichos promedios permite generar cuatro zonas: el área “HH” que contiene las localidades cuyo valor de la variable clergy es superior al promedio y su vecindario también. Las localidades que se sitúen en el área “LL” presentan valores de la variable cleargy inferiores al promedio y su entorno también. El área “LH” contiene las localidades cuyo valor de la variable cleargy es inferior al promedio, pero su vecindario supera el valor promedio, lo contrario sucede en el área “HL”.partir del diagrama es posible observar la situación de una variable respecto su entorno. Si las localidades se sitúan mayoritariamente en las zonas “HH” y “LL”, las localidades con altos valores (superiores al promedio) de la variable de interés están rodeadas por localidades con altos valores de dicha variable, y las localidades con valores bajos (inferiores al promedio) están rodeadas de localidades con valores bajos, lo cual es una señal de existencia de autocorrelación espacial positiva. Si la concentración tiene lugar en las áreas “HL” y “LH” la autocorrelación espacial negativa.\nFigura 42.3: Diagrama de Moran de la variable Clergy\nLa Fig. 42.3 da indicios de la existencia de autocorrelación positiva, es decir que las localidades francesas tienden estar rodeadas de localidades con numero similares de clérigos. El diagrama de Moran es una herramienta gráfica, para comprobar estadísticamente la existencia de autocorrelación espacial se utilizará el indicador \\(\\) de Moran.","code":"\nlibrary(\"spdep\")\nlibrary(\"CDR\")\nw_reina_francia<-poly2nb(guerry, queen=TRUE) |> \n  nb2listw()\n## Diagrama de Moran\nmoran.plot(guerry$clergy,w_reina_francia, xlab=\"Clergy\",\n           ylab=\"Retardo espacial de Clergy\")"},{"path":"cap-econom-esp.html","id":"el-indicador-i-de-moran","chapter":"Capítulo 42 Modelos econométricos espaciales","heading":"42.2.1 El indicador I de Moran","text":"Se define \\(\\mathbf{z}=\\mathbf{y}-\\bar{y}\\), el indicador de Moran se calcula como:Su campo de variación es \\([-1,1]\\) y el signo coincide con el tipo de autocorrelación: valores positivos son indicativos de autocorrelación positiva y valores negativos son indicadores de la existencia de autocorrelación negativa. Nótese que \\(\\) es sino el cociente entre la covarianza de la variable \\(\\mathbf{y}\\) y su retardo espacial \\(\\mathbf{Wy}\\), y la varianza de la variable \\(\\mathbf{y}\\). Por tanto, el coincide con el coeficiente de una regresión lineal de \\(\\mathbf{Wy}\\) sobre \\(\\mathbf{y}\\). La linea con pendiente positiva presente en 42.3 es precisamente el resultado de la regresión lineal de \\(\\mathbf{Wy}\\) e \\(\\mathbf{y}\\) y por tanto su pendiente es el indicador \\(\\) de Moran. En este sentido, cuanto mayor sea la pendiente de esta recta mayor será el grado de autocorrelación espacial existente. La significatividad estadística del indicador \\(\\) se establece bajo la hipótesis nula de aleatoriedad espacial. La aleatoriedad espacial implica la inexistencia de autocorrelación en la variable analiza; es decir, considera que la variable que se analiza está distribuida en forma aleatoria entre las localidades. En este contexto, \\(p\\)-valores bajos permiten rechazar la hipótesis de aleatoriedad espacial, indicando la existencia de autocorrelación espacial en la variable estudiada (Anselin 2013).El \\(p\\)-valor permite rechazar la hipótesis nula de aleatoriedad espacial favor de la existencia de autocorrelación positiva.","code":"\nmoran.test((guerry$clergy),w_reina_francia,randomisation=TRUE\n           ,alternative=\"two.sided\")\n#>\n#> Moran I test under randomisation\n#>\n#> data: (guerry$clergy)\n#> weights: w_reina_francia\n#>\n#> Moran I statistic standard deviate = 6.1632, p-value = 7.13e-10\n#> alternative hypothesis: two.sided\n#> sample estimates:\n#> Moran I statistic       Expectation         Variance\n#>       0.421118648      -0.011904762      0.004936422"},{"path":"cap-econom-esp.html","id":"modelos-econométricos-espaciales-de-corte-transversal","chapter":"Capítulo 42 Modelos econométricos espaciales","heading":"42.3 Modelos econométricos espaciales de corte transversal","text":"Los modelos espaciales deben ser identificados, antes de proceder su estimación y contraste. Para ello, es importante disponer de una estrategia de identificación propia, que permita al investigador estimar los parámetros poblacionales partir de la observación de una muestra de datos.Tradicionalmente, la econometría espacial ha resuelto este problema asumiendo que la especificación de los modelos es algo que se conoce priori, bien partir de la teoría económica existente o bien aplicando ciertas estrategias consistentes en la comparación de varios modelos competitivos. Dentro de esta última opción, se pueden destacar dos estrategias de modelización ampliamente utilizadas: la que va de lo particular (modelo básico sin efectos de autocorrelación espacial) lo general (modelo con variables explicativas espacialmente retardadas), y la que parte de un modelo general para terminar en un modelo de autocorrelación espacial más sencillo. partir estos los dos enfoques previos, es posible plantear la estrategia híbrida de Elhorst (2010).Según se presenta en la Fig 42.4, la estrategia híbrida comienza con la estimación de un modelo básico sin efectos espaciales:siendo \\(\\mathbf{y}\\) el vector de la variable dependiente, de orden \\((n\\times 1)\\); \\(\\mathbf{X}\\) la matriz de variables explicativas, de orden \\((n\\times k)\\); \\(\\boldsymbol{\\iota_n}\\) un vector formado por unos, de orden \\((n \\times 1)\\); \\(\\alpha\\), \\(\\boldsymbol{\\beta}\\) son el conjunto de \\((p+1)\\) parámetros estimar; y \\(\\boldsymbol{\\epsilon}\\) es el vector de perturbaciones aleatorias, de orden \\((n\\times 1)\\), que se distribuye como \\(\\boldsymbol{\\epsilon} \\sim N(\\mathbf{0},\\sigma_{\\epsilon}\\mathbf{}_n)\\), siendo \\(\\mathbf{}_n\\) la matriz identidad de orden \\(n\\).Este modelo se estima por MCO y luego se llevan cabo dos contrastes LM del Multiplicador de Lagrange sobre los errores de la regresión para contrastar si son ruido blanco desde el punto de vista espacial. Se trata de dos tests que contrastan una única hipótesis alternativa: el LMLAG, para la hipótesis de variable dependiente espacialmente retardada, y el LMERR, para la hipótesis de dependencia residual. La hipótesis básica se rechaza en cuanto que alguno de los estadísticos de contraste, que se distribuyen como una Chi cuadrado con 1 grado de libertad (\\(\\chi_1^2\\)) bajo la hipótesis nula, resulte estadísticamente significativo.En primer lugar, si alguno de los tests LM resulta significativo, se recomienda seleccionar el modelo Durbin espacial (SDM), que es un modelo general (Anselin 1988):siendo \\(\\rho\\) un parámetro y \\(\\boldsymbol{\\theta}\\) un vector de p parámetros autorregresivos espaciales.Este modelo general incluye dos tipos de interacción espacial: los efectos endógenos (\\(\\mathbf{Wy}\\)) y exógenos (\\(\\mathbf{WX}\\)). La variable endógena espacialmente retardada (\\(\\mathbf{Wy}\\)) referida al mismo momento del tiempo que la variable dependiente (\\(\\mathbf{y}\\)) produce en los estimadores MCO una situación de simultaneidad y, por tanto, sesgo, ineficiencia e inconsistencia. Por eso, se recomienda su estimación por el método de máxima verosimilitud, o “maximum likelihood” en inglés (ML), que supone normalidad en la distribución de los errores (ver Anselin (1988), Cap. 6).La estimación ML de este modelo permite utilizar la ratio de verosimilitud o “likelihood ratio” (LR), cuya distribución sigue una Chi al cuadrado con \\(k\\) grados de libertad, como estadístico para contrastar las hipótesis nulas \\(H_0 (\\boldsymbol{\\theta}=0)\\) y \\(H_0(\\rho=0)\\), siendo las hipótesis alternativas las opciones contrarias. En este punto, se pueden dar tres casos:Si se rechaza la primera hipótesis, pero sí la segunda, siempre y cuando los valores de los tests LMLAG > LMERR, el SDM debería simplificarse un modelo del retardo espacial o modelo autorregresivo espacial de orden 1 (SAR):Este modelo se estima por ML si los errores de la estimación por MCO se distribuyen como una normal.Si se rechaza la segunda hipótesis, pero sí la primera, y los valores de los tests LMERR > LMLAG, debería seleccionarse el modelo del error espacial (SEM):siendo \\(\\lambda\\) un parámetro autorregresivo espacial estimar. La estimación MCO produciría estimadores insesgados, consistentes, pero ineficientes. Por eso, se considera aceptable estimar el modelo SEM por MCO realizando una inferencia robusta de la matriz de varianzas y covarianzas de los estimadores por el método KP-HET propuesto por Kelejian Prucha (2010), que tiene en cuenta la existencia conjunta de heteroscedasticidad y autocorrelación espacial en los errores de la regresión.Si se rechazan ambas hipótesis nulas o hubiera acuerdo entre los resultados del test LR y los tests LM, entonces el SDM sería el modelo que mejor describiría los datos.En segundo lugar, si tras la estimación MCO del modelo básico ninguno de los tests LM fuera estadísticamente significativo, entonces dicho modelo tendría que ser reestimado como un modelo espacial regresivo transversal (SLX):Este modelo puede estimarse por MCO ya que, si las variables explicativas son exógenas, también lo serán sus correspondientes retardos espaciales. Este modelo puede considerar todas las variables exógenas espacialmente retardadas o un subconjunto de ellas, para contrastar la hipótesis nula \\(H_0(\\boldsymbol{\\theta}=0)\\). Si esta hipótesis fuese rechazada debería elegirse el modelo básico como el que mejor describe los datos, es decir, existiría evidencia alguna de la necesidad de efectos de autocorrelación espacial para explicar la variable dependiente. Pero si, por el contrario, la hipótesis \\(H_0(\\boldsymbol{\\theta}=0)\\) fuese rechazada, habría que estimar el modelo SDM con las variables \\(\\mathbf{WX}\\) estadísticamente significativas, para contrastar, de nuevo, la hipótesis nula \\(H_0(\\rho=0)\\). Si se rechaza esta hipótesis, el modelo seleccionado sería el SDM, pero en caso contraraio, sería el modelo SLX el que mejor describiría los datos.Todos estos modelos pueden también estimarse con metodología bayesiana utilizando el enfoque Markov Chains Monte Carlo (MCMC), tal y como se explica en J. LeSage Pace (2009) Cap. 5.\nFigura 42.4: Estrategia de especificación híbrida Elhorst (2010)\nEl siguiente conjunto de secuencias de código muestran cómo estimar los modelos que intervienen en la estrategia de modelización de Elhorst. Para ello, se utiliza un conjunto de datos de los 120 municipios grandes de España (capitales de provincia y ciudades con población superior 50.000 habitantes) que forman parte de las áreas urbanas de España (Mella Chasco (2006)). Con estos datos, se formula un modelo de crecimiento económico urbano en España, en el que la tasa media de variación del PIB per cápita, en logaritmos, durante el período 1985-2003 (LPGH), se explica en función del PIB per capita en logaritmos de 1985 (LGH85), la tasa de variación del número de entidades bancarias en el período 1985-2003 (BANK), el porcentaje de personas con educación secundaria y universitaria sobre la población de 16 y más años en el año 2001 (UNI01) y la tasa del número de patentes por habitante en el año 2000 (PAT00).","code":"\nlibrary(\"spatialreg\")\nlibrary(\"tseries\")\n\n# Matriz de pesos espaciales\ncoord <- sp::coordinates(gdpmap)\nk6 <- knearneigh(coord, k=6) \ndmins <-  knn2nb(k6) |> nb2listw(style=\"W\")\n# Estimación modelo básico por MCO\ngdp_ols <- lm(LPGH~LGH85+BANK+UNI01+PAT00, data=gdpmap)\nsummary(gdp_ols)\njarque.bera.test(gdp_ols$res) # Test normalidad de residuos\nlm.LMtests(gdp_ols, dmins, test=\"all\") # Grupo de tests LM\n\n# Estimación modelo SDM por ML\ngdp_sdm <- lagsarlm(LPGH~LGH85+BANK+UNI01+PAT00, data=gdpmap, listw=dmins, type=\"mixed\")\nsummary(gdp_sdm)\n\n# Estimación modeelo SAR por ML\ngdp_sar <- lagsarlm(LPGH~LGH85+BANK+UNI01+PAT00, data=gdpmap, listw=dmins)\nsummary(gdp_sar)\nLR.Sarlm(gdp_sdm, gdp_sar) # Test LR: SDM vs. SAR\n\n# Estimación del modelo SEM por ML\ngdp_err <- errorsarlm(LPGH~LGH85+BANK+UNI01+PAT00, data=gdpmap, listw=dmins, tol.solve=1e-16)\nsummary(gdp_err)\nLR.Sarlm(gdp_sdm, gdp_err) # Test LR: SDM vs. SEM\n\n# Estimación del modelo SLX por MCO\n\n# Cálculo retardos espaciales\ngdpmap$WLGH85 <- lag(dmins, gdpmap$LGH85)\ngdpmap$WBANK <- lag(dmins, gdpmap$BANK)\ngdpmap$WUNI01 <- lag(dmins, gdpmap$UNI01)\ngdpmap$WPAT00 <- lag(dmins, gdpmap$PAT00)\n\ngdp_slx <- lm(LPGH~LGH85+BANK+UNI01+PAT00+WLGH85+WBANK+WUNI01+WPAT00, data=gdpmap)\nsummary(gdp_slx) # Modelo SLX completo\n\ngdp_slx2 <- lm(LPGH~LGH85+BANK+UNI01+PAT00+WLGH85+WPAT00, data=gdpmap)\nsummary(gdp_slx2) # Modelo SLX parsimonioso\nLR.Sarlm(gdp_sdm, gdp_slx2) # Test LR: SDM vs. SLX"},{"path":"cap-econom-esp.html","id":"estimación-sar","chapter":"Capítulo 42 Modelos econométricos espaciales","heading":"42.3.1 Estimación SAR","text":"continuación se muestra la salida de la estimación del modelo SAR, pudiéndose observar que todos los parámetros estimados, incluido \\(\\rho\\), son estadísticamente significativos.","code":"\nlibrary(\"CDR\")\nlibrary(\"spatialreg\")\nlibrary(\"tseries\")\nlibrary(\"spdep\")\n\n# Matriz de pesos espaciales\ncoord <- sp::coordinates(gdpmap)\nk6 <- knearneigh(coord, k=6) \ndmins <-  knn2nb(k6) |> nb2listw(style=\"W\")\n# Estimación modeelo SAR por ML\ngdpsar <- lagsarlm(LPGH~LGH85+BANK+UNI01+PAT00, data=gdpmap,\n                       listw=dmins)\nsummary(gdpsar)\n\n#> Call:lagsarlm(formula = LPGH ~ LGH85 + BANK + UNI01 + PAT00, data = gdpmap,\n#> listw = dmins)\n#>\n#> Residuals:\n#> Min 1Q Median 3Q Max\n#> -0.0184657 -0.0034523 0.0012278 0.0032544 0.0194746\n#>\n#> Type: lag\n#> Coefficients: (asymptotic standard errors)\n#> Estimate Std. Error z value Pr(>|z|)\n#> (Intercept) 2.5745e-01 3.0703e-02 8.3851 < 2.2e-16\n#> LGH85 -3.2962e-02 4.4472e-03 -7.4119 1.246e-13\n#> BANK 6.6493e-05 1.3086e-05 5.0811 3.753e-07\n#> UNI01 4.6884e-04 8.4080e-05 5.5762 2.459e-08\n#> PAT00 4.7256e-02 1.8113e-02 2.6090 0.009082\n#>\n#> Rho: 0.36545, LR test value: 16.353, p-value: 5.2578e-05\n#> Asymptotic standard error: 0.078929\n#> z-value: 4.6302, p-value: 3.6538e-06\n#> Wald statistic: 21.438, p-value: 3.6538e-06\n#>\n#> Log likelihood: 447.309 for lag model\n#> ML residual variance (sigma squared): 3.7602e-05, (sigma: 0.006132)\n#> Number of observations: 122\n#> Number of parameters estimated: 7\n#> AIC: -880.62, (AIC for lm: -866.27)\n#> LM test for residual autocorrelation\n#> test value: 6.4996, p-value: 0.01079"},{"path":"cap-econom-esp.html","id":"comparando-sar-con-sdm","chapter":"Capítulo 42 Modelos econométricos espaciales","heading":"42.3.2 Comparando SAR con SDM","text":"continuación se muestra el resultado del test comparando el modelo SDM con el SAR. la luz de los valores de la LR se rechaza que el valor de los parámetros restringidos sea cero y, por tanto, el modelo SDM es más adecuado para explicar esta variable que el modelo SAR","code":"\n# Matriz de pesos espaciales\ncoord <- coordinates(gdpmap)\nk6 <- knearneigh(coord, k=6) \ndmins <-  knn2nb(k6) |> nb2listw(style=\"W\")\n\n# Estimación modelo SDM por ML\ngdpsdm <- lagsarlm(LPGH~LGH85+BANK+UNI01+PAT00, data=gdpmap,\n                         listw=dmins, type=\"mixed\")\n\n# Estimación modeelo SAR por ML\ngdpsar <- lagsarlm(LPGH~LGH85+BANK+UNI01+PAT00, data=gdpmap,\n                       listw=dmins)\n\nLR.Sarlm(gdpsdm, gdpsar) # Test LR: SDM vs. SAR\n\n#> Likelihood ratio for spatial linear models\n#>\n#> data:\n#> Likelihood ratio = 11.559, df = 4, p-value = 0.02095\n#> sample estimates:\n#> Log likelihood of gdpsdm Log likelihood of gdpsar\n#> 453.0885 447.3090"},{"path":"cap-econom-esp.html","id":"interpretación-de-los-estimadores-de-los-modelos-de-autocorrelación-espacial","chapter":"Capítulo 42 Modelos econométricos espaciales","heading":"42.3.3 Interpretación de los estimadores de los modelos de autocorrelación espacial ","text":"Sólo en los modelos de autocorrelación espacial en los que el efecto endógeno (\\(\\mathbf{Wy}\\)) está presente en la parte derecha del modelo, los coeficientes estimados (\\(\\hat{\\beta}\\)) pueden interpretarse de forma directa, como en el modelo básico sin efectos espaciales. Es decir, el efecto marginal de un cambio del valor de una variable explicativa continua en la variable explicada coincide con la estimación del coeficiente correspondiente dicha variable, para todas y cada una de las localizaciones.En los modelos SAR y SDM, la correcta interpretación de los estimadores implica antes pasar de su forma estructural su forma reducida. Así, por ejemplo, en el modelo SAR de la expresión (42.2) la forma reducida sería (bajo ciertas condiciones de invertibilidad):El término \\((\\mathbf{}-\\rho\\mathbf{W})^{-1}\\) se denomina multiplicador espacial y, utilizando la expansión potencial, puede expresarse también del modo siguiente:Si se utiliza esta nueva expresión en la ecuación (42.4) se observa más claramente que el valor de \\(\\mathbf{y}\\) en una determinada localización \\(\\) es función sólo del valor de las variables explicativas en esa localización, sino también del valor de las explicativas en las localizaciones vecinas (través de término \\(\\rho\\mathbf{WX}\\boldsymbol{\\beta}\\)), del valor de las explicativas en las localizaciones vecinas las vecinas (través del término \\(\\rho^2\\mathbf{W}^2\\mathbf{X}\\boldsymbol{\\beta}\\)), etc., hasta llegar los límites del sistema espacial en estudio.J. LeSage Pace (2009) presentan el cambio (también llamado efecto o impacto) experimentado por \\(\\mathbf{y}\\) en una localización \\(\\), sea cual sea \\(\\), debido un cambio en el valor \\(\\mathbf{x}_k\\) sobre otra localización, \\(j\\), sea cual sea \\(j\\). Dicho conjunto de impactos o efectos se presenta en una matriz completa, \\(\\mathbf{S}_k(\\mathbf{W})_{ij}\\), de orden \\(n\\times n\\). Así, cada variable explicativa \\(\\mathbf{x}_k\\) del modelo tendrá una matriz completa propia de impactos sobre la variable dependiente.En los modelos SAR y SDM, también podemos distinguir efectos directos e indirectos. El efecto directo sería el efecto causado por cambios en el valor de \\(\\mathbf{x}_k\\), en una localización \\(\\) sobre el valor de \\(\\mathbf{y}\\) en esa misma localización. Estos efectos son los valores de la diagonal principal de la matriz, \\(\\mathbf{S}_k(\\mathbf{W})_{ii}\\). El efecto indirecto viene dado por el resto de los valores de la matriz \\(\\mathbf{S}_k(\\mathbf{W})_{ij}\\), que serían los “bucles de retroalimentación” en los que el valor de \\(\\mathbf{x}_k\\), en una localización \\(j\\) afecta al valor de \\(\\mathbf{y}\\) en la localización \\(\\), y viceversa, pudiéndose dar recorridos más largos en los que el efecto en una localización podría llegar la última localización observada \\(n\\) y luego volver de nuevo al punto de partida.Por ejemplo, \\(\\mathbf{S}_k(\\mathbf{W})_{11}\\) es el efecto directo de un cambio unitario en el valor de la variable \\(\\mathbf{x}_k\\) en la localización 1 (\\(\\mathbf{x}_{k1}\\)) sobre el valor de la variable \\(\\mathbf{y}\\) en esa misma localización (\\(y_1\\)), mientras que el valor de \\(\\mathbf{S}_k(\\mathbf{W})_{12}\\) sería el efecto indirecto de un cambio unitario en el valor de la variable \\(\\mathbf{x}_{k}\\), en la primera localización, sobre el valor de la variable \\(\\mathbf{y}\\) en la segunda (\\(y_2\\)). En las filas, la matriz \\(\\mathbf{S}_k(\\mathbf{W})_{ij}\\) tiene los efectos de un cambio unitario en \\(\\mathbf{x}_k\\) en la variable \\(\\mathbf{y}\\) desde cada localización \\(\\) “hacia” todas y cada una de las localizaciones \\(j\\), mientras que las columnas representan el efecto de un cambio unitario en \\(\\mathbf{x}_k\\) en la variable \\(\\mathbf{y}\\), provocado “desde” todas y cada una de las localizaciones \\(\\) sobre la localización \\(j\\).Dado que es posible contrastar si todos los impactos directos e indirectos contenidos en la matriz \\(\\mathbf{S}_k(\\mathbf{W})_{ij}\\) son significativamente distintos de cero, o construir intervalos de confianza para ellos, LeSage y Pace proponen llevar cabo el proceso inferencial sobre el valor medio de los efectos directos y totales, extrayendo los efectos indirectos por diferencia: \\[\\begin{align}\n\\bar{M}(k)_{directo}=tr(\\mathbf{S}_k(\\mathbf{W}))/n \\\\\n\\bar{M}(k)_{total}=\\boldsymbol{\\iota'_n}\\mathbf{S}_k(\\mathbf{W})\\boldsymbol{\\iota_n}/n \\\\\n\\bar{M}(k)_{indirecto}=\\bar{M}(k)_{total}- \\bar{M}(k)_{directo}\n\\end{align}\\]donde \\(\\bar{M}\\) indica que se trata de un efecto promedio.El siguiente conjunto de secuencias presentan el cálculo de las matrices de efectos directos, indirectos y totales, y la inferencia para los modelos SAR y SDM correspondientes al ejemplo del modelo estimado para los municipios urbanos de España.","code":"\nlibrary(\"coda\")\n# Cálculo de los efectos para el modelo SAR (LeSage y Pace)\nWsp <- as(as_dgRMatrix_listw(dmins), \"CsparseMatrix\")\ntrMat <- trW(Wsp, type=\"mult\")\nset.seed(1234) # Simulaciones para el proceso inferencial\ngdpsar_impacts <- impacts(gdpsar, tr=trMat, R=1000)\nsummary(gdpsar_impacts, zstats=TRUE, short=TRUE)\nHPDinterval(gdpsar_impacts, choice=\"direct\")\nHPDinterval(gdpsar_impacts, choice=\"indirect\")\nHPDinterval(gdpsar_impacts, choice=\"total\")\nplot(gdpsar_impacts, choice=\"direct\")\nplot(gdpsar_impacts, choice=\"indirect\")\nplot(gdpsar_impacts, choice=\"total\")\nplot(gdpsar_impacts, trace=TRUE, density=FALSE, choice=\"total\")\n\n# Cálculo de la matriz de impactos para la variable LGH85\nclear.pr <- rep(NA,dim(gdpmap)[1])\nnames(clear.pr) <- gdpmap$MUNICIPIO\nsvec <- rep(0,dim(gdpmap)[1])\neye <- matrix(0,nrow=dim(gdpmap)[1],ncol=dim(gdpmap)[1])\ndiag(eye) <- 1\nfor(i in 1:length(clear.pr)){\n  cvec <- svec \n  cvec[i] <- 1\n  res <- solve(eye - gdpsar[[\"rho\"]]*Wsp) %*% cvec*gdpsar[[\"coefficients\"]][[\"LGH85\"]]\n  clear.pr[i] <- res[i]\n}\nmult <- solve(eye - gdpsar[[\"rho\"]]*Wsp)\nderiv_LGH85 <- solve(eye - gdpsar[[\"rho\"]]*Wsp)*gdpsar[[\"coefficients\"]][[\"LGH85\"]]\n\n# Cálculo de los efectos para el modelo SDM (LeSage y Pace)\nset.seed(1234) # Simulaciones para el proceso inferencial\ngdpsdm_impacts <- impacts(gdpsdm, tr=trMat, R=1000)\nsummary(gdpsdm_impacts, zstats=TRUE, short=TRUE)\nHPDinterval(gdpsdm_impacts, choice=\"direct\")\nHPDinterval(gdpsdm_impacts, choice=\"indirect\")\nHPDinterval(gdpsdm_impacts, choice=\"total\")\nplot(gdpsdm_impacts, choice=\"direct\")\nplot(gdpsdm_impacts, choice=\"indirect\")\nplot(gdpsdm_impacts, choice=\"total\")\nplot(gdpsdm_impacts, trace=TRUE, density=FALSE, choice=\"total\")"},{"path":"cap-econom-esp.html","id":"impacto-del-sdm","chapter":"Capítulo 42 Modelos econométricos espaciales","heading":"42.3.4 Impacto del SDM","text":"continuación se presenta la salida del cálculo de los efectos para el modelo SDM (J. LeSage Pace 2009) pudiéndose observar que todos son estadísticamente significativos y, salvo en el caso de la variable LGH85, todos ellos son positivos. El signo negativo del coeficiente de LGH85 demuestra la existencia de convergencia en renta en el grupo de grandes ciudades españolas. El impacto total de un crecimiento del 10% del PIB per cápita en una ciudad en el período inicial (1985) supuso una caída de la tasa media de variación del PIB per cápita en el período 1985-2003 del -0,63% en dicha ciudad. Este impacto es la suma del efecto directo causado por el crecimiento del PIB per cápita en la propia ciudad (-0,43), que es el efecto directo, y el efecto indirecto proveniente del crecimiento del PIB per cápita en el resto de ciudades (-0,20). Por su parte, el efecto total del crecimiento de 10 patentes por habitante supuso un crecimiento del PIB per cápita en el período del 4,38%, del cual un 0,5% procedía del crecimiento de las patentes per cápita en la propia ciudad efecto directo y el 3,88% restante fue causado indirectamente por el crecimiento de las patentes en el resto de ciudades.\nFigura 42.5: Impactos directos (SDM)\n\nFigura 42.6: Impactos indirectos (SDM)\n\nFigura 42.7: Impactos totales (SDM)\nComo puede observarse en la Fig. 42.5, la Fig. 42.6 y la Fig. 42.7, para cada variable explicativa se estiman tres estimadores, de forma que el efecto total causado por el cambio unitario en el valor de dicha variable sobre el valor de la variable explicada, en una ciudad determinada, es la suma de dos efectos, uno directo, ocasionado por el cambio acaecido en la propia ciudad, y otro indirecto, proveniente del cambio acaecido en el resto de ciudades de España, existiendo tantos efectos como ciudades.","code":"\n# Cálculo de los efectos para el modelo SAR (LeSage y Pace)\nWsp <- as(as_dgRMatrix_listw(dmins), \"CsparseMatrix\")\ntrMat <- trW(Wsp, type=\"mult\")\nset.seed(1234) # Simulaciones para el proceso inferencial\ngdp_sdm_impacts <- impacts(gdpsdm, tr=trMat, R=1000)\nsummary(gdp_sdm_impacts, zstats=TRUE, short=TRUE)\n#> Impact measures (mixed, trace):\n#> Direct Indirect Total\n#> LGH85 -3.967704e-02 -1.656718e-02 -5.624422e-02\n#> BANK 7.484574e-05 -3.878932e-05 3.605642e-05\n#> UNI01 5.459202e-04 3.900067e-04 9.359269e-04\n#> PAT00 5.029859e-02 1.621204e-01 2.124189e-01\n#> ========================================================\n#> Simulation results ( variance matrix):\n#> ========================================================\n#> Simulated standard errors\n#> Direct Indirect Total\n#> LGH85 4.835913e-03 1.205378e-02 1.208788e-02\n#> BANK 1.289395e-05 4.475846e-05 4.830235e-05\n#> UNI01 8.434442e-05 3.397464e-04 3.621312e-04\n#> PAT00 2.037202e-02 1.317136e-01 1.441993e-01\n#>\n#> Simulated z-values:\n#> Direct Indirect Total\n#> LGH85 -8.246556 -1.3907437 -4.6859617\n#> BANK 5.826543 -0.9252065 0.6980269\n#> UNI01 6.504595 1.2126777 2.6527099\n#> PAT00 2.454730 1.2664303 1.5035704\n#>\n#> Simulated p-values:\n#> Direct Indirect Total\n#> LGH85 2.2204e-16 0.16430 2.7865e-06\n#> BANK 5.6587e-09 0.35486 0.4851604\n#> UNI01 7.7903e-11 0.22525 0.0079848\n#> PAT00 0.014099 0.20536 0.1326920\nplot(gdp_sdm_impacts, choice=\"direct\")\nplot(gdp_sdm_impacts, choice=\"indirect\" )\nplot(gdp_sdm_impacts, choice=\"total\")"},{"path":"cap-econom-esp.html","id":"resumen-32","chapter":"Capítulo 42 Modelos econométricos espaciales","heading":"Resumen","text":"En este capítulo se introduce la componente espacial en la estimación econométrica y, en particular, el efecto de dependencia espacial inherente en alguna de las variables involucradas en el proceso de modelización. Primero, se observa la heterogeneidad espacial de los datos partir de los mapas temáticos y se presenta el indicador de autocorrelación espacial de Moran. Posteriormente, se construye la matriz de pesos espaciales bajo distintas especificaciones. Por último, se muestra la taxonomía de los modelos econométricos espaciales, presentado la estrategia de especificación híbrida y la interpretación de los coeficientes estimados.","code":""},{"path":"cap-pp.html","id":"cap-pp","chapter":"Capítulo 43 Procesos de puntos","heading":"Capítulo 43 Procesos de puntos","text":"Jorge Mateu\\(^{}\\) y Mehdi Moradi\\(^{b}\\)\\(^{}\\)Universidad Jaume \n\\(^{b}\\)Umeå Universitet","code":""},{"path":"cap-pp.html","id":"introducción-21","chapter":"Capítulo 43 Procesos de puntos","heading":"43.1 Introducción","text":"La estadística espacial es una rama de la estadística que se ha desarrollado rápidamente durante los últimos treinta años, tanto en el plano teórico como en el práctico. ello ha contribuido, de manera significativa, la creciente disponibilidad de potencia computacional y variedad en software, que han estimulado la capacidad de resolver problemas cada vez más complejos. Lo cierto es que estos problemas tienen como elemento común la estructura espacial. En general, se observa un desarrollo científico que ha ocurrido en el campo de la estadística espacial: problemas bien definidos con un carácter común saltaron la agenda del investigador, y la disponibilidad de datos motivaron nuevos desarrollos teóricos.La estadística espacial reconoce y explota las ubicaciones espaciales de los datos al diseñar, recopilar, administrar, analizar y mostrar dichos datos. Los datos espaciales suelen ser dependientes, y se necesitan clases de modelos espaciales que permitan la predicción de procesos y la estimación de parámetros. Patrones espaciales ocurren en una variedad sorprendentemente amplia de disciplinas científicas: los ecologistas estudian las interacciones entre plantas y animales, los silvicultores y agricultores deben investigar la capacidad de las plantas y tener en cuenta las variaciones del suelo en sus experimentos. Así pues, cualquier disciplina que trabaje con datos recopilados de diferentes ubicaciones espaciales, necesita desarrollar modelos que indican cuándo hay dependencia entre mediciones en diferentes lugares. Referencias modernas sobre estadística espacial incluyen los libros de Diggle (2013); N. Cressie Wikle (2015); José-Marı́Montero, Fernández-Avilés, Mateu (2015); Wikle, Zammit-Mangion, Cressie (2019); Diggle Giorgi (2019) entre otros.Este capítulo se centra en patrones espaciales de puntos. Datos en forma de un conjunto de puntos, distribuidos irregularmente dentro de una región del espacio, surgen en muchos contextos diferentes; por ejemplo localizaciones de incendios forestales (Fig. 43.1), delitos (Fig. 43.2), árboles en un bosque, nidos en una colonia de cría de pájaros, ubicación de núcleos en una sección microscópica de tejido, depósitos de oro mapeados en un estudio geológico, estrellas en un cúmulo estelar, accidentes de tráfico, terremotos, llamadas de teléfonos móviles, avistamientos de animales o casos de una enfermedad rara.Se llama patrón espacial de puntos, cualquier conjunto de datos de este tipo. La disposición espacial de los puntos es el principal foco de investigación. Son muchos los campos de la ciencia donde este tipo de estructuras son de interés; por ejemplo, en ecología, epidemiología, geociencia, astronomía, econometría e investigación criminal.\nEl análisis estadístico de la disposición espacial de los puntos puede revelar características importantes, como una tendencia que los yacimientos de oro se encuentren cerca de una gran falla geológica, o que los casos de una enfermedad sean más frecuentes cerca de una fuente de contaminación.El análisis de los datos de patrones de puntos ha proporcionado evidencia fundamental para importantes investigaciones, desde la transmisión del cólera hasta el comportamiento de los asesinos en serie y la estructura gran escala\ndel universo. Los puntos en un patrón de puntos pueden tener todo tipo de atributos. Un estudio forestal podría registrar cada ubicación, especie y diámetro del árbol; un catálogo de estrellas puede dar sus posiciones en el cielo, masas, formas y colores; las ubicaciones de los casos de enfermedades pueden estar vinculadas registros clínicos detallados. Esta información auxiliar adjunta cada punto en el patrón de puntos se llama marca y en ese caso se habla de un patrón de puntos marcado. La colección de localizaciones de un patrón puntual puede venir definida en una región plana (Sec. 43.2) o bien en una red lineal (Sec. 43.3), haciendo que las distancias dejen de ser euclidianas para pasar ser del camino más corto. Esto introduce ciertos cambios metodológicos en cuanto las construcciones de ciertas características, que en el caso de intensidades de primer orden trataremos en este capítulo.","code":""},{"path":"cap-pp.html","id":"secplana","chapter":"Capítulo 43 Procesos de puntos","heading":"43.2 Patrones puntuales espaciales en \\(\\mathbb R^2\\)","text":"La teoría de procesos puntuales espaciales constituye la base para el análisis de eventos observados geográficamente través de sus coordenadas (longitud, latitud) en un espacio bi-dimensional.\nEsta rama de los procesos puntuales pertenece al campo de la estadística espacial en conjunción con la de procesos estocásticos. De hecho, un proceso puntual espacial es un proceso estocástico cuyas realizaciones consisten en un conjunto numerable de puntos en el plano (patrón puntual). Heurísticamente, se trata de un conjunto de datos que se encuentran en una región concreta (o área de estudio).Sea \\({\\mathbf x}=\\{ x_1, x_2, \\ldots, x_n \\}, 0 \\leq n < \\infty,\\) una realización (patrón puntual) observada de un proceso puntual simple (.e. sin múltiples eventos por localización) y finito \\(X\\) en \\(\\mathbb R^2\\) en la región \\(W \\subset \\mathbb R^2\\) y con la métrica (distancia) asociada \\(d(u,v)\\). En general, las realizaciones consisten en un conjunto numerable de puntos (llamados en muchas ocasiones eventos). Consultar las Figs. 43.1 y 43.2 para ver algunos ejemplos de patrones puntuales. Para cualquier conjunto arbitrario \\(\\subset \\mathbb R^2\\), el cardinal de \\(X\\) viene dado por la función de conteo\n\\[\\begin{equation*}\nN(X \\cap )\n=\n\\sum\\limits_{x \\X} \\mathbf 1 \\{ x \\\\} < \\infty\n.\n\\end{equation*}\\]\nAdemás, y gracias la fórmula de Campbell (Baddeley, Rubak, Turner 2015), para cualquier función medible \\(f: \\mathbb R^2 \\[0, \\infty)\\) se cumple que\n\\[\\begin{equation*}\n\\mathbb E\n\\left[\n\\sum\\limits_{x \\X}\nf(x)\n\\right]\n=\n\\int_{\\mathbb R ^2}\nf(u)\n\\lambda(u)\n\\mathrm{d} u,\n\\tag{43.1}\n\\end{equation*}\\]\ndonde \\(\\lambda (\\cdot)\\) determina la función de intensidad de \\(X\\), y gobierna su distribución espacial. De hecho, \\(\\lambda(u)\\) proporciona el valor esperado de eventos por unidad de área en un entorno de \\(u \\\\mathbb R^2\\). Teniendo en cuenta que \\(f(x) = {\\mathbf 1} \\{ x \\\\}\\), se puede observar fácilmente la relación entre la función de intensidad \\(\\lambda(\\cdot)\\) y la de conteo \\(N\\), establecida como\n\\[\\begin{equation*}\n\\mathbb E\n\\left[\nN(X \\cap )\n\\right]\n=\n\\int_A\n\\lambda(u)\n\\mathrm{d} u.\n\\end{equation*}\\]\nSi la función de intensidad \\(\\lambda(\\cdot)\\) es constante, .e. \\(\\lambda(\\cdot) = \\lambda\\), se dice que el proceso \\(X\\) es homogéneo, mientras que, en caso contrario, se dice que es inhomogéneo; en este último caso, la distribución espacial varía lo largo de la región soporte. Para el lector con un mayor interés en conceptos y desarrollos, se aconseja consultar Møller Waagepetersen (2003);Illian et al. (2008);Diggle (2013) y Baddeley, Rubak, Turner (2015).En la práctica se suele observar sólo una única realización, y por ello es importante disponer de una estimación de \\(\\lambda(\\cdot)\\) que pueda imitar la distribución espacial del proceso subyacente, el cual ha generado el patrón observado. Por ello, se consideran diferentes tipos de estimadores paramétricos de la intensidad.","code":""},{"path":"cap-pp.html","id":"secestimaciones","chapter":"Capítulo 43 Procesos de puntos","heading":"43.2.1 Estimación de la intensidad basada en funciones núcleo","text":"Dos estimadores paramétricos, basados en funciones núcleo, de la función de intensidad ampliamente utilizados en patrones puntuales en \\(\\mathbb R^2\\), vienen dados por\n\\[\\begin{equation}\n\\widehat \\lambda^{\\text{U}}_{\\sigma}(u)\n=\n\\frac{1}{c_{\\sigma,W}(u)} \\sum_{=1}^n \\kappa_{\\sigma}(u - x_i),\n\\quad u \\W,\n\\tag{43.2}\n\\end{equation}\\]\ny\n\\[\\begin{equation}\n\\widehat \\lambda^{\\text{JD}}_{\\sigma}(u)\n=\n\\sum_{=1}^n \\frac{\\kappa_{\\sigma}(u - x_i)}{c_{\\sigma,W}(x_i)},\n\\quad u \\W,\n\\tag{43.3}\n\\end{equation}\\]\ndonde \\(\\kappa_{\\sigma}\\) es una función de densidad de probabilidad en \\(\\mathbb R^2\\) con parámetro de suavizado (ancho de banda) \\(\\sigma\\), y\\[\\begin{equation}\nc_{\\sigma,W}(u)\n=\n\\int_W \\kappa_{\\sigma}(u - v) \\mathrm{d} v,\n\\quad u \\W,\n\\end{equation}\\]\nes el área del núcleo centrado en \\(u \\W\\), y equivale un corrector de borde que compensa por la falta de información fuera de \\(W\\). Hay que recordar que, en la práctica, sólo se observa una realización de \\(X\\) en la región acotada \\(W\\). Más allá de la elección de \\(\\sigma\\), el estimador (43.2) es insesgado si la función de intensidad es constante (Diggle 1985), mientras que el estimator (43.3) conserva la masa total (Jones 1993). Los estimadores (43.2) y (43.3) suelen ser llamados ‘uniformly-edge-corrected’ y ‘Jones-Diggle’ (Rakshit et al. 2019). En este capítulo, se considera en todo momento la función núcleo Gaussiana (Bernard W. Silverman 1986).En términos prácticos, la adecuación de los estimadores basados en núcleos depende del parámetro de suavizado, de forma que un suavizamiento pequeño lleva un sesgo (por debajo) y varianza alta, mientras que un parámetro de suavizado alto resulta en un sesgo alto y poca varianza. Para un cierto patrón puntual \\({\\mathbf x}\\), los estimadores (43.2) y (43.3) pueden ser calculados utilizando la función density.ppp() de spatstat.core especificando diggle=FALSE y diggle=TRUE, respectivamente.","code":""},{"path":"cap-pp.html","id":"secbw","chapter":"Capítulo 43 Procesos de puntos","heading":"43.2.1.1 Selección del parámetro de suavizado","text":"Scott (1992) propuso elegir este parámetro través de un regla un tanto naive (llamada rule thumb), de la forma\n\\[\n(s_x n^{-1/6}, s_y n^{-1/6}),\n\\]\npara cada coordenada cartesiana \\(x,y\\), donde \\(s_x, s_y\\) son las desviaciones típicas de las coordenadas \\(x,y\\) de los eventos. Este procedimiento es útil para análisis exploratorios. La función ‘bw.scott’ de ‘spatstat.explore’ proporciona este estimador. Nótese que, en el caso de Scott, el parámetro de suavizado es, por construcción, un vector de dos componentes para suavizar ambas coordenadas cartesianas.Cronie Van Lieshout (2018) propusieron encontrar el parámetro óptimo minimizando\n\\[\nCvL(\\sigma)\n=\n\\left(\n|W|\n-\n\\sum\\limits_{=1}^n\n1\n/\n\\widehat{\\lambda}^*_{\\sigma}(x_i)\n\\right)^2,\n\\]\ndonde \\(\\widehat{\\lambda}^*_{\\sigma}(x_i)\\) es un estimador de la intensidad sin corregir (bien sea (43.2) o (43.3) pero sin el término de corrección) evaluado en \\(x_i\\) y con parámetro de suavizado \\(\\sigma\\). La idea de este estimador proviene de la fórmula de Campbell, ya que\n\\[\n\\mathbb E\n\\left[\n\\sum\\limits_{x \\X}\n1\n/\n\\lambda(x)\n\\right]\n=\n\\int_W\n(1\n/\n\\lambda(x)\n)\n\\lambda(x)\n\\mathrm{d} u\n=\n|W|.\n\\]\nPara un patrón puntual \\({\\mathbf x}\\), la función ‘bw.CvL’ de ‘spatstat.explore’ calcula el parámetro de suavizado mediante el método de Cronie y van Lieshout (se denotará por Cronie–van Lieshout).","code":""},{"path":"cap-pp.html","id":"ejemplos-prácticos","chapter":"Capítulo 43 Procesos de puntos","heading":"43.2.2 Ejemplos prácticos","text":"En esta sección se hace uso de los estimadores de la intensidad anteriormente mostrados y de los diferentes métodos de selección del parámetro de suavizado para analizar la distribución espacial de dos conjuntos de datos: incendios forestales en Nepal (Fig. 43.1), y eventos de crímenes en Medellín, Colombia (Fig. 43.2). En este capítulo, haremos uso de las librerías ‘spatstat, versión 2.3-0,’ (Baddeley Turner 2005a; Baddeley, Rubak, Turner 2015) para el análisis de patrones puntuales y ‘raster, versión 3.5-15,’ (Hijmans 2022a) para ciertas representaciones gráficas. Nótese que la librería ‘spatstat’ ha sido recientemente dividida en una familia de sub-librerías ‘spatstat.utils’, ‘spatstat.data’, ‘spatstat.sparse’, ‘spatstat.geom’, ‘spatstat.random’, ‘spatstat.core’, ‘spatstat.linnet’, ‘spatstat.explore’, ‘spatstat.model’, de forma que ‘spatstat’ actúa como una libería paraguas de todas ellas. Los lectores deben estar atentos posibles futuros cambios en ‘spatstat’ para satisfacer ciertas restricciones de CRAN en relación con los tamaños de sus librerías.","code":""},{"path":"cap-pp.html","id":"secnepalfire","chapter":"Capítulo 43 Procesos de puntos","heading":"43.2.2.1 Ejemplo 1: Incendios forestales en Nepal","text":"Por cortesía de Ganesh Prasad Sigdel, se dispone de localizaciones georeferenciadas de incendios forestales en Nepal durante 2016, datos cedidos por la institución ICIMOD-Nepal. En 2016, Nepal sufrió 5757 incendios, de los cuales 475 ocurrieron en el distrito de Surkhet, en la provincia de Karnali, en el medio-oeste de Nepal. Se comienza llamando algunas librerías de R, útiles para nuestros propósitos.Utilizando los métodos descritos en la Sec. 43.2.1.1, se estima el correspondiente parámetro de suavizado. La regla de Scott proporciona los valores (50253.47 m, 21158.42 m) y el método de validación cruzada de Cronie–van Lieshout estima el valor como 36513.16 m. Mediante el argumento ‘ns’ de ‘bw.CvL’, se puede controlar mejor la búsqueda del parámetro óptimo través de un grid más fino.Conocido el parámetro de suavizado, se estima la intensidad mediante los estimadores (43.2) y (43.3). La función ‘density.ppp’ proporciona una estimación basada en funciones núcleo para patrones en \\(\\mathbb R^2\\), teniendo en cuenta que, por defecto, esta función hace uso del estimador con corrección uniforme para los bordes (‘uniformly-edge-corrected estimator’) (43.2) con un núcleo Gaussiano. Se fija ‘leaveoneout=FALSE’ para calcular el estimador leave-one-, mientras que se establece ‘positive=TRUE’ para forzar valores positivos en la densidad. Esto último obedece que, debido errores numéricos en el cálculo de la Transformada Rápida de Fourier, se pueden obtener valores negativos en ciertas áreas (ver la ayuda de ‘density.ppp’).Se estima ahora la intensidad mediante el estimador de Jones-Diggle (43.2) escribiendo ‘diggle=TRUE’ en ‘density.ppp’.Tras obtener diferentes estimadores de la intensidad bajo diferentes métodos de selección del parámetro de suavizado, continuación se muestran estas estimaciones y se comentan sus discrepancias. Para una mejor representación gráfica, se convierten las imágenes de intensidad dadas en la clase ‘im’ objetos de clase ‘raster’ para luego juntarlas en un ‘RasterStack’. La Fig. 43.1 muestra estas estimaciones, observándose una mayor intensidad en el sur y sur-oeste de Nepal, indicando una clara distribución uniforme de dicha intensidad, lo que, su vez, indica un alto grado de inhomogeneidad.\nFigura 43.1: Estimación basada en funciones núcleo para los incendios forestales (puntos negros) en Nepal en 2016. Las etiquetas de los nombres comienzan con el método de suavizado, seguido del núcleo utilizado y de la corrección de borde. Los valores de la intensidad indican número de incendios por diez mil km cuadrados. Se usa JD y U para indicar los estimadores de ‘Jones-Diggle’ y ‘uniformly-edge-corrected’.\n","code":"\n#library(\"raster\")\n#library(\"spatstat\")\n#library(\"CDR\")\ndata(nepal)\nscott_nepal <- bw.scott(nepal) # Scott’s rule\nCvL_nepal <- bw.CvL(nepal) # Cronie and van Lieshout’s criterio\nd_scott_nepal <- density.ppp(nepal, sigma = scott_nepal, leaveoneout = FALSE, positive = TRUE)\nd_cvl_nepal <- density.ppp(nepal, sigma = CvL_nepal, leaveoneout = FALSE, positive = TRUE)\nd_scott_dig_nepal <- density.ppp(nepal, sigma = scott_nepal, leaveoneout = FALSE, positive = TRUE, diggle = TRUE)\nd_cvl_dig_nepal <- density.ppp(nepal, sigma = CvL_nepal, leaveoneout = FALSE, positive = TRUE, diggle = TRUE)\nsp_int_nepal <- stack(raster(d_scott_nepal), raster(d_cvl_nepal), raster(d_scott_dig_nepal), raster(d_cvl_dig_nepal))\nsp_int_nepal <- sp_int_nepal * 10^7\nnames(sp_int_nepal) <- c(\"scott_gaus_U\", \"CvL_gaus_U\", \"scott_gaus_JD\", \"CvL_gaus_JD\")\n\nat <- c(seq(0, 1.4, 0.2))\npts_nepal <- as.data.frame(nepal)\ncoordinates(pts_nepal) <- ~ x + y\nlibrary(\"latticeExtra\")\nspplot(sp_int_nepal, at = at, scales = list(draw = FALSE), col.regions = rev(topo.colors(20)), colorkey = list(labels = list(cex = 3)), par.strip.text = list(cex = 3)) + layer(sp.points(pts_nepal, pch = 20, col = 1))"},{"path":"cap-pp.html","id":"ejemplo-2-crímenes-en-medellín","chapter":"Capítulo 43 Procesos de puntos","heading":"43.2.2.2 Ejemplo 2: Crímenes en Medellín","text":"Medellín es la segunda ciudad con más población en Colombia (DANE 2019), con un territorio urbano de \\(105\\) km\\(^2\\), que ha sufrido de múltiples acciones criminales durante muchos años, como es bien concido. En 2018, la Secretaría de Seguridad de Medellín reportó que el \\(40\\%\\) de los ciudadanos se sentía inseguro, proporcinando, modo de ejemplo, 20607 quejas de robos (Restrepo 2019). Adicionalmente, el departamento de policía reconocía la necesidad de contratar al menos 2000 policías más para luchar contra los homicidios, robos y micro-tráfico (Monsalve 2019).En esta sección, sólo se analiza la distribución espacial de los eventos georeferenciados de crímenes ocurridos en Medellín durante 2005 (Sanabria et al. 2022). En 2005, ocurrieron 910 crímenes, de los cuales el porcentaje de víctimas varones fue del \\(66\\%\\), \\(28\\%\\) fueron cometidos durante los fines de semana, el porcentaje de robos fue del \\(42\\%\\), y el de víctimas con edades entre 20 y 40 fue del \\(60\\%\\).Nótese notar que el conjunto de localizaciones de estos crímenes necesariamente ocurrió en las calles de la ciudad, y por tanto se considera que el patrón puntual tiene como dominio de definición todo \\(\\mathbb R^2\\).La regla de Scott estima el parámetro de suavizado en (691.31m, 954.20m) mientras que el criterio de validación cruzada (CvL) nos lleva 692.31m. Se hace uso de la función ‘density.ppp’ para obtener los correspondientes estimadores de la intensidad (43.2) y (43.3) bajo los mismos escenarios que en la Sección 43.2.2.1.La Fig. 43.2 muestra la intensidad estimada bajo diferentes parámetros de suavizado. Se observa, en general, una distribución homogénea de los crímenes. Independientemente del método utilizado, se observan dos grandes hotspots en la zona central de Medellín, aunque con diferentes magnitudes. El efecto de la corrección de borde es sólo marginal.\nFigura 43.2: Estimación de la intensidad basada en funciones núcleo para los datos de Medellín (puntos negros), durante 2005. Las etiquetas de los nombres comienzan con el método de suavizado, seguido del núcleo utilizado y de la corrección de borde. Los valores de la intensidad indican número de crímenes por cien km cuadrados. Se usa JD y U para indicar los estimadores de ‘Jones-Diggle’ y ‘uniformly-edge-corrected’.\n","code":"\ndata(medellin)\nscott_med <- bw.scott(medellin) # Scott’s rule\nCvL_med <- bw.CvL(medellin) # Cronie and van Lieshout’s criterio\nd_scott_med <- density.ppp(medellin, sigma = scott_med, leaveoneout = FALSE, positive = TRUE)\nd_cvl_med <- density.ppp(medellin, sigma = CvL_med, leaveoneout = FALSE, positive = TRUE)\n\nd_scott_dig_med <- density.ppp(medellin, sigma = scott_med, leaveoneout = FALSE, positive = TRUE, diggle = TRUE)\nd_cvl_dig_med <- density.ppp(medellin, sigma = CvL_med, leaveoneout = FALSE, positive = TRUE, diggle = TRUE)\nsp_int_med <- stack(raster(d_scott_med), raster(d_cvl_med), raster(d_scott_dig_med), raster(d_cvl_dig_med))\nsp_int_med <- sp_int_med * 10^5\nnames(sp_int_med) <- names(sp_int_nepal)\nat <- seq(0, 3, by = 0.2)\npts <- as.data.frame(medellin)\ncoordinates(pts) <- ~ x + y\n\nsp::spplot(sp_int_med, at = at, scales = list(draw = FALSE), col.regions = rev(topo.colors(20)), colorkey = list(labels = list(cex = 3)), par.strip.text = list(cex = 3)) + layer(sp.points(pts, pch = 20))"},{"path":"cap-pp.html","id":"estimación-de-la-intensidad-basada-en-funciones-núcleo-en-dominios-irregulares","chapter":"Capítulo 43 Procesos de puntos","heading":"43.2.3 Estimación de la intensidad basada en funciones núcleo en dominios irregulares","text":"Los estimadores (43.2) y (43.3) pueden mostrar deficiencias importantes como cumplir la condición de que la integral sea el número de puntos, sesgo cerca de las fronteras o presentar suavizamientos artificiales que lleven resultados inverosímiles en ciertas ocasiones (Baddeley et al. 2022). Estos problemas son más aparentes en caso de dominios irregulares. Como remedio, Baddeley et al. (2022) propusieron estimar la intensidad via una función núcleo-calor (heat kernel), la cual puede ser definida como una densidad de probabilidad de transición de un movimiento Browniano en \\(W\\) que respeta las fronteras. De hecho, su propuesta, llamada estimador de difusión, toma la forma\n\\[\\begin{equation}\n\\widehat \\lambda_t (u)\n=\n\\sum\\limits_{=1}^n\n\\kappa_t (u|x_i)\n,\n\\tag{43.4}\n\\end{equation}\\]\ndonde \\(t= \\sigma^2\\) (\\(\\sigma\\) es el parámetro de suavizado en (43.2) y (43.3)) y \\(\\kappa_t (\\cdot|x_i)\\) es el núcleo-calor. Este estimador es insesgado (bajo homogeneidad) y preserva la masa (es decir, integra el número de puntos). Baddeley et al. (2022) proponen algunos nuevos métodos de selección del parámetro de suavizado, adaptados su estimador de difusión, incluyendo el de Cronie–van Lieshout. El estimador de difusión se puede calcular con la función ‘densityHeat.ppp’, y el criterio de Cronie–van Lieshout viene en la función ‘bw.CvLHeat’. Todas estas funciones pertenecen al ‘spatstat.explore’.continuación, se utiliza el estimador de difusión para analizar su comportamiento comparándolo, con el estimador (43.2) sobre unos datos de incendios activos en EEUU y América Central (sin considerar las islas) desde el 24 de Febrero al 3 de Marzo 2022. Hay que hacer notar que las localizaciones, en este caso, necesariamente confirman la existencia de un incendio, sino más bien píxeles susceptibles de existencia de incendio, los cuales han sido clasificados por medio de algoritmos preparados para ello. Este formato está relacionado con el contexto de datos en Near Real-Time (NRT).Los parámetros de suavizado para los estimadores (43.2) y (43.4) siguen el criterio de Cronie–van Lieshout. Nótese que al considerar un área mucho más grande que en los ejemplos precedentes, se considera ‘ns=50’, es decir, se usa un vector de tamaño 50 para buscar el parámetro de suavizado óptimo (por defecto es 16), y ‘dimyx=512’ para obtener imágenes de intensidad con una mejor resolución (por defecto, las imágenes son de tamaño \\(128\\times 128\\) píxeles). Los parámetros de suavizado elegidos para calcular (43.2) y (43.4) son 556.3km y 104.9km.Ambas estimaciones se juntan en un objeto RasterBrick que se representa en la Fig. 43.3. Obsérvese que el dominio es regular pues los estados de Florida, California del Sur y América Central hacen que la región objeto de estudio sea ciertamente irregular. En este caso, sería poco realista si el estimador utilizado proporciona intensidad en zonas como el Golfo de California/Mexico. El mapa de intensidad que se muestra la izquierda de la Fig. 43.3 muestra que el estimador con corrección uniforme (‘uniformly-edge-corrected’) distribuye la masa total por toda la región, provocando una sobre-suavización. Sin embargo, el mapa de la derecha, construido con el estimador de difusión, muestra una situación más realista, distribuyendo la masa de la intensidad acorde los sucesos ocurridos.\nFigura 43.3: Estimación basada en función núcleo para incendios (puntos negros) en EEUU y Centro América (sin las islas) desde el 24 de Febrero hasta el 3 de Marzo de 2022. Izquierda: estimador con corrección uniforme con núcleo Gaussiano. Derecha: estimador de difusión. El parámetro de suavizado fue obtenido con el criterio de Cronie–van Lieshout. Los valores de la intensidad son fuegos por mil km cuadrados.\n","code":"\ndata(activefires)\nCvL_northcentre <- bw.CvL(activefires, ns = 50)\nd_CvL_northcentre <- density.ppp(activefires, sigma = CvL_northcentre, leaveoneout = FALSE, dimyx = 512)\n\nheat_CvL_northcentre <- bw.CvLHeat(activefires, ns = 50)\ndheat_CvL_northcentre <- densityHeat.ppp(activefires, sigma = heat_CvL_northcentre, leaveoneout = FALSE, dimyx = 512)\nd_northcentre_stack <- stack(raster(d_CvL_northcentre), raster(dheat_CvL_northcentre))\nnames(d_northcentre_stack) <- c(\"CvL_gaus_U\", \"Diffusion\")\npts_northcentre <- as.data.frame(activefires)\ncoordinates(pts_northcentre) <- ~ x + y\nd_northcentre_stack <- d_northcentre_stack * 10^6\n\nspplot(d_northcentre_stack, scales = list(draw = FALSE), col.regions = rev(terrain.colors(20)), colorkey = list(labels = list(cex = 5)), par.strip.text = list(cex = 5)) + layer(sp.points(pts_northcentre, pch = 20, col = 1))"},{"path":"cap-pp.html","id":"estimadores-basados-en-teselaciones-de-voronoi","chapter":"Capítulo 43 Procesos de puntos","heading":"43.2.4 Estimadores basados en teselaciones de Voronoi","text":"Como se ha visto, el comportamiento de los estimadores basados en funciones núcleo depende del parámetro de suavizado, e incluso en situaciones en las que hay cambios abruptos en la distribución espacial de los puntos, un único valor constante de este parámetro puede representar el suavizamiento necesario en toda la región. Para dar una solución este problema, se propuso un parámetro con variación espacial (adaptable la estructura espacial) aunque costa de una mayor complejidad (Davies Baddeley 2018; Baddeley et al. 2022). Sin embargo, y como alternativa esta propuesta, se pueden utilizar estimadores basados en teselaciones de Voronoi, que son paramétricos (Barr Schoenberg 2010).Para cada \\(x\\{\\mathbf x}\\), su celda de Voronoi/Dirichlet \\({\\mathcal V}_{x}\\), consistente en todos los \\(u \\W\\) que están más cercanos \\(x\\) que cualquier otro elemento \\(y\\{\\mathbf x}\\setminus\\{x\\}\\), viene dada por\n\\[\\begin{align}\n\\mathcal V_{x}\n=\n\\{ u \\W: d(x,u) \\leq d(y,u) \\text{ para todo } y\\\\mathbf{x}\\setminus\\{x\\}\\}.\n\\end{align}\\]\nEl estimador basado en teselaciones de Voronoi, evaluado en cualquier punto arbitrario \\(u \\W\\), es de la forma\n\\[\\begin{align}\n\\widehat{\\lambda}^{V}(u)\n=\n\\sum_{x\\\\mathbf{x}}\n\\frac{\\mathbf 1\\{u\\\\mathcal V_{x}\\}}{|\\mathcal V_{x}|}.\n\\tag{43.5}\n\\end{align}\\]\nEl estimador \\(\\widehat{\\lambda}^{V}(u)\\) conserva la masa (al igual que \\(\\widehat \\lambda^{\\text{JD}}_{\\sigma}(u)\\)), y es insesgado si la intensidad real es constante (igual que \\(\\widehat \\lambda^{\\text{U}}_{\\sigma}(u)\\)), propiedades compartidas por el estimador de difusión. Sin embargo, Moradi et al. (2019) demostraron que \\(\\widehat{\\lambda}^{V}(u)\\) tiene una varianza alta, lo que implica una infrasuavización en áreas densas de puntos y una sobresuavización en áreas con pocos puntos. Por tanto, estos autores proponen corregir el problema de \\(\\widehat{\\lambda}^{V}(u)\\) mediante un sub-muestreo de \\(m \\geq 1\\) copias re-escaladas \\({\\mathbf x}\\) través de adelgazamientos independientes (independent \\(p\\)-thinning). Su propuesta viene dada por\n\\[\\begin{align}\n\\widehat{\\lambda}_{p,m}^{V}(u)\n=\n\\frac{1}{m}\\sum_{=1}^m\n\\frac{\\widehat{\\lambda}_i^{V}(u)}{p}\n,\n\\ u\\W,\n\\tag{43.6}\n\\end{align}\\]\ndonde \\(\\widehat{\\lambda}_i^{V}(u)\\) es el estimador de Voronoi del \\(\\)-ésimo patrón adelgazado. La idea es que este nuevo estimador balancee mejor la varianza en función de la cantidad de puntos presentes en la subregión, y este efecto se consigue con muestras de menor tamaño procedentes del patrón original. El estimador \\(\\widehat{\\lambda}_{p,m}^{V}(u)\\) se conoce como estimador de remuestreo-suavizado (resample-smoothed), y adicionalmente las propiedades estadísticas de \\(\\widehat{\\lambda}^{V}(u)\\), tiene una varianza bastante más pequeña. En este caso, también se debe seleccionar priori (\\(m,p\\)); sin embargo, Moradi et al. (2019) proponen tanto una rule--thumb (\\(m=400\\) y \\(p \\leq 0.2\\)) como una validación cruzada. Ambos estimadores (43.5) y (43.6) son accesibles por medio de la función ‘densityVoronoi.ppp’ de ‘spatstat.explore’ y en la que los argumentos ‘f’ y ‘nrep’ controlan la probabilidad \\(p\\) y el número de adelgazamientos \\(m\\). Fijando ‘f=1’ se puede obtener el estimador basado en Voronoi (43.5).modo de ejemplo, se estima la intensidad de los incendios en Nepal (Sec. 43.2.2.1) mediante el método de Voronoi resample-smoothed (43.6) considerando diferentes probabilidades de retención para el adelgazamiento correspondiente.Las estimaciones obtenidas, al igual que las que proceden de density.ppp, son de clase im y se unen en un objeto RasterBrick para su representación gráfica.La Fig. 43.4 muestra las intensidades procedentes de los estimadores Voronoi resample-smoothed para los incendios de Nepal, y para diferentes probabilidades de retención. Se puede observar un menor suavizamiento y mayor varianza para altas probabilidades. Asímismo, se puede ver que para probabilidades de retención menores que 0.2, el estimador proporciona mejores suavizamientos locales que los basados en suavizamientos fijos.\nFigura 43.4: Estimaciones de la intensidad de Voronoi resample-smoothed para los incendios en Nepal en 2016 para diferentes probabilidades de retención. La intensidad proporciona el número de incendios por diez mil km cuadrados.\n","code":"\nd_vor_1_nepal <- densityVoronoi.ppp(nepal, f = 1, nrep = 1)\nd_vor_2_nepal <- densityVoronoi.ppp(nepal, f = 0.8, nrep = 400)\nd_vor_3_nepal <- densityVoronoi.ppp(nepal, f = 0.6, nrep = 400)\nd_vor_4_nepal <- densityVoronoi.ppp(nepal, f = 0.5, nrep = 400)\nd_vor_5_nepal <- densityVoronoi.ppp(nepal, f = 0.4, nrep = 400)\nd_vor_6_nepal <- densityVoronoi.ppp(nepal, f = 0.2, nrep = 400)\nd_vor_7_nepal <- densityVoronoi.ppp(nepal, f = 0.1, nrep = 400)\nd_vor_8_nepal <- densityVoronoi.ppp(nepal, f = 0.05, nrep = 400)\nsp_int_nepal_v <- stack(raster(d_vor_1_nepal), raster(d_vor_2_nepal), raster(d_vor_3_nepal), raster(d_vor_4_nepal), raster(d_vor_5_nepal), raster(d_vor_6_nepal), raster(d_vor_7_nepal), raster(d_vor_8_nepal))\nnames(sp_int_nepal_v) <- NULL\nnames <- as.character(sort(c(seq(.2, 1, .2), 0.1, 0.05, 0.5), decreasing = TRUE))\nnames <- paste(\"p =\", names)\n\nsp_int_nepal_v <- sp_int_nepal_v * 10^7\nat <- c(0, 0.3, 0.7, seq(2, 5, 1), 30)\n\nspplot(sp_int_nepal_v, at = at, colorkey = list(labels = list(cex = 3)), col.regions = topo.colors(20), scales = list(draw = FALSE), par.strip.text = list(cex = 3), names.attr = names)"},{"path":"cap-pp.html","id":"características-de-segundo-orden-la-función-k-de-ripley","chapter":"Capítulo 43 Procesos de puntos","heading":"43.2.5 Características de segundo orden: la función \\(K\\) de Ripley","text":"La función de intensidad presentada en las secciones anteriores describe el número esperado de puntos por unidad de espacio, y tiene en cuenta la estructura de dependencia entre dichos puntos. Esta estructura, sin embargo, viene caracterizada través de lo que se llaman características de segundo orden. Las funciones de segundo orden determinan la estructura de dependencia espacial (o en su caso espacio-temporal, si interviene el tiempo) inherente al patrón puntual. La literatura ha propuesto varias funciones de segundo orden, de entre las cuales la función \\(K\\) de Ripley es posiblemnte la más utilizada. Esta función se define de forma pragmática como el número medio de eventos en un radio \\(r\\) alrededor de cualquier otro evento. Dicho de otra forma, la función \\(K(r)\\) representa el número medio de eventos dentro de un círculo de radio \\(r\\) alrededor de un evento típico del patrón (sin contar dicho evento central). De esta forma, \\(K(r)\\) describe características del proceso de puntos muchas escalas (tantas como diferentes \\(r\\) se consideren). Esta función puede venir corregida por la intensidad de primer orden en el caso de procesos inhomogéneos. Ambas versiones de la función \\(K\\) vienen implementadas en ‘spatstat’ través de las funciones ‘Kest’ y ‘Kinhom’ para los casos homogéneo e inhomogéneo. Una propiedad interesante de esta función es que tiene una forma cerrada bajo el caso de aleatoriedad espacial completa, es decir, bajo la situación en la que el patrón de puntos es totalmente aleatorio, sin dependencia espacial alguna (llamado, en este caso, proceso de Poisson). Como bajo esta suposición, \\(K(r)=\\pi r^2\\) se puede contrastar si un cierto patrón es o aleatorio, construyendo bandas de confianza sobre la función \\(K\\) evaluada bajo simulaciones de aleatoriedad y evaluando la función \\(K\\) empírica procedente de los datos. La función ‘envelope’ construye tales intervalos de confianza.También se han utilizado otras funciones para describir y contrastar patrones espaciales; estas funciones están basadas en la distribución de distancias entre puntos que existiría en un patrón de Poisson, como por ejemplo, la función de distribución de distancias al vecino más próximo, la función de distribución de distancias un punto fijo aleatorio, o la función \\(J\\), una combinación de las anteriores. Todas estas funciones, incluida la función \\(K\\), son en cierta forma funciones de distribución ya que, cada escala o distancia \\(r\\), todos los pares de puntos separados por una distancia menor que \\(r\\) se usan para estimar el valor de la correspondiente función. En ocasiones, puede ser necesario disponer de una función que caracterice de forma acumulativa el patrón, es decir, que tenga en cuenta tan sólo los pares de puntos que se encuentran separados por una distancia exactamente igual o similar la distancia \\(r\\). La función de correlación de par \\(g(r)\\) (pair correlation function) es la herramienta apropiada en este caso (Baddeley, Rubak, Turner 2015).continuación se muestra el código y resultados de llevar la práctica la estimación de la funcion \\(K\\) (inhomogénea) y los correspondientes intervalos de confianza bajo aleatoriedad para los casos de incendios en Nepal y delitos en Medellín.\nFigura 43.5: Funciones \\(K\\) de Ripley para incendios en Nepal y delitos en Medellín\n","code":"\nd_nepal <- density.ppp(nepal, bw.scott, leaveoneout = TRUE)\nen_nepal <- envelope(nepal, fun = Kinhom, correction = \"border\", nsim = 99, simulate = expression(rpoispp(d_nepal)), sigma = bw.scott, normpower = 2)\n\nd_med <- density.ppp(medellin, bw.scott, leaveoneout = TRUE)\nen_med <- envelope(medellin, fun = Kinhom, correction = \"border\", nsim = 99, simulate = expression(rpoispp(d_med)), sigma = bw.scott, normpower = 2)\nen_nepal$mmean <- NULL\nplot(en_nepal, main = \"\", lwd = 3, cex.axis = 2.5, cex.lab = 2.5, legend = FALSE)\n\nen_med$mmean <- NULL\nplot(en_med, main = \"\", lwd = 3, cex.axis = 2.5, cex.lab = 2.5, legend = FALSE)"},{"path":"cap-pp.html","id":"seclineal","chapter":"Capítulo 43 Procesos de puntos","heading":"43.3 Patrones puntuales espaciales sobre redes lineales","text":"En los últimos diez años, los patrones de puntos en redes lineales han recibido mucha atención científica. Una red lineal es un conjunto de segmentos (o aristas) unidos por nodos con un formato lineal, tipo una combinación convexa entre dos nodos. La explicación inicial detrás de la consideración de redes lineales, como espacios de estado de algunos procesos puntuales, podría estar en el hecho de que los objetos definidos en tales estructuras pueden usar todo el espacio, y sus movimientos dependen fuertemente de su libertad sobre tales estructuras (Okabe Sugihara 2012). En consecuencia, entre otras cosas, la distribución espacial de los puntos, así como la correlación entre ellos, debe estudiarse con respecto la red subyacente. Sin embargo, ha sido tan fácil lidiar con este cambio de soporte cuando se pretende adaptar metodologías estadísticas para el análisis de patrones de puntos en redes lineales. Los principales desafíos fueron solo matemáticos/estadísticos, sino también computacionales (Moradi 2018; Baddeley et al. 2021).Una red lineal es una unión de segmentos de línea \\(l_i=[u_i,v_i]=\\{tu_i + (1-t)v_i:0\\leq t\\leq 1\\} \\subset \\mathbb R^2\\), y una elección común de métrica sobre dicha estructura ha sido inicialmente la distancia de ruta más corta (shortest-path distance) \\(d_L(u,v)\\), aunque más tarde Rakshit, Nair, Baddeley (2017) propusieron otros tipos de distancias, incluida la distancia euclidiana. La idea es que moverse por la red de segmentos implica respetar la geometría de dicha red y por tanto las lineas rectas (que sería el caso de usar distancias euclideanas) son adecuadas. La distancia de ruta más corta si que permite adaptarse esta geometría. Sea \\(Y\\) un proceso puntual en una red lineal \\(L\\), la fórmula de Campbell (43.1) se adapta como\n\\[\\begin{equation*}\n\\mathbb E\n\\left[\n\\sum\\limits_{y \\Y}\nf(y)\n\\right]\n=\n\\int_{L}\nf(z)\n\\lambda(z)\n\\mathrm{d}_1 z,\n\\end{equation*}\\]\ndonde \\(\\mathrm{d}_1\\) denota integración con respecto la longitud de arco. En este caso, \\(\\lambda(z)\\) proporciona el número esperado de puntos por unidad de longitud de \\(L\\) en una vecindad de \\(z \\L\\). Se han desarrollado distintos estimadores de la intensidad para patrones en redes considerando métricas adecuadas y resolviendo ciertos obstáculos matemáticos. El lector puede leer más al respecto en Moradi (2018) y Baddeley et al. (2021); en particular, se recomienda leer sobre el método de estimación noparamétrica basado en convoluciones bi-dimensionales de (Rakshit et al. 2019). Dada una realización \\({\\mathbf y}= \\{ y_1, y_2, \\ldots, y_n \\}\\) de un proceso puntual \\(Y\\) sobre una red lineal \\(L\\), estos autores propusieron\n\\[\\begin{equation}\n    \\widehat{\\lambda}^\\text{U}_{\\sigma}(z)\n    =\n    \\frac{1}{c_{\\sigma,L}(z)}\n    \\sum_{=1}^{n}\n    \\kappa_{\\sigma}(z-y_i),\n    \\qquad\n    z \\L,\n    \\tag{43.7}\n\\end{equation}\\]\ncon una corrección uniforme, y\\[\\begin{equation}\n    \\widehat{\\lambda}^\\text{JD}_{\\sigma}(z)\n    =\n    \\sum_{=1}^{n}\n    \\frac{\n    \\kappa_{\\sigma}(z-y_i)\n    }{\n    c_{\\sigma,L}(y_i)\n    },\n    \\qquad z \\L,\n    \\tag{43.8}\n\\end{equation}\\]\ncon la corrección de Jones-Diggle, donde \\(\\kappa_{\\sigma}\\) es una función núcleo bivariante con suavizado \\({\\sigma}\\), y\n\\[\\begin{equation*}\n    c_{\\sigma,L}(z)=\\int_L \\kappa_{\\sigma}(z-v) \\mathrm{d}_1 v,\n\\end{equation*}\\]\nes una corrección de borde.Los dos estimadores anteriores tienen propiedades estadísticas similares las de sus análogos para patrones de puntos espaciales en \\(\\mathbb R^2\\) (es decir, los estimadores (43.2) y\n(43.3)), y se pueden calcular rápidamente incluso en redes grandes y para grandes anchos de banda (parámetros de suavización). El cálculo rápido se logra mediante la transformada rápida de Fourier (FFT) (Bernhard W. Silverman 1982). Además, Rakshit et al. (2019) propusieron utilizar las versiones adaptadas de la regla de Scott, la cual se puede acceder través de la funciones ‘bw.scott.iso’ de ‘spatstat.linnet’, para obtener un ancho de banda óptimo. Nótese que el cálculo rápido de los estimadores anteriores simplifica aún más el cálculo de los estimadores de intensidad basados en el núcleo adaptativo y el riesgo relativo sobre las estructuras de red (Rakshit et al. 2019).También se recuerda que Moradi et al. (2019) propusieron su enfoque de sub-muestreo basado en Voronoi para procesos de puntos generales; para patrones de puntos en redes lineales puede calcularse mediante la función ‘densityVoronoi.lpp’ de ‘spatstat.linnet’.Como ejemplo práctico para esta sección, se estudia la distribución espacial de delitos callejeros en Valencia. Valencia es la tercera ciudad más grande de España, siendo la capital de la Comunidad Valenciana. El territorio urbano de Valencia encierra un área de 134,65 km\\(^2\\), con más de 800000 habitantes en el municipio. El conjunto de datos consta de las ubicaciones de 90247 delitos callejeros como agresión (55610 casos), robo (25342 casos), robo contra la mujer con violencia (454 casos) y otros tipos de delitos (8841 casos). Estos delitos se cometieron entre 2010 y 2020. Sin embargo, en lo que sigue, el análisis únicamente se centra en los datos correspondientes al año 2020, que incluye 6868 casos, de los cuales 4077 son agresiones, 2060 son robos y 66 se relacionan con delitos contra la mujer con violencia. Este conjunto de datos es propiedad de la Generalitat Valenciana (GV), se obtuvieron través del teléfono de emergencias 112, y se puso disposición de los autores gracias un convenio entre GV y la Universidad Jaume .continuación, estimar el parámetro de suavizado utilizando la regla general de Scott, que da 584,1m. La función ‘densityQuick.lpp’ de ‘spatstat.linnet’ se usa para calcular cualquiera de los estimadores (43.7) y (43.8) en los que su valor predeterminado calcula el estimador de borde uniforme corregido (43.7).Las imágenes obtenidas son de tipo ‘linim’, y se convierten en objetos de clase ‘im’ antes de pasarlas objetos ‘raster’.La Fig. 43.6 muestra la intensidad estimada junto con los eventos de delitos. Dicha intensidad identifica las zonas central y norte de la ciudad de Valencia como áreas de alto riesgo junto\ncon otras zonas de bajo riesgo como son el este y la costa de la ciudad.\nFigura 43.6: Intensidad estimada por función núcleo, usando un estimador borde uniforme corregido (izquierda), para los datos de delitos (puntos rojos) en Valencia durante 2020 (derecha). Los valores de intensidad muestran número de crímenes por km lineal.\nFinalmente, se muestra la función \\(K\\) de Ripley y el intervalo de confianza bajo un proceso de Poisson en una red lineal (ver Fig. 43.7 (Ang, Baddeley, Nair 2012; Rakshit, Baddeley, Nair 2019). Se observa que la función \\(K\\) empírica cae dentro de las bandas indicando que el tipo de delitos considerados, en 2020, es compatible con un proceso aleatorio. Obsérvese que al considerar el tiempo, podemos detectar clusters espaciales que existen en realidad pues estos desaparecerían con la evolución temporal.\nFigura 43.7: Función \\(K\\) para los delitos en Valencia, junto con la envoltura bajo un proceso de Poisson.\n","code":"\ndata(valencia)\nscott_valencia <- bw.scott.iso(valencia) # Scott rule\nd_scott_valencia <- densityQuick.lpp(valencia, sigma = scott_valencia, leaveoneout = FALSE, positive = TRUE, dimyx = 512)\nd_scott_valencia <- d_scott_valencia * 1000\npar(mfrow = c(1, 2))\nplot(valencia$domain$window, lwd = 4)\nplot(valencia, pch = 20, main = \"\", lwd = 4, cex = 1, add = T, cols = \"red\", col = \"blue\")\nplot(raster(as.im(d_scott_valencia)), main = \"\", axis.args = list(cex.axis = 4), legend.width = 2, zlim = c(0, 6))\nplot(valencia$domain$window, add = TRUE, lwd = 4)\npar(mfrow = c(1, 1))\nd_vlc <- densityQuick.lpp(valencia, sigma = scott_valencia, leaveoneout = TRUE, positive = TRUE, at = \"points\", dimyx = 512)\nd_vlc_im <- densityQuick.lpp(valencia, sigma = scott_valencia, leaveoneout = TRUE, positive = TRUE, dimyx = 512)\nsim_vlc <- rpoislpp(lambda = d_vlc_im, L = net_vlc, nsim = 199)\nlibrary(spatstat.Knet)\nK_vlc <- Knetinhom(valencia, lambda = as.numeric(d_vlc))\nr <- K_vlc$r\n\nK_sim <- lapply(X = 1:199, function(i) {\n  sigma <- bw.scott.iso(sim_vlc[[i]])\n  lambda <- densityQuick.lpp(sim_vlc[[i]], sigma = sigma, leaveoneout = TRUE, positive = TRUE, at = \"points\", dimyx = 512)\n  Ksim <- Knetinhom(sim_vlc[[i]], lambda = as.numeric(lambda), r = r)\n  return(Ksim)\n})\nK_nsim_df <- as.data.frame(do.call(cbind, d_nsim))\nK_nsim_df_est <- K_nsim_df[, seq(3, 399, by = 2)]\n\nmaxn <- function(n) function(x) order(x, decreasing = TRUE)[n]\nminn <- function(n) function(x) order(x, decreasing = FALSE)[n]\n\nKmin <- apply(K_nsim_df_est, 1, function(x) x[minn(5)(x)])\nKmax <- apply(K_nsim_df_est, 1, function(x) x[maxn(5)(x)])\nplot(r, Kmin, type = \"n\", col = \"grey\", ylim = c(0, 270), xlab = \"r\", ylab = expression(italic(K[inhom])))\npoints(r, Kmax, type = \"n\", col = \"grey\")\npolygon(c(r, rev(r)), c(Kmax, rev(Kmin)), col = \"grey\", border = \"grey\")\npoints(r, K_vlc$est, type = \"l\")"},{"path":"cap-pp.html","id":"resumen-33","chapter":"Capítulo 43 Procesos de puntos","heading":"Resumen","text":"La teoría de procesos puntuales espaciales constituye la base para el análisis de eventos observados geográficamente través de sus coordenadas (longitud, latitud) en un espacio bi-dimensional. Esta es una de las ramas del campo de la estadística espacial en conjunción con la de procesos estocásticos. De hecho, un proceso puntual espacial es un proceso estocástico cuyas realizaciones consisten en un conjunto numerable de puntos (llamados en muchas ocasiones eventos). Heurísticamente, se trata de un conjunto de datos que se encuentran en una región concreta (área de estudio). Los puntos pueden representar cualquier población espacialmente explícita, como localizaciones de animales, nidos de aves, epicentros de terremotos, galaxias, crímenes, etc. El modelo estadístico más conocido para el análisis de patrones puntuales espaciales es el proceso puntual espacial de Poisson (asociado la condición de aleatoriedad espacial completa). partir del modelo de Poisson se construyen modelos más complejos. La modelización pasa por determinar las intensidades de primer y segundo orden que caracterizarán las propiedades básicas del comportamiento de los puntos. En este capítulo se proponen varios estimadores de la función de intensidad de primer orden junto con sus elementos asociados relacionados con funciones núcleo, parámetro de suavizado y correcciones de borde. Se consideran también algunos aspectos de medidas de segundo orden. El capítulo finaliza con aspectos sobre el cambio de soporte del plano euclídeo redes lineales.","code":""},{"path":"id_120007-informes.html","id":"id_120007-informes","chapter":"Capítulo 44 Informes reproducibles con R Markdown y Quarto","heading":"Capítulo 44 Informes reproducibles con R Markdown y Quarto","text":"Emilio L. CanoUniversidad Rey Juan Carlos","code":""},{"path":"id_120007-informes.html","id":"por-qué-informes-reproducibles","chapter":"Capítulo 44 Informes reproducibles con R Markdown y Quarto","heading":"44.1 ¿Por qué informes reproducibles?","text":"\nEl resultado final de un proyecto de análisis de datos terminará\ncomunicándose distintos niveles, tanto aguas arriba como aguas abajo.\nEsta comunicación es “la última milla” del flujo de análisis que se\nesquematizaba en la Fig. ??. Se llama genéricamente\n“informe” cualquiera de estos resultados que se pueden producir en\ndistintos formatos de destino. Estos informes estarán compuestos de múltiples elementos como\ntexto, gráficos, resultados numéricos, tablas, etc. Además, es posible que haya\nque generarlos en distintos formatos (por ejemplo html o pdf, entre otros), para diferentes destinos, como la Web, documentos imprimibles o\npresentaciones. Finalmente, con una alta probabilidad varias personas\nintervendrán en el proceso, y la trazabilidad (reproducibilidad) del análisis\nmejorará el proyecto de forma global.La forma de abordar el problema es típicamente con un enfoque corta-pega,\nen el que primero se realiza todo el análisis de datos con el software\nestadístico y después se utilizan\nlos resultados del análisis como base de un informe escrito, posiblemente\ncon algunas iteraciones del proceso si el proyecto tiene cierta envergadura.\nEl software estadístico comercial suele incluir formas de generar resultados listos\npara integrar en un informe, pero habitualmente bajo este paradigma de incluir el\nresultado posteriori (Leisch 2002).Esta forma de trabajar genera un alto coste de mantenimiento (debido la regeneración manual del informe) la vez que\nprovoca inconsistencias (por ejemplo entre unos grupos y otros,\nentre diferentes analistas, etc.), errores, contenidos desactualizados o \nreproducibles. Este enfoque es propenso fallos de organización y gestión de un proyecto, lo se traduce en\nvulnerabilidades especialmente en la ejecución de software, simulaciones, etc.\nAdemás, cada vez que hay que hacer un cambio, hay que hacerlo en muchos\nsitios, con la consiguiente pérdida de tiempo y posibles errores.El enfoque de la investigación reproducible229 supera muchos de los obstáculos la\nhora de preparar informes de análisis de datos. El objetivo es\nunir instrucciones para análisis de datos con datos experimentales de forma que\nlos resultados se puedan volver obtener automáticamente, entender mejor y verificar.\nUn concepto muy relacionado que se utiliza en R es la programación\nliteraria230, mediante la cual se combina un lenguaje de programación como R con documentación de todo tipo (por ejemplo comentarios en el\ncódigo fuente o inclusión de ficheros readme).Con el enfoque de la investigación reproducible, lo que se hace es darle la vuelta\nal enfoque corta-pega, de forma que se escribe el informe la vez que se realiza\nel análisis, incrustando el código dentro del propio informe. Obviamente, es necesario\nun sistema que consolide el informe con los resultados del código, y esto es lo\nque nos permite R y RStudio mediante archivos R Markdown y su evolución reciente Quarto. El desarrollo de Quarto está patrocinado por la empresa Posit, PBC, donde anteriormente crearon R Markdown que compartía los mismos objetivos, pero estaba dirigido principalmente usuarios del lenguaje R. El mismo equipo central trabaja tanto en R Markdown como en Quarto231.Las ventajas de utilizar un enfoque reproducible se pueden resumir en:Si el mismo analista tiene que volver al análisis en el futuro, los\nresultados se pueden volver obtener automáticamente de nuevo fácil y comprensiblemente.Si el mismo analista tiene que volver al análisis en el futuro, los\nresultados se pueden volver obtener automáticamente de nuevo fácil y comprensiblemente.En el caso de que en el proyecto participen más analistas, toda la explicación\nestá mano.En el caso de que en el proyecto participen más analistas, toda la explicación\nestá mano.Cualquier cambio en un punto del análisis (por ejemplo, añadir una\nvariable un modelo) se puede realizar de una sola vez y los cambios\nen los resultados y gráficos se actualizarán automáticamente.Cualquier cambio en un punto del análisis (por ejemplo, añadir una\nvariable un modelo) se puede realizar de una sola vez y los cambios\nen los resultados y gráficos se actualizarán automáticamente.Los resultados se pueden verificar por terceros en caso necesario. Un\ncaso paradigmático fue el escándalo de los ensayos de cáncer en Duke en 2011232.\nobstante es un tema que cada vez se demanda más en otros campos\nfuera de la investigación clínica (por ejemplo en publicaciones de\ncualquier tipo).Los resultados se pueden verificar por terceros en caso necesario. Un\ncaso paradigmático fue el escándalo de los ensayos de cáncer en Duke en 2011232.\nobstante es un tema que cada vez se demanda más en otros campos\nfuera de la investigación clínica (por ejemplo en publicaciones de\ncualquier tipo).El flujo de trabajo sería el siguiente: los contenidos se encuentran en ficheros\nde texto plano, con código y texto explicativo. Estos ficheros fuente, se compilan y producen los materiales en los\nformatos necesarios. Los cambios se hacen una vez, y todos los materiales son\nactualizados adecuadamente.continuación se abordará el enfoque reproducible. Para el otro enfoque,\nsimplemente basta copiar los resultados de la consola y los gráficos de la pestaña Plots\ndel panel inferior derecho en cualquier editor de documentos.","code":""},{"path":"id_120007-informes.html","id":"markdown-r-markdown-quarto-y-rstudio","chapter":"Capítulo 44 Informes reproducibles con R Markdown y Quarto","heading":"44.1.1 Markdown, R Markdown, Quarto y RStudio","text":"Markdown es un lenguaje de marcas ligero que fue creado con la intención de ser más legible y fácil de escribir que el código HTML, aunque actualmente se utiliza para otros formatos de salida.\nAl ser ficheros de texto plano los ficheros se pueden leer bajo\ncualquier circunstancia, con una sintaxis muy sencilla que permite leerlo\ndirectamente por las personas, o ser convertido por un ordenador\nen otro formato más elaborado, como por ejemplo HTML\n(página web), pdf o Microsoft Word. En RStudio, se pueden crear ficheros R Markdown233\nutilizando esta sintaxis para\nlas explicaciones del análisis, e incluir dentro “trozos” (chunks) de\ncódigo, de forma que, al generar el informe, el resultado de ese código queda\nincluido en el documento de salida. Así, si una vez terminado el informe se ha olvidado,\npor ejemplo,\nincluir un gráfico, sólo hay que añadir las líneas de código que lo crean y\nvolver generar el informe.R Markdown ha evolucionado un nuevo formato denominado Quarto234, que extiende aún más la funcionalidad de Markdown y está pensado para ser usado con otros lenguajes de programación. En esencia, y los efectos de este capítulo, hay pocas diferencias.Para poder utilizar las capacidades de R Markdown y Quarto, es necesario tener instalado\nel paquete knitr (Xie 2017), que utiliza también otros paquetes como rmarkdown.\nAunque knitr forma parte del tidyverse, sí es un enfoque moderno de R\nque vino hacer más fácil la generación de documentos que se hacía en R base\ncon la función Sweave (Leisch 2002). Para usar Quarto se necesita también el paquete quarto (Allaire 2022) y tener instalado el software quarto en el ordenador235.Para crear un nuevo documento Quarto, se selecciona Quarto Document… en el\nicono de nuevo archivo de la barra de herramientas o en el menú File. Entonces\nse abre el cuadro de diálogo New Quarto Document. Hay varios\ntipos de archivos que se pueden crear, que producirán formatos diferentes: Document (documento),\nPresentation (presentación de diapositivas) e Interactive (aplicación web interactiva con Shiny u Observable JS). De momento se verán los documentos. Se puede crear un archivo Quarto vacío si se quiere crear la estructura. Para que se cree con una estructura mínima,\nse necesita un título del documento y un autor, que después se podrán cambiar. También se selecciona un formato de salida por defecto, que puede ser HTML (para ver en el navegador), PDF, o Word. Esto también se podrá cambiar después, por lo que la forma más eficiente de trabajar es empezar con HTML, cuya previsualización es más rápida, y cuando esté el resultado final generar el archivo en el formato deseado.\nSe puede seleccionar la Engine entre knitr (R) y Jupyter (Python), y también elegir si se quiere utilizar el editor visual (por defecto). Con el editor visual se pueden utilizar menús para editar el texto y dar el formato Markdown sin esfuerzo. Al hacer clic en el botón Create de la ventana New Quarto Document, se abre en\nel editor de RStudio un documento quarto (extensión .qmd) con una estructura básica modo de plantilla. Los elementos principales de un archivo Quarto aparecen en esta plantilla:Encabezado YAML: constituyen la configuración del documento, y controlan sobre todo las opciones del formato de salida, es decir, cómo se verá el resultado final. Este encabezado se encuentra entre dos líneas con tres guiones (---),\ndonde se expresan las opciones como opcion: valor, y estos valores además se pueden anidar. Dispone de ayuda contextual, de forma que pulsando la combinación de teclas CTRL+ESPACIO aparecen las opciones que se pueden configurar y los posibles valores. Esta parte del documento es constituye el formato del documento.Encabezado YAML: constituyen la configuración del documento, y controlan sobre todo las opciones del formato de salida, es decir, cómo se verá el resultado final. Este encabezado se encuentra entre dos líneas con tres guiones (---),\ndonde se expresan las opciones como opcion: valor, y estos valores además se pueden anidar. Dispone de ayuda contextual, de forma que pulsando la combinación de teclas CTRL+ESPACIO aparecen las opciones que se pueden configurar y los posibles valores. Esta parte del documento es constituye el formato del documento.Texto formateado: con una sintaxis muy sencilla, se puede dar formato al texto, como negritas, listas, etc. En el editor visual se puede hacer con los menús y botones de la barra de herramientas del editor.Texto formateado: con una sintaxis muy sencilla, se puede dar formato al texto, como negritas, listas, etc. En el editor visual se puede hacer con los menús y botones de la barra de herramientas del editor.Fragmentos de código (chunks): al generar el documento, se ejecutará el código dentro de estos\nfragmentos, y en el documento resultante se mostrará el resultado. En cada fragmento de código aparecen dos botones que sirven para ejecutar todos los chunks anteriores y para ejecutar el chunk actual. Junto con el texto formateado constituyen el contenido del documento.La barra de herramientas del editor ofrece algunas opciones:El botón Render convierte (“renderiza” en lenguaje informático) el documento Quarto produciendo el archivo de salida configurado. Se puede desplegar un menú para cambiar el formato de salida y otras opciones. Al crear el documento solo aparece el formato de salida elegido, pero se puede cambiar el encabezado para poder convertir el documento distintos formatos. Por ejemplo si se cambia el encabezado que se ha creado por defecto por el siguiente, se puede generar el archivo de salida en html o word seleccionando en la lista desplegable junto al botón Render:\n````\n---\ntitle: \"Título del informe\"\nformat: \nhtml: default\ndocx: default\neditor: visual\n---\n````El botón Render convierte (“renderiza” en lenguaje informático) el documento Quarto produciendo el archivo de salida configurado. Se puede desplegar un menú para cambiar el formato de salida y otras opciones. Al crear el documento solo aparece el formato de salida elegido, pero se puede cambiar el encabezado para poder convertir el documento distintos formatos. Por ejemplo si se cambia el encabezado que se ha creado por defecto por el siguiente, se puede generar el archivo de salida en html o word seleccionando en la lista desplegable junto al botón Render:El botón de opciones permite cambiar la forma en que se mostrarán las salidas al ejecutar el código desde el editor.El botón de opciones permite cambiar la forma en que se mostrarán las salidas al ejecutar el código desde el editor.El botón Insert new code chunk nos permite insertar un nuevo fragmento de código.El botón Insert new code chunk nos permite insertar un nuevo fragmento de código.Las flechas de navegación permiten moverse entre los chunks del\ndocumento. También\nse puede usar el selector de esquema en la parte inferior para ir un fragmento de código o\napartado concreto del documento.Las flechas de navegación permiten moverse entre los chunks del\ndocumento. También\nse puede usar el selector de esquema en la parte inferior para ir un fragmento de código o\napartado concreto del documento.Desde el menú Run se puede ejecutar el código de los distintos chunks.Desde el menú Run se puede ejecutar el código de los distintos chunks.El menú Publish nos permitiría publicar el documento en algún servicio como RPubsEl menú Publish nos permitiría publicar el documento en algún servicio como RPubsEl botón Outline muestra un esquema para navegar por el documento, donde aparecerán los encabezados formateados con Markdown.El botón Outline muestra un esquema para navegar por el documento, donde aparecerán los encabezados formateados con Markdown.Se puede cambiar entre el editor visual y el del código fuente con los botones Source y Visual en la parte superior del editor.Se puede cambiar entre el editor visual y el del código fuente con los botones Source y Visual en la parte superior del editor.Para generar el documento, se guarda el archivo en cualquier carpeta de nuestro\nproyecto y se utiliza el icono de conversión (“renderizado”). La Fig. 44.1 muestra en el panel izquierdo el archivo fuente en el editor visual, con alguna opción adicional añadida la plantilla por defecto, y en el panel derecho el informe renderizado. Si en vez de pulsarlo directamente se selecciona el\ntriángulo de la derecha, se puede seleccionar el formato de salida (html, pdf o Word)\nsi se han incluido esos formatos en el encabezado YAML como se ha indicado.\nEl formato PDF requiere tener instalada una\ndistribución del sistema de edición libre LaTeX236.\nEl archivo de destino, con extensión .html, .pdf o .docx según el caso,\nquedará guardado en la carpeta donde se encuentre el archivo quarto.\nDependiendo de las opciones configuradas, el archivo se abrirá automáticamente\nen una ventana nueva de RStudio (por defecto),\no en la pestaña Viewer del panel inferior derecho, o en el visor de pdf\nintegrado en RStudio (pdf). Para poder abrir archivos .docx será necesario tener\ninstalado Microsoft Word o algún otro programa que pueda abrirlo, como LibreOffice.\nFigura 44.1: Informe Quarto y el documento de salida que produce su conversión (“renderizado”) con algunas opciones adicionales\nSe puede compilar el informe tantas veces como se quiera con el icono Render.\nPara trabajo en curso, se recomienda ir previsualizando en formato HTML, y una vez\nsea definitivo generar el formato de destino final. Para la conversión de\nformatos, RStudio integra la aplicación\npandoc.Hay una guía rápida de\nMarkdown (Markdown Quick Reference) disponible en el menú de ayuda de RStudio, así como enlaces dos\nCheatsheets: R Markdown Cheatsheet y R Markdown Reference Guide. Esta última es\nla más completa y donde se encuentran todas las opciones disponibles (que sirven\npara los documentos Quarto aunque en la propia web de Quarto hay una documentación más completa). En los siguientes\napartados se revisan las opciones más habituales que cubren un amplio\nabanico de proyectos.","code":"````\n---\ntitle: \"Título del informe\"\nformat: \nhtml: default\ndocx: default\neditor: visual\n---\n````"},{"path":"id_120007-informes.html","id":"documentos-quarto","chapter":"Capítulo 44 Informes reproducibles con R Markdown y Quarto","heading":"44.2 Documentos Quarto","text":"En esta sección se detalla cómo añadir contenido y configuración un documento Quarto con algunas de las opciones más interesantes para la Ciencia de Datos reproducible.","code":""},{"path":"id_120007-informes.html","id":"encabezado-yaml-y-configuración","chapter":"Capítulo 44 Informes reproducibles con R Markdown y Quarto","heading":"44.2.1 Encabezado YAML y configuración","text":"Las opciones de configuración del documento se establecen en este encabezamiento.\nAl crear el documento con la plantilla, se crea el siguiente\nencabezado:Ya se ha visto cómo se pueden añadir más formatos, poniendo uno en cada línea e “indentando” con el tabulador las distintas opciones. Cada formato su vez puede incluir opciones, que de nuevo se indican con nuevas líneas que se “indentan” debajo del formato.La “indentación” se refiere al número de espacios en blanco (o tabulaciones)\nal principio de cada línea. Las opciones que estén “dentro” de otra, deben tener la misma “indentación” (el mismo número de espacios en blanco al principio de la línea). Véase un ejemplo más completo debajo.\nEn el encabezado YAML la incorrecta “indentación” puede provocar errores al generar el informe.Además del título (opción title), se pueden incluir autor (opción author) y fecha (opción date).\nEstos elementos son cadenas de texto que aparecerán al principio del documento\nde salida. Se debe cuidar que estén entre comillas para evitar\nposibles errores. El elemento format indica el formato de salida.La cantidad de opciones que se pueden incluir en el encabezado YAML es enorme y tiene cabida en este capítulo. Algunas de las más utilizadas son lang para indicar el idioma del documento (“es” para español),\nbibliography para indicar un fichero bibtex de bibliografía, o toc para incluir una tabla de contenidos. Algunas son específicas del formato. Por ejemplo, una muy útil es reference-doc para documentos de Word, con la que se puede indicar una plantilla personalizada para usar colores corporativos u otras opciones de diseño del informe237. Para documentos html se puede incluir una hoja de estilos con la opción css. La lista completa para cada uno de los formatos soportados por Quarto se puede consultar en la guía de referencia en https://quarto.org/docs/reference/.El siguiente encabezado YAML fijaría el ancho y el alto de las figuras (en pulgadas) para el formato de salida html, además de las otras opciones comentadas:Una explicación detallada del uso de hojas de estilo CSS queda fuera del alcance de este libro. Un ejemplo sencillo para formatear bloques con identificador (nombres precedidos por #) y bloques con clase (nombres precedidos por .) sería el siguiente:","code":"---\ntitle: \"Título del informe\"\nformat: html\neditor: visual\n------\ntitle: \"Título del informe\"\nformat: \n  html:\n    fig-width: 8\n    fig-height: 6\n    css: estilos.css\n  docx: \n    toc: true\n    reference-doc: plantilla.docx\n  pdf: default\nlang: \"es\"\nbibliography: bibliografia.bib\n---#parrafoazul {\n   color: blue;\n}\n\n.enfatizar {\n   font-size: 1.2em;\n}"},{"path":"id_120007-informes.html","id":"formateado-de-texto","chapter":"Capítulo 44 Informes reproducibles con R Markdown y Quarto","heading":"44.2.2 Formateado de texto","text":"Incluir títulos, énfasis en el texto y listas\nes muy sencillo, y menudo se necesita mucho más para realizar un informe.\nEn el informe que se crea con la plantilla ya se ven algunas opciones:Los encabezados se crean poniendo al principio de la línea tantos símbolos\nalmohadilla (##) como nivel de título se desee (dos almohadillas, apartado, tres\nalmohadillas, subapartado, etc.) Una almohadilla sería para el título del\ninforme, si se especificara en el encabezado.Los encabezados se crean poniendo al principio de la línea tantos símbolos\nalmohadilla (##) como nivel de título se desee (dos almohadillas, apartado, tres\nalmohadillas, subapartado, etc.) Una almohadilla sería para el título del\ninforme, si se especificara en el encabezado.Para poner texto en negrita, se incluye entre dos asteriscos cada lado: **negrita**.Para poner texto en negrita, se incluye entre dos asteriscos cada lado: **negrita**.Para poner texto en formato monoespaciado, tipo código, se pone entre tildes\ngraves (backticks, ` ): `monoespaciado`.Para poner texto en formato monoespaciado, tipo código, se pone entre tildes\ngraves (backticks, ` ): `monoespaciado`.Los enlaces se crean con: [texto del enlace](http://ejemplo.com).Los enlaces se crean con: [texto del enlace](http://ejemplo.com).Existen otras opciones sencillas, que se pueden ver en la Markdown Quick Reference\ndel menú Help:Cursiva rodeando el texto con un solo asterisco cada lado (o guión bajo): _cursiva_.Cursiva rodeando el texto con un solo asterisco cada lado (o guión bajo): _cursiva_.Listas poniendo al principio de la línea un asterisco, guión o signo más:Listas poniendo al principio de la línea un asterisco, guión o signo más:Listas ordenadas poniendo un número y un punto al principio de la línea:Imágenes de cualquier tipo como: ![](ruta_a_la_imagen).Imágenes de cualquier tipo como: ![](ruta_a_la_imagen).Superíndicessup y subíndicessub con el texto entre símbolos ^ y ~ respectivamente.Superíndicessup y subíndicessub con el texto entre símbolos ^ y ~ respectivamente.Ecuaciones en formato LaTeX, por ejemplo \\(\\sum x_i\\) sería $\\sum x_i$.Ecuaciones en formato LaTeX, por ejemplo \\(\\sum x_i\\) sería $\\sum x_i$.Saltos de línea (añadiendo más de dos espacios al final de la línea) y saltos de página (tres o más asteriscos, *, o guiones medios, -, en una línea).Saltos de línea (añadiendo más de dos espacios al final de la línea) y saltos de página (tres o más asteriscos, *, o guiones medios, -, en una línea).Tablas, usando guiones medios y barras verticales para separar filas y columnas:Tablas, usando guiones medios y barras verticales para separar filas y columnas:Con estas opciones se cubren las necesidades de la práctica totalidad de\nproyectos de análisis. obstante, dependiendo del formato de salida se\npueden añadir otras opciones de formato.","code":"    * Primer elemento de la lista\n    * Segundo elemento de la lista\n      + Primer elemento dentro del segundo\n      + Segundo elemento dentro del segundo\n    * ...    1. Primer elemento\n    2. segundo elemento\n    3. ...    Primer encabezado      | Segundo encabezado\n    ---------------------- | ---------------------\n    Contenido de la celda  | Contenido de la celda\n    Contenido de la celda  | Contenido de la celda"},{"path":"id_120007-informes.html","id":"inclusión-de-código-en-el-documento","chapter":"Capítulo 44 Informes reproducibles con R Markdown y Quarto","heading":"44.2.3 Inclusión de código en el documento","text":"Se pueden crear archivos Quarto sin incluir nada de código, simplemente\npara crear documentos editables fácilmente. Sin embargo, la verdadera potencia\nde Quarto es la posibilidad de incluir código de R (y también de otros\nlenguajes) en los documentos. Como ya se avazó, el código se incluye, principalmente,\nen forma de chunks o bloques de código.Un chunk consta de unos marcadores\nde inicio y final del chunk, entre los cuales se insertan expresiones de R que se\nejecutarán al generar el documento de salida. El marcador de inicio son tres\nsímbolos de tilde grave seguidos de unas llaves con la letra r\ndentro. El marcador de cierre del chunk son de nuevo\ntres tildes graves, sin más. Y dentro del chunk se pueden poner expresiones de R de la misma forma en que se trabaja con los scripts. Al convertir (“renderizar”) el documento, el código se ejecutará con las opciones que se indiquen como se explica más adelante, y el archivo de salida incluirá el resultado de la ejecución del chunk. El siguiente sería un ejemplo de código para incluir gráficos en el informe.En todo caso, hay que escribir los marcadores de inicio y final, ya que\nse dispone del atajo de teclado CTRL+ALT+que lo hace automáticamente, o,\nalternativamente, el icono Insert new code chunk de la barra de herramientas del\neditor.\nUna vez se tiene el cursor dentro de un chunk, se puede ejecutar una expresión\ncomo del mismo modo que se hace en un script (CTRL+ENTER), o el chunk completo\n(MAYUS+CTRL+ENTER).veces es necesario incluir algún resultado de R en medio del texto\ny como un bloque. En esos casos se puede insertar un bloque en línea\nponiendo, entre dos tildes graves, la letra r como primer carácter,\ny después una expresión de R que se pueda “imprimir” como cadena de texto:Una opción muy interesante de los informes de Quarto es la parametrización.\nEsta opción es muy útil para informes automatizados que pueden\ncambiar dependiendo de algún valor, por ejemplo del fichero de datos, la fecha,\no cualquier otro valor. Estos parámetros se crean como elementos del encabezado\nYAML de la forma:que después se pueden usar en los chunks de código como params$parametro.\nLa verdadera potencia de esta característica es cuando se convierte el documento\ndesde un script en el que los parámetros son resultados de algún tipo de\noperación en los datos (por ejemplo, un informe de análisis de inventario solo de una\ntienda donde se han producido roturas de stock el día x).\nEn vez de utilizar el botón Render, en estos casos se usa la función quarto_render(),\nuna de cuyas opciones es execute_params, donde se pasarían los valores de los parámetros en forma\nde lista cuyos elementos tienen el nombre de los parámetros.","code":"```\nlibrary(CDR)\nlibrary(corrplot)\nmcor_tic <- cor(TIC2021)  \ncorrplot.mixed(mcor_tic, order = 'AOE')\n````r expresion_de_R `    params:\n  parametro: valor"},{"path":"id_120007-informes.html","id":"opciones-de-los-bloques-de-código-chunks","chapter":"Capítulo 44 Informes reproducibles con R Markdown y Quarto","heading":"44.2.4 Opciones de los bloques de código (chunks)","text":"Al renderizar un informe que contiene chunks sin configurar ninguna opción,\nel informe mostrará por defecto el código de entrada y las salidas\n(textos y gráficos), así como todos los mensajes que se pueden producir.Opcionalmente, justo después del marcador de inicio del chunk se pueden añadir opciones del mismo mediante líneas que comienzan por el llamado hashpipe, que es una almohadilla seguida de la barra vertical, #| y continuación la opción y su valor, de la misma forma que se hacía en el encabezado YAML para las opciones del documento, es decir, opcion: valor.El chunk que se muestra continuación tiene como identificador “ejemplo”,\ny como opciones echo: false y fig.align: 'center', lo que indica que el código\nse mostrará en el informe final y que el gráfico producido se alineará en el centro\ndel texto.Las opciones de chunk se pueden incluir de forma global en el documento estableciéndolas en el encabezado YAML del mismo, por ejemplo para mantener las opciones anteriores en todos los chunks por defecto:Es importante señalar que las opciones establecidas en los chunks tienen prioridad las opciones establecidas en el documento.Hay varias opciones de chunk que tienen que ver con la presentación en la salida. Por defecto, si se produce un error, el proceso se detiene y se genera\nel archivo de destino. Este comportamiento, y otras muchas opciones,\nse pueden configurar como opciones del chunk. Las más habituales son: error: true para mostrar los errores y detener la generación del documento; warning: false y message: false para mostrar warnings ni mensajes respectivamente; include: false para ejecutar el código pero mostrar ningún tipo de salida; eval: false para ejecutar el codigo; results: \"hide\" para indicar que se muestren los resultados (otras opciones son asis, hold o markup); comment: simbolo para cambiar el símbolo que se usará como comentario del output (veces es conveniente simplemente poner comentario, es decir, \"\"). Estas opciones del chunk se pueden incluir nivel global en el encabezado YAML como se ha indicado anteriormente. La lista completa de opciones se encuentra en la R Markdown reference guide\nque está disponible en el menú Help/Cheatsheets, o la documentación de Quarto sobre opciones de ejecución238.Una característica muy cómoda es usar la ayuda contextual al escribir las opciones del chunk. Al comenzar escribir, o pulsando la combinación de teclas CTRL+ESPACIO, se\nmuestran las opciones disponibles, y al seleccionar una opción, si tiene\nvarios posibles valores aparecen también para seleccionar.","code":"```\n#| label: \"Ejemplo\"\n#| echo: false\n#| fig-align: 'center'\nplot(cars)\n```---\ntitle: \"Mi documento\"\nformat: html\nknitr:\n  opts_chunk: \n    echo: false\n    fig-align: 'center'\n---"},{"path":"id_120007-informes.html","id":"referencias-cruzadas-y-formateo-de-tablas","chapter":"Capítulo 44 Informes reproducibles con R Markdown y Quarto","heading":"44.2.5 Referencias cruzadas y formateo de tablas","text":"La salida tabular por defecto de la consola normalmente es adecuada para un informe. En su lugar, lo que se desea es tener una tabla formateada adecuadamente para el formato de salida. Se pueden incluir en los informes de Quarto tablas formateadas de calidad. Para ello, se debe utilizar alguna función que formatee la tabla de acuerdo al formato de salida (HTML, PDF, Word) y, veces, configurar la opción results del chunk como 'asis'. Muchas de estas funciones preparan automáticamente el formato según el fichero de salida que se está generando.\nPor ejemplo, el siguiente chunk generaría una tabla en cualquiera de los formatos de salida usando la función kable del paquete knitr:Hay otros paquetes con multitud de opciones de formato y presentación para las tablas como xtable, flextable, kableExtra, DT, o gt. Se anima al lector consultar la documentación de estos paquetes para aprender crear tablas de calidad que comuniquen adecuadamente los resultados de los análisis.Tanto las tablas como las figuras se deben referenciar adecuadamente en los informes. Para ello se utilizan las referencias cruzadas de los informes Quarto. Para poder referenciar un gráfico creado en un chunk, es necesario: ) que el chunk tenga una etiqueta (label: 'etiqueta'); ii) que el chunk tenga una opción fig-cap para el título de la figura. Entonces el gráfico se puede referenciar en cualquier lugar del documento Quarto simplemente escribiendo @etiqueta. Por ejemplo, el siguiente chunk crearía el gráfico de la figura 44.2, y en el texto se referenciaría como “Figura @fig-tic”.\nFigura 44.2: Ventas vs. % Empresas con banda ancha\nEn cuanto las tablas, igualmente el chunk que las crea debe tener una etiqueta. El título de la tabla en este caso lo proporcionará la propia función que la crea. modo de ejemplo, el siguiente chunk crearía la Tabla 44.1 ya formateada con la función flextable() del paquete homónimo (Gohel Skintzos 2022), y en el texto se referenciaría como “Tabla @tab-tic”.\nTabla 44.1: Contaminación media NOx según tipo de estación.\ntipoMedianDesv.TipPerdidosFondo64.1110849,65661.4860370Suburbanas32.8757412,41432.1342639Tráfico81.2739233,10468.1397483","code":" ```{r}\n knitr::kable(TIC2021)\n ```\n ```{r}\n#| label: \"fig-tic\"\n#| fig-cap: \"Ventas vs. % empresas con banda ancha\"\nlibrary('ggplot2')\nlibrary('CDR')\nTIC2021 |> \n  ggplot(aes(ebroad, esales)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n``````{r}\n#| label: \"tab-tic\"\n#| fig-cap: \"Contaminación media NOx según tipo de estación\"\nlibrary(dplyr)\nlibrary(flextable)\ncontam_mad |>\n  filter(nom_abv == \"NOx\") |>\n  group_by(tipo) |> \n  summarise(Media = mean(daily_mean, na.rm = TRUE), n = n(), \n            Desv.Tip = sd(daily_mean, na.rm = TRUE),\n            Perdidos = sum(is.na(daily_mean))) |> \n  flextable() |> \n  set_caption(\"Contaminación media NOx según tipo de estación.\") |> \n  autofit()\n```"},{"path":"id_120007-informes.html","id":"otros-formatos","chapter":"Capítulo 44 Informes reproducibles con R Markdown y Quarto","heading":"44.3 Otros formatos","text":"El formato Quarto es solo uno de los que se pueden utilizar\npara aplicar la reproducibilidad que motiva este capítulo. Como se ha comentado, es la nueva generación de R Markdown, archivos que pueden seguir creándose con RStudio. En R base\nse pueden crear archivos Sweave, con sintaxis LaTeX para la\nnarrativa y bloques de código R. También es posible crear archivos R HTML,\ncon la narrativa en HTML. Los identificadores de los bloques son ligeramente\ndistintos, así como las opciones disponibles, aunque la filosofía es la misma.\nR Markdown y ahora Quarto han ido desplazando estos otros formatos al ser más versátil (puede\ngenerar cualquiera de los otros formatos, y además otros como .docx).En cuanto los formatos de salida, hay una cantidad de opciones muy interesante\nque queda fuera de los objetivos de este libro. continuación se relacionan\nalgunos de ellos, si bien se puede consultar el libro de Xie, Allaire, Grolemund (2019) y la documentación de Quarto para\nver detalles.Notebooks: es un tipo especial de salida HTML, más indicado cuando\nse quieren ir probando cosas guardando el resultado parcial de lo ejecutado en el html.Notebooks: es un tipo especial de salida HTML, más indicado cuando\nse quieren ir probando cosas guardando el resultado parcial de lo ejecutado en el html.Presentaciones: es posible crear presentaciones PowerPoint y usar una plantilla,\ncomo se vio con los documentos de Word. Se utiliza una sintaxis muy sencilla\ny se puede incluir código y resultados igual que en un informe. Otros formatos de\npresentaciones son Reveal JS y beamer (LaTeX) y con Quarto, además, ioslides, slidify y xaringan (HTML).Presentaciones: es posible crear presentaciones PowerPoint y usar una plantilla,\ncomo se vio con los documentos de Word. Se utiliza una sintaxis muy sencilla\ny se puede incluir código y resultados igual que en un informe. Otros formatos de\npresentaciones son Reveal JS y beamer (LaTeX) y con Quarto, además, ioslides, slidify y xaringan (HTML).Tableros (dashboards): pueden ser estáticos, usando el paquete flexdashboard,\nútiles para comunicar resultados en un par de pantallazos. Tableros (dashboards): pueden ser estáticos, usando el paquete flexdashboard,\nútiles para comunicar resultados en un par de pantallazos. Shiny: aplicaciones web interactivas que responden inputs del usuario (reactive). Estas aplicaciones se tratan en detalla en el Cap. 45. Shiny: aplicaciones web interactivas que responden inputs del usuario (reactive). Estas aplicaciones se tratan en detalla en el Cap. 45. Websites: websites sencillos con páginas enlazadas en el mismo directorio.Websites: websites sencillos con páginas enlazadas en el mismo directorio.Blog: directamente como proyecto Quarto, o con el paquete blogdown, se pueden generar sitios web completos al estilo de un blog con páginas.Blog: directamente como proyecto Quarto, o con el paquete blogdown, se pueden generar sitios web completos al estilo de un blog con páginas.Libros: directamente como proyecto Quarto, o con el paquete bookdown, se pueden crear libros en varios formatos. El material de este libro está creado con bookdown.Libros: directamente como proyecto Quarto, o con el paquete bookdown, se pueden crear libros en varios formatos. El material de este libro está creado con bookdown.Tutoriales: aparecieron en RStudio 1.3; se pueden crear documentos interactivos con preguntas y navegación usando sintaxis R Markdown.Tutoriales: aparecieron en RStudio 1.3; se pueden crear documentos interactivos con preguntas y navegación usando sintaxis R Markdown.Tufte Handouts: un tipo especial de documento con anotaciones al margen\nque comunica muy bien.Tufte Handouts: un tipo especial de documento con anotaciones al margen\nque comunica muy bien.","code":""},{"path":"id_120007-informes.html","id":"resumen-34","chapter":"Capítulo 44 Informes reproducibles con R Markdown y Quarto","heading":"Resumen","text":"En la comunicación de resultados, es esencial seguir un enfoque reproducible.R Markdown y su evolución Quarto es el formato más versátil para crear informes reproducibles que\npermitan una trazabilidad de los análisis.RStudio permite trabajar eficientemente con R Markdown y Quarto través de ayuda y opciones.El encabezado YAML del informe contiene la configuración global, que puede incluir parámetros para automatización.La narrativa del informe se escribe en Markdown, con una sintaxis extremadamente sencilla.El código se puede incluir en forma de bloques (chunks) o en línea.En las opciones del chunk se puede personalizar\nla forma en que se ejecutará y mostrará en el informe.Los informes R Markdown y Quarto se pueden generar en formatos HTML, PDF y Word, entre otros.En los informes Quarto se pueden hacer referencias cruzadas tablas, figuras y otros elementos del documento.Hay otros formatos más elaborados que merece la pena explorar.","code":""},{"path":"shiny.html","id":"shiny","chapter":"Capítulo 45 Creación de aplicaciones web interactivas con Shiny","heading":"Capítulo 45 Creación de aplicaciones web interactivas con Shiny","text":"Aurora González Vidal","code":""},{"path":"shiny.html","id":"introducción-22","chapter":"Capítulo 45 Creación de aplicaciones web interactivas con Shiny","heading":"45.1 Introducción","text":"Shiny es un paquete de R que permite crear aplicaciones web interactivas que cuentan con todos los elementos de R. Shiny se ha convertido en un referente ya que, para aquellos que tienen conocimiento de R, es muy sencillo crear una aplicación en cuestión de horas (Winston et al. 2020). Para crear una aplicación mínima, se necesitan conocimientos de HTML (HyperText Markup Language), CSS (Cascading Style Sheets) o JavaScript y sus dependencias. Además, es necesario pensar en elementos técnicos para hacerla accesible en la web como, por ejemplo, el puerto, ya que Shiny se encarga de esos detalles si se cambian las opciones por defecto. Ésas son algunas de las razones principales por las cuales Shiny se ha vuelto tan popular lo largo de los años, ya que se pueden crear pruebas de concepto de un producto, mostrar algoritmos o presentar resultados de investigación con claridad través de interfaces de usuario accesibles, reproducibles y amigables (Fay et al. 2021).El primer paso para usar Shiny consiste en instalar el paquete que está disponible en CRAN:Para asegurarse de que la versión instalada es igual o superior la 1…5.0. hay que usar packageVersion(\"shiny\").\ncontinuación, se puede cargar el paquete y ver algunos ejemplos que se incluyen directamente en el mismo utilizando distintas opciones para el argumento example.","code":"\ninstall.packages(\"shiny\")\nlibrary(\"shiny\")\nrunExample(example = \"01_hello\")\n# otras: 02_text, 03_reactivity, 04_mpg, 05_sliders, 06_tabsets, 07_widgets, 08_html, 09_upload, 10_download,  11_timer."},{"path":"shiny.html","id":"componentes-mínimos-de-una-aplicación-shiny-y-disposición-básica","chapter":"Capítulo 45 Creación de aplicaciones web interactivas con Shiny","heading":"45.2 Componentes mínimos de una aplicación Shiny y disposición básica","text":"Las aplicaciones Shiny tienen dos componentes:Una interfaz de usuario ui, que es un script yUn server que es un script de servidor o secuencia de comandos de servidor.Estos componentes pueden encontrarse en el mismo script o estar separadas en dos scripts con nombres fijos: ui.R y server.R. En este caso, se ha elegeido la segunda opción para ilustrar los ejemplos con mayor claridad.\nUna aplicacion Shiny es un directorio que contiene estos scripts y otros ficheros adicionales (conjuntos de datos, fichero donde se definen funciones dinámicas, etc.).El código mínimo para crear una aplicación con un título, panel lateral y panel principal es el que sigue:ui.Rserver.RLa aplicación se puede lanzar de dos maneras diferentes. La primera mediante el comando runApp(), que tiene como argumento la ruta del directorio que almacena los ficheros que componen la aplicación.La segunda es lanzar la aplicación directamente desde Rstudio mediante el botón RunApp que aparece en cualquiera de los dos scripts ui.R, server.R reemplazando al Run habitual.En este capítulo, además de ver los distintos componentes de Shiny, se construye una aplicación para la visualización de algunos gráficos presentados en el Cap. 52. Los datos relacionados se encuentran en el paquete CDR del libro.Además del ui.R y el sever.R, puede ser muy útil tener un fichero donde recoger las funciones, paquetes y datos necesarios para el funcionamiento de la aplicación. Este fichero se puede cargar mediante la función source desde el ui.R o desde el server.R. Una recomendación es denominarlo source.R para mejor organización y legibilidad.","code":"\nshinyUI(fluidPage(\n  titlePanel(\"TITULO\"), # panel de encabezado TÍTULO\n  sidebarPanel(), # panel lateral\n  mainPanel() # panel principal\n))\nshinyServer(function(input, output) {})\nlibrary(shiny)\nrunApp(\"ruta al directorio\")"},{"path":"shiny.html","id":"diseño-de-una-aplicación-shiny","chapter":"Capítulo 45 Creación de aplicaciones web interactivas con Shiny","heading":"45.3 Diseño de una aplicación Shiny","text":"Shiny incluye una serie de opciones para el diseño o la disposición de los distintos componentes de una aplicación. En esta sección se ven dos muy componentes sencillos:sidebarLayout(): para colocar un sidebarPanel(), es decir, un panel lateral de entradas junto un mainPanel() de contenido de salida.tabsetPanel() y navlistPanel() para la segmentación de diseños.Hasta ahora se ha utilizado el primero, sin introducirlo específicamente, para mostrar distintos ejemplos, por ser el más sencillo.","code":""},{"path":"shiny.html","id":"dispag","chapter":"Capítulo 45 Creación de aplicaciones web interactivas con Shiny","heading":"45.3.1 Diseño de las páginas: fluidPage()","text":"Un diseño de página fluido fluidPage() consiste en filas que su vez incluyen columnas. Las filas tienen como propósito asegurar que sus elementos aparezcan en la misma línea (si el navegador tiene el ancho adecuado). El objetivo de las columnas es definir cuánto espacio horizontal, dentro de una cuadrícula de 12 unidades de ancho, deben ocupar sus elementos. Las páginas fluidas escalan sus componentes en tiempo real para llenar todo el ancho disponible del navegador.Una fluidPage() tiene 2 argumentos: headerPanel() con el título de la aplicación,\ny sidebarLayout(), que es un punto de partida útil para la mayoría de las aplicaciones. Éste su vez tiene 2 argumentos más: sidebarPanel(), que es una barra lateral para las entradas y mainPanel(), un gran área principal para la salida.\nFigura 45.1: Aplicación shiny con sidebarPanel posicionado por defecto la izquierda\nLa barra lateral puede posicionarse la izquierda (por defecto) o la derecha del área principal. Por ejemplo, para posicionar la barra lateral la derecha se debe utilizar position = 'right' como se aprecia en la Fig. \\(\\ref{fig:dcha}\\), donde el resto de argumentos son igual que para generar la Fig.\\(\\ref{fig:izda}\\) .\nFigura 45.2: Aplicación shiny con sidebarPanel la derecha\nLas funciones radioButtons() y plotOutput() se introducirán en detalle en las respectivas secciones de este capítulo.","code":"\nshinyUI(fluidPage(\n  headerPanel(\"Evolución del paro\"), # panel de encabezado\n  sidebarLayout(\n    sidebarPanel( # panel lateral\n      radioButtons(\n        \"vble\", \"Variable\", # botones circulares: nombre y etiqueta\n        c(\n          \"sexo\" = \"sexo\",\n          \"tramo_edad\" = \"tramo_edad\",\n          \"tiempo_búsqueda_empleo_agregado\" = \"tiempo_búsqueda_empleo_agregado\",\n          \"sector\" = \"sector\",\n          \"tiempo_búsqueda_empleo\" = \"tiempo_búsqueda_empleo\"\n        ), \"sexo\"\n      )\n    ),\n    mainPanel(\n      plotOutput(\"gra1\")\n    )\n  )\n))\nshinyUI(fluidPage(\n  headerPanel(\"Evolución del paro\"),\n  sidebarLayout(position = \"right\", ...)\n))"},{"path":"shiny.html","id":"segmentación-de-diseños-tabsetpanel-y-navlistpanel","chapter":"Capítulo 45 Creación de aplicaciones web interactivas con Shiny","heading":"45.3.2 Segmentación de diseños: tabsetPanel() y navlistPanel()","text":"Para subdividir el panel principal en varias secciones discretas, es decir, crear pestañas, se puede usar tabsetPanel() y tabPanel() como sigue:\nFigura 45.3: Aplicación shiny con varias ventanas\nEn el ejemplo de la Fig. \\(\\ref{fig:tabs}\\), se aprecia uqe hay 3 ventanas y se muestra la tercera que es la evolución del paro considerando el tiempo de búsqueda de empleo y que es una gráfica reactiva sino estática.navlistPanel() es una alternativa tabsetPanel() cuando existan muchas separaciones. Un navlist presenta los distintos componentes como una lista de la barra lateral en lugar de utilizar pestañas y se hace en el mainPanel.","code":"\nmainPanel(\n  tabsetPanel(\n    tabPanel(\n      \"Elección con botones circulares\",\n      radioButtons(\n        \"vble\", \"Variable\", # botones circulares: nombre y etiqueta\n        c(\n          \"sexo\" = \"sexo\",\n          \"tramo_edad\" = \"tramo_edad\",\n          \"tiempo_búsqueda_empleo_agregado\" = \"tiempo_búsqueda_empleo_agregado\",\n          \"sector\" = \"sector\",\n          \"tiempo_búsqueda_empleo\" = \"tiempo_búsqueda_empleo\"\n        ), \"sexo\"\n      ),\n      plotOutput(\"gra1\")\n    ),\n    tabPanel(\"Tramo edad fijo\", plotOutput(\"gra2\")),\n    tabPanel(\"Tiempo búsqueda empleo fijo\", plotOutput(\"gra3\"))\n  )\n)\nui <- fluidPage(\n  titlePanel(\"Application Title\"),\n  navlistPanel(\n    \"Header A\",\n    tabPanel(\"Component 1\"),\n    tabPanel(\"Component 2\"),\n    \"Header B\",\n    tabPanel(\"Component 3\")\n  )\n)"},{"path":"shiny.html","id":"elementos-para-la-introducción-de-datos","chapter":"Capítulo 45 Creación de aplicaciones web interactivas con Shiny","heading":"45.4 Elementos para la introducción de datos","text":"Para que el usuario de la aplicación Shiny introduzca datos manualmente, hay diversos elementos que se enumeran continuación:Control deslizante: un control deslizante permite que el usuario seleccione entre un intervalo de valores moviendo un control de posición por una pista. En Shiny se crea con la función sliderInput() que tiene, entre otros, los siguientes argumentos:\ninputId: la entrada que se utiliza para acceder al valor\nlabel: etiqueta o nombre que aparece en la interfaz\nmin: el mínimo del control deslizante\nmax: el máximo del control deslizante\nvalue: el valor inicial\nstep: el intervalo entre cada valor seleccionable. NULL significa que va de uno en uno\nanimate: booleano que indica si los valores se cambian automáticamente para animar la aplicación\ninputId: la entrada que se utiliza para acceder al valorlabel: etiqueta o nombre que aparece en la interfazmin: el mínimo del control deslizantemax: el máximo del control deslizantevalue: el valor inicialstep: el intervalo entre cada valor seleccionable. NULL significa que va de uno en unoanimate: booleano que indica si los valores se cambian automáticamente para animar la aplicaciónSus características incluyen:La posibilidad de introducir un único valor y rangosLa posibilidad de introducir un único valor y rangosFormatos personalizados (por ejemplo, para entradas relativas al dinero)Formatos personalizados (por ejemplo, para entradas relativas al dinero)Pueden ser animados y recorrer los valores de forma automática (argumento animate)Pueden ser animados y recorrer los valores de forma automática (argumento animate)Algunos ejemplos son:Botón circular: un botón circular es un tipo de selector que proporciona una lista de opciones entre las cuales solo se puede seleccionar una. En Shiny se crean con la función radioButtons que tiene, entre otros, los siguientes argumentos autoexplicativos:Un ejemplo donde la variable “sexo” está elegida por defecto se puede ver en el primer trozo de código de la subsección 45.3.1 sidebarLayout().Selección múltiple: un cuadro de selección múltiple es un tipo de selector que proporciona una lista de opciones entre las cuales se pueden seleccionar varias. En Shiny se crean con la función selectInput que tiene, entre otros, los siguientes argumentos autoexplicativos:checkboxGroupInputMuy similar al anterior, este componente crea un grupo de casillas que se pueden utilizars para alternar varias opciones de forma independiente.Entrada numéricaEntrada de textoOtras opciones de entrada que se invita al lector analizar se relacionan con las fechas: dateInput(), dateRangeInput() y con un área de texto: textAreaInput().\nFigura 45.4: Distintos elementos para la introducción de datos\nLectura de ficheros de datosTambién es posible introducir información través de la lectura de ficheros de datos, con la función fileInput(). Se pueden combinar valores por defecto de la función utilizada para la lectura de datos con algunos de los elementos anteriores para definir las características del dataset (seaprador, decimal, cabecera). En el siguiente ejemplo se utiliza la función read.csv(), y se da elegir si tiene cabecera o con el checkboxInput(), así como el tipo de decimal con el radioButtons. Otra particularidad del ejemplo es que se asume que los datos están separados por punto y coma, tal y como se aprecia en el argumento sep del read.csv().\nFigura 45.5: Lectura de datos\n","code":"\nsliderInput(inputId, label, min, max, value, step = NULL, animate = FALSE)\nsliderInput(\"enteros\", \"Enteros:\", min = 0, max = 1000, value = 500)\nsliderInput(\"decimales\", \"Decimales:\", min = 0, max = 1, value = 0.5, step = 0.1)\nsliderInput(\"rango\", \"Rango:\", min = 1, max = 1000, value = c(200, 500))\nsliderInput(\"animacion\", \"Animacion:\", 10, 200, 10, step = 10, animate = animationOptions(loop = T))\nradioButtons(inputId, label, choices, selected = NULL)\nselectInput(inputId, label, choices, multiple = FALSE)\nselectInput(\"año\", \"Año:\",\n  c(\n    \"año1\" = \"2007\",\n    \"año2\" = \"2013\",\n    \"año3\" = \"2019\",\n    \"año4\" = \"2022\"\n  ),\n  multiple = TRUE\n)\ncheckboxGroupInput(inputId, label, choices, multiple = FALSE)\ncheckboxGroupInput(\n  \"variable\", \"Variables to show:\",\n  c(\n    \"año1\" = \"2007\",\n    \"año2\" = \"2013\",\n    \"año3\" = \"2019\",\n    \"año4\" = \"2022\"\n  )\n)\nnumericInput(\"obs\", \"Numero de observaciones:\", 10)\nhelpText(\"aclaraciones\")\nshinyUI(fluidPage(\n  headerPanel(\"Lectura de datos\"),\n  sidebarPanel(\n    h4(\"Cargar fichero CSV\"),\n    fileInput(\"file1\", \"\",\n      accept = c(\"text/csv\", \"text/comma-separated-values,text/plain\", \".csv\")\n    ),\n    checkboxInput(\"header\", \"Header (el csv tiene nombres de variables)\", TRUE),\n    radioButtons(\n      \"dec\", \"Separador de decimales:\",\n      c(\n        \"Punto\" = \",\",\n        \"Coma\" = \".\"\n      )\n    )\n  ),\n  mainPanel(\n    tabsetPanel(\n      tabPanel(\n        \"CSV\",\n        h4(\"Vista del fichero CSV\"),\n        tableOutput(\"contents\")\n      )\n    )\n  )\n))\n\nshinyServer(function(input, output) {\n  output$contents <- renderTable({\n    inFile <- input$file1\n\n    if (is.null(inFile)) {\n      return(NULL)\n    }\n\n    read.csv(inFile$datapath, header = input$header, dec = input$dec, sep = \";\")\n  })\n})"},{"path":"shiny.html","id":"elementos-para-visualización-salida","chapter":"Capítulo 45 Creación de aplicaciones web interactivas con Shiny","heading":"45.5 Elementos para visualización (salida)","text":"Tras la introducción de ciertos parámetros en el ui.R, éstos se pueden utilizar en el script server.R mediante la expresión input. El código de R que construye el objeto basado en esos datos se desarrolla en el servidor, y para generar dicho objeto se utilizan las funciones renderX, donde X es el tipo de objeto devolver. Por úlitmo, este objeto se referencia nuevamente en el ui.R en el lugar que se desea mostrar (panel) través de la expresión XOutput.El hecho de colocar una función en ui le dice Shiny dónde mostrar su objeto. continuación, hay que decirle Shiny cómo construir el objeto.\nEsto se hace proporcionando el código R que construye el objeto en la función del servidor.En concreto, algunas posibilidades se pueden ver en la Tabla ??.Gráficos: para generar la aplicación de la Fig. ??, se utiliza renderPlot en el server.R como sigue:Se ha llamado gra1 la variable que es el gráfico y que se crea con renderPlot(). En el interior se utiliza una función denominada graf_evol() que es compleja y se crea en elsource.R que se carga al principio. Lo que interesa de esta función es que tiene como único argumento el nombre de la variable de interés cuya evolución se desea mostrar. Ésta puede ser una de las diversas opciones que se dan través del radioButton denominado vble que ha sido creado anteriormente y que, como vemos, se utiliza input$vble para invocar la selección realizada en la interfaz de usuario.Tablas: para mostrar la tabla de la Fig. 45.5 se utiliza renderTable() en el server y tableOutput() en el ui.","code":"\nsource(\"source.R\")\nshinyServer(function(input, output) {\n  output$gra1 <- renderPlot({\n    graf_evol(input$vble)\n  })\n})\nshinyServer(function(input, output) {\n  output$contents <- renderTable({\n    inFile <- input$file1\n    if (is.null(inFile)) {\n      return(NULL)\n    }\n    read.csv(inFile$datapath, header = input$header, dec = input$dec, sep = \";\")\n  })\n})"},{"path":"shiny.html","id":"reactividad","chapter":"Capítulo 45 Creación de aplicaciones web interactivas con Shiny","heading":"45.6 Reactividad","text":"La programación reactiva es un paradigma de programación que se encarga de los flujos de datos y la propagación de los cambios. Ésto significa que cuando un flujo de datos es emitido por un componente, el cambio se propagará otros componentes.El modelo de reactividad que utiliza Shiny es el siguiente: hay una fuente reactiva, un conductor reactivo y un punto final de la reactividad (Hadley Wickham 2021).\nLa fuente reactiva suele ser lo que el usuario introduce y el punto de parada lo que se muestra por pantalla. lo que el usuario introduce se accede con el objeto input y lo que se muestra por pantalla con el objeto output.\nUn ejemplo que ya se ha usado es el siguiente:El objeto output$gra1 es un punto final de la reactividad, y usa la fuente reactiva input$vble. Cuando input$vbles cambia, output$gra1 se le notifica que necesita ejecutarse de nuevo.","code":"\noutput$gra1 <- renderPlot({\n  graf_evol(input$vble)\n})"},{"path":"shiny.html","id":"conductores-reactivos-y-control-de-la-reactividad","chapter":"Capítulo 45 Creación de aplicaciones web interactivas con Shiny","heading":"45.6.1 Conductores reactivos y control de la reactividad","text":"También es posible crear componentes reactivos que conecten los inputs y los outputs. En el siguiente ejemplo se ha creado un objeto reactivo datos que genera datos que siguen una distribución que el usuario selecciona través del radioButton dist y cuya muestra tiene tantos elementos como el usuario haya especificado en el numericInput obs.Shiny, además, permite controlar la reactividad través de los actionButtons. Se pueden modificar las entradas sin obtener una respuesta hasta que se apriete dicho botón.Se ha creado un panel nuevo dentro del mainPanel y éste contiene, además del plot previo, un actionButton con la etiqueta Presiona.continuación, se hace referencia ese botón para cada una de las expresiones reactivas que se aislarán con la función isolate():","code":"\nshinyUI(fluidPage(\n  headerPanel(\"Controlar reactividad\"),\n  sidebarPanel(\n    radioButtons(\"dist\", \"Tipo de distribucion:\",\n      c(\n        \"Normal\" = \"norm\",\n        \"Uniforme\" = \"unif\",\n        \"Log-normal\" = \"lnorm\",\n        \"Exponencial\" = \"exp\"\n      ),\n      selected = \"Exponencial\"\n    ),\n    numericInput(\"obs\", \"Numero de observaciones:\", 10),\n  ),\n  mainPanel(\n    tabPanel(\n      \"Histograma distribucion RadioButton\",\n      \"Plot\", plotOutput(\"plot\"), actionButton(\"botonReac\", \"Presiona\")\n    )\n  )\n))\nshinyServer(function(input, output) {\n  datos <- reactive({\n    if (input$botonReac == 0) {\n      return(dist(rexp(input$obs)))\n    }\n    isolate({\n      dist <- switch(input$dist,\n        norm = rnorm,\n        unif = runif,\n        lnorm = rlnorm,\n        exp = rexp,\n        rnorm\n      )\n\n      dist(input$obs)\n    })\n  })\n\n  output$plot <- renderPlot({\n    if (input$botonReac == 0) {\n      return(NULL)\n    }\n    isolate({\n      hist(datos(),\n        main = paste(\"r\", input$dist, \"(\", input$obs, \")\", sep = \"\")\n      )\n    })\n  })\n})"},{"path":"shiny.html","id":"publicación-de-la-aplicación-en-la-web","chapter":"Capítulo 45 Creación de aplicaciones web interactivas con Shiny","heading":"45.7 Publicación de la aplicación en la web","text":"Después del desarrollo de una aplicación Shiny, suele ser interesante publicarla para su explotación científica o empresarial. Rstudio ofrece diversas soluciones que se analizarán, con distintos niveles de complejidad y libertad, para poder publicar la aplicación web: () shinyapps.io, (ii) Shiny Server y (iii) RStudio Connect.continuación se introduce, muy brevemente, cada uno de ellos, y se proporcionan los enlaces para que el lector pueda indagar en profundidad.Shinyapps.ioRstudio ofrece un servicio de hospedage denominado Shinyapps.io que permite subir la aplicación directamente desde la sesión de R un servidor que se mantiene por Rstudio. Hay un control casi completo sobre la aplicación, incluyendo la administración del servidor. Tiene distintos planes, desde gratuito hasta profesional, siendo el primero más restringido en cuanto servicios (número de aplicaciones, horas activo…) y el último más completo (autenticación, personalización, etc).Lo único que se necesita es:Un entorno de desarrollo de R, como RStudio IDE.Un entorno de desarrollo de R, como RStudio IDE.La última versión del paquete rsconnect.La última versión del paquete rsconnect.En la web shinyapps.io en esl apartado “Dashboard” se realiza el registro. Shinyapps.io genera de forma automática un token que el paquete rsconnect utiliza para acceder la cuenta.Para desplegar la aplicación se utiliza deployApp() como sigue:Existen opciones gratuitas, que carecen de ciertas ventajas, como la posibilidad de restringir el acceso la aplicación shiny, es decir, la aplicación será privada con el plan gratuito aunque sí existen las opciones de autentificación con otros planes. Para más información sobre este método, consúltese la página https://shiny.rstudio.com/articles/shinyapps.html.Shiny ServerShiny server construye un servidor web diseñado para hospedar aplicaciones Shiny. Es gratuito, de código abierto y está disponible en GitHub.Para usar el Shiny Server, es necesario tener un servidor Linux que tenga soporte explícito para Ubuntu 12.04 superior (64 bit) y CentOS/RHEL 5 (64 bit). Aunque se esté utilizando una distribución con soporte explícito, también se puede utilizar, si bien construyéndolo desde el paquete fuente.En el mismo Shiny Server se pueden hospedar múltiples aplicaciones Shiny. Para ver instrucciones detalladas para su instalación y configuración, se recomienda la guía Shiny Server https://docs.rstudio.com/shiny-server.La seguridad y privacidad quedarán supeditadas los conocimientos del usuario, ya que dependerán de su propio servidor.RStudio ConnectCuando Shiny se utiliza en entornos con fines lucrativos, existen herramientas de servidor que se pueden comprar y que vienen equipadas con los programas habituales de un servidor de pago:Soporte SSLHerramientas de administradorSoporte prioritarioPrivilegios de usuarioOpción con DockerPara tener dichas herramientas de servidor, la plataforma de publicación RStudio Connect puede ser una solución. Esta herramienta permite compartir aplicacciones Shiny, informes RMarkdown, cuadros de mando, gráficos, Jupyter Notebooks y más.\nCon RStudio Connect se puede programar la ejecución de informes y políticas de seguridad flexibles.Además, RStudio Connect permite seleccionar privilegios de usuario en aplicaciones Shiny. La aplicación Shiny puede reconocer un usuario basándose en la información de inicio de sesión y ofrecerle contenido personalizado, de manera que se puede controlar quién ve qué contenido y cuándo lo ve.","code":"\nrsconnect::setAccountInfo(name = \"<ACCOUNT>\", token = \"<TOKEN>\", secret = \"<SECRET>\")\nlibrary(rsconnect)\ndeployApp()"},{"path":"shiny.html","id":"extensiones-de-shiny","chapter":"Capítulo 45 Creación de aplicaciones web interactivas con Shiny","heading":"45.8 Extensiones de Shiny","text":"Shiny es una herramienta totalmente expansible. Lo que se ha mostrado en este capítulo hasta ahora es un aperitivo en relación todas las posibilidades que existen en el mundo de Shiny. Hay repositorios que recopilan información sobre paquetes que proveen de mejoras las aplicaciones Shiny en su estilo y funcionalidad Gilmore et al. (2017). En esta sección se mencionan algunos de ellos pero, sobre todo, se recomienda al lector visitar dichos repositorios para una mayor profundidad en este tema.shinydashboard, shinydashboardPlus y flexdashboardsEn temas de estilo, merece la pena destacar estos tres paquetes. Los dos primeros presentan una serie de plantillas predefinidas para la creación de las aplicaciones Shiny, de manera que los colores combinan y los elementos visuales tienen cierta armonía.Por su parte, flexdashboards tiene como base un documento R Markdown y los distintos niveles del mismo definen los paneles de la aplicación crear.shinyWidgetsEste paquete ofrece widgets239 personalizados y diversos componentes para mejorar las aplicaciones. Se pueden reemplazar los checkboxes por switch buttons, añadir colores los radioButtons y al grupo de casillas de verificación (checkboxGroupInput), etc. Cada widget tiene un método de actualización para cambiar el valor de una entrada del server.shinycssloadersCuando una salida de Shiny (un gráfico, una tabla, etc.) se está calculando, permanece visible pero en gris. Si hay procesos algo más complejos, pueden tardar en mostrarse. Utilizando shinycssloaders, se puede añadir una rueda de carga (spinner) las salidas en lugar de hacerlas grises. Envolviendo una salida Shiny en withSpinner(), el spinner aparecerá automáticamente mientras la salida se recalcula…. Hay ocho tipos de animación incorporadas y personalizables en color y tamaño, pero también se pueden cargar otras animaciones.Visualizaciones interactivasPaquetes como heatmaply o leaflet se pueden combinar perfectamente con Shiny para crear mapas de calor y mapas geográficos interactivos, y utilizarlos en las aplicaciones.","code":""},{"path":"shiny.html","id":"resumen-35","chapter":"Capítulo 45 Creación de aplicaciones web interactivas con Shiny","heading":"Resumen","text":"Shiny es un paquete de R que permite crear aplicaciones web interactivas requiriendo únicamente conocimientos de R. En la primera parte de este capítulo se muestran los elementos básicos de una aplicación Shiny: user interface (ui.R) y servidor (server.R), así como los posibles diseños en relación los componentes que una aplicación puede tener: barra lateral, paneles discretos, paneles de navegación, etc. continuación, se repasan los elementos de introducción de datos en una aplicación Shiny, incluyendo la carga de conjuntos de datos y también los elementos de salida, como gráficas y tablas. También se aborda el modelo reactividad, es decir, cómo al cambiar algo en los parámetros de entrada de forma dinámica cambia la salida y cómo controlar éste proceso y se muestran distintas opciones para la publicación de las aplicaciones Shiny. Por último, se mencionan algunas de las posibles extensiones al paquete.","code":""},{"path":"github.html","id":"github","chapter":"Capítulo 46 Git y GitHub R","heading":"Capítulo 46 Git y GitHub R","text":"Michal Kinel","code":""},{"path":"github.html","id":"qué-es-git-y-github","chapter":"Capítulo 46 Git y GitHub R","heading":"46.1 ¿Qué es Git y GitHub?","text":"Git es un sistema de control de versiones distribuido, diseñado para\nregistrar y rastrear los cambios realizados en un archivo o conjunto de\narchivos lo largo del tiempo (Chacon 2009). Al utilizar Git, se pueden ver y\nrestaurar versiones anteriores de un archivo, así como fusionar cambios\nrealizados por diferentes personas en una única versión actualizada.Por otro lado, GitHub es una plataforma de alojamiento de código online\nque utiliza Git como sistema de control de versiones subyacentes. Esta\nplataforma permite los desarrolladores compartir y colaborar en\nproyectos de software, alojando el código fuente en la nube (Astigarraga Cruz-Alonso 2022). Además de\nalojar repositorios de Git, GitHub ofrece herramientas adicionales como\nseguimiento de problemas, solicitudes de extracción, y wiki de\nproyectos, lo que la hace una herramienta popular para el desarrollo de\nsoftware colaborativo y de código abierto.El uso de Git y GitHub se ha extendido otros campos más allá del\ndesarrollo de software, como la ciencia de datos, la documentación\ntécnica y la colaboración en general. Su popularidad se debe en gran\nparte la facilidad de uso, la flexibilidad y la capacidad de trabajar\nen proyectos de software colaborativos de manera eficiente y segura tanto.","code":""},{"path":"github.html","id":"por-qué-usar-git-y-github","chapter":"Capítulo 46 Git y GitHub R","heading":"46.2 ¿Por qué usar Git y GitHub?","text":"Git y GitHub son herramientas para el desarrollo de software moderno, y\nsu uso se ha extendido otros campos como la ciencia de datos, la\ndocumentación técnica y la colaboración entre los desarrolladores. continuación, se\npresentan tres ventajas clave de uso de Git y GitHub:Control de versiones y colaboración eficiente Git es un sistema\nde control de versiones distribuido que permite registrar y rastrear\nlos cambios realizados en un archivo o conjunto de archivos lo\nlargo del tiempo. Esto es especialmente útil cuando se trabaja en\nproyectos de software colaborativos, donde múltiples personas pueden\nestar editando el mismo archivo al mismo tiempo. Con Git, se pueden\nver y restaurar versiones anteriores de un archivo, y también\nfusionar cambios realizados por diferentes personas en una única\nversión actualizada. Además, GitHub ofrece herramientas adicionales\ncomo seguimiento de problemas, solicitudes de extracción, y wiki de\nproyectos.Control de versiones y colaboración eficiente Git es un sistema\nde control de versiones distribuido que permite registrar y rastrear\nlos cambios realizados en un archivo o conjunto de archivos lo\nlargo del tiempo. Esto es especialmente útil cuando se trabaja en\nproyectos de software colaborativos, donde múltiples personas pueden\nestar editando el mismo archivo al mismo tiempo. Con Git, se pueden\nver y restaurar versiones anteriores de un archivo, y también\nfusionar cambios realizados por diferentes personas en una única\nversión actualizada. Además, GitHub ofrece herramientas adicionales\ncomo seguimiento de problemas, solicitudes de extracción, y wiki de\nproyectos.Mejora la eficiencia y la seguridad en el desarrollo de software\nEl uso de Git y GitHub permite los desarrolladores trabajar de\nmanera más eficiente y segura en proyectos de software. Al utilizar\nun sistema de control de versiones como Git, los desarrolladores\npueden colaborar de manera más efectiva y reducir el riesgo de\nconflictos o errores en el código. Además, GitHub ofrece\ncaracterísticas como la integración continua y la entrega continua\n(CI/CD), que automatizan y agilizan el proceso de desarrollo de\nsoftware.Mejora la eficiencia y la seguridad en el desarrollo de software\nEl uso de Git y GitHub permite los desarrolladores trabajar de\nmanera más eficiente y segura en proyectos de software. Al utilizar\nun sistema de control de versiones como Git, los desarrolladores\npueden colaborar de manera más efectiva y reducir el riesgo de\nconflictos o errores en el código. Además, GitHub ofrece\ncaracterísticas como la integración continua y la entrega continua\n(CI/CD), que automatizan y agilizan el proceso de desarrollo de\nsoftware.Fomenta la transparencia y la comunidad GitHub es una plataforma\nde alojamiento de código abierto , lo que significa que los proyectos\nalojados en ella son de acceso público y pueden ser revisados y\nmejorados por otros desarrolladores. Esto fomenta la transparencia\nde código abierto como privado dentro de una empresa. Además, GitHub\ncuenta con una gran comunidad de desarrolladores que pueden ofrecer\nsoporte y retroalimentación otros miembros de la comunidad.Fomenta la transparencia y la comunidad GitHub es una plataforma\nde alojamiento de código abierto , lo que significa que los proyectos\nalojados en ella son de acceso público y pueden ser revisados y\nmejorados por otros desarrolladores. Esto fomenta la transparencia\nde código abierto como privado dentro de una empresa. Además, GitHub\ncuenta con una gran comunidad de desarrolladores que pueden ofrecer\nsoporte y retroalimentación otros miembros de la comunidad.","code":""},{"path":"github.html","id":"sec-instala-git","chapter":"Capítulo 46 Git y GitHub R","heading":"46.3 Instalación y/o actualización de R y RStudio","text":"R es un lenguaje de programación utilizado en la estadística y la\nciencia de datos para realizar análisis, modelado y visualización de\ndatos. RStudio, por otro lado, es un entorno de desarrollo integrado\n(IDE) que proporciona una interfaz gráfica de usuario para trabajar con\nR. Instalar o actualizar R y RStudio es un proceso relativamente\nsencillo, y se pueden seguir los siguientes pasos:Descargar e instalar R Lo primero que se debe hacer es descargar\nR desde la página oficial de\nR. Dependiendo del sistema\noperativo, se debe elegir la versión correcta de R para\ndescargar. Una vez que se haya descargado el archivo, se debe seguir\nel asistente de instalación para instalar R en el equipo.Descargar e instalar R Lo primero que se debe hacer es descargar\nR desde la página oficial de\nR. Dependiendo del sistema\noperativo, se debe elegir la versión correcta de R para\ndescargar. Una vez que se haya descargado el archivo, se debe seguir\nel asistente de instalación para instalar R en el equipo.Descargar e instalar RStudio: una vez instalado R, se puede\nproceder instalar RStudio desde su página\noficial. Al igual que\ncon R, se debe elegir la versión adecuada para el sistema\noperativo y seguir el asistente de instalación para instalar\nRStudio en el equipo.Descargar e instalar RStudio: una vez instalado R, se puede\nproceder instalar RStudio desde su página\noficial. Al igual que\ncon R, se debe elegir la versión adecuada para el sistema\noperativo y seguir el asistente de instalación para instalar\nRStudio en el equipo.Actualización de R y RStudio Para actualizar R, se debe\nabrir R y ejecutar el siguiente comando en la consola:Actualización de R y RStudio Para actualizar R, se debe\nabrir R y ejecutar el siguiente comando en la consola:Esto instalará el paquete installr y actualizará automáticamente R\nla última versión disponible. Para actualizar RStudio, se debe\nabrir RStudio y verificar si hay una actualización disponible en el\nmenú “Help” -> “Check Updates”. Si hay una actualización\ndisponible, se debe seguir el asistente de actualización para instalar\nla última versión de RStudio.En resumen, la instalación o actualización de R y RStudio es un\nproceso sencillo que se puede realizar siguiendo los pasos mencionados\nanteriormente. Mantener estas herramientas actualizadas es importante\npara asegurarse de tener acceso las últimas características y\ncorrecciones de errores.","code":"\ninstall.packages(\"installr\") \nlibrary(installr)\nupdateR()"},{"path":"github.html","id":"configurar-git-github","chapter":"Capítulo 46 Git y GitHub R","heading":"46.4 Configuración de Git y GitHub","text":"Configurar Git y GitHub es un paso importante antes de comenzar \ntrabajar en proyectos de software colaborativos. Se pueden seguir los\nsiguientes pasos para configurar Git y GitHub:Instalar Git . En primer lugar, es necesario instalar Git en el\nequipo. Git puede descargarse desde la página oficial de\nGit. Una vez que se haya descargado el\narchivo, se debe seguir el asistente de instalación para instalar Git en\nel equipo. Además, en la página oficial se encuentra un manual completo\nsobre el uso de Git.Instalar Git . En primer lugar, es necesario instalar Git en el\nequipo. Git puede descargarse desde la página oficial de\nGit. Una vez que se haya descargado el\narchivo, se debe seguir el asistente de instalación para instalar Git en\nel equipo. Además, en la página oficial se encuentra un manual completo\nsobre el uso de Git.Configurar Git. Una vez que se ha instalado Git, se debe configurar\nel nombre de usuario y la dirección de correo electrónico para que los\ncambios que se realicen en los repositorios estén correctamente\netiquetados. Para hacer esto, se debe abrir la línea de comandos, Git\nBash o la terminal de RStudio, y ejecutar los siguientes comandos:Configurar Git. Una vez que se ha instalado Git, se debe configurar\nel nombre de usuario y la dirección de correo electrónico para que los\ncambios que se realicen en los repositorios estén correctamente\netiquetados. Para hacer esto, se debe abrir la línea de comandos, Git\nBash o la terminal de RStudio, y ejecutar los siguientes comandos:\nFigura 46.1: Terminal de RStudio\nEsto configurará el nombre de usuario y la dirección de correo\nelectrónico de forma global en Git.Crear una cuenta en GitHub. Para utilizar GitHub, es necesario crear\nuna cuenta en la página oficial de GitHub (https://github.com/join).\nUna vez que se haya creado la cuenta, se debe iniciar sesión en GitHub.Crear una cuenta en GitHub. Para utilizar GitHub, es necesario crear\nuna cuenta en la página oficial de GitHub (https://github.com/join).\nUna vez que se haya creado la cuenta, se debe iniciar sesión en GitHub.Configurar la clave SSH (protocolo Secure Shell), una credencial de acceso para el protocolo de red que permite el acceso remoto través de una conexión cifrada. Para autentificar las conexiones con GitHub de\nmanera segura (véase capítulo 10 de (Jenny Bryan 2021)), se recomienda configurar una clave SSH en el equipo y\nagregarla la cuenta de GitHub. Para ello, se debe abrir la línea de\ncomandos, Git Bash o la terminal de RStudio, y ejecutar los siguientes\ncomandos:Configurar la clave SSH (protocolo Secure Shell), una credencial de acceso para el protocolo de red que permite el acceso remoto través de una conexión cifrada. Para autentificar las conexiones con GitHub de\nmanera segura (véase capítulo 10 de (Jenny Bryan 2021)), se recomienda configurar una clave SSH en el equipo y\nagregarla la cuenta de GitHub. Para ello, se debe abrir la línea de\ncomandos, Git Bash o la terminal de RStudio, y ejecutar los siguientes\ncomandos:Esto generará una clave SSH. continuación, se debe agregar la clave\nSSH al agente de SSH:Finalmente, se debe copiar la clave SSH al portapapeles:y agregarla la cuenta de GitHub siguiendo las instrucciones en la página de configuración de la cuenta de GitHub:En la esquina superior derecha dela página del inicio, haga clic en la\nfoto del perfil y, luego, en “Settings” (Configuración).En la esquina superior derecha dela página del inicio, haga clic en la\nfoto del perfil y, luego, en “Settings” (Configuración).En la sección “Accsess” de la barra lateral, haga clic en “SSH adn GPG keys”.En la sección “Accsess” de la barra lateral, haga clic en “SSH adn GPG keys”.Haga clic en “New SSH key” para agregar la clave SSH.Haga clic en “New SSH key” para agregar la clave SSH.\nFigura 46.2: Llaves SSH en GitHub\nEn el campo “Title” (Título), agregue una etiqueta descriptiva para\nla clave nueva. Por ejemplo, si está utilizando un portátil\npersonal, puede llamar esta clave “Portátil personal”.En el campo “Title” (Título), agregue una etiqueta descriptiva para\nla clave nueva. Por ejemplo, si está utilizando un portátil\npersonal, puede llamar esta clave “Portátil personal”.Seleccione el tipo de clave, ya sea de autentificación o de firma.\nPara obtener más información sobre la firma de una confirmación,\nconsulte\naquí.Seleccione el tipo de clave, ya sea de autentificación o de firma.\nPara obtener más información sobre la firma de una confirmación,\nconsulte\naquí.Pegue su clave pública en el campo “Key”.Pegue su clave pública en el campo “Key”.\nFigura 46.3: Añadir llave SSH en GitHub\nHaga clic en “Add SSH key” para agregar la clave SSH.Haga clic en “Add SSH key” para agregar la clave SSH.Si se le solicita, confirme su contraseña en GitHub. Para obtener\nmás información, véase Modo\nsudo.Si se le solicita, confirme su contraseña en GitHub. Para obtener\nmás información, véase Modo\nsudo.Además de utilizar una clave SSH para autentificar las conexiones con\nGitHub, también se puede utilizar la autentificación basada en token de\nacceso personal, PAT , de GitHub. Esta forma de autentificación es\nrecomendada por GitHub como una forma segura de autentificar conexiones,\nespecialmente cuando se trabaja con aplicaciones y herramientas que\nrequieren acceso repositorios de GitHub. Para más información sobre el token de aceso personal, PAT, consulte el capítulo 9 de (Jenny Bryan 2021).continuación, se describen los pasos para utilizar la autentificación\nbasada en token de acceso personal de GitHub:Generar el token PAT: existen dos librerías usethis y gitcreds\nque facilitan la generación del PAT y almacenarlo, para ello, se\nintroduce en la consola de RStudio:Seguir las instrucciones en GitHub: continuación se abrirá el sito\nweb de GitHub, se ingresa mediante el usuario y contraseña, con el\ncuadro para generación del PAT, New personal access token (classic). En Note se introduce una nota identificativa al igual\nque en el procedimiento anterior y se selecciona el tiempo de\nvalidez del PAT en la pestaña Expiration, dejando las demás\nopciones por defecto. Se hace clic en Generate token para crear el\ntoken. En la nueva ventana se copia el token para posteriormente introducir en la consola:En password se pega el token copiado anteriormente.Para verificar que el nuevo PAT está configurado se introduce en la\nconsola:Si la autentificación fue correcta se generará una salida similar la\nsiguiente:Una vez conectado RStudio y GitHub mediante SHH o PAT se puede proceder\ncon la creación del repositorio en Git.","code":"$ git config --global user.name \"Su Nombre\"\n$ git config --global user.email \"su.correo@ejemplo.com\"$ ssh-keygen -t rsa -b 4096 -C \"su.correo@ejemplo.com\"$ eval \"$(ssh-agent -s)\"\n$ ssh-add ~/.ssh/id_rsa$ clip < ~/.ssh/id_rsa.pub\nlibrary(usethis)\nusethis::create_github_token()\nlibrary(gitcreds)\ngitcreds::gitcreds_set()\ngitcreds::gitcreds_get(use_cache = FALSE) <gitcreds>\n   protocol: https\n   host    : github.com\n   username: mi_usuario\n   password: <-- hidden -->"},{"path":"github.html","id":"conectar-git-y-github-con-rstudio","chapter":"Capítulo 46 Git y GitHub R","heading":"46.5 Conectar Git y GitHub con Rstudio","text":"","code":""},{"path":"github.html","id":"conectar-rstudio-primero","chapter":"Capítulo 46 Git y GitHub R","heading":"46.5.1 Rstudio primero","text":"Este apartado se centra en el enfoque de creación de un nuevo proyecto\nen un ordenador local para posteriormente subirlo GitHub, en remoto.Una vez instalados y configurado Git en nuestro sistema y con la cuenta\nde GitHub hay que seguir los siguientes pasos para conectar Git y GitHub\ncon RStudio:Configurar Git en RStudio: Una vez que Git está instalado en el\nsistema, se debe configurar Git en RStudio. Para ello, se debe ir \nla pestaña “Tools” en la barra de menú principal, seleccionar\n“Global Options” y luego seleccionar “Git/SVN”. Desde allí, se debe\nconfigurar la ubicación del ejecutable de Git en el sistema.\nFigura 46.4: Tools de Rstudio\nVerificar la versión de Git, introduciendo en la Terminal:Si la salida es la versión de Git entonces la instalación fue ejecutada correctamente.Crea un proyecto nuevo desde “File” -> “New project”\nFigura 46.5: Nuevo proyecto de Rstudio\nEn el siguiente cuadro se procede dando clic en “New directory”\ny en la siguiente ventana se rellenan los datos como el nombre del\nproyecto y se marca la opción “Create git repository” para crear\nun nevo proyecto con repositorio de Git.\nFigura 46.6: Nuevo proyecto en un directorio nuevo\nEn el icono de Git en la parte superor se accede la ventana de\nrevisión de cambios, se añaden los ficheros pinchando en los ticks,\nse añade el mensaje de confirmación y se hace clic en “commit”.\nFigura 46.7: Revisión de cambios\nAlternativamente se puede utilizar la pestaña de Git, marcando los\nficheros modificados o creados y confirmando mediante clic en “commit”\ntras el cual se abrirá el cuadro de diálogo anterior.\nFigura 46.8: Revisión de cambios\nPara subir los cambios realizados en el proyecto recién configurado\nen RStudio, habiendo configurado Git y GitHub en los pasos\nanteriores, se ejecuta en la consola los siguientes pasosLa función usethis::use_github() en sus valores por defecto creará un\nrepositorio público con el nombre de proyecto en la cuenta asociada.\nPara ver más opciones acuda la ayuda de la función, ejecutando en la\nconsola ?usethis::use_github.","code":"$ git --version\nlibrary(\"usethis\")\nusethis::use_github()"},{"path":"github.html","id":"github-primero","chapter":"Capítulo 46 Git y GitHub R","heading":"46.5.2 GitHub Primero","text":"Este apartado se centra en el enfoque de creación de un nuevo proyecto\nen un ordenador local partir de un repositorio disponible en GitHub, en\nremoto. Antes de todo hay que verificar si se tiene instalado Git, basta\nintroduciendo en la terminal de RStudio al igual que en el apartado anterior.Cuando la salida de la terminal arroje la versión de Git entonces la instalación fue correcta. En el caso de que la salida arroje la versión vuelva la Sec. 46.4 o consulte el manual de la página oficial de Git en: https://git-scm.com.continuacción, se describe paso paso sobre cómo conectar GitHub y\nRStudio partir de un proyecto ya existente en GitHub y con Git\nconfigurado previamente:Abra RStudio y seleccione la opción “New Project” en la pestaña\n“File” del menú principal. Posteriormente haga clic en la opción\n“Version Control”.\nFigura 46.9: Nuevo proyecto de Rstudio\nEn la ventana emergente que aparece, elija “Git”.\nFigura 46.10: Crear proyecto desde control de versiones\nEn la siguiente ventana, pegue la URL del repositorio que desee\nclonar y presione “Create Project”. RStudio le preguntará en qué\ncarpeta desea guardar el proyecto, una vez que hayas elegido una\nubicación, el proyecto se clonará en tu computadora.\nFigura 46.11: Nuevo proyecto desde un repositorio de Git\nLos cambios realizados en el repositorio se realizan de la misma forma\nque en el apartado anterior.","code":"$ git --version"},{"path":"github.html","id":"flujo-de-trabajo-general-de-git-y-github-en-rstudio","chapter":"Capítulo 46 Git y GitHub R","heading":"46.6 Flujo de trabajo general de Git y GitHub en RStudio","text":"continuación se describe un flujo básico de trabajo, comenzando desde\nRStudio:Iniciar un repositorio local: Lo primero que hay que hacer es\ninicializar un repositorio local en RStudio. Para ello, abra RStudio\ny seleccione la opción “New Project” en la pestaña “File” del menú\nprincipal. Luego, seleccione la opción “New Directory” y elija una\nubicación para su proyecto. continuación, seleccione “Version\nControl” y luego “Git”. RStudio le preguntará si desea inicializar\nun repositorio en este directorio; haga clic en “Yes”. Tal y como se\nha descrito en el punto 1 de la Sec. 46.5.1.Iniciar un repositorio local: Lo primero que hay que hacer es\ninicializar un repositorio local en RStudio. Para ello, abra RStudio\ny seleccione la opción “New Project” en la pestaña “File” del menú\nprincipal. Luego, seleccione la opción “New Directory” y elija una\nubicación para su proyecto. continuación, seleccione “Version\nControl” y luego “Git”. RStudio le preguntará si desea inicializar\nun repositorio en este directorio; haga clic en “Yes”. Tal y como se\nha descrito en el punto 1 de la Sec. 46.5.1.Añadir archivos al repositorio: Ahora, dene añadir los archivos\nde su proyecto al repositorio. Para ello, haga clic en la pestaña\n“Git” en la parte superior derecha de RStudio, y luego seleccione\nlos archivos que desea agregar al repositorio. Haga clic en el botón\n“Stage”, y los archivos seleccionados pasarán la sección “Staged”\nen la parte inferior de la pestaña “Git”. Si desea agregar todos los\narchivos del proyecto al repositorio, haz clic en el botón “Stage\n”.Añadir archivos al repositorio: Ahora, dene añadir los archivos\nde su proyecto al repositorio. Para ello, haga clic en la pestaña\n“Git” en la parte superior derecha de RStudio, y luego seleccione\nlos archivos que desea agregar al repositorio. Haga clic en el botón\n“Stage”, y los archivos seleccionados pasarán la sección “Staged”\nen la parte inferior de la pestaña “Git”. Si desea agregar todos los\narchivos del proyecto al repositorio, haz clic en el botón “Stage\n”.Hacer un “commit” de los cambios: Una vez que los archivos están\nen la sección “Staged”, debe hacer un “commit” para registrar los\ncambios. Para hacerlo, escriba un mensaje breve que describa los\ncambios que ha realizado en la sección “Commit message”. Luego, haga\nclic en el botón “Commit”. Los cambios se registrarán en el\nrepositorio local.Hacer un “commit” de los cambios: Una vez que los archivos están\nen la sección “Staged”, debe hacer un “commit” para registrar los\ncambios. Para hacerlo, escriba un mensaje breve que describa los\ncambios que ha realizado en la sección “Commit message”. Luego, haga\nclic en el botón “Commit”. Los cambios se registrarán en el\nrepositorio local.Crear una rama (opcional): Si desea trabajar en una nueva\nfunción o corregir un error sin afectar la rama principal (master o\nmain), debe crear una nueva rama. Para ello, haga clic en el botón\n“New Branch” en la pestaña “Git”. Escriba un nombre para la nueva\nrama y haga clic en “Create”. Ahora ya es posible hacer cambios en\nlos archivos en la nueva rama sin afectar la rama principal.Crear una rama (opcional): Si desea trabajar en una nueva\nfunción o corregir un error sin afectar la rama principal (master o\nmain), debe crear una nueva rama. Para ello, haga clic en el botón\n“New Branch” en la pestaña “Git”. Escriba un nombre para la nueva\nrama y haga clic en “Create”. Ahora ya es posible hacer cambios en\nlos archivos en la nueva rama sin afectar la rama principal.Subir los cambios al repositorio remoto: Una vez que ha hecho un\n“commit” o confirmación de sus cambios, es hora de subirlos al\nrepositorio remoto en GitHub. Para ello, haga clic en el botón\n“Push” en la pestaña “Git”. Los cambios se subirán al repositorio\nremoto en GitHub, que fue configurado en la Sec. 46.4.Subir los cambios al repositorio remoto: Una vez que ha hecho un\n“commit” o confirmación de sus cambios, es hora de subirlos al\nrepositorio remoto en GitHub. Para ello, haga clic en el botón\n“Push” en la pestaña “Git”. Los cambios se subirán al repositorio\nremoto en GitHub, que fue configurado en la Sec. 46.4.Solicitar un pull request (opcional): Si trabaja en un proyecto\ncolaborativo con otros usuarios, debe solicitar un “pull request”\nantes de fusionar los cambios en la rama principal. Para hacerlo,\nhaga clic en la pestaña “Pull Requests” en la interfaz de GitHub.\nLuego, haga clic en el botón “New Pull Request” y siga las\ninstrucciones para crear la solicitud.Solicitar un pull request (opcional): Si trabaja en un proyecto\ncolaborativo con otros usuarios, debe solicitar un “pull request”\nantes de fusionar los cambios en la rama principal. Para hacerlo,\nhaga clic en la pestaña “Pull Requests” en la interfaz de GitHub.\nLuego, haga clic en el botón “New Pull Request” y siga las\ninstrucciones para crear la solicitud.Fusionar los cambios en la rama principal (opcional): Si trabaja\nen una nueva rama y desea fusionar los cambios en la rama principal,\ndebe crear una solicitud de “pull request”. Si la solicitud es\naceptada por el propietario del repositorio, los cambios se\nfusionarán en la rama principal.Fusionar los cambios en la rama principal (opcional): Si trabaja\nen una nueva rama y desea fusionar los cambios en la rama principal,\ndebe crear una solicitud de “pull request”. Si la solicitud es\naceptada por el propietario del repositorio, los cambios se\nfusionarán en la rama principal.Repositorio local y remoto:Repositorio local en Git es una copia completa de un proyecto que se\nencuentra en el equipo del usuario. Con un repositorio local, los\nusuarios pueden trabajar en un proyecto sin conexión Internet y\nluego enviar los cambios al repositorio remoto cuando estén\nconectados.Repositorio local en Git es una copia completa de un proyecto que se\nencuentra en el equipo del usuario. Con un repositorio local, los\nusuarios pueden trabajar en un proyecto sin conexión Internet y\nluego enviar los cambios al repositorio remoto cuando estén\nconectados.Repositorio remoto en GitHub es una versión en línea del proyecto\nque está almacenada en los servidores de GitHub. Los usuarios pueden\nclonar un repositorio remoto su equipo para tener una copia local\ndel proyecto y trabajar en ella. Los cambios realizados en la copia\nlocal pueden ser enviados al repositorio remoto para compartirlos\ncon otros usuarios.Repositorio remoto en GitHub es una versión en línea del proyecto\nque está almacenada en los servidores de GitHub. Los usuarios pueden\nclonar un repositorio remoto su equipo para tener una copia local\ndel proyecto y trabajar en ella. Los cambios realizados en la copia\nlocal pueden ser enviados al repositorio remoto para compartirlos\ncon otros usuarios.En resumen, el flujo de trabajo general de Git y GitHub en RStudio\nimplica inicializar un repositorio local, añadir archivos al\nrepositorio, hacer un “commit” de los cambios, crear una nueva rama si es\nnecesario, subir los cambios al repositorio remoto en GitHub.Todas las operaciones se pueden realizar desde la terminal de RStudio.\nAquí hay algunos de los comandos más comunes que se utilizan en Git:git init: Este comando se utiliza para crear un nuevo\nrepositorio de Git. Se ejecuta en el directorio raíz del proyecto y\nestablece la estructura necesaria para que Git rastree los cambios\nen el código fuente.git init: Este comando se utiliza para crear un nuevo\nrepositorio de Git. Se ejecuta en el directorio raíz del proyecto y\nestablece la estructura necesaria para que Git rastree los cambios\nen el código fuente.git clone: Este comando se utiliza para clonar un repositorio\nexistente de Git. Es útil cuando se desea trabajar en un proyecto\nque ya está en GitHub o en otro servicio de alojamiento de\nrepositorios de Git.git clone: Este comando se utiliza para clonar un repositorio\nexistente de Git. Es útil cuando se desea trabajar en un proyecto\nque ya está en GitHub o en otro servicio de alojamiento de\nrepositorios de Git.git add: Este comando se utiliza para agregar archivos nuevos o\nmodificados al área de preparación “Stage” de Git. La preparación es\nel primer paso para confirmar los cambios en Git.git add: Este comando se utiliza para agregar archivos nuevos o\nmodificados al área de preparación “Stage” de Git. La preparación es\nel primer paso para confirmar los cambios en Git.git commit: Este comando se utiliza para confirmar los cambios\nrealizados en el repositorio de Git. Los cambios confirmados se\nguardan en la base de datos de Git y se etiquetan con un mensaje que\ndescribe los cambios.git commit: Este comando se utiliza para confirmar los cambios\nrealizados en el repositorio de Git. Los cambios confirmados se\nguardan en la base de datos de Git y se etiquetan con un mensaje que\ndescribe los cambios.git push: Este comando se utiliza para enviar los cambios\nconfirmados un repositorio remoto, como GitHub. Esto actualiza el\nrepositorio remoto con los cambios realizados en el repositorio\nlocal.git push: Este comando se utiliza para enviar los cambios\nconfirmados un repositorio remoto, como GitHub. Esto actualiza el\nrepositorio remoto con los cambios realizados en el repositorio\nlocal.git pull: Este comando se utiliza para actualizar el repositorio\nlocal con los cambios realizados en el repositorio remoto. Es útil\ncuando se está trabajando en un proyecto colaborativo y otros\ncolaboradores han realizado cambios en el repositorio remoto.git pull: Este comando se utiliza para actualizar el repositorio\nlocal con los cambios realizados en el repositorio remoto. Es útil\ncuando se está trabajando en un proyecto colaborativo y otros\ncolaboradores han realizado cambios en el repositorio remoto.git branch: Este comando se utiliza para crear, listar y\neliminar ramas en el repositorio de Git. Las ramas son una forma de\ntrabajar en diferentes versiones del proyecto sin afectar la rama\nprincipal.git branch: Este comando se utiliza para crear, listar y\neliminar ramas en el repositorio de Git. Las ramas son una forma de\ntrabajar en diferentes versiones del proyecto sin afectar la rama\nprincipal.git merge: Este comando se utiliza para fusionar ramas\ndiferentes del repositorio de Git. Esto se utiliza comúnmente cuando\nse trabaja en diferentes ramas y se desea integrar los cambios\nrealizados en una rama en la rama principal.git merge: Este comando se utiliza para fusionar ramas\ndiferentes del repositorio de Git. Esto se utiliza comúnmente cuando\nse trabaja en diferentes ramas y se desea integrar los cambios\nrealizados en una rama en la rama principal.git status: Este comando se utiliza para verificar el estado del\nrepositorio de Git. Proporciona información sobre los archivos que\nse han modificado y los archivos que se han agregado al área de\npreparación.git status: Este comando se utiliza para verificar el estado del\nrepositorio de Git. Proporciona información sobre los archivos que\nse han modificado y los archivos que se han agregado al área de\npreparación.git log: Este comando se utiliza para ver un registro detallado\nde los cambios confirmados en el repositorio de Git. Muestra\ninformación como el autor del cambio, la fecha y la descripción del\ncambio.git log: Este comando se utiliza para ver un registro detallado\nde los cambios confirmados en el repositorio de Git. Muestra\ninformación como el autor del cambio, la fecha y la descripción del\ncambio.Para conocer más fondo la mecánica de Git es muy recomendable el\nmanual (Chacon 2009) o la hoja resumen proporcionada por GitHub,\ndisponible en\nhttps://training.github.com/downloads/es_ES/github-git-cheat-sheet.pdf.","code":""},{"path":"github.html","id":"resumen-36","chapter":"Capítulo 46 Git y GitHub R","heading":"Resumen","text":"Git es un sistema de control de versiones distribuido utilizado para rastrear cambios en archivos lo largo del tiempo, mientras que GitHub es una plataforma de alojamiento de código que utiliza Git como su sistema de control de versiones subyacente.Git es un sistema de control de versiones distribuido utilizado para rastrear cambios en archivos lo largo del tiempo, mientras que GitHub es una plataforma de alojamiento de código que utiliza Git como su sistema de control de versiones subyacente.La instalación y configuración de Git y GitHub es sencilla y permite una colaboración eficiente y un control de versiones en el desarrollo de software.La instalación y configuración de Git y GitHub es sencilla y permite una colaboración eficiente y un control de versiones en el desarrollo de software.Conectar GitHub y RStudio implica configurar las credenciales de Git, hacer cambios en los archivos y enviar los cambios al repositorio de GitHub.Conectar GitHub y RStudio implica configurar las credenciales de Git, hacer cambios en los archivos y enviar los cambios al repositorio de GitHub.El flujo de trabajo general en Git y GitHub implica inicializar un repositorio local, agregar archivos, comprometer cambios, crear una nueva rama si es necesario, enviar cambios al repositorio remoto, solicitar una solicitud de extracción si se trabaja en colaboración y fusionar cambios en la rama principal.El flujo de trabajo general en Git y GitHub implica inicializar un repositorio local, agregar archivos, comprometer cambios, crear una nueva rama si es necesario, enviar cambios al repositorio remoto, solicitar una solicitud de extracción si se trabaja en colaboración y fusionar cambios en la rama principal.","code":""},{"path":"geoproces.html","id":"geoproces","chapter":"Capítulo 47 Geoprocesamiento en nube","heading":"Capítulo 47 Geoprocesamiento en nube","text":"Dominic RoyéFundación de la Investigación del Clima","code":""},{"path":"geoproces.html","id":"introducción-23","chapter":"Capítulo 47 Geoprocesamiento en nube","heading":"47.1 Introducción","text":"Cuando se plantea un problema basado en datos desde diversos proveedores, habitualmente implica la descarga de grandes volúmenes. La actual proliferación de servicios de Open Data, despliegues de sensores y diversas fuentes incluyendo los satélites dificulta su procesamiento en equipos personales. El gran crecimiento en grandes volúmenes de datos espacio-temporales de tipo vectorial o ráster lleva la necesidad en trabajar con servicios en nube para ahorrar tiempo computacional y espacio de almacenamiento. En la actualidad existen diferentes servicios de geoprocesamiento en nube que ayudan hacer análisis online sin necesidad de descargar los datos ni preocuparse por el rendimiento computacional. Uno de estos servicios es Google Earth Engine (GEE), donde se combina un catálogo de varios petabytes de imágenes satelitales y conjuntos de datos geoespaciales multidimensionales (vectorial y ráster) de alta resolución con capacidades de análisis escala planetaria. Este servicio gratuito para uso comercial incluye incluso la posibilidad en crear aplicaciones.GEE consiste en una API con bibliotecas de cliente para JavaScript y Python, que traducen los análisis geoespaciales y hacen posible acceder los datos. es necesario descargar grandes volúmenes de datos ni configurar la computación. Para el lenguaje de R se puede hacer uso del paquete rgee que hace puente entre R y la API GEE.Se hará uso del dataset con el nombre “NOAA CDR OISST v02r01”, una interpolación óptima diaria de temperatura de la superficie del mar (OISST, por sus siglas en inglés) con una resolución de 1/4 grados (27 km). Los datos los proporciona la National Oceanic Atmospheric Administration (NOAA) con campos completos de temperatura del océano construidos mediante la combinación de observaciones ajustadas por sesgo de diferentes plataformas (satélites, barcos, boyas) en una cuadrícula global regular, con lagunas estimadas por interpolación (https://developers.google.com/earth-engine/datasets/catalog/NOAA_CDR_OISST_V2_1) (R. W. Reynolds, Banzon, Program (2008)).El objetivo de este capítulo es mostrar el potential del uso de APIs directamente en R. El resultado se empleará en el Cap. \\(\\ref{cambioclimatico}\\).","code":""},{"path":"geoproces.html","id":"sintaxis-de-google-earth-engine","chapter":"Capítulo 47 Geoprocesamiento en nube","heading":"47.2 Sintaxis de Google Earth Engine","text":"Con ayuda de GEE se preprocesan los datos de tal manera que el resultado son las anomalías estivales en forma de ráster para cada año entre 1981 y 2022. El primer paso consiste en crear el usuario en earthengine.google.com. Además, es necesario instalar CLI de gcloud (https://cloud.google.com/sdk/docs/install?hl=es-419).Antes se deben conocer algunos conceptos fundamentales sobre la sintaxis en GEE. En general, el lenguaje nativo es Javascript el que se caracteriza por la forma combinando funciones y variables usando el punto, el que se sustuye por el $ en R. Todas las funciones GEE en R empiezan por el prefijo ee_* (ee_print(), ee_image_to_drive()). Los términos más relevantes son los siguientes:ImageCollection: serie temporal de imágenes.Geometry: dato vectorial.Functions: map() aplica funciones sobre ImageCollections, ee.Date() define una fecha, filterDate() permite filtrar por fecha una ImageCollection, select() selecciona una banda, etc.Muchas funciones son similares las de tidyverse.Se puede obtener más ayuda en https://r-spatial.github.io/rgee/reference/rgee-package.html y en la propia página de GEE.","code":""},{"path":"geoproces.html","id":"primeros-pasos","chapter":"Capítulo 47 Geoprocesamiento en nube","heading":"47.3 Primeros pasos","text":"Después de darse de alta en GEE y haber instalado CLI gcloud en el sistema operativo, se crea un entorno virtual de Python con todas las dependencias de GEE usando la función ee_install().Antes de pasar programar con la sintaxis propia de GEE, se debe autenticar e inicializar GEE empleando la función ee_Initialize().Hay que tener en cuenta que, únicamente cuando se envían tareas, GEE ejecuta el cálculo en los servidores enviando todos los objetos creados. En la mayoría de los pasos se crean objetos EarthEngine, que se usan una vez que se construyó un mapa interactivo, la exportación o la impresión en consola de un objeto.Por ejemplo, se puede selecionar la banda NDVI del producto MODIS MOD13A2 e imprimir los metadatos del primer día disponible con ee_print(). Existe un límite 5000 elementos que se podrían ver usando esta función.","code":"\nlibrary(rgee)\n\nee_install() # crear entorno virtual de Python; ¡sólo una vez!\nee_check() # comprobar si todo está correcto\nee_Initialize(drive = TRUE) # autenticar e inicializar GEE\nee_user_info() # inf sobre usuario\n# imageCollection NDVI\nimg <- ee$ImageCollection('MODIS/006/MOD13A2')$select('NDVI')\n\n# metadatos\nee_print(img$first())\n  "},{"path":"geoproces.html","id":"cálculo-de-anomalias","chapter":"Capítulo 47 Geoprocesamiento en nube","heading":"47.4 Cálculo de anomalias","text":"","code":""},{"path":"geoproces.html","id":"definiciones-previas","chapter":"Capítulo 47 Geoprocesamiento en nube","heading":"47.4.1 Definiciones previas","text":"Los datos NOAA CDR OISST contienen la temperatura superficial de los océanos nivel global, por eso, se fija un rectángulo que cubre la extensión del Mar Mediterráneo como objeto de estudio.En el siguiente paso se define el período de interés, desde el año 1982 hasta el 2022.Se puede acceder todas las colecciones (ImageCollection) indicando su identifcación. Además, se filtran y se recortan los datos con respecto al periodo y la extensión fijada. Finalmente se selecciona la banda o variable de de interés “sst” (surface sea temperature).Finalmente, se procede calcular el número de años en el período fijado.","code":"\n# extensión del Mar Mediterráneo\ngeom <- ee$Geometry$Polygon(coords = list(\n    c(-6.046418548121442, 46.733937391710846), \n    c(-6.046418548121442, 29.680544334046786),\n    c(42.469206451878556, 29.680544334046786),\n    c(42.469206451878556, 46.733937391710846)\n    ),\n    proj = \"EPSG:4326\",\n    geodesic = FALSE)\n\ngeom #vemos que es un objeto EarthEngine de tipo geometría\nstr(geom) # construcción javascript\nstartDate <- ee$Date('1982-01-01') # fecha inicio\nendDate <- ee$Date('2023-01-16') # fecha final\ncollection_era5 <- ee$ImageCollection(\"NOAA/CDR/OISST/V2_1\")$\n                    filterDate(startDate, endDate)$\n                      filterBounds(geom)$\n                       select('sst')\nnumberOfyears <- endDate$difference(startDate, 'years')$round()"},{"path":"geoproces.html","id":"promedio-estival","chapter":"Capítulo 47 Geoprocesamiento en nube","heading":"47.4.2 Promedio estival","text":"Después de las anteriores definiciones se crea una nueva colección con el promedio estival de cada año durante el periodo objeto de estudio. Para ello se crea una lista de los años sobre la que se mapea otra función. Esta función se debe pasar con ee_utilspyfunc(), que traduce una función R una de Python.En la función personalizada se filtran los meses de verano, se calcula el promedio y se multiplica por 0,01, un factor de escala. Cuando se crean nuevas colecciones es importante fijar la nueva fecha con set().En el siguiente paso se calcula la temperatura media estival entre 1982 y 2010, como período de referencia para las anomalías.Se aplica otra función personalizada sobre las medias estivales de todos los años, en la que se resta la temperatura del periodo de referencia, obteniéndose así las diferencias entre la temperatura media de cada año en el periodo estival y la temperatura media global del periodo estival en el periodo 1982-2010.Es puede crear un mapa interactivo de un año concreto aplicando la función Map.addLayer() (Fig. 47.1). En este paso es la primera vez que GEE calcula lo que se ha creado anteriormente.\nFigura 47.1: Mapa interactivo del mar Mediterraneo (1982)\nHasta este momento, se ha enviado una tarea para que GEE la realice. Para ello hay que exportar las anomalías de cada año en formato geotiff. La función ee_imagecollection_to_local() facilita la exportación de todas las capas de una ImageCollection. En cambio, la función ee_image_to_drive() exporta datos individuales de una única imagen Google Drive. El argumento scale indica con qué resolución se exporta. Aunque la resolución de los datos originales es de 27 km, se fija una resolución de 2 km, lo que implica una interpolación en la exportación por razones de estética en la visualización.Este ejemplo ha mostrado una pequeña parte de la capacidad del geoprocesamiento en manejar grandes volúmenes de datos sin que implique su descarga ni el cálculo localmente. Pero también es posible manejar datos vectoriales u otros datos de alta resolución. Incluso se puede llegar crear apliaciones basadas en GEE.","code":"\nyearly <- ee$ImageCollection(\n  ee$List$sequence(0, numberOfyears$subtract(1L))$ \n  map(ee_utils_pyfunc(function(dayOffset) {\n    yr = startDate$advance(dayOffset, 'years')$get('year')\n    start = ee$Date$fromYMD(yr, 12L, 1L)\n    end = ee$Date$fromYMD(yr$add(1L), 2L, 28L)\n    return(collection_era5$\n    filterDate(start, end)$\n    mean()$\n    multiply(0.01)$\n    set('system:time_start', start$millis()))\n  }))\n)\nmsst <- collection_era5$filterDate('1982-01-01','2010-12-31')$\n         filter(ee$Filter$calendarRange(12L,2L,'month'))$\n          mean()$\n            multiply(0.01)\nanom <- yearly$map(ee_utils_pyfunc(function (im) {\n  return(im$subtract(msst)$set('system:time_start',           \n                         im$get('system:time_start')))\n}))\n# metadatos \nee_print(anom$first()) # año 1982\n\n# mapa interactiva del año 1982\nMap$setCenter(9, 40, 5) # centrar mapa en el mediterráneo con nivel de zoom 5\n\n# crear mapa con leyenda\nMap$addLayer(\n  eeObject = anom$first(),\n  visParams = list(\n    palette = rev(RColorBrewer::brewer.pal(11, \"RdBu\")),\n    min = -3,\n    max = 3\n  ),\n  name = \"MED_SST\"\n) + \nMap$addLegend(list(min = -3, max = 3, \n                    palette = rev(RColorBrewer::brewer.pal(11, \"RdBu\"))), \n              name = \"SST Anomaly\", \n              position = \"bottomright\", \n              bins = 4)\ntmp <- tempdir() # carpeta temporal o cualquier otra ruta\n\nic_drive_files_2 <- ee_imagecollection_to_local(\n  ic = anom,\n  region = geom,\n  fileFormat = \"GEO_TIFF\",\n  scale = 2000,\n  lazy = FALSE,\n  dsn = file.path(tmp, \"rast_\"), # prefijo del archivo\n  add_metadata = TRUE\n)"},{"path":"geoproces.html","id":"resumen-37","chapter":"Capítulo 47 Geoprocesamiento en nube","heading":"Resumen","text":"El crecimiento de volúmenes de datos en la actualidad lleva la necesidad de emplear servicios de geoprocesamiento en nube que ayuden hacer análisis online sin necesidad de descargar los datos ni preocuparse por el rendimiento computacional. Uno de estos servicios es Google Earth Engine, donde se combina un catálogo de imágenes satelitales y conjuntos de datos geoespaciales multidimensionales de alta resolución con capacidades de análisis escala planetaria. En este ejemplo se ha mostrado cómo procesar la temperatura superficial del mar calculando las anomalías estivales de la cuenca mediterránea desde 1982 2022.","code":""},{"path":"cap-crimen.html","id":"cap-crimen","chapter":"Capítulo 48 Análisis de una red criminal","heading":"Capítulo 48 Análisis de una red criminal","text":"F. Liberatore\\(^{}\\), L. Quijano-Sánchez\\(^{b}\\), M. Camacho-Collados\\(^{c}\\)\\(^{}\\)Cardiff University, \\(^{b}\\)Universidad Autónoma de Madrid, \\(^{c}\\)Ministerio del Interior","code":""},{"path":"cap-crimen.html","id":"introducción-24","chapter":"Capítulo 48 Análisis de una red criminal","heading":"48.1 Introducción","text":"En este capítulo se plantea la idea de realizar un análisis de una red de crimen organizado. Para ello, se estudia la red derivada de un dataset real, relativo la operación Oversize. El estudio se llevará cabo usando la librería igraph.","code":""},{"path":"cap-crimen.html","id":"el-conjunto-de-datos-oversize","chapter":"Capítulo 48 Análisis de una red criminal","heading":"48.2 El conjunto de datos Oversize","text":"Los datos que se van analizar se han obtenido de la operación Oversize (Berlusconi et al. 2016) (Tribunale di Milano, Ufficio del giudice per le indagini preliminari 2006) (Tribunale di Lecco, 2a Sezione Penale 2009), un proceso italiano contra un grupo mafioso. La investigación duró del 2000 al 2006, y se enfocó en más de 50 sospechosos involucrados en tráfico internacional de drogas, homicidios y robos. El juicio empezó en el 2007 y duró hasta el 2009, cuando se dictó la sentencia y los principales sospechosos fueron condenados con penas de 5 22 años de cárcel. La mayoría de los sospechosos eran afiliados de la ’Ndrangheta, una mafia de Calabria (una región del sur de Italia) con ramificaciones en otras regiones y en el extranjero.En particular, se va estudiar la red obtenida de las escuchas telefónicas. Los datos hacen referencia todas las conversaciones telefónicas transcritas por la policía y consideradas relevantes. En esta red, los nodos representan sospechosos (los datos son anónimos y los nombres asignados en la red se han generado de forma aleatoria). Las aristas conectan los sospechosos que han tenido al menos una conversación telefónica relevante al caso durante la investigación.","code":""},{"path":"cap-crimen.html","id":"creación-de-la-red-mafiosa","chapter":"Capítulo 48 Análisis de una red criminal","heading":"48.3 Creación de la red mafiosa","text":"El dataset Oversize_nodes contiene el listado de nodos con sus propiedades, en este caso el nombre (ficticio) del sospechoso. Oversize_edges contiene las aristas del grafo, representadas como parejas de nodos, su vez identificados por su ID. partir de estos datasets la librería igraph permite crear un grafo, tal y como se ilustra continuación.La vista previa del grafo indica lo siguiente:El grafo es dirigido (UN) y está compuesto por 182 nodos y 247 aristas.El grafo es dirigido (UN) y está compuesto por 182 nodos y 247 aristas.El único atributo es el nombre de los nodos (attr: name (v/c)).El único atributo es el nombre de los nodos (attr: name (v/c)).Finalmente, se proporciona una previsualización de un subconjunto de aristas, indicando para cada una los dos nodos conectados (ej. Casto Ben --Gustavo Mango).Finalmente, se proporciona una previsualización de un subconjunto de aristas, indicando para cada una los dos nodos conectados (ej. Casto Ben --Gustavo Mango).","code":"\nlibrary('igraph')\nlibrary('CDR')\ndata(oversize_edges, oversize_nodes)\nnet <- graph_from_data_frame(d=oversize_edges, \n                             vertices=oversize_nodes, \n                             directed=F) \nnet\n#> IGRAPH 3f31a61 UN-- 182 247 -- \n#> + attr: name (v/c)\n#> + edges from 3f31a61 (vertex names):\n#>  [1] Casto Ben          --Gustavo Mango          \n#>  [2] Casto Ben          --Metrofane Abbatiello   \n#>  [3] Uranio Natoli      --Fidenziano Marcellino  \n#>  [4] Lancilotto Di Biasi--Romolo Gemignani       \n#>  [5] Lancilotto Di Biasi--Fidenziano Marcellino  \n#>  [6] Senesio Rabito     --Pacifico Caliri        \n#>  [7] Senesio Rabito     --Michelangelo Piccininni\n#>  [8] Romolo Gemignani   --Alighiero Mazzarella   \n#> + ... omitted several edges"},{"path":"cap-crimen.html","id":"visualización-de-la-red-mafiosa","chapter":"Capítulo 48 Análisis de una red criminal","heading":"48.4 Visualización de la red mafiosa","text":"Para hacerse una idea de que aspecto tiene el grafo, se procede su visualización, usando el comando plot() de R.Como se puede apreciar, el resultado es muy claro. Todos los nodos tienen el mismo tamaño y se solapan entre ellos. Además, se muestran los nombres de todos los actores dentro de la red, lo cual dificulta ulteriormente su interpretación.Se puede mejorar esta presentación usando unos parámetros de plot(), específicos de igraph. En particular:vertex.size: determina el tamaño de los nodos.vertex.size: determina el tamaño de los nodos.vertex.label: define el texto asociado cada nodo. Por defecto se asume que es su nombre. En el ejemplo de abajo, se excluyen los nombres de la visualización.vertex.label: define el texto asociado cada nodo. Por defecto se asume que es su nombre. En el ejemplo de abajo, se excluyen los nombres de la visualización.En la Fig. ?? se puede ver cómo el grafo permite una mejor valoración de la distribución de los actores dentro de la red. Por ejemplo, hay dos grupos pequeños (de cuatro y dos actores) completamente desconectados de la red principal.","code":"\nplot(net, asp=0)\nplot(net, vertex.size=2, vertex.label=c(''),  asp=0)"},{"path":"cap-crimen.html","id":"importancia-de-los-actores-delincuentes","chapter":"Capítulo 48 Análisis de una red criminal","heading":"48.5 Importancia de los actores (delincuentes)","text":"Las medidas de centralidad permiten asignar un valor cada actor que establece su importancia relativa los demás. Existen diversas medidas, cada una con sus características y finalidad. En este ejemplo se van usar las siguientes:Grado: número de aristas que llegan al nodo o salen de él. Cuanto más alto sea este valor, más vecinos tendrá el nodo.Grado: número de aristas que llegan al nodo o salen de él. Cuanto más alto sea este valor, más vecinos tendrá el nodo.Intermediación: cuantifica el número de veces que un nodo se encuentra en el camino más corto entre otros actores. Cuanto más alto este valor, más información pasará por el nodo.Intermediación: cuantifica el número de veces que un nodo se encuentra en el camino más corto entre otros actores. Cuanto más alto este valor, más información pasará por el nodo.continuación, se muestran los actores con los valores más altos en cada medida de centralidad.Las medidas de centralidad se pueden usar para mejorar la visualización del grafo. Primero, se filtran todos los nodos que tengan grado menor que dos, ya que representan actores muy marginales en la red. Luego, se representa el tamaño de cada nodo en función de su valor de intermediación, escalando con un tamaño máximo de cinco.Como se puede apreciar en la Fig. ??, gracias las medidas de centralidad se puede tener una mejor idea de cómo se configura la red respecto sus actores más importantes.","code":"\ndgr <-  degree(net) # Centralidad de grado\nbtwn <- betweenness(net) # Centralidad de intermediación\nhead(sort(dgr, decreasing = T))\n#>        Gustavo Mango      Pacifico Caliri Metrofane Abbatiello \n#>                   32                   31                   18 \n#>        Olindo Iacona         Arturo Gizzi      Guido Minervini \n#>                   17                   16                   16\nhead(sort(btwn, decreasing = T))\n#>        Gustavo Mango            Bino Lana      Pacifico Caliri \n#>             4602.167             4292.902             4056.435 \n#>        Olindo Iacona Metrofane Abbatiello      Donato Di Santi \n#>             3397.907             3387.931             2978.427\nvertex_filter <- dgr > 1 # deteccion actores marginales\nscaled_btwn = 0.1+ 4.9*btwn/max(btwn) # Escalado del tamaño del nodo en funcion de la intermediacion\nnet2 = induced.subgraph(net, which(vertex_filter)) # creacion subgrafo\nplot(net2, \n     vertex.size=scaled_btwn[vertex_filter], \n     vertex.label=c(''), \n     rescale=T, \n     asp = 0) # visualizacion subgrafo"},{"path":"cap-crimen.html","id":"identificación-de-comunidades-de-la-mafia","chapter":"Capítulo 48 Análisis de una red criminal","heading":"48.6 Identificación de comunidades de la mafia","text":"continuación, se procede identificar las comunidades existentes en el grafo de la operación Overdrive. igraph proporciona una gran variedad de algoritmos para la detección de comunidades en redes sociales. En el siguiente ejemplo, se usa el algoritmo Louvain (Blondel et al. 2008) que es el más popular.El algoritmo identifica distintas comunidades, cada una con su número asignado.","code":"\nlouvain_partition <- cluster_louvain(net) # Ejecucion del algoritmo Louvain\nnet$community <- louvain_partition$membership # Asignacion de las comunidades al grafo\nunique(net$community) # Visualizacion de las comunidades encontradas\n#> [1] 1 2 3 4 5 6 7 8 9"},{"path":"cap-crimen.html","id":"visualización-de-comunidades-de-la-mafia","chapter":"Capítulo 48 Análisis de una red criminal","heading":"48.7 Visualización de comunidades de la mafia","text":"Se procede visualizar las comunidades detectadas en el subgrafo, representando cada una de ellas en un color distinto. Además, para mejorar la calidad de la información representada, se resalta la importancia de cada actor representando los nodos asociados y sus nombres en tamaños proporcionales su centralidad en toda la red.Se puede mejorar aún más el aspecto del grafo. Para ello, se va experimentar con una disposición diferente de los nodos. En este ejemplo, se usa el algoritmo Fruchterman-Reingold (Fruchterman Reingold 1991). Además, se aplica un efecto de curvatura las aristas asignando un valor positivo al parámetro edge.curved. El resultado se puede ver en la Fig. ??.Finalmente, se puede exportar el grafo como PDF usando la función pdf() de R.Como se ha podido observar tras las acciones anteriores, en la red se aprecian siete distintas comunicades. Tres destacan por su importancia, lideradas por Gustavo Mango, Bino Lana y Pacifico Caliri. Bino Lana, en particular, tiene especial relevancia ya que actúa como un puente entre Gustavo Mango y Pacificio Caliri.","code":"\nV(net2)$size <- scaled_btwn[vertex_filter] # Tamaño del nodo en funcion de su centralidad\nV(net2)$frame.color <- \"grey\"\nV(net2)$color <- net$community[vertex_filter] # Color del nodo en funcion de su comunidad\nV(net2)$label <- V(net2)$name\nV(net2)$label.cex <- (1+scaled_btwn[vertex_filter])/6 # Escalado del nombre en funcion de su centralidad\nV(net2)$label.color <- 'black'\n\n# Definicion del color de las aristas en funcion de la comunidad de origen\nedge.start <- ends(net2, es = E(net2), names = F)[,1] \nE(net2)$color <- V(net2)$color[edge.start]\n\nplot(net2, asp=0) # Los resultados puede ser distintos con cada ejecucion\nl1 <- layout_with_fr(net2) # algoritmo Fruchterman-Reingold\nplot(net2, \n     layout=l1, \n     asp = 0, \n     edge.curved=0.5) # Los resultados pueden ser distintos con cada ejecucion\npdf('grafo_final.pdf')\nplot(net2, layout=l1, asp = 0, edge.curved=0.5) # Los resultados puede ser distintos con cada ejecucion\ndev.off()\n#> agg_png \n#>       2"},{"path":"cap-publicidad.html","id":"cap-publicidad","chapter":"Capítulo 49 Optimización de inversiones publicitarias","heading":"Capítulo 49 Optimización de inversiones publicitarias","text":"Carlos Real UgenaDeloitte","code":""},{"path":"cap-publicidad.html","id":"metodologías-para-optimizar-las-inversiones-publicitarias","chapter":"Capítulo 49 Optimización de inversiones publicitarias","heading":"49.1 Metodologías para optimizar las inversiones publicitarias","text":"Uno de los principales retos los que se enfrentan los departamentos de marketing de cualquier compañía es cuantificar el impacto de la publicidad en el negocio. Esta medición es clave en la optimización de las inversiones que destinan cada medio publicitario, existiendo múltiples métodos para medir el ROI (Return Investment)}, es decir, el retorno que se obtiene por cada euro invertido en publicidad. Antes de revisar un ejemplo práctico, es necesario entender bien las características que presentan cada uno de ellos. Los métodos de cuantificación del impacto de la publicidad en el negocio, según la investigación llevada cabo por la consultora Gartner240, se pueden clasificar en cuatro grandes grupos:Marketing Mix Modeling (MMM): modelos de series temporales que sirven para estimar la contribución del marketing u otras variables explicativas las ventas desde un punto de vista estratégico. Marketing Mix Modeling (MMM): modelos de series temporales que sirven para estimar la contribución del marketing u otras variables explicativas las ventas desde un punto de vista estratégico. Multitouch Attribution (MTA): modelos de atribución que valoran cada punto de contacto (touchpoint) del recorrido del cliente (customer journey), asignando cada uno un peso en la conversión (venta, descarga de folleto, etc). Son modelos tácticos que normalmente se centran en el canal online.Multitouch Attribution (MTA): modelos de atribución que valoran cada punto de contacto (touchpoint) del recorrido del cliente (customer journey), asignando cada uno un peso en la conversión (venta, descarga de folleto, etc). Son modelos tácticos que normalmente se centran en el canal online.Holdout Testing o Experiments (EXP): modelos causales que evalúan el impacto de una campaña publicitaria partir de una muestra de control y otra de test.Holdout Testing o Experiments (EXP): modelos causales que evalúan el impacto de una campaña publicitaria partir de una muestra de control y otra de test.Unified Measurement Approaches (UMA): combinación de los modelos anteriores (MMM+MTA+EXP) con el objetivo de tener una visión unificada de los resultados.Unified Measurement Approaches (UMA): combinación de los modelos anteriores (MMM+MTA+EXP) con el objetivo de tener una visión unificada de los resultados.En la comparativa (Fig. 49.1) que se muestra continuación se resume el objetivo de cada uno de los modelos, así como las preguntas que permiten responder:\nFigura 49.1: Comparativa de metodologías de medición\nEn los últimos años se han producido una serie de cambios en el entorno de la industria del marketing digital enfocados garantizar el más estricto control de la privacidad de los usuarios. Desde el lanzamiento de la nueva ley de protección de datos “GDPR” en 2018, hasta la reciente confirmación por parte de Google de la prohibición de uso de cookies de tercera parte en “Chrome”, la posibilidad de acceder datos de identificación personal para la medición y optimización del impacto de las campañas de publicidad digital está cada vez más limitada.Esta situación está provocando que el primer tipo de enfoque, el MMM, esté siendo el gran beneficiado, dado que es una técnica que depende del acceso datos nivel de individuo. El ejemplo más claro sobre el protagonismo que está alcanzando el MMM es que grandes compañías como Meta, Google y Uber están desarrollando soluciones open-source basadas en técnicas de Machine Learning que integran grandes avances y mejoras sobre las metodologías tradicionales. Entre las metodologías que han desarrollado, hay claras diferencias en las bases teóricas sobre las que se rigen, así como diferencias en tiempos de computación, lenguaje en el que se han desarrollado o capacidades funcionales. Se detalla continuación las principales características de cada una de ellas:• Robyn241: desarrollado por Meta, Robyn (Facebook Marketing Science (2021)) está pensado para datasets con gran cantidad de variables independientes dado que trabaja con regresiones ridge (cubiertas en el Cap. 19), las cuales están pensadas para lidiar con problemas de multicolinealidad, muy presentes en este tipo de análisis. Cabe destacar que, entre las pruebas que realiza, ofrece una serie de outputs visuales avanzados que permiten al usuario seleccionar el que mejor se adapta al contexto de negocio y necesidad. Requiere tiempos de cómputo alto, pudiendo llegar hasta las tres horas, y permite hacer optimizaciones de presupuesto.• Lightweight242: desarrollado por Google, sus fundamentos teóricos se basan en modelos bayesianos. La particularidad de esta solución es la posibilidad de incluir datos geográficos para su posterior segregación. Lightweight también considera distintos tipos de adstock (recuerdo publicitario), así como la tendencia y estacionalidad de la serie explicar. El uso de esta herramienta es más sencillo, aunque los resultados son expuestos en un notebook y posee outputs más elaborados como el resto de soluciones. Por último, también permite realizar optimizaciones de presupuesto.• Orbit243: desarrollado por Uber, se basa en modelos bayesianos. Permite al usuario medir los retornos lo largo del tiempo, lo cual hace que sea un modelo adecuado para compañías con grandes picos de ventas. Incluye análisis de estacionalidad mediante la descomposición de la serie en series de Fourier. Es el modelo más complejo de desarrollar, sin embargo, el tiempo de ejecución es bajo. Cabe destacar que es la librería más estable y, por lo tanto, se podría utilizar para realizar reportes con mayor frecuencia. incluye la posibilidad de optimizar presupuestos.","code":""},{"path":"cap-publicidad.html","id":"robyn-como-alternativa-open-source-en-r","chapter":"Capítulo 49 Optimización de inversiones publicitarias","heading":"49.2 Robyn como alternativa open-source en R","text":"Robyn está ganando en popularidad debido las mejoras continuas que están aplicando desde Meta, así como gracias su adaptabilidad la realidad del anunciante. Además, existe un paquete en R validado y subido al CRAN que lo sitúa como una gran alternativa de código abierto en R para iniciarse en el campo de la medición y optimización del retorno de las inversiones publicitarias. La fórmula empleada es la siguiente:\n\\[\\begin{equation}\ny_t= \\beta_0 + SCurve(x_j) + \\beta_{hol} hol_t + \\beta_{sea} sea_t + \\beta_{trend} trend_t + ... + \\beta_{ETC} ETC_t + \\epsilon_t\n\\end{equation}\\]\ndonde:\\[\\begin{equation}\n\\begin{aligned}\ny_t &= \\text{ventas en el instante} \\,t \\\\\nt &= \\text{instante de las variables (por ejemplo, semanas)} \\\\\nj &= \\text{subíndice del medio (por ejemplo, TV o Display)}  \\\\\n\\beta_0 &= \\text{intercepto} \\\\\nx_{decay_{t,j}} &= x_{t,j} + \\theta_j x_{decay_{ t,j-1}} \\text{(adstock, es decir, el recuerdo publicitario)} \\\\\nx_j &= \\text{inversión publicitaria en cada medio}\\,j \\\\\n\\theta_j &= \\text{tasa fija de decrecimiento en cada medio}\\,j \\\\\nS Curve(x,j) &= \\beta_j \\times  \\frac{x_{decay_{ t,j}}^\\alpha}{x_{decay_{ t,j}}^\\alpha + \\gamma^\\alpha} \\, \\text{transformación lineal de curva en S} \\\\\n\\alpha,\\gamma &= \\text{hiperparámetros que definen la S-Curve} \\\\\n\\gamma &: \\text{implementada en la SCurve, donde} \\,\\gamma_{tran} = cuantil(x_{decay_j}, \\gamma) \\\\\n\\beta_j &= \\text{coeficientes de cada medio}\\,j \\\\\nhol &=\\, \\text{festivos} \\\\\nsea &=\\, \\text{estacionalidad} \\\\\ntren d&=\\, \\text{tendencia} \\\\\nETC &=\\, \\text{resto de variables independientes (precio, promociones, etc)} \\\\\n\\epsilon_t &= \\text{término de error en el instante} \\,t \\\\\n\\end{aligned}\n\\end{equation}\\]Las variables de medios suelen presentar un efecto lineal sobre la variable de negocio que se modeliza, sino que la publicidad tiende presentar rendimientos decrecientes lineales. Para modelizar este efecto se utiliza la función biparamétrica de S Curve o curva de rendimientos decrecientes. Esta curva permite optimizar los repartos presupuestarios en todos los canales de medios, ya que su forma en “S” indica tanto el umbral partir del cual los resultados del gasto presupuestario mejoran significativamente el objetivo (por ejemplo, ventas), como en qué punto está saturando y, por lo tanto, perdiendo eficacia.continuación, se aplica Robyn sobre un ejemplo con información simulada del sector hotelero, donde el objetivo es predecir el número de reservas de un hotel en función de una serie de predictores (entre ellos, las inversiones publicitarias). Toda la información sobre cómo aplicar esta metodología se puede consultar en Github244. La versión utilizada en este ejemplo práctico es la 3.6.3, disponible en el CRAN o descargable desde Github245. Este ejercicio traza el camino más corto que se puede seguir hasta llegar los principales outputs y la interpretación de los mismos. Sin embargo, para conocer en detalle la metodología, se recomienda seguir profundizando través de la realización de pruebas adicionales más complejas.Para comenzar, se carga el paquete Robyn, comprobando que se está trabajando con la versión 3.6.4 y forzando el uso de multicore al utilizar Rstudio:Se carga una librería de Python llamada nevergrad (Facebook Research AI (2019)), necesaria en el proceso de estimación de los hiperparámetros, invocándola desde R. Hay varias opciones para llevar cabo este proceso, una de ellas es instalar primero el paquete reticulate (Ushey, Allaire, Tang (2022)) y, continuación, nevergrad vía pip:En caso de encontrar alguna dificultad al cargar nevergrad, existen distintas alternativas para su instalación que se pueden consultar en Github246.Posteriormente, se carga la tabla que recoge la información simulada del sector hotelero con la que se medirá el impacto de la publicidad:Se detalla continuación la información que contiene cada variable:semana: lunes de la semana de referencia.reservas: número de reservas que ha conseguido la cadena hotelera en cada una de las semanas bajo análisis.turismo: número de pernoctaciones.covid_mov: movilidad desde el comienzo de la pandemia.notoriedad: conocimiento espontáneo de la marca lo largo del tiempo.temperatura: temperatura media.tv_grps20 y tv_inv: métrica de impactos (GRPs) e inversión realizada en TV.resto_off_inv: resto de inversiones offline realizadas.paidsearch_imp y paidsearch_inv: métrica de impactos (impresiones) e inversión realizada en Paid Search.display_imp y display_inv: métrica de impactos (impresiones) e inversión realizada en Display.onlinevideo_imp y onlinevideo_inv: métrica de impactos (impresiones) e inversión realizada en Online Videocompetidores: inversión realizada por la competencia.eventos: recoge eventos que tienen impacto en la serie de reservas. En este caso se considera el Black Friday.El objetivo de este ejercicio es estimar el impacto de cada una de las variables detalladas sobre las reservas, poniendo especial foco en las varibles de medios —TV, Resto Medios Offline, Paid Search, Display y Online Video— para medir sus efectos y, posteriormente, optimizar sus inversiones.Con este fin, se cargan los festivos de una tabla auxiliar:Se fija dónde se guardarán los resultados:Y se define la configuración inicial del modelo:Después, se obtienen los nombres de los hiperparámetros ajustar:continuación, se definen los rangos en los que se moverán los hiperparámetros que definen la S-curve de cada medio. En este ejemplo se utilizan los mismos límites para \\(\\alpha\\) y \\(\\gamma\\), que controlan la forma de la curva y el punto de inflexión, respectivamente. Por otro lado, \\(\\theta_j\\) define el adstock, que se puede acotar teniendo en cuenta los intervalos recomendados por Meta: TV (entre 0,3 y 0,8), Resto Medios Offline (entre 0,1 y 0,4) y Digital (entre 0 y 0,3). En este caso, se deja \\(\\theta\\) libre entre 0 y 0,99 para todos los medios:Se añaden los hiperparámetros al input para ajustar los modelos:Y se ejecuta el modelo definiendo el número de iteraciones y trials. En este ejercicio, el número de iteraciones será 2000 y el de trials 10, obteniendo un total de 20000 posibles soluciones del modelo:Los mejores resultados de la modelización según el frente de Pareto (conjunto de óptimos de Pareto que minimizan el error cuadrático medio normalizado (Normalized Root Mean Square Error, NRMSE) y la descomposición de la suma cuadrática de la distancia247 (Decomposition Root Sum Squared Distance, DECOMP.RSSD)) se guardan en la carpeta seleccionada:Para finalizar, se revisan los distintos modelos obtenidos y el resumen gráfico proporcionado por Robyn. Se recomienda profundizar en la interpretación de uno de los modelos obtenidos con buen ajuste. través del one-pager (Fig. 49.2), se puede consultar toda la información relativa al impacto de la publicidad, incluyendo métricas relativas la bondad del ajuste como el coeficiente de determinación o R2 y el NRMSE en la parte superior:\nFigura 49.2: One-pager de resultados de Robyn\nLos principales outputs evaluar de los modelos se visualizan en los gráficos del one-pager:Descomposición en cascada de la repuesta por predictor (Response Decomposition Waterfall Predictor): representa el peso de cada una de las variables en el modelo. En el ejemplo, el peso de la publicidad es del 54,5% (suma de los pesos de las variables de inversiones).Descomposición en cascada de la repuesta por predictor (Response Decomposition Waterfall Predictor): representa el peso de cada una de las variables en el modelo. En el ejemplo, el peso de la publicidad es del 54,5% (suma de los pesos de las variables de inversiones).Respuesta real vs. estimada (Actual vs. Predicted Response): muestra el ajuste del modelo. En este caso, la línea del ajuste (azul) y la real (naranja) se aproximan bastante, indicando que el modelo es capaz de explicar la mayor parte de la variabilidad de la serie de reservas.Respuesta real vs. estimada (Actual vs. Predicted Response): muestra el ajuste del modelo. En este caso, la línea del ajuste (azul) y la real (naranja) se aproximan bastante, indicando que el modelo es capaz de explicar la mayor parte de la variabilidad de la serie de reservas.Cuota de gasto frente cuota de efecto con CPA total (Share Spend vs Share Effect total CPA): muestra la relación entre la cuota de inversión y la cuota de contribución generada por las inversiones publicitarias. En este ejercicio, se puede observar que tanto Online Video como Display obtienen un aporte mayor de lo esperado por su cuota de inversión, lo que hace que el Coste Por Adquisición o CPA (valores en azul) sean menores para estos medios. La TV se encuentra en el polo opuesto, con el máximo CPA, es decir, es el medio menos eficiente la hora de generar reservas.Cuota de gasto frente cuota de efecto con CPA total (Share Spend vs Share Effect total CPA): muestra la relación entre la cuota de inversión y la cuota de contribución generada por las inversiones publicitarias. En este ejercicio, se puede observar que tanto Online Video como Display obtienen un aporte mayor de lo esperado por su cuota de inversión, lo que hace que el Coste Por Adquisición o CPA (valores en azul) sean menores para estos medios. La TV se encuentra en el polo opuesto, con el máximo CPA, es decir, es el medio menos eficiente la hora de generar reservas.Curvas de respuesta y gastos medios por canal (Response Curves Mean Spends Channel): muestra la relación lineal existente entre las inversiones y las reservas. Es el input principal la hora de optimizar las inversiones.Curvas de respuesta y gastos medios por canal (Response Curves Mean Spends Channel): muestra la relación lineal existente entre las inversiones y las reservas. Es el input principal la hora de optimizar las inversiones.Adstock geométrico: Tasa constante de decrecimiento en el tiempo (Geometric Adstock: Fixed Decay Rate Time): muestra el efecto recuerdo (adstock) de cada medio publicitario. Se puede observar que los medios offline (TV y Resto Medios Offline) son aquellos que presentan un efecto recuerdo más prolongado. se debe olvidar que la configuración del modelo permite fijar el rango de valores que puede tomar el adstock en cada uno de los medios.Adstock geométrico: Tasa constante de decrecimiento en el tiempo (Geometric Adstock: Fixed Decay Rate Time): muestra el efecto recuerdo (adstock) de cada medio publicitario. Se puede observar que los medios offline (TV y Resto Medios Offline) son aquellos que presentan un efecto recuerdo más prolongado. se debe olvidar que la configuración del modelo permite fijar el rango de valores que puede tomar el adstock en cada uno de los medios.Ajustados frente residuos (Fitted vs. Residual): muestra la nube de puntos para los datos ajustados (eje x) y los residuos (eje y). Este gráfico debe mostrar que los puntos están aleatoriamente ubicados alrededor del eje horizontal.Ajustados frente residuos (Fitted vs. Residual): muestra la nube de puntos para los datos ajustados (eje x) y los residuos (eje y). Este gráfico debe mostrar que los puntos están aleatoriamente ubicados alrededor del eje horizontal.Y, ¿cómo se pueden optimizar las inversiones en medios empleando estos resultados? En primer lugar, se pueden utilizar las curvas obtenidas (parámetros guardados en el fichero all_hyperparameters.csv) para simular distintos escenarios y seleccionar el que genere un mayor número de reservas. La segunda opción es utilizar la documentación de código 248 incluida en el Step 5 para generar el reparto óptimo de inversión basado en un presupuesto pre-establecido.En este capítulo se dan los primeros pasos en la medición y optimización del ROI de las inversiones publicitarias. Se anima al lector que siga profundizando en este campo tan apasionante y complejo para basar las decisiones futuras de inversión en análisis desarrollados en R.","code":"\nlibrary(Robyn) \npackageVersion(\"Robyn\")\nSys.setenv(R_FUTURE_FORK_ENABLE = \"true\")\noptions(future.fork.enable = TRUE)\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\nvirtualenv_create(\"r-reticulate\")\n use_virtualenv(\"r-reticulate\", required = TRUE)\npy_install(\"nevergrad\", pip = TRUE)\npy_config()\nlibrary(\"CDR\")\ndata('hotel_tablonsemanal')\nhead(hotel_tablonsemanal)\ndata('festivos')\nhead(festivos)\nrobyn_object <- \"data/MyRobyn.RDS\"\nruta_outputs <- \"data\"\nhotel_tablonsemanal$eventos <- hotel_tablonsemanal$eventos |> replace_na('na')\nInputCollect <- robyn_inputs(\n    dt_input = hotel_tablonsemanal\n    ,dt_holidays = festivos\n    ,date_var = \"semana\" # tiene que tener este formato \"2020-01-01\"\n    ,dep_var = \"reservas\" # sólo una variable dependiente\n    ,dep_var_type = \"conversion\" # \"revenue\" o \"conversion\". En nuestro caso son reservas.\n    ,prophet_vars = c(\"trend\", \"season\", \"holiday\") # \"trend=tendencia\",\"season=estacionalidad\", \"weekday=dia de la semana\" & \"holiday=festivos\"\n    ,prophet_signs = c(\"default\", \"default\", \"default\")\n    ,prophet_country = \"ES\"# seleccione un pais. España en nuestro caso\n    ,context_vars = c(\"eventos\", \"turismo\", \"covid_mov\", \"notoriedad\", \"temperatura\", \"competidores\") # variables de contexto que no sean medios\n    ,context_signs = c(\"default\", \"positive\", \"negative\", \"positive\", \"default\", \"negative\")# signos fijados a priori (por ejemplo, el turismo debe tener un signo positivo puesto que estamos analizando reservas de hotel)\n    ,paid_media_spends = c(\"display_inv\",\"onlinevideo_inv\", \"paidsearch_inv\"    ,\"resto_off_inv\", \"tv_inv\") # variables de inversión\n    ,paid_media_signs = c(\"positive\", \"positive\", \"positive\", \"positive\", \"positive\")\n    ,paid_media_vars = c(\"display_imp\", \"onlinevideo_imp\"   ,   \"paidsearch_imp\"    ,\"resto_off_inv\" ,\"tv_grps20\") # variables de impacto si están disponibles. Si no están disponibles utilice el coste como en el caso de Resto_off_inversion\n    ,factor_vars = c(\"eventos\") # especifique variables que son factores. En nuestro caso, sólo la variable de Eventos\n    ,window_start = \"2018-10-01\"#fecha de inicio del modelo. En nuestro caso, octubre 2010 porque previamente no se tienen datos de reservas\n    ,window_end = \"2021-09-27\" #fecha fin del modelo\n    ,adstock = \"geometric\" # tipo de adstock. Seleccione el adstock geométrico para reducir tiempos de cómputo\n  )\nprint(InputCollect)\nhyper_names(adstock = InputCollect$adstock, all_media = InputCollect$all_media)\nhyperparameters <- list(\n  display_inv_alphas = c(0.0001, 3),display_inv_gammas = c(0.3, 1),display_inv_thetas = c(0, 0.99)\n  \n  ,onlinevideo_inv_alphas = c(0.0001, 3),onlinevideo_inv_gammas = c(0.3, 1),onlinevideo_inv_thetas = c(0, 0.99)\n  \n  ,paidsearch_inv_alphas = c(0.0001, 3),paidsearch_inv_gammas = c(0.3, 1),paidsearch_inv_thetas = c(0, 0.99)\n  \n  ,resto_off_inv_alphas = c(0.0001, 3),resto_off_inv_gammas = c(0.3, 1),resto_off_inv_thetas = c(0, 0.99)\n  \n  ,tv_inv_alphas = c(0.0001, 3),tv_inv_gammas = c(0.3, 1),tv_inv_thetas = c(0, 0.99)\n)\nInputCollect <- robyn_inputs(InputCollect = InputCollect, hyperparameters = hyperparameters)\nprint(InputCollect)\nOutputModels <- robyn_run(\n  InputCollect = InputCollect \n  , iterations = 2000\n  , trials = 10\n  , outputs = FALSE # se desactivan los outputs que se guardarán después\n)\nprint(OutputModels)\nOutputCollect <- robyn_outputs(\n  InputCollect, OutputModels\n  , pareto_fronts = 3\n  , csv_out = \"all\" \n  , clusters = TRUE\n  , plot_pareto = TRUE \n  , plot_folder = ruta_outputs #ruta para exportar los resultados\n)\nprint(OutputCollect)"},{"path":"cap-twitter.html","id":"cap-twitter","chapter":"Capítulo 50 ¿Cómo twitea Elon Musk?","heading":"Capítulo 50 ¿Cómo twitea Elon Musk?","text":"Mariluz CongostoUniversidad Carlos III de madrid","code":""},{"path":"cap-twitter.html","id":"introducción-25","chapter":"Capítulo 50 ¿Cómo twitea Elon Musk?","heading":"50.1 Introducción ","text":"El objetivo de este caso de uso es arrojar luz, de manera objetiva, sobre un fenómeno social de interés general: el uso de Twitter por parte de Elon Musk. Este multimillonario adquirió la red social el 28 de octubre de 2022 y, desde entonces, ha tomado decisiones drásticas, como reducir la plantilla y lanzar nuevos servicios de pago de manera apresurada. Su constante cambio de rumbo queda reflejado en su actividad frenética en Twitter, donde es un usuario muy activo.Este caso de estudio aborda, una vez descargados los tweets mediante las APIs de Twitter249, cómo adaptarlos mediante minería de textos (38) para su análisis y visualización. Se representa el contenido de estos mensajes mediante nubes de palabras, scatters plots y timelines. Este conjunto de gráficas ofrecerán distintas vistas de los datos que, sin duda, ayudarán comprender los cambios de comunicación desde que adquirió Twitter.","code":""},{"path":"cap-twitter.html","id":"análisis-visual-de-los-tweets-de-elon-musk","chapter":"Capítulo 50 ¿Cómo twitea Elon Musk?","heading":"50.2 Análisis visual de los tweets de Elon Musk ","text":"Las librerías que se utilizan son las siguientes:El rango de fechas elegido para delimitar temporalmente los tweets de Elon Musk va desde el 16-06-2022 hasta el 22-12-2022. Este rango es adecuado para visualizar la actividad e impacto del perfil de Musk antes y después de la adquisición de Twitter.Se cargan los datos de la librería CDR del libro.Una vez obtenidos los datos, se les puede dar forma. Los datos incluyen fechas, textos, tipos de tweets y métricas que pueden ser representados mediante gráficos. continuación, se definen unos parámetros generales todas las gráficas: la fecha de la compra de Twitter, el color de los distintos tipos de mensajes (original, quoted, reply, retweeted).","code":"\nlibrary(\"tidyverse\")    # manipulación de datos\nlibrary(\"lubridate\")    # formato de fechas\nlibrary(\"scales\")       # manejo de escalas numéricas\nlibrary(\"tidytext\")     # manipulación de textos\nlibrary(\"ggwordcloud\")  # creación de una nube de palabras\nlibrary(\"RColorBrewer\") # paleta de colores\ntweets_user <- CDR::elon_musk |>\n  # Cambiar a formato fecha\n  mutate(created_at = as.POSIXct(created_at, format = \"%Y-%m-%dT%H:%M:%S\", tz = \"UTC\" )) \n# Fecha en la que Elon Musk compró Twitter\ncompra_twitter <- as.POSIXct(\"2022-10-28\")\n\n# Se ordena la leyenda del tipo de tweet\norder_tipo_tweet <- c(\"original\", \"quoted\", \"reply\", \"retweeted\")\ntweets_user$tipo_tweet <- factor(tweets_user$tipo_tweet, levels = order_tipo_tweet)\n\n# Se define el color de los elementos de la leyenda del tipo de tweet\nmy_color <- c(\"retweeted\" = \"purple\", \"reply\" = \"blue\", \"quoted\" = \"green\", \"original\" = \"red\")"},{"path":"cap-twitter.html","id":"cuáles-son-los-temas-más-recurrentes","chapter":"Capítulo 50 ¿Cómo twitea Elon Musk?","heading":"50.2.1 ¿Cuáles son los temas más recurrentes? ","text":"Para representar los términos más frecuentes en los tweets de Elon Musk se utiliza una nube de palabras. Esta representación gráfica se crea mediante la librería ggwordcloud, que funciona en el entorno ggplot.El texto de los mensajes se encuentra en la variable full_text, la cual se limpia eliminando las URLs y los handles de los usuarios. Además, se añade una columna para distinguir los textos anteriores y posteriores la compra de Twitter.Posteriormente, se descomponen los textos en palabras independientes, se eliminan las stop words y se calcula la frecuencia de aparición de cada palabra.Para generar la nube de palabras de Elon Musk, se utiliza la función geom_text_wordcloud_area(). Esta función toma como entrada la lista de palabras y su frecuencia, y genera una comparación entre antes y después de la compra de Twitter (Fig. 50.1.La proporción del tamaño de las palabras en la nube está en función de su frecuencia y se utiliza la librería RColorBrewer para definir la paleta de colores.El resultado muestra que, antes de la compra de Twitter, Musk centraba su atención en sus empresas y en la guerra de Rusia-Ucrania. Sin embargo, tras la adquisición, su temática se relaciona con su nueva propiedad.\nFigura 50.1: Palabras más frecuentes de Elon Musk en Twitter, antes y después de su compra\n","code":"\ndata(stop_words) # Descarga las stop words de la librería tidytext\n\ncorpus_text <- tweets_user |>\n  mutate(text_plain = gsub(\"http\\\\S+\\\\s*\", \"\", full_text)) |> # Quita las URL\n  mutate(text_plain = gsub(\"RT @\\\\w+:\", \"\", text_plain)) |> # Quita los RTs\n  mutate(text_plain = gsub(\"&amp;\", \"&\", text_plain)) |> # Rectifica el &\n  mutate(text_plain = gsub(\"@\\\\w+\", \"\", text_plain)) |> # Quita las menciones\n  # Crea una columna para distinguir el periodo antes/después de la compra\n  mutate(periodo = ifelse(created_at < compra_twitter,\n    \"Antes de la compra\", \"Después de la compra\" )) |>\n  select(text_plain, periodo) |>\n  unnest_tokens(word, text_plain) |> # Convierte las frases en un conjunto de palabras\n  anti_join(stop_words) |> # Elimina las stop words\n  group_by(word, periodo) |> # Agrupa por palabras\n  summarise( freq = n(), .groups = \"drop\" ) |> # Calcula la frecuencia de cada palabra\n  ungroup() |>\n  arrange(desc(freq)) # Ordena de mayor a menor frecuencia de aparición\n\n# print (corpus_text) # Descomentar para ver el resultado final \npaleta <- brewer.pal(8, \"Dark2\")\n\nggplot() +\n  geom_text_wordcloud_area(\n    data = corpus_text |> \n      top_n(300),\n    aes(label = word, group = periodo, size = freq, color = freq),\n    angle = 0.35\n  ) +\n  scale_size_area(max_size = 24) + # tamaño de las letras según frecuencia\n  scale_color_gradientn(colors = paleta) +\n  facet_wrap(~periodo) + # desdobla gráfica\n  theme_minimal() +\n  theme (strip.text = element_text(color = \"grey50\", size = 18))"},{"path":"cap-twitter.html","id":"quiénes-son-los-usuarios-con-los-que-más-conversa","chapter":"Capítulo 50 ¿Cómo twitea Elon Musk?","heading":"50.2.2 ¿Quiénes son los usuarios con los que más conversa? ","text":"Es posible visualizar con quiénes ha conversado Elon Musk con mayor frecuencia. Para ello, se pueden utilizar las respuestas que ha dado otros usuarios en Twitter. Estas respuestas se obtienen de la variable full_text.Para identificar con quiénes ha interactuado más Musk, se extraen los handles de los comentarios y se añade una columna para distinguir las menciones antes y después de la adquisición de Twitter. continuación, se calcula la frecuencia de aparición de cada handle.Una vez que los datos han sido procesados, se utiliza la función geom_text_wordcloud_area() para generar la nube de palabras correspondiente las menciones en los tweets de Elon Musk.Para ello, se toma la lista de menciones y su frecuencia, y se utiliza la misma operación que se realizó con la nube de palabras anterior.El resultado (Fig. 50.2) muestra que algunos interlocutores se mantienen, otros pierden protagonismo y aparecen otros nuevos. Se mantienen @BillyM2k (comediante) y @WholeMarsBlog (relacionado con temas de Marte). Pierden protagonismo @teslaownersSVm, @EvaFoxU, @PPathole y @Teslarati (relacionados con Tesla). Ganan protagonismo @stillgray (influencer), @micsolana (capital riesgo) y @Jason (emprendedor).\nFigura 50.2: Usuarios con los que dialoga Elon Musk antes y después de la compra de Twitter\n","code":"\ndata(stop_words)\ncorpus_menciones <- tweets_user |>\n  # Extrae los handles de los comentarios con una expresión regular \"@\\\\w+\"\n  mutate(mentions = ifelse(tipo_tweet == \"reply\", str_extract(full_text, \"@\\\\w+\"), NA)) |>\n  # Crea una columna para distinguir el periodo antes/después de la compra\n  mutate(periodo = ifelse(created_at < compra_twitter,\n    \"Antes de la compra\", \"Después de la compra\" )) |>\n  filter(!is.na(mentions)) |>    # elimina las filas vacías\n  select(mentions, periodo) |>   # selecciona menciones y periodo\n  group_by(mentions, periodo) |>\n  summarise( freq = n(),  .groups = \"drop\" ) |> # calcula frec. de palabra\n  ungroup() |>\n  arrange(desc(freq)) # ordena de mayor a menor frec. de aparición\n\n# print (corpus_menciones) # Descomentar para ver el resultado final \npaleta <- brewer.pal(8, \"Dark2\")\nggplot() +\n  geom_text_wordcloud_area( #  dibuja la nube de palabras\n    data = corpus_menciones |> top_n(50),\n    aes(label = mentions, size = freq, color = freq), angle = 0.35\n  ) +\n  scale_size_area(max_size = 12) +\n  scale_color_gradientn(colors = paleta) +\n  facet_wrap(~periodo) +\n  theme_minimal()+\n  theme (strip.text = element_text(color = \"grey50\", size = 18))"},{"path":"cap-twitter.html","id":"cuál-es-su-rutina-de-publicación","chapter":"Capítulo 50 ¿Cómo twitea Elon Musk?","heading":"50.2.3 ¿Cuál es su rutina de publicación? ","text":"Para analizar la distribución horaria de los tweets de Elon Musk, se examina la frecuencia de publicación de tweets cada hora de cada día. Dado que la residencia declarada de Musk es Austin (Texas), se ajustará la hora de los tweets al huso horario de esta ciudad, ya que la hora proporcionada por Twitter está en GMT.Debido que los datos abarcan un período largo, desde junio hasta diciembre, se acotarán 15 días antes y después de la compra de Twitter. Es importante tener en cuenta que la fecha de creación de los tweets (created_at) se presenta en formato fecha-hora, y cada día consta de 86.400 segundos (60 segundos * 60 minutos * 24 horas).continuación, se recalcan los días de la semana que son festivos en color rojo para apreciar si hay distinta rutina.Finalmente, se representa un gráfico de dispersión (scatter plot) con las coordenadas de las horas del día (eje X) y los días seleccionados (eje Y), utilizando la función geom_point(). El tamaño del punto es proporcional al número de tweets en esa hora y día, y el color el tipo de tweet (original, reply, quoted y retweeted). Se marca una línea horizontal con la función geom_hline() en la fecha de compra de Twitter y se crea un eje X doble para que sea más fácil ver las horas debido la altura de la gráfica.La Fig. 50.3 muestra que hay una rutina clara en la publicación de tweets de Elon Musk. Esto podría deberse que viaja mucho. La mayoría de sus mensajes son comentarios y han aumentado considerablemente desde la compra de Twitter. El máximo número de tweets por hora fue 10.\nFigura 50.3: Rutina de publicación de Elon Musk. (huso horario de Texas)\n","code":"\ntweets_user_hour <- tweets_user |>\n  # Cambiamos al huso horario de Texas\n  mutate(created_at = lubridate::with_tz(created_at, \"US/Central\")) |>\n  # Filtra los tweets anteriores a la compra de de Twitter\n  filter(created_at >= (compra_twitter - (60 * 60 * 24 * 15))) |>\n  filter(created_at <= (compra_twitter + (60 * 60 * 24 * 15))) |>\n  # Creamos una nueva columna para la fecha\n  mutate(time_in_days = as.POSIXct(floor_date(created_at, \"day\"))) |>\n  # Creamos una nueva columna para la hora\n  mutate(hour_tweet = hour(created_at)) |>\n  # Agrupamos el número de tweets por tipo y hora\n  group_by(time_in_days, hour_tweet, tipo_tweet) |>\n  # Calculamos el número de tweets por día, hora y tipo\n  summarise( num_tweets = n(), .groups = \"drop\" ) |>\n  ungroup()\n\n# print (tweets_user_hour) # Descomentar para ver el resultado final \nfestivos <- tweets_user |>\n  # Cambiamos al huso horario de Texas\n  mutate(created_at = lubridate::with_tz(created_at, \"US/Central\")) |>\n  # Filtramos los tweets anteriores a la compra de de Twitter\n  filter(created_at >= (compra_twitter - (60 * 60 * 24 * 15))) |>\n  filter(created_at <= (compra_twitter + (60 * 60 * 24 * 15))) |>\n  # Creamos una columna con el tiempo en días\n  mutate(time_in_days = floor_date(created_at, \"1 day\")) |>\n  # Agrupamos por día\n  group_by(time_in_days) |>\n  # calculamos el número de tweets por día\n  summarise( num_tweets = n(), .groups = \"drop\" ) |>\n  ungroup() |>\n  # Creamos una columna con el día de la semana\n  mutate(week_day = wday(time_in_days)) |>\n  # Creamos una columna para colorear los días según sean festivos o no\n  mutate(festivo = ifelse(wday(time_in_days) == 7 |\n    (wday(time_in_days) == 1), \"red\", \"black\"))\n\n# print (festivos) # Desencomentar para ver el resultado final \nggplot() +\n  geom_point(\n    data = tweets_user_hour,\n    aes(\n      x = hour_tweet,\n      y = time_in_days, \n      size = num_tweets,\n      color = tipo_tweet\n     ),\n    alpha = 0.5\n  ) +\n  # separa las fechas antes y después de la compra\n  geom_hline(aes(yintercept = compra_twitter), linetype = 2) +\n  # define una etiqueta de tiempo por día\n  scale_y_datetime(\n    date_labels = \"%d-%b-%y(%a)\", # formato fecha (día semana abreviado)\n    date_breaks = \"1 day\", # una marca de tiempo cada día\n    expand = c(0, 0, 0.02, 0.02)\n  ) + # ajustes de márgenes\n  # Definimos una etiqueta para cada hora\n  scale_x_continuous(\n    breaks = seq(0, 23, 1), # crea un vector de 0 a 23\n    sec.axis = dup_axis() # duplica el eje X\n  ) +  \n  labs( x = \"\", y = \"\", color = \"\", size = \"N. tweets\") +\n  # ajusta las leyendas en dos filas para que no se trunquen\n  guides(color = guide_legend(nrow = 2, override.aes = list(size = 4))) +\n  theme_minimal() +\n  # indica la posición de la leyenda y el color de las fechas\n  theme(\n    panel.grid.major.x = element_line(),\n    legend.position = \"top\",\n    axis.text.y = element_text(colour = festivos$festivo)\n  )"},{"path":"cap-twitter.html","id":"cuál-es-su-timeline-de-publicación","chapter":"Capítulo 50 ¿Cómo twitea Elon Musk?","heading":"50.2.4 ¿Cuál es su timeline de publicación? ","text":"Ahora se analiza cómo se distribuyen los tweets en el tiempo por tipo de tweet. Se resaltará la fecha de compra de Twitter con una anotación para facilitar la comparación de la frecuencia anterior y posterior esta fecha.Se crea una columna con la fecha redondeada días, se agrupan los tweets por fecha y el tipo de tweet y se calcula su número para cada día.En la Fig. 50.4 se puede observar un incremento en el número de publicaciones después de la compra de Twitter. De hecho, se publicaron casi el doble de tweets en comparación con el periodo anterior la adquisición de la plataforma. Asimismo, se puede ver, al igual que en la Fig. 50.3, que la mayoría de los tweets de Elon Musk fueron comentarios.\nFigura 50.4: Publicación de Tweets por día de Elon Musk\n","code":"\ntweets_user_day <- tweets_user |>\n  # Creamos una columna con el tiempo en días\n  mutate(time_in_days = floor_date(created_at, \"1 day\")) |>\n  # Agrupamos el número de tweets por día y tipo\n  group_by(time_in_days, tipo_tweet) |>\n  # calculalos el número de tweets por día y tipo\n  summarise( num_tweets = n(),  .groups = \"drop\" ) |>\n  ungroup()\n\n# print (tweets_user_day) # descomentar para ver el resultado\nggplot(data = tweets_user_day) +\n  geom_col(aes(x = time_in_days, y = num_tweets, fill = tipo_tweet),alpha = 0.7 ) +\n  geom_vline(aes(xintercept = compra_twitter), linetype = 2) + # compra de Twitter\n  geom_label( # señala el evento\n    aes(\n      x = compra_twitter - (60 * 60 * 24 * 25),\n      y = max(num_tweets),\n      label = \"Elon Musk\\ncompra Twitter\"\n    ),\n    color = \"gray45\"\n  ) +\n  geom_curve(   # flecha con curva para señalar el evento\n    aes(\n      x = compra_twitter - (60 * 60 * 24 * 10),\n      y = max(num_tweets),\n      xend = compra_twitter,\n      yend = max(num_tweets) * 0.80\n    ),\n    arrow = arrow(length = unit(0.08, \"inch\")), linewidth = 0.5,\n    color = \"gray20\", curvature = -0.3\n  ) +\n  scale_x_datetime( # ajusta la escala de tiempo y su formato\n    date_labels = \"%d\\n%b\",\n    date_breaks = \"2 week\"\n  ) +\n  scale_y_continuous(  # ajusta el formato del eje Y\n    name = \"Num. Tweets por día\",\n    labels = label_number(scale_cut = cut_short_scale())\n  ) +\n  scale_color_manual(values = my_color) + # aplica colores definidos\n  labs( x = \"\", y = \"Num. Tweets por día\", fill = \"\") +\n  theme_minimal() +\n  theme(legend.position = \"top\")"},{"path":"cap-twitter.html","id":"cuál-es-el-impacto-de-sus-tweets","chapter":"Capítulo 50 ¿Cómo twitea Elon Musk?","heading":"50.2.5 ¿Cuál es el impacto de sus tweets? ","text":"Para comparar los tweets propios publicados (sin retweets) y el impacto que reciben (retweets recibidos), se utilizará una gráfica de doble escala. Dado que ambas variables tienen diferentes órdenes de magnitud, este tipo de gráfica permitirá una mejor comparación. Además, se incluirá una anotación con la fecha de compra de Twitter para distinguir los cambios antes y después de este evento.En esta gráfica se podrá ver cómo se van superponiendo capas de dibujo.Se preparan los datos en dos data.frames y se calcula la relación de escala:tweets_propios_day con de los tweets propios por día y los mensajes originales/hora:tweets_RT_day con los retweets recibidos por día:Se calculan las escalas:La Fig. 50.5 muestra un incremento masivo de los retweets recibidos desde la compra de Twitter, siendo el día que tomó posesión, el que generó el mayor pico: 800K RTs.\nFigura 50.5: Tweets vs. retweets de Elon Musk\n","code":"\ntweets_propios_day <- tweets_user |>\n  # Creamos una columna con el tiempo en días\n  mutate(time_in_days = floor_date(created_at, \"1 day\")) |>\n  filter(tipo_tweet != \"RT\") |>   # elimina los retweets\n  group_by(time_in_days) |>  # agrupa los tweets por día\n  summarise( num_tweets = n(), .groups = \"drop\" ) |> \n  ungroup()\n\n# print (tweets_propios_day) # Descomentar para ver el resultado\ntweets_RT_day <- tweets_user |>\n  # Creamos una columna con el tiempo en días\n  mutate(time_in_days = floor_date(created_at, \"1 day\")) |>\n  filter(tipo_tweet != \"RT\") |>\n  group_by(time_in_days, tipo_tweet) |>\n  summarise( num_tweets = sum(retweet_count), .groups = \"drop\" ) |>\n  ungroup()\n\n# print(tweets_RT_day) # descomentar para ver el resultado\n# Máximo número de tweets propios\nmax_tweets <- max(tweets_propios_day$num_tweets, na.rm = TRUE)\n# Máximo número de retweets recibidos\nmax_RT <- max(tweets_RT_day$num_tweets, na.rm = TRUE)\najuste_escala <- max_RT / max_tweets # Ajsute de escala\n# print (ajuste_escala)  # desencomentar para ver el ajuste\nmy_color <- c(\"Num. original tweets\" = \"steelblue4\", \"RTs\" = \"red4\")\nggplot() +\n  # Pinta la evolución de los tweets propios/día\n  # Pinta el área que representa los tweets propios por día\n  geom_area(\n    data = tweets_propios_day,\n    aes(x = time_in_days, y = num_tweets), fill = \"steelblue4\",\n    alpha = 0.5\n  ) +\n  # Pinta el borde del área por estética\n  geom_line( data = tweets_propios_day,\n    aes(x = time_in_days, y = num_tweets, color = \"Num. original tweets\")\n  ) +\n  # Pinta la evolución de los RTs/día\n  geom_line(\n    data = tweets_RT_day,\n    aes(x = time_in_days, y = num_tweets / ajuste_escala, color = \"RTs\")\n  ) +\n  # Marcamos la linea de la compra de Twitter por Elon Musk\n  geom_vline(aes(xintercept = compra_twitter), linetype = 2) +\n  # Anotamos el evento\n  geom_label(\n    data = tweets_propios_day,\n    aes(\n      x = compra_twitter - (60 * 60 * 24 * 25),\n      y = max(num_tweets) * .95,\n      label = \"Elon Musk\\ncompra Twitter\"),\n      color = \"grey50\"\n  ) +\n  # Dibuja una flecha con curva para señalar el evento\n  geom_curve(\n    data = tweets_propios_day,\n    aes(\n      x = compra_twitter - (60 * 60 * 24 * 10),\n      y = max(num_tweets),\n      xend = compra_twitter,\n      yend = max(num_tweets) * 0.90\n    ),\n    arrow = arrow(length = unit(0.08, \"inch\")), linewidth = 0.5,\n    color = \"gray20\", curvature = -0.3\n  ) +\n  # Ajusta la escala de tiempo y su formato\n  scale_x_datetime(\n    date_labels = \"%d\\n%b\",\n    date_breaks = \"2 week\"\n  ) +\n  # doble escala, derecha: tweets propios, izquierda: retweets\n  scale_y_continuous(\n    name = \"Num. Original tweets por día\",\n    labels = label_number(scale_cut = cut_short_scale()),\n    sec.axis = sec_axis(\n      trans = (~ . * ajuste_escala), name = \"RTs por día\",\n      labels = label_number(scale_cut = cut_short_scale())\n    )\n  ) +\n  scale_color_manual(values = my_color) +\n  labs(x = \"\",  color = \"\" ) +\n  theme_minimal(base_family = \"sans\") +\n  theme(\n    legend.position = \"top\",\n    axis.title.y = element_text(color = \"steelblue4\", size = 12),\n    axis.title.y.right = element_text(color = \"red4\", size = 12),\n    axis.text.y = element_text(color = \"steelblue4\"),\n    axis.text.y.right = element_text(color = \"red4\")\n  )"},{"path":"cap-periodismo.html","id":"cap-periodismo","chapter":"Capítulo 51 Análisis electoral: de Rstudio a su periódico","heading":"Capítulo 51 Análisis electoral: de Rstudio a su periódico","text":"Borja Andrino TurónEL PAÍS\n","code":""},{"path":"cap-periodismo.html","id":"motivación","chapter":"Capítulo 51 Análisis electoral: de Rstudio a su periódico","heading":"51.1 Motivación","text":"El uso de R en el entorno profesional ha llegado también los periódicos. Cada vez es más habitual encontrar en los medios analistas de datos que lo utilizan en su día día. En EL PAÍS, muchos de los contenidos que se publican en la Unidad de Datos surgen de un notebook de RStudio. continuación, se muestra un análisis sobre las últimas elecciones andaluzas, de RStudio su periódico favorito.","code":""},{"path":"cap-periodismo.html","id":"obtención-de-los-datos","chapter":"Capítulo 51 Análisis electoral: de Rstudio a su periódico","heading":"51.2 Obtención de los datos","text":"Los datos electorales siempre son igual de accesibles. Los de las elecciones que dependen del Ministerio del Interior se publican en el portal Infoelectoral. En el caso de las elecciones andaluzas, los resultados nivel de mesa se han publicado en los portales de cada convocatoria, aunque pueden encontrarse entre los contenidos del libro para replicar estos análisis.En primer lugar se compondrá un diccionario de municipios que se usará para filtrar y agrupar los resultados por provincia. Primero se escrapeará de la web del INE la relación de códigos de provincia con la librería rvest. Se lee el código html de la página y se buscan los elementos table con clase miTabla. continuación, se usa la función html_table para convertir las tres tablas en un objeto tibble. La información con los nombres de municipios y provincias se leerá en la web del INE.Se añade la información sobre municipios al dataset de elecciones que tiene los datos de cada sección censal.","code":"\npacman::p_load(CDR, ggplot2, dplyr, rvest, lubridate, sf, ggtext, \n               rio, janitor, here, purrr, stringr, scales)\n\nurl_provincias <- \n  \"https://www.ine.es/daco/daco42/codmun/cod_provincia.htm\"\n\ncod_provincias <- \n  read_html(url_provincias) |> \n  html_nodes(\"table.miTabla\") |> \n  html_table() |> \n  map_df(as_tibble) |> \n  rename(codigo_prov = 1, name_prov = 2) |> \n  mutate(codigo_prov = str_pad(codigo_prov, width = 2, pad = \"0\", side = \"left\"))\n\nurl_municipios <- \n  \"https://www.ine.es/daco/daco42/codmun/codmun20/20codmun.xlsx\"\n\ncod_municipios <- \n  import(url_municipios, skip = 1) |>\n  clean_names() |> \n  transmute(codigo_prov = cpro, \n            codigo_mun = str_glue(\"{codigo_prov}{cmun}\"), \n            name_mun = nombre) |> \n  left_join(cod_provincias) |> \n  select(codigo_mun, name_mun, name_prov)\ndatos_elecciones <- \n  datos_elecciones |> \n  left_join(cod_municipios) |> \n  select(codigo_secc, codigo_mun, name_mun, name_prov, convocatoria, everything())"},{"path":"cap-periodismo.html","id":"transformación-y-primeros-gráficos","chapter":"Capítulo 51 Análisis electoral: de Rstudio a su periódico","heading":"51.3 Transformación y primeros gráficos","text":"En el primer gráfico se mostrará la evolución de los votos partidos de izquierda y de derecha en toda Andalucía desde 2015. Primero se calculan los votos válidos en cada convocatoria. Como en la estructura de datos ese dato está repetido para cada combinación de convocatoria-sección-partido se usará la función distinct antes de agrupar y sumar los votos validos de todas las secciones.Ahora se calcula la suma de votos de cada bloque en cada convocatoria. En este caso, como cada fila tiene el dato de votos de un partido distinto es necesaria la función distinct.continuación, se realiza el gráfico con los daton que se han calculado antes. Se definen los colores que representan cada bloque, las fechas para poder etiquetar en el gráfico los ticks del eje x y se programa el gráfico.\nFigura 51.1: Evolución del voto en Andalucía\nReplicar la Fig. 51.1 para cada provincia es complicado. Sólo se descartarán los datos de toda Andalucía y se usará la función facet_wrap() que realizará el mismo gráfico con el mismo estilo para cada provincia.\nFigura 51.2: Evolución del voto provincial\nLa Fig. 51.2 cuenta una historia complementaria al primero. El giro se ha producido igual en toda Andalucía, es igual el de Almería que el de Sevilla. Para intentar buscar nuevas diferencias territoriales se explorarán los mapas de ganadores nivel municipal. Se procede de igual manera que con los datos de provincias, salvo que en este caso se agrega partir de la columna codigo_mun. Para calcular el ganador se agrupa por esta columna y se usa la función slice_max(), que tomará para cada municipio la fila del partido con el mayor número de votos.Para realizar el gráfico se tomará el objeto sf con los recintos de los municipios andaluces y se les añadirá los datos de ganadores calculados anteriormente con la función left_join(). Se usa el color del bloque para el relleno y el porcentaje de votos que suma el bloque ganador para la transparencia, de forma que de un vistazo se pueden encontrar feudos de uno u otro bloque.\nFigura 51.3: Resultados de las elecciones andaluzas\nEn los mapas se encuentran nuevas historias. En 2015 la derecha era fuerte en la costa de Almería y Málaga. Su presencia creció en 2018, aunque la izquierdas seguía ganando el interior de la comunidad. En 2018 el dominio del bloque de derechas se extiende por casi todo el territorio, en especial en las zonas donde ya era fuerte en 2015.","code":"\ndatos_elecciones_validos_total <- \n  datos_elecciones |> \n  distinct(convocatoria, codigo_secc, .keep_all = T) |> \n  mutate(region = \"Andalucía\") |>  \n  group_by(convocatoria, region) |> \n  summarise(validos = sum(validos), .groups = \"drop\")\n\ndatos_elecciones_validos_provs <- \n  datos_elecciones |> \n  distinct(convocatoria, codigo_secc, .keep_all = T) |> \n  mutate(region = name_prov) |>  \n  group_by(convocatoria, region) |> \n  summarise(validos = sum(validos), .groups = \"drop\")\n\ndatos_elecciones_validos <- \n  datos_elecciones_validos_total |>\n  bind_rows(datos_elecciones_validos_provs)\ndatos_bloques_total <- \n  datos_elecciones |> \n  mutate(region = \"Andalucía\") |> \n  group_by(convocatoria, region, bloque) |> \n  summarise(votos_bloque = sum(votos_partido), .groups = \"drop\")\n\ndatos_bloques_provs <- \n  datos_elecciones |> \n  mutate(region = name_prov) |> \n  group_by(convocatoria, region, bloque) |> \n  summarise(votos_bloque = sum(votos_partido), .groups = \"drop\")\n\ndatos_bloques <- \n  datos_bloques_total |>\n  bind_rows(datos_bloques_provs) |> \n  left_join(datos_elecciones_validos) |> \n  mutate(votos_bloque_pc = votos_bloque / validos)\ntitle <- \n  \"Evolución de voto a partidos de <b style='color:#457b9d;'>derecha<\/b>, <b style='color:#e63946;'>izquierda<\/b><br>y <b style='color:#676767;'>otros<\/b> desde 2015\"\n\nconvocatorias_dates <- \n  datos_bloques |> \n  distinct(convocatoria) |> \n  pull(convocatoria)\n\ndatos_bloques |> \n  filter(region == \"Andalucía\") |>\n  ggplot(aes(x = convocatoria, y = votos_bloque_pc, \n                color = bloque, group = bloque)) + \n  geom_line(size = 1) +\n  geom_point(size = 2) + \n  geom_hline(yintercept = 0, width = 0.2) + \n  scale_color_manual(values = colors_bloques) +\n  scale_x_date(breaks = convocatorias_dates, date_labels = \"%Y\") +\n  scale_y_continuous(labels = percent) + \n  labs(title = title, x = \"Convocatoria\", y = \"Votos bloque\", \n       caption = \"Fuente: Junta de Andalucía\") +\n  theme_minimal() + \n  theme(legend.position = \"none\", \n        plot.title = element_markdown(margin=margin(0,0,30,0)))\ndatos_bloques |> \n  filter(region != \"Andalucía\") |>\n  ggplot(aes(x = convocatoria, y = votos_bloque_pc, \n                color = bloque, group = bloque)) + \n  geom_line(size = 1) +\n  geom_point(size = 2) + \n  geom_hline(yintercept = 0, width = 0.2) + \n  scale_color_manual(values = colors_bloques) +\n  scale_x_date(breaks = convocatorias_dates, date_labels = \"%Y\") +\n  scale_y_continuous(labels = percent) + \n  labs(title = title, x = \"Convocatoria\", y = \"Votos bloque\", \n       caption = \"Fuente: Junta de Andalucía\") +\n  facet_wrap(~region, ncol = 4) + \n  theme_minimal() + \n  theme(legend.position = \"none\", \n        plot.title = element_markdown(margin=margin(0,0,10,0)))\ndatos_elecciones_validos_muns <- \n  datos_elecciones |> \n  distinct(convocatoria, codigo_secc, .keep_all = T) |> \n  group_by(convocatoria, codigo_mun) |> \n  summarise(validos = sum(validos), \n            .groups = \"drop\")\n\ndatos_bloques_muns <- \n  datos_elecciones |> \n  group_by(convocatoria, codigo_mun, bloque) |> \n  summarise(votos_bloque = sum(votos_partido), .groups = \"drop\") |> \n  left_join(datos_elecciones_validos_muns) |> \n  mutate(votos_bloque_pc = votos_bloque / validos, 1)\n\n# Ahora calculamos los ganadores\ndatos_winners_muns <- \n  datos_bloques_muns |>\n  group_by(convocatoria, codigo_mun) |> \n  slice_max(votos_bloque, n = 1, with_ties = F) |> \n  select(convocatoria, codigo_mun, \n         winner = bloque, votos_bloque_pc) \nmap_munis |> \n  left_join(datos_winners_muns) |> \n  mutate(convocatoria = year(convocatoria)) |> \n  ggplot() + \n  geom_sf(aes(fill = winner, alpha = votos_bloque_pc), \n          size = 0.01) + \n  scale_fill_manual(values = colors_bloques) + \n  facet_wrap(~convocatoria, ncol = 1) + \n  labs(title = title, \n       caption = \"Fuente: Junta de Andalucía\") + \n  coord_sf(label_graticule = \"\", ndiscr=0) +\n  theme_minimal() + \n  theme(legend.position = \"none\", \n        plot.title = element_markdown(margin=margin(0,0,10,0)))"},{"path":"paro-clm.html","id":"paro-clm","chapter":"Capítulo 52 Crisis: impacto en el paro de Castilla-La Mancha","heading":"Capítulo 52 Crisis: impacto en el paro de Castilla-La Mancha","text":"Isidro Hidalgo Arellano\\(^{}\\) y Ángel Jiménez Rojas\\(^{}\\)\\(^{}\\)Observatorio del Mercado de Trabajo de Castilla-La Mancha","code":""},{"path":"paro-clm.html","id":"planteamiento","chapter":"Capítulo 52 Crisis: impacto en el paro de Castilla-La Mancha","heading":"52.1 Planteamiento","text":"En los últimos 15 años el mundo ha sufrido dos grandes periodos de crisis económica: en 2008, de tipo financiero; y en 2020, causa de la pandemia de COVID-19. Uno de los parámetros socioeconómicos que se ven más afectados por este tipo de procesos es el paro registrado. El paro registrado se define como el conjunto de los demandantes inscritos en las oficinas de empleo, una vez excluidos los inscritos sin disponibilidad para trabajar y los demandantes parados, tales como estudiantes, desempleados en formación, etc. (Toharia 2012).\nCastilla-La Mancha, comunidad autónoma interior de España, ha sido ajena las crisis económicas mencionadas, por lo que en este trabajo se quiere analizar el impacto de las mismas en la estructura del paro registrado de la región. Para ello, se utilizan las siguientes variables explicativas: sexo y edad de la persona desempleada, sector de actividad económica de procedencia y tiempo de búsqueda de empleo. El conjunto de datos utilizado comprende la media anual del paro registrado en la comunidad autónoma de Castilla-La Mancha desagregado según estas variables, lo largo de los años que van desde 2007 2022.Para el análisis se usan las librerías y objetos (paletas de colores para los gráficos) siguientes:Para cargar el conjunto de datos, parados_clm, incluido en el paquete CDR, y mostrar la estructura de la tibble se usa:","code":"\nlibrary(\"CDR\")\nlibrary(\"tidyverse\")\nlibrary(\"ggpubr\")\npaleta_heatmaps <- c(rgb(.7,1,0,.5),  rgb(.13,.22,.58,1))\npaleta_lineas <- c(\"blue4\", \"orange\",\"darkgreen\")\ndata(\"parados_clm\")\nparados_clm\n# A tibble: 92,215 × 8\n# anyo   sexo    edad   sector  t_bus_e    tramo_edad  t_bus_e_agr  parados\n# <ord>  <fct>   <dbl>  <fct>   <ord>      <ord>       <ord>       <dbl>\n# 2007   hombre  16     agricu  t<=7 días  <30 años    t<=6 meses   0.66666667\n# 2018   mujer   36     sinact  t<=7 días  30-44 años  t<=6 meses   1.66666667\n# 2012   mujer   30     agricu  t<=7 días  30-44 años  t<=6 meses   5.33333333\n# 2022   mujer   49     constr  t<=7 días  >44 años    t<=6 meses   0.75000000\n# 2007   mujer   54     indust  t<=7 días  >44 años    t<=6 meses   1.50000000\n# … with 92,210 more rows"},{"path":"paro-clm.html","id":"evolución-del-paro-medio-anual-en-castilla-la-mancha","chapter":"Capítulo 52 Crisis: impacto en el paro de Castilla-La Mancha","heading":"52.2 Evolución del paro medio anual en Castilla-La Mancha","text":"Para ver el paro medio anual en función del tiempo, se construye un gráfico de evolución. Para ello, representamos el paro medio por año, marcando los años que suponen un máximo o mínimo en la serie:\nFigura 52.1: Evolución del paro medio anual en CLM\nDe un primer análisis visual de la Fig. 52.1 se toman como puntos de referencia los años previos las crisis: 2007 y 2019, y el último año, 2022. Se puede observar que, si bien la crisis de la COVID-19 ha tenido profundos efectos sectoriales, principalmente en turismo, comercio y restauración, la crisis de 2008 tuvo un impacto enorme y generalizado en toda la economía, por lo que su efecto en el paro registrado fue devastador, multiplicando por un factor mayor de 3 la cifra total de paro en la región desde 2007. Sin embargo, partir del año 2013 el paro registrado inicia una tendencia la baja muy pronunciada que aún hoy continúa, después de haber repuntado ligeramente por la crisis de la COVID-19.","code":"\nresumen <- parados_clm |>\n     group_by(anyo) |>\n     summarise(parados = sum(parados)) |>\n     mutate(anyo = as.numeric(as.character(anyo)))\nanyos <- c(2007, 2013, 2019, 2020, 2022)\nparo_anyos <- resumen |>\n     filter(anyo %in% anyos) |>\n     select(parados) |>\n     mutate(parados = round(parados, 0))\npuntos <- data.frame(anyos, paro_anyos)\n\ngraf <- ggplot(resumen, aes(anyo, parados)) +\n     geom_line(linewidth = 2, col = paleta_lineas[1], alpha = 0.5) +\n     xlab(\"\")+ ylab(\"número de parados\") +\n     geom_point(puntos, mapping = aes(x = anyos, y = parados,\n          shape = \"circle filled\", size = 1, fill = paleta_lineas[1],\n          alpha = 0.5)) +\n     theme(legend.position = \"none\",\n          axis.title = element_text(face=\"bold\", size = 10),\n          axis.text = element_text(face=\"bold\", size = 10),\n          strip.text = element_text(size = 9, face = \"bold\")) +\n     scale_y_continuous(labels = function(x) format(x, big.mark = \".\",\n          scientific = FALSE))\ngraf"},{"path":"paro-clm.html","id":"evolución-del-paro-medio-anual-en-función-de-la-edad-y-el-sexo","chapter":"Capítulo 52 Crisis: impacto en el paro de Castilla-La Mancha","heading":"52.3 Evolución del paro medio anual en función de la edad y el sexo","text":"Para ver cómo ha cambiado la estructura del paro registrado en función de la edad y el sexo de los parados se pueden utilizar diferentes gráficos. En este análisis, se usan mapas de calor y gráficos de distribución de densidad. Para hacer un mapa de calor que permita comparar dos variables simultáneamente, se construye la siguiente función:Si se lanza la función heatmap_anyos() para las variables edad y sexo, tomando como años comparativos 2007, 2019 y 2022, se obtiene:\nFigura 52.2: Paro medio anual según edad y sexo en 2007, 2019 y 2022\nEn la Fig. 52.2 se puede apreciar que en los dos procesos críticos se ha producido un desplazamiento del paro hacia los intervalos de mayor edad, siendo este cambio más pronunciado en las mujeres.\nEl mapa de calor es muy útil para una primera impresión de estos cambios, pero si se desea observar detalladamente cómo ha cambiado la distribución del paro según el sexo y la edad, es mejor programar la función densidad_compara(), que proporciona mayor nivel de detalle: produce un cuadro de gráficos comparando la distribución de la edad, para cada estrato de la variable elegida, para tres años diferentes (2007, 2019 y 2022 por defecto). Los parámetros alpha y size permiten ajustar tamaño y opacidad de las líneas, mejorando la apariencia general del gráfico.Ejecutando esta función para la variable sexo se obtiene:\nFigura 52.3: Distribución del paro medio anual por edad y sexo (2007, 2019 y 2022)\nEn la Fig. 52.3 se observa que en 2007, antes de ambas crisis, los hombres parados presentan dos máximos, en torno 25 y 60 años, mientras que las mujeres desempleadas tienen una distribución bastante centrada entre 30 y 40 años. En cambio, en 2022 se aprecia el desplazamiento de la distribución de los parados de ambos sexos hacia los estratos de edad mayores de 50 años. Este desplazamiento es algo más intenso en las mujeres.\nSe observa igualmente que comparando las distribuciones de las mujeres de 2019 y 2022, la crisis de la COVID-19 ha incrementado entre 5 y 10 años la distribución de la edad de las mujeres paradas. Este desplazamiento es inferior en los hombres, donde supone menos de 5 años.","code":"\nheatmap_anyos <- function(var1, var2, inicio = 2007, intermedio = 2019,\n                          fin = 2022){\n     tabla <- select(parados_clm, anyo, var1, var2, parados) |>\n          filter(anyo %in% c(inicio, intermedio, fin))\n     names(tabla) <- c(\"anyo\", \"var1\", \"var2\", \"parados\")\n     tabla <- tabla |>\n          group_by(anyo, var1, var2) |>\n          summarise(parados = sum(parados))\n     graf <- ggplot(tabla, aes(x = var1, y= var2, fill = parados)) +\n          geom_raster() +\n          scale_fill_gradientn(colours = paleta_heatmaps) +\n          facet_wrap(~ anyo) + \n          labs(x = \"\", y = \"\") +\n          theme(axis.text = element_text(size = 10, face = \"bold\"),\n                axis.title = element_text(size = 10, face = \"bold\"),\n                strip.text = element_text(size = 10, face = \"bold\"))\n     return(graf)}\nheatmap_anyos(\"sexo\", \"edad\")\ndensidad_compara <- function(variab, inicio = 2007, medio = 2019,\n                             fin = 2022){\n     tabla <- select(parados_clm, anyo, variab, edad, parados) |>\n          filter(anyo %in% c(inicio, medio, fin))\n     names(tabla) <- c(\"anyo\", \"variable\", \"edad\", \"parados\")\n     tabla <- tabla |>\n          group_by(anyo, edad, variable) |> \n          summarise(parados = sum(parados))# |>\n     \n     graf <- ggplot(tabla, aes(x = edad, y = parados, color = anyo,\n                    fill = anyo)) + geom_line(alpha=0.6, size = 1) +\n          facet_wrap(~ variable, ncol = dim(table(tabla$variable))[1]) +\n          ylab(\"número de parados\") + labs(color=\"año\") +\n          scale_color_manual(values = paleta_lineas) +\n          scale_y_continuous(labels = function(x) format(x,\n               big.mark = \".\",\n               scientific = FALSE)) +\n          theme(strip.text = element_text(size = 10, face = \"bold\"),\n                axis.title = element_text(size = 10, face = \"bold\"),\n                axis.text = element_text(size = 10, face = \"bold\"))\n     return(graf)}\ndensidad_compara(\"sexo\")"},{"path":"paro-clm.html","id":"evolución-del-paro-medio-anual-según-el-tiempo-de-búsqueda-de-empleo","chapter":"Capítulo 52 Crisis: impacto en el paro de Castilla-La Mancha","heading":"52.4 Evolución del paro medio anual según el tiempo de búsqueda de empleo","text":"Se define el tiempo de búsqueda de empleo como el tiempo transcurrido ininterrumpidamente desde la última inscripción de la persona en el paro registrado (Pérez Infante 2006).\nSi, para simplificar, se agregan los doce intervalos que considera la estadística de paro registrado para el tiempo de búsqueda de empleo en tan solo cuatro, ejecutando la función densidad_compara() para la variable t_bus_e_agr se obtiene:\nFigura 52.4: Distribución del paro medio anual por edad y tiempo de búsqueda de empleo\nEn la Fig. 52.4 se pone de manifiesto que el tramo con mayor incremento de número de parados es el correspondiente más de 24 meses de búsqueda de empleo (paro de muy larga duración), ya que la crisis financiera de 2008 les redujo su probabilidad de encontrar empleo. Se puede afirmar también que los dos períodos de crisis han provocado la creación de un paro estructural de larga duración.Se deja al lector ejecutar la función heatmap_anyos() para las variables sexo y t_bus_e, tomando como años comparativos 2007, 2019 y 2022. Observará en el gráfico resultante que el incremento en el paro de muy larga duración es más intenso en el colectivo de las mujeres. El código utilizar es:","code":"\ndensidad_compara(\"t_bus_e_agr\")\nheatmap_anyos(\"sexo\", \"t_bus_e\")"},{"path":"paro-clm.html","id":"evolución-del-paro-medio-anual-según-sexo-edad-y-sector-de-procedencia","chapter":"Capítulo 52 Crisis: impacto en el paro de Castilla-La Mancha","heading":"52.5 Evolución del paro medio anual según sexo, edad y sector de procedencia","text":"La variable sector de procedencia es un tanto particular, ya que, cuando un parado lleva mucho tiempo buscando empleo ininterrumpidamente, “pierde” el sector de procedencia y se clasifica automáticamente en la rúbrica “sin actividad”. la hora de analizar esta variable, por tanto, es importante tener en cuenta que una parte de los parados ubicados en la rúbrica “sin actividad”, realmente tuvieron un trabajo hace mucho tiempo.\nLa visualización de los cambios producidos en estas variables con un mapa de calor, se puede llevar cabo ejecutando de nuevo la función heatmap_anyos() obteniendo la Fig. 52.5:\nFigura 52.5: Paro medio anual según sexo y sector de procedencia\nEn la Fig. 52.5 se aprecia el incremento del paro registrado en el sector servicios, especialmente en el colectivo femenino.\nEjecutando la función densidad_compara() para la variable sector se obtiene:\nFigura 52.6: Distribución del paro medio anual por edad y tiempo de búsqueda de empleo\nComo se observa en la Fig. 52.6, las diferencias lo largo del tiempo del número de parados por sector de actividad económica revelan algunas particularidades interesantes. Industria y construcción se comportan de modo similar: hay un fuerte desplazamiento en edad desde 2007, pero se mantiene el volumen de paro en ambos sectores lo largo de los 15 años de estudio. El paro en el sector agropecuario y en el sector servicios también presenta desplazamiento en edad, pero además se ha incrementado notablemente en estos 15 años; ambos efectos son mucho más evidentes en el sector servicios. Finalmente, en el colectivo sin actividad se aprecian dos características: en primer lugar, los parados menores de 30 años suponen el mayor volumen en este colectivo, como era de esperar, ya que la población joven que accede al mercado laboral por primera vez, cuenta con experiencia previa; en segundo lugar, desde 2007 2019, y algo menos desde 2019 2022, hay un incremento de volumen de paro en los mayores de 45 años que, con toda probabilidad, corresponde los parados de larga duración de mayor edad.\nEn todos los sectores se aprecia el descenso del volumen total de paro registrado desde 2019 2022, pesar de la crisis sanitaria de la COVID-19.","code":"\nheatmap_anyos(\"sexo\", \"sector\")\ndensidad_compara(\"sector\")"},{"path":"paro-clm.html","id":"conclusiones","chapter":"Capítulo 52 Crisis: impacto en el paro de Castilla-La Mancha","heading":"52.6 Conclusiones","text":"La crisis de 2008 tuvo un gran impacto en el paro registrado de Castilla-La Mancha, multiplicándolo por un factor mayor de 3 desde 2007. Sin embargo, partir del año 2013 el paro registrado inicia una tendencia la baja muy pronunciada que aún hoy continúa, después de haber sufrido un rebote debido la crisis de la COVID-19.\nLa estructura interna de la población parada en la región ha cambiado sustancialmente atendiendo las variables analizadas. En efecto, la población mayor de 45 años, las mujeres, los parados de larga duración y el sector servicios son los grandes perjudicados por ambos procesos de crisis.","code":""},{"path":"cap-rfm.html","id":"cap-rfm","chapter":"Capítulo 53 Segmentación de clientes en el comerico minorista","heading":"Capítulo 53 Segmentación de clientes en el comerico minorista","text":"Jaime Fierro Martín\\(^{}\\), Rocío González Martínez\\(^{}\\) y Cristina Sánchez Figueroa\\(^{b}\\)\\(^{}\\)Analyticae, SL, \\(^{b}\\)Universidad Nacional Distancia","code":""},{"path":"cap-rfm.html","id":"motivación-y-conceptos-clave","chapter":"Capítulo 53 Segmentación de clientes en el comerico minorista","heading":"53.1 Motivación y conceptos clave","text":"Los comercios minoristas (retailers) se mueven en un entorno\nturbulento y necesitan acercarse sus clientes para asegurar su\nsupervivencia. Su producto, o servicio, es nexo clave en dicho proceso.\nEn este contexto, conocer el perfil de los\nclientes permitirá detectar en qué\nmomento de su ciclo de vida con la empresa se encuentran y desarrollar\npropuestas de valor que convengan en cada momento.Segmentar se define como el proceso de dividir \nlos clientes actuales o potenciales, en diferentes grupos o segmentos\nconsistentes en individuos con características y niveles similares de\ninterés (véase el Cap. 31 para una explicación detallada de\nlas técnicas del cluster jerárquico). Es un proceso creativo e iterativo con el fin de satisfacer con\nmayor acierto las necesidades de los clientes, proporcionando una\nventaja competitiva y sostenible la compañía. La segmentación viene\ndada por las necesidades de los clientes, de la compañía, y debería\nser revisada periódicamente.Este caso práctico de negocio está basado en un proyecto real impulsado\npor el departamento de marketing de una empresa del sector retail que\nnecesitaba mejorar el conocimiento de sus clientes, agrupándolos en\nfunción de su comportamiento de compra. Los resultados obtenidos fueron\nclave para definir la estrategia de marketing relacional de la compañía.","code":""},{"path":"cap-rfm.html","id":"el-modelo-recency-frequency-monetary-tradicional","chapter":"Capítulo 53 Segmentación de clientes en el comerico minorista","heading":"53.2 El modelo Recency, frequency, monetary tradicional","text":"El modelo RFM es una técnica popular que se\nutiliza para analizar el comportamiento de compra de los clientes: cómo\ncompran, su frecuencia de compra y cuánto gastan. Es un método útil para\nenriquecer la segmentación de los clientes en varios grupos que permitan\nla personalización e identificación de los clientes más proclives \nresponder las promociones. El análisis RFM depende de las medidas de\nactualidad(recency)(R), frecuencia\n(frequency)(F) y valor monetario\n(monetary)(M), que son tres importantes\nvariables relacionadas con la compra que influyen en las posibilidades\nde compra futura de los clientes.El modelo RFM tradicional categoriza el valor de las variables\ndividiéndolas en quintiles, partir de los cuales se calcula una\npuntuación única que representa el valor del cliente. Sin embargo, es\nmuy preciso. Si el intervalo de frecuencia de compras se fija entre 0 y\n20, en términos de negocio podría interpretarse como que un cliente con\nuna sola compra será igual que otro que tenga 20. Por ello,los enfoques\nde conjuntos clásicos pueden resultar poco funcionales\n(Martı́nez et al. 2019). En este caso práctico, se propone una mejora\nen la definición de los intervalos mediante la aplicación del ranking\nde percentiles. Este método, que se ha\ndenominado modelo RFM extendido, proporciona un método robusto para\ntratar los valores atípicos (outliers), y además normaliza las\nvariables entre 0 y 1 para evitar la diferencias de peso entre las\nvariables, permitiendo así la correcta implementación del algoritmo de\nsegmentación.","code":""},{"path":"cap-rfm.html","id":"el-modelo-recency-frequency-monetary-extendido","chapter":"Capítulo 53 Segmentación de clientes en el comerico minorista","heading":"53.3 El modelo Recency, frequency, monetary extendido","text":"Los autores de este caso práctico recomiendan seguir una metodología de\ngestión de proyectos. La metodología\nCRISP-DM (Chapman et al. 2000b), presentada\nen el Cap. @ref(metodología) es un estándar ampliamente utilizado\nen los proyectos de ciencia de datos.Una vez definido el problema (mejorar el conocimiento que una empresa de\ncomercio minorista tiene de sus clientes, agrupándolos en función de su\ncomportamiento de compra), la recopilación y comprensión de los datos\n(primera etapa del modelo CRISP-DM) se establece como etapa esencial\npara el desarrollo del proyecto.","code":""},{"path":"cap-rfm.html","id":"recopilación-y-comprensión-de-los-datos","chapter":"Capítulo 53 Segmentación de clientes en el comerico minorista","heading":"53.3.1 Recopilación y comprensión de los datos","text":"Hoy en día, la mayoría de las empresas de e-commerce y comercio\nminorista tradicional cuentan con sistemas que permiten registrar los\ndatos básicos de cada una de sus ventas (fecha, artículo, cantidad e\nimporte), asociados un código único de cliente. La información\ncontenida en estos datos de compra atesora gran valor, ya que carecen\ndel sesgo y subjetividad propias de otras informaciones obtenidas\nmediante encuestas de opinión, estudios de mercado, entrevistas y grupos\nde discusión, etc. Estos datos suelen encontrarse en las plataformas ERP\n(Enterprise Resource Planning) de gestión de pedidos y ventas, o CRM\n(Customer Relationship Management) de las empresas.El lector es, o será, consciente de que la fase de extracción, carga y\nlimpieza de los datos es la más exigente del proyecto, y donde se\nempleará gran parte de los recursos y tiempo de todo el proyecto. R\ncuenta con gran cantidad de paquetes y recursos que facilitan la\nextracción desde diferentes tipos de bases de datos.Para este caso práctico serán necesarias las siguientes librerías:Cualquier tipo de estudio o proyecto de ciencia de datos requiere\nfamiliarizarse con los datos y determinar si presentan suficiente\nexactitud, completitud, consistencia, credibilidad y actualidad\n(Muñoz-Reja, Carretero, Cejudo 2018). Los datos de transacciones de venta registrados por\nlas empresas pueden contener datos atípicos (p.ej. valores perdidos,\ninexactos, outliers, etc.). Para determinar la acción tomar, o , de\nlimpieza o corrección de los datos de partida, es esencial conocer el\nnegocio y las consecuencias que éstas operaciones tendrán en el\nresultado final de la segmentación.El conjunto de datos de muestra contiene 200.000 observaciones\ncorrespondientes transacciones de compra. Las siguientes variables\niniciales están explicadas en el set de datos:","code":"\nlibrary(\"tidyverse\")\nlibrary(\"lubridate\")\nlibrary(\"factoextra\")\nlibrary(\"ggpubr\")\nlibrary(\"CDR\")\ndata(\"datos_retail\")\nhead(datos_retail)\n#> # A tibble: 6 x 4\n#> id_ticket fecha importe_venta codigo_socio\n#> <chr> <date> <dbl> <chr>\n#> 1 num_1646673 2021-10-30 12.4 id_1076134\n#> 2 num_2762559 2021-12-03 38.8 id_0552641\n#> 3 num_0309422 2022-01-07 67.8 id_0537369"},{"path":"cap-rfm.html","id":"cálculo-de-las-variables-del-modelo-rfm","chapter":"Capítulo 53 Segmentación de clientes en el comerico minorista","heading":"53.3.2 Cálculo de las variables del modelo RFM","text":"Identificadas las variables iniciales, es necesario calcular los\nfactores clave del Modelo RFM:La variable actualidad, recency (R), es el intervalo de tiempo\ntranscurrido desde la última compra de un cliente hasta la fecha de\nelaboración del modelo RFM.La variable actualidad, recency (R), es el intervalo de tiempo\ntranscurrido desde la última compra de un cliente hasta la fecha de\nelaboración del modelo RFM.La variable frecuencia, frequency (F), se obtiene agrupando las\ncompras por cliente y contando el número total de tickets únicos.La variable frecuencia, frequency (F), se obtiene agrupando las\ncompras por cliente y contando el número total de tickets únicos.La variable valor monetario, monetary (M), se calcula sumando\ntodos los importes de venta por cliente.La variable valor monetario, monetary (M), se calcula sumando\ntodos los importes de venta por cliente.","code":"\nfecha_estudio_rfm <- ymd(\"2022-08-01\")\n\nrfm <- datos_retail |>\n  group_by(codigo_socio) |>\n  summarise(\n    frecuencia = n_distinct(id_ticket),\n    monetario = sum(importe_venta, na.rm = TRUE),\n    fecha_transaccion_reciente = first(fecha, order_by = desc(fecha))\n  ) |>\n  mutate(actualidad = time_length(interval(start = fecha_transaccion_reciente, end = fecha_estudio_rfm), unit = \"days\"), .keep = \"unused\")\nhead(rfm) # el lector puede ver las variables del Modelo RFM"},{"path":"cap-rfm.html","id":"breve-análisis-exploratorio-de-las-variables-del-modelo-rfm","chapter":"Capítulo 53 Segmentación de clientes en el comerico minorista","heading":"53.3.3 Breve análisis exploratorio de las variables del modelo RFM","text":"Del análisis puede concluirse que:107.929 clientes han realizado una media de 1,85 compras, con un\nimporte medio total de 70,56€ y 450,4 días de media desde la última\ncompra hasta la fecha de realización del estudio, con una fuerte\nasimetría positiva de los valores frequency y monetary (ver Fig. 53.1).107.929 clientes han realizado una media de 1,85 compras, con un\nimporte medio total de 70,56€ y 450,4 días de media desde la última\ncompra hasta la fecha de realización del estudio, con una fuerte\nasimetría positiva de los valores frequency y monetary (ver Fig. 53.1).Se detecta una gran estacionalidad de las compras, como se puede\napreciar en la agrupación de las observaciones de recency.\nTeniendo en cuenta la fecha en la que ser realiza el análisis, los\nvalores obtenidos en la variable recency, se pueden interpretar\ncomo el periodo de ventas de la campaña navideña.Se detecta una gran estacionalidad de las compras, como se puede\napreciar en la agrupación de las observaciones de recency.\nTeniendo en cuenta la fecha en la que ser realiza el análisis, los\nvalores obtenidos en la variable recency, se pueden interpretar\ncomo el periodo de ventas de la campaña navideña.\nFigura 53.1: Box-plot\n","code":"\nset.seed(12345)\nplot_data <- rfm |>\n  slice_sample(n = 2000) |>\n  pivot_longer(!codigo_socio, names_to = \"variable\", values_to = \"valor\")\nplot_data |>\n  ggplot(aes(x = variable, y = valor)) +\n  geom_boxplot(outlier.shape = NA, color = \"red\") +\n  geom_jitter(alpha = 1 / 10) +\n  facet_wrap(~variable, ncol = 6, scales = \"free\") +\n  theme(strip.text.x = element_blank(), text = element_text(size = 9))"},{"path":"cap-rfm.html","id":"cálculo-del-ranking-de-percentiles","chapter":"Capítulo 53 Segmentación de clientes en el comerico minorista","heading":"53.3.4 Cálculo del ranking de percentiles","text":"Los valores de ranking son relativos entre clientes y pueden ser\nutilizados para objetivos de negocio, basados en valores absolutos de\npuntuación por cliente.Se podría decir que el análisis RFM combina tres atributos clave de los\nclientes para construir un ranking que permite agruparlos de forma útil\npara el negocio. Así, si un cliente que compró en una fecha reciente\n(Recency) se le otorgan más puntos. Si compró muchas veces (Frecuency),\ntambién se le coloca más arriba en el ranking. Finalmente, si gastó más\nen el total de sus compras (Monetary), también puntúa más alto.\nCombinando estos tres parámetros, se obtiene un ranking RFM. Para la\nelaboración de este ranking se parte del concepto de percentil . La idea\nes asignarle cada cliente una puntuación según las tres variables o\nfactores clave del modelo RFM, de modo que los mejores clientes serán\nlos que tengan una puntuación mayor.Una vez que se tienen los rankings de percentiles en las tres variables\npara todos los clientes, se procede su clusterización mediante el\nmétodo k-means.","code":"\nrfm_rank <- rfm |>\n  mutate(across(.cols = c(\"frecuencia\", \"monetario\"), percent_rank, .names = \"rank_{.col}\")) |>\n  mutate(across(.cols = c(\"actualidad\"), ~ percent_rank(-.x), .names = \"rank_{.col}\")) # menor recency indica mayor puntuación en rank\nhead(rfm_rank) # el lector puede ver la puntuación del ranking"},{"path":"cap-rfm.html","id":"modelado-rfm-mediante-k-means","chapter":"Capítulo 53 Segmentación de clientes en el comerico minorista","heading":"53.3.5 Modelado: RFM mediante k-means","text":"El modelo establecido debe proporcionar una segmentación de clientes con\nsentido de negocio. En este caso práctico se opta por el algoritmo de\nclustering estándar que presenta la ventaja de ser muy intuitivo y\npermite trabajar con grandes conjuntos de datos. Como el lector ha\npodido comprobar, existen otros muchos algoritmos de aprendizaje \nsupervisado que pueden ser empleados.El número óptimo de clusters (o segmentos, en la jerga del marketing)\nes uno de los retos la hora de aplicar técnicas de clustering. \nexiste una manera exclusiva de encontrar el número adecuado de clusters.\nSe trata de un proceso subjetivo que depende de los datos, del tipo de\nclustering empleado y, en este caso, de que el número elegido tenga\nsentido y utilidad en el negocio. Existen numerosos métodos para\nfacilitar la elección del número de clusters; entre ellos destacan el\nElbow method, el Average silhouette method y el Gap statistic\nmethod, que gracias la función fviz_nbclust() del paquete\nfactoextra, se pueden calcular con facilidad para realizar una buena\nelección.Con el método Elbow, número óptimo de clusters se calcula como sigue:\nFigura 53.2: Número óptimo de clusters\nEn la Fig. 53.2 se observa que la varianza total intra-cluster apenas mejora partir\ndel cuarto cluster.El algoritmo de clustering k-means se entrena con las variables R-F-M\nnormalizadas con el ranking. La salida de la función kmeans() del\npaquete base stats es un objeto que, entre otros componentes, ofrece\nun vector numérico indicativo del clusters al que pertenece cada uno\nde los clientes.","code":"\nset.seed(123)\nmuestra_clusters <- rfm_rank |>\n  slice_sample(n = 5000) |>\n  dplyr::select(matches(\"rank\"))\n\nfviz_nbclust(x = muestra_clusters, FUNcluster = kmeans, method = \"wss\", k.max = 10)\nset.seed(123)\nkm_fit <- kmeans(x = rfm_rank[, 5:7], centers = 4, nstart = 10)\n\nclientes_segmentos <- rfm_rank |>\n  mutate(segmento = km_fit$cluster)\nhead(clientes_segmentos) # el lector puede ver el segmento al que pertence el cliente"},{"path":"cap-rfm.html","id":"descriptivos-e-interpretación-de-los-segmentos","chapter":"Capítulo 53 Segmentación de clientes en el comerico minorista","heading":"53.3.6 Descriptivos e interpretación de los segmentos","text":"Los segmentos obtenidos deben tener sentido y utilidad de negocio. Para\nello es imprescindible proporcionar los estadísticos descriptivos de\ncada segmento y proceder su interpretación de perfil de cliente.Interpretación de los segmentos:1-Nuevos probando: segmento que agrupa nuevos clientes que están\nrealizando compras desde hace poco tiempo y tienen un gran potencial\nde desarrollo. Es un segmento de clientes con interés para la\nempresa.2-podemos perder: se trata de los clientes ‘churn’250 que\nfueron buenos clientes en términos monetarios y de frecuencia pero\nque hace tiempo que realizan nuevas compras. La compañía debe\nhacer un esfuerzo en recuperar estos clientes para convertirlos al\nsegmento TOP.3-Top: reúne los mejores clientes de la empresa. Son clientes\nque compran con frecuencia, están activos y aportan ventas la\ncompañía. Es el segmento de clientes con mayor interés para la\nempresa.4-Una compra: segmento formado por aquellos clientes que han\nrealizado una sola compra hace tiempo. Presentan frecuencia,\nactualidad y valor monetario bajo. Se trata de un segmento de\nclientes con escaso interés para la compañía.La Fig 53.3 muestra cada uno de los segmentos indicados. El ranking obtenido ayuda identificar las diferencias en los tipos de clientes y es útil para decidir qué segmentos enfocarse y qué estrategias usar para cada uno.\nFigura 53.3: Lollipop de variables RFM\n","code":"\ndescriptivo_segmentos <- clientes_segmentos |>\n  group_by(segmento) |>\n  summarise(across(c(\"monetario\", \"frecuencia\", \"actualidad\"),\n    .fns = mean, .names = \"md_{.col}\"\n  ), n_clientes = n()) |>\n  ungroup() |>\n  relocate(segmento, n_clientes)\n\nhead(descriptivo_segmentos)\n#> # A tibble: 4 x 5\n#> segmento n_clientes md_monetario md_frecuencia md_actualidad\n#> <int> <int> <dbl> <dbl> <dbl>\n#> 1 1 23551 36.2 1.07 239.\n#> 2 2 23632 77.0 2.26 567.\n#> 3 3 28809 128. 3.11 188.\n#> 4 4 31937 39.3 1.00 757.\nsegmentos_descriptivo <- clientes_segmentos |>\n  mutate(segmento = case_when(\n    segmento == 1 ~ \"1_Nuevos probando\",\n    segmento == 2 ~ \"2_No perder\",\n    segmento == 3 ~ \"3_Top\",\n    segmento == 4 ~ \"4_Una compra\"\n  )) |>\n  group_by(segmento) |>\n  summarise(\n    across(\n      .cols = where(is.numeric),\n      .fns = mean\n    ),\n    n_clientes = n()\n  ) |>\n  ungroup() |>\n  relocate(segmento, n_clientes)\n\ntable_dot_plot <- segmentos_descriptivo |>\n  # select(starts_with(\"rank\")) |>\n  pivot_longer(cols = c(\"rank_monetario\", \"rank_frecuencia\", \"rank_actualidad\"), names_to = \"Variable RFM\", values_to = \"Puntuación\")\n\nggdotchart(\n  table_dot_plot,\n  x = \"Variable RFM\", y = \"Puntuación\",\n  group = \"segmento\", color = \"segmento\", palette = \"jco\",\n  add = \"segment\", position = position_dodge(0.3),\n  sorting = \"none\", facet.by = \"segmento\", dot.size = 5,\n  rotate = TRUE, legend = \"none\"\n)"},{"path":"cap-rfm.html","id":"puesta-en-producción","chapter":"Capítulo 53 Segmentación de clientes en el comerico minorista","heading":"53.3.7 Puesta en producción","text":"Calculado el modelo RFM k-means, la compañía puede incorporar\nperiódicamente los datos de los clientes nuevos, o actualizados. De este\nmodo, los segmentos de clientes se actualizarán y, más allá de las\nacciones de marketing mix que realicen las compañías gracias la\nsegmentación, podrán analizarse las migraciones de clientes entre los\ndiferentes segmentos en el periodo estudiado. La función cl_predict()\nfacilita la actualización periódica de los segmentos con el modelo\nentrenado.","code":""},{"path":"cap-medicina.html","id":"cap-medicina","chapter":"Capítulo 54 Análisis de datos en medicina","heading":"Capítulo 54 Análisis de datos en medicina","text":"Alberto M. Borobia\\(^{}\\) y María Jiménez-González\\(^{}\\)\\(^{}\\)Hospital Universitario La Paz - IdiPAZ","code":""},{"path":"cap-medicina.html","id":"justificación","chapter":"Capítulo 54 Análisis de datos en medicina","heading":"54.1 Justificación","text":"La aplicación de la estadística en la investigación clínica ha sido una de las herramientas clave en los últimos dos años. La pandemia mundial causada por la enfermedad por coronavirus (COVID‑19) es una enfermedad infecciosa provocada por el virus SARS-CoV-2. Drante el año 2020, más de 13 millones de casos diagnosticados en España arrojaban un diagnóstico claro: se necesita más investigación.El primer apartado de este capítulo señala la importancia de la identificación de los sesgos (en concreto, del sesgo de selección) que aparecen en los estudios de investigación aleatorizados. Tras ello, se abordará un ejemplo práctico de una de las aplicaciones más significativas de la bioestadística: el análisis de supervivencia, con el que se resuelven preguntas tan importantes cómo: ¿qué factores de riesgos están asociados la mortalidad provocada por coronavirus?.\n","code":""},{"path":"cap-medicina.html","id":"introducción-al-uso-de-datos-en-investigación-clínica-y-ensayos-clínicos","chapter":"Capítulo 54 Análisis de datos en medicina","heading":"54.2 Introducción al uso de datos en investigación clínica y ensayos clínicos","text":"En este capítulo, y modo ilustrativo del ámbito de la investigación clínica, se abordarán tres análisis partir de los datos:Un análisis relativo la eliminación de sesgos, o más concretamente, la eliminación del sesgo de selección.Un análisis relativo la estimación e interpretación de las curvas de supervivencia.Un análisis relativo la estimación e interpretación de la Regresión de COX.","code":""},{"path":"cap-medicina.html","id":"qué-es-un-ensayo-clínico","chapter":"Capítulo 54 Análisis de datos en medicina","heading":"54.2.1 ¿Qué es un ensayo clínico?","text":"\nEn la investigación clínica existen dos tipos de estudios: estudios observacionales y ensayos clínicos.Los ensayos clínicos aleatorios se definen como el diseño experimental óptimo para proporcionar evidencia, eficacia y seguridad de una intervención (X. Liu et al. 2020). Los tratamientos estudiados o investigados son asignados aleatoriamente en grupos que garantizan que las diferencias en los resultados después del tratamiento reflejen los efectos del mismo (Rosenbaum 2005). Cuando estas condiciones ideales son posibles (falta de recursos, financiación, tiempo, etc), se definen como estudios observacionales.Previo la puesta en marcha de un ensayo clínico, es imprescindible la redacción de un Protocolo y un Plan de Análisis Estadístico (PAE).El protocolo, elaborado por los investigadores del estudio, precisa y justifica los métodos y planes del proceso que se llevará cabo en el ensayo clínico (Rivera et al. 2020).El protocolo, elaborado por los investigadores del estudio, precisa y justifica los métodos y planes del proceso que se llevará cabo en el ensayo clínico (Rivera et al. 2020).El PAE detalla las características principales del eventual análisis estadístico de los datos, que deben describirse en la sección estadística del protocolo (Lewis 1999).El PAE detalla las características principales del eventual análisis estadístico de los datos, que deben describirse en la sección estadística del protocolo (Lewis 1999).Los documentos anteriormente mencionados, y el resto de directrices necesarias para un ensayo clínico, están regulados por la “Conferencia Internacional sobre armonización de requisitos técnicos para el registro de productos farmacéuticos para uso humano” (sus siglas ICH en inglés).","code":""},{"path":"cap-medicina.html","id":"limitaciones-de-los-estudios-observacionales","chapter":"Capítulo 54 Análisis de datos en medicina","heading":"54.2.2 Limitaciones de los estudios observacionales","text":"En el apartado anterior, se puso de manifiesto la importancia de lso ensayos clínicos aleatorizados. Sin embargo, la posible falta de recursos, financiación, tiempo o materiales, dificultan la puesta en marcha y realización de los mismos.En consecuencia, la puesta en práctica de la investigación puede ser la ideal. Los estudios observacionales, sin embargo, son una herramienta elemental en circunstancias tan óptimas, ya que permiten analizar e investigar (contra viento y marea).La limitación principal de los estudios observacionales es ue introducen sesgos en el análisis. Los ensayos clínicos tienen como principal objetivo eliminar el sesgo de selección: cuando los sujetos son asignados aleatoriamente, por ejemplo, los resultados diferentes pueden reflejar estas diferencias iniciales en lugar de los efectos de los tratamientos (Rosenbaum 2005).\n","code":""},{"path":"cap-medicina.html","id":"índice-de-propensión-propensity-score","chapter":"Capítulo 54 Análisis de datos en medicina","heading":"54.2.3 Índice de propensión (propensity score)","text":"Una solución aconsejable y recomendable ante los sesgos “escondidos” en los estudios observacionales es la técnica propensity score o índice de propensión. Esta técnica de emparejamiento equilibra las covariables observadas sesgadas ajustando por su índice de propensión, eliminando presumiblemente el sesgo. Habitualmente, el índice de propensión se obtiene partir de un modelo de regresión cuya variable dependiente corresponde la intervención o el resultado principal (por ejemplo, la muerte) y las variables independientes o covariables corresponden las variables que puedan tener un efecto confusor en la variable dependiente [molina2015indices].","code":""},{"path":"cap-medicina.html","id":"ejemplo-práctico-en-r-de-un-estudio-observacional","chapter":"Capítulo 54 Análisis de datos en medicina","heading":"54.2.4 Ejemplo práctico en R de un estudio observacional","text":"El dataset sintético datos_observacional reproduce los datos de un hipotético estudio observacional sobre una enfermedad X. El objetivo del estudio es estudiar los factores de riesgo asociados la mortalidad causada por esa enfermedad.En la literatura, se ha evidenciado que las mujeres de mayor edad y con una o más comorbilidades tienen más riesgo de fallecer (exitus) por la enfermedad XX. En investigación, la estructura de\nlos resultados en un paper o en un informe estadístico, independientemente de la revista o PAE,\ncomienza en el mismo punto: una tabla resumen de las características basales de la población\nobjeto de estudio El paquete tableone (sencillo juego de palabras) integra funciones específicas para la creación de dichas tablas. La función principal de este paquete es CreateTableOne().Para presentar la tabla de resultados formateada basta con usar la función kable():\nTabla 54.1: Características basales de la población\nEn la Tabla 54.1 y acorde la bibliografía existente, se confirma el sesgo de selección través del desequilibrio de la variable principal (exitus) en las variables sexo, edad y comorbilidades, evidenciado través de la significación de éstas. Un argumento que motiva la aplicación, en este caso, de la técnica propensity score se fundamenta en la viabilidad de, por ejemplo, un modelo multivariante (como puede ser un modelo de predicción). La recogida de datos de un estudio observacional, como el de este ejemplo, normalmente viene dada por la disponibilidad de la población: sujetos ingresados en el Hospital por la enfermedad (en nuestro caso, coronavirus). Por tanto, esta muestra seleccionada recogerá pacientes con pronóstico más grave que la población general (mujer de mayor edad con una o más comorbilidades).El paquete MatchIt integra las funciones principales para el ajuste de la técnica propensity score, concretamente la función matchit() integra la teoría de (D. E. Ho et al. 2007) para el emparejamiento óptimo de los grupos estudiados. Los argumentos más importantes de esta función son:\n- formula: modelo de regresión que estudia la relación entre la variable principal de estudio (exitus) con las variables sesgadas (sexo, edad y comorbilidades).\n- method: especifica el método de matching.\n- distance: especifica el método para la estimación del índice de propensión.La función get_matches() empareja, poteriormente, las coincidencias que resultan del MatchIt.\nNota: es imprescindible que los casos del dataset estén completos.\nPara comprobar que el sesgo evidenciado en estudios anteriores ha desaparecido, se reproduce la tabla anterior.Se formatea la salida de la tabla:\nTabla 54.2: Características basales de la población aplicando la técnica de propensity score\nEn la Tabla @ref(tab:tab1_corregida) se observa que el sesgo de selección existente en la muestra se ha resuelto equilibrando las variables (aunque reduciendo la muestra). Tras este paso previo, podría realizarse un análisis estándar de esta muestra intentando aproximarse lo máximo posible un estudio aleatorizado.","code":"#> # A tibble: 5 × 8\n#>      ID fecha_hospitalizacion sexo    edad comorbilidades         fecha…¹ exitus\n#>   <dbl> <dttm>                <chr>  <dbl> <chr>                  <chr>    <dbl>\n#> 1     1 2015-04-17 00:00:00   Mujer     76 1 o más comorbilidades 17/04/…      1\n#> 2     2 2015-03-21 00:00:00   Mujer     64 1 o más comorbilidades 31/03/…      0\n#> 3     3 2015-04-09 00:00:00   Hombre    65 1 o más comorbilidades 16/04/…      0\n#> 4     4 2015-04-04 00:00:00   Hombre    77 1 o más comorbilidades 13/04/…      0\n#> 5     5 2015-03-24 00:00:00   Mujer     66 1 o más comorbilidades 27/03/…      0\n#> # … with 1 more variable: fecha_exitus <dttm>, and abbreviated variable name\n#> #   ¹​fecha_alta\nlibrary(\"tableone\")\nmy_vars <- c(\"sexo\", \"edad\", \"comorbilidades\")\nnonnormal <- c(\"edad\")\nfactor_vars <- c(\"sexo\", \"comorbilidades\")\n\n# crea la tabla\ntab1 <- CreateTableOne(\n  vars = my_vars, factorVars = factor_vars,\n  strata = \"exitus\", data = datos_observacional\n)\n\n# imprime la tabla\ntab1 <- print(tab1,\n  showAllLevels = TRUE, formatOptions = list(big.mark = \",\"),\n  exact = \"stage\", nonnormal = nonnormal\n)\nknitr::kable(tab1,\n  caption = \"Características basales de la población\",\n  col.names = c(\"level\", \"Vivo\", \"Exitus\", \"p-valor\", \"test\")\n)\nlibrary(\"MatchIt\")\nmatch <- matchit(exitus ~ edad + as.factor(sexo) + as.factor(comorbilidades),\n  method = \"nearest\", distance = \"mahalanobis\",\n  data = datos_observacional\n)\ndatos_observacional_match <- get_matches(match, datos_observacional)\ntab1_corregida <- CreateTableOne(\n  vars = my_vars, factorVars = factor_vars,\n  strata = \"exitus\", data = datos_observacional_match\n)\n\n# se imprime en el objeto tab1_corregida\ntab1_corregida <- print(tab1_corregida,\n  showAllLevels = TRUE, formatOptions = list(big.mark = \",\"),\n  exact = \"stage\", nonnormal = nonnormal\n)\nknitr::kable(tab1_corregida,\n  caption = \"Características basales de la población aplicando la técnica de propensity score\",\n  col.names = c(\"level\", \"Vivo\", \"Exitus\", \"p-valor\", \"test\")\n)"},{"path":"cap-medicina.html","id":"análisis-de-supervivencia","chapter":"Capítulo 54 Análisis de datos en medicina","heading":"54.3 Análisis de supervivencia","text":"Durante la pandemia ocasionada por el SARS-CoV-2, la pregunta principal de los investigadores clínicos se centró en un mismo objetivo: factores de riesgo asociados la mortalidad causada por COVID-19. El análisis de supervivencia ha permitido los investigadores intentar explicar las causas más factibles que producen esa mayor probabilidad de fallecer. El análisis de supervivencia permite estudiar los factores de riesgo asociados la mortalidad. La ventaja principal de este análisis frente un análisis estándar (como puede ser una regresión logística) se centra en la integración en la variable respuesta del evento y del tiempo hasta el evento, que tiene como consecuencia la interpretación de “riesgo” y de “probabilidad” en los resultados.\nEl dataset utilizado, datos_supervivencia está incluido en el paquete CDR y está formado por 301 pacientes, 101 diagnosticados con infección por SARS-CoV-2 y 100 exitus.","code":"\nhead(datos_supervivencia, 5)\n#> # A tibble: 5 × 7\n#>      id EXITUS_TIME DIAG_COVID EXITUS N_COMORBIDITIES SEX     EDAD\n#>   <dbl>       <dbl>      <dbl>  <dbl>           <dbl> <chr>  <dbl>\n#> 1   262           0          1      1               5 Hombre    83\n#> 2   236           1          1      1               5 Hombre    72\n#> 3   170          11          0      0               2 Mujer     65\n#> 4   204          11          1      1               4 Hombre    80\n#> 5    46          14          1      1               5 Hombre    90"},{"path":"cap-medicina.html","id":"estimación-y-comparación-de-curvas-de-supervivencia","chapter":"Capítulo 54 Análisis de datos en medicina","heading":"54.3.1 Estimación y comparación de curvas de supervivencia","text":"La función (o curva) de supervivencia estudia la probabilidad de que el paciente o sujeto, sobreviva un tiempo X. El estimador más común utilizado para el ajuste de la función de supervivencia es el estimador paramétrico Kaplan-Meier y su función escalonada. Una vez generadas estas curvas de supervivencia, existen diferentes métodos (paramétricos y paramétricos) para su comparación. En este apartado, se utiliza la prueba de Mantel-Cox (o test Log-Rank) para el contraste de funciones.Los paquetes survival y survminer integran las funciones principales de la técnica:La función Surv(), de la librería survival, crea un objeto de supervivencia formado por el evento (exitus) y el tiempo hasta la ocurrencia del evento.La función Surv(), de la librería survival, crea un objeto de supervivencia formado por el evento (exitus) y el tiempo hasta la ocurrencia del evento.La función survfit(), de la librería survival, estima la función de supervivencia mediante el método Kaplan-Meier del objeto Surv y los factores de riesgo asociados.La función survfit(), de la librería survival, estima la función de supervivencia mediante el método Kaplan-Meier del objeto Surv y los factores de riesgo asociados.La función ggsurvplot(), genera el gráfico de la gurva de supervivencia (basada en la librería ggplot2). El argumento principal de la función es la función de supervivencia estimada, survfit(). Los argumentos más importantes (y recomendables) la hora de graficar la función de supervivencia son:\npval: muestra el p-valor correspondiente la comparación través del test Log-Rank.\nconf.int: muestra los intervalos de confianza de la(s) curva(s) de supervivencia.\nrisk.table: añade el número de sujetos (absoluto o relativo) en riesgo en cada momento del periodo objeto de estudio.\nLa función ggsurvplot(), genera el gráfico de la gurva de supervivencia (basada en la librería ggplot2). El argumento principal de la función es la función de supervivencia estimada, survfit(). Los argumentos más importantes (y recomendables) la hora de graficar la función de supervivencia son:pval: muestra el p-valor correspondiente la comparación través del test Log-Rank.pval: muestra el p-valor correspondiente la comparación través del test Log-Rank.conf.int: muestra los intervalos de confianza de la(s) curva(s) de supervivencia.conf.int: muestra los intervalos de confianza de la(s) curva(s) de supervivencia.risk.table: añade el número de sujetos (absoluto o relativo) en riesgo en cada momento del periodo objeto de estudio.risk.table: añade el número de sujetos (absoluto o relativo) en riesgo en cada momento del periodo objeto de estudio.Se cragan los paquetesSe ajusta el modelo y, posteriormente, se representa:En la Fig. ??, dónde el eje X corresponde al tiempo en días y el eje Y la probabilidad de supervivencia, se observa que la probabilidad de supervivencia de las personas expuestas COVID es significativamente menor (p-valor < 0.001) las personas sanas. La mediana de supervivencia (línea trazada desde el 0.5 del eje Y, correspondiente al 50% de la probabilidad de supervivencia) corresponde los 120 días, es decir, el 50% de los sujetos diagnosticados por COVID y objeto de estudio sobrevivieron, al menos, 120 días.Por tanto, se puede concluir que se ha encontrado evidencia sobre el aumento de mortalidad asociada la enfermedad COVID-19.","code":"\nlibrary(\"survival\")\nlibrary(\"survminer\")\nfit <- survfit(Surv(EXITUS_TIME, EXITUS) ~ DIAG_COVID,\n  data = datos_supervivencia\n)\n\nggsurvplot(fit,\n  data = datos_supervivencia,\n  pval = TRUE,\n  conf.int = TRUE,\n  ggtheme = theme_bw(),\n  palette = c(\"#E7B800\", \"#2E9FDF\"),\n  xlab = \"Tiempo (días)\",\n  ylab = \"Probabilidad de supervivencia\",\n  legend.labs = c(\"Sano\", \"COVID\"),\n  # añade tabla de supervivencia\n  risk.table = TRUE,\n  tables.height = 0.2,\n  tables.theme = theme_cleantable()\n)"},{"path":"cap-medicina.html","id":"regresión-de-cox","chapter":"Capítulo 54 Análisis de datos en medicina","heading":"54.4 Regresión de COX","text":"La regresión de Cox251 o modelo de riesgos proporcionales es una técnica utilizada para el estudio del efecto de covariables sobre el tiempo hasta la ocurrencia de un evento (exitus, recaída, progresión, etc). La regresión de Cox es realmente una Regresión en la que la variable dependiente es siempre una función de riesgo o supervivencia (están íntimamente relacionadas) y los predictores son una función del tiempo y una función de las variables consideradas como explicativas. En general, se suele expresar así:\n\\[h(t, x_1, x_2,..., x_ p)=h_0(t)+g(x_1,x_2,...,x_p),\\]\ny más concretamente,\n\\[h(t, x_1, x_2,..., x_ p)=h_0(t)+e^g(x_1,x_2,...,x_p),\\] donde \\(g\\), normalmente, indica una combinación lineal de las covariables o variables explicativas. Es, por tanto, una técnica semi-paramétrica.La función principal para el ajuste de un modelo de regresión de Cox es coxph(). Esta función, al igual que la función survfit(), está formada por un objeto Surv y las covariables del modelo.\nEl output principal de una regresión de Cox,\n\\[h(t, x_1, x_2,..., x_ p),\\] son las razones de riesgos o *hazard ratios (HR). Es decir, la relación entre las dos funciones de riesgo en función de los cambios operados en las variables explicativas. En concreto, la exponencial del coeficiente estimado para la vaEl output principal de una regresión de Cox ,h(t, x_1, x_2,…, x_ p), son las razones de riesgos o hazard ratios (HR). Es decir, la relación entre las dos funciones de riesgo en función de los cambios operados en las variables explicativas. En concreto, la exponencial del coeficiente estimado para la variable explicativa X_i indica el incremento en el riesgo de fallecer cuando la variable explicativa aumenta en una unidad y las demás permanecen constantes. Esta razón de riesgos oscila entre 0 \\(\\infty\\) , siendo el intervalo [0,1] una relación de riesgo bajo y [1, \\(\\infty\\) ] una relación de riesgo alto.NotaLos HR localizados entre 1 y 2 se interpretan en porcentaje. Es decir, HR = 1.5 indica un aumento del riesgo del 50%.Los HR localizados entre 1 y 2 se interpretan en porcentaje. Es decir, HR = 1.5 indica un aumento del riesgo del 50%.Los HR localizados entre 2 e \\(\\infty\\) se interpretan en “veces”. Es decir, HR = 3 indica un aumento del riesgo de 3 veces.Los HR localizados entre 2 e \\(\\infty\\) se interpretan en “veces”. Es decir, HR = 3 indica un aumento del riesgo de 3 veces.Los HR localizados entre 0 y 1 se interpretan como una reducción del riesgo del \\((1-HR)\\times 100 \\%\\). Es decir, HR = 0.8 indica una disminución del riesgo del 20%.Los HR localizados entre 0 y 1 se interpretan como una reducción del riesgo del \\((1-HR)\\times 100 \\%\\). Es decir, HR = 0.8 indica una disminución del riesgo del 20%.De la compleja salida del modelo, deben resaltarse y explicarse los siguientes puntos:Apartado 1: Fórmula del modelo, tamaño muestral y número de eventos.Apartado 2: Tabla con los coeficientes del modelo y su p-valor.Apartado 3: Tabla con los Hazard Ratio (exponencial de los coeficientes de la tabla anterior) y sus intervalos de confianza (lower .95 y upper .95).Apartado 4: parámetros de bondad de ajuste del modelo.Por tanto, de este modelo se pueden concluir las siguientes interpretaciones:Un paciente diagnosticado de COVID-19 tiene 3.6 veces más riesgo de fallecer que un paciente sano.Un paciente diagnosticado de COVID-19 tiene 3.6 veces más riesgo de fallecer que un paciente sano.Una mujer tiene un 67.6% menos riesgo de fallecer que un hombre.Una mujer tiene un 67.6% menos riesgo de fallecer que un hombre.Por cada comorbilidad, el riesgo de fallecer aumenta un 17.9%.Por cada comorbilidad, el riesgo de fallecer aumenta un 17.9%.","code":"\nfit_cox <- coxph(Surv(EXITUS_TIME, EXITUS) ~ DIAG_COVID + EDAD + SEX + N_COMORBIDITIES,\n                 data = datos_supervivencia\n)\nsummary(fit_cox)\n#> Call:\n#> coxph(formula = Surv(EXITUS_TIME, EXITUS) ~ DIAG_COVID + EDAD + \n#>     SEX + N_COMORBIDITIES, data = datos_supervivencia)\n#> \n#>   n= 271, number of events= 100 \n#>    (30 observations deleted due to missingness)\n#> \n#>                       coef  exp(coef)   se(coef)      z Pr(>|z|)    \n#> DIAG_COVID       1.3023581  3.6779594  0.5184547  2.512   0.0120 *  \n#> EDAD             0.0006006  1.0006008  0.0116113  0.052   0.9587    \n#> SEXMujer        -1.1256901  0.3244285  0.2360183 -4.770 1.85e-06 ***\n#> N_COMORBIDITIES  0.1643743  1.1786554  0.0774043  2.124   0.0337 *  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#>                 exp(coef) exp(-coef) lower .95 upper .95\n#> DIAG_COVID         3.6780     0.2719    1.3314   10.1605\n#> EDAD               1.0006     0.9994    0.9781    1.0236\n#> SEXMujer           0.3244     3.0823    0.2043    0.5153\n#> N_COMORBIDITIES    1.1787     0.8484    1.0127    1.3717\n#> \n#> Concordance= 0.815  (se = 0.025 )\n#> Likelihood ratio test= 130.1  on 4 df,   p=<2e-16\n#> Wald test            = 117.9  on 4 df,   p=<2e-16\n#> Score (logrank) test = 165.9  on 4 df,   p=<2e-16"},{"path":"cap-medicina.html","id":"conclusión","chapter":"Capítulo 54 Análisis de datos en medicina","heading":"54.5 Conclusión","text":"Ha sido necesaria una pandemia mundial para que la sociedad empiece dar visibilidad y reconocimiento sólo la bioestadística, sino la investigación clínica y la necesidad de gestionar el uso masivo de datos en salud. pesar de los múltiples estudios y experiencias pasadas que llamaban la prudencia y la acción concreta si se daba una situación similar, el mundo ha sido incapaz de actuar convenientemente. Esto último se ve reflejado en el mínimo aumento de inversión, reconocimiento y notoriedad sólo en investigación o desarrollo, sino en el apoyo la ciencia.","code":""},{"path":"cap-futbol.html","id":"cap-futbol","chapter":"Capítulo 55 Messi y Ronaldo: dos ídolos desde la perspectiva de los datos","heading":"Capítulo 55 Messi y Ronaldo: dos ídolos desde la perspectiva de los datos","text":"Borja Andrino TurónEL PAÍS","code":""},{"path":"cap-futbol.html","id":"motivación-1","chapter":"Capítulo 55 Messi y Ronaldo: dos ídolos desde la perspectiva de los datos","heading":"55.1 Motivación","text":"El uso de estadísticas avanzadas en los deportes, especialmente en el fútbol, ha despegado en los últimos años. Una buena señal de su irrupción es la apuesta de algunos medios deportivos —como FiveThirtyEight o Athletic— por contenidos basados en el análisis y la visualización de estas estadísticas para explicar las fortalezas y debilidades de jugadores y equipos. Además, la generación de estadísticas avanzadas, como los goles esperados, la amenaza o el valor con balón están comenzando sustituir las métricas tradicionales en la narración y las crónicas de los encuentros.\n","code":""},{"path":"cap-futbol.html","id":"las-estadísticas-y-el-fútbol","chapter":"Capítulo 55 Messi y Ronaldo: dos ídolos desde la perspectiva de los datos","heading":"55.2 Las estadísticas y el fútbol","text":"En el presente capítulo se usarán estadísticas de la web especializada Fbref.com para visualizar el dominio de Cristiano Ronaldo y Lionel Messi durante más de 15 años. Para usar estos datos podríamos usar técnicas de web scraping esta página web o usar la librería worldfootballR, desarrollada por Jason Zivkovic. La librería permite obtener datos de diferentes plataformas.La publicación y explotación de estadísticas avanzadas es reciente, de las últimas seis temporadas, con lo que para analizar las carreras completas de estos dos jugadores tendremos que conformarnos, de momento, con métricas tradicionales.En la Fig. 55.1 se ve la evolución de las principales cifras que definen un atacante: los goles y las asistencias. Esta estandarización nos permite poder comparar ambos jugadores independientemente del número de minutos, aunque se ha añadido un filtro de al menos 1.000 minutos jugados en la temporada para evitar ruido.\nPara realizar el gráfico, se toman los datos originales y se filtran para que solo aparezcan los jugadores seleccionados, en las temporadas con muestra suficiente. continuación, se seleccionan las columnas que se usarán en el plot y se giran las dos métricas para poder añadirlas en un único geom_line().\nFigura 55.1: Evolución de goles y asistencias por 90 minutos de Cristiano y Messi desde 2005\nLa Fig. 55.1 arroja un dato increíble, durante 10 años, tener Messi o Cristiano en el campo significaba contar en ese partido con un gol y casi media asistencia.Pero la visualización solo nos habla de estos dos futbolistas. Para compararlos con otros jugadores se puede calcular el percentil de goles y asistencias por 90 minutos, temporada temporada, de los jugadores que hayan jugado más de 1.000 minutos (véase Fig. 55.2). El resultado de nuevo es impactante: durante 13 temporadas Messi y Cristiano han estado entre el 1% de jugadores con más goles. Además, el argentino ha terminado la temporada entre el 1% con más asistencias en 9 ocasiones.\nFigura 55.2: Percentil de goles y asistencias por 90 minutos cada temporada desde 2005\nDesde la temporada 2017/18 en esta web publican estadísticas avanzadas de jugadores por partido y temporada. En la Fig. 55.3 se representan los goles esperados (miden cómo de probable es el gol dado un disparo) y las asistencias esperadas (suma de los goles esperados que suman los pases que desembocan en un tiro) por 90 minutos de los jugadores con más de 1.000 minutos. De nuevo el gráfico destaca nuestros dos protagonistas, que se sitúan en el arco más alejado del origen de coordenadas, donde se juntan los jugadores con mejores números de asistencias y goles esperados.\n\nFigura 55.3: Goles y asistencias por jugador y temporada\nLa métrica de goles esperados permite también hablar de efectividad. Cuando un jugador suma más goles con sus disparos de lo que era esperable su efectividad es alta; cuando por el contrario el jugador termina anotando menos goles de los que se preveían por sus disparos su efectividad es baja. En la Fig. ?? se muestra para cada jugador y temporada esta relación. Se vuelve observar cómo Cristiano y Messi destacan en la generación de goles esperados, aunque hay una ligera diferencia: entre 2018 y 2021 la efectividad del argentino fue mayor que la del portugués. Los puntos de Cristiano se sitúan sobre la línea que representa lo esperado: mismo número de goles que probabilidad de que los disparos acaben en gol. Los de Messi se sitúan por encima, ha anotado más goles que los que sus disparos hacían prever.Con estos gráficos se puede hacer una primera evaluación de los datos de estos dos grandes jugadores (y de cualquier otro) y quizás logremos contestar la pregunta de quién ha sido el mejor, aunque para algunos con esto ya esté claro.\nFigura 55.4: Goles esperados y anotados por jugador y temporada\n","code":"\npacman::p_load(CDR, tidyverse, janitor, ggbeeswarm, here, \n               patchwork, ggtext, ggrepel)\n\n\ndatos_players |>\n  filter(player %in% c(\"Cristiano Ronaldo\", \"Lionel Messi\"),\n         min_playing > 1000) |>\n  select(season_end_year, player, Goles = gls_per, Asistencias = ast_per) |>\n  pivot_longer(c(Goles, Asistencias), names_to = \"metric\", values_to = \"value\") |>\n  ggplot(aes(x = season_end_year, y = value, color = player)) +\n  geom_line(size = 1) +\n  geom_point(size = 2, alpha = 0.5) +\n  scale_color_manual(values = c(\"Lionel Messi\" = \"#118ab2\",\n                                \"Cristiano Ronaldo\" = \"#ef476f\")) +\n  scale_x_continuous(breaks = seq(2005, 2023, 3)) +\n  geom_hline(yintercept = 0, size = 0.2) +\n  facet_wrap(~metric) +\n  labs(title = \"Evolución de los goles y asistencias por cada 90 minutos de<br><b style='color:#ef476f;'>Cristiano Ronaldo<\/b> y <b style='color:#118ab2;'>Lionel Messi<\/b>\",\n       x = \"Temporada\", y = \"Valor\", caption = \"Fuente: Fbref.com\") +\n  theme_minimal() +\n  theme(legend.position = \"none\", \n        panel.grid.minor = element_blank(),\n        plot.title = element_markdown(margin=margin(0,0,10,-30), \n                                      size=12))\npercentiles_to_plot <- \n  datos_players |>\n  clean_names() |>\n  filter(min_playing > 1000) |>\n  select(season_end_year, player, min_playing, gls_per, ast_per) |> \n  group_by(season_end_year) |>\n  mutate(across(c(gls_per, ast_per), ntile, 100,\n                .names = \"{.col}_centil\")) |> \n  ungroup() |>\n  mutate(highlighted_player = if_else(player %in%\n                                        c(\"Cristiano Ronaldo\", \"Lionel Messi\"),\n                                      T,\n                                      F)) |>\n  select(season_end_year, player, highlighted_player, \n         Goles = gls_per_centil, Asistencias = ast_per_centil)\n\npercentiles_to_plot |>\n  pivot_longer(c(Goles, Asistencias), names_to = \"metric\", values_to = \"value\") |>\n  ggplot(aes(x = season_end_year, y = value, group = season_end_year)) +\n  geom_jitter(aes(alpha = highlighted_player, color = player)) +\n  scale_color_manual(values = c(\"Lionel Messi\" = \"#118ab2\",\n                                \"Cristiano Ronaldo\" = \"#ef476f\")) +\n  geom_hline(yintercept = 0, size = 0.1) +\n  labs(title = \"Percentil de goles y asistencias por cada 90<br>minutos de <b style='color:#ef476f;'>Cristiano Ronaldo<\/b> y <b style='color:#118ab2;'>Lionel Messi<\/b>\",\n       x = \"Temporada\", y = \"Percentil\", caption = \"Fuente: Fbref.com\") +\n  facet_wrap(~metric, scales = \"free\") +\n  scale_x_continuous(breaks = seq(2005,2023)) +\n  scale_alpha_manual(values = c(0.01, 1)) +\n  coord_flip() +\n  guides(alpha = \"none\") +\n  theme_minimal() +\n  theme(legend.position = \"none\", \n        panel.grid.minor = element_blank(),\n        plot.title = element_markdown(margin=margin(0,0,0,-30), size=12))\nexpected_data <- \n  datos_players |> \n  clean_names() |> \n  filter(season_end_year >= 2018,\n         min_playing > 1000, \n         x_g_per > 0 | x_ag_per > 0) |> \n  mutate(highlighted_player = if_else(player %in% c(\"Cristiano Ronaldo\", \"Lionel Messi\"), \n                                      T, \n                                      F), \n         label = if_else(player %in% c(\"Cristiano Ronaldo\", \"Lionel Messi\"), \n                         as.character(season_end_year), \n                         NA_character_))\n\nexpected_data |> \n  select(season_end_year, player, highlighted_player, label,\n         Goles = x_g_per, Asistencias = x_ag_per) |> \n  ggplot(aes(x = Asistencias, y = Goles)) + \n  geom_point(aes(alpha = highlighted_player, \n                 color = player)) +\n  geom_text_repel(aes(label = str_sub(label, 3, 4))) + \n  scale_color_manual(values = c(\"Lionel Messi\" = \"#118ab2\",\n                                \"Cristiano Ronaldo\" = \"#ef476f\")) +\n  geom_hline(yintercept = 0, size = 0.2) +\n  geom_vline(xintercept = 0, size = 0.2) +\n  scale_alpha_manual(values = c(0.1, 1)) + \n  labs(title = \"Goles y asistencias esperadas por 90 minutos cada temporada de<br><b style='color:#ef476f;'>Cristiano Ronaldo<\/b>, <b style='color:#118ab2;'>Lionel Messi<\/b> y el resto de jugadores\",\n       x = \"Asistencias esperadas\", y = \"Goles esperados\", caption = \"Fuente: Fbref.com\") +\n  guides(alpha = \"none\") + \n  theme_minimal() + \n  theme(legend.position = \"none\",\n        plot.title = element_markdown(margin=margin(0,0,10,-30), \n                                      size=12), \n        legend.title = element_blank())\n\nexpected_data |> \n  select(season_end_year, player, highlighted_player, label,\n         Goles = gls_per, `Goles esperados` = x_g_per) |> \n  ggplot(aes(x = `Goles esperados`, y = Goles)) + \n  geom_point(aes(alpha = highlighted_player, \n                 color = player)) +\n  geom_text_repel(aes(label = str_sub(label, 3, 4))) + \n  scale_color_manual(values = c(\"Lionel Messi\" = \"#118ab2\",\n                                \"Cristiano Ronaldo\" = \"#ef476f\")) +\n  geom_hline(yintercept = 0, size = 0.2) +\n  geom_vline(xintercept = 0, size = 0.2) +\n  geom_abline(slope = 1) +\n  geom_text(x = 1, y = 1.4, \n            label = \"Por encima de la línea\\nlos jugadores más efectivos\", \n            size = 3, hjust = 1, vjust = 0.5) + \n  geom_curve(x = 1.01, y = 1.4, xend = 1.2, yend = 1.2, \n             size = 0.2, curvature = -0.25, arrow = arrow(length = unit(0.02, \"npc\"))) + \n  scale_alpha_manual(values = c(0.1, 1)) + \n  scale_x_continuous(limits = c(0, 1.5)) + \n  scale_y_continuous(limits = c(0, 1.5)) + \n  labs(title = \"Goles esperados y conseguidos por 90 minutos cada temporada de<br><b style='color:#ef476f;'>Cristiano Ronaldo<\/b>, <b style='color:#118ab2;'>Lionel Messi<\/b> y el resto de jugadores\",\n       x = \"Goles esperados\", y = \"Goles\", caption = \"Fuente: Fbref.com\") +\n  guides(alpha = \"none\") + \n  theme_minimal() + \n  theme(legend.position = \"none\", \n        plot.title = element_markdown(size=12), \n        legend.title = element_blank())"},{"path":"cambioclimatico.html","id":"cambioclimatico","chapter":"Capítulo 56 Un dato sobre el cambio climático","heading":"Capítulo 56 Un dato sobre el cambio climático","text":"Dominic RoyéFundación de la Investigación del Clima","code":""},{"path":"cambioclimatico.html","id":"introducción-26","chapter":"Capítulo 56 Un dato sobre el cambio climático","heading":"56.1 Introducción","text":"La temperatura media global en la superficie ha aumentado en 1.1 ºC desde la era preindustrial (1880-1900). pesar de parecer un leve incremento en la temperatura, implica un aumento significativo en el calor acumulado del sistema tierra. Cuando se combinan el aumento de la temperatura con respecto la superficie terrestre y el océano, la tasa promedio es de 0,08 ºC por década desde 1880. Sin embargo, la tasa promedio de aumento desde 1981 ha sido más del doble: con 0,18ºC. Los océanos se caracterizan por una menor tasa de calentamiento debido su capacidad calorífica. obstante, son los océanos los que absorben la mayoría del calor adicional del planeta debido al cambio climático252.Entre todas las regiones, la región mediterránea se está calentando un 20% más rápido que el promedio mundial. Este lugar representa actualmente el punto crítico más importante del cambio climático, donde se percibe un significativo aumento de las vulnerabilidades. La temperatura de las aguas superficiales en el Mediterráneo ha estado subiendo 0,34ºC cada década desde principios de los 80 (Cramer et al. (2020)).En este caso práctico con datos sobre el cambio climátio se tratan las anomalías de la temperatura superficial del mar Mediterráneo en los meses estivales desde 1982 2022. Se hará uso del dataset con el nombre “NOAA CDR OISST v02r01”, una interpolación óptima diaria de temperatura superficial del mar (OISST, por sus siglas en inglés) con una resolución de 1/4 grados (27 km). Los datos los proporciona la National Oceanic Atmospheric Administration (NOAA) con campos completos de temperatura del océano construidos mediante la combinación de observaciones ajustadas por sesgo de diferentes plataformas (satélites, barcos, boyas) en una cuadrícula global regular, con lagunas estimadas por interpolación (https://developers.google.com/earth-engine/datasets/catalog/NOAA_CDR_OISST_V2_1). El geoprocesamiento en nube está explicado en el Cap. @ref{geoproces}.","code":""},{"path":"cambioclimatico.html","id":"consideraciones-iniciales","chapter":"Capítulo 56 Un dato sobre el cambio climático","heading":"56.2 Consideraciones iniciales","text":"La información espacio-temporal es clave en muchas disciplinas, especialmente en la climatología o la meteorología, y ello hace necesario disponer de un formato que permita una estructura multidimensional. Además, es importante que ese formato tenga un alto grado de compatibilidad de intercambio y pueda almacenar un elevado número de datos. Estas características llevaron al desarrollo del estándar abierto netCDF (Network Common Data Form). El formato netCDF es un estándar abierto de intercambio de datos científicos multidimensionales que se utiliza con datos de observaciones o modelos, principalmente en disciplinas como la climatología, la meteorología y la oceanografía. Se trata de un formato espacio-temporal con una cuadrícula regular o irregular. La estructura multidimensional en forma de matriz (array) permite usar sólo datos espacio-temporales, sino también multidimensionales. Los datos multidimensionales en formato geotiff son menos comúnes, pero también se pueden llegar usar. Además, es posible crear objetos multidimensionales importando multiples archivos ráster.","code":""},{"path":"cambioclimatico.html","id":"paquetes","chapter":"Capítulo 56 Un dato sobre el cambio climático","heading":"56.3 Paquetes","text":"El manejo de datos en formato netCDF o múltiples archivos ráster es posible través de varios paquetes de forma directa o indirecta. Destaca el paquete ncdf4, específicamente diseñado para esto, del que hacen uso también otros paquetes de forma oculta. El manejo con ncdf4 es algo complejo, particularmente por la necesidad de gestionar la memoria RAM cuando se tratan con grandes conjuntos de datos o también por la forma de manejar la clase array. Otro paquete muy potente es terra, clave en el trabajo con datos ráster y permite usar sus funciones también para el manejo del formato netCDF.","code":"\n# paquetes\nlibrary(\"tidyverse\")\nlibrary(\"sf\")\nlibrary(\"terra\")\nlibrary(\"lubridate\")\nlibrary(\"fs\")\nlibrary(\"patchwork\")\nlibrary(\"giscoR\")\nlibrary(\"scales\")\nlibrary(\"rmapshaper\")\nlibrary(\"RColorBrewer\")\nlibrary(\"CDR\")"},{"path":"cambioclimatico.html","id":"visualización-de-mapas-pequeños-múltiples","chapter":"Capítulo 56 Un dato sobre el cambio climático","heading":"56.4 Visualización de mapas “pequeños múltiples”","text":"Una forma muy efectiva para mostrar cambios espacio-temporales son los mapas de pequeños múltiples, donde se representan en una rejilla para cada año las anomalías observadas, lo que permite una comparación sencilla.","code":""},{"path":"cambioclimatico.html","id":"datos","chapter":"Capítulo 56 Un dato sobre el cambio climático","heading":"56.4.1 Datos","text":"Se importa el polígono del Mar Mediterráneo para limitar los datos al área de interés.continuación, se importan todos los años empleando la función dir_ls() del paquete fs y la función rast() de terra. La primera función crea un vector de todos los archivos ubicados en la carpeta “data”. Finalmente se renombran todas las capas con los correspondientes años. Es importante que se garantice el correcto orden de los archivos. Siempre que se haya realizado el geoprocesamiento en nube de las anomalías (Cap. @ref{geoproces}) se puede usar la alternativa: anom <- dir_ls(\"data-cc\", regexp = \"tif\") |> rast().","code":"\ndata(\"med_limit\") \n# importar\nanom <- dir_ls(system.file(\"external/data-cc/\", package = \"CDR\"), regexp = \"tif\") |> rast()\n\n# renombrar las capas\nnames(anom) <- 1982:2022 "},{"path":"cambioclimatico.html","id":"preparación-de-los-datos","chapter":"Capítulo 56 Un dato sobre el cambio climático","heading":"56.4.2 Preparación de los datos","text":"Después de importar el polígono del Mar Mediterráneo, es necesario recortar y enmascarar el área de interés. Para ello, se usa primero la función crop() con los límites del Mar Mediterraneo y se pasa el resultado la función mask(). El paquete terra necesita los datos vectoriales en su propia clase SpatVector, por eso, se pasa con la función vect(), que lo convierte de la clase sf SpatVector. Finalmente, se reproyectan los rásters ETRS89-extended / LAEA Europe con el código EPSG:3035.En la Fig. 56.1 se puede ver el resultados de los primeros años.\nFigura 56.1: Selección de anomalías 1982 1997 de los datos brutos.\nUn ráster consiste en latitud, longitud y un único o múltiples valores, también llamados capas. Para poder visualizarlo en ggplot2, es necesario convertirlo en un data.frame. En este caso, se obtienen 41 columnas para las anomalías, además de las primeras dos que corresponden la longitud y latitud.obstante, es necesario realizar cambios en las distribución de las variables. Ahora mismo se tiene la misma variable, la anomalía, distribuida en muchas columnas, obstante la estructura adecuada debe ser un conjunto total de dos columnas: una que represente las anomalías y una segunda que contenga los años. Para conseguirlo, se hace uso de pivot_longer(), indicando el total de columnas que deben ser fusionadas y los nombres de las dos columnas resultantes.Se añaden los años de la década de los 80 que faltan (1980, 1981) y se limitan las anomalías un rango entre -2 y +2.Previo la construcción del gráfico, se estima la media de la anomalía global para toda la cuenca mediterránea de cada año. Estos datos se añadirán como texto cada mapa. Con el objetivo de obtener las coordenadas de la posición en la proyección EPSG:3035, se fija un punto vectorial que reproyectamos.","code":"\nanom <- crop(anom, med_limit) |> mask(vect(med_limit))\nanom <- project(anom, \"EPSG:3035\")\nplot(anom)\ndf <- as.data.frame(anom, xy = TRUE)\ndf <- pivot_longer(df, 3:length(df), \n                   names_to = \"yr\",\n                   values_to = \"anom\")\ndf <- bind_rows(df, filter(df, yr == \"1982\") |> \n                    mutate(yr = \"1981\", anom = NA),\n                   filter(df, yr == \"1982\") |> \n                    mutate(yr = \"1980\", anom = NA)\n                ) |>\n      mutate(anom2 = case_when(anom > 2 ~ 2,\n                               anom < -2 ~ -2,\n                               TRUE ~ anom))\n# media global\nmed_anom <- global(anom, fun = \"mean\", na.rm = TRUE)\nmed_anom <- rownames_to_column(med_anom, \"yr\")\n\n# posición \npos_global <- st_point(c(34.24, 41.5)) |> \n                 st_sfc(crs = 4326) |> \n                   st_transform(3035) |> \n                     st_coordinates()"},{"path":"cambioclimatico.html","id":"construcción-del-gráfico-de-múltiples-mapas","chapter":"Capítulo 56 Un dato sobre el cambio climático","heading":"56.4.3 Construcción del gráfico de múltiples mapas","text":"En el primer paso se definen los estilos partiendo de theme_void(), configurando los títulos, la leyenda y el color de fondo en theme().Para representar datos ráster en forma de xyz se utiliza geom_tile() o geom_raster() en ggplot2. La última geometría requiere una rejilla regular. Para este primer ensayo, se filtra sólo el año 2003, y además se añade, con geom_sf(), el límite del Mar Mediterráneo. La función coord_sf() permite fijar una proyección para objetos sf, y por último, se cambia el estilo definido anteriormente.Siguiendo el ejemplo, se modifica la gama de colores con scale_fill_gradientn(), en la que se pasa la paleta de colores, los extremos de valores, se reajustan los valores una escala divergente se definen las etiquetas y sus posiciones. Dentro de la función guides(), se cambia el ancho y altura de la barra colores empleando la función guide_colorbar().Las geometrías geom_point() y geom_text() añadirán la información de la anomalía global. La posición se pasa de forma directa en aes(); además, se definen el color y el tamaño de texto. El objetivo es situar el texto la derecha del punto. Por esa razón, es necesario un ajuste en longitud indicando un valor correspondiente en el argumento nudge_x en la unidad del sistema de coordenadas (SC). Recuerdese que el SC está en metros.La función number() del paquete scales facilita formatear las cifras con un decimal y los símbolos negativo y positivo.En los datos se ha añadido dos años con valores perdidos (1980 y 1981) con el objetivo de obtener por cada fila 10 años, evitando que el facet grid empiece por 1982 sin posibilidad de mantener en cada fila la década correspondiente. obstante, para que los límites de la cuenca mediterránea aparezca en las facetas de los años 1980/81, se debe repetir la geometría para todos los años.continuación, se construye todo el gráfico con todas las facetas de mapas. Lo único nuevo es la función facet_wrap(), en la que se indica la variable por la que se crean las facetas. diferencia de facet_grid(), esta variante permite fijar el número de filas y/o columnas. Además, se pasa una función menor en la función labeller() en el mismo argumento. Esta función permite modificar las etiquetas de las facetas (aquí únicamente el texto de los años 1980/81).Finalmente, se combinan el objeto con definiciones finales, como los títulos, el sistema de coordenadas y el estilo. Es importante indicar clip = “”, dado que en caso contrario se cortan visualmente los valores de las anomalías globales al encontrarse fuera de los límites de cada mapa.priori, sería necesario una ampliación del resultado. obstante, en ocasiones se requiere un mapa de orientación.","code":"\ntheme_SST_facet <- function(base_family = \"Bahnschrift\",\n                            base_size = 11, \n                            base_line_size = base_size/22, \n                            base_rect_size = base_size/22) {\n  \ntheme_void(base_family = base_family, base_size = base_size,\n           base_line_size = base_line_size, base_rect_size = base_rect_size) +\n  theme(strip.text = element_text(colour = \"white\", \n                                  face = \"bold\",\n                                  size = 12, \n                                  margin = margin(b = 15)),\n        legend.text = element_text(colour = \"white\"),\n        legend.position = \"top\",\n        legend.justification = .48,\n        plot.margin = margin(20, 20, 20, 20),\n        plot.title = element_text(colour = \"white\", size = 30, hjust = .5),\n        plot.subtitle = element_text(colour = \"white\", size = 15, hjust = .5,\n                                     margin = margin(t = 5, b = 5)),\n        plot.caption = element_text(colour = \"white\", size = 10, hjust = 0),\n        plot.background = element_rect(fill = \"grey10\", colour = NA),\n        panel.spacing = unit(2, \"lines\"),\n        panel.background = element_rect(fill = \"grey10\", colour = NA))\n}\nfilter(df, yr == \"2003\") |> \n ggplot() +\n  geom_tile(aes(x, y, fill = anom2)) +\n  geom_sf(data = med_limit, \n          fill = NA, colour = \"white\", size = .1) +\n   coord_sf(crs = 3035) +\n    theme_SST_facet()\n# gama de colores\nrdbu_pal <- rev(brewer.pal(11, \"RdBu\"))\n\n# mapa 2003\nfilter(df, yr == \"2003\") |> \nggplot() +\n  geom_tile(aes(x, y, fill = anom2)) +\n   geom_sf(data = med_limit, \n          fill = NA,\n          colour = \"white\",\n          size = .1) +\n  geom_point(data = filter(med_anom, yr == \"2003\"),\n             aes(x = pos_global[1,1], y = pos_global[1,2], fill = mean),\n             size = 3.5, shape = 21, colour = \"white\") +\n  geom_text(data = filter(med_anom, yr == \"2003\"),\n            aes(x = pos_global[1,1], y = pos_global[1,2], \n                label = number(mean, .1, style_positive = \"plus\")),\n            size = 3.5, nudge_x = 700000, colour = \"white\") +\n  scale_fill_gradientn(colours = rdbu_pal, na.value = NA,\n                       values = rescale(c(-2, 0, 2)),\n                       limits = c(-2, 2),\n                       breaks = c(-2, -1.5, -1, -0.5, 0, \n                                  .5, 1, 1.5, 2),\n                       labels = c(\"< -2.0\", \"-1.5\", \"-1.0\", \"-0.5\", \"0.0\", \n                                  \"0.5\", \"1.0\", \"1.5\", \"> 2.0\")) +\n  guides(fill = guide_colorbar(barwidth = 20, \n                               barheight = .5)) +\n  coord_sf(crs = 3035) +\n  theme_SST_facet()\nmed <- slice(med_limit, rep(1, 41)) |> \n            dplyr::select(geometry) |> \n             mutate(yr = as.character(1982:2022))\n# paso 1\ng <- ggplot(df) +\n      geom_tile(aes(x, y, fill = anom2)) +\n      geom_sf(data = med, fill = NA, colour = \"white\", size = .1) +\n      geom_point(data = med_anom,\n             aes(x = pos_global[1,1], y = pos_global[1,2], fill = mean),\n             size = 3.5, shape = 21, colour = \"white\") +\n      geom_text(data = med_anom,\n                aes(x = pos_global[1,1], y = pos_global[1,2], \n                    label = number(mean, .1, style_positive = \"plus\")),\n                size = 3.5, nudge_x = 700000, colour = \"white\") +\n scale_fill_gradientn(colours = rdbu_pal,\n                       na.value = NA, values = rescale(c(-2, 0, 2)),\n                       limits = c(-2, 2),\n                       breaks = c(-2, -1.5, -1, -0.5, 0, \n                                  .5, 1, 1.5, 2),\n                       labels = c(\"< -2.0\", \"-1.5\", \"-1.0\", \"-0.5\", \"0.0\", \n                                  \"0.5\", \"1.0\", \"1.5\", \"> 2.0\")) +\n  guides(fill = guide_colorbar(barwidth = 20, \n                               barheight = .5)) +\n  facet_wrap(yr ~ ., \n             ncol = 10,\n             labeller = labeller(yr = function(lab){ \n               ifelse(lab %in% c(\"1980\", \"1981\"), \"\", lab)})) \n# paso 2\ng <- g + labs(title = \"ANOMALÍA ESTIVAL DE LA TEMPERATURA DE SUPERFICIE DEL\\nMar Mediterráneo\", subtitle = \"Periodo de referencia 1982-2010.\", fill = \"\") +\n  coord_sf(crs = 3035, clip = \"off\") +\n   theme_SST_facet()"},{"path":"cambioclimatico.html","id":"mapa-de-orientación","chapter":"Capítulo 56 Un dato sobre el cambio climático","heading":"56.4.4 Mapa de orientación","text":"través del paquete giscoR se obtienen los límites administrativos, de los que únicamente se queda con una selección. También se limita la extensión aproxidamente la de la cuenca mediterreánea. La función ms_innerlines() del paquete rmapshaper facilita la obtención de los límites compartidos o interiores de los países selecionados. Los nombres de los países, en forma de código ISO-3, se incluyen con ayuda de geom_sf_text().","code":"\n# límites de países\ncountries_med <- gisco_get_countries() |> \n                    filter(ISO3_CODE %in% c(\"ESP\", \"MAR\", \"FRA\", \"ITA\",\n                                            \"GRC\", \"TUR\", \"DZA\", \"TUN\",\n                                            \"LBY\", \"EGY\", \"ALB\")) |> \n                     st_crop(xmin = -6, xmax = 36, ymin = 28, ymax = 45)\n\n# límites inernos\ninnerlimit <- ms_innerlines(countries_med)\n\n# mapa \ninsetp <- ggplot() +\n            geom_sf(data = med, size = .4, colour = NA, fill = \"grey90\") +\n            geom_sf(data = innerlimit, size = .2, colour = \"white\") +\n            geom_sf_text(data = countries_med, \n                          aes(label = ISO3_CODE), \n                         size = 2, colour = \"white\", fontface = \"bold\", nudge_y = .1) +\n      coord_sf(crs = 3035, expand = FALSE) +\n      theme_void() +\n      theme(plot.background = element_blank(),\n            panel.background = element_blank()) "},{"path":"cambioclimatico.html","id":"exportar-mapa-final","chapter":"Capítulo 56 Un dato sobre el cambio climático","heading":"56.4.5 Exportar mapa final","text":"El mapa de orientación se insertará en el gráfico, como elemento adicional, en la esquina derecha-arriba. El paquete patchwork puede ayudar crear composiciones de distinos gráficos. La función empleada inset_element() indica la posición relativa en xmin, ymin, xmax, y ymax. Es importante recordar que cualquier modificación del tamaño de impresión (veáse height y width en ggsave()), puede llevar ajustes en la posición. El argumento align_to = “full” permite posicionar sobre todo el lienzo.El resultado final, como gráfico de múltiples mapas, puede verse en la Fig. 56.2.\nLos mapas muestran claramente el efecto del calentamiento global, siendo los año 2003 y 2022 de mayor anomalía positiva. Destaca el hecho de que ha habido un año con temperaturas más bajas de lo normal desde el año 1997.\nFigura 56.2: Anomalía estival de la temperatura de superficie del mar\n","code":"\n# paso 3\np_final <- g + inset_element(insetp, 0, .75, .25, .95,\n                             align_to = \"full\")\n\nggsave(\"sst_anom_med2.png\", \n       p_final, \n       bg = \"grey10\",\n       height = 10, \n       width = 20, \n       unit = \"in\",\n       type = \"cairo-png\",\n       dpi = 400)"},{"path":"cap-ree.html","id":"cap-ree","chapter":"Capítulo 57 Predicción de consumo eléctrico con redes neuronales","heading":"Capítulo 57 Predicción de consumo eléctrico con redes neuronales","text":"Jose Manuel Sanz CandalesRed Eléctrica de España","code":""},{"path":"cap-ree.html","id":"introducción-27","chapter":"Capítulo 57 Predicción de consumo eléctrico con redes neuronales","heading":"57.1 Introducción","text":"\nRed Eléctrica, como Operador del Sistema, tiene como principal misión garantizar la continuidad del suministro eléctrico en España. Para ello, entre otras muchas tareas, se desarrollan, evolucionan y mantienen algoritmos de previsión del consumo eléctrico y de la producción con las principales energías renovables (eólica y solar) para distintos horizontes (próximas horas, días, meses, años, etc.) y distintas escalas temporales (anual, horario, quinceminutal).Este caso de uso se sitúa en el departamento de Ciencia de Datos del Operador del Sistema. Es el principio del año 2018, y el área de planificación de la empresa solicita una predicción de el consumo eléctrico en España para el año actual y el siguiente (2018 y 2019).IMPORTANTE: Este desarrollo está previsto en el presupuesto del año, por lo que tanto el software como los datos de entrada deben ser, ser posible, gratuitos.","code":""},{"path":"cap-ree.html","id":"ree-datos","chapter":"Capítulo 57 Predicción de consumo eléctrico con redes neuronales","heading":"57.2 Datos de entrada","text":"Respecto los datos de entrada para el modelo, se requiere tanto una serie histórica de la variable predecir así como de otras variables que sean capaces de explicar adecuadamente el comportamiento del consumo eléctrico. En este caso es necesario utilizar un modelo de aprendizaje supervisado de regresión, dado que el consumo eléctrico es una variable numérica continua.En cuanto la serie histórica de consumo eléctrico anual, Red Eléctrica, en su Web corporativa, publica datos estadísticos accesibles de forma abierta. Sin embargo, el histórico publicado comienza en 2012 y sería conveniente tener un periodo de tiempo más amplio para un entrenamiento adecuado de los modelos potencialmente candidatos ser utilizados. Se realiza una búsqueda de otras fuentes y, afortunadamente, se encuentra que el Instituto para la Diversificación y Ahorro de la Energía (en adelante IDAE) publica datos desde 1990, con agregación anual, del consumo final de energía eléctrica en miles de toneladas equivalentes de petróleo -ktep-.Como los datos se deben entregar en MWh, las unidades de la predicción resultante se tendrán que convertir con el coeficiente que indican en la Web del IDAE (1 MWh = 0,086 tep), pero en los modelos se utilizarán las unidades originales porque más adelante se comprobará que dichas unidades resultan muy útiles para ver cómo se relaciona el consumo eléctrico con la variable predictora que se va utilizar.De este modo se consigue, por tanto, una parte de los datos necesarios para entrenar los modelos predictivos: la serie histórica de nuestra variable target (o variable predecir).Para completar el conjunto de datos del modelo se necesitan, además, las features o variables explicativas. Se sabe que, históricamente, las variaciones interanuales de el consumo eléctrico dependen del comportamiento de la economía de una forma directa: si la economía crece, también crece el consumo eléctrico. Como indicador del comportamiento de la economía se decide tomar el PIB per cápita que se puede encontrar en el siguiente enlace, disponible de forma pública en Expansión - datos macro: https://datosmacro.expansion.com/pib/espana.Adicionalmente, se utilizarán otras dos variables explicativas, relacionadas con el mercado inmobiliario y con el empleo, respectivamente. Dado que son públicas, están anonimizadas y escaladas entre 0 y 1 (dividiendo todos los valores de cada variable entre el mayor de su serie). Debido que se dispone de datos de estas dos variables desde el año 2000, el dataset comienza en este año.Una vez definido el dataset de entrada para los modelos (como se verá continuación, es un conjunto de datos muy pequeño y sencillo), se puede comenzar construir el modelo en R.","code":""},{"path":"cap-ree.html","id":"modelización-1","chapter":"Capítulo 57 Predicción de consumo eléctrico con redes neuronales","heading":"57.3 Modelización","text":"En la siguiente celda de código se lee el conjunto de datos, consumoelectricoanual_2, del paquete CDR, se convierte al formato data.table data.frame y se visualizan sus primeras 3 filas:En este caso, ya se dispone de los datos reales de 2018 y 2019 (esto permitirá validar la precisión del modelo), pero en un caso real, principios de 2018 el PIB per cápita de 2018 y 2019 será una predicción. Lógicamente, el consumo eléctrico anual también será desconocido, ya que es lo que se necesita predecir. Es decir, se supone que los datos de consumo eléctrico de 2018 y 2019 para el modelo que se va construir existen, y se pueden utilizar ni para entrenar ni para evaluar la precisión del modelo (para ello habrá que utilizar datos pasados).La siguiente celda de código proporciona la matriz de varianzas-covarianzas de las variables PIB, Consumo, Inmob, Empleo:Como se puede observar en la matriz anterior, la correlación entre la variable predecir y las distintas variables explicativas es fuerte y positiva (es decir, cuando crece una también crece la otra). Si, además, se visualiza la gráfica entre PIB y Consumo en el tiempo, se apreciará de forma aún más clara esta intensa correlación:\nFigura 57.1: Evolución del PIB y el Consumo\nEn la Fig. 57.1 se observa que las curvas que representan la evolución temporal de ambas variables están prácticamente superpuestas, pero desde 2006 líneas se separan. ¿qué puede deberse? Uno de los principales motivos probablemente sean las medidas de eficiencia energética que se han ido introduciendo en los últimos lustros (iluminación led, electrodomésticos, dispositivos con menor consumo, etc.).Una vez se han explorado los datos (en este caso ha sido muy breve, pero es muy habitual en proyectos reales que la exploración y limpieza de los datos requiera en torno al 80% del tiempo), se procederá dividir el conjunto de datos (desde 2000 hasta 2017, ya que 2018 y 2019 son los años los que se pretende dar respuesta, por lo que se supone que es conocido todavía) en dos partes:entrenamiento+validación (90% de las filas)test (10% restante)Previamente esto, se debe escalar también la variable PIB valores entre 0 y 1, para que la red neuronal funcione de forma correcta:Ahora se deberían probar distintos modelos de machine learning y comparar sus resultados para determinar cuál es el más preciso para este conjunto de datos. En este ejemplo, por simplicidad se incluye este proceso de prueba y comparación entre distintos modelos, que en el caso de uso real da lugar elegir una red neuronal simple o perceptrón multicapa (véase Cap. 36), también conocido por su acrónimo en inglés MLP (Multi Layer Perceptron), que se utilizará más adelante en este capítulo al obtener los mejores resultados.Para elegir los hiperparámetros que mejor resultado obtienen para el modelo se van utilizar dos técnicas que son grid search (para probar distintas combinaciones de hiperparámetros) y cross validation (para entrenar y validar aprovechando todos los registros del conjunto de entrenamiento-validación).En este caso de uso, se van hacer distintas pruebas combinando el número de neuronas por cada capa oculta. En concreto, en la primera y la segunda capa oculta se va dejar un número constante de neuronas (5), y es en la tercera capa oculta donde se va probar con 4 neuronas y 6 neuronas. Es decir, se entrenará un modelo con 5 neuronas en cada capa oculta y otro con 5 neuronas en las dos primeras capas y 6 neuronas en la tercera capa.En la siguiente celda, se importan los paquetes necesarios (neuralnet y caret), se construye la estructura de la red en la variable ‘grid’ y se define el número de folds (en cuántas partes se divide en conjunto de entrenamiento para entrenar y validar con todos los datos del conjunto) de la validación cruzada. Por último, se entrena el modelo.El proceso está muy simplificado para que sea fácil de entender. obstante, lo habitual en la práctica es probar más opciones de grid search y hacer una división mayor del conjunto de datos para cross validation (es bastante habitual entre 5 y 10 folds):Para mostrar los resultados se aplica la función ‘print’ sobre la variable ‘model’, cuya salida es el texto comentado debajo de la línea de ‘print’.El modelo con mejor resultado (menor error en el conjunto de entrenamiento / validación) es el que tiene 3 capas con 5 neuronas cada una. Con este modelo, se predice el consumo eléctrico con el modelo entrenado para la parte del conjunto de datos que se habían reservado para test (años 2006 y 2007), para comprobar que el modelo generaliza bien (es decir, para datos nuevos los resultados de las predicciones tienen un error del orden de los que resultan del entrenamiento del modelo). Para ello, se calcula, por ejemplo, el MAE de las predicciones para dicho conjunto de test:El modelo seleccionado ya está listo para realizar predicciones de consumo eléctrico para los años solicitados (2018 y 2019). Como se avanzó anteriormente, el objetivo del conjunto de test es comprobar la precisión del modelo con datos totalmente desconocidos para él (es decir, utilizados en la fase de entrenamiento-validación), principalmente para asegurar que el modelo funciona bien para datos distintos los utilizados en el entrenamiento.Para predecir el consumo eléctrico anual para 2018 y 2019, que es el dato que solicitaron desde el área de planificación de la empresa. Simplemente se utiliza la función predict() del modelo. Previamente, añade una columna en el conjunto de datos original -df- que contendrá los valores predichos, para más adelante comprobar gráficamente el valor predicho frente al real que, en este caso ficticio, ya es conocido:Ahora se hace la predicción del año 2018 y se añade el resultado esta nueva columna del conjunto de datos:Se hace también la predicción para el año 2019 y se visualizan las predicciones añadidas al conjunto de datos para ambos años:Estos son los datos que se entregarían como resultado de la petición de información (convertidos MWh aplicando el coeficiente que se mencionó en la Secc. \\(\\ref{ree-datos}\\)).Claro está, en el momento que se entregan las predicciones para 2018 y 2019 todavía se sabría cómo de precisas han sido, pero principios de 2020 sí es posible calcular la bondad del modelo seleccionado, y es lo que se hará en las siguientes celdas:\nFigura 57.2: Consumo y predicción del modelo de red neuronal MLP\nEn la Fig. 57.2, las predicciones (puntos azules) tienen unos errores del orden de los que se habían visto en el conjunto de test cuando se hizo el entrenamiento de los modelos, por lo que parece que hay sobreentrenamiento en el modelo.","code":"\nlibrary(CDR)\ndf <- CDR::consumoelectricoanual_2\nclass(df) <- class(as.data.frame(df))\nhead(df, 3)\n#>    Año   PIB Consumo     Inmob    Empleo\n#> 1 2000 15.97  16.205 0.7525093 0.6561190\n#> 2 2001 17.20  17.279 0.7687356 0.6832698\n#> 3 2002 18.09  17.671 0.7836143 0.7104178\ncormat <- round(cor(df[c(\"PIB\", \"Consumo\", \"Inmob\", \"Empleo\")]), 2)\nhead(cormat)\n#>          PIB Consumo Inmob Empleo\n#> PIB     1.00    0.81  0.90   0.93\n#> Consumo 0.81    1.00  0.64   0.71\n#> Inmob   0.90    0.64  1.00   0.99\n#> Empleo  0.93    0.71  0.99   1.00\nlibrary(\"ggplot2\")\nlibrary(\"reshape2\")\ndf_m <- melt(df[c(\"Año\",\"PIB\",\"Consumo\")], id.vars = \"Año\")\noptions(repr.plot.width = 15, repr.plot.height = 8)\nggplot(df_m, aes(Año, value, col = variable)) +\n  geom_line(size = 2.5)\n# estandarizar la variable explicativa \"PIB\" entre 0 y 1\ndf$PIB = df$PIB/max(df$PIB)\nset.seed(123)\ndf_aux <- df[df$Año < 2018, ]\nn <- nrow(df_aux)\ntrainIndex <- sample(1:n, size = round(0.85 * n), replace = FALSE)\ndf_train <- df_aux[trainIndex, ]\ndf_test <- df_aux[-trainIndex, ]\ndf_test\n# lee paquetes\nlibrary(\"neuralnet\")\nlibrary(\"caret\")\n# define la estructura de la red\ngrid <- expand.grid(layer1 = c(5), layer2 = c(5), layer3 = c(4,5))\n# establece semilla para que los resultados del entrenamiento sean siempre los mismos\nset.seed(123)\n# define el número de folds en validación cruzada\ntrain_control <- trainControl(method = \"cv\",\n                              number = 2,\n                              verbose = TRUE)\n# entrenar el modelo\nmodel <- train(Consumo ~ PIB+Inmob+Empleo,\n               data = df_train,\n               trControl = train_control,\n               method = \"neuralnet\",\n               tuneGrid = grid)\nprint(model)\n#> Neural Network\n#>\n#> 15 samples\n#> 3 predictor\n#>\n#> No pre-processing\n#> Resampling: Cross-Validated (2 fold)\n#> Summary of sample sizes: 8, 7\n#> Resampling results across tuning parameters:\n#>\n#>  layer3  RMSE       Rsquared  MAE      \n#>  4       0.9713547  0.7631649 0.8521702\n#>  5       0.9452518  0.72532010.8165410\n#>\n#> Tuning parameter 'layer1' was held constant at a value of 5\n#> Tuning parameter 'layer2' was held constant at a value of 5\n#> RMSE was used to select the optimal model using the smallest value.\n#> The final values used for the model were layer1 = 5, layer2 = 5 and layer3 = 5.\ndf_test[c(\"Año\", \"Consumo\")]\n#> Año     Consumo\n#> 7  2006 21.163\n#> 8  2007 21.564\n#> 18 2017 20.559\n\npredict(model, df_test)\n#>       7        8       18\n#>   21.50816 21.83445 20.12596\n\nMAE_test = (abs(21.163-21.50816)+abs(21.564-21.83445)+abs(20.559-20.12596))/3\nMAE_test\ndf[\"Prediccion_MLP\"] <- NA\ndf_pred_2018 <- df[df$Año == 2018, ]\ndf$Prediccion_MLP[df$Año == 2018] <- predict(model, df_pred_2018[c(\"PIB\",\"Inmob\",\"Empleo\")])\ndf_pred_2019 <- df[df$Año == 2019, ]\ndf$Prediccion_MLP[df$Año == 2019] <- predict(model, df_pred_2019[c(\"PIB\",\"Inmob\",\"Empleo\")])\ntail(df, 3)\n#>    Año Consumo Prediccion_MLP\n#> 18 2017 20.559          NA\n#> 19 2018 20.504    20.22787\n#> 20 2019 20.166    20.16409\ndf_m_mlp <- melt(df[c(\"Año\",\"Consumo\",\"Prediccion_MLP\")], id.vars = \"Año\")\nggplot(df_m_mlp, aes(Año, value, col = variable)) +  \n  geom_point(size = 2) + \n  geom_line()"},{"path":"cap-sist-exp.html","id":"cap-sist-exp","chapter":"Capítulo 58 Implementación de un sistema experto en el ámbito pediátrico","heading":"Capítulo 58 Implementación de un sistema experto en el ámbito pediátrico","text":"Arturo Peralta\\(^{,b}\\), José Ángel Olivas\\(^{}\\) y Eusebio Angulo\\(^{}\\)\\(^{}\\)Universidad Internacional de Valencia, \\(^{b}\\)Universidad Internacional de la Rioja","code":""},{"path":"cap-sist-exp.html","id":"introducción-28","chapter":"Capítulo 58 Implementación de un sistema experto en el ámbito pediátrico","heading":"58.1 Introducción","text":"\nSin lugar duda, el análisis de situaciones complejas para la evaluación y toma de decisiones es un proceso para el que tradicionalmente se requiere el apoyo de un especialista dispuesto poner en uso todo su conocimiento. Sin embargo, el desarrollo de sistemas automáticos capaces de modelar el conocimiento que un experto podría tener sobre un ámbito concreto, y de procesarlo para alcanzar una respuesta adecuada una consulta relacionada, resulta cada día más extendido como mecanismo de ayuda. este tipo de herramientas se les denomina Sistemas Expertos (SE).En este capítulo se introducen los conceptos teóricos fundamentales de la Ingeniería del Conocimiento, los componentes y el funcionamiento de los SE para, posteriormente, presentar cómo su aplicación puede apoyar en el proceso de evaluación clínica en el ámbito pediátrico de atención primaria.\nFinalmente, se incluye una sencilla implementación en R del SE enfocado la ayuda en esta problemática.","code":""},{"path":"cap-sist-exp.html","id":"marco-teórico","chapter":"Capítulo 58 Implementación de un sistema experto en el ámbito pediátrico","heading":"58.2 Marco teórico","text":"Un Sistema Experto (SE) es un programa de ordenador que trata de emular el comportamiento de una persona experta en un dominio de conocimiento específico ante un problema que se plantee en dicho dominio y cómo llega su solución.La Ingeniería del Conocimiento se ocupa, entre otras cosas, del proceso de especificación, análisis y desarrollo de un sistema experto (Martínez R. 2005).Los principales componentes de un SE son:\n() La Base de Hechos (BH), que contiene la definición del entorno sobre el que se van resolver problemas. Hace el papel de “ojos” del SE.\n(ii) La Base de Conocimientos (BC), que contiene la información del dominio específico, convenientemente representado, capaz de resolver problemas. También puede considerar la representación de incertidumbre.\n(iii) El Motor de Inferencias, que es el proceso de razonamiento que usa el SE. Combina hechos y conocimiento para emitir una conclusión.\n(iv) El Interfaz de entrada/salida para comunicarse con los usuarios y/o expertos.En la Fig.\n58.1 se muestra un diagrama con los principales componentes de un SE.\nFigura 58.1: Componentes de un SE\nLas principales limitaciones en la construcción de SE vienen dadas porque el conocimiento experto humano es experiencia compilada, es heurístico, esto es, basado en experiencia y en reglas prácticas. Es incompleto, impreciso e incierto, y veces inconsistente y con errores o imprecisiones.\nEs por ello que las limitaciones de todo SE pueden ser que conoce lo que conoce ni por qué, carece de imaginación, emociones, inteligencia innata, sentido común, etc. Tiene poco conocimiento de sí mismo, del usuario y del contexto de cada interacción y capacidad de razonamiento limitada por su estrategia de construcción.En este contexto, los sistemas de producción son modelos de cálculo que han probado su eficiencia en la Ingeniería del Conocimiento tanto en el desarrollo de algoritmos de búsqueda como en el modelado de problemas del dominio humano. Sus componentes principales son:Las reglas de producción: son la forma más extendida de representar el conocimiento, constan de Condiciones (Hipótesis) y Acciones (Conclusiones) y tienen la forma:Es la forma más extendida de representar el conocimiento. Ejemplo de Regla de producción:Memoria de trabajo: contiene una descripción del estado actual del mundo o entorno de la aplicación en cada paso del proceso de razonamiento.\nEsta descripción es un modelo que servirá para asociar los antecedentes de las reglas con las observaciones del mundo, con el objetivo de seleccionar o producir las acciones apropiadas. En el momento en que se cumplen todas las condiciones de una regla se produce el “disparo” de la misma, ejecutándose la acción. Esta operación alterará el contenido de la memoria de trabajo.Memoria de trabajo: contiene una descripción del estado actual del mundo o entorno de la aplicación en cada paso del proceso de razonamiento.\nEsta descripción es un modelo que servirá para asociar los antecedentes de las reglas con las observaciones del mundo, con el objetivo de seleccionar o producir las acciones apropiadas. En el momento en que se cumplen todas las condiciones de una regla se produce el “disparo” de la misma, ejecutándose la acción. Esta operación alterará el contenido de la memoria de trabajo.Ciclo de reconocimiento y actuación: es el procedimiento de control de un sistema de producción. Es un procedimiento de feedforward o hacia adelante. La memoria de trabajo se inicializa con la descripción del problema. Los modelos almacenados en la memoria de trabajo se tratan de superponer en las condiciones de las producciones. Tras ello, se crea un conjunto “conflicto”, es decir, un subconjunto de producciones cuyas condiciones se cumplen. Se escoge una producción y se “dispara” o se activa. La acción de la regla es “disparada” cambiando el contenido de la memoria de trabajo. Se repite todo el proceso descrito con la memoria de trabajo modificada. El proceso continúa hasta que haya condiciones en las reglas que cumplan el contenido de los modelos de la memoria de trabajo.Ciclo de reconocimiento y actuación: es el procedimiento de control de un sistema de producción. Es un procedimiento de feedforward o hacia adelante. La memoria de trabajo se inicializa con la descripción del problema. Los modelos almacenados en la memoria de trabajo se tratan de superponer en las condiciones de las producciones. Tras ello, se crea un conjunto “conflicto”, es decir, un subconjunto de producciones cuyas condiciones se cumplen. Se escoge una producción y se “dispara” o se activa. La acción de la regla es “disparada” cambiando el contenido de la memoria de trabajo. Se repite todo el proceso descrito con la memoria de trabajo modificada. El proceso continúa hasta que haya condiciones en las reglas que cumplan el contenido de los modelos de la memoria de trabajo.Una de las principales ventajas de los sistemas de producción en los SE es la separación del conocimiento y del control. Se pueden hacer cambios fáciles de reglas sin cambiar el control y viceversa. Otra es la modularidad de las reglas de producción y la independencia del lenguaje de programación usado.","code":"    Si (If)\n      Condición 1 \n      y Condición 2 \n      ... \n      y Condición n\n    Entonces (Then)\n      Conclusión 1\n      Conclusión 2\n      ...\n      y Conclusión m    Si \n      ha fallado la bombilla \n      y hay una de repuesto \n      y está útil\n    Entonces\n      cambiar la bombilla por la de repuesto\n      y seguir trabajando"},{"path":"cap-sist-exp.html","id":"razonamiento","chapter":"Capítulo 58 Implementación de un sistema experto en el ámbito pediátrico","heading":"58.2.1 Razonamiento","text":"El razonamiento se define como el proceso de obtención de inferencias o conclusiones partir de unos hechos u observaciones reales o asumidos y de un conocimiento previo. La inferencia es el proceso por el que partir de unos hechos conocidos se obtienen conclusiones acerca de otros desconocidos (Fleitas 2017) y Begu04. La realización de este tipo de procesamiento es llevada cabo por el denominado motor de inferencia.El razonamiento automático ya se utilizaba en los 50 en juegos. En 1963 se presentó el sistema “General Problem Solver” capaz de hacer inferencias lógicas (Newel y Simon).Tipos de razonamiento en SE:Forward chaining (encadenamiento hacia delante, deductivo, progresivo, dirigido por datos o hechos): Síntomas \\(\\rightarrow\\) CausasForward chaining (encadenamiento hacia delante, deductivo, progresivo, dirigido por datos o hechos): Síntomas \\(\\rightarrow\\) CausasBackward chaining (encadenamiento hacia atrás, inductivo, regresivo, dirigido por metas u objetivos): Síntomas \\(\\leftarrow\\) CausasBackward chaining (encadenamiento hacia atrás, inductivo, regresivo, dirigido por metas u objetivos): Síntomas \\(\\leftarrow\\) CausasPasos del motor de inferencia:Elaboración de un conjunto conflicto con todas las reglas cuyas condiciones se cumplen.Elaboración de un conjunto conflicto con todas las reglas cuyas condiciones se cumplen.Detección (filtro) de reglas pertinentes o selección de reglas partir de unos hechos. Se trata de obtener de la Base de Cconocimiento (BC) el conjunto de reglas aplicables en una situación determinada o estado de la Base de Hechos (BH).Detección (filtro) de reglas pertinentes o selección de reglas partir de unos hechos. Se trata de obtener de la Base de Cconocimiento (BC) el conjunto de reglas aplicables en una situación determinada o estado de la Base de Hechos (BH).Aplicación de reglas o resolución del conflicto. Consiste en seleccionar una regla del conjunto conflicto y dispararla (ejecutar su conclusión). Se altera la BH o memoria de trabajo incluyendo el consecuente de la regla “disparada”.Aplicación de reglas o resolución del conflicto. Consiste en seleccionar una regla del conjunto conflicto y dispararla (ejecutar su conclusión). Se altera la BH o memoria de trabajo incluyendo el consecuente de la regla “disparada”.Vuelta 1 hasta que el conjunto conflicto esté vacío.Vuelta 1 hasta que el conjunto conflicto esté vacío.Ciclo de “razonamiento hacia delante”:Parte de unas observaciones (hechos).Parte de unas observaciones (hechos).partir de los hechos observados, se seleccionan las reglas cuyas condiciones están relacionadas con estos.partir de los hechos observados, se seleccionan las reglas cuyas condiciones están relacionadas con estos.Las reglas seleccionadas son examinadas para ver si cumplen todas sus condiciones. Aquellas que las verifican constituyen el “conjunto conflicto”.Las reglas seleccionadas son examinadas para ver si cumplen todas sus condiciones. Aquellas que las verifican constituyen el “conjunto conflicto”.Del total de reglas que forman el conjunto conflicto se selecciona una sola y se activa (se “dispara”). La selección de una regla del conjunto conflicto se denomina “resolución del conflicto”Del total de reglas que forman el conjunto conflicto se selecciona una sola y se activa (se “dispara”). La selección de una regla del conjunto conflicto se denomina “resolución del conflicto”La activación de la regla provocará la aparición de otros hechos que se añaden los observados y se actualiza la base de hechos.La activación de la regla provocará la aparición de otros hechos que se añaden los observados y se actualiza la base de hechos.Volver al paso 2 hasta analizar todos los hechos observados y deducidos.Volver al paso 2 hasta analizar todos los hechos observados y deducidos.En la Fig.\n58.2 se muestra el algoritmo correspondiente al ciclo de inferencia hacia adelante.\nFigura 58.2: Ciclo de razonamiento hacia delante\n","code":""},{"path":"cap-sist-exp.html","id":"sistema-experto-para-el-ámbito-pediátrico-en-atención-primaria","chapter":"Capítulo 58 Implementación de un sistema experto en el ámbito pediátrico","heading":"58.3 Sistema experto para el ámbito pediátrico en atención primaria","text":"En la actualidad, uno de los principales problemas los que se enfrentan los profesionales de la sanidad en el ámbito pediátrico de atención primaria en España es la falta del tiempo suficiente para realizar una evaluación clínica del estado del paciente. La necesidad de un mayor número de médicos especialistas y la aparición de picos de demanda motivados fenómenos como el COVID, o de modo estacional por otras enfermedades recurrentes, favorecen esta situación.Ante esta problemática, los centros de salud tratan de optimizar sus recursos mediante diferentes vías, poniendo especial interés en realizar procesos de triaje que les permitan priorizar la atención los pacientes según su nivel de urgencia. Para ello, en España se utilizan escalas como el MTS (Manchester Triage System), el SET (Sistema Español de Triaje) y el CPTAS (Canadian Pediatric Triage Acuity Scale) [Soler2010] para establecer el tiempo que un paciente puede esperar para recibir atención médica en base sus síntomas y evolución.Sin embargo, realizar un correcto proceso de triaje, además de requerir de un gran conocimiento experto, hace necesario un tiempo para una evaluación clínica que veces resulta difícil dedicar. En este contexto, se plantea el desarrollo de un SE capaz de ayudar en el proceso de evaluación médica, con el objetivo de facilitar el proceso de triaje.El primer paso para el desarrollo de un SE es la definición de un conjunto de reglas que modelen el conocimiento con el que se nutrirá. Para ello, es posible recurrir al apoyo de expertos, capaces de definir su conocimiento como reglas, o la aplicación de mecanismos de extracción de conocimiento partir del procesamiento de conjuntos de datos y sucesos.En este ejemplo, se recopilaron y procesaron un conjunto de datos relativos los motivos de consulta pediátrica en un centro de salud, donde la escala de triaje utilizada fue el CPTAS. Los datos fueron anonimizados, seleccionando únicamente aquellos campos que pudieran resultar clave para la extracción de conocimiento y la conformación de reglas. Adicionalmente, se contó con el apoyo de profesionales especialistas del ámbito pediátrico para revisar y complementar algunas de las reglas extraidas. Un extracto de los datos utilizados para el proceso de extracción de reglas se muestra en la Tabla 58.1.Tabla 58.1: . Ejemplo de datos de motivos de consulta y triaje.partir del procesamiento de un total de 400 visitas médicas mediante un algoritmo de extracción de reglas de asociación como “Magnum Opus”, basado en la definición original de Webb (2011), y mediante la aplicación del conocimiento de experto proporcionado por un panel de pediatras, se extrajeron un conjunto de reglas con suficiente calidad. continuación, se muestra un ejemplo de un conjunto de reglas con \\(10\\) de ellas.R1: Si Causa Ginecologica o Edad mayor de 12 años \\(\\rightarrow\\) Tiempo de Evolución mayor de 73hR1: Si Causa Ginecologica o Edad mayor de 12 años \\(\\rightarrow\\) Tiempo de Evolución mayor de 73hR2: Si Causa Ginecologica \\(\\rightarrow\\) Sexo MujerR2: Si Causa Ginecologica \\(\\rightarrow\\) Sexo MujerR3: Si Edad menor de 7 días o Causa Fiebre \\(\\rightarrow\\) Tiempo de evolución de 1hR3: Si Edad menor de 7 días o Causa Fiebre \\(\\rightarrow\\) Tiempo de evolución de 1hR4: Si Edad mayor de 12 años y Tiempo de Evolución mayor de 73h \\(\\rightarrow\\) Causa RespiratoriaR4: Si Edad mayor de 12 años y Tiempo de Evolución mayor de 73h \\(\\rightarrow\\) Causa RespiratoriaR5: Si Tiempo Evolución mayor de 73h y Causa Ocular \\(\\rightarrow\\) Triaje 1R5: Si Tiempo Evolución mayor de 73h y Causa Ocular \\(\\rightarrow\\) Triaje 1R6: Si Causa Respiratoria y Sexo Mujer \\(\\rightarrow\\) Triaje 3R6: Si Causa Respiratoria y Sexo Mujer \\(\\rightarrow\\) Triaje 3R7: Si Tiempo de Evolución es 2-6h o Causa Neurológica \\(\\rightarrow\\) Triaje 2R7: Si Tiempo de Evolución es 2-6h o Causa Neurológica \\(\\rightarrow\\) Triaje 2R8: Si Causa Respiratoria y Tiempo de Evolución es 2-6h \\(\\rightarrow\\) Triaje 4R8: Si Causa Respiratoria y Tiempo de Evolución es 2-6h \\(\\rightarrow\\) Triaje 4R9: Si Tiempo de Evolución es 13-24h y Causa Ginecológica y Sexo Mujer \\(\\rightarrow\\) Triaje 5R9: Si Tiempo de Evolución es 13-24h y Causa Ginecológica y Sexo Mujer \\(\\rightarrow\\) Triaje 5R10: Si Tiempo de Evolución mayor de 73h \\(\\rightarrow\\) Triaje 4R10: Si Tiempo de Evolución mayor de 73h \\(\\rightarrow\\) Triaje 4A continuación, se muestra el código en R para la implementación de un SE capaz de procesar reglas como las anteriores, realizando una ejecución para obtener el valor de triaje correspondiente. Para este ejemplo se considera una niña mayor de \\(12\\) años de edad, cuyo motivo de consulta es ginecológico con un tiempo de evolución superior \\(73\\) horas.Es importante señalar que, habitualmente, un SE partirá de una base de hechos compuesta por decenas o cientos de reglas. obstante, para simplificar el siguiente fragmento de código, se considera únicamente la carga de \\(10\\), reglas modo de ejemplo.Se declaran las reglas que conformarán la base conocimiento del SE:La BC del SE contiene 10 reglas.La BC del SE contiene 10 reglas.Cada regla se modela como una lista de antecedentes y un consecuente.Cada regla se modela como una lista de antecedentes y un consecuente.La relación entre los consecuentes se modela con el atributo “operador” del siguiente modo: 1 = Y lógico, 0 = O lógico, -1 = hay operacionesLa relación entre los consecuentes se modela con el atributo “operador” del siguiente modo: 1 = Y lógico, 0 = O lógico, -1 = hay operacionesSe inicializa la BC con el conjunto de Reglas y la BH con la circunstancia evaluar.Se define una función para comprobar la existencia de un número en una lista. Esta función será usada por el motor del SE.Se implementa el motor del SE.El algoritmo ejecuta un bucle en el que, en cada iteración, evalúa las reglas disponibles contenidas en la BC. Considerando los items de la BH, si una regla puede ser “disparada”, se añade al Conjunto Conflicto. La regla “disparada” en una iteración será la primera disponible en el Conjunto Conflicto. El Conjunto Conflicto se inicializa en cada iteración. El consecuente de la regla “disparada” se añade la BH. Cada regla solo puede “dispararse” una vez, por lo que se actualiza una lista de reglas “disparadas”. El algoritmo finaliza cuando el Conjunto Conflicto queda vacío, al haber sido “disparadas” todas las reglas o existir más candidatas ser “disparadas”.Ahora considerese, por ejemplo, una posible paciente con más de 12 años de Edad, que acude consulta por causa Ginecológica con un Tiempo de Evolución de los síntomas mayor de 73h. ¿Cuál será el triaje correspondiente? Para conocerlo, se inicializa la BH con la situación propuesta (Causa_Ginecologica, Edad_>12a y TiempoEvolucion_>73h) y se ejecuta el motor del SE.El resultado del algoritmo, en este caso (motivo de consulta ginecológico y mayor de \\(12\\) años y con un tiempo de evolución de los síntomas mayor de 73h), es un triaje de valor \\(4\\). Es decir, el tiempo de espera máximo para recibir atención médica debería ser inferior \\(60\\) minutos. La Tabla 58.2 muestra el proceso realizado por el algoritmo en cada iteración.Table: (#tab:TablaIteracionesAlgoritmo). Proceso de ejecución del Sistema Experto.Como puede observarse, el algoritmo finaliza tras \\(5\\) iteraciones, al alcanzar un conjunto conflicto vacío, dando como resultado un valor de \\(4\\) para el triaje. continuación, se describe el proceso ejecutado en cada una de las iteraciones.Iteración 0: La Base de Hechos se inicializa con las condiciones establecidas en el ejemplo de consulta médica considerado, es decir, Causa_Ginecologica, Edad_>12a y TiempoEvolucion_>73h. En el Conjunto Conflicto se incluyen aquellas reglas que podrían ser lanzadas con los elementos contenidos en la BH es decir, las reglas R1, R2, R4, R10. Se dispara la regla R1, al ser la primera de la lista de reglas del Conjunto Conflicto. El consecuente de la regla disparada (TiempoEvolucion_>73h) se añade la BH para la siguiente iteración, aunque en este caso es necesario porque ya lo contiene. La iteración finaliza estableciendo como conclusión el consecuente de la regla disparada, es decir, TiempoEvolucion_>73h.Iteración 0: La Base de Hechos se inicializa con las condiciones establecidas en el ejemplo de consulta médica considerado, es decir, Causa_Ginecologica, Edad_>12a y TiempoEvolucion_>73h. En el Conjunto Conflicto se incluyen aquellas reglas que podrían ser lanzadas con los elementos contenidos en la BH es decir, las reglas R1, R2, R4, R10. Se dispara la regla R1, al ser la primera de la lista de reglas del Conjunto Conflicto. El consecuente de la regla disparada (TiempoEvolucion_>73h) se añade la BH para la siguiente iteración, aunque en este caso es necesario porque ya lo contiene. La iteración finaliza estableciendo como conclusión el consecuente de la regla disparada, es decir, TiempoEvolucion_>73h.Iteración 1: El Conjunto Conflicto se inicializa con las reglas que podrían ser lanzadas, excluyendo las ya ejecutadas (R1), partir de los elementos incluidos en la Base de Hechos tras la iteración anterior, es decir, las reglas R2, R4 y R10. Se lanza la primera de las reglas del Conjunto Conflicto, es decir, R2. El consecuente de la regla disparada (Sexo_Mujer) se añade la BH. La iteración finaliza estableciendo como conclusión el consecuente de la regla disparada, es decir, Sexo_Mujer.Iteración 1: El Conjunto Conflicto se inicializa con las reglas que podrían ser lanzadas, excluyendo las ya ejecutadas (R1), partir de los elementos incluidos en la Base de Hechos tras la iteración anterior, es decir, las reglas R2, R4 y R10. Se lanza la primera de las reglas del Conjunto Conflicto, es decir, R2. El consecuente de la regla disparada (Sexo_Mujer) se añade la BH. La iteración finaliza estableciendo como conclusión el consecuente de la regla disparada, es decir, Sexo_Mujer.Iteración 2: El Conjunto Conflicto se inicializa con las reglas que podrían ser lanzadas, excluyendo las ya ejecutadas (R1 y R2), partir de los elementos contenidos en la BH tras la iteración anterior, es decir, las reglas R4 y R10. Se lanza la primera de las reglas del Conjunto Conflicto, es decir, R4. El consecuente de la regla disparada (Causa_Respiratoria) se añade la BH. La iteración finaliza estableciendo como conclusión el consecuente de la regla disparada, es decir, Causa_Respiratoria.Iteración 2: El Conjunto Conflicto se inicializa con las reglas que podrían ser lanzadas, excluyendo las ya ejecutadas (R1 y R2), partir de los elementos contenidos en la BH tras la iteración anterior, es decir, las reglas R4 y R10. Se lanza la primera de las reglas del Conjunto Conflicto, es decir, R4. El consecuente de la regla disparada (Causa_Respiratoria) se añade la BH. La iteración finaliza estableciendo como conclusión el consecuente de la regla disparada, es decir, Causa_Respiratoria.Iteración 3: El Conjunto Conflicto se inicializa con las reglas que podrían ser lanzadas, excluyendo las ya ejecutadas (R1, R2 y R4), partir de los elementos contenidos en la BH tras la iteración anterior, es decir, las reglas R6 y R10. Se lanza la primera de las reglas del Conjunto Conflicto, es decir, R6. El consecuente de la regla disparada (Triaje_3) se añade la BH. La iteración finaliza estableciendo como conclusión el consecuente de la regla disparada, es decir, Triaje_3.Iteración 3: El Conjunto Conflicto se inicializa con las reglas que podrían ser lanzadas, excluyendo las ya ejecutadas (R1, R2 y R4), partir de los elementos contenidos en la BH tras la iteración anterior, es decir, las reglas R6 y R10. Se lanza la primera de las reglas del Conjunto Conflicto, es decir, R6. El consecuente de la regla disparada (Triaje_3) se añade la BH. La iteración finaliza estableciendo como conclusión el consecuente de la regla disparada, es decir, Triaje_3.Iteración 4: El Conjunto Conflicto se inicializa con las reglas que podrían ser lanzadas, excluyendo las ya ejecutadas (R1, R2, R4 y R6), partir de los elementos contenidos en la BH tras la iteración anterior, en este caso únicamente R10. Se lanza por tanto la regla R10. El consecuente de la regla disparada (Triaje_4) se añade la BH. La iteración finaliza estableciendo como conclusión el consecuente de la regla disparada, es decir, Triaje_4.Iteración 4: El Conjunto Conflicto se inicializa con las reglas que podrían ser lanzadas, excluyendo las ya ejecutadas (R1, R2, R4 y R6), partir de los elementos contenidos en la BH tras la iteración anterior, en este caso únicamente R10. Se lanza por tanto la regla R10. El consecuente de la regla disparada (Triaje_4) se añade la BH. La iteración finaliza estableciendo como conclusión el consecuente de la regla disparada, es decir, Triaje_4.Iteración 5: El Conjunto Conflicto queda vacío, al ser posible incluir en él reglas disparadas aún y que pudieran ser ejecutadas partir de los elementos contenidos en la BH. Por tanto, el algoritmo finaliza concluyendo como resultado el consecuente de la última regla lanzadas, es decir, Triaje_4.Iteración 5: El Conjunto Conflicto queda vacío, al ser posible incluir en él reglas disparadas aún y que pudieran ser ejecutadas partir de los elementos contenidos en la BH. Por tanto, el algoritmo finaliza concluyendo como resultado el consecuente de la última regla lanzadas, es decir, Triaje_4.Sin lugar duda, la implementación del algoritmo, las reglas y el modelado considerado para este caso de estudio resulta una simplificación intencionada del problema, con el único objetivo de facilitar su comprensión desde una perspectiva académica y docente.En un escenario de aplicación real, el volumen y la complejidad de las reglas debería ser mayor, considerando además la posibilidad de incorporar otras características que permitieran modelar de un modo más completo el estado de salud del paciente y su motivo de consulta.Actualmente resultan innumerables los ámbitos donde la aplicación de SE puede ser de ayuda. En este caso de estudio, el objetivo ha sido facilitar y mejorar el proceso de triaje del nivel de urgencia en el ámbito pediátrico en atención primaria. Pero, en este mismo contexto, podría valorarse su uso como herramienta de apoyo para, por ejemplo, el diagnóstico de enfermedades o la prescripción de tratamientos.La tecnología actual y lenguajes de programación como R facilitan la implementación de SE de un modo rápido y sencillo. Por tanto, pueden ser considerados como herramienta de ayuda para situaciones en las que aplicar conocimiento experto resulte clave para la solución de problemas.","code":"\nr1 <- list(\n  antecedentes = list(\"Causa_Ginecologica\", \"Edad_>12a\"),\n  consecuente = list(\"TiempoEvolucion_>73h\"), operador = 0\n)\nr2 <- list(\n  antecedentes = list(\"Causa_Ginecologica\"),\n  consecuente = list(\"Sexo_Mujer\"), operador = -1\n)\nr3 <- list(\n  antecedentes = list(\"Edad_<7d\", \"Causa_Fiebre\"),\n  consecuente = list(\"TiempoEvolucion_1h\"), operador = 0\n)\nr4 <- list(\n  antecedentes = list(\"Edad_>12a\", \"TiempoEvolucion_>73h\"),\n  consecuente = list(\"Causa_Respiratoria\"), operador = 1\n)\nr5 <- list(\n  antecedentes = list(\"TiempoEvolucion_>73h\", \"Causa_Ocular\"),\n  consecuente = list(\"Triaje_1\"), operador = 1\n)\nr6 <- list(\n  antecedentes = list(\"Causa_Respiratoria\", \"Sexo_Mujer\"),\n  consecuente = list(\"Triaje_3\"), operador = 1\n)\nr7 <- list(\n  antecedentes = list(\"TiempoEvolucion_2-6h\", \"Causa_Neurologica\"),\n  consecuente = list(\"Triaje_2\"), operador = 0\n)\nr8 <- list(\n  antecedentes = list(\"Causa_Respiratoria\", \"TiempoEvolucion_2-6h\"),\n  consecuente = list(\"Triaje_4\"), operador = 1\n)\nr9 <- list(\n  antecedentes = list(\"TiempoEvolucion_13-24h\", \"Causa_Ginecologica\", \"Sexo_Mujer\"),\n  consecuente = list(\"Triaje_5\"), operador = 1\n)\nr10 <- list(\n  antecedentes = list(\"TiempoEvolucion_>73h\"),\n  consecuente = list(\"Triaje_4\"), operador = -1\n)\n\n# r1 regla completa\n# r1[1] lista antecedentes\n# r1[[1]] [1] primero de los antecedentes\n# r1[2] lista consecuentes\n# r1[[2]] [1] primero de los consecuentes\n\nb_hechos <- list(\"Causa_Ginecologica\", \"Edad_>12a\", \"TiempoEvolucion_>73h\")\n# Se añaden todas las reglas a la Base de Conocimiento\nb_conocimiento <- list(r1, r2, r3, r4, r5, r6, r7, r8, r9, r10)\n# Función para comprobar si una lista contiene un número\n\ncontiene <- function(numero, lista) {\n  existe <- FALSE\n  if (length(lista) > 0) {\n    for (i in 1:length(lista)) {\n      if (numero == lista[[i]]) existe <- TRUE\n    }\n  }\n  return(existe)\n}\n# Motor del Sistema Experto\n\nAlgoritmoSE <- function(b_hechos, b_conocimiento) {\n  c_conflicto <- list()\n  r_disparadas <- list()\n  condicion <- TRUE\n  iteracion <- 0\n  while (condicion) {\n    c_conflicto <- list()\n    cat(\"Iteración: \", iteracion, \"\\n\")\n    iteracion <- iteracion + 1\n    for (i in 1:length(b_conocimiento)) {\n      if (!contiene(i, r_disparadas)) {\n        if (b_conocimiento[[i]][[3]][[1]] == 1) {\n          r_disparada <- TRUE\n        } else {\n          r_disparada <- FALSE\n        }\n        for (j in 1:length(b_conocimiento[[i]][[1]])) {\n          antecedente <- FALSE\n          for (k in 1:length(b_hechos)) {\n            if (b_conocimiento[[i]][[1]][[j]] == b_hechos[[k]]) {\n              if (b_conocimiento[[i]][[3]][[1]] == 0 || b_conocimiento[[i]][[3]][[1]] == -1) r_disparada <- TRUE\n              antecedente <- TRUE\n            }\n          }\n          if (b_conocimiento[[i]][[3]][[1]] == 1) {\n            if (!antecedente) r_disparada <- FALSE\n          }\n        }\n        if (r_disparada) {\n          cat(\"Regla\", i, \"añadida a Conjunto conflicto\\n\")\n          c_conflicto[length(c_conflicto) + 1] <- i\n        }\n      }\n    }\n    if (length(c_conflicto) > 0) {\n      # str(\"Conjunto conflicto:\")\n      # str(c_conflicto)\n      r_disparadas[length(r_disparadas) + 1] <- c_conflicto[1]\n      cat(\"Regla\", r_disparadas[[length(r_disparadas)]], \"disparada\\n\")\n      b_hechos[length(b_hechos) + 1] <- b_conocimiento[[r_disparadas[[length(r_disparadas)]]]][[2]][[1]]\n      str(\"Base de hechos:\")\n      str(b_hechos)\n      cat(\"Consecuente:\", b_conocimiento[[r_disparadas[[length(r_disparadas)]]]][[2]][[1]], \"\\n\")\n    } else {\n      condicion <- FALSE\n    }\n  }\n}\n\n# Se inicializa la base de hechos con la situación propuesta (paciente de más de 12 años, por causa ginecológica con tiempo de evolución mayor de 73h)\nb_hechos <- list(\"Causa_Ginecologica\", \"Edad_>12a\", \"TiempoEvolucion_>73h\")\n\n# Se lanza la ejecución del motor del Sistema Experto\nAlgoritmoSE(b_hechos, b_conocimiento)"},{"path":"nlp-textil.html","id":"nlp-textil","chapter":"Capítulo 59 El procesamiento del lenguaje natural para tendencias de moda en textil","heading":"Capítulo 59 El procesamiento del lenguaje natural para tendencias de moda en textil","text":"Ambrosio Nguema Ansue","code":""},{"path":"nlp-textil.html","id":"introducción-29","chapter":"Capítulo 59 El procesamiento del lenguaje natural para tendencias de moda en textil","heading":"59.1 Introducción","text":"El Procesamiento del Lenguaje Natural (NLP, por sus siglas en inglés), abarca una amplia gama de técnicas y algoritmos, entre los que se encuentra el modelado de temas.\nEl modelado de temas es un modelo de predicción en sí mismo. En cambio, es una técnica de aprendizaje supervisado que tiene como objetivo descubrir estructuras ocultas (temas) dentro de un conjunto de documentos o textos aunque está relacionado con el NLP, son lo mismo, el modelado de temas es una de las muchas técnicas que forman parte del NLP. La relación entre ambos radica en que el modelado de temas utiliza enfoques del NLP para analizar y procesar el lenguaje en los textos, pero se enfoca en una tarea específica: extraer temas.\nEn este capítulo, exploraremos cómo el modelado de temas y otras técnicas de NLP pueden aplicarse al análisis de tendencias en el mundo de la moda. El modelado de temas aplicado la industria textil puede proporcionar información valiosa sobre las preferencias y opiniones de los clientes, lo que puede mejorar la toma de decisiones y la experiencia del cliente en el ámbito del comercio electrónico de ropa.","code":""},{"path":"nlp-textil.html","id":"análisis-de-tendencias-de-moda-en-textil","chapter":"Capítulo 59 El procesamiento del lenguaje natural para tendencias de moda en textil","heading":"59.2 Análisis de tendencias de moda en textil","text":"El conjunto clothes de datos incluido en el paquete CDR de reseñas y calificaciones de ropa de comercio electrónico para mujeres contiene 23.486 entradas relacionadas con la edad y la revisión dada por el cliente y sus opiniones sobre la ropa de mujer de varios minoristas.Las variables incluidas pueden verse con la ejecutando names(clothes) y una descripción de las variables\ncon el comando ??clothes. El primer registro presenta la siguiente estructura la información:El conjunto de datos consta de 23.486 entradas que incluyen información acerca de la edad del cliente, las calificaciones otorgadas y las opiniones sobre la ropa comprada en comercios electrónicos para mujeres. Los datos se organizan en columnas, algunas de las cuales contienen valores enteros y otras almacenan caracteres. Todas las columnas con valores enteros están completas, mientras que algunas columnas de caracteres presentan valores faltantes (NA). La variable con la mayor cantidad de valores NA Título.\nEn el presente capítulo, se explora la aplicación de técnicas de análisis de texto en un conjunto de datos de reseñas y calificaciones de ropa de comercio electrónico para mujeres. En primer lugar, se realiza un análisis del porcentaje de reseñas y calificaciones en cada departamento, destacando los departamentos con mayor y menor porcentaje. Además, se lleva cabo un análisis de bigramas para identificar las frases más comunes asociadas con diferentes calificaciones. Finalmente, se utiliza el modelado de temas con Latent Dirichlet Allocation (LDA) para explorar las características clave de las revisiones en el departamento de Tendencias. Los resultados del análisis proporcionan información valiosa para las empresas sobre el grupo demográfico objetivo, las preferencias de los clientes y las características clave de las prendas.\nFigura 59.1: Percentage Reviews Department\nLos tops y vestidos son los departamentos que cuentan con la mayoría de las reseñas y calificaciones en el conjunto de datos, mientras que las chaquetas y la sección de tendencias tienen la menor cantidad. Dado que la sección de tendencias presenta una mezcla de ropa que puede pertenecer otros departamentos, y solo representa un 0,51% del conjunto de datos, se ha decidido excluir esta sección del análisis.\nFigura 59.2: Percentage reviews department\nSe ha observado que en todos los departamentos, la calificación de 5 estrellas es la más común. pesar de tener una menor cantidad de reseñas en general, las chaquetas tienen la mayor proporción de calificaciones de 5 estrellas en su categoría. Una posible razón de esto es que las chaquetas suelen ser más fáciles de ajustar diferentes formas corporales en comparación con vestidos y blusas, que pueden ser más difíciles de adaptarse correctamente, especialmente cuando se compran en línea.\nFigura 59.3: Number Reviews Department\nSe ha observado que la tendencia en la distribución de reseñas por departamento (es decir, tops con el mayor número de reseñas y vestidos con el segundo mayor número) es similar en la mayoría de los grupos de edad. Esto indica que la popularidad de los diferentes tipos de ropa se mantiene en gran medida constante entre los grupos de edad más jóvenes y de mediana edad.Análisis de bigramas\nEl análisis de bigramas es una técnica útil para identificar patrones y tendencias en el lenguaje utilizado en las reseñas de productos. Un bigrama es un par consecutivo de palabras en un texto y puede proporcionar información valiosa sobre la frecuencia con la que ciertas palabras aparecen juntas y las combinaciones de palabras que son relevantes en las opiniones de los clientes. Al utilizar el análisis de bigramas, se espera comprender mejor las opiniones de los clientes sobre los productos de ropa en el conjunto de datos.Para llevar cabo el análisis de bigramas, se procede procesar las palabras de las reseñas eliminando las palabras vacías (también conocidas como stop words), que son palabras comunes sin un significado contextual importante, y los dígitos que representan la calificación de las reseñas. Una vez procesadas las palabras, se agrupan según sus calificaciones y se representan gráficamente los 10 bigramas más comunes para cada nivel de calificación. De esta forma, se puede identificar y comprender mejor las combinaciones de palabras que son relevantes para las opiniones de los clientes y para cada nivel de calificación.\n\nFigura 59.4: Common Bigrams (Ratings)\nModelado de temas con Latent Dirichlet AllocationEl enfoque de modelado de temas de Latent Dirichlet Allocation (LDA) es una técnica ampliamente utilizada en NLP para extraer temas latentes de un corpus de texto. LDA es un algoritmo supervisado que utiliza el aprendizaje automático para identificar patrones en grandes conjuntos de datos de texto, agrupando palabras similares en temas y asignando probabilidades cada tema en cada documento.En este estudio, se ha utilizado el enfoque de modelado de temas de LDA para explorar las 118 revisiones del Departamento de Tendencias. Se ajustó un modelo LDA utilizando muestreo de Gibbs y se eligió k = 5 para los departamentos de Bottoms, Dresses, Intimate, Jackets y Tops. través del análisis de los resultados, se pudieron identificar las 5 palabras principales de cada tema y obtener una mejor comprensión de las características clave de las revisiones en cada departamento. De esta forma, se pudo obtener información valiosa sobre las preferencias y opiniones de los clientes en diferentes departamentos de ropa en el conjunto de datos.\nFigura 59.5: Modelo LDA (K=5)\nEn el modelo LDA, cada tema se representa por un conjunto de palabras que aparecen juntas con mayor frecuencia en las revisiones. Por ejemplo, al observar el tema 3, se puede identificar que está caracterizado por palabras como “colors”, “wear”, “bit”, “jacket” y “price”, lo que sugiere que los clientes pueden estar comentando sobre la variedad de colores disponibles, la durabilidad de la prenda y su precio. Por otro lado, el tema 1 se caracteriza por palabras como “love”, “fit”, “fabric”, “wear” y “length”, lo que sugiere que los clientes pueden estar hablando sobre su experiencia con la prenda en términos de comodidad, ajuste y calidad de la tela. Al identificar estos temas, se pueden obtener ideas valiosas sobre las opiniones y preferencias de los clientes para mejorar la calidad de la ropa y satisfacer sus necesidades y deseos. Esto permite las empresas tomar decisiones informadas para satisfacer las necesidades de sus clientes y mejorar la experiencia del usuario en el ámbito del comercio electrónico de ropa.Como conclsuión, destacar que el análisis de este conjunto de datos proporciona información valiosa sobre las preferencias y opiniones de los clientes en cuanto la ropa femenina. Las reseñas de 5 estrellas son dominantes en cada departamento, y las chaquetas son las prendas que obtienen la proporción más alta de calificaciones positivas. Además, se ha observado que los clientes de entre 30 y 40 años dejan la mayoría de las reseñas y que factores como el ajuste, la comodidad/calidad del material y la estética de la prenda influyen en la calificación. La realización de análisis de datos exploratorios y de bigramas puede ayudar las empresas comprender mejor lo que funciona y lo que , y seleccionar artículos con telas flexibles y cómodas puede conducir una mayor satisfacción del cliente y mayores ventas. Por último, el modelado de temas con LDA es una herramienta útil en situaciones en las que se tienen reseñas sin marcar y puede proporcionar información valiosa sobre las características clave de las revisiones. En general, estos análisis pueden ayudar las empresas tomar decisiones informadas y mejorar la experiencia del usuario en el ámbito del comercio electrónico de ropa.","code":"\nlibrary(\"CDR\")\nlibrary(\"readr\")\nlibrary(\"tidyverse\")\nlibrary(\"tidytext\")\nhead(clothes)[1, ]\n#>    ID Age Title                                                Review Rating\n#> 1 767  33  <NA> Absolutely wonderful - silky and sexy and comfortable      4\n#>   Recommend Liked  Division     Dept     Class\n#> 1         1     0 Initmates Intimate Intimates\nlibrary(\"ggplot2\")\nclothes |>\n  dplyr::count(Dept) |>\n  dplyr::mutate(prop = n / sum(n)) |>\n  ggplot(aes(x = Dept, y = prop * 100)) +\n  geom_bar(stat = \"identity\", fill=\"blue\") +\n  xlab(\"Department Name\") +\n  ylab(\"Percentage of Reviews/Ratings (%)\") +\n  geom_text(aes(label = round(prop * 100, 2)), vjust = -0.25) \nclothes |>\n  filter(!is.na(Dept), Dept != \"Trend\") |>\n  mutate(Dept = factor(Dept)) |>\n  group_by(Dept, Rating) |>\n  summarize(n = n()) |>\n  mutate(perc = n / sum(n)) |>\n  ggplot(aes(x = Rating, y = perc * 100, fill = Dept)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) +\n  facet_wrap(~Dept) +\n  ylab(\"Percentage of reviews (%)\") +\n  geom_text(aes(label = round(perc * 100, 2)), vjust = -.2) +\n  scale_y_continuous(limits = c(0, 65))\nclothes |>\n  filter(!is.na(Age), !is.na(Dept), Dept != \"Trend\") |>\n  select(ID, Age, Dept) |>\n  mutate(Age_group = cut(Age, breaks = c(18, 29, 39, 49, 59, 69, 79, 89, 99))) |>\n  mutate(Age_group = as.character(Age_group)) |>\n  mutate(Age_group = factor(Age_group, levels = c(\"18-29\", \"30-39\", \"40-49\", \"50-59\", \"60-69\", \"70-79\", \"80-89\", \"90-99\"))) |>\n  mutate(Dept = factor(Dept, levels = rev(c(\"Tops\", \"Dresses\", \"Bottoms\", \"Intimate\", \"Jackets\")))) |>\n  filter(Age < 80) |>\n  group_by(Age_group, Dept) |>\n  summarize(n = n()) |>\n  ggplot(aes(Dept, n, fill = Age_group)) +\n  geom_bar(stat = \"identity\", fill=\"red\") +\n  facet_wrap(~Age_group, scales = \"free\") +\n  xlab(\"Department\") +\n  ylab(\"Number of Reviews\") +\n  geom_text(aes(label = n), hjust = -0.1) +\n  scale_y_continuous(expand = c(.1, 0)) +\n  coord_flip() +\n  scale_fill_manual(values = hcl.colors(8))\nclothesr <- clothes |> filter(!is.na(Review))\nnotitle <- clothesr |>\n  filter(is.na(Title)) |>\n  select(-Title)\nwtitle <- clothesr |>\n  filter(!is.na(Title)) |>\n  unite(Review, c(Title, Review), sep = \" \")\n\nmain <- bind_rows(notitle, wtitle)\nbigramming <- function(data) {\n  cbigram <- data |> unnest_tokens(bigram, Review, token = \"ngrams\", n = 2)\n  cbigram_sep <- cbigram |> separate(bigram, c(\"first\", \"second\"), sep = \" \")\n  cbigram2 <- cbigram_sep |>\n    filter(!first %in% stop_words$word, !second %in% stop_words$word, !str_detect(first, \"\\\\d\"), !str_detect(second, \"\\\\d\")) |>\n    unite(bigram, c(first, second), sep = \" \")\n  return(cbigram2)\n}\ntop_bigrams <- bigramming(main) |>\n  mutate(Rating = factor(Rating, levels <- c(5:1))) |>\n  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) |>\n  group_by(Rating) |>\n  count(bigram, sort = TRUE) |>\n  top_n(10, n) |>\n  ungroup()\n\ntop_bigrams |> ggplot(aes(bigram, n, fill = Rating)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~Rating, ncol = 3, scales = \"free\") +\n  labs(x = NULL, y = \"frequency\") +\n  coord_flip()\nlibrary(\"topicmodels\")\nlibrary(\"tm\")\nlibrary(\"LDAvis\")\n\ntrend_count <- main |>\n  filter(Dept == \"Trend\") |>\n  unnest_tokens(word, Review) |>\n  anti_join(stop_words, by = \"word\") |>\n  filter(!str_detect(word, \"\\\\d\")) |>\n  count(ID, word, sort = TRUE) |>\n  ungroup()\n\ntrend_dtm <- trend_count |> cast_dtm(ID, word, n)\ntrendy <- tidy(LDA(trend_dtm, k = 5, method = \"GIBBS\", control = list(seed = 4444, alpha = 1)), matrix = \"beta\")\ntop_trendy <- trendy |>\n  group_by(topic) |>\n  top_n(5, beta) |>\n  ungroup() |>\n  arrange(topic, desc(beta))\n\ntop_trendy |>\n  mutate(term = reorder(term, beta)) |>\n  ggplot(aes(term, beta, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~topic, scales = \"free\") +\n  coord_flip()"},{"path":"cap-fraude.html","id":"cap-fraude","chapter":"Capítulo 60 Detección de fraude de tarjetas de crédito","heading":"Capítulo 60 Detección de fraude de tarjetas de crédito","text":"Pedro Albarracín García","code":""},{"path":"cap-fraude.html","id":"introducción-30","chapter":"Capítulo 60 Detección de fraude de tarjetas de crédito","heading":"60.1 Introducción","text":"En un informe publicado en diciembre de 2021 por Nilson Report\n(https://nilsonreport.com/upload/content_promo/NilsonReport_Issue1209.pdf),\nse informó de que los emisores de tarjetas de crédito, comerciantes y\nconsumidores sufrieron un total de 28.580 millones de dólares de\npérdidas por fraude en 2020, es decir, 6,8 centavos por cada 100 dólares\nen volumen de compras. El fraude sólo en USA representa el 35,83% del\ntotal mundial.En Europa la situación es más alentadora. Según un informe del Banco\nCentral Europeo publicado en 2020\n(https://www.ecb.europa.eu/pub/cardfraud/html/ecb.cardfraudreport202008~521edb602b.en.html#toc2),\nel valor total de las transacciones con tarjeta en la zona SEPA\nascendieron 4.84 billones de euros en 2018, de los cuales 1.800\nmillones correspondieron operaciones fraudulentas.Las entidades financieras trabajan diario en el desarrollo de modelos\nde machine learning y deep learning que les permitan detectar, con la\nmayor precisión posible, aquellas operaciones de compra con tarjetas de\ncrédito, débito o prepago que puedan ser sospechosas de fraude, o que al\nmenos puedan ser identificadas como anómalas. En este sentido, es\nimportante destacar que existe una única solución posible, ya que el\nproblema presenta, en la mayoría de los casos, múltiples variantes que\nhacen de éste, un problema complejo y que puede y debe ser abordado\ndesde múltiples perspectivas y con diferentes enfoques.En primer lugar, es posible identificar dos tipos de fraude. Por un\nlado, el que se comete físicamente, como, por ejemplo, la compra o la\nretirada de efectivo con tarjetas robadas o falsas. Por otro lado, están\naquellas transacciones fraudulentas que se cometen online, en las que \nes necesaria la tarjeta física, y en las que se utilizan los datos de\nlas tarjetas obtenidas por los delincuentes mediante técnicas como el\nphishing y que son utilizados posteriormente para realizar pagos online.Otro hándicap asociado este tipo de escenarios es el derivado de la\ngran diversidad de fuentes de datos que forman parte de una transacción\ny que pueden dar lugar divergencias metodológicas, tanto en la\nrecogida y transmisión de los datos, como en su posterior almacenaje, lo\nque ocasiona que, en muchos casos, la calidad de los datos disponibles\nsea la esperada, o simplemente nos encontremos ante datasets\ninconsistentes. Los datos requeridos para este caso de uso pueden\ncategorizarse en variables relativas :• Cliente• Transacción• Geolocalización• Comercio• Tarjetas• Hábitos de compraCada una de estas categorías, y otras que puedan aparecer aportan\ninformación que permite abordar el problema desde diferentes ángulos.\nPor un lado es posible enfocar el problema desde el punto de vista del\ncliente y sus hábitos de compra para ver si existe alguna característica\nanómala en una transacción, tal vez la hora de la compra, o quizás\nanalizar los datos de geolocalización junto con los del comercio para\nanalizar si es una compra en un comercio habitual y desde una\nlocalización conocida, etc.","code":""},{"path":"cap-fraude.html","id":"modelización-del-fraude-en-la-compra-con-tarjetas-de-crédito","chapter":"Capítulo 60 Detección de fraude de tarjetas de crédito","heading":"60.2 Modelización del fraude en la compra con tarjetas de crédito","text":"El objetivo de este caso de uso es la construcción de un modelo que\npermita detectar si una operación de compra realizada con tarjeta de\ncrédito es fraudulenta o . Para ello, se utilizará un dataset\nanonimizado de operaciones con tarjeta de crédito ya etiquetadas\ndisponible desde la web de Kaggle en el siguiente enlace:\nhttps://www.kaggle.com/datasets/mlg-ulb/creditcardfraud.\nobstante, los datos se han incorporado la paquete CDR del libro\ncon el nombre creditcard. El conjunto de datos consta de\n284.807 transacciones, de las cuales 492 están\netiquetadas como fraudulentas, es decir, sólo un 0,172% del total de las\ntransacciones. Es un dataset por lo tanto muy desequilibrado, lo que\nañade cierto grado de dificultad. El dataset creditcard tiene un conjunto de 31\nvariables, de las cuales 28 están identificadas como V1, …, V28, una\nvariable Time que registra los segundos transcurridos entre esa\ntransacción y la primera, una variable Amount que registra el importe\nde la transacción, y la variable dependiente class que indica, con\nvalor 0, que la operación es “fraudulenta”, y con valor 1 las\noperaciones fraudulentas.Para concluir esta breve descripción del dataset, es necesario recordar\nque todos los valores de entrada son numéricos y que ya han sufrido\nalgunas transformaciones. Por motivos de confidencialidad, las variables\nV1 V28 incluyen sus nombres originales ni se añade más información\nde contexto.Carga de los datos y obtención de algunos descriptivosDivisión de los datosA continuación es necesario dividir los datos en dos dataframes que\ndenominados creditcard_X y creditcard_y, de esta forma se separan\nlas variables independientes de la variable dependiente o Class.Tratamiento de datos desequilibradosUno de los principales problemas la hora de abordar este tipo de\nescenarios, es lo que se conoce como “datos desequilibrados”.\nSe dice que un dataset está desequilibrado cuando la variable dependiente\npresenta más observaciones de una clase que de otra. En el caso de\ntransacciones fraudulentas con tarjeta de crédito, es evidente que la\nmayoría de las operaciones son legítimas o benignas, y que sólo un\npequeño porcentaje resultan ser maliciosas. ¿Cuál es el problema?Por lo general, los modelos entrenados con datasets desequilibrados \nse comportan bien cuando tienen que generalizar, es decir, cuando tienen\nque realizar predicciones sobre conjuntos de datos que han sido\nvistos anteriormente por el modelo. El desequilibrio de los datos es un\nsesgo hacia la clase mayoritaria, por lo que, en última instancia,\nmuestra una tendencia al sobreajuste u overfitting hacia esa clase.\nExisten diversas técnicas que permiten corregir esta situación:Undersampling o Submuestreo. Esta técnica consiste en reducir el\nnúmero de observaciones de la clase mayoritaria, estableciendo quizás\nuna ratio de 60/40. Esta técnica resulta efectiva si se respetan los\ngrupos naturales que existen en los datos, así como el resto de las\ncaracterísticas presentes en la clase mayoritaria.Undersampling o Submuestreo. Esta técnica consiste en reducir el\nnúmero de observaciones de la clase mayoritaria, estableciendo quizás\nuna ratio de 60/40. Esta técnica resulta efectiva si se respetan los\ngrupos naturales que existen en los datos, así como el resto de las\ncaracterísticas presentes en la clase mayoritaria.Oversampling o Sobremuestreo. Esta técnica consiste en aumentar\nel número de observaciones de la clase minoritaria mediante la creación\nde datos sintéticos que, al igual que la técnica anterior, respeten las\ncaracterísticas de esa clase.Oversampling o Sobremuestreo. Esta técnica consiste en aumentar\nel número de observaciones de la clase minoritaria mediante la creación\nde datos sintéticos que, al igual que la técnica anterior, respeten las\ncaracterísticas de esa clase.Para la creación de datos sintéticos en escenarios de oversampling\nexisten varios algoritmos que proporcionan buenos resultados. Quizás el\nmás conocido y utilizado sea Synthetic Minority Oversampling\nTEchnique (SMOTE). SMOTE realiza una copia de las observaciones del\ndataset, sino que en su lugar genera nuevos datos de forma sintética\nutilizando los vecinos más cercanos de esos casos, respetando las\ncaracterísticas estadísticas de la clase. Además, los ejemplos de la\nclase mayoritaria también son submuestreados, lo que da lugar un\nconjunto de datos más equilibrado. En R, el algoritmo SMOTE pertenece al\npaquete DMwR.Para este caso particular, se utilizará una técnica simple de\nsubmuestreo basada en el paquete unbalanced, que actualmente se\nencuentra disponible en el repositorio de CRAN por lo que, para su\ninstalación, se debe ejecutar el siguiente código:Una vez instalado, al igual que todas sus dependencias, se realiza el\nsubmuestreo del dataset siguiendo los siguientes pasos:1.- Convertir la variable dependiente “Class” en factor:2.- continuación, ejecutar la función de submuestreo:3.- Comprobar el número de casos en el dataset sobre el que se ha\nejecutado la función de submuestreo4.- Realizar la gráfica para visualizar el número de elementos de cada\nclase después de realizar el submuestreoModelo de clasificación mediante regresión lógísticaA continuación se procederá la construcción de un modelo de regresión\nlogística (véase Cap. 16) para una clasificación binaria\nen relación al fraude en transacciones con tarjeta de crédito partir\nde los datos equilibrados obtenidos anteriormente. El dataframe que se\nutilizará, por lo tanto, será “undersampled_combined” que contiene 984\nobservaciones, un 50% de las cuales son transacciones identificadas como\nfraude.Lo primero, será realizar un par de pequeños cambios en el dataset, es\ndecir, eliminar las variables Time y Amount, ya que van ser\nrelevantes para el modelo, y cambiar por 0 y 1 las etiquetas “Legitima”\ny “Fraude”, respectivamente.Lo siguiente será dividir el conjunto de datos en los datasets de\nentrenamiento y test, para lo cual se aplicará la función “split()” con\nun SplitRatio de 0.80, es decir, un 80% de los datos irán de forma\naleatoria al dataset de entrenamiento, 788 observaciones, frente las\n196 observaciones que formarán el dataset de testing.Con los datasets necesarios ya disponibles, el siguiente paso es\nentrenar el modelo de regresión logística que clasificará las\ntransacciones en legítimas o fraudulentas. Para ello se utilizará el\nalgoritmo GLM, creando un clasificador que se identificará como\nundersampledModely al que se le pasarán los parámetros siguientes:• formula: con este parámetro se indica la variable dependiente, class,\nseguida del simbolo ~ y un punto (con el punto se hace referencia al\nresto de variables del dataset, es decir, V1 V28).• data: el dataset con los datos de entrenamiento.• family: al ser un clasificador con dos valores posibles (0, 1), se\nindica que será de tipo “binomial”.Para ver,Con el modelo ya entrenado, se realizan las predicciones para los datos\ndel conjunto de testing, utilizando para ello la función “predict()”.\nLos parámetros son simples: el primero es el modelo o clasificador que\nse va utilizar y que será “undersampledModel”; continuación el tipo\nde dato que devolverá, en este caso “response”, el cual indica que el\nalgoritmo devolverá las probabilidades de fraude listadas en un único\nvector, el cual estará disponible partir de la variable “fraud_prob”;\ny por último, el parámetro “newdata” que hace referencia al dataset en\nel que se descarta la última columna por ser la que representa la\nvariable dependiente.La visualización del vector con las predicciones puede parecer algo\nconfusa, por lo que, menudo, es preciso realizar una conversión de\nesas predicciones en valores 0 y 1, dependiendo del rango de valores \npartir del cual se estime que una transacción es fraudulenta: por\nejemplo, partir del 60% de probabilidad, la transacción será\netiquetada como “1”; en caso contrario será etiquetada como “0”. Para\nello se utiliza el siguiente código “ifelse”:La matriz de confusiónComo último paso del ejercicio se crea la matriz de confusión con el fin\nde visualizar qué tal se ha comportado el algoritmo, es decir, cuántos\npositivos y negativos ha logrado predecir correctamente. Para ello se\ncreará la variable confusionMatrix, en la cual se almacenará el\nresultado de la comparación entre el vector del dataset de testing, es\ndecir, los datos etiquetados originalmente, y el vector de sus\ntraducciones 0 y 1, resultado del algoritmo. El resultado, como se\npuede comprobar es que ha evaluado correctamente 186 de las 196\nobservaciones.","code":"\ncreditcard <- CDR::creditcard\nhead(creditcard)\n#>   Time         V1          V2        V3         V4          V5          V6\n#> 1    0 -1.3598071 -0.07278117 2.5363467  1.3781552 -0.33832077  0.46238778\n#> 2    0  1.1918571  0.26615071 0.1664801  0.4481541  0.06001765 -0.08236081\n#> 3    1 -1.3583541 -1.34016307 1.7732093  0.3797796 -0.50319813  1.80049938\n#> 4    1 -0.9662717 -0.18522601 1.7929933 -0.8632913 -0.01030888  1.24720317\n#> 5    2 -1.1582331  0.87773675 1.5487178  0.4030339 -0.40719338  0.09592146\n#> 6    2 -0.4259659  0.96052304 1.1411093 -0.1682521  0.42098688 -0.02972755\n#>            V7          V8         V9         V10        V11         V12\n#> 1  0.23959855  0.09869790  0.3637870  0.09079417 -0.5515995 -0.61780086\n#> 2 -0.07880298  0.08510165 -0.2554251 -0.16697441  1.6127267  1.06523531\n#> 3  0.79146096  0.24767579 -1.5146543  0.20764287  0.6245015  0.06608369\n#> 4  0.23760894  0.37743587 -1.3870241 -0.05495192 -0.2264873  0.17822823\n#> 5  0.59294075 -0.27053268  0.8177393  0.75307443 -0.8228429  0.53819555\n#> 6  0.47620095  0.26031433 -0.5686714 -0.37140720  1.3412620  0.35989384\n#>          V13        V14        V15        V16         V17         V18\n#> 1 -0.9913898 -0.3111694  1.4681770 -0.4704005  0.20797124  0.02579058\n#> 2  0.4890950 -0.1437723  0.6355581  0.4639170 -0.11480466 -0.18336127\n#> 3  0.7172927 -0.1659459  2.3458649 -2.8900832  1.10996938 -0.12135931\n#> 4  0.5077569 -0.2879237 -0.6314181 -1.0596472 -0.68409279  1.96577500\n#> 5  1.3458516 -1.1196698  0.1751211 -0.4514492 -0.23703324 -0.03819479\n#> 6 -0.3580907 -0.1371337  0.5176168  0.4017259 -0.05813282  0.06865315\n#>           V19         V20          V21          V22         V23         V24\n#> 1  0.40399296  0.25141210 -0.018306778  0.277837576 -0.11047391  0.06692807\n#> 2 -0.14578304 -0.06908314 -0.225775248 -0.638671953  0.10128802 -0.33984648\n#> 3 -2.26185710  0.52497973  0.247998153  0.771679402  0.90941226 -0.68928096\n#> 4 -1.23262197 -0.20803778 -0.108300452  0.005273597 -0.19032052 -1.17557533\n#> 5  0.80348692  0.40854236 -0.009430697  0.798278495 -0.13745808  0.14126698\n#> 6 -0.03319379  0.08496767 -0.208253515 -0.559824796 -0.02639767 -0.37142658\n#>          V25        V26          V27         V28 Amount Class\n#> 1  0.1285394 -0.1891148  0.133558377 -0.02105305 149.62     0\n#> 2  0.1671704  0.1258945 -0.008983099  0.01472417   2.69     0\n#> 3 -0.3276418 -0.1390966 -0.055352794 -0.05975184 378.66     0\n#> 4  0.6473760 -0.2219288  0.062722849  0.06145763 123.50     0\n#> 5 -0.2060096  0.5022922  0.219422230  0.21515315  69.99     0\n#> 6 -0.2327938  0.1059148  0.253844225  0.08108026   3.67     0\n# psych::describe(creditcard)  # descomentar para ver los descriptivos\n# Se dividen los datos\ncreditcard_X <- creditcard[,-31]\ncreditcard_y <- creditcard$Class\n#install.packages(\"devtools\") # descomentar para instalar \nlibrary(\"devtools\")\ndevtools::install_github(\"dalpozz/unbalanced\")\nlibrary(\"unbalanced\")\ncreditcard$Class <-as.factor(creditcard$Class)\nlevels(creditcard$Class) <- c('0', '1') \nundersampled_creditcard <- ubBalance(creditcard_X, creditcard$Class, type='ubUnder', verbose = TRUE)\n#> Proportion of positives after ubUnder : 50 % of 984 observations\nundersampled_combined <- cbind(undersampled_creditcard$X,   \n                               undersampled_creditcard$Y)\n\nnames(undersampled_combined)[names(undersampled_combined) == \"undersampled_creditcard$Y\"] <- \"Class\"\nlevels(undersampled_combined$Class) <- c('Legítima', 'Fraude')\ncreditcard$Class <-as.factor(creditcard$Class)\nlevels(creditcard$Class) <- c('0', '1') \nlibrary(\"ggplot2\")\nggplot(data = undersampled_combined, aes(fill = Class))+\n  geom_bar(aes(x = Class))+\n  ggtitle(\"Número de casos de cada clase después de submuestreo\", \n          subtitle=\"Total muestrass: 984\")+\n  xlab(\"\")+\n  ylab(\"Muestras\")+\n  scale_y_continuous(expand = c(0,0))+\n  scale_x_discrete(expand = c(0,0))+\n  theme(legend.position = \"ninguna\", \n        legend.title = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.background = element_blank())\nundersampled_combined <- subset( undersampled_combined, select = -c(Time, Amount ) )\nundersampled_combined$Class <- ifelse(undersampled_combined$Class == \"Fraude\",1,0)\n#install.packages(\"caTools\") # descomentar para instalar\nlibrary(\"caTools\")\nset.seed(123)\nsplit = sample.split(undersampled_combined$Class, SplitRatio = 0.80)\ntraining = subset(undersampled_combined, split == TRUE)\ntest = subset(undersampled_combined, split == FALSE)\nundersampledModel = glm(Class ~ ., data = training, family = binomial())\nsummary(undersampledModel)\n#> \n#> Call:\n#> glm(formula = Class ~ ., family = binomial(), data = training)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -5.6665  -0.2824  -0.0100   0.0207   2.8822  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept) -3.07010    0.27924 -10.995  < 2e-16 ***\n#> V1          -0.11097    0.09205  -1.206 0.227989    \n#> V2           0.01988    0.11120   0.179 0.858113    \n#> V3          -0.05131    0.10141  -0.506 0.612875    \n#> V4           0.73337    0.14655   5.004  5.6e-07 ***\n#> V5           0.06131    0.12566   0.488 0.625621    \n#> V6          -0.25720    0.18061  -1.424 0.154428    \n#> V7           0.04104    0.14351   0.286 0.774892    \n#> V8          -0.59200    0.17242  -3.433 0.000596 ***\n#> V9           0.09649    0.25616   0.377 0.706430    \n#> V10         -0.36799    0.23336  -1.577 0.114816    \n#> V11          0.17731    0.17308   1.024 0.305633    \n#> V12         -0.42350    0.18651  -2.271 0.023165 *  \n#> V13         -0.46607    0.17870  -2.608 0.009105 ** \n#> V14         -0.60792    0.16403  -3.706 0.000210 ***\n#> V15         -0.20302    0.19558  -1.038 0.299246    \n#> V16         -0.05732    0.24961  -0.230 0.818359    \n#> V17          0.03321    0.17355   0.191 0.848234    \n#> V18         -0.02035    0.27096  -0.075 0.940146    \n#> V19         -0.17014    0.21817  -0.780 0.435475    \n#> V20          0.20927    0.21526   0.972 0.330979    \n#> V21         -0.17569    0.24956  -0.704 0.481436    \n#> V22          0.37758    0.26040   1.450 0.147059    \n#> V23         -0.13444    0.13370  -1.006 0.314618    \n#> V24          0.10680    0.35270   0.303 0.762037    \n#> V25          0.01945    0.35147   0.055 0.955869    \n#> V26         -0.24275    0.39213  -0.619 0.535882    \n#> V27         -0.44250    0.33671  -1.314 0.188780    \n#> V28          1.01477    0.78764   1.288 0.197619    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 1092.40  on 787  degrees of freedom\n#> Residual deviance:  254.81  on 759  degrees of freedom\n#> AIC: 312.81\n#> \n#> Number of Fisher Scoring iterations: 9\nfraud_prob = predict(undersampledModel, type = \"response\", newdata = test[,-29])\nhead(fraud_prob)\n#>        620       2171       2342       2934       5051       6109 \n#> 0.04012917 0.22457711 0.01231549 0.09225823 0.23155212 0.99999667\ny_pred = ifelse(fraud_prob > 0.6, 1, 0)\nconfusionMatrix = table(test[, 29], y_pred)\nconfusionMatrix\n#>    y_pred\n#>      0  1\n#>   0 97  1\n#>   1 11 87"},{"path":"info-session.html","id":"info-session","chapter":"AnexoA Información de la sesión","heading":"AnexoA Información de la sesión","text":"","code":"\nsessionInfo()\n#> R version 4.2.1 (2022-06-23 ucrt)\n#> Platform: x86_64-w64-mingw32/x64 (64-bit)\n#> Running under: Windows 10 x64 (build 19045)\n#> \n#> Matrix products: default\n#> \n#> locale:\n#> [1] LC_COLLATE=Spanish_Spain.utf8  LC_CTYPE=Spanish_Spain.utf8   \n#> [3] LC_MONETARY=Spanish_Spain.utf8 LC_NUMERIC=C                  \n#> [5] LC_TIME=Spanish_Spain.utf8    \n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#> [1] flextable_0.8.1   fontawesome_0.4.0\n#> \n#> loaded via a namespace (and not attached):\n#>  [1] styler_1.8.1      tidyselect_1.2.0  xfun_0.35         bslib_0.4.2      \n#>  [5] purrr_0.3.5       colorspace_2.0-3  vctrs_0.5.1       generics_0.1.3   \n#>  [9] htmltools_0.5.4   yaml_2.3.5        base64enc_0.1-3   utf8_1.2.2       \n#> [13] rlang_1.0.6       R.oo_1.25.0       pillar_1.8.1      jquerylib_0.1.4  \n#> [17] glue_1.6.2        withr_2.5.0       DBI_1.1.3         R.utils_2.12.2   \n#> [21] gdtools_0.2.4     uuid_1.1-0        R.cache_0.16.0    lifecycle_1.0.3  \n#> [25] stringr_1.4.1     munsell_0.5.0     gtable_0.3.1      ragg_1.2.2       \n#> [29] R.methodsS3_1.8.2 zip_2.2.1         evaluate_0.16     memoise_2.0.1    \n#> [33] knitr_1.39        fastmap_1.1.0     fansi_1.0.3       Rcpp_1.0.9       \n#> [37] scales_1.2.1      cachem_1.0.6      jsonlite_1.8.3    systemfonts_1.0.4\n#> [41] fs_1.5.2          textshaping_0.3.6 ggplot2_3.4.0     digest_0.6.30    \n#> [45] stringi_1.7.8     bookdown_0.28     dplyr_1.0.10      grid_4.2.1       \n#> [49] cli_3.4.1         tools_4.2.1       magrittr_2.0.3    sass_0.4.4       \n#> [53] tibble_3.1.8      pkgconfig_2.0.3   downlit_0.4.2     data.table_1.14.6\n#> [57] xml2_1.3.3        assertthat_0.2.1  rmarkdown_2.14    officer_0.4.4    \n#> [61] rstudioapi_0.14   R6_2.5.1          compiler_4.2.1"},{"path":"referncias.html","id":"referncias","chapter":"Referncias","heading":"Referncias","text":"","code":""}]
