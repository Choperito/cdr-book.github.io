[["modelización-de-series-temporales.html", "Fundamentos de ciencia de datos con R Capítulo 1 Modelización de series temporales 1.1 Conceptos básicos 1.2 Modelos ARIMA 1.3 Análisis de series temporales con R", " Fundamentos de ciencia de datos con R Gema Fernández-Avilés y José-María Montero 2022-12-02 Capítulo 1 Modelización de series temporales Mª Carmen García Centeno 1.1 Conceptos básicos El análisis de series temporales es muy útil para analizar tendencias y comportamiento de datos. Con dicho análisis, se tratará de obtener modelos que expliquen su dinámica y puedan utilizarse para predecir valores futuros y tomar decisiones de la forma más efectiva posible. Una serie temporal se puede definir como el conjunto de valores observados, en periodos consecutivos de tiempo de la misma amplitud, correspondientes a distintas variables aleatorias. Algunos conceptos claves sobre los que se fundamenta el análisis de series temporales son los siguientes: Función de autocorrelación parcial de un proceso estocástico (PACF). Está formada por el conjunto de las correlaciones parciales obtenidas para los distintos valores de k. Así, para dos valores de una serie temporal \\(Y_t\\) e \\(Y_{t-k}\\), mide la correlación existente entre ellos ajustada del efecto de los periodos intermedios (\\(Y_{t-1},Y_{t-2},..., Y_{t-(k-1)}\\)). Ergodicidad. Un proceso estocástico es ergódico cuando a partir de un determinado desfase temporal entre las variables, la relación existente entre ellas tiende a desaparecer. Es decir, las covarianzas y el coeficiente de correlación tienden a cero, (\\(\\lim\\limits_{k \\to \\infty}\\gamma(k)=0\\) y \\(\\lim\\limits_{k \\to \\infty}\\rho(k)=0\\)). Ruido blanco. Es un proceso puramente aleatorio que se puede expresar de la siguiente forma: \\[Y_t=a_t\\] Se caracteriza por tener esperanza nula, varianza constante y covarianzas nulas. Es decir, \\[E(a_t)=0 \\quad\\forall t; \\quad E(a_t^2)=\\sigma^2 \\quad\\forall t;\\quad E(a_ta_s)=0 \\quad\\forall t\\neq s\\] 1.2 Modelos ARIMA Una de las metodologías utilizadas para modelizar el comportamiento de las series temporales son los modelos ARIMA (AutoRegresive Integrated Moving Average) desarrollados por Box y Jenkins. Estos modelos tratan de captar la dinámica y la dependencia que existe entre los datos de una serie temporal, (Hamilton1994?), (Uriel2000?), (Cryer2008?), (Pemberton2011?), (Minguez2011?), (Brockwell2016?), (Shumway2017?). En la modelización ARIMA se pueden destacar varias fases. La primera se centra en la identificación del modelo ARIMA más adecuado que ha podido generar los datos de la serie temporal. En ella, hay que decidir los órdenes del proceso tanto de la parte regular como de la estacional. Las funciones de autocorrelación simple (ACF) y parcial (PACF) desempeñan un papel clave en la identificación de estos órdenes. También en esta fase, es necesario comprobar si las series son estacionarias. En el caso de que no lo sean, se realizarán las transformaciones necesarias para convertirla en estacionaria. Las dos razones fundamentales por las cuales puede no ser estacionaria se deben a que la media o la varianza no se mantengan constantes. Si la serie no sea estacionaria en varianza, de las transformaciones posibles de Box-Cox se utilizará la transformación logarítmica. Si la serie no es estacionaria en media, puede ser debido a la existencia de tendencia o de estacionalidad. En el caso de tendencia se calcularán diferencias regulares (una o dos como máximo, según el tipo de tendencia lineal o no lineal, respectivamente) y en el caso de estacionalidad, se calculará una diferencia estacional como máximo para convertirla en estacionaria. Después de proponer el modelo en la segunda fase se procede a su estimación. En esta fase, se obtienen los valores de los parámetros correspondientes al modelo propuesto, así como, sus desviaciones típicas y los residuos del modelo. Posteriormente, en la fase de validación o diagnóstico se realizarñan, por un lado, los contrastes necesarios que permitan determinar si el modelo propuesto es adecuado o no. Y, por otro, comprobar si los residuos del modelo siguen un proceso ruido blanco o no. En el caso de que no lo sean será necesario corregir el modelo con la información proporcionada por los residuos hasta obtener un modelo cuyos residuos sean ruido blanco. Finalmente, se procederá a la utilización del modelo para predecir. 1.3 Análisis de series temporales con R Las librerías necesarios en la modelización de series temporales con R son: library(tseries) library(astsa) library(forecast) library(lubridate) library(foreign) library(quantmod) library(readxl) library(ggplot2) Posteriormente es necesario cargar los datos. Como se ha comentado en capítulos anteriores los datos pueden tener diferente formato (csv, Excel, etc.). En este caso, se utilizarán datos mensuales del INE (www.ine.es) correspondientes al IPC general en el periodo muestral comprendido entre enero de 2002 y marzo de 2022, en formato Excel. La fecha en la que se observan los datos está incluida en la primera columna de este fichero. Los datos se leerán de la siguiente forma: # library(CDR) ipc &lt;- read_excel(&quot;data/ipc.xlsx&quot;) Si los datos se cargan sin fecha, para trabajar con series temporales es necesario ponerle la fecha. Por ejemplo, para series mensuales, el código sería: ipc_ts &lt;- ts(ipc$ipc, start = c(2002, 1), end = c(2022, 3), frequency = 12) La representación gráfica de la serie original, puede ayudar a saber si la serie ha sido generada por un proceso estacionario o no. Para obtener este gráfico, se ejecutará el siguiente código: ipc$Time &lt;- as.Date(ipc$Time) ggplot( data = ipc, aes(x = Time, y = ipc) ) + geom_line(colour = &quot;blue&quot;) (#fig:Serie_ipc)Evolución del IPC. Perido muestral: Enero 2002 - Marzo 2022 También puede ayudar a determinar si la serie es estacionario o no, la descomposición en tendencia, componente estacional estacional, componente cíclico e irregular de la serie del IPC y su representación gráfica. componentes_ts &lt;- decompose(ipc_ts) autoplot(componentes_ts) Figura 1.1: Descomposición aditiva del IPC Tanto en el gráfico de la serie original del IPC como en su descomposición se aprecia que la serie tiene tendencia y componente estacional, por lo tanto, no es estacionaria en media. Tampoco lo es en varianza. Si una serie no es estacionaria ni en media ni en varianza, para evitar problemas de cálculo matemático, es necesario empezar corriguiendo la no estacionariedad en varianza y, posteriormente, la no estacionariedad en media. Las transformaciones de Box-Cox son las más utilizadas para corregir el problema de no estacionariedad en varianza. De todas estas transformaciones, la más habitual es el logaritmo. Así, para obtener la representación gráfica del logaritmo de \\(Y_t\\), se puede ejecutar el siguiente código: logipc &lt;- log10(ipc_ts) ts_logipc &lt;- data.frame(value = logipc, Time = time(logipc)) ggplot( data = ts_logipc, aes(x = Time, y = logipc) ) + geom_line(colour = &quot;blue&quot;) Figura 1.2: Logaritmo del IPC Como se observa en el gráfico del logaritmo del IPC, la serie sigue sin ser estacionaria en media, ya que, tiene estacionalidad y una tendencia creciente. Para correguir la tendencia, se calculará una diferencia regular del logaritmo del IPC \\((dlogipc_{t} = log(ipc_t) - log(ipc_{t-1})\\). El código para calcular esta diferencia regulgar y su represenación gráfica es: dlogipc &lt;- diff(logipc, differences = 1) ts_dlogipc &lt;- data.frame(value = dlogipc, Time = time(dlogipc)) ggplot( data = ts_dlogipc, aes(x = Time, y = dlogipc) ) + geom_line(colour = &quot;green&quot;) Figura 1.3: Diferencia regular del Logaritmo del IPC Para correguir la estacionalidad, se calculará una diferencia estacional \\((d12logipc_{t} = log(ipc_t) - log(ipc_{t-12})\\). El código para calcular esta diferencia estacional y su represenación gráfica es: d12dlogipc &lt;- diff(dlogipc, 12) ts_d12dlogipc &lt;- data.frame(value = d12dlogipc, Time = time(d12dlogipc)) ggplot(data = ts_d12dlogipc, aes(x = Time, y = d12dlogipc)) + geom_line() + geom_line(colour = &quot;purple&quot;) Figura 1.4: Diferencia estacional de la diferencia regular del Logaritmo del IPC También, para comprobar si la serie es estacionaria en media o no, se puede utilizar el test de raíces unitarias de Dickey-Fuller. La hipótesis nula de este contraste implica la existencia de raíces unitarias y, por lo tanto, que la serie no es estacionaria, mientras que la hipótesis alternativa implica que sí es estacionaria. Si se realiza el test el test sobre la serie original (ipc_ts) y sobre la serie transformada del IPC (d12dLogIPC), se puede comprobar que esta última es la transformación estacionaria. El código utilizado para realizar este contraste es: adf.test(ipc_ts) #&gt; #&gt; Augmented Dickey-Fuller Test #&gt; #&gt; data: ipc_ts #&gt; Dickey-Fuller = -1.8396, Lag order = 6, p-value = 0.6433 #&gt; alternative hypothesis: stationary adf.test(d12dlogipc) #&gt; #&gt; Augmented Dickey-Fuller Test #&gt; #&gt; data: d12dlogipc #&gt; Dickey-Fuller = -3.3727, Lag order = 6, p-value = 0.06002 #&gt; alternative hypothesis: stationary Teniendo en cuenta el p-valor para los valores originales del IPC, para un nivel de significación del 5%, se acepta la hipótesis nula (y, por lo tanto, la serie no es estacionaria). Sin embargo, para la transformación estacionaria (d12dLogipc) se rechaza la \\(H_0\\). Por lo tanto, se puede concluir que dicha transformación la ha convertido en estacionaria. 1.3.1 Identificación o especificación del modelo Obtenida la transformación estacionaria, es necesario, calcular las funciones de autocorrelación simple (ACF) y parcial (PACF) estimadas. A partir de ellas se especificará el modelo más adecuado para la serie del IPC en el periodo muestral analizado. Los códigos R para ejecutar los comandos que permiten calcular la ACF y PACF de la transformación estacionaria del IPC, con 40 retardos, son respectivamente: acf(ts(d12dlogipc, frequency = 1), lag.max = 40, main = &quot;&quot;) Figura 1.5: ACF. Función de autocorrelación simple de d12dLogIPC pacf(ts(d12dlogipc, frequency = 1), lag.max = 40, main = &quot;&quot;) Figura 1.6: PACF. Función de autocorrelación parcial de d12dLogIPC Conseguida la transformación estacionaria, estas funciones son muy útiles para identificar los ordenes p y q de los procesos puros autorregresivo (AR(p)), de móviles (MA(q)) o mixto ARMA(p,q) de la parte regular. Del mismo modo, se procederá con órdenes P y Q correspondientes a los modelos SAR(P), SMA(Q) o SARMA(P,Q) de la parte estacional (por ejemplo, en el caso de series mensuales, habría que fijarse en los coeficientes correspondientes a los retardos 12, 24, 36, etc. para identificar el modelo de la parte estacional). El comportamiento de estas funciones según el tipo de modelo es el siguiente: En un MA(q) o SMA(Q): La función de autocorrelación simple muestral (ACF) se anula para órdenes superiores a q (o de Q en el caso estacional). Es decir, solo los q primeros coeficientes son significativos (o Q en el caso estacional). El resto de los coeficientes son nulos (o estadísticamente nulos en el caso de la ACF muestral). La función de autocorrelación parcial muestral (PACF) decrece de forma exponencial o de forma sinusoidal hacía cero. Ejemplo de la ACF y PAC teóricas, para un proceso estacionario (Yt), generado por un medias móviles de primer orden o MA(1). La ecuación que describe la dinámica de un modelo MA(1) o ARMA(0,1) es: \\[Y_t=a_t-\\theta a_{t-1}=a_t(1-\\theta L)\\] Donde, \\(L\\) es el operador de retardos y \\(a_t\\) es un ruido blanco, es decir: \\(E(a_t)=0 \\;\\forall t\\); \\(E(a_t^2)=\\sigma^2 \\; \\forall t\\); \\(E(a_ta_s)=0 \\quad\\forall t\\neq s\\). Este modelo siempre es estacionario y para que sea invertible es necesario que las raíces del polinomio estén fuera del circulo unidad o lo que es lo mismo que \\(|\\theta|&lt;1\\). La función de autocorrelación (ACF) es: \\[ \\rho(k)=\\left\\{ \\begin{array}{ll} \\frac{-\\theta}{(1+\\theta^2)} &amp; \\text{si } k=1 \\\\ 0 &amp; \\text{}\\forall k&gt;1 \\end{array} \\right. \\] La función de autocorrelación parcial (PACF) se obtiene de la siguiente forma: \\[ \\phi_{kk}=\\frac{-\\theta^k(1-\\theta^2)}{1-\\theta^{2(k+1)}}\\quad \\text {para} \\quad k\\geq 1 \\] En un AR(p) o SAR(P): La ACF decrece rápidamente de forma exponencial o sinusoidal hacia cero. La PACF se anula para órdenes superiores a p (o de P, en el caso estacional). Es decir, solo los p (o P en el caso estacional) primeros coeficientes son significativos. El resto de los coeficientes son nulos (o estadísticamente nulos si se analiza la PACF muestral). Ejemplo de la ACF y PAC teóricas, para un proceso estacionario (Yt), generado por un autorregresivo de primer orden o AR(1). La ecuación que describe la dinámica de un modelo AR(1) o ARMA(1,0) es: \\[Y_t=\\phi Y_{t-1}+ a_t\\] O bien en forma polinómica: \\(Y_t(1-\\phi L)=a_t\\) Donde, \\(L\\) es el operador de retardos y \\(a_t\\) es un ruido blanco. Este modelo siempre es invertible y para que sea estacionario es necesario que las raíces del polinomio de retardos estén fuera del circulo unidad o lo que es lo mismo que \\(|\\phi|&lt;1\\). La función de autocorrelación (ACF) es: \\[ \\rho(k)=\\phi\\rho(k-1)=\\phi^k \\] La PACF se obtiene de la siguiente forma: \\[ \\phi_{kk}=\\left\\{ \\begin{array}{ll} \\rho_1=\\phi &amp; \\text{si } k=1 \\\\ 0 &amp; \\text{}\\forall k&gt;1 \\end{array} \\right. \\] En un ARMA(p,q) o SARMA(P,Q): La ACF tiene un comportamiento irregular en los q (o de Q en el caso estacional) coeficientes. A partir del orden q (o de Q en el caso estacional), se comporta como la de un AR(p) (o un SAR(P) en el caso estacional). La PACF tiene un comportamiento irregular en los p coeficientes (o de P en el caso estacional). A partir del orden p (o de orden P en el caso estacional), se comporta como la de un MA(q) (o de un SMA(Q) en el caso estacional). Ejemplo de la ACF y PAC teóricas, para un proceso estacionario (Yt), que sigue un ARMA(1,1). La ecuación que describe la dinámica de un modelo ARMA(1,1) es: \\(Y_t=\\phi Y_{t-1}+ a_t-\\theta a_{t-1}\\) o bien en forma polinómica, \\(Y_t(1-\\phi L)=(1-\\theta L)a_t\\) Donde, \\(L\\) es el operador de retardos y \\(a_t\\) es un ruido blanco. Para que sea estacionario es necesario que las raíces del polinomio de la parte autorregresiva estén fuera del círculo unidad (o que \\(|\\phi|&lt;1\\)). Para que sea invertible es necesario que las raíces del polinomio de las medias móviles estén fuera del círculo unidad (o que \\(|\\theta |&lt;1\\)). Además, es necesario que no existan raíces comunes, es decir, \\(\\phi \\neq \\theta\\). La ACF es igual a: \\[ \\rho(k)=\\left\\{ \\begin{array}{ll} \\frac{(1-\\phi\\theta)(\\phi-\\theta)}{1+\\theta^2-2\\phi\\theta} &amp; \\text{si } k=1 \\\\ \\phi\\rho(k-1) &amp; \\text{}\\forall k&gt;1 \\end{array} \\right. \\] La función de autocorrelación parcial (FACP) se obtiene: \\[ \\begin{array}{l} \\phi_{11}=\\rho_1 \\\\ \\\\ \\phi_{22}=\\frac{\\begin{vmatrix} 1&amp;\\rho_1 \\\\ \\rho_1 &amp;\\rho_2\\end{vmatrix}}{{\\begin{vmatrix} 1&amp;\\rho_1 \\\\ \\rho_1 &amp;1\\end{vmatrix}}}=\\frac{\\rho_2-\\rho_1^2}{1-\\rho_1^2}\\\\ \\\\ \\phi_{33}=\\frac{\\begin{vmatrix} 1&amp;\\rho_1&amp;\\rho_1 \\\\ \\rho_1 &amp;1&amp;\\rho_2\\\\\\rho_2 &amp;\\rho_1&amp;\\rho_3\\end{vmatrix}}{{\\begin{vmatrix} 1&amp;\\rho_1&amp;\\rho_2 \\\\ \\rho_1 &amp;1&amp;\\rho_1\\\\\\rho_2 &amp;\\rho_1&amp;1 \\end{vmatrix}}}=\\frac{\\rho_1^3-\\rho_1\\rho_2(2-\\rho_2)+\\rho_3(1-\\rho_1^2)}{1-\\rho_2^2-2\\rho_1^2(1-\\rho_2)}\\\\ \\\\ \\vdots \\end{array} \\] Tendiendo en cuenta lo anterior, en el caso concreto del IPC, si se analiza el comportamiento de las ACF y PACF muestrales obtenidas para la transformación estacionaria, el modelo ARIMA sería un ARIMA(1,1,0)(0,1,1)12 o SARIMA(1,1,0)(0,1,1). 1.3.2 Estimación del modelo Para estimar los parámetros del modelo, ya que, el modelo es no lineal, se utiliza un procedimiento iterativo de estimación no lineal. El código en R que permite obtener las estimaciones de los parámetros y sus desviaciones típicas correspondiente al modelo propuesto es: modelo &lt;- arima(ipc_ts, c(1, 1, 0), c(0, 1, 1)) modelo #&gt; #&gt; Call: #&gt; arima(x = ipc_ts, order = c(1, 1, 0), seasonal = c(0, 1, 1)) #&gt; #&gt; Coefficients: #&gt; ar1 sma1 #&gt; 0.4665 -0.8302 #&gt; s.e. 0.0701 0.0860 #&gt; #&gt; sigma^2 estimated as 0.1082: log likelihood = -77.76, aic = 161.51 1.3.3 Diagnosis, validación y contrastación En la metodología ARIMA desarrollada por Box y Jenkins es necesario determinar si el modelo propuesto es correcto o no y si se ajusta o no, correctamente, a los datos de la muestra. Para ello, fundamentalmente, es necesario comprobar que: Los parámetros del modelo cumplen con las condiciones de estacionariedad e invertibilidad. El modelo estimado para el IPC, es estacionario e invertible, ya que, el AR(1) estimado para la parte regular es invertible y como el parámetro estimado es \\(|0.4665| &lt;1\\), también es estacionario. La parte estacional sigue un SMA(1) que es estacionario y como el parámetro estimado \\(|-0.8302|&lt;1\\), también es invertible. Los parámetros estimados son estadísticamente significativos. Para comprobar si los parámetros son estadísticamente significativos o no. O también, para comprobar si se ha sobreparametrizado o infraparametrizado, se realiza un contraste de significatividad individual para cada uno de ellos. Por ejemplo, para el parámetro del AR(1) sería: \\[ \\begin{array}{l} H_0:\\phi=0 \\\\ H_1:\\phi\\neq 0 \\end{array} \\] Para realizar el contraste se utiliza el estadístico \\(t\\) que sigue una distribución t-Student con \\((n-k)\\) grados de libertad. Este estadístico t se calcula dividiendo el valor estimado del parámetro entre su correspondiente error estándar. En concreto para los parámetros estimados de este modelo sería: \\[ t=\\frac{0,4675}{0,0701} = 6,669 \\quad y\\quad |t|=|\\frac{-0,8302}{0,0860}|=9,653 \\] Como, para un nivel de significación del 5% y los grados de libertad, el valor de la t-Student es aproximadamente 1,96, se rechaza la \\(H_0\\) y, por lo tanto, son estadísticamente distintos de cero. Los residuos del modelo son ruido blanco. En la fase de validación es necesario comprobar que los residuos del modelo estimado son consistentes con el supuesto de ruido blanco. El siguiente código R, permite obtener los residuos, residuos &lt;- residuals(modelo) ts_residuos &lt;- data.frame(value = residuos, Time = time(residuos)) ggplot(data = ts_residuos, aes(x = Time, y = residuos)) + geom_line() + geom_line(colour = &quot;red&quot;) Figura 1.7: Gráfico de los residuos Para contrastar que los residuos están incorrelacionados y que se comportan como un ruido blanco, se puede utilizar el estadístico de Box-Ljung. En este caso concreto, su hipótesis nula, es que los primero 40 coeficientes de correlación son cero frente a la hipótesis alternativa de que no lo sean. Para un nivel de significación del 5%, se acepta \\(H_0\\), ya que, el p-valor de la X-squared es igual a 0.5448. El código R para obtener el test de Box-Ljung es, Box.test(residuos, type = &quot;Ljung-Box&quot;) #&gt; #&gt; Box-Ljung test #&gt; #&gt; data: residuos #&gt; X-squared = 0.36682, df = 1, p-value = 0.5447 Además, se puede calcular ACF y PACF de los residuos para comprobar que están incorrelacionados. Los correlogramas obtenidos, con el código siguiente, muestran que los residuos son ruido blanco. par(mfrow = c(2, 1), mar = c(1, 1, 1, 1) + 0.1) acf(ts(residuos, frequency = 1), lag.max = 40) pacf(ts(residuos, frequency = 1), lag.max = 40) Figura 1.8: ACF y PACF estimadas de los residuos También se debe comprobar la capacidad de ajuste del modelo comparando los valores observados y estimados, utilizando el código R siguiente: plot(ipc_ts, lwd = 2) lines(ipc_ts - modelo$residuals, col = &quot;red&quot;, lwd = 2) Figura 1.9: Ajuste con el modelo estimado Los criterios de información como el AIC se pueden utilizar para comparar entre diferentes modelos cuál se ajusta mejor a los datos. Así, será mejor el modelo que tenga un menor valor del criterio de información utilizado. 1.3.4 Predicción Después de comprobar que el modelo es adecuado se puede utilizar para predecir. Para obtener 12 predicciones, sus correspondientes intervalos de predicción al 80% y 95% de confianza y su representación gráfica, con el modelo ARIMA estimado, el código R que se puede utilizar es: forecast::forecast(modelo, h = 12) #&gt; Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 #&gt; Apr 2022 109.7116 109.2900 110.1332 109.0668 110.3564 #&gt; May 2022 110.5926 109.8443 111.3410 109.4481 111.7371 #&gt; Jun 2022 111.1030 110.0714 112.1346 109.5253 112.6806 #&gt; Jul 2022 110.5518 109.2748 111.8289 108.5987 112.5050 #&gt; Aug 2022 110.7714 109.2787 112.2641 108.4885 113.0543 #&gt; Sep 2022 111.0288 109.3435 112.7140 108.4515 113.6060 #&gt; Oct 2022 111.9631 110.1034 113.8228 109.1189 114.8073 #&gt; Nov 2022 112.1740 110.1540 114.1939 109.0847 115.2632 #&gt; Dec 2022 112.3896 110.2209 114.5583 109.0728 115.7064 #&gt; Jan 2023 111.6049 109.2968 113.9130 108.0750 115.1349 #&gt; Feb 2023 111.6666 109.2270 114.1061 107.9355 115.3976 #&gt; Mar 2023 112.5012 109.9369 115.0656 108.5794 116.4231 predicciones &lt;- forecast::forecast(modelo, h = 12) autoplot(predicciones) Figura 1.10: Predicciones con el modelo ARIMA(1,1,0)(0,1,1)12 estimado RESUMEN Los modelos univariantes de series temporales ARIMA son muy útiles para proponer el modelo que mejor capte la dinámica de una serie temporal. Fundamentalmente, se pueden utilizar para predicir y la toma de decisiones. "],["mineria-textos.html", "Capítulo 2 Minería de textos 2.1 Introducción 2.2 Conceptos y tareas fundamentales 2.3 Análisis de sentimientos 2.4 Caso de aplicación", " Capítulo 2 Minería de textos Víctor Casero, Ángela Celis y María Lozano Zahonero1 2.1 Introducción En la actualidad, entre el 80 % y el 90 % de los datos que se generan diariamente son datos no estructurados (vistos en el capítulo \\(\\ref{datos-no-sql}\\)). Un ejemplo típico de datos no estructurados son los textos, desde los comentarios o mensajes de las redes sociales, reseñas, blogs y microblogs, chats o whatsapp hasta las noticias periodísticas, los discursos políticos o las obras literarias. En consecuencia, aprender a procesar y analizar datos exige aprender a procesar y analizar textos. Los textos precisan, sin embargo, un tratamiento especial. A diferencia de la mayoría de los datos que se tratan en este libro, que son datos estructurados, los datos textuales requieren que se les otorgue un orden y estructura para su manejo y análisis con el software R. Además, al utilizar un lenguaje natural –es decir, un idioma como, por ejemplo, el español, el chino o el inglés–, los textos no pueden ser procesados directamente por un ordenador. Es preciso ‘traducirlos’ antes a un lenguaje formal que los ordenadores puedan entender. La minería de textos (en inglés, text mining), también conocida como análisis de textos (en inglés, text analysis), puede definirse como el proceso para detectar, extraer, clasificar, analizar y visualizar la información no explícita que contienen los textos, transformando los datos textuales en datos estructurados y el lenguaje natural en lenguaje formal a fin de determinar, después, de manera automática patrones recurrentes y desviaciones en los mismos. La minería de textos utiliza muchas técnicas y métodos diferentes, la mayor parte de los cuales proceden del procesamiento del lenguaje natural (PLN), un ámbito de la inteligencia artificial que se ocupa de la comunicación entre los seres humanos y las máquinas mediante el tratamiento computacional del lenguaje humano. Este capítulo constituye una primera aproximación a la minería de textos con R. Su objetivo es proporcionar un marco teórico y aplicado básico de este ámbito. Para ello, en el segundo apartado, se presentan los conceptos y fases fundamentales de la minería de textos, mencionando algunos paquetes de R que permiten realizar análisis textuales de distintos tipos. El tercer apartado está dedicado al análisis de sentimientos, que constituye uno de los campos de la minería de textos de mayor desarrollo en la actualidad. Cierra el capítulo un caso práctico, en el que se aplica y se amplía lo estudiado anteriormente. Unas útiles referencias sobre el tema son (fradejas?) y (jockers2014?). 2.2 Conceptos y tareas fundamentales Lo primero que se necesita para hacer un análisis de textos son los textos. Esta afirmación podría parecer banal, pero no lo es. El volumen de textos en circulación es ingente, pero, en la mayor parte de los casos, es necesario realizar una serie de operaciones complejas para poder extraer y recopilar los datos textuales que se quiere analizar. Es también difícil muchas veces acceder después a estos datos ya que los textos pueden presentar formatos muy heterogéneos, no siempre interpretables o fáciles de convertir en un formato interpretable. Baste pensar, por ejemplo, en una nota escrita a mano. Dado que este capítulo es una primera aproximación a la minería de textos, se parte del supuesto de que el texto o textos están disponibles ya en un fichero, denominado corpus, legible por R. En este contexto, corpus es la colección de textos con el mismo origen, por ejemplo, el corpus de las obras de un autor, que para poder manejarse requieren metadatos con detalles adicionales. 2.2.1 Preparación de los datos Una vez constituido el corpus, la primera fase es la preparación de los datos. Los textos suelen contener un cierto grado de ‘suciedad’, es decir, elementos que alteran o impiden el análisis. De una buena ‘limpieza’ inicial, dependerá en gran parte la validez de los resultados que se obtengan. Entre las operaciones de ‘limpieza’ generales figuran una serie de transformaciones cuya finalidad es evitar el recuento incorrecto de palabras como el cambio de mayúsculas por minúsculas y la eliminación de los signos de puntuación, los números y los espacios en blanco en exceso. La siguiente operación de preparación, que tiene un importante peso en el análisis, es la eliminación de las stopwords o palabras vacías. En la lengua no todas las palabras tienen el mismo tipo de significado. Las palabras con significado léxico, como mesa o corpus, son palabras a las que corresponde un concepto que se puede definir o explicar. Otras palabras, sin embargo, son palabras funcionales, cuyo contenido es puramente gramatical. Son palabras como el artículo el, la preposición de o la conjunción o: se puede explicar cómo se usan, pero no definirlas asociándolas a un concepto porque carecen de contenido léxico-semántico. Las palabras vacías son, con gran diferencia respecto de las palabras léxicas, las más frecuentes de la lengua, pero, dado su escaso o nulo significado léxico, en los análisis de tipo semántico, como el análisis de sentimientos o el modelado de temas, carecen de valor informativo, por lo que es conveniente eliminarlas. No es aconsejable eliminarlas, sin embargo, en otros tipos de análisis, como los análisis estilométricos, donde tienen un importante valor informativo como se verá en el apartado 2.2.3. Las palabras vacías pertenecen a clases cerradas, es decir a clases de palabras con un número de elementos limitado, finito. Es posible confeccionar, por tanto, listas de palabras vacías para permitir su eliminación. En el caso aplicado, se aprenderá a usar estas listas y se podrá apreciar con detalle la diferente información que proporciona una tabla de frecuencias con y sin palabras vacías. 2.2.2 Segmentación del texto: tokenización La segunda fase de la minería de textos consiste en la segmentación del texto, denominada también tokenización. El texto se divide en token, secuencias de texto con valor informativo. De esta manera, se pasa del lenguaje natural a un lenguaje formal comprensible por el software, dándole formato de ‘vector’ o ‘tabla’. Así se pueden aplicar algunas de las herramientas que se utilizan con datos numéricos para manejar el texto y obtener resúmenes y visualizaciones que muestren la información no explícita contenida en él en forma de patrones recurrentes. Generalmente, los token son palabras, es decir, secuencias de caracteres entre dos espacios en blanco y/o signos de puntuación, pero pueden ser también oraciones, líneas, párrafos o n-gramas. Como se verá en el caso aplicado, un primer análisis del significado consiste en eliminar las palabras vacías y obtener las frecuencias2 de las palabras con valor informativo para responder a la pregunta ‘¿Qué se dice?’ (silge2017?). N-gramas El análisis puede proseguir estudiando la frecuencia de los n-gramas, secuencias de n palabras consecutivas en el mismo orden. Se tienen así bigramas o 2-gramas (secuencias de dos palabras), trigramas o 3-trigramas (secuencias de tres palabras), etc. El estudio de los n-gramas responde al principio de Firth: ‘You shall know a word by the company it keeps’ (firth?). Este principio es el fundamento del llamado análisis de colocaciones: para conocer el significado de una palabra es preciso conocer las palabras con las que aparece, el contexto relevante. En un sentido amplio, el análisis de colocaciones consiste en examinar los contextos izquierdo y/o derecho de una palabra. La segmentación en n-gramas permite tener en cuenta este contexto relevante que indicará, por ejemplo, que banco es con toda probabilidad un asiento en las secuencias banco de madera o banco en la terraza, pero no lo es en secuencias como banco de peces, banco de arena, banco de inversiones, banco de datos o banco de pruebas. La división en n-gramas permitirá también considerar en el análisis, al menos hasta cierto punto, el peso de la ambigüedad, la negación o el distinto significado que pueden tener las palabras según el ámbito temático. Por ejemplo, la forma larga no tiene el mismo significado en los bigramas falda larga, mano larga y cara larga, ni tiene tampoco el mismo valor informativo en es larga / no es larga o en de larga experiencia (valor positivo) y en se me hizo larga (valor negativo). En el caso aplicado, se verá en la práctica la segmentación en n-gramas y cómo la visualización de redes contribuye a complementar el análisis. Stemming y lematización La tokenización se puede refinar mediante el stemming, o reducción de las palabras ‘flexionadas’ a su raíz, y la lematización, o extracción del lema de cada palabra. Un ejemplo de stemming sería reducir las palabras texto, textos, textual y textuales, que R cuenta como cuatro palabras diferentes, a la raíz ‘text’. El stemming puede proporcionar un recuento más preciso en algunos casos, pero en otros, al eliminar los sufijos de las palabras, puede crear confusión. Además, como en el ejemplo anterior, las raíces pueden no coincidir con palabras existentes, lo que hace que sean difíciles de interpretar y resulten extrañas si se visualizan en nubes de palabras. Con la lematización se reducen las formas flexionadas de una misma palabra al lema, que es la forma que encabeza la entrada de la palabra en el diccionario. Por ejemplo, si se quiere buscar el significado de la palabra niñas no se encontrará como tal sino bajo el lema niño y si se quiere buscar iremos se tendrá que buscar el lema ir. En el caso anterior, la lematización reduciría las formas texto, textos, textual y textuales a dos lemas: texto y textual. La lematización evita la dispersión de significado en varias formas, pero a veces es compleja y puede conducir a la pérdida de información pertinente. 2.2.3 Campos de aplicación de la minería de textos La minería de textos tiene varios campos de aplicación. Entre ellos destacan tres: el análisis de sentimientos, el modelado de temas o tópicos y el análisis estilométrico o estilometría. El análisis de sentimientos se tratará con detalle en la sección 2.3 y en el caso de aplicación (sección 2.4.4), mientras que el modelado de temas o tópicos, se ilustrará en el capítulo \\(\\ref{nlp-textil}\\). Dadas las limitaciones del manual sólo se presentan brevemente el modelado de temas y la estilometría. El modelado de temas (en inglés, topic modelling), como su propio nombre indica, tiene por objeto identificar los temas principales sobre los que versa el texto haciendo uso de técnicas de clasificación no supervisada del campo del aprendizaje automático, como por ejemplo LDA (Latent Dirichlet Allocation). La estilometría es una aplicación de la minería de textos cuya finalidad consiste en determinar las relaciones existentes entre el estilo de los textos y los metadatos incluidos en ellos. Se utiliza principalmente en la atribución de autoría. El concepto base es el de huella lingüística, constituida por el conjunto de rasgos lingüísticos que caracterizan el estilo de un autor como un estilo individual y único y permiten identificarlo. Un punto clave es que, contrariamente a lo que podría pensarse, los rasgos que conforman en mayor medida la huella lingüística son los que tienen un mayor índice de frecuencia. La mayor parte de los enfoques utilizan el vector de las ‘palabras más frecuentes’ (MFW, por su sigla en inglés), que son, como se ha visto antes, las palabras vacías y no las palabras con significado léxico, para determinar el estilo de un autor. Esto es debido fundamentalmente a que las palabras vacías se usan de manera involuntaria e inconsciente, configurando de esta manera, sin ningún tipo de filtros racionales, una clave estilística idiosincrásica (zahonero2020?). De lo anterior se deduce fácilmente que en este tipo de análisis no deben eliminarse las palabras vacías. En la actualidad, el análisis estilométrico se usa en ámbitos muy dispares: desde la criminología o los servicios de inteligencia para identificar a los autores de mensajes o notas en casos de asesinatos, terrorismo, secuestro o acoso, por ejemplo, hasta el derecho civil o la literatura en cuestiones de derechos de autor o detección de plagio, entre muchas otras cuestiones. 2.2.4 Minería de textos en R En R existen diversos paquetes y funciones que facilitan la minería de textos, entre los que destacan: tidytext: con la filosofía del tidyverse puede combinarse con los conocidos paquetes dplyr, broom, ggplot2, etc. Se puede destacar la función unnest_tokens() que automatiza el proceso de tokenización y el almacenamiento en formato tidy en un único paso. tm: destaca por tener soporte back-end de base de datos integrado, gestión avanzada de metadatos y soporte nativo para leer en varios formatos de archivo. tokenizers: incluye tokenizadores de palabras, oraciones, párrafos, n-gramas, tweets, expresiones regulares, así como funciones para contar caracteres, palabras y oraciones, y para dividir textos más largos en documentos separados, cada uno con el mismo número de palabras. wordcloud: permite visualizar nubes de palabras. Las palabras más frecuentes aparecen en mayor tamaño permitiendo de un vistazo obtener las palabras clave del texto. quanteda: maneja matrices de documentos-términos y destaca en tareas cuantitativas como recuento de palabras o sílabas. syuzhet: incluye distintas funciones que facilitan el análisis de textos, en particular el análisis de sentimientos de textos literarios. gutenbergr: almacena las obras del proyecto Gutenberg3; muy útil si se quieren analizar textos literarios. 2.3 Análisis de sentimientos El análisis de sentimientos (en inglés, sentiment analysis) es una aplicación de la minería de textos que tiene como finalidad la detección, extracción, clasificación, análisis y visualización de la dimensión subjetiva asociada a los temas o tópicos presentes en los textos. La dimensión subjetiva comprende no solo los sentimientos, sino también las emociones, sensaciones y estados afectivos y anímicos, así como las opiniones, creencias, percepciones, puntos de vista, actitudes, juicios y valoraciones. De ahí que reciba también el nombre de minería de opinión (en inglés, opinion mining) (zahonero2020?). El análisis de sentimientos asigna a esta dimensión subjetiva una polaridad, que puede ser positiva o negativa (pang2008opinion?). Algunas técnicas añaden además una polaridad neutra. En algunos casos, el análisis de sentimientos se refina hasta llegar a las emociones básicas: este subcampo del análisis de sentimientos se conoce como detección de emociones. La primera aplicación del análisis de sentimientos fue la investigación de mercados. A partir del año 2000, se registra un crecimiento exponencial de textos como reseñas, chats, foros, blogs, microblogs o comentarios y mensajes de las redes sociales, en los que predomina la expresión de emociones y opiniones personales. Mediante el análisis de sentimientos se extrae de ellos información que permite conocer los gustos del consumidor y diseñar productos a su medida. Esta idea se extenderá después a otros ámbitos, en especial a aquellos en los que predomina la comunicación persuasiva como las campañas publicitarias o políticas. Recientemente, ha empezado a utilizarse también con fines predictivos y preventivos en muchas esferas: desde cuáles son los políticos, las empresas, las películas, canciones u obras literarias que obtendrán un mayor rendimiento, mejores resultados o más votos o ventas hasta cómo detectar y prevenir, por ejemplo, conductas suicidas mediante el análisis de mensajes en las redes sociales. En el análisis de sentimientos y la detección de emociones existen dos enfoques principales: el enfoque basado en el aprendizaje automático (machine learning), en el que se usan algoritmos de aprendizaje supervisado, y el enfoque semántico basado en diccionarios o lexicones. Este último enfoque es el que se verá en detalle en el caso aplicado. En R están implementados varios lexicones para el análisis de sentimientos. Dos de los más utilizados son bing, de Bing Liu y colaboradores (liu2015sentiment?), y NRC, de Saif Mohammad y Peter Turney, ambos incluidos tanto en el paquete tidytext como en syuzhet (jockers2017?). Estos lexicones tienen en común que están basados en unigramas, es decir, en palabras sueltas, y que tienen como idioma original el inglés, si bien hay disponibles versiones traducidas automáticamente a distintas lenguas. La diferencia principal entre los dos lexicones es que bing clasifica las palabras de forma binaria en polaridad positiva/negativa, mientras que NRC además de la polaridad positiva/negativa permite detectar también ocho emociones básicas (ira, miedo, anticipación, confianza, sorpresa, tristeza, alegría, asco). En el caso aplicado se compararán ambos diccionarios. Como se verá, los resultados del análisis dependerán en buena medida del lexicón elegido, así como del idioma del texto y de si el lexicón se elaboró originalmente en ese idioma o es una versión traducida automáticamente de otra lengua. 2.4 Caso de aplicación 2.4.1 Declaración institucional del estado de alarma 2020 La ‘Declaración institucional del presidente del gobierno sobre el estado de alarma en la crisis del coronavirus’ (en adelante, ‘la Declaración’), dada en La Moncloa el 13 de marzo de 2020 es el objeto de análisis. Ésta se puede encontrar en el paquete CDR que acompaña este libro. Se le van a aplicar las operaciones y técnicas mencionadas en la sección 2.2. load(&quot;data/declaracion-estado-alarma.Rdata&quot;) 2.4.2 Segmentación en palabras y oraciones Las primeras tareas del análisis son la preparación, limpieza y segmentación o tokenización de los textos. A continuación, se verá una segmentación en palabras individuales. La función tokenize_words() del paquete tokenizers prepara el texto convirtiéndolo a minúsculas, elimina todos los signos de puntuación y finalmente segmenta el texto en palabras. library(tokenizers) palabras &lt;- tokenize_words(declaracion) tokenizers::count_words(declaracion) #&gt; [1] 922 Con la última sentencia se obtiene la longitud de la Declaración, el número de palabras utilizadas: 922. La frecuencia de cada palabra se pueden obtener y presentar con el código de abajo. La primera sentencia crea la tabla de frecuencias, la tercera la transforma en el tipo tibble, creando la columna recuento, y ordena la tabla de forma descendente, de mayor a menor frecuencia. tabla &lt;- table(palabras[[1]]) (tabla &lt;- tabla |&gt; tibble(palabra = names(tabla), recuento = as.numeric(tabla)) |&gt; arrange(desc(recuento))) #&gt; # A tibble: 390 × 3 #&gt; tabla palabra recuento #&gt; &lt;table[1d]&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 43 de 43 #&gt; 2 41 y 41 #&gt; 3 35 la 35 #&gt; 4 31 a 31 #&gt; 5 26 los 26 #&gt; 6 22 en 22 #&gt; 7 20 que 20 #&gt; 8 17 el 17 #&gt; 9 14 al 14 #&gt; 10 14 para 14 #&gt; # … with 380 more rows En la primera fila de la salida se indican las dimensiones de la tibble, por lo que se puede ver que en esta Declaración hay 390 ‘palabras’ distintas (considera los números como palabras). El resultado son las palabras más utilizadas en el texto, que, como puede apreciarse, son palabras vacías. Esto no debería sorprender porque, como ya se ha visto, estas palabras son las más frecuentes. En el siguiente apartado, se verá cómo eliminarlas para obtener datos con valor informativo. Para otras formas de segmentar el texto (oraciones, párrafos, tweets, etc.): véase ?tokenize_words. Por ejemplo, para segmentar en oraciones: oraciones &lt;- tokenize_sentences(declaracion) count_sentences(declaracion) #&gt; [1] 44 Las tres primeras oraciones y la última se obtienen con el siguiente código. oraciones[[1]][1:3] # primeras 3 oraciones #&gt; [1] &quot;Buenas tardes.&quot; #&gt; [2] &quot;Estimados compatriotas.&quot; #&gt; [3] &quot;En el día de hoy, acabo de comunicar al Jefe del Estado la celebración, mañana, de un Consejo de Ministros extraordinario, para decretar el Estado de Alarma en todo nuestro país, en toda España, durante los próximos 15 días.&quot; oraciones[[1]][count_sentences(declaracion)] # última #&gt; [1] &quot;Buenas tardes.&quot; También podría medirse la longitud de cada oración, en número de palabras, típicamente para comparaciones con otros textos. Para ello se debe separar cada oración en palabras y con la función sapply se puede obtener la longitud de cada oración, que puede verse en la Figura 2.1. palabras_oracion &lt;- tokenize_words(oraciones[[1]]) longitud_o &lt;- sapply(palabras_oracion, length) head(longitud_o) #&gt; [1] 2 2 39 33 33 32 Figura 2.1: Número de palabras en cada oración de la Declaración 2.4.3 Análisis exploratorio Eliminación de palabras vacías Se va a hacer uso del paquete stopwords que contiene listas de palabras vacías en diferentes idiomas. Para el ejemplo se define una tabla con la misma estructura que la tabla de la Declaración con las 308 palabras vacías españolas que tiene el paquete: library(stopwords) tabla_stopwords &lt;- tibble(palabra = stopwords(&quot;es&quot;)) La siguiente sentencia ‘limpia’ la tabla de la Declaración quitando las palabras vacías españolas. Además, se hace uso de la función kable para una visualización más sofisticada de la tabla (con la longitud que se desee): tabla &lt;- tabla |&gt; anti_join(tabla_stopwords) knitr::kable(tabla[1:10, ], caption = &quot;Palabras más frecuentes (sin palabras vacías)&quot; ) Tabla 2.1: Palabras más frecuentes (sin palabras vacías) tabla palabra recuento 9 virus 9 7 recursos 7 5 social 5 4 alarma 4 4 conjunto 4 4 emergencia 4 4 españa 4 4 semanas 4 4 va 4 3 cada 3 El resultado, Tabla 2.1, se puede considerar el primer análisis léxico con valor informativo: la palabra más frecuente es virus, seguida de recursos y social. Se podría ver que en total hay 319 palabras distintas. El método de eliminar palabras con el paquete stopwords no es perfecto. Por ejemplo, va y cada (posiciones 9 y 10 de la tabla) no son muy informativas. En estos casos, como se ha visto antes, se pueden utilizar listas de palabras vacías de otros paquetes como, por ejemplo tidytext o tokenizers o el listado en español propuesto por (fradejas?), o pueden confeccionarse listas ad hoc. Nubes de palabras Una manera habitual de mostrar la información de forma visual es con las denominadas nubes de palabras, acudiendo a la función wordcloud del paquete con el mismo nombre. Al contener dicha función un componente aleatorio se fija con set.seed() (para la reproducibilidad del gráfico por parte del lector). set.seed(12) library(wordcloud) wordcloud(tabla$palabra, tabla$recuento, max.words = 50, colors = rainbow(3) ) Figura 2.2: Nube de palabras más frecuentes de la Declaración El resultado se muestra en la Figura 2.2. Como se puede observar, el tamaño de letra de la palabra, y en este caso también el color, están relacionados con su frecuencia. 2.4.4 Análisis de sentimientos y detección de emociones Lexicón bing El lexicón bing, como se ha visto en la sección 2.3, es uno de los repertorios léxicos para el análisis de sentimientos que se pueden encontrar en R. Es un diccionario de polaridad (positiva/negativa) cuyo idioma original es el inglés. Se puede obtener con la función get_sentiments del paquete tidytext. Contiene 2005 palabras positivas y 4781 palabras negativas, por lo que hay un marcado sesgo hacia la polaridad negativa. Para ilustrar el uso de bing, se ha traducido al inglés (automáticamente) la Declaración. A continuación se carga el texto y se genera el objeto tabla replicando el procedimiento descrito arriba de preparación, limpieza, segmentación en palabras, eliminación de palabras vacías (obviamente, en idioma inglés). declaracion_EN &lt;- paste(read_lines(&quot;data/EN-declaracion-estado-alarma.txt&quot;), collapse = &quot;\\n&quot;) tabla &lt;- table(tokenize_words(declaracion_EN)[[1]]) tabla &lt;- tibble( word = names(tabla), recuento = as.numeric(tabla) ) tabla &lt;- tabla |&gt; anti_join(tibble(word = stopwords(&quot;en&quot;))) |&gt; arrange(desc(recuento)) Los sentimientos positivos de la Declaración se pueden obtener con: library(tidytext) pos &lt;- get_sentiments(&quot;bing&quot;) |&gt; dplyr::filter(sentiment == &quot;positive&quot;) pos_EN &lt;- tabla |&gt; semi_join(pos) knitr::kable(pos_EN) Análogamente, se pueden obtener los sentimientos negativos. Las siete palabras más frecuentes de cada tipo que aparecen en la Declaración se presentan conjuntamente en la Tabla 2.2. Lexicón NRC Para poder observar las similitudes y diferencias en el análisis según el lexicón elegido, se aplica también NRC a la Declaración (véase la Tabla 2.2). Tabla 2.2: Palabras más frecuentes de la Declaración utilizando bing y NRC positivas bing fr negativas bing fr positivas NRC fr negativas NRC fr extraordinary 6 virus 9 resources 7 virus 9 protect 4 alarm 4 extraordinary 6 alarm 4 work 4 emergency 4 protect 4 emergency 4 like 3 vulnerable 3 maximum 3 government 3 decisive 2 difficult 2 public 3 discipline 2 good 2 hard 2 council 2 avoid 1 adequate 1 unfortunately 2 good 2 combat 1 Con el léxico NRC pueden detectarse emociones. La misma palabra puede tener asociada distintas emociones/sentimientos. En la Figura 2.3 se puede observar la dispar frecuencia de palabras de cada tipo: emo &lt;- get_sentiments(&quot;nrc&quot;) emo |&gt; ggplot(aes(sentiment)) + geom_bar(aes(fill = sentiment), show.legend = FALSE) Figura 2.3: Gráfico de barras con la frecuencia de las emociones del lexicón NRC El análisis de sentimientos y la detección de emociones de la Declaración mediante NRC se puede realizar con el siguiente código en el que se obtiene la tabla de frecuencias por emociones y sentimientos: emo_tab &lt;- tabla |&gt; inner_join(emo) head(emo_tab, n = 7) #&gt; # A tibble: 7 × 3 #&gt; word recuento sentiment #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 virus 9 negative #&gt; 2 resources 7 joy #&gt; 3 resources 7 positive #&gt; 4 resources 7 trust #&gt; 5 extraordinary 6 positive #&gt; 6 alarm 4 fear #&gt; 7 alarm 4 negative Como se ha mencionado antes, algunas palabras tienen asociados distintos sentimientos, por ejemplo, resources. La información de la tabla se puede visualizar bien con gráfico de barras (Figura 2.4) bien con nubes de palabras (Figura 2.5). emo_tab |&gt; dplyr::count(sentiment) |&gt; ggplot(aes(x = sentiment, y = n)) + geom_bar(stat = &quot;identity&quot;, aes(fill = sentiment), show.legend = FALSE) + geom_text(aes(label = n), vjust = -0.25) Figura 2.4: Frecuencia de emociones de la Declaración utilizando NRC Entre las distintas opciones para dibujar nubes de palabras para el análisis de sentimientos es interesante la que se obtiene con el paquete syuzhet dado que permite visualizar las palabras agrupadas por emociones. Su obtención requiere distintos pasos en los que primero las palabras se agrupan por emoción, y después se organizan en una matriz de documentos con la función TermDocumentMatrix del paquete tm. Finalmente la función comparison.cloud permite visualizar el gráfico (tiene distintos argumentos opcionales que admiten distintas posibilidades). En el ejemplo que figura a continuación solo se han escogido tres emociones4: library(syuzhet) palabras_EN2 &lt;- get_tokens(declaracion_EN) emo_tab2 &lt;- get_nrc_sentiment(palabras_EN2, lang = &quot;english&quot;) emo_vec &lt;- c( paste(palabras_EN2[emo_tab2$anger &gt; 0], collapse = &quot; &quot;), paste(palabras_EN2[emo_tab2$anticipation &gt; 0], collapse = &quot; &quot;), paste(palabras_EN2[emo_tab2$disgust &gt; 0], collapse = &quot; &quot;) ) library(tm) corpus &lt;- Corpus(VectorSource(emo_vec)) TDM &lt;- as.matrix(TermDocumentMatrix(corpus)) colnames(TDM) &lt;- c(&quot;anger&quot;, &quot;anticipation&quot;, &quot;disgust&quot;) set.seed(1) comparison.cloud(TDM, random.order = FALSE, colors = c(&quot;firebrick&quot;, &quot;forestgreen&quot;, &quot;orange3&quot;), title.size = 1.5, scale = c(3.5, 1), rot.per = 0 ) Figura 2.5: Nube de palabras de tres emociones NRC seleccionadas 2.4.5 N-gramas El siguiente código muestra la obtención de n-gramas con tokenizers. bigramas &lt;- tokenize_ngrams(declaracion, n = 2, stopwords = tabla_stopwords$palabra ) head(bigramas[[1]], n = 3) #&gt; [1] &quot;buenas tardes&quot; &quot;tardes estimados&quot; &quot;estimados compatriotas&quot; trigramas &lt;- tokenize_ngrams(declaracion, n = 3, stopwords = tabla_stopwords$palabra ) head(trigramas[[1]], n = 3) #&gt; [1] &quot;buenas tardes estimados&quot; &quot;tardes estimados compatriotas&quot; #&gt; [3] &quot;estimados compatriotas día&quot; Se ha procedido a eliminar de los bigramas y trigramas aquellas combinaciones con al menos una palabra vacía (stopword). Se procede ahora a obtener los bigramas con tidytext. Para el resto de n-gramas el procedimiento es análogo haciendo las modificaciones oportunas. En el último paso se ordenan por frecuencia (de mayor a menor): declara2 &lt;- tibble(texto = declaracion) bigramas &lt;- declara2 |&gt; unnest_tokens(bigram, texto, token = &quot;ngrams&quot;, n = 2) |&gt; dplyr::count(bigram, sort = TRUE) bigramas[1:5, ] #&gt; # A tibble: 5 × 2 #&gt; bigram n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 todos los 6 #&gt; 2 de la 5 #&gt; 3 de los 5 #&gt; 4 del estado 5 #&gt; 5 estado de 5 Una forma de eliminar las palabras vacías: bigramas_limpios &lt;- bigramas |&gt; separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) |&gt; dplyr::filter(!word1 %in% tabla_stopwords$palabra) |&gt; dplyr::filter(!word2 %in% tabla_stopwords$palabra) |&gt; unite(bigram, word1, word2, sep = &quot; &quot;) bigramas_limpios[1:5, ] #&gt; # A tibble: 5 × 2 #&gt; bigram n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 autoridades sanitarias 2 #&gt; 2 buenas tardes 2 #&gt; 3 disciplina social 2 #&gt; 4 haga falta 2 #&gt; 5 ministros extraordinario 2 Significado y contexto Como se ha visto en la sección 2.2.2, con los n-gramas se puede hacer un análisis de colocaciones para extraer los distintos significados y valores informativos a partir del contexto. En este caso, se puede ver cómo la palabra atender cambia de sentido cuando va precedida de no o sin. A continuación, se filtran los bigramas cuya primera palabra es no: bigramas_no &lt;- bigramas |&gt; separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) |&gt; dplyr::filter(word1 == &quot;no&quot;) |&gt; dplyr::count(word1, word2, sort = TRUE) bigramas_no #&gt; # A tibble: 3 × 3 #&gt; word1 word2 n #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 no atiende 1 #&gt; 2 no cabe 1 #&gt; 3 no es 1 Estos resultados se pueden utilizar para el análisis de sentimientos y la detección de emociones. 2.4.6 Análisis de redes Se proporcionan las instrucciones para realizar un análisis básico de redes, utilizando los paquetes igraph y ggraph. Dada la corta extensión de la Declaración no es posible obtener conclusiones. En la figura 2.6 se pueden ver los gráficos de redes de bigramas, tanto sin palabras vacías como con ellas. library(igraph) library(ggraph) set.seed(1) graf_bigramas_l &lt;- bigramas_limpios |&gt; separate(bigram, c(&quot;first&quot;, &quot;second&quot;), sep = &quot; &quot;) |&gt; dplyr::filter(n &gt; 1) |&gt; graph_from_data_frame() g1 &lt;- ggraph(graf_bigramas_l, layout = &quot;fr&quot;) + geom_edge_link() + geom_node_point(size = 0) + geom_node_text(aes(label = name)) graf_bigramas &lt;- bigramas |&gt; separate(bigram, c(&quot;first&quot;, &quot;second&quot;), sep = &quot; &quot;) |&gt; dplyr::filter(n &gt; 2) |&gt; graph_from_data_frame() g2 &lt;- ggraph(graf_bigramas, layout = &quot;fr&quot;) + geom_edge_link() + geom_node_point(size = 0) + geom_node_text(aes(label = name)) library(patchwork) g1 + g2 Figura 2.6: Redes de bigramas sin palabras vacías y con ellas Resumen En este capítulo se ha introducido al lector en la minería de textos, en particular: Se han presentado los conceptos y tareas fundamentales de este ámbito, así como sus campos de aplicación principales. Se ha puesto de relieve la importancia de la preparación de los datos y su segmentación (a distintos niveles) para obtener buenos resultados, acordes con el objetivo de la investigación. Se ha mostrado el uso de R para el análisis de textos y de sentimientos. Se ha presentado un caso aplicado para ilustrar las técnicas de minería de textos. Se han mencionado otros análisis plausibles de minería de textos como la estilometría o el modelado de temas (véase el capítulo 58). Universidad de Castilla-La Mancha, Universidad de Castilla-La Mancha y Università di Roma Tor Vergata↩︎ Frecuencias relativas si se comparan distintos textos. También será útil para ello obtener la tasa de riqueza léxica (TIR).↩︎ Proyecto desarrollado por Michael Hart en 1971 para crear una biblioteca de libros electrónicos gratuitos, y accesibles en internet, a partir de libros en soporte físico, generalmente de dominio público. Cuenta con más de 50 000 libros.↩︎ Se deja al lector el análisis de la Declaración con más emociones, en castellano, etc.↩︎ "],["grafos.html", "Capítulo 3 Análisis de grafos y redes sociales 3.1 Introducción 3.2 Teoría de grafos 3.3 Elementos de un grafo 3.4 El paquete igraph 3.5 Análisis de influencia en un grafo aplicado a redes sociales 3.6 Otras utilidades de grafos", " Capítulo 3 Análisis de grafos y redes sociales José Javier Galán 3.1 Introducción El origen de la teoría de grafos se debe al problema de los Siete puentes de Königsberg (Euler1736?), es considerado el primer artículo sobre teoría de grafos. El problema se centra en la ciudad Königsberg en Prusia, ahora Kaliningrado (Rusia), donde existen varios puentes y el problema plantea trazar una ruta que cruce todos los puentes una única vez (ver Fig.3.1). Euler mediante el uso de grafos demostró que no era posible. Figura 3.1: Siete puentes de Königsberg, Euler. 3.2 Teoría de grafos Un grafo es un conjunto de nodos (vértices) que pueden estar unidos por aristas (enlaces). Si se piensa en cada nodo como una persona y en cada arista como la relación que los une, entonces se puede representar mediante grafos una red social (ver Fig.3.2). # Cargar el paquete de igraph # if(!require(&#39;igraph&#39;)) install.packages(&#39;igraph&#39;) library(igraph) datos_facebook &lt;- read.csv(&quot;data/DatosFacebook.csv&quot;, header = FALSE, sep = &quot; &quot;, col.names = c(&quot;Origen&quot;, &quot;Destino&quot;)) grafo_facebook &lt;- graph.data.frame(datos_facebook, directed = T) plot(grafo_facebook, vertex.label = NA) Figura 3.2: Estructura de una red social representada como grafo. Una red social es una estructura de red invisible que une mediante relaciones a distintos actores a través de sus intereses o valores comunes, estableciendo una relación personal entre individuos o grupos de individuos conectados. 3.3 Elementos de un grafo El análisis de redes sociales mediante le teoría de grafos requiere conocer previamente una serie de conceptos básicos, (Perez2021?). Las aristas son la relaciones que unen los nodos. Son dirigidas (ver Fig.3.3) si tienen un sentido definido y no dirigidas (ver Fig.3.4) en caso contrario. datos_grafo &lt;- read.csv(&quot;data/DatosGrafos.csv&quot;, sep = &quot;;&quot;) grafo &lt;- graph.data.frame(datos_grafo, directed = T) plot(grafo, edge.label = paste(E(grafo)$weight)) Figura 3.3: Grafo dirigido. grafo &lt;- graph.data.frame(datos_grafo, directed = F) plot(grafo, edge.label = paste(E(grafo)$weight)) Figura 3.4: Grafo no dirigido. En una red social como LinkedIn las aristas representan la relación que une las personas. Las personas forman parte de un grupo con intereses comunes, formando un grado no dirigido. Pero también pueden seguir a alguien sin necesariamente ser seguido, en ese caso podemos representar un grafo dirigido. Los vértices, vertex , representan nodos que serán unidos mediante aristas. En una red social cada vértice representa una de las personas de dicha red, unidas en ocasiones por intereses comunes a otras. Un grafo es un conjunto de vértices y de aristas que podemos representar mediante \\(G = (V, E)\\). Donde \\(V\\) es el conjunto de nodos o vértices del grafo y \\(E\\) es un conjunto de pares de vértices llamado arista, arco o edge. Una matriz de adyacencia representa una matriz cuadrada de \\(n \\times n\\), siendo n el número de vértices del grafo. Se inicia con valor 0 donde a cada valor \\(a_{ij}\\) se suma 1 por cada arista que relaciona los vértices \\(i\\) y \\(j\\). Si no existe relación el elemento \\(a_{ij}\\) vale 0. matriz_adyacencia &lt;- get.adjacency(grafo, sparse = FALSE) matriz_adyacencia #&gt; 1 2 3 4 5 #&gt; 1 0 1 1 1 1 #&gt; 2 1 0 1 2 0 #&gt; 3 1 1 0 2 1 #&gt; 4 1 2 2 0 0 #&gt; 5 1 0 1 0 0 El grado de un nodo son el numero de aristas que tiene dicho nodo \\(x\\), y se representa mediante grado(x), g(x) o gr(x), siendo un vértice de grado 0 un vértice aislado. En un grafo \\(G\\) existirá un grado máximo \\(\\Delta (G)\\) y un grado mínimo \\(\\delta (G)\\), mientras que el grado del grafo, g(G), es la suma de todos los grados de sus vértices. En una red social representa el número de relaciones que existen, en una red social como Facebook significaría conocer cuántos amigos tienes. degree(grafo) #&gt; 1 2 3 4 5 #&gt; 4 4 5 5 2 Un camino une dos vértices constituyendo un conjunto de aristas no recursivas. Entre dos vértices puede no existir un camino, puede haber varios y se puede incluir el mismo vértice en el camino mas de una vez. Evidentemente siempre habrá un camino mas corto , aquel que menos aristas ha recorrido. Si entre todos los pares de vértices existe un camino estamos hablando de un grafo conexo. # Camino más corto entre el nodo 2 y el 5 caminos &lt;- get.shortest.paths(grafo, from = &quot;2&quot;, to = &quot;5&quot;) V(grafo)[caminos$vpath[[1]]] #&gt; + 4/5 vertices, named, from a9aa3df: #&gt; [1] 2 4 3 5 3.4 El paquete igraph Existen diversos paquetes, pero el más utilizado y popularizado por sencillez y eficacia es igraph (igraph2022?). Se trata de un paquete que permite crear y manipular grafos para poder analizar redes en R de forma muy sencilla (ver Fig.3.5). nodes &lt;- cbind(&quot;nodos&quot; = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;)) edges &lt;- cbind(&quot;from&quot; = c(&quot;A&quot;, &quot;C&quot;, &quot;B&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;), &quot;to&quot; = c(&quot;B&quot;, &quot;D&quot;, &quot;C&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;)) red &lt;- graph_from_data_frame(edges, directed = F, vertices = nodes) plot(red) Figura 3.5: Ejemplo de grafo con Igraph. Para crear un grafo a partir de un dataframe se han usado los argumentos: graph_from_data_frame(edges, directed = TRUE, vertices = NULL) donde: edges Es un data frame donde las dos primeras columnas representan una lista de aristas. directed es un Valor lógico que indica si es un grafo dirigido o no dirigido. vertices es un data frame con los valores de los vertices o NULL. library(igraph) nodes &lt;- cbind(&quot;actores&quot; = c(&quot;Jim Carrey&quot;, &quot;Arnold Swarzenneger&quot;, &quot;George Cloney&quot;, &quot;Cameron Diaz&quot;), &quot;descripcion&quot; = c(&quot;actor&quot;, &quot;actor&quot;, &quot;actor&quot;, &quot;actriz&quot;)) edges &lt;- cbind( &quot;from&quot; = c(&quot;Jim Carrey&quot;, &quot;Jim Carrey&quot;, &quot;George Cloney&quot;, &quot;Jim Carrey&quot;), &quot;to&quot; = c(&quot;Arnold Swarzenneger&quot;, &quot;George Cloney&quot;, &quot;Arnold Swarzenneger&quot;, &quot;Cameron Diaz&quot;), &quot;pelicula&quot; = c(&quot;Batman y Robin&quot;, &quot;Batman y Robin&quot;, &quot;Batman y Robin&quot;, &quot;La mascara&quot;) ) red &lt;- graph_from_data_frame(edges, directed = F, vertices = nodes) plot(red) Figura 3.6: Grafo representativo de la relacion de actores respecto a peliculas En la Fig. 3.6 se puede observar como el actor Jim Carrey tuvo relación con todos los actores de la red propuesta, mientras que la actriz Cameron Diaz solo participo con uno de ellos. 3.5 Análisis de influencia en un grafo aplicado a redes sociales Existen paquetes para obtener informacion de distintas redes sociales, por ejemplo, se puede utilizar el paquete Rfacebook para conectarse a Facebook y obtener información de lo contactos existentes, para ello sera necesario activar la API desde https://developers.facebook.com. La información necesaria podemos encontrarla en su página web https://developers.facebook.com/docs. Para ilustrar un ejemplo didactico, sin ncesidad de que el lector necesite conocimientos de desarrollo para descargar datos, se ha generado un fichero excel que simula la relacion entre amigos de una red social como podria ser perfertamente Facebook. Se incorporan los datos y se muestra su cabecera. Los datos han sido recogidos en un fichero csv, constando de dos columnas separadas por espacio. La primera columna contiene el id de la primera persona, la cual establece relación con la persona indicada también por su id en la segunda columna. datos_faceboook &lt;- read.csv(&quot;data/DatosFacebook.csv&quot;, header = FALSE, sep = &quot; &quot;, col.names = c(&quot;Origen&quot;, &quot;Destino&quot;)) head(datos_faceboook) #&gt; Origen Destino #&gt; 1 3434 3409 #&gt; 2 3493 3361 #&gt; 3 3329 3324 #&gt; 4 3496 3384 #&gt; 5 3370 3415 #&gt; 6 3383 3490 Un grafo no tiene una centralidad real porque no tiene coordenadas, pero tenemos distintas medidas de centralidad que en un grafo social nos ayudara a identificar el poder social de cada individuo, es decir, su influencia. Se puede decir que la centralidad de un grafo coincide con el grado de vértices a analizar, si un vértice representa una persona de una red social de amigos su centralidad representa el número de amigos que tiene. En los grafos dirigidos cuantas más aristas dirigidas a un nodo existan significará que más personas intentar interactuar con una persona concreta y esta más prestigio tendrá. Pero si la interacción hacia esta persona no es directa y se realiza a través de un camino más largo pasando por más nodos quiere decir que su influencia es elevada. Se convierten los datos a formato igraph y mostramos el grafo (ver Fig.3.7). Se compone el data frame de tipo igraph necesario indicando que es un grafo dirigido, finalmente mediante ‘plot’ se muestra el grado al mismo tiempo que se establecen sus propiedades. grafo_faceboook &lt;- graph.data.frame(datos_faceboook, directed = T) plot.igraph(grafo_faceboook, layout = layout.fruchterman.reingold, vertex.label = NA, vertex.label.cex = 1, vertex.size = 3, edge.curved = TRUE, edge.arrow.size = 0 ) Figura 3.7: Aplicación de Igraph a Redes Sociales. Relaciones de la red social. Se puede observar que el numero de aristas que contiene cada vertice son el numero de personas con las que se relaciona cada individuo, por lo tanto son las relaciones que contiene cada persona. table(degree(grafo_faceboook)) #&gt; #&gt; 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #&gt; 1 2 4 4 7 4 8 14 19 15 14 19 22 15 11 13 9 5 3 3 4 1 4 Se personalizan los datos (ver Fig.3.8). Son muchos nodos y para focalizar este caso de estudio se centra el codigo en los nodos que sean de grado igual o superior a 26 y se les asigna nombre. bad_network &lt;- V(grafo_faceboook)[degree(grafo_faceboook) &lt;= 26] grafo_faceboook &lt;- delete.vertices(grafo_faceboook, bad_network) V(grafo_faceboook)$name &lt;- c(&quot;Gema&quot;, &quot;Patricia&quot;, &quot;Ramon&quot;, &quot;Jose&quot;, &quot;Maria&quot;, &quot;Angeles&quot;, &quot;Gabriel&quot;, &quot;Javier&quot;, &quot;Victor&quot;, &quot;Leonor&quot;, &quot;Ana&quot;, &quot;Isabel&quot;, &quot;Cristobal&quot;, &quot;Rosa&quot;, &quot;Aurola&quot;) plot(grafo_faceboook) Figura 3.8: Aplicación de Igraph a Redes Sociales, grafo dendrograma Se pueden observar las relaciones entre las distintas personas que lo componen, como siguen a otras y son seguidas. Tambien se pueden observar los casos extremos como el de Gema a quien nadie sigue, o el de Leonor que no sigue a nadie pero varios podrian llegar hasta ella. Detección de comunidades En el análisis de una red social es importante detectar las distintas comunidades que la componen. Se puede observar dos comunidades detectadas con Walktrap en la (ver Fig.3.9) Walktrap_grafo &lt;- walktrap.community(grafo_faceboook, steps = 5, modularity = TRUE) plot(Walktrap_grafo, grafo_faceboook, edge.arrow.size = 0.25, vertex.label = (grafo_faceboook)$name) Figura 3.9: Walktrap Community on Full dataset dend_g_network &lt;- as.dendrogram(Walktrap_grafo) plot(dend_g_network) Figura 3.10: Grafo dendrograma de Walktrap Walktrap detecta las comunidades basandose en el concepto de que las caminatas aleatorias cortas permanecen en la misma comunidad. Debe indicarse èl largo de la caminata aleatoria, se recomienda usar 5 caminatas. Se puede ver su dendrograma en la Fig.3.10. El dendograma es un tipo de diagrama en forma de arbol, en el cual se subdivide los datos en subcategorias sucesivamente hasta llegar a las vistas deseadas. La técnica de Centralidad de intermediación (betweenness) se basa en el número de caminos mínimos en los que un nodo esta involucrado. Por lo tanto, en un red social una persona tendrá mayor influencia cuanto mayor betweenness tenga porque comunicara mucha información a través de los nodos de la red. Si puede llegar a un grupo grande, aunque sea a través de un nodo puede alcanzar un gran nivel de viralización (ver Fig.3.11 y Fig.3.12). edge_g_network &lt;- edge.betweenness.community(grafo_faceboook) plot(as.dendrogram(edge_g_network), main = &quot;Grafo dendrograma de Betweenness&quot;) Figura 3.11: Grafo dendrograma de Betweenness plot(edge_g_network, grafo_faceboook, edge.arrow.size = 0.25, main = &quot;Edge Betweenness Community&quot;) Figura 3.12: Grafo dendrograma de Betweenness En este ejemplo vemos como se han detectado mediante este algoritmo siete comunidades. Centralidad de vector propio (eigenvector) se basa en la centralidad de los nodos con los que se relaciona, la centralidad de un nodo es proporcional a la suma de las centralidades de sus nodos vecinos. cl_g_network &lt;- leading.eigenvector.community(grafo_faceboook) plot(cl_g_network, grafo_faceboook, edge.arrow.size = 0.25, main = &quot;Leading Eigenvector Community&quot;) Figura 3.13: Aplicación de Igraph a Redes Sociales Se representa mediante \\(c_{i} = \\lambda \\sum_{j} a_{ij}c_{j}\\), donde \\(\\lambda\\) es la constante de proporcionalidad y \\(a_{ij}\\) es el valor de la fila \\(i\\) y la columna \\(j\\) de la matriz de adyacencia A de la red social. En este ejemplo vemos como se han detectado mediante este algoritmo tres comunidades. Segun este metodo, no es tan importante que tengas muchos amigos, lo importante es que tus amigos sean muy influyentes (ver Fig.3.13). 3.6 Otras utilidades de grafos En la Fig 3.14 se muestran los tipos de accidente ocurridos en Madrid durante el año 2020 que tiene como relación la zona en la que sucedió, en este caso en “Barajas”. Ademas se añade a las aristas el numero de accidentes sucedido por cada tipo. library(igraph) library(CDR) datos_accidentalidad_2020 &lt;- accidentes2020_data conjunto &lt;- datos_accidentalidad_2020[, c(&quot;distrito&quot;, &quot;tipo_accidente&quot;)] distrito &lt;- datos_accidentalidad_2020[, c(&quot;distrito&quot;)] tipo_accidente &lt;- datos_accidentalidad_2020[, c(&quot;tipo_accidente&quot;)] grupos &lt;- group_by(conjunto, distrito, tipo_accidente) contados &lt;- dplyr::summarise(grupos, num = n() ) zona_Barajas &lt;- dplyr::filter(contados, distrito == &quot;BARAJAS&quot;) g3 &lt;- graph_from_data_frame(zona_Barajas) plot.igraph(g3, edge.label = zona_Barajas$num) Figura 3.14: Accidentes en Barajas Ahora mostramos los principales puntos conflictivos de Barajas unidos para ver si existen caminos que pasaern por otros accidentes y evitarlos, Fig ??. library(proj4) library(igraph) library(sp) library(leaflet) library(CDR) datos &lt;- accidentes2020_data datos &lt;- datos[, c(&quot;localizacion&quot;, &quot;distrito&quot;, &quot;coordenada_x_utm&quot;, &quot;coordenada_y_utm&quot;)] datos &lt;- dplyr::filter(datos, distrito == &quot;BARAJAS&quot;) datos &lt;- datos[, c(&quot;localizacion&quot;, &quot;coordenada_x_utm&quot;, &quot;coordenada_y_utm&quot;)] datos &lt;- distinct(datos) meta &lt;- data.frame(name = datos[[&quot;localizacion&quot;]], lon = datos[[&quot;coordenada_x_utm&quot;]], lat = datos[[&quot;coordenada_y_utm&quot;]]) proj4string &lt;- &quot;+proj=utm +zone=30 +ellps=WGS84 +datum=WGS84 +units=m +no_defs &quot; xy &lt;- data.frame(meta$lon, meta$lat) # data frame con utm pj &lt;- project(xy, proj4string, inverse = TRUE) meta &lt;- data.frame(name = datos[[&quot;localizacion&quot;]], lat = pj$y, lon = pj$x) df &lt;- data.frame(from = datos[[&quot;localizacion&quot;]], to = datos[[&quot;localizacion&quot;]]) meta &lt;- head(distinct(meta)) df &lt;- head(distinct(df)) mat &lt;- matrix(ncol = 0, nrow = 0) df2 &lt;- data.frame(mat) for (f in df[, 1]) { for (t in df[, 2]) { fila_nueva &lt;- c(f, t) df2 &lt;- rbind(df2, fila_nueva) } } df &lt;- df2 g &lt;- graph.data.frame(df, directed = FALSE, vertices = meta) gg &lt;- get.data.frame(g, &quot;both&quot;) vert &lt;- gg$vertices coordinates(vert) &lt;- ~ lon + lat edges &lt;- gg$edges edges &lt;- lapply(1:nrow(edges), function(i) { as( rbind( vert[vert$name == edges[i, &quot;from&quot;], ], vert[vert$name == edges[i, &quot;to&quot;], ] ), &quot;SpatialLines&quot; ) }) for (i in seq_along(edges)) { edges[[i]] &lt;- spChFIDs(edges[[i]], as.character(i)) } edges &lt;- do.call(rbind, edges) leaflet(vert) %&gt;% addTiles() %&gt;% addMarkers(data = vert) %&gt;% addPolylines(data = edges) RESUMEN El uso de los grafos para el estudio de las redes sociales resultad de gran utilidad y aunque existen distintos paquetes uno de los mas potentes sigue siendo igraph. Se ha aprendido que tres componentes son necesarios: obtener datos de una red social, conocer las propiedades de los grafos y aplicarlos mediante el paquete igraph. Gracias a los grafos podemos representar las relaciones que unen a los individuos en una red social, asi como las comunidades que forman e incluso la popularidad de cada uno, sus seguidores y a quien siguen. "],["aplicaciones-webs-interactivas-con-shiny.html", "Capítulo 4 Aplicaciones webs interactivas con Shiny 4.1 Introducción 4.2 Partes mínimas de una aplicación Shiny y disposición básica 4.3 Diseño de una aplicación Shiny 4.4 Elementos para entrada de datos 4.5 Elementos para visualización (salida) 4.6 Reactividad 4.7 Publicación de la aplicación en la web 4.8 Extensiones de Shiny", " Capítulo 4 Aplicaciones webs interactivas con Shiny Aurora González Vidal 4.1 Introducción Shiny es un paquete de R que permite crear aplicaciones web interactivas que cuentan con todos los elementos de R. Shiny se ha convertido en un referente ya que, para aquellos que tienen conocimiento de R, es muy sencillo crear una aplicación en cuestión de horas (changshiny?). Para crear una aplicación mínima, no se necesitan conocimientos de HTML (HyperText Markup Language), CSS (Cascading Style Sheets) o JavaScript y sus dependencias. Además, no es necesario pensar en elementos técnicos para hacerla accesible en la web como, por ejemplo, el puerto, ya que Shiny se encarga de esos detalles si no se cambian las opciones por defecto. Éstas son algunas de las razones principales por las cuales Shiny se ha vuelto tan popular a lo largo de los años, ya que con poco esfuerzo se pueden crear pruebas de concepto de un producto, mostrar algoritmos o presentar resultados de investigación de forma elegante a través de interfaces de usuario accesibles, reproducibles y amigables (fay2021engineering?). El primer paso para disfrutar de Shiny consiste en instalar el paquete que está disponible en CRAN: install.packages(&quot;shiny&quot;) Para asegurarse de que la versión instalada es igual o superior a la 1.5.0. use packageVersion(\"shiny\"). A continuación se puede cargar el paquete y ver algunos ejemplos que se incluyen directamente en el mismo utilizando distintas opciones para el argumento example. library(&quot;shiny&quot;) runExample(example = &quot;01_hello&quot;) # otras: 02_text, 03_reactivity, 04_mpg, 05_sliders, 06_tabsets, 07_widgets, 08_html, 09_upload, 10_download, 11_timer. 4.2 Partes mínimas de una aplicación Shiny y disposición básica Las aplicaciones Shiny tienen dos componentes: Una interfaz de usuario ui, que es un script y Un server que es un script de servidor o secuencia de comandos de servidor. Estas partes pueden encontrarse en el mismo script o estar separadas en dos scripts con nombres fijos: ui.R y server.R. En este caso se ha elegeido la segunda opción para ilustrar los ejemplos con mayor claridad. Una aplicacion Shiny es un directorio que contiene estos scripts y otros ficheros adicionales (conjuntos de datos, fichero donde se definen funciones no dinámicas, etc) El código mínimo para crear una aplicación con un título, panel lateral y panel principal es el que sigue: ui.R shinyUI(fluidPage( headerPanel(&quot;TITULO&quot;), sidebarPanel(), mainPanel() )) server.R shinyServer(function(input, output) {}) Para lanzar la aplicación existen dos maneras, mediante el comando runApp que tiene como argumento la ruta del directorio que almacena los ficheros que componen la aplicación. library(shiny) runApp(&quot;ruta al directorio&quot;) O directamente desde Rstudio mediante el botón RunApp que aparece en cualquiera de los dos scripts ui.R, server.R reemplazando al Run habitual. En este capítulo, además de ver los distintos componentes de Shiny, construiremos una aplicación para la visualización de algunos gráficos presentados en el capítulo “Perfilado estadístico de parados” {#paro-clm}. Además del ui.R y el sever.R, puede ser muy útil tener un fichero source.R en la misma carpeta para cargar los paquetes y datos estáticos necesarios para el funcionamiento de la aplicación. 4.3 Diseño de una aplicación Shiny Shiny incluye una serie de opciones para el diseño o la disposición de los distintos componentes de una aplicación. En este apartado veremos dos muy sencillos: sidebarLayout(): para colocar un sidebarPanel() de entradas junto a un mainPanel() de contenido de salida. tabsetPanel() y navlistPanel() para la segmentación de diseños Hasta ahora se ha utilizado el primero sin introducirlo específicamente para mostrar distintos ejemplos por ser el más sencillo. 4.3.1 Diseño de las páginas: fluidPage() Un diseño de página fluido fluidPage() consiste en filas que a su vez incluyen columnas. Las filas existen con el propósito de asegurar que sus elementos aparezcan en la misma línea (si el navegador tiene el ancho adecuado). Las columnas existen con el propósito de definir cuánto espacio horizontal dentro de una cuadrícula de 12 unidades de ancho deben ocupar sus elementos. Las páginas fluidas escalan sus componentes en tiempo real para llenar todo el ancho disponible del navegador. Una fluidPage() presenta 2 argumentos: headerPanel() con el título de la aplicación, y sidebarLayout().sidebarLayout() es un punto de partida útil para la mayoría de las aplicaciones. Este a su vez tiene 2 argumentos más: sidebarPanel(), que es una barra lateral para las entradas y mainPanel(), una gran área principal para la salida. shinyUI(fluidPage( headerPanel(&quot;Evolución del paro&quot;), sidebarLayout( sidebarPanel( radioButtons( &quot;vble&quot;, &quot;Variable&quot;, c( &quot;sexo&quot; = &quot;sexo&quot;, &quot;tramo_edad&quot; = &quot;tramo_edad&quot;, &quot;tiempo_busqueda_empleo_agregado&quot; = &quot;tiempo_busqueda_empleo_agregado&quot;, &quot;sector&quot; = &quot;sector&quot;, &quot;tiempo_busqueda_empleo&quot; = &quot;tiempo_busqueda_empleo&quot; ), &quot;sexo&quot; ) ), mainPanel( plotOutput(&quot;gra1&quot;) ) ) )) Figura 4.1: fluidPage, sidebarLayout, sidebarPanel por defecto La barra lateral puede posicionarse a la izquierda (por defecto) o a la derecha del área principal. Por ejemplo, para posicionar la barra lateral a la derecha se debe utilizar position = 'right' y éste es el resultado. shinyUI(fluidPage( headerPanel(&quot;Evolución del paro&quot;), sidebarLayout(position = &quot;right&quot;, ...) )) Figura 4.2: fluidPage, sidebarLayout a la derecha Las funciones radioButtons y plotOutput se introducirán en detalle en las respectivas secciones de este capítulo. 4.3.2 Segmentación de diseños: tabsetPanel() y navlistPanel() Para subdividir el panel principal en varias secciones discretas, se puede usar tabsetPanel() y `tabPanel()`` como sigue: mainPanel( tabsetPanel( tabPanel(&quot;Plot1&quot;, plotOutput(&quot;gra1&quot;)), tabPanel(&quot;Plot2&quot;, plotOutput(&quot;gra2&quot;)), tabPanel(&quot;Plot3&quot;, plotOutput(&quot;gra3&quot;)) ) ) navlistPanel() es una alternativa a tabsetPanel() cuando existan muchas separaciones. Un navlist presenta los distintos componentes como una lista de la barra lateral en lugar de utilizar pestañas y no se hace en el mainPanel. ui &lt;- fluidPage( titlePanel(&quot;Application Title&quot;), navlistPanel( &quot;Header A&quot;, tabPanel(&quot;Component 1&quot;), tabPanel(&quot;Component 2&quot;), &quot;Header B&quot;, tabPanel(&quot;Component 3&quot;) ) ) 4.4 Elementos para entrada de datos Para que el usuario de la aplicación Shiny introduzca datos manualmente, hay diversos elementos que se enumeran a continuación: Control deslizante: Un control deslizante permite que el usuario seleccione entre un intervalo de valores moviendo un control de posición por una pista. En Shiny se crean con la función sliderInput que tiene, entre otros, los siguientes argumentos autoexplicativos: sliderInput(inputId, label, min, max, value, step = NULL, animate = FALSE) Sus características incluyen: - La posibilidad de introducir un único valor y rangos - Formatos customizaods (por ejemplo para entradas relativas al dinero) - Pueden ser animados y recorrer los valores de forma automática (argumento `animate`) Algunos ejemplos son: sliderInput(&quot;enteros&quot;, &quot;Enteros:&quot;, min = 0, max = 1000, value = 500) sliderInput(&quot;decimales&quot;, &quot;Decimales:&quot;, min = 0, max = 1, value = 0.5, step = 0.1) sliderInput(&quot;rango&quot;, &quot;Rango:&quot;, min = 1, max = 1000, value = c(200, 500)) sliderInput(&quot;animacion&quot;, &quot;Animacion:&quot;, 10, 200, 10, step = 10, animate = animationOptions(loop = T)) Botón circular: Un botón circular es un tipo de selector que da una lista de opciones entre las cuales solo se puede seleccionar una. En Shiny se crean con la función radioButtons que tiene, entre otros, los siguientes argumentos autoexplicativos: radioButtons(inputId, label, choices, selected = NULL) Un ejemplo donde la variable “sexo” está elegida por defecto se puede ver en el primer trozo de código de la subsección 2.1.1.1 sidebarLayout(). Selección múltiple: Un cuadro de selección múltiple es un tipo de selector que da una lista de opciones entre las cuales se pueden seleccionar varias. En Shiny se crean con la función selectInput que tiene, entre otros, los siguientes argumentos autoexplicativos: selectInput(inputId, label, choices, multiple = FALSE) selectInput(&quot;año&quot;, &quot;Año:&quot;, c( &quot;año1&quot; = &quot;2007&quot;, &quot;año2&quot; = &quot;2013&quot;, &quot;año3&quot; = &quot;2019&quot;, &quot;año4&quot; = &quot;2022&quot; ), multiple = TRUE ) checkboxGroupInput Muy similar al anterior, este componente crea un grupo de casillas que se pueden utilizars para alternar varias opciones de forma independiente. checkboxGroupInput(inputId, label, choices, multiple = FALSE) checkboxGroupInput( &quot;variable&quot;, &quot;Variables to show:&quot;, c( &quot;año1&quot; = &quot;2007&quot;, &quot;año2&quot; = &quot;2013&quot;, &quot;año3&quot; = &quot;2019&quot;, &quot;año4&quot; = &quot;2022&quot; ) ) Entrada numérica numericInput(&quot;obs&quot;, &quot;Numero de observaciones:&quot;, 10) Entrada de texto helpText(&quot;aclaraciones&quot;) Otras opciones de entrada que se invita al lector a analizar se relacionan con las fechas: dateInput(), dateRangeInput() y con un áreas de texto: textAreaInput(). Figura 4.3: Distintos elementos para la entrada de datos 4.4.1 Lectura de ficheros de datos También es posible introducir información a través de la lectura de ficheros de datos con la función fileInput(). Podemos combinar valores por defecto de la función utilizada para la lectura de datos con algunos de los elementos anteriores para definir las características del dataset (seaprador, decimal, cabecera). En el siguiente ejemplo se utiliza la función read.csv y como separador siempre punto y coma, mientras que los demás elementos se dan a elegir. shinyUI(fluidPage( headerPanel(&quot;Lectura de datos&quot;), sidebarPanel( h4(&quot;Cargar fichero CSV&quot;), fileInput(&quot;file1&quot;, &quot;&quot;, accept = c(&quot;text/csv&quot;, &quot;text/comma-separated-values,text/plain&quot;, &quot;.csv&quot;) ), checkboxInput(&quot;header&quot;, &quot;Header (el csv tiene nombres de variables)&quot;, TRUE), radioButtons( &quot;dec&quot;, &quot;Separador de decimales:&quot;, c( &quot;Punto&quot; = &quot;,&quot;, &quot;Coma&quot; = &quot;.&quot; ) ) ), mainPanel( tabsetPanel( tabPanel( &quot;CSV&quot;, h4(&quot;Vista del fichero CSV&quot;), tableOutput(&quot;contents&quot;) ) ) ) )) Figura 4.4: Lectura de datos 4.5 Elementos para visualización (salida) Tras la introducción de ciertos parámetros en el ui.R, éstos se pueden utilizar en el script server.R mediante la expresión input. El código de R que construye el objeto basado en esos datos se desarrolla en el servidor y para generar esos objetos se utilizan las funciones renderX donde X es el tipo de objeto a devolver. Por úlitmo, este objeto se referencia nuevamente en el ui.R en el lugar que se desea mostrar (panel) a través de la expresión XOutput. El hecho de colocar una función en ui le dice a Shiny dónde mostrar su objeto. A continuación, hay que decirle a Shiny cómo construir el objeto. Esto se hace proporcionando el código R que construye el objeto en la función del servidor. En concreto, algunas posibildiades son se pueden ver en la Tabla @ref(tab:my_table). Server Ui Crea renderImage imageOutput Imagen renderPlot plotOutput Grafico renderTable tableOutput Tabla renderText textOutput Texto htmlOutput HTML verbatimTextOutput Texto verbatim Gráficos: Para generar la aplicación de la Figura 4.1, se utiliza renderPlot en el server.R como sigue: source(&quot;source.R&quot;) shinyServer(function(input, output) { output$gra1 &lt;- renderPlot({ graf_evol(input$vble) }) }) Se ha llamado gra1 a la variable que es el gráfico y que se crea con renderPlot(). En el interior se utiliza una función denominada graf_evol que es compleja y se crea en elsource.R que se carga al principio. Lo que interesa de esta función es que tiene como único argumento el nombre de la variable por el cual crear el gráfico, que puede ser una de las diversas opciones que se dan a través del radioButton vble creado anteriormente y que, como vemos, se utiliza input$vble para invocar a la selección realizada en la interfaz de usuario. Tablas: Para mostrar la tabla de la Figura 4.4 se utiliza renderTable() en el server y tableOutput() en el ui. shinyServer(function(input, output) { output$contents &lt;- renderTable({ inFile &lt;- input$file1 if (is.null(inFile)) { return(NULL) } read.csv(inFile$datapath, header = input$header, dec = input$dec, sep = &quot;;&quot;) }) }) 4.6 Reactividad El modelo de reactividad que utiliza Shiny es el siguiente: hay una fuente reactiva, un conductor reactivo y un punto final de la reactividad. La fuente reactiva suele ser lo que el usuario introduce y el punto de parada lo que se muestra por pantalla. A lo que el usuario introduce se accede con el objeto input y a lo que se muestra por pantalla con el objeto output. Un ejemplo que ya se ha usado es el siguiente: output$gra1 &lt;- renderPlot({ graf_evol(input$vble) }) El objeto output$gra1 es un punto final de la reactividad, y usa la fuente reactiva input$vble. Cuando input$vbles cambia, a output$gra1 se le notifica que necesita ejecutarse de nuevo. 4.6.1 Conductores reactivos y control de la reactividad También es posible crear componentes reactivos que conecten los inputs y los outputs. En el siguiente ejemplo se ha creado un objeto reactivo datos que genera datos que siguen una distribución que el usuario selecciona a través del radioButton dist y cuya muestra tiene tantos elementos como el usuario haya especificado en el numericInput obs. Shiny, además, permite controlar la reactividad a través de los actionButtons. Se pueden modificar las entradas sin obtener una respuesta hasta que se apriete dicho botón. Se ha creado un panel nuevo dentro del mainPanel y éste contendrá, además del plot previo, un actionButton con la etiqueta Presiona. shinyUI(fluidPage( headerPanel(&quot;Controlar reactividad&quot;), sidebarPanel( radioButtons(&quot;dist&quot;, &quot;Tipo de distribucion:&quot;, c( &quot;Normal&quot; = &quot;norm&quot;, &quot;Uniforme&quot; = &quot;unif&quot;, &quot;Log-normal&quot; = &quot;lnorm&quot;, &quot;Exponencial&quot; = &quot;exp&quot; ), selected = &quot;Exponencial&quot; ), numericInput(&quot;obs&quot;, &quot;Numero de observaciones:&quot;, 10), ), mainPanel( tabPanel( &quot;Histograma distribucion RadioButton&quot;, &quot;Plot&quot;, plotOutput(&quot;plot&quot;), actionButton(&quot;botonReac&quot;, &quot;Presiona&quot;) ) ) )) A continuación, se hace referencia a ese botón para cada una de las expresiones reactivas que se aislarán con la función isolate: shinyServer(function(input, output) { datos &lt;- reactive({ if (input$botonReac == 0) { return(dist(rexp(input$obs))) } isolate({ dist &lt;- switch(input$dist, norm = rnorm, unif = runif, lnorm = rlnorm, exp = rexp, rnorm ) dist(input$obs) }) }) output$plot &lt;- renderPlot({ if (input$botonReac == 0) { return(NULL) } isolate({ hist(datos(), main = paste(&quot;r&quot;, input$dist, &quot;(&quot;, input$obs, &quot;)&quot;, sep = &quot;&quot;) ) }) }) }) Figura 4.5: Barras 4.7 Publicación de la aplicación en la web Después del desarrollo de una aplicación Shiny, suele ser interesante publicarla para su explotación científica o empresarial. Rstudio ofrece diversas soluciones que se analizarán, con distintos niveles de complejidad y libertad para poder publicar la aplicación web : (i) shinyapps.io, (ii) Shiny Server y (iii) RStudio Connect. Se dará una introducción muy breve a cada uno de ellos y enlaces para que el lectro pueda indagar en profundidad Shinyapps.io Rstudio ofrece un servicio de hosting denominado Shinyapps.io que permite subir la aplicación directamente desde la sesión de R a un servidor que se mantiene por Rstudio. Hay un control casi completo sobre la aplicación, incluyendo la administración del servidor. Lo único que se necesita es: - Un entorno de desarrollo de R, como RStudio IDE - La última versión del paquete rsconnect En la web shinyapps.io en el apartado “Dashboard” se realiza el registro. Shinyapps.io genera de forma automática un token que el paquete rsconnect utiliza para acceder a la cuenta. rsconnect::setAccountInfo(name = &quot;&lt;ACCOUNT&gt;&quot;, token = &quot;&lt;TOKEN&gt;&quot;, secret = &quot;&lt;SECRET&gt;&quot;) Para desplegar la aplicación utiliza deployApp() como sigue library(rsconnect) deployApp() Para más información sobre este método, consulta la página https://shiny.rstudio.com/articles/shinyapps.html Shiny Server Shiny server construye un servidor web diseñado para hospedar aplicaciones Shiny. Es gratuito, de código abierto y está disponible en GitHub. Para usar el Shiny Server, es necesario tener un servidor Linux que tenga soporte explícito para Ubuntu 12.04 or superior (64 bit) y CentOS/RHEL 5 (64 bit). Si no se está utilizando una distribución con soporte explícito, se puede aún así utilizar construyéndolo desde el paqeute fuente. En el mismo Shiny Server se pueden hospedar múltiples aplicaciones Shiny. Para ver instrucciones detalladas para su instalación y configuración, se recomienda la guía Shiny Server https://docs.rstudio.com/shiny-server.` RStudio Connect Cuando Shiny se utiliza en entornos con fines lucrativos, existen herramientas de servidor que se pueden comprar y que vienen equipadas con los programas habituales de un servidor de pago: Soporte SSL Herramientas de administrador Soporte prioritario Para ello, la plataforma de publicación RStudio Connect puede ser una solución. Esta herramienta permite compartir aplicacciones Shiny, informes RMarkdown, cuadros de mando, gráficos, Jupyter Notebooks y más. Con RStudio Connect se puede programar la ejecución de informes y políticas de seguridad flexibles. 4.8 Extensiones de Shiny Shiny es una herramienta totalmente expansible. Lo que se ha mostrado en este capítulo hasta ahora es un aperitivo en relación a todas las posibilidades que existen en el mundo de Shiny. Hay repositorios que recopilan información sobre paquetes que proveen de mejoras a las aplicaciones Shiny en su estilo y funcionalidad (awesome2?). En esta sección se mencionarán algunos de ellos pero, sobre todo, se econmienda al lector els visitar dichos repositorios para una mayor profundidad en este tema. shinydashboard, shinydashboardPlus y flexdashboards En temas de estilo, se destacan éstos tres paquetes. Los dos primeros presentan una serie de plantillas predefinidas para la creación de las aplicaciones Shiny, de manera que los colores combinan y los elementos visuales tienen cierta armonía. Por su parte, flexdashboards tiene como base un documento R Markdown y los distintos niveles del mismo definen los paneles de la aplicación a crear. shinyWidgets Este paquete ofrece widgets personalizados y diversos componentes para mejorar las aplicaciones. Se pueden reemplazar los checkboxes por switch buttons, añadir colores a los radioButtons y al grupo de casillas de verificación (checkboxGroupInput), etc. Cada widget tiene un método de actualización para cambiar el valor de una entrada del server. shinycssloaders Cuando una salida de Shiny (un gráfico, una tabla, etc.) se está calculando, permanece visible pero en gris. Si hay procesos algo más complejos, pueden tardar en mostrarse. Utilizando shinycssloaders, se puede añadir una rueda de carga (spinner) a las salidas en lugar de hacerlas grises. Envolviendo una salida Shiny en withSpinner(), el spinner aparecerá automáticamente mientras la salida se recalcula. Hay 8 tipos de animación incorporadas y personalizables en color y tamaño, pero también se pueden cargar otras animaciones. Visualizaciones interactivas Paquetes como heatmaply o leaflet se pueden combinar perfectamente con Shiny para crear mapas de calor y mapas geográficos interactivos y utilizarlos en las aplicaciones. RESUMEN Shiny es un paquete de R que permite crear aplicaciones web interactivas requiriendo únicamente conocimientos de R. En la primera parte de este capítulo se muestran los elementos básicos de una aplicación Shiny: user interface (ui.R) y servidor (server.R), se muestran posibles diseños en relación a los componentes que una aplicación puede tener: barra lateral, paneles discretos, paneles de navegación, etc. A continuación, se repasan los elementos de entrada de datos en una aplicación Shiny, incluyendo la carga de conjuntos de datos y también los elementos de salida como gráficas y tablas. También se repasa el modelo reactividad, es decir, cómo al cambiar algo en los parámetros de entrada de forma dinámica cambia la salida y cómo controlarlo y se muestran distintas opciones para la publicación de las aplicaciones Shiny. Por último, se mencionan algunas de las posibles extensiones al paquete. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
