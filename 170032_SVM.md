

# Máquinas de vector soporte {#cap-svm}

*Ramón A. Carrasco*$^{a}$ e *Itzcóatl Bueno*$^{b,a}$

$^{a}$Universidad Complutense de Madrid 
$^{b}$Instituto Nacional de Estadística

## Introducción

Aunque las máquinas de vector soporte se desarrollaron en los años 90 dentro de la comunidad informática ([@boser1992training], [@cortes1995support]) como un método de clasificación binaria, su aplicación se ha extendido a problemas de clasificación múltiple y regresión. Como técnica de clasificación, las máquinas de vector soporte (SVM por sus siglas inglés *Support Vector Machines*) son similares a la regresión logística pero la SVM enfatiza en un margen de error aceptable en torno a la frontera de decisión. \index{frontera!de decisión}

<div class="figure" style="text-align: center">
<img src="img/svm_vs_log.png" alt="SVM vs Regresión logística." width="60%" />
<p class="caption">(\#fig:svm-log)SVM vs Regresión logística.</p>
</div>

En la Fig. \@ref(fig:svm-log) se muestra que la regresión logística divide las observaciones en dos clases de tal forma que se minimice la distancia entre los puntos y la **frontera de decisión** (A). Por otro lado, la frontera de decisión (B) de la SVM separa los datos en dos clases, pero maximizando la distancia entre esta y los puntos de ambas clases. El **margen** \index{margen} es la distancia entre la frontera de decisión y los puntos más cercanos. El margen es una parte clave del SVM, puesto que evita clasificaciones erróneas de casos futuros como podría pasar en el caso de la regresión logística y como se ilustra en la Fig. \@ref(fig:new-svm-obs).

<div class="figure" style="text-align: center">
<img src="img/new_svm_obs.png" alt="Nueva observación clasificada en SVM vs Regresión logística." width="60%" />
<p class="caption">(\#fig:new-svm-obs)Nueva observación clasificada en SVM vs Regresión logística.</p>
</div>

En resumen, los nuevos datos pueden ser clasificados dentro del margen. Cuanto mayor sea este margen, mayor será la capacidad para clasificar correctamente estos puntos. Por tanto, para obtener una clasificación errónea en la SVM es necesario que una observación se clasifique aún más allá del margen que en cualquier otro discriminante lineal. En problemas reales, es difícil que los discriminantes lineales, vistos en el Cap. \@ref(cap-discriminante), logren una línea que divida perfectamente las categorías a clasificar. Sin embargo, en la SVM, se incluye en la función objetivo (que mide la calidad del ajuste de los datos de entrenamiento) una penalización a los puntos que queden del lado equivocado del límite de decisión. En caso de que los datos puedan ser divididos linealmente, no se cometerá ninguna penalización y se maximizará el margen. Mientras que si los datos no son linealmente separables, el mejor ajuste vendrá dado por el equilibrio entre una penalización del error total bajo y un margen de decisión grande. La penalización a una observación mal clasificada es proporcional a la distancia desde la frontera de decisión.

Sin embargo, la SVM también tiene desventajas reseñables. En primer lugar, la SVM no es adecuada en conjuntos de datos grandes porque la complejidad\index{complejidad} de entrenamiento es elevada. Además, la SVM no funciona bien cuando los datos tienen mucho ruido, es decir, cuando las clases se superponen. Finalmente, si el conjunto de datos de entrenamiento tiene más variables que observaciones, el rendimiento del modelo disminuirá.

## Algoritmo SVM para clasificación binaria
\index{SVM}
\index{clasificación binaria}
El algoritmo por el que se obtiene un modelo SVM [@vapnik1997support] se basa en la ecuación del hiperplano compuesta por dos hiperparámetros: un vector de números reales $\omega$ de la misma dimensión que el vector de variables de entrada $x$, y un número real $b$ tal que:

```{=tex}
\begin{equation}
  \omega x- b=0
\end{equation}
```

Donde $\omega x$ es $\omega^{(1)}x^{(1)} + \omega^{(2)}x^{(2)} + \dots + \omega^{(p)}x^{(p)}$ siendo $p$ el número de variables incluidas en $x$. De este modo, la predicción para una instancia de $x$ viene dada por:

```{=tex}
\begin{equation}
  y=sign(\omega x-b)
\end{equation}
```

Siendo $sign$ el operador que devuelve +1 para cualquier valor positivo y -1 para los valores negativos. Por tanto, el objetivo es ajustar los valores óptimos de $\omega$ y $b$ para el algoritmo. Estos hiperparámetros se obtienen resolviendo un problema de optimización sujeto a las siguientes restricciones:

\begin{eqnarray}
\omega x_i - b\geq 1 \textrm{ si } y_i &= +1 \textrm{ y } \\
\omega x_i - b\leq 1 \textrm{ si } y_i &= -1 
\end{eqnarray}

Además, el objetivo del problema de optimización es maximizar el margen \index{margen} en torno a la frontera de decisión. Para conseguir esto es necesario minimizar la norma euclídea, y, por tanto, el problema a resolver es:

\begin{center}
$\min||\omega||$ sujeto a 
\end{center}
\begin{center}
$y_i(\omega x_i -b)\geq1$ para $i=1,\dots,N$
\end{center}

## ¿Y si tengo más de dos clases? 
\index{clasificación!multiclase}

Hasta ahora se ha presentado la SVM como un algoritmo solo aplicable a la clasificación de dos clases pero ¿y si se tienen más de dos clases? En general, hay dos enfoques para resolver esto: **uno contra todos** (OVA, por *One Vs All*) y **uno contra uno** (OVO, por *One Vs One*). En el enfoque OVA, se ajusta una SVM para cada clase, es decir una clase contra las demás y se clasifica a la clase para la cual el margen es mayor. En cambio, en el enfoque OVO se ajustan todas las SVM por pares y se clasifica a la clase que gane las competiciones por pares. 

## Truco del *kernel*: tratando con la no linealidad

Las SVM funcionan muy bien si la separación entre clases es lineal. Sin embargo, si la separación es más compleja se intenta transformar el espacio en otro de mayor dimensionalidad donde las clases sí sean separables linealmente. Para ello, el modelo SVM se extiende incluyendo la función de pérdida ($\ell$) \index{función!de pérdida}
"hinge" ([@gentile1998linear],[@lee2013study]) definida como: 

```{=tex}
\begin{equation}
\ell(y_i) = \max(0,1-y_i(\omega x_i-b))
\end{equation}
```

En machine learning, esta función de pérdida se utiliza para entrenar clasificadores, más concretamente para la clasificación por el margen máximo (métodos de clasificación binaria que se utiliza cuando hay una frontera lineal que separa perfectamente los datos de entrenamiento de una categoría de los de la otra), sobre todo para las SVM. La función de pérdida es cero cuando se cumplen las restricciones, es decir, si $\omega x_i$ es clasificado en el lado correcto de la frontera de decisión. Por otro lado, si un dato es mal clasificados, el valor obtenido con la función de pérdida es proporcional a la distancia hasta la frontera de decisión. Por tanto, el objetivo es minimizar la función de coste:

```{=tex}
\begin{equation}
C||\omega||^{2}+\frac{1}{N}\sum_{i=1}^{N}{\max(0,1-y_i(\omega x_i-b))}
(\#eq:svmloss)
\end{equation}
```

Donde $C$ es un hiperparámetro que controla la compensación entre incrementar el tamaño de la frontera de decisión y asegurar que cada $x_i$ sea clasificado en el lado correcto de la frontera de decisión. 

Un modelo SVM que optimiza la función de pérdida se denomina SVM *soft-margin*\index{soft margin} mientras que el modelo original es conocido como SVM *hard-margin*\index{hard margin}. La ecuación \@ref(eq:svmloss) muestra que para valores grandes de $C$ el segundo término es despreciable, por lo que el algoritmo ignorará por completo la clasificación errónea y tratará de obtener el mayor margen \index{margen} posible. Si se reduce el valor de $C$, se penaliza más cada error de clasificación, por lo que se cometerán menos errores sacrificando amplitud del margen.

A veces no es posible separar los datos por un hiperplano\index{hiperplano} en su espacio original. Sin embargo, el **truco del kernel**\index{truco!del kernel} utiliza una función que implícitamente transforma el espacio original a un espacio de mayor dimensión durante la optimización de la función de coste, como se muestra en la Fig. \@ref(fig:kernel-trick). Así, es posible transformar un espacio de datos bidimensional no separable linealmente en un espacio de datos tridimensional linealmente separable usando un mapeo específico definido por $\phi:x\rightarrow\phi(x)$ donde $\phi(x)$ es un vector de mayor dimensión que $x$. Sin embargo, no se conoce la función de mapeo que funcionará en los datos. Si se prueban todas las transformaciones posibles, podría ser ineficiente y no llegar a la resolución del problema de clasificación planteado.

<div class="figure" style="text-align: center">
<img src="img/kernel-trick.png" alt="Izquierda: Las dos clases en el espacio original (2-D). Derecha: Las dos clases en el espacio de sobredimensionado (3-D)." width="100%" />
<p class="caption">(\#fig:kernel-trick)Izquierda: Las dos clases en el espacio original (2-D). Derecha: Las dos clases en el espacio de sobredimensionado (3-D).</p>
</div>

Se puede trabajar eficientemente en espacios de mayor dimensión sin necesidad de hacer las transformaciones explícitamente. Utilizando el truco del *kernel* se puede evitar este proceso costoso de transformación de tal manera que se evita calcular el producto escalar reemplazándolo por una operación más simple con las variables originales que proporciona el mismo resultado. A continuación se explican algunos de estos operadores especiales, llamados *kernels*, que permiten llevar a cabo dicha transformación.

### Algunos *kernels* populares

Los *kernels* ([@scholkopf1997prior], [@scholkopf1997comparing])\index{kernel} más populares en el entrenamiento de SVM están incluidos dentro de la función `svm()` del paquete `e1071` donde se puede se especifican en el hiperparámetro `kernel`. Estos kernel son:

+ lineal: $K(u,v) = \langle u,v\rangle$
+ polinomial de grado $\delta$: $K(u,v) = \gamma(k_1+\langle u,v\rangle)^\delta$
+ base radial: $K(u,v) = e^{\gamma||u-v||^2}$ 
+ sigmoidal: $K(u,v) = \tanh(\gamma\langle u,v\rangle+k_1)$


Donde $\langle u,v\rangle = \sum_{i=1}^{n}{u_iv_i}$ es el producto escalar. Cada uno de estos kernels tiene sus propios hiperparámetros\index{hiperparámetro}, como $\delta$ o $\gamma$, que es necesario tunear\index{ajuste automático} para optimizar el rendimiento de la SVM. En machine learning, el término **tunear**, hace referencia al hecho de ajustar automáticamente tratando de optimizar los hiperparámetros del algoritmo. A la hora de ajustar en **R** un modelo SVM se puede conocer los hiperparámetros a ajustar utilizando la función `modelLookup` de `caret`. Por ejemplo, para una SVM con kernel de base lineal\index{kernel} se usaría así:


```r
modelLookup("svmLinear")
      model parameter label forReg forClass probModel
1 svmLinear         C  Cost   TRUE     TRUE      TRUE
```

El hiperparámetro que se puede ajustar en un modelo SVM con kernel de base lineal en **R** es el coste (C), el cual representa a la constante $C$ en la ecuación \@ref(eq:svmloss).

## Procedimiento con R: la función `svm()`

En el paquete `e1071` de R se encuentra la función `svm()` que se utiliza para entrenar un modelo máquinas vector soporte:


```r
svm(x, y, scale = TRUE, type = NULL, kernel = ..., ...)
```

+ `x`: conjunto de datos de entrenamiento que contiene los predictores
+ `y`: vector respuesta con las clases o valores de la variable respuesta.
+ `scale`: booleano que indica si es necesario escalar las variables.
+ `type`: indica si se pretende resolver un problema de clasificación o de regresión.
+ `kernel`: *kernel* utilizado durante el entrenamiento y la predicción.


##  Aplicación de un modelo SVM Radial con ajuste automático en R

Los datos utilizados para entrenar el modelo SVM en este capítulo se cargan desde la librería `CDR`. Además, para su entrenamiento se requieren las librerías `caret` y `e1071`.


```r
library("CDR")
library("caret")
library("e1071")
library("reshape")
library("ggplot2")

data(dp_entr_NUM)

```

Se entrena un modelo SVM con kernel radial utilizando el conjunto de entrenamiento con todas las variables numéricas. Previamente, se aplica una normalización z-score, presentadas en el Cap. \@ref(chap-feature), al conjunto de entrenamiento. De este modo, las variables que inicialmente tenían distintas escalas de medida, ahora todas se miden en la misma escala. Además, se ajustan automáticamente \index{ajuste automático} los hiperparámetros de dicho algoritmo durante el proceso de entrenamiento.


```r
trControl <- trainControl(
  method = "cv", 
  number = 10, 
  classProbs = TRUE,   
  preProcOptions = list("center"),
  summaryFunction = twoClassSummary  
)

# Se especifica un rango de valores para los hiperparámetros
tuneGrid <- expand.grid(sigma = seq(from=0.1, to=0.2, by=0.05),
                        C = 10**(-2:4))
```

Se define como procedimiento de muestreo una validación cruzada, como la presentada en el Cap. \@ref(chap-feature), de 10 folds. Además, se le indica al modelo que debe calcular las probabilidades de clase en cada remuestreo en caso de estar entrenando un modelo de clasificación. Con el argumento `summaryFunction = twoClassSummary` se le indica al modelo que para resumir los resultados se calculen la sensibilidad, especificidad y el área bajo la curva ROC. Como se ha comentado, conviene estandarizar los datos, esto se le indica a la función a través del argumento `preProcOptions` con la opción `center`. A su vez, se define una red de hiperparametros\index{hiperparámetro} a optimizar. A través de la función `train()` se ajusta automáticamente\index{ajuste automático} el modelo con los hiperparametros óptimos.


```r
# Se fija la semilla aleatoria
set.seed(101)

# Se entrena el modelo
model <- train(CLS_PRO_pro13 ~ ., 
             data=dp_entr_NUM, 
             method="svmRadial", 
             metric="ROC",
             trControl=trControl,
             tuneGrid=tuneGrid)

model


Support Vector Machines with Radial Basis Function Kernel 

558 samples
 19 predictor
  2 classes: 'S', 'N' 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 502, 502, 502, 503, 503, 502, ... 
Resampling results across tuning parameters:

  sigma  C      ROC        Sens       Spec     
  0.10   1e-02  0.9553241  0.8785714  0.7071429
  0.10   1e-01  0.9566327  0.8924603  0.8247354
  0.10   1e+00  0.9434902  0.8604497  0.8496032
  0.10   1e+01  0.9227230  0.8460317  0.8423280
  0.10   1e+02  0.8804894  0.8567460  0.8279101
  0.10   1e+03  0.8645692  0.8674603  0.8206349
  0.10   1e+04  0.8548469  0.8423280  0.8242063
  0.15   1e-02  0.9527636  0.8535714  0.6642857
  0.15   1e-01  0.9513653  0.9105820  0.8105820
  0.15   1e+00  0.9310091  0.8783069  0.8494709
  0.15   1e+01  0.8941421  0.8531746  0.8387566
  0.15   1e+02  0.8602088  0.8781746  0.8242063
  0.15   1e+03  0.8369331  0.8458995  0.8134921
  0.15   1e+04  0.8369284  0.8637566  0.8064815
  0.20   1e-02  0.9443925  0.8535714  0.6321429
  0.20   1e-01  0.9440098  0.9250000  0.7384921
  0.20   1e+00  0.9199310  0.8818783  0.8387566
  0.20   1e+01  0.8752031  0.8674603  0.8207672
  0.20   1e+02  0.8477324  0.8674603  0.8063492
  0.20   1e+03  0.8308296  0.8638889  0.8134921
  0.20   1e+04  0.8308296  0.8638889  0.8099206

ROC was used to select the optimal model using the largest value.
The final values used for the model were sigma = 0.1 and C = 0.1.
```



Los argumentos que requiere la función son la `formula`, es decir, indicar la variable respuesta y qué predictores intervienen en el modelo. Los datos que se van a utilizar, así como el algoritmo a entrenar, en este caso la SVM con kernel de base radial. Además, se indica una métrica para el rendimiento del modelo, en caso de no indicarlo **R** asigna la más acorde de acuerdo a la variable respuesta. Finalmente, se incluyen las opciones de entrenamiento y la red de hiperparámetros a probar para determinar la combinación óptima. Los hiperparámetros del modelo entrenado son $\sigma=0.1$ y $C=0.1$. Este resultado queda definido en la salida del modelo, pero también es representable como en la Fig. \@ref(fig:006-002-202SVMTUNEPLOT). En este gráfico el eje y mide el rendimiento del modelo para ciertos valores de los hiperparámetros. Cada línea representa un valor para el hiperparámetro sigma, y se mide su rendimiento variando distintos niveles del parámetro coste (C), que queda representado en el eje x. Así, se observa que la línea roja (sigma=0,1) alcanza el mayor nivel de precisión en el valor C=0,1.



```r
ggplot(model)
```

<div class="figure" style="text-align: center">
<img src="img/svm-tune-process.png" alt="Optimización de los parámetros C y sigma de una SVM." width="60%" />
<p class="caption">(\#fig:006-002-202SVMTUNEPLOT)Optimización de los parámetros C y sigma de una SVM.</p>
</div>

Los boxplot de la Fig. \@ref(fig:006-002-202SVMRESULTS3) muestran un resumen del rendimiento del modelo en las distintas repeticiones del proceso de validación cruzada. De esta manera, se observa como la sensibilidad es superior al 75% y la especificidad supera valores del 70%, esto indica que el modelo entrenado es capaz de predecir correctamente tanto a los clientes que van a comprar el nuevo producto como los que no lo van a hacer. 


```r
ggplot(melt(model$resample[,-4]), aes(x = variable, y = value, fill=variable)) + 
  geom_boxplot(show.legend=FALSE) + 
  xlab(NULL) + ylab(NULL)
```

<div class="figure" style="text-align: center">
<img src="img/svm-boxplot.png" alt="Resultados del modelo obtenidos durante la validación cruzada." width="60%" />
<p class="caption">(\#fig:006-002-202SVMRESULTS3)Resultados del modelo obtenidos durante la validación cruzada.</p>
</div>

### Importancia de las variables

En machine learning\index{machine learning}, muchos de los algoritmos de caja negra\index{caja negra} (definidos en el Cap. \@ref(cap-etica)), no proporcionan información sobre la importancia\index{importancia} que tiene cada variable en el modelo. Este es el caso de las máquinas de vector soporte. Tanto para la SVM como para otros algoritmos, es posible cuantificar la importancia de cada variable utilizando paquetes de **R** como `DALEX`, `iml` o `vip`.
 
Este último paquete incluye una función con el mismo nombre `vip()`. Para medir la importancia\index{importancia}, se indica qué métrica se utilizó en el proceso de entrenamiento del modelo, en el caso de la SVM se indicará que fue el área bajo la curva (`metric="auc"`). En el argumento `pred_wrapper` se indica una función de medida que contenga tanto los valores observados como los valores predichos. Dado que la SVM entrenada utiliza AUC para medir el rendimiento del modelo ajustado, la función de medida indicada en `pred_wrapper` deberá devolver la probabilidad de que el modelo asigne una observación a la clase de referencia. En este ejemplo, la clase de referencia es "SI", puesto que interesa saber si un cliente comprará el nuevo producto. Entonces, la función de predicción se define como:


```r
prob_si <- function(object, newdata) {
  predict(object, newdata = newdata, type = "prob")[, "S"]
}
```

Ejecutando la función `vip()` con los argumentos mencionados se genera la Fig. \@ref(fig:vip-svm). Este gráfico indica la importancia\index{importancia} de cada variable en el modelo de más a menos importante. En este caso, la variable más importante es el importe gastado en el *smartchwatch fitness*, seguida muy de cerca por la variable que indica si el cliente compra o no el *smartchwatch fitness*. En el otro extremo, se observa que las variables que indican si el cliente tiene un nivel de educación básico o no, no son muy relevantes en la SVM entrenada.


```r
library("vip")

set.seed(101)  
vip(model, train = dp_entr_NUM, target = "CLS_PRO_pro13", metric = "auc",
    reference_class = "S", pred_wrapper = prob_si, method="permute",
    aesthetics = list(color = "steelblue2", fill = "steelblue2"))
```


<div class="figure" style="text-align: center">
<img src="img/vip-svm.png" alt="Importancia de las variables incluidas en la SVM." width="60%" />
<p class="caption">(\#fig:vip-svm)Importancia de las variables incluidas en la SVM.</p>
</div>

A partir de la Fig. \@ref(fig:vip-svm) se puede concluir que para predecir si un cliente comprará o no el *tensiómetro digital* las variables que más importancia tienen: son el importe que gastó en el *smartchwatch fitness*, si compró o no el *smartchwatch fitness*, el importe que gastó en la *depiladora eléctrica* y si compró o no la *depiladora eléctrica*.

De forma similar se podrían probar el resto de *kernels*\index{kernel} disponibles para el algoritmo SVM\index{SVM}.

::: {.infobox_resume data-latex=""}
### Resumen {-}
En este capítulo se introduce al lector en el algoritmo de máquinas vector soporte, en particular:

- Se presenta el concepto de margen de decisión, y las ventajas de la SVM respecto a otras técnicas de clasificación.
- Se explica el truco del kernel cuando los datos no son separables por un hiperplano en su espacio original
- Se da un repaso a los kernels más utilizados.
- Se presenta la aplicación de una SVM con kernel radial en **R** para la clasificación de datos con respuesta binaria, en el que se ajustan automáticamente los hiperparámetros. 
- Se obtiene la importancia de las variables del modelo final.
:::
