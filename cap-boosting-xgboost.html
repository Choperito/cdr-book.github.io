<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Capítulo 29 \(\bf \textit{Boosting}\) y el algoritmo XGBoost | Fundamentos de ciencia de datos con R</title>
<meta name="author" content="Gema Fernández-Avilés y José-María Montero">
<meta name="description" content="Ramón A. Carrasco\(^{a}\), Itzcóatl Bueno\(^{a,\hspace{0,05cm}b}\) y José-María Montero\(^{c}\) \(^{a}\)Universidad Complutense de Madrid\(^{b}\)Instituto Nacional de...">
<meta name="generator" content="bookdown 0.37 with bs4_book()">
<meta property="og:title" content="Capítulo 29 \(\bf \textit{Boosting}\) y el algoritmo XGBoost | Fundamentos de ciencia de datos con R">
<meta property="og:type" content="book">
<meta property="og:image" content="/img/cover.png">
<meta property="og:description" content="Ramón A. Carrasco\(^{a}\), Itzcóatl Bueno\(^{a,\hspace{0,05cm}b}\) y José-María Montero\(^{c}\) \(^{a}\)Universidad Complutense de Madrid\(^{b}\)Instituto Nacional de...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Capítulo 29 \(\bf \textit{Boosting}\) y el algoritmo XGBoost | Fundamentos de ciencia de datos con R">
<meta name="twitter:description" content="Ramón A. Carrasco\(^{a}\), Itzcóatl Bueno\(^{a,\hspace{0,05cm}b}\) y José-María Montero\(^{c}\) \(^{a}\)Universidad Complutense de Madrid\(^{b}\)Instituto Nacional de...">
<meta name="twitter:image" content="/img/cover.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.6.1/transition.js"></script><script src="libs/bs3compat-0.6.1/tabs.js"></script><script src="libs/bs3compat-0.6.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><link href="libs/tabwid-1.1.3/tabwid.css" rel="stylesheet">
<script src="libs/tabwid-1.1.3/tabwid.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css">
<link rel="stylesheet" href="bs4_book.css">
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Fundamentos de ciencia de datos con <strong>R</strong></a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Buscar" aria-label="Buscar">
</form>

      <nav aria-label="Contenido"><h2>Contenido</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Prefacio</a></li>
<li><a class="" href="pr%C3%B3logo-by-julia-silge.html">Prólogo (by Julia Silge)</a></li>
<li><a class="" href="pr%C3%B3logo-por-yanina-bellini.html">Prólogo (por Yanina Bellini)</a></li>
<li class="book-part">Ciencia, datos, software… y científicos</li>
<li><a class="" href="ciencia-datos.html"><span class="header-section-number">1</span> ¿Es la ciencia de datos una ciencia?</a></li>
<li><a class="" href="metodologia.html"><span class="header-section-number">2</span> Metodología en ciencia de datos</a></li>
<li><a class="" href="ch-110003.html"><span class="header-section-number">3</span> R para ciencia de datos</a></li>
<li><a class="" href="cap-etica.html"><span class="header-section-number">4</span> Ética en la ciencia de datos</a></li>
<li class="book-part">Bienvenidos a la jungla de datos</li>
<li><a class="" href="datos-sql.html"><span class="header-section-number">5</span> Gestión de bases de datos relacionales</a></li>
<li><a class="" href="cap-nosql.html"><span class="header-section-number">6</span> Gestión de bases de datos NoSQL</a></li>
<li><a class="" href="DGDQM.html"><span class="header-section-number">7</span> Gobierno, gestión y calidad del dato</a></li>
<li><a class="" href="cap-130009.html"><span class="header-section-number">8</span> Integración y limpieza de datos</a></li>
<li><a class="" href="chap-feature.html"><span class="header-section-number">9</span> Selección y transformación de variables</a></li>
<li><a class="" href="chap-herramientas.html"><span class="header-section-number">10</span> Herramientas para el análisis en ciencia de datos</a></li>
<li><a class="" href="cap-120006-aed.html"><span class="header-section-number">11</span> Análisis exploratorio de datos</a></li>
<li class="book-part">Fundamentos de estadística</li>
<li><a class="" href="Funda-probab.html"><span class="header-section-number">12</span> Probabilidad</a></li>
<li><a class="" href="Fundainfer.html"><span class="header-section-number">13</span> Inferencia estadística</a></li>
<li><a class="" href="muestreo.html"><span class="header-section-number">14</span> Muestreo y remuestreo</a></li>
<li class="book-part">Modelización estadística</li>
<li><a class="" href="cap-lm.html"><span class="header-section-number">15</span> Modelización lineal</a></li>
<li><a class="" href="cap-glm.html"><span class="header-section-number">16</span> Modelos lineales generalizados</a></li>
<li><a class="" href="cap-gam.html"><span class="header-section-number">17</span> Modelos aditivos generalizados</a></li>
<li><a class="" href="cap-mxm.html"><span class="header-section-number">18</span> Modelos mixtos</a></li>
<li><a class="" href="cap-sparse.html"><span class="header-section-number">19</span> Modelos \(\textit{sparse}\) y métodos penalizados de regresión</a></li>
<li><a class="" href="cap-series-temp.html"><span class="header-section-number">20</span> Modelización de series temporales</a></li>
<li><a class="" href="cap-discriminante.html"><span class="header-section-number">21</span> Análisis discriminante</a></li>
<li><a class="" href="cap-conjunto.html"><span class="header-section-number">22</span> Análisis conjunto</a></li>
<li><a class="" href="tablas-contingencia.html"><span class="header-section-number">23</span> Análisis de tablas de contingencia</a></li>
<li class="book-part">Machine learning supervisado</li>
<li><a class="" href="cap-arboles.html"><span class="header-section-number">24</span> Árboles de clasificación y regresión</a></li>
<li><a class="" href="cap-svm.html"><span class="header-section-number">25</span> Máquinas de vector soporte</a></li>
<li><a class="" href="cap-knn.html"><span class="header-section-number">26</span> Clasificador \(k\)-vecinos más próximos</a></li>
<li><a class="" href="cap-naive-bayes.html"><span class="header-section-number">27</span> Naive Bayes</a></li>
<li><a class="" href="cap-bagg-rf.html"><span class="header-section-number">28</span> Métodos ensamblados: \(\bf \textit {bagging}\) y \(\bf \textit{random}\) \(\bf \textit{forest}\)</a></li>
<li><a class="active" href="cap-boosting-xgboost.html"><span class="header-section-number">29</span> \(\bf \textit{Boosting}\) y el algoritmo XGBoost</a></li>
<li class="book-part">Machine learning no supervisado</li>
<li><a class="" href="cap-cluster.html"><span class="header-section-number">30</span> Análisis clúster: clusterización jerárquica</a></li>
<li><a class="" href="no-jerarquico.html"><span class="header-section-number">31</span> Análisis clúster: clusterización no jerárquica</a></li>
<li><a class="" href="acp.html"><span class="header-section-number">32</span> Análisis de componentes principales</a></li>
<li><a class="" href="af.html"><span class="header-section-number">33</span> Análisis factorial</a></li>
<li><a class="" href="mds.html"><span class="header-section-number">34</span> Escalamiento multidimensional</a></li>
<li><a class="" href="correspondencias.html"><span class="header-section-number">35</span> Análisis de correspondencias</a></li>
<li class="book-part">Deep learning</li>
<li><a class="" href="capNN.html"><span class="header-section-number">36</span> Redes neuronales artificiales</a></li>
<li><a class="" href="cap-redes-convol.html"><span class="header-section-number">37</span> Redes neuronales convolucionales</a></li>
<li class="book-part">Ciencia de datos de texto y redes</li>
<li><a class="" href="mineria-textos.html"><span class="header-section-number">38</span> Minería de textos</a></li>
<li><a class="" href="grafos.html"><span class="header-section-number">39</span> Análisis de grafos y redes sociales</a></li>
<li class="book-part">Ciencia de datos espaciales</li>
<li><a class="" href="datos-espaciales.html"><span class="header-section-number">40</span> Trabajando con datos espaciales</a></li>
<li><a class="" href="geo.html"><span class="header-section-number">41</span> Geoestadística</a></li>
<li><a class="" href="cap-econom-esp.html"><span class="header-section-number">42</span> Modelos econométricos espaciales</a></li>
<li><a class="" href="cap-pp.html"><span class="header-section-number">43</span> Procesos de puntos</a></li>
<li class="book-part">Comunica y colabora</li>
<li><a class="" href="cap-120007-informes.html"><span class="header-section-number">44</span> Informes reproducibles con R Markdown y Quarto</a></li>
<li><a class="" href="shiny.html"><span class="header-section-number">45</span> Creación de aplicaciones web interactivas con Shiny</a></li>
<li><a class="" href="github.html"><span class="header-section-number">46</span> Git y GitHub R</a></li>
<li><a class="" href="geoproces.html"><span class="header-section-number">47</span> Geoprocesamiento en nube</a></li>
<li class="book-part">Casos de estudio en ciencia de datos</li>
<li><a class="" href="cap-crimen.html"><span class="header-section-number">48</span> Análisis de una red criminal</a></li>
<li><a class="" href="cap-publicidad.html"><span class="header-section-number">49</span> Optimización de inversiones publicitarias</a></li>
<li><a class="" href="cap-twitter.html"><span class="header-section-number">50</span> ¿Cómo tuitea Elon Musk?</a></li>
<li><a class="" href="cap-periodismo.html"><span class="header-section-number">51</span> Análisis electoral: de RStudio a su periódico favorito</a></li>
<li><a class="" href="paro-clm.html"><span class="header-section-number">52</span> El impacto de las crisis financiera y de la COVID-19 en el paro de CLM</a></li>
<li><a class="" href="cap-rfm.html"><span class="header-section-number">53</span> Segmentación de clientes en el comercio minorista</a></li>
<li><a class="" href="cap-medicina.html"><span class="header-section-number">54</span> Análisis de datos en medicina</a></li>
<li><a class="" href="cap-futbol.html"><span class="header-section-number">55</span> Messi y Ronaldo: dos ídolos desde la perspectiva de los datos</a></li>
<li><a class="" href="cambioclimatico.html"><span class="header-section-number">56</span> Una nota sobre el cambio climático</a></li>
<li><a class="" href="cap-sist-exp.html"><span class="header-section-number">57</span> Implementación de un sistema experto en el ámbito pediátrico</a></li>
<li><a class="" href="cap-ree.html"><span class="header-section-number">58</span> Predicción de consumo eléctrico con redes neuronales artificiales</a></li>
<li><a class="" href="nlp-textil.html"><span class="header-section-number">59</span> El procesamiento del lenguaje natural para tendencias de moda en textil</a></li>
<li><a class="" href="cap-fraude.html"><span class="header-section-number">60</span> Detección de fraude de tarjetas de crédito</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="info-session.html"><span class="header-section-number">A</span> Información de la sesión</a></li>
<li><a class="" href="referencias.html">Referencias</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="cap-boosting-xgboost" class="section level1" number="29">
<h1>
<span class="header-section-number">Capítulo 29</span> <span class="math inline">\(\bf \textit{Boosting}\)</span> y el algoritmo XGBoost<a class="anchor" aria-label="anchor" href="#cap-boosting-xgboost"><i class="fas fa-link"></i></a>
</h1>
<p><em>Ramón A. Carrasco</em><span class="math inline">\(^{a}\)</span>, <em>Itzcóatl Bueno</em><span class="math inline">\(^{a,\hspace{0,05cm}b}\)</span> y <em>José-María Montero</em><span class="math inline">\(^{c}\)</span></p>
<p><span class="math inline">\(^{a}\)</span>Universidad Complutense de Madrid<br><span class="math inline">\(^{b}\)</span>Instituto Nacional de Estadística<br><span class="math inline">\(^{c}\)</span>Universidad de Castilla-La Mancha</p>
<div id="métodos-ensamblados-bagging-vs.-boosting" class="section level2" number="29.1">
<h2>
<span class="header-section-number">29.1</span> Métodos ensamblados: <em>bagging vs. boosting</em><a class="anchor" aria-label="anchor" href="#m%C3%A9todos-ensamblados-bagging-vs.-boosting"><i class="fas fa-link"></i></a>
</h2>
<p><strong><em>Boosting</em></strong> es el segundo de los paradigmas de aprendizaje ensamblado introducido en el Cap. <a href="cap-bagg-rf.html#cap-bagg-rf">28</a>. La principal diferencia con el primero de ellos, <em>bagging</em>, radica en cómo se combinan los modelos individuales (también denominados “débiles”) para obtener una predicción final.
Para diferenciar uno de otro de una forma intuitiva, si se quisiera organizar una fiesta y se tuviese que tomar una decisión sobre la la decoración, el <em>bagging</em> pediría opinión a distintos amigos al mismo tiempo. Después se combinarán todas sus ideas para tomar una decisión final sobre la decoración de la fiesta (la unión hace la fuerza). Sin embargo, el <em>boosting</em> pediría opinión a un amigo en particular. Si su respuesta no es convincente, entonces buscaría la ayuda de otro amigo, quien trataría de mejorar la respuesta anterior. Este proceso continuaría secuencialmente, solicitando la ayuda de diferentes amigos y construyendo sobre las respuestas anteriores para obtener una decisión final más precisa y refinada sobre la decoración de la fiesta.</p>
</div>
<div id="qué-es-el-boosting" class="section level2" number="29.2">
<h2>
<span class="header-section-number">29.2</span> ¿Qué es el <em>boosting</em>?<a class="anchor" aria-label="anchor" href="#qu%C3%A9-es-el-boosting"><i class="fas fa-link"></i></a>
</h2>
<p>Como se a avanzado en la sección anterior, el <em>boosting</em> <span class="citation">(<a href="referencias.html#ref-schapire2012">Schapire &amp; Freund, 2012</a>)</span> es, junto con el <em>bagging</em>, el paradigma de aprendizaje ensamblado más popular. Como el <em>bagging</em>, el <em>boosting</em> agrega múltiples modelos con menor precisión (débiles) combinando sus predicciones para obtener un metamodelo con un porcentaje de clasificación correcta más alto. Los árboles de decisión son los modelos base o débiles que se usan más frecuentemente. En este caso, para llegar al metamodelo a partir de los modelos base, es necesario introducir ponderaciones a los árboles basándose en las clasificaciones erróneas del árbol previamente entrenado.</p>
<p>Continuando con las diferencias entre ambos paradigmas, en cuanto al objetivo, el principal objetivo intrínseco de los algoritmos de <em>bagging</em> es el de la reducción de la varianza, mientras que el de los algoritmos de <em>boosting</em> es la reducción del sesgo.</p>
<p>Otra diferencia entre ambos paradigmas, como se deduce de la introducción intuitiva anteriormente expuesta, es que en el <em>bagging</em> los algoritmos simples se usan en paralelo para aprovecharse entre ellos, ya que el error se puede reducir significativamente al promediar las salidas de los modelos simples. En términos coloquiales, es como si a la hora de dar solución a un problema y preguntar a varias personas, independientes entre sí, se adopta la solución mayoritaria.
El <em>boosting</em> utiliza los modelos simples de forma secuencial, sacando ventaja de la dependencia entre dichos modelos simples. El rendimiento del modelo global o metamodelo mejora haciendo que un modelo simple posterior le dé más importancia a los errores cometidos por un modelo simple anterior. Volviendo al símil de la resolución de un problema, es como si se aprovechase el conocimiento de los errores de otros para mejorar no cometiéndolos posteriormente.</p>
<p>Finalmente, aunque hay más, en el <em>boosting</em> las predicciones de cada modelo simple se combinan por medio de una votación “ponderada” (para problemas de clasificación) o de una suma ponderada (para problemas de regresión) para producir la predicción final. Ello se debe a que, a diferencia del <em>bagging</em>, los modelos simples no se entrenan de forma independiente, sino que se ponderan según los errores de los anteriores.</p>
<p>Además de la reducción del sesgo, uno de los mayores problemas del <em>machine learning</em>, el <em>boosting</em> tiene las siguientes ventajas:</p>
<ul>
<li><p>Su sencilla implementación mediante algoritmos fáciles de comprender e interpretar que aprenden de sus propios errores.</p></li>
<li><p>No requieren preprocesamiento de los datos y tienen rutinas que tratan la cuestión de los datos faltantess.</p></li>
<li><p>Tiene una alta eficacia computacional ya que, al tener la precisión predictiva como prioridad principal en la fase de entrenamiento, pues selecciona las variables que aumentan su poder predictivo, reduce la dimensión del problema.</p></li>
</ul>
<p>En cuanto a las desventajas, merece la pena destacar las siguientes:</p>
<ul>
<li><p>Mientras que en el <em>bagging</em> (por ejemplo, en el <em>random forest</em>) agregar más árboles al <em>random forest</em> ayuda a estabilizar el error sin provocar un sobreajuste del modelo, en el <em>boosting</em> sí puede dar lugar a sobreajuste y, por ello, hay que ser cauteloso a la hora de agregar nuevos árboles.</p></li>
<li><p>Como el <em>boosting</em> aprende iterativamente de los errores en árboles anteriores, puede llevar a sobreajustar el modelo. Aunque este enfoque produce predicciones más precisas, muchas veces mejores que la mayoría de algoritmos, puede llevar a ajustar las observaciones atípicas. Esta es una desventaja respecto del <em>bagging</em> y por ello esta última es preferible cuando se trabaja con conjuntos de datos muy complejos con un gran número de observaciones atípicas.</p></li>
<li><p>Otra desventaja del <em>boosting</em> es que su tiempo de procesamiento es muy elevado, puesto que su entrenamiento sigue una lógica secuencial. En el proceso de entrenamiento, un árbol debe esperar a que el inmediatamente anterior sea entrenado para iniciar su entrenamiento, y esto limita la escalabilidad del modelo. Mientras tanto, el <em>bagging</em> entrena los árboles en paralelo, lo que hace que su tiempo de procesamiento sea más rápido. Por tanto, el <em>boosting</em> tiene más eficacia computacional que el <em>bagging</em> (solo utiliza variables “importantes”), pero es menos eficaz respecto al número de recursos utilizados.</p></li>
<li><p>Finalmente, igual que el <em>bagging</em>, los algoritmos de <em>boosting</em> presentan el inconveniente de la dificultad de interpretación que tienen respecto a los árboles de decisión. En este sentido, ambos algoritmos de ensamblado se pueden considerar como “de caja negra”.</p></li>
</ul>
</div>
<div id="bf-textitgradient-boosting" class="section level2" number="29.3">
<h2>
<span class="header-section-number">29.3</span> <span class="math inline">\(\bf \textit{Gradient boosting}\)</span><a class="anchor" aria-label="anchor" href="#bf-textitgradient-boosting"><i class="fas fa-link"></i></a>
</h2>
<p></p>
<p>Uno de los algoritmos de <em>boosting</em> más conocidos es el <strong><em>gradient boosting</em></strong> (GB).<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;El término &lt;em&gt;gradient boosting machine&lt;/em&gt; viene del hecho de que este procedimiento puede generalizarse a funciones de pérdida distintas a la suma del cuadrado del error de predicción. Por otra parte, el &lt;em&gt;gradient descent&lt;/em&gt; (descenso por el gradiente) es un algoritmo de optimización muy genérico capaz de ajustar parámetros iterativamente con el objetivo de minimizar una función de coste. La idea es descender por la “montaña” de la función de coste buscando, en cada punto, la parte más empinada, para llegar antes al mínimo (a la base de la montaña, donde el gradiente es nulo).&lt;/p&gt;"><sup>200</sup></a> Mientras que el <em>random forest</em>} selecciona combinaciones aleatorias de variables en cada proceso de construcción de un árbol, el <em>gradient boosting</em> selecciona las variables que mejoran la precisión con cada nuevo árbol. Por lo tanto, la construcción del modelo es secuencial, puesto que cada nuevo árbol se construye utilizando información derivada del árbol anterior y, en consecuencia, la construcción de estos árboles no es independiente. En cada iteración se registran los errores cometidos en los datos de entrenamiento y se tienen en cuenta para la siguiente ronda de entrenamiento. Además, se ponderan las observaciones (o instancias), como se observa en la Fig. <a href="cap-boosting-xgboost.html#fig:boosting">29.1</a>, en base a los resultados de la iteración anterior (si la observación se clasificó correcta o erróneamente). Las ponderaciones más altas se aplicarán a las observaciones que fueron erróneamente clasificadas, no prestándose tanta atención a las bien clasificadas. Este proceso se repite hasta que se llega a un nivel bajo de error. El resultado final se obtiene a través de la media ponderada de las predicciones de los árboles de decisión simples. Las ponderaciones generalmente se establecen como la tasa de aprendizaje, que suele ser de pequeña magnitud.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:boosting"></span>
<img src="img/boosting.png" alt="Ejemplo de $gradient$ $boosting$." width="100%"><p class="caption">
Figura 29.1: Ejemplo de <span class="math inline">\(gradient\)</span> <span class="math inline">\(boosting\)</span>.
</p>
</div>
<p>Matemáticamente, un algoritmo <em>gradient boosting</em> para clasificación sigue los pasos que se detallan a continuación. Sea un problema de clasificación binaria. Asumiendo que se tienen <span class="math inline">\(K\)</span> árboles de clasificación, la predicción del <em>gradient boosting</em> se obtiene utilizando la función sigmoide, como en la regresión logística (véase Cap. <a href="cap-glm.html#cap-glm">16</a>), tal que:</p>
<span class="math display">\[\begin{equation}
P(y=1|x,f)=\frac{1}{1+e^{-f(x)}},
\end{equation}\]</span>
<p>donde <span class="math inline">\(f(x)=\sum_{\kappa=1}^{K}{f_{\kappa}(x)}\)</span> y <span class="math inline">\(f_{\kappa}\)</span> es un árbol de decisión. De nuevo, como en la regresión logística, se aplica la estimación máximo verosímil, tratando de hallar una función <span class="math inline">\(f\)</span> que maximice la log-verosimilitud: <span class="math inline">\(\mathcal{L}_f = \sum_{i=1}^{N}{\ln\left(P(y_i=1|x_i,f)\right)}\)</span>.</p>
<p>El algoritmo, en origen, es un modelo constante de la forma <span class="math inline">\(f=f_0=\frac{p}{1-p}\)</span> donde <span class="math inline">\(p=\frac{1}{N}\sum^{N}_{i=1} x_i\)</span>. Tras cada iteración se añade un nuevo árbol al modelo; así, tras la kappa-ésima iteracción se añade el modelo (árbol) <span class="math inline">\(f_\kappa\)</span>. Para encontrar el mejor “nuevo” árbol, <span class="math inline">\(f_\kappa\)</span>, se calcula la primera derivada parcial de la log-verosimilitud, <span class="math inline">\(g_i,\hspace{0,1cm} i=1,\dots,N\)</span>:</p>
<span class="math display">\[\begin{equation}
\frac{\delta g_i} {\delta f} = \frac{\delta\mathcal{L}_f}{\delta f}=\frac {\delta \left(P(y=1|x,f\right)}{\delta f}= \frac{\delta \left(\frac {1}{1+e^{-f(x)}}\right)} {\delta f}=\frac{1}{e^{f(x_i)}+1}, \hspace{0,3cm} i=1,\dots,N,
\end{equation}\]</span>
<p>donde <span class="math inline">\(f\)</span> es el modelo de clasificación ensamblado construido en la iteración previa.</p>
<p>A continuación, en el conjunto de entrenamiento, se reemplaza el valor de la categoría original <span class="math inline">\(y_i, \hspace{0,1cm} i=1,\dots, N\)</span> por el que indica su correspondiente derivada parcial, <span class="math inline">\(g_i\)</span>, y se construye el “nuevo” modelo <span class="math inline">\(f_\kappa\)</span> utilizando el conjunto de entrenamiento transformado. Una vez obtenido el nuevo modelo, se obtiene la ponderación óptima (<span class="math inline">\(\rho_\kappa\)</span>) como:</p>
<span class="math display">\[\begin{equation}
\rho_\kappa = \arg \max\limits_{\rho}{\mathcal{L}_{f+\rho f_\kappa}}.
\end{equation}\]</span>
<p>Al terminar la iteración <span class="math inline">\(\kappa\)</span>, se actualiza el modelo ensamblado <span class="math inline">\(f\)</span> añadiendo el nuevo árbol <span class="math inline">\(f_\kappa\)</span>:</p>
<span class="math display">\[\begin{equation}
f\leftarrow f+\alpha\rho_\kappa f_\kappa.
\end{equation}\]</span>
<p>donde <span class="math inline">\(\alpha\)</span> es el aprendizaje en el gradiente, un valor entre 0 y 1 que controla la contribución de cada árbol débil al modelo ensamblado.</p>
<p>Se itera hasta que <span class="math inline">\(\kappa=K\)</span>, entonces el proceso se detiene y se obtiene el modelo ensamblado final <span class="math inline">\(f\)</span>.</p>
<div id="hiperparámetros-del-modelo-gradient-boosting" class="section level3" number="29.3.1">
<h3>
<span class="header-section-number">29.3.1</span> Hiperparámetros del modelo <em>gradient boosting</em><a class="anchor" aria-label="anchor" href="#hiperpar%C3%A1metros-del-modelo-gradient-boosting"><i class="fas fa-link"></i></a>
</h3>
<p>Un modelo de <em>gradient boosting</em> tiene dos tipos de hiperparámetros:</p>
<ul>
<li>Hiperparámetros de <em>boosting</em>.</li>
<li>Hiperparámetros del árbol.</li>
</ul>
<div id="hiperparámetros-de-boosting" class="section level4" number="29.3.1.1">
<h4>
<span class="header-section-number">29.3.1.1</span> Hiperparámetros de <em>boosting</em><a class="anchor" aria-label="anchor" href="#hiperpar%C3%A1metros-de-boosting"><i class="fas fa-link"></i></a>
</h4>
<p>Los hiperparámetros de <em>boosting</em> son principalmente dos: el número de árboles y la tasa de aprendizaje.</p>
<p>El primero indica el número de árboles a construir que, como se ha comentado, es importante optimizar para evitar el sobreajuste del modelo. A diferencia de los modelos <em>random forest</em> o <em>bagging</em>, en el <em>boosting</em> el conjunto de árboles débiles crece de forma secuencial para que cada árbol corrija los errores del anterior. El número de árboles necesarios para que el modelo sea buen predictor puede verse incrementado en función de los valores que tomen los otros hiperparámetros.</p>
<p>La tasa de aprendizaje es el hiperparámetro que determina la contribución de cada árbol al resultado final y controla la rapidez con la que el algoritmo avanza por el descenso del gradiente, es decir, la velocidad a la que aprende. Este hiperparámetro toma valores entre 0 y 1, aunque los valores habituales oscilan entre 0,001 y 0,3. Cuando esta tasa toma valores bajos, el modelo se robustece frente a las características específicas de cada árbol, permitiendo una buena generalización. Además, valores bajos de la tasa de aprendizaje facilitan también una parada temprana, antes del sobreajuste del modelo. En contraposición, los modelos se vuelven computacionalmente más exigentes, lo cual dificulta alcanzar el modelo óptimo con un número fijo de árboles. En resumen, cuanto menor sea este valor, más preciso puede ser el modelo, pero también requerirá más árboles en la secuencia.</p>
</div>
<div id="hiperparámetros-de-árbol" class="section level4" number="29.3.1.2">
<h4>
<span class="header-section-number">29.3.1.2</span> Hiperparámetros de árbol<a class="anchor" aria-label="anchor" href="#hiperpar%C3%A1metros-de-%C3%A1rbol"><i class="fas fa-link"></i></a>
</h4>
<p>Los principales hiperparámetros de árbol son: la profundidad del árbol y el número mínimo de observaciones en nodos terminales, como se vio en el Cap. <a href="cap-arboles.html#cap-arboles">24</a>.</p>
<p>El primer hiperparámetro controla la profundidad de los árboles individuales. Los valores habituales de profundidad oscilan entre 3 y 8. Los árboles de menor profundidad son eficientes computacionalmente, pero menos precisos. Sin embargo, los árboles de mayor profundidad permiten que el algoritmo capture interacciones únicas, aunque aumentan el riesgo de sobreajuste.</p>
<p>El segundo hiperparámetro, además de controlar el número mínimo de observaciones en los nodos terminales, controla la complejidad de cada árbol. Los valores típicos de este hiperparámetro suelen estar entre 5 y 15. Los valores más altos ayudan a evitar que un modelo aprenda relaciones que pueden ser muy específicas de la muestra particular seleccionada para entrenar el árbol, evitando así el sobreajuste. Los valores más pequeños pueden ser “recomendables” en el caso de problemas de clasificación en los que las clases de la variable respuesta estén muy desequilibradas en cuanto a número de observaciones, con lo cual el número de ellas en alguna (o algunas) de las clases pudiera ser muy pequeño.</p>
</div>
</div>
<div id="estrategia-de-ajuste-de-hiperparámetros" class="section level3" number="29.3.2">
<h3>
<span class="header-section-number">29.3.2</span> Estrategia de ajuste de hiperparámetros<a class="anchor" aria-label="anchor" href="#estrategia-de-ajuste-de-hiperpar%C3%A1metros"><i class="fas fa-link"></i></a>
</h3>
<p>A diferencia del <em>random forest</em>, la precisión de los modelos <em>gradient boosting</em> puede variar mucho en función de la configuración de sus hiperparámetros. Por ello, el ajuste de dichos hiperparámetros puede requerir seguir una estrategia. Una estrategia recomendable es la siguiente:</p>
<ul>
<li>Elegir una tasa de aprendizaje relativamente alta. El valor predeterminado es 0,1 y generalmente proporciona buenos resultados. Sin embargo, para la mayoría de problemas, valores entre 0,05 y 0,2 funcionan muy bien.</li>
<li>Determinar el número óptimo de árboles para la tasa de aprendizaje elegida.</li>
<li>Ajustar los hiperparámetros del árbol y la tasa de aprendizaje y evaluar el rendimiento del modelo <em>vs.</em> la velocidad de aprendizaje.</li>
<li>Ajustar los hiperparámetros específicos del árbol para determinar la tasa de aprendizaje.</li>
<li>Una vez que se ajustan los parámetros específicos del árbol, se reduce la tasa de aprendizaje para evaluar cualquier mejora en la precisión.</li>
<li>Utilizar la configuración final de hiperparámetros y aumentar el número de grupos en el proceso de validación cruzada para obtener estimaciones más robustas. Si se utiliza validación cruzada en los pasos anteriores, entonces este paso no es necesario.</li>
</ul>
</div>
<div id="procedimiento-con-r-la-función-gbm" class="section level3" number="29.3.3">
<h3>
<span class="header-section-number">29.3.3</span> Procedimiento con <strong>R</strong>: la función <code>gbm()</code><a class="anchor" aria-label="anchor" href="#procedimiento-con-r-la-funci%C3%B3n-gbm"><i class="fas fa-link"></i></a>
</h3>
<p>La función <code>gbm()</code> incluida en el paquete con el mismo nombre <code>gbm()</code> de <strong>R</strong> es la que se utiliza en esta sección para entrenar un modelo <em>gradient boosting</em>:</p>
<div class="sourceCode" id="cb417"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">gbm</span><span class="op">(</span><span class="va">formula</span>,data<span class="op">=</span><span class="va">...</span>, <span class="va">...</span><span class="op">)</span></span></code></pre></div>
<ul>
<li><p><code>formula</code>: indica cuál es la variable dependiente y cuáles son las variables predictoras: <span class="math inline">\(Y \sim X_1 + ... + X_p\)</span>; los signos “+” indican un conjunto de variables predictoras y no una relación lineal con la variable dependiente (que puede o no serlo).</p></li>
<li><p><code>data</code>: conjunto de datos con el que entrenar el árbol de acuerdo a la fórmula indicada.</p></li>
</ul>
</div>
<div id="aplicación-del-modelo-gradient-boosting-en-r" class="section level3" number="29.3.4">
<h3>
<span class="header-section-number">29.3.4</span> Aplicación del modelo <em>gradient boosting</em> en <strong>R</strong><a class="anchor" aria-label="anchor" href="#aplicaci%C3%B3n-del-modelo-gradient-boosting-en-r"><i class="fas fa-link"></i></a>
</h3>
<p>Igual que en todos estos capítulos dedicados al <em>machine learning</em> supervisado, la implementación en <strong>R</strong> del
modelo <em>gradient boosting</em> se lleva a cabo a partir de los datos de compras <code>dp_entr</code>, incluidos en el paquete <code>CDR</code>, con los cuales se trata de clasificar a los clientes de una empresa en compradores o no de un nuevo producto de la empresa: un <em>tensiómetro digital</em>. El modelo se entrena con el conjunto de datos de entrenamiento sin transformar (en su escala original). Así, en lugar de utilizar las variables categóricas transformadas mediante <em>one-hot-encoding</em>,<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Estrategia de codificación consistente en crear una columna binaria (que solo puede contener los valores 0 o 1) para cada valor único o categoría que exista en la variable categórica objeto de codificación.&lt;/p&gt;"><sup>201</sup></a> se usan en su escala original, como ocurre con el caso de la variable que mide el nivel educativo. En cuanto a los hiperparámetros, el método <code>gbm</code> (<em>gradient boosting machine</em>) ha elegido los valores 1, 2 y 3 para la profundidad del árbol (<code>interaction.depth</code>) y 50, 100 y 150 para el número de árboles (<code>ntrees</code>); los valores del número mínimo de observaciones en los nodos terminales (<code>nminobsmode</code>) y de la tasa de aprendizaje (<code>shrinkage</code>), toman los valores establecidos por defecto: 10 y 0,1, respectivamente.</p>
<div class="sourceCode" id="cb418"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"CDR"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/topepo/caret/">"caret"</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/gbm-developers/gbm">"gbm"</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="http://had.co.nz/reshape">"reshape"</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://ggplot2.tidyverse.org">"ggplot2"</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">dp_entr</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb419"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># se determina la semilla aleatoria</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">101</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># se entrena el modelo</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/train.html">train</a></span><span class="op">(</span><span class="va">CLS_PRO_pro13</span> <span class="op">~</span> <span class="va">.</span>, </span>
<span>            data<span class="op">=</span><span class="va">dp_entr</span>, </span>
<span>            method<span class="op">=</span><span class="st">"gbm"</span>, </span>
<span>            metric<span class="op">=</span><span class="st">"Accuracy"</span>,</span>
<span>            trControl <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/trainControl.html">trainControl</a></span><span class="op">(</span>classProbs <span class="op">=</span> <span class="cn">TRUE</span>, </span>
<span>                                     method <span class="op">=</span> <span class="st">"cv"</span>, number <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span>            <span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb420"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb420-1"><a href="cap-boosting-xgboost.html#cb420-1" tabindex="-1"></a><span class="co"># se muestra la salida del modelo</span></span>
<span id="cb420-2"><a href="cap-boosting-xgboost.html#cb420-2" tabindex="-1"></a>model</span>
<span id="cb420-3"><a href="cap-boosting-xgboost.html#cb420-3" tabindex="-1"></a></span>
<span id="cb420-4"><a href="cap-boosting-xgboost.html#cb420-4" tabindex="-1"></a>Stochastic Gradient Boosting </span>
<span id="cb420-5"><a href="cap-boosting-xgboost.html#cb420-5" tabindex="-1"></a></span>
<span id="cb420-6"><a href="cap-boosting-xgboost.html#cb420-6" tabindex="-1"></a><span class="dv">558</span> samples</span>
<span id="cb420-7"><a href="cap-boosting-xgboost.html#cb420-7" tabindex="-1"></a> <span class="dv">17</span> predictor</span>
<span id="cb420-8"><a href="cap-boosting-xgboost.html#cb420-8" tabindex="-1"></a>  <span class="dv">2</span> classes<span class="sc">:</span> <span class="st">'S'</span>, <span class="st">'N'</span> </span>
<span id="cb420-9"><a href="cap-boosting-xgboost.html#cb420-9" tabindex="-1"></a></span>
<span id="cb420-10"><a href="cap-boosting-xgboost.html#cb420-10" tabindex="-1"></a>No pre<span class="sc">-</span>processing</span>
<span id="cb420-11"><a href="cap-boosting-xgboost.html#cb420-11" tabindex="-1"></a>Resampling<span class="sc">:</span> Cross<span class="sc">-</span><span class="fu">Validated</span> (<span class="dv">10</span> fold) </span>
<span id="cb420-12"><a href="cap-boosting-xgboost.html#cb420-12" tabindex="-1"></a>Summary of sample sizes<span class="sc">:</span> <span class="dv">502</span>, <span class="dv">502</span>, <span class="dv">502</span>, <span class="dv">503</span>, <span class="dv">503</span>, <span class="dv">502</span>, ... </span>
<span id="cb420-13"><a href="cap-boosting-xgboost.html#cb420-13" tabindex="-1"></a>Resampling results across tuning parameters<span class="sc">:</span></span>
<span id="cb420-14"><a href="cap-boosting-xgboost.html#cb420-14" tabindex="-1"></a></span>
<span id="cb420-15"><a href="cap-boosting-xgboost.html#cb420-15" tabindex="-1"></a>  interaction.depth  n.trees  Accuracy   Kappa    </span>
<span id="cb420-16"><a href="cap-boosting-xgboost.html#cb420-16" tabindex="-1"></a>  <span class="dv">1</span>                   <span class="dv">50</span>      <span class="fl">0.8564610</span>  <span class="fl">0.7130031</span></span>
<span id="cb420-17"><a href="cap-boosting-xgboost.html#cb420-17" tabindex="-1"></a>  <span class="dv">1</span>                  <span class="dv">100</span>      <span class="fl">0.8690909</span>  <span class="fl">0.7383556</span></span>
<span id="cb420-18"><a href="cap-boosting-xgboost.html#cb420-18" tabindex="-1"></a>  <span class="dv">1</span>                  <span class="dv">150</span>      <span class="fl">0.8762338</span>  <span class="fl">0.7526413</span></span>
<span id="cb420-19"><a href="cap-boosting-xgboost.html#cb420-19" tabindex="-1"></a>  <span class="dv">2</span>                   <span class="dv">50</span>      <span class="fl">0.8690909</span>  <span class="fl">0.7382344</span></span>
<span id="cb420-20"><a href="cap-boosting-xgboost.html#cb420-20" tabindex="-1"></a>  <span class="dv">2</span>                  <span class="dv">100</span>      <span class="fl">0.8762338</span>  <span class="fl">0.7526413</span></span>
<span id="cb420-21"><a href="cap-boosting-xgboost.html#cb420-21" tabindex="-1"></a>  <span class="dv">2</span>                  <span class="dv">150</span>      <span class="fl">0.8799026</span>  <span class="fl">0.7599227</span></span>
<span id="cb420-22"><a href="cap-boosting-xgboost.html#cb420-22" tabindex="-1"></a>  <span class="dv">3</span>                   <span class="dv">50</span>      <span class="fl">0.8763636</span>  <span class="fl">0.7528004</span></span>
<span id="cb420-23"><a href="cap-boosting-xgboost.html#cb420-23" tabindex="-1"></a>  <span class="dv">3</span>                  <span class="dv">100</span>      <span class="fl">0.8781494</span>  <span class="fl">0.7563575</span></span>
<span id="cb420-24"><a href="cap-boosting-xgboost.html#cb420-24" tabindex="-1"></a>  <span class="dv">3</span>                  <span class="dv">150</span>      <span class="fl">0.8835390</span>  <span class="fl">0.7671499</span></span>
<span id="cb420-25"><a href="cap-boosting-xgboost.html#cb420-25" tabindex="-1"></a></span>
<span id="cb420-26"><a href="cap-boosting-xgboost.html#cb420-26" tabindex="-1"></a>Tuning parameter <span class="st">'shrinkage'</span> was held constant at a value of <span class="fl">0.1</span></span>
<span id="cb420-27"><a href="cap-boosting-xgboost.html#cb420-27" tabindex="-1"></a>Tuning</span>
<span id="cb420-28"><a href="cap-boosting-xgboost.html#cb420-28" tabindex="-1"></a> parameter <span class="st">'n.minobsinnode'</span> was held constant at a value of <span class="dv">10</span></span>
<span id="cb420-29"><a href="cap-boosting-xgboost.html#cb420-29" tabindex="-1"></a>Accuracy was used to select the optimal model using the largest value.</span>
<span id="cb420-30"><a href="cap-boosting-xgboost.html#cb420-30" tabindex="-1"></a>The final values used <span class="cf">for</span> the model were n.trees <span class="ot">=</span> <span class="dv">150</span>, interaction.depth <span class="ot">=</span></span>
<span id="cb420-31"><a href="cap-boosting-xgboost.html#cb420-31" tabindex="-1"></a> <span class="dv">3</span>, shrinkage <span class="ot">=</span> <span class="fl">0.1</span> and n.minobsinnode <span class="ot">=</span> <span class="fl">10.</span></span></code></pre></div>
<p>El modelo resultante del proceso de entrenamiento es un <em>gradient boosting</em> que ha ajustado los hiperparámetros a 150 árboles y una profundidad igual a 3. Además, como se avanzó, los valores del número mínimo de observaciones en los nodos terminales y de la tasa de aprendizaje fueron los valores establecidos por defecto: 10 y 0,1, respectivamente. Los resultados en el proceso de validación cruzada se muestran en la Fig. <a href="cap-boosting-xgboost.html#fig:GBMBOXPLOT">29.2</a>, en la que se observa cómo la precisión (<code>accuracy</code>) oscila entre el 84% y el 93% en las iteraciones.</p>
<div class="sourceCode" id="cb421"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/reshape/man/melt-24.html">melt</a></span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">resample</span><span class="op">[</span>,<span class="op">-</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">variable</span>, y <span class="op">=</span> <span class="va">value</span>, fill<span class="op">=</span><span class="va">variable</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_boxplot.html">geom_boxplot</a></span><span class="op">(</span>show.legend<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:GBMBOXPLOT"></span>
<img src="img/boxplotgbm.png" alt="Resultados del modelo GB obtenidos durante el proceso de validación cruzada." width="60%"><p class="caption">
Figura 29.2: Resultados del modelo GB obtenidos durante el proceso de validación cruzada.
</p>
</div>
</div>
<div id="gradient-boosting-con-ajuste-automático" class="section level3" number="29.3.5">
<h3>
<span class="header-section-number">29.3.5</span> <em>Gradient Boosting</em> con ajuste automático<a class="anchor" aria-label="anchor" href="#gradient-boosting-con-ajuste-autom%C3%A1tico"><i class="fas fa-link"></i></a>
</h3>
<p>En la sección precedente los valores de los hiperparámetros se tomaron por defecto (número mínimo de observaciones en los nodos terminales y de la tasa de aprendizaje) o fueron elegidos entre los que la función <code>gbm()</code> establece por defecto (la profundidad del árbol y el número de árboles). En esta sección, los cuatro hiperparámetros del método <code>gbm</code> anteriormente referidos se ajustan de manera automática entre los valores establecidos por el investigador: </p>
<div class="sourceCode" id="cb422"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb422-1"><a href="cap-boosting-xgboost.html#cb422-1" tabindex="-1"></a><span class="fu">modelLookup</span>(<span class="st">"gbm"</span>)</span>
<span id="cb422-2"><a href="cap-boosting-xgboost.html#cb422-2" tabindex="-1"></a>model         parameter                   label forReg forClass probModel</span>
<span id="cb422-3"><a href="cap-boosting-xgboost.html#cb422-3" tabindex="-1"></a><span class="dv">1</span>   gbm           n.trees   <span class="co"># Boosting Iterations   TRUE     TRUE      TRUE</span></span>
<span id="cb422-4"><a href="cap-boosting-xgboost.html#cb422-4" tabindex="-1"></a><span class="dv">2</span>   gbm interaction.depth          Max Tree Depth   <span class="cn">TRUE</span>     <span class="cn">TRUE</span>      <span class="cn">TRUE</span></span>
<span id="cb422-5"><a href="cap-boosting-xgboost.html#cb422-5" tabindex="-1"></a><span class="dv">3</span>   gbm         shrinkage               Shrinkage   <span class="cn">TRUE</span>     <span class="cn">TRUE</span>      <span class="cn">TRUE</span></span>
<span id="cb422-6"><a href="cap-boosting-xgboost.html#cb422-6" tabindex="-1"></a><span class="dv">4</span>   gbm    n.minobsinnode Min. Terminal Node Size   <span class="cn">TRUE</span>     <span class="cn">TRUE</span>      <span class="cn">TRUE</span></span></code></pre></div>
<p>Siguiendo la estrategia descrita anteriormente, los rangos de posibles valores para los hiperparámetros a optimizar son los siguientes:</p>
<div class="sourceCode" id="cb423"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Se especifica un rango de valores posibles para los hiperparámetros</span></span>
<span><span class="va">tuneGrid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/expand.grid.html">expand.grid</a></span><span class="op">(</span>interaction.depth <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span>,<span class="fl">6</span>,<span class="fl">8</span><span class="op">)</span>,</span>
<span>                        n.trees <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">10</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">ncol</a></span><span class="op">(</span><span class="va">dp_entr</span><span class="op">)</span>,<span class="fl">300</span>,<span class="fl">500</span><span class="op">)</span>, </span>
<span>                        shrinkage <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.05</span>,<span class="fl">0.1</span>,<span class="fl">0.2</span><span class="op">)</span>, </span>
<span>                        n.minobsinnode <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">5</span>,<span class="fl">10</span>,<span class="fl">15</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Esta red de posibles valores para los hiperparámetros del modelo se incorpora a la función de entrenamiento. Cuanto más exhaustiva sea la red, mayor será el tiempo de ajuste del modelo. La red presentada está formada por las 81 combinaciones posibles que se pueden llevar a cabo con los valores establecidos para los hiperparámetros.</p>
<div class="sourceCode" id="cb424"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># se fija la semilla aleatoria</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">101</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># se entrena el modelo</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/train.html">train</a></span><span class="op">(</span><span class="va">CLS_PRO_pro13</span><span class="op">~</span><span class="va">.</span>, </span>
<span>            data<span class="op">=</span><span class="va">dp_entr</span>, </span>
<span>            method<span class="op">=</span><span class="st">"gbm"</span>, </span>
<span>            metric<span class="op">=</span><span class="st">"Accuracy"</span>,</span>
<span>            trControl<span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/caret/man/trainControl.html">trainControl</a></span><span class="op">(</span>classProbs <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                                   method<span class="op">=</span><span class="st">"cv"</span>, number<span class="op">=</span><span class="fl">10</span><span class="op">)</span>,</span>
<span>            tuneGrid<span class="op">=</span><span class="va">tuneGrid</span><span class="op">)</span></span></code></pre></div>
<p>El modelo que mejores resultados proporciona es aquel que ajusta los hiperparámetros a los siguientes valores: 180 árboles, una profundidad igual a 6, una tasa de aprendizaje de 0,05 y un número mínimo de 10 observaciones en los nodos terminales.</p>
<div class="sourceCode" id="cb425"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb425-1"><a href="cap-boosting-xgboost.html#cb425-1" tabindex="-1"></a><span class="co"># se muestra la salida del modelo</span></span>
<span id="cb425-2"><a href="cap-boosting-xgboost.html#cb425-2" tabindex="-1"></a>model<span class="sc">$</span>bestTune</span>
<span id="cb425-3"><a href="cap-boosting-xgboost.html#cb425-3" tabindex="-1"></a>   n.trees interaction.depth shrinkage n.minobsinnode</span>
<span id="cb425-4"><a href="cap-boosting-xgboost.html#cb425-4" tabindex="-1"></a><span class="dv">13</span>     <span class="dv">180</span>                 <span class="dv">6</span>      <span class="fl">0.05</span>             <span class="dv">10</span></span></code></pre></div>
<p>En la Fig. <a href="cap-boosting-xgboost.html#fig:modelgbmboxplot">29.3</a> se muestran los resultados obtenidos durante el proceso de validación cruzada. Se puede ver que los resultados son similares a los del modelo anterior, aunque hay diferencias importantes. En primer lugar, se alcanza un valor máximo de precisión superior al del modelo <code>gbm</code> sin ajuste automático anterior, pues en este caso la precisión oscila entre el 84 % y el 95%. En segundo lugar, el valor mediano de la precisión ha aumentado desde el 87,5 % hasta el 90 %. Por último, nótese que la automatización del proceso de ajuste de los hiperparámetros no ha llevado a una gran mejoría en el rendimiento del modelo, lo que confirma lo ya expuesto sobre el buen rendimiento del modelo de <em>gradient boosting</em> que surge de entrenar el modelo con los valores establecidos por defecto para los hiperparámetros.</p>
<div class="sourceCode" id="cb426"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/reshape/man/melt-24.html">melt</a></span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">resample</span><span class="op">[</span>,<span class="op">-</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">variable</span>, y <span class="op">=</span> <span class="va">value</span>, fill<span class="op">=</span><span class="va">variable</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_boxplot.html">geom_boxplot</a></span><span class="op">(</span>show.legend<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:modelgbmboxplot"></span>
<img src="img/boxplottunedgbm.png" alt="Resultados del modelo GB con ajuste automático obtenidos durante el proceso de validación cruzada." width="60%"><p class="caption">
Figura 29.3: Resultados del modelo GB con ajuste automático obtenidos durante el proceso de validación cruzada.
</p>
</div>
</div>
</div>
<div id="bftextitextreme-bftextitgradient-bftextitboosting" class="section level2" number="29.4">
<h2>
<span class="header-section-number">29.4</span> <span class="math inline">\(\bf\textit{eXtreme}\)</span> <span class="math inline">\(\bf\textit{gradient}\)</span> <span class="math inline">\(\bf\textit{boosting}\)</span><a class="anchor" aria-label="anchor" href="#bftextitextreme-bftextitgradient-bftextitboosting"><i class="fas fa-link"></i></a>
</h2>
<p>El eXtreme Gradient Boosting es una implementación eficiente y escalable del modelo <em>gradient boosting</em>. Este modelo, abreviado como XGBoost, es un paquete de código abierto en C++, Java, Python <span class="citation">(<a href="referencias.html#ref-wade2020hands">Wade, 2020</a>)</span>, <strong>R</strong>, Julia, Perl y Scala. En <strong>R</strong>, el modelo se incluye dentro del paquete <code>xgboost</code> <span class="citation">(<a href="referencias.html#ref-chen2015xgboost">Chen et al., 2015</a>)</span>. El paquete incluye un procedimiento para la solución eficiente de modelos lineales y un algoritmo de aprendizaje de árboles.</p>
<p><code>Xgboost</code> es compatible con funciones objetivo de regresión, clasificación y ranking. Además, tiene varias características importantes:</p>
<ol style="list-style-type: decimal">
<li><p>Velocidad: <code>xgboost</code> puede realizar automáticamente cálculos paralelos. Por lo general, es 10 veces más rápido que el modelo <em>gradient boosting</em>.</p></li>
<li><p>Tipo de entrada: <code>xgboost</code> toma varios tipos de datos de entrada en <strong>R</strong>:</p></li>
</ol>
<ul>
<li>Matriz densa (<code>matrix</code>).</li>
<li>Matriz dispersa (<code>Matrix::dgCMatrix</code>).</li>
<li>Archivo de datos locales.</li>
<li>Un tipo de datos propio del paquete: <code>xgb.DMatrix</code>.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li><p>Dispersión: <code>xgboost</code> acepta datos de entrada dispersos para los modelos incluidos.</p></li>
<li><p>Personalización: <code>xgboost</code> admite tanto funciones de objetivo y funciones de evaluación personalizadas.</p></li>
<li><p>Rendimiento: <code>xgboost</code> alcanza generalmente una mayor precisión que <em>gradient boosting</em>.</p></li>
</ol>
<div id="hiperparámetros-del-modelo-xgboost" class="section level3" number="29.4.1">
<h3>
<span class="header-section-number">29.4.1</span> Hiperparámetros del modelo XGBoost<a class="anchor" aria-label="anchor" href="#hiperpar%C3%A1metros-del-modelo-xgboost"><i class="fas fa-link"></i></a>
</h3>
<p><code>Xgboost</code> proporciona no solo los hiperparámetros del <em>gradient boosting</em>, sino también otros adicionales que pueden servir de ayuda a la hora de reducir las posibilidades de sobreajuste, lo que lleva a una menor varianza de predicción y, por lo tanto, a una mayor precisión. Estos hiperparámetros son los de regularización y el de <em>dropout</em> (descarte).</p>
<p>Los parámetros de regularización se incluyen para ayudar a evitar el sobreajuste y reducir la complejidad del modelo. Existen tres hiperparámetros que tienen esta función: gamma (<span class="math inline">\(\gamma\)</span>), alpha (<span class="math inline">\(\alpha\)</span>) y lambda (<span class="math inline">\(\lambda\)</span>). El hiperparámetro <span class="math inline">\(\gamma\)</span> es un hiperparámetro de pseudorregularización, conocido como multiplicador Lagrangiano, tal que para hacer una partición adicional en un nodo es necesaria que la reducción en la pérdida (medida por SSE o similar) sea superior a la especificada por <code>gamma</code>. Es un hiperparámetro de poda; así, el modelo XGBoost hace crecer los árboles hasta una profundidad máxima establecida, pero en el proceso de poda eliminará las divisiones que no cumplan con la regularización <span class="math inline">\(\gamma\)</span>. Este hiperparámetro toma valores entre 0 e infinito (<span class="math inline">\(\infty\)</span>), siguiendo la regla de que cuanto mayor sea su valor, mayor será la regularización. Los otros hiperparámetros de regularización, <span class="math inline">\(\alpha\)</span> y <span class="math inline">\(\lambda\)</span>, son más clásicos. Mientras que <span class="math inline">\(\alpha\)</span> proporciona una regularización <span class="math inline">\(L_1\)</span>, <span class="math inline">\(\lambda\)</span> proporciona una regularización <span class="math inline">\(L_2\)</span>. Estos parámetros de regularización establecen un límite a a la magnitud de los pesos de los nodos en un árbol. Sus valores se encuentran, al igual que los de <span class="math inline">\(\gamma\)</span>, entre 0 y <span class="math inline">\(\infty\)</span>. Tanto <span class="math inline">\(\alpha\)</span> como <span class="math inline">\(\lambda\)</span> hacen que el modelo sea más conservador al aumentar su valor.</p>
<p>El <em>dropout</em> es un enfoque alternativo para reducir el sobreajuste. Cuando se entrena un modelo de <em>gradient boosting</em>, los primeros árboles tienden a dominar el rendimiento del modelo, mientras que los que se agregan después suelen mejorar la predicción solo para un pequeño grupo de variables. Esto puede llevar a que se incremente el riesgo de sobreajuste. Con el <em>dropout</em>, se descartan árboles aleatoriamente en el proceso de entrenamiento.</p>
<p>En su implementación en <strong>R</strong>, el modelo XGBoost incluye principalmente los siguientes hiperparámetros: número de iteraciones, profundidad máxima de los árboles, tasa de aprendizaje y regularización <span class="math inline">\(\gamma\)</span>.</p>
<div class="sourceCode" id="cb427"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb427-1"><a href="cap-boosting-xgboost.html#cb427-1" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">modelLookup</span>(<span class="st">"xgbTree"</span>),<span class="dv">4</span>)</span>
<span id="cb427-2"><a href="cap-boosting-xgboost.html#cb427-2" tabindex="-1"></a>    model parameter                  label forReg forClass probModel</span>
<span id="cb427-3"><a href="cap-boosting-xgboost.html#cb427-3" tabindex="-1"></a><span class="dv">1</span> xgbTree   nrounds  <span class="co"># Boosting Iterations   TRUE     TRUE      TRUE</span></span>
<span id="cb427-4"><a href="cap-boosting-xgboost.html#cb427-4" tabindex="-1"></a><span class="dv">2</span> xgbTree max_depth         Max Tree Depth   <span class="cn">TRUE</span>     <span class="cn">TRUE</span>      <span class="cn">TRUE</span></span>
<span id="cb427-5"><a href="cap-boosting-xgboost.html#cb427-5" tabindex="-1"></a><span class="dv">3</span> xgbTree       eta              Shrinkage   <span class="cn">TRUE</span>     <span class="cn">TRUE</span>      <span class="cn">TRUE</span></span>
<span id="cb427-6"><a href="cap-boosting-xgboost.html#cb427-6" tabindex="-1"></a><span class="dv">4</span> xgbTree     gamma Minimum Loss Reduction   <span class="cn">TRUE</span>     <span class="cn">TRUE</span>      <span class="cn">TRUE</span></span></code></pre></div>
</div>
<div id="procedimiento-con-r-la-función-xgboost" class="section level3" number="29.4.2">
<h3>
<span class="header-section-number">29.4.2</span> Procedimiento con R: la función <code>xgboost()</code><a class="anchor" aria-label="anchor" href="#procedimiento-con-r-la-funci%C3%B3n-xgboost"><i class="fas fa-link"></i></a>
</h3>
<p>La función <code>xgboost()</code> del paquete <code>xgboost</code> de <strong>R</strong> se utiliza en esta sección para entrenar un modelo <em>extreme gradient boosting</em>:</p>
<div class="sourceCode" id="cb428"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">xgboost</span><span class="op">(</span>data <span class="op">=</span> <span class="va">...</span>, label <span class="op">=</span> <span class="va">...</span>, <span class="va">...</span><span class="op">)</span></span></code></pre></div>
<ul>
<li>
<code>data</code>: conjunto de datos con el que entrenar el modelo.</li>
<li>
<code>label</code>: vector con la variable respuesta.</li>
</ul>
</div>
<div id="aplicación-del-modelo-xgboost-en-r" class="section level3" number="29.4.3">
<h3>
<span class="header-section-number">29.4.3</span> Aplicación del modelo XGBoost en <strong>R</strong><a class="anchor" aria-label="anchor" href="#aplicaci%C3%B3n-del-modelo-xgboost-en-r"><i class="fas fa-link"></i></a>
</h3>
<p>Este modelo se entrena utilizando el conjunto de entrenamiento sin transformar (en su escala original). Para ilustrar su implementación en <strong>R</strong>, sin y con ajuste automatizado de los hiperparámetros, se retoma el ejemplo utilizado en la implementación del modelo <em>gradient boosting</em>.</p>
<p>Se considera primeramente el entrenamiento del modelo con los valores que asigna por defecto la función <code>xgboost()</code> a los hiperparámetros: para la regularización <span class="math inline">\(\gamma=1\)</span> (0) y para el tamaño mínimo del nodo (<code>n.minobsinnode</code>= 1). Los demás hiperparámetros sí se ajustan.</p>
<div class="sourceCode" id="cb429"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># se determina la semilla aleatoria</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">101</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># se entrena el modelo</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/train.html">train</a></span><span class="op">(</span><span class="va">CLS_PRO_pro13</span><span class="op">~</span><span class="va">.</span>,</span>
<span>               data<span class="op">=</span><span class="va">dp_entr</span>,</span>
<span>               method<span class="op">=</span><span class="st">"xgbTree"</span>,</span>
<span>               metric<span class="op">=</span><span class="st">"Accuracy"</span>,</span>
<span>               trControl<span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/caret/man/trainControl.html">trainControl</a></span><span class="op">(</span>classProbs <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                                      method <span class="op">=</span> <span class="st">"cv"</span>,</span>
<span>                                      number<span class="op">=</span><span class="fl">10</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>El modelo XGBoost resultante tiene 50 iteraciones, una profundidad máxima igual a 2 y una tasa de aprendizaje de 0,3. Los resultados de la validación cruzada (Fig. <a href="cap-boosting-xgboost.html#fig:XGBRESULTS">29.4</a>) muestran que la precisión obtenida oscila entre el 85% y el 95%, resultado similar al del <em>gradient boosting</em> con hiperparámetros ajustados. Sin embargo, el valor mediano de la precisión es del 88%, ligeramente inferior al observado en el modelo <em>gradient boosting</em> con ajuste automático.</p>
<div class="sourceCode" id="cb430"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/reshape/man/melt-24.html">melt</a></span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">resample</span><span class="op">[</span>,<span class="op">-</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">variable</span>, y <span class="op">=</span> <span class="va">value</span>, fill<span class="op">=</span><span class="va">variable</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_boxplot.html">geom_boxplot</a></span><span class="op">(</span>show.legend<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:XGBRESULTS"></span>
<img src="img/boxplotxgbm.png" alt="Resultados del modelo durante la validación cruzada." width="60%"><p class="caption">
Figura 29.4: Resultados del modelo durante la validación cruzada.
</p>
</div>
</div>
<div id="xgboost-con-ajuste-automático" class="section level3" number="29.4.4">
<h3>
<span class="header-section-number">29.4.4</span> XGBoost con ajuste automático<a class="anchor" aria-label="anchor" href="#xgboost-con-ajuste-autom%C3%A1tico"><i class="fas fa-link"></i></a>
</h3>
<p>En esta sección los hiperparámetros más relevantes del modelo se ajustan automáticamente. Para ello se genera una red de posibles valores para tales hiperparámetros. Por motivos computacionales, esta no se hace excesivamente exhaustiva (para evitar largos tiempos de entrenamiento). Sin embargo, si se dispone de tiempo suficiente para el entrenamiento, es aconsejable ampliar dicha red.</p>
<div class="sourceCode" id="cb431"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Se especifica un rango de valores típicos para los hiperparámetros</span></span>
<span><span class="va">tuneGrid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/expand.grid.html">expand.grid</a></span> <span class="op">(</span>nrounds<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">50</span>,<span class="fl">100</span>,<span class="fl">500</span><span class="op">)</span>, </span>
<span>                         max_depth <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">4</span>,<span class="fl">8</span><span class="op">)</span>,</span>
<span>                         eta <span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.05</span>,<span class="fl">0.1</span>,<span class="fl">0.2</span>,<span class="fl">0.3</span><span class="op">)</span>,</span>
<span>                         gamma<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">0.5</span>,<span class="fl">5</span><span class="op">)</span>,</span>
<span>                         colsample_bytree<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.8</span><span class="op">)</span>,</span>
<span>                         min_child_weight<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span>,</span>
<span>                         subsample<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.5</span><span class="op">)</span><span class="op">)</span> </span></code></pre></div>
<div class="sourceCode" id="cb432"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># se determina la semilla aleatoria</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">101</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># se entrena el modelo</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/train.html">train</a></span><span class="op">(</span><span class="va">CLS_PRO_pro13</span><span class="op">~</span><span class="va">.</span>, </span>
<span>               data<span class="op">=</span><span class="va">dp_entr</span>, </span>
<span>               method<span class="op">=</span><span class="st">"xgbTree"</span>, </span>
<span>               metric<span class="op">=</span><span class="st">"Accuracy"</span>,</span>
<span>               trControl<span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/caret/man/trainControl.html">trainControl</a></span><span class="op">(</span>classProbs <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                                      method <span class="op">=</span> <span class="st">"cv"</span>,</span>
<span>                                      number <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>,</span>
<span>               tuneGrid<span class="op">=</span><span class="va">tuneGrid</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb433"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb433-1"><a href="cap-boosting-xgboost.html#cb433-1" tabindex="-1"></a><span class="co"># se muestra la salida del modelo</span></span>
<span id="cb433-2"><a href="cap-boosting-xgboost.html#cb433-2" tabindex="-1"></a>model<span class="sc">$</span>bestTune[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>]</span>
<span id="cb433-3"><a href="cap-boosting-xgboost.html#cb433-3" tabindex="-1"></a>   nrounds max_depth eta gamma</span>
<span id="cb433-4"><a href="cap-boosting-xgboost.html#cb433-4" tabindex="-1"></a><span class="dv">71</span>     <span class="dv">100</span>         <span class="dv">4</span> <span class="fl">0.2</span>     <span class="dv">5</span></span></code></pre></div>
<p>Como puede verse en los resultados obtenidos, el modelo resultante establece que se utilicen 100 iteraciones, que los árboles tengan una profundidad máxima de 4, que la tasa de aprendizaje sea 0,2 y que la regularización <span class="math inline">\(\gamma\)</span> tome el valor 5.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;En los resultados aparece el hiperparámento &lt;code&gt;colsample_bytree&lt;/code&gt;, que representa la fracción de columnas que se muestrean aleatoriamente en cada árbol, pues ello podría reducir el sobreajuste. Su campo de variación es [0, 1] y su valor predeterminado es 1. El hiperparámetro &lt;code&gt;eta&lt;/code&gt;(&lt;code&gt;shrinkage&lt;/code&gt;) es la tasa de aprendizaje.&lt;/p&gt;"><sup>202</sup></a></p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:xgb-tuned-RESULTS"></span>
<img src="img/boxplottunedxgbm.png" alt="Resultados del modelo durante la validación cruzada." width="50%"><p class="caption">
Figura 29.5: Resultados del modelo durante la validación cruzada.
</p>
</div>
<p>Los resultados obtenidos durante la validación cruzada (Fig. <a href="cap-boosting-xgboost.html#fig:xgb-tuned-RESULTS">29.5</a>) muestran que la precisión es muy similar a la del modelo de la sección anterior (entre el 85% y el 95%); sin embargo, el valor mediano de la precisión aumenta hasta el 90%.</p>
</div>
<div id="resumen-28" class="section level3 unnumbered infobox_resume">
<h3>Resumen<a class="anchor" aria-label="anchor" href="#resumen-28"><i class="fas fa-link"></i></a>
</h3>
<p>En este capítulo se introduce al lector en el algoritmo de aprendizaje supervisado conocido como <em>gradient boosting</em>. En concreto:</p>
<ul>
<li><p>Se presenta el segundo de los paradigmas de aprendizaje ensamblado: el <em>boosting</em>.</p></li>
<li><p>Se explica el modelo basado en este paradigma, el <em>gradient boosting</em>, así como sus diferencias con el <em>bagging</em> y el <em>random forest</em>.</p></li>
<li><p>Se exponen los hiperparámetros más relevantes a la hora de optimizar un modelo de <em>gradient boosting</em>.</p></li>
<li><p>Se presenta el <em>eXtreme gradient boosting</em>, una implementación eficiente y escalable del modelo <em>gradient boosting</em>, y se comentan los hiperparámetros de regularización y otros parámetros importantes en esta implementación.</p></li>
<li><p>Se aplican ambos algoritmos en <strong>R</strong> a un caso práctico de clasificación binaria de datos.</p></li>
</ul>
</div>

</div>
</div>




  <div class="chapter-nav">
<div class="prev"><a href="cap-bagg-rf.html"><span class="header-section-number">28</span> Métodos ensamblados: \(\bf \textit {bagging}\) y \(\bf \textit{random}\) \(\bf \textit{forest}\)</a></div>
<div class="next"><a href="cap-cluster.html"><span class="header-section-number">30</span> Análisis clúster: clusterización jerárquica</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="Índice del capítulo"><h2>Índice del capítulo</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#cap-boosting-xgboost"><span class="header-section-number">29</span> \(\bf \textit{Boosting}\) y el algoritmo XGBoost</a></li>
<li><a class="nav-link" href="#m%C3%A9todos-ensamblados-bagging-vs.-boosting"><span class="header-section-number">29.1</span> Métodos ensamblados: bagging vs. boosting</a></li>
<li><a class="nav-link" href="#qu%C3%A9-es-el-boosting"><span class="header-section-number">29.2</span> ¿Qué es el boosting?</a></li>
<li>
<a class="nav-link" href="#bf-textitgradient-boosting"><span class="header-section-number">29.3</span> \(\bf \textit{Gradient boosting}\)</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#hiperpar%C3%A1metros-del-modelo-gradient-boosting"><span class="header-section-number">29.3.1</span> Hiperparámetros del modelo gradient boosting</a></li>
<li><a class="nav-link" href="#estrategia-de-ajuste-de-hiperpar%C3%A1metros"><span class="header-section-number">29.3.2</span> Estrategia de ajuste de hiperparámetros</a></li>
<li><a class="nav-link" href="#procedimiento-con-r-la-funci%C3%B3n-gbm"><span class="header-section-number">29.3.3</span> Procedimiento con R: la función gbm()</a></li>
<li><a class="nav-link" href="#aplicaci%C3%B3n-del-modelo-gradient-boosting-en-r"><span class="header-section-number">29.3.4</span> Aplicación del modelo gradient boosting en R</a></li>
<li><a class="nav-link" href="#gradient-boosting-con-ajuste-autom%C3%A1tico"><span class="header-section-number">29.3.5</span> Gradient Boosting con ajuste automático</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#bftextitextreme-bftextitgradient-bftextitboosting"><span class="header-section-number">29.4</span> \(\bf\textit{eXtreme}\) \(\bf\textit{gradient}\) \(\bf\textit{boosting}\)</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#hiperpar%C3%A1metros-del-modelo-xgboost"><span class="header-section-number">29.4.1</span> Hiperparámetros del modelo XGBoost</a></li>
<li><a class="nav-link" href="#procedimiento-con-r-la-funci%C3%B3n-xgboost"><span class="header-section-number">29.4.2</span> Procedimiento con R: la función xgboost()</a></li>
<li><a class="nav-link" href="#aplicaci%C3%B3n-del-modelo-xgboost-en-r"><span class="header-section-number">29.4.3</span> Aplicación del modelo XGBoost en R</a></li>
<li><a class="nav-link" href="#xgboost-con-ajuste-autom%C3%A1tico"><span class="header-section-number">29.4.4</span> XGBoost con ajuste automático</a></li>
<li><a class="nav-link" href="#resumen-28">Resumen</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Fundamentos de ciencia de datos con <strong>R</strong></strong>" coordinado por <a href="https://blog.uclm.es/gemafaviles/" class="text-light">Gema Fernández-Avilés y José-María Montero</a>. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>Este libro ha sido generado con el paquete de R <a class="text-light" href="https://bookdown.org">bookdown</a>.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
