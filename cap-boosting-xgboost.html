<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Capítulo 29 Boosting y el algoritmo XGBoost | Fundamentos de ciencia de datos con R</title>
<meta name="author" content="Gema Fernández-Avilés y José-María Montero">
<meta name="description" content="Ramón A. Carrasco\(^{a}\) e Itzcóatl Bueno\(^{b,a}\) \(^{a}\)Universidad Complutense de Madrid \(^{b}\)Instituto Nacional de Estadística  29.1 Métodos ensamblados: bagging vs boosting En el Cap....">
<meta name="generator" content="bookdown 0.28 with bs4_book()">
<meta property="og:title" content="Capítulo 29 Boosting y el algoritmo XGBoost | Fundamentos de ciencia de datos con R">
<meta property="og:type" content="book">
<meta property="og:image" content="/img/cover.png">
<meta property="og:description" content="Ramón A. Carrasco\(^{a}\) e Itzcóatl Bueno\(^{b,a}\) \(^{a}\)Universidad Complutense de Madrid \(^{b}\)Instituto Nacional de Estadística  29.1 Métodos ensamblados: bagging vs boosting En el Cap....">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Capítulo 29 Boosting y el algoritmo XGBoost | Fundamentos de ciencia de datos con R">
<meta name="twitter:description" content="Ramón A. Carrasco\(^{a}\) e Itzcóatl Bueno\(^{b,a}\) \(^{a}\)Universidad Complutense de Madrid \(^{b}\)Instituto Nacional de Estadística  29.1 Métodos ensamblados: bagging vs boosting En el Cap....">
<meta name="twitter:image" content="/img/cover.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.2/transition.js"></script><script src="libs/bs3compat-0.4.2/tabs.js"></script><script src="libs/bs3compat-0.4.2/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><link href="libs/tabwid-1.1.0/tabwid.css" rel="stylesheet">
<link href="libs/tabwid-1.1.0/scrool.css" rel="stylesheet">
<script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css">
<link rel="stylesheet" href="bs4_book.css">
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Fundamentos de ciencia de datos con R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Buscar" aria-label="Buscar">
</form>

      <nav aria-label="Contenido"><h2>Contenido</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Prefacio</a></li>
<li class="book-part">Ciencia, datos, software… y científicos</li>
<li><a class="" href="ciencia-datos.html"><span class="header-section-number">1</span> ¿Es la ciencia de datos una ciencia?</a></li>
<li><a class="" href="metodologia.html"><span class="header-section-number">2</span> Metodología en ciencia de datos</a></li>
<li><a class="" href="ch-110003.html"><span class="header-section-number">3</span> R para ciencia de datos</a></li>
<li><a class="" href="cap-etica.html"><span class="header-section-number">4</span> Ética en la ciencia de datos</a></li>
<li class="book-part">Bienvenidos a la jungla de datos</li>
<li><a class="" href="datos-sql.html"><span class="header-section-number">5</span> Gestión de bases de datos relacionales</a></li>
<li><a class="" href="cap-nosql.html"><span class="header-section-number">6</span> Gestión de bases de datos NoSQL</a></li>
<li><a class="" href="DGDQM.html"><span class="header-section-number">7</span> Gobierno, gestión y calidad del dato</a></li>
<li><a class="" href="id_130009.html"><span class="header-section-number">8</span> Integración y limpieza de datos</a></li>
<li><a class="" href="chap-feature.html"><span class="header-section-number">9</span> Selección y transformación de variables</a></li>
<li><a class="" href="chap-herramientas.html"><span class="header-section-number">10</span> Herramientas para el análisis en ciencia de datos</a></li>
<li><a class="" href="id_120006-aed.html"><span class="header-section-number">11</span> Análisis exploratorio de datos</a></li>
<li class="book-part">Fundamentos de estadística</li>
<li><a class="" href="Funda-probab.html"><span class="header-section-number">12</span> Probabilidad</a></li>
<li><a class="" href="Fundainfer.html"><span class="header-section-number">13</span> Inferencia estadística</a></li>
<li><a class="" href="muestreo.html"><span class="header-section-number">14</span> Muestreo y remuestreo</a></li>
<li class="book-part">Modelización estadística</li>
<li><a class="" href="cap-lm.html"><span class="header-section-number">15</span> Modelización lineal</a></li>
<li><a class="" href="cap-glm.html"><span class="header-section-number">16</span> Modelos lineales generalizados</a></li>
<li><a class="" href="cap-gam.html"><span class="header-section-number">17</span> Modelos aditivos generalizados</a></li>
<li><a class="" href="cap-mxm.html"><span class="header-section-number">18</span> Modelos mixtos</a></li>
<li><a class="" href="cap-sparse.html"><span class="header-section-number">19</span> Modelos sparse y métodos penalizados de regresión</a></li>
<li><a class="" href="cap-series-temp.html"><span class="header-section-number">20</span> Modelización de series temporales</a></li>
<li><a class="" href="cap-discriminante.html"><span class="header-section-number">21</span> Análisis discriminante</a></li>
<li><a class="" href="cap-conjunto.html"><span class="header-section-number">22</span> Análisis conjunto</a></li>
<li><a class="" href="tablas-contingencia.html"><span class="header-section-number">23</span> Análisis de tablas de contingencia</a></li>
<li class="book-part">Machine learning supervisado</li>
<li><a class="" href="cap-arboles.html"><span class="header-section-number">24</span> Árboles de clasificación y regresión</a></li>
<li><a class="" href="cap-svm.html"><span class="header-section-number">25</span> Máquinas de vector soporte</a></li>
<li><a class="" href="cap-knn.html"><span class="header-section-number">26</span> Clasificador k-vecinos más próximos</a></li>
<li><a class="" href="cap-naive-bayes.html"><span class="header-section-number">27</span> Naive Bayes</a></li>
<li><a class="" href="cap-bagg-rf.html"><span class="header-section-number">28</span> Métodos ensamblados: bagging y random forest</a></li>
<li><a class="active" href="cap-boosting-xgboost.html"><span class="header-section-number">29</span> Boosting y el algoritmo XGBoost</a></li>
<li class="book-part">Machine learning no supervisado</li>
<li><a class="" href="jerarquico.html"><span class="header-section-number">30</span> Análisis cluster: clusterización jerárquica</a></li>
<li><a class="" href="no-jerarquico.html"><span class="header-section-number">31</span> Análisis cluster: clusterización no jerárquica</a></li>
<li><a class="" href="acp.html"><span class="header-section-number">32</span> Análisis de componentes principales</a></li>
<li><a class="" href="an%C3%A1lisis-factorial.html"><span class="header-section-number">33</span> Análisis factorial</a></li>
<li><a class="" href="escalamiento-multidimensional.html"><span class="header-section-number">34</span> Escalamiento multidimensional</a></li>
<li><a class="" href="correspondencias.html"><span class="header-section-number">35</span> Análisis de correspondencias</a></li>
<li class="book-part">Deep learning</li>
<li><a class="" href="capNN.html"><span class="header-section-number">36</span> Redes neuronales artificiales</a></li>
<li><a class="" href="cap-redes-convol.html"><span class="header-section-number">37</span> Redes neuronales convolucionales</a></li>
<li class="book-part">Ciencia de datos de texto y redes</li>
<li><a class="" href="mineria-textos.html"><span class="header-section-number">38</span> Minería de textos</a></li>
<li><a class="" href="grafos.html"><span class="header-section-number">39</span> Análisis de grafos y redes sociales</a></li>
<li class="book-part">Ciencia de datos espaciales</li>
<li><a class="" href="datos-espaciales.html"><span class="header-section-number">40</span> Trabajando con datos espaciales</a></li>
<li><a class="" href="geo.html"><span class="header-section-number">41</span> Geoestadística</a></li>
<li><a class="" href="cap-econom-esp.html"><span class="header-section-number">42</span> Modelos econométricos espaciales</a></li>
<li><a class="" href="cap-pp.html"><span class="header-section-number">43</span> Procesos de puntos</a></li>
<li class="book-part">Comunica y colabora</li>
<li><a class="" href="id_120007-informes.html"><span class="header-section-number">44</span> Informes reproducibles con R Markdown y Quarto</a></li>
<li><a class="" href="shiny.html"><span class="header-section-number">45</span> Creación de aplicaciones web interactivas con Shiny</a></li>
<li><a class="" href="github.html"><span class="header-section-number">46</span> Git y GitHub R</a></li>
<li><a class="" href="geoproces.html"><span class="header-section-number">47</span> Geoprocesamiento en nube</a></li>
<li class="book-part">Casos de estudio en ciencia de datos</li>
<li><a class="" href="cap-crimen.html"><span class="header-section-number">48</span> Análisis de una red criminal</a></li>
<li><a class="" href="cap-publicidad.html"><span class="header-section-number">49</span> Optimización de inversiones publicitarias</a></li>
<li><a class="" href="cap-twitter.html"><span class="header-section-number">50</span> ¿Cómo twitea Elon Musk?</a></li>
<li><a class="" href="cap-periodismo.html"><span class="header-section-number">51</span> Análisis electoral: de Rstudio a su periódico</a></li>
<li><a class="" href="paro-clm.html"><span class="header-section-number">52</span> Crisis: impacto en el paro de Castilla-La Mancha</a></li>
<li><a class="" href="cap-rfm.html"><span class="header-section-number">53</span> Segmentación de clientes en el comerico minorista</a></li>
<li><a class="" href="cap-medicina.html"><span class="header-section-number">54</span> Análisis de datos en medicina</a></li>
<li><a class="" href="cap-futbol.html"><span class="header-section-number">55</span> Messi y Ronaldo: dos ídolos desde la perspectiva de los datos</a></li>
<li><a class="" href="cambioclimatico.html"><span class="header-section-number">56</span> Un dato sobre el cambio climático</a></li>
<li><a class="" href="cap-ree.html"><span class="header-section-number">57</span> Predicción de consumo eléctrico con redes neuronales</a></li>
<li><a class="" href="cap-sist-exp.html"><span class="header-section-number">58</span> Implementación de un sistema experto en el ámbito pediátrico</a></li>
<li><a class="" href="nlp-textil.html"><span class="header-section-number">59</span> El procesamiento del lenguaje natural para tendencias de moda en textil</a></li>
<li><a class="" href="cap-fraude.html"><span class="header-section-number">60</span> Detección de fraude de tarjetas de crédito</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="info-session.html"><span class="header-section-number">A</span> Información de la sesión</a></li>
<li><a class="" href="referncias.html">Referncias</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="cap-boosting-xgboost" class="section level1" number="29">
<h1>
<span class="header-section-number">Capítulo 29</span> Boosting y el algoritmo XGBoost<a class="anchor" aria-label="anchor" href="#cap-boosting-xgboost"><i class="fas fa-link"></i></a>
</h1>
<p><em>Ramón A. Carrasco</em><span class="math inline">\(^{a}\)</span> e <em>Itzcóatl Bueno</em><span class="math inline">\(^{b,a}\)</span></p>
<p><span class="math inline">\(^{a}\)</span>Universidad Complutense de Madrid
<span class="math inline">\(^{b}\)</span>Instituto Nacional de Estadística</p>
<div id="métodos-ensamblados-bagging-vs-boosting" class="section level2" number="29.1">
<h2>
<span class="header-section-number">29.1</span> Métodos ensamblados: bagging vs boosting<a class="anchor" aria-label="anchor" href="#m%C3%A9todos-ensamblados-bagging-vs-boosting"><i class="fas fa-link"></i></a>
</h2>
<p>En el Cap. <a href="cap-bagg-rf.html#cap-bagg-rf">28</a> se presentó la idea de métodos ensamblados. Una serie de modelos útiles cuando ningún modelo de aprendizaje supervisado es capaz de explicar bien la variable dependiente de interés. En el aprendizaje ensamblado se entrenan un gran número de modelos con menor precisión, y se combinan sus predicciones para obtener un metamodelo de una precisión más alta. Los dos modelos de aprendizaje ensamblado más populares son el <em>bagging</em> (Cap. <a href="cap-bagg-rf.html#cap-bagg-rf">28</a>) y el <em>boosting</em>.</p>
<p>La principal diferencia entre ellos radica en cómo se combinan los modelos individuales para obtener una predicción final. Por ejemplo, si se quisiera organizar una fiesta y se tuviese que tomar una decisión sobre el tema para la decoración, se podría hacer tanto con <em>bagging</em> como con <em>boosting</em>. Si se utilizase el <em>bagging</em>, se pediría opinión a distintos grupos de amigos. Después, se combinarán todas sus ideas para tomar una decisión final sobre la decoración de la fiesta. Por otro lado, si se utiliza <em>boosting</em>, en lugar de preguntarle a diferentes grupos de amigos al mismo tiempo, se pediría a un amigo en particular su opinión. Si su respuesta no es convincente, entonces se busca la ayuda de otro amigo, quien se enfocará en mejorar la respuesta anterior. Este proceso continúa secuencialmente, solicitando la ayuda de diferentes amigos y construyendo sobre las respuestas anteriores para obtener una decisión final más precisa y refinada sobre la decoración de la fiesta.</p>
</div>
<div id="qué-es-el-boosting" class="section level2" number="29.2">
<h2>
<span class="header-section-number">29.2</span> ¿Qué es el boosting?<a class="anchor" aria-label="anchor" href="#qu%C3%A9-es-el-boosting"><i class="fas fa-link"></i></a>
</h2>
<p>El <em>boosting</em> <span class="citation">(<a href="referncias.html#ref-schapire2012">Schapire and Freund 2012</a>)</span> es el otro de los paradigmas de aprendizaje ensamblado, presentado en el Cap. <a href="cap-bagg-rf.html#cap-bagg-rf">28</a>. Como el <em>bagging</em>, el <em>boosting</em> agrega múltiples modelos con menor precisión (débiles) combinando sus predicciones para obtener un metamodelo con una precisión más alta. Los árboles de decisión son los modelos base o débiles que se usan más frecuentemente. En este caso, para llegar al metamodelo a partir de los modelos base, es necesario introducir ponderaciones a los árboles basándose en las clasificaciones erróneas del árbol entrenado previamente a dicho árbol.</p>
<p>El <em>boosting</em> reduce el problema del sobreajuste utilizando menos árboles que un modelo <em>random forest</em>. Mientras que agregar más árboles al <em>random forest</em> ayuda a compensar el sobreajuste, también puede llevar a un aumento del mismo y, por ello, hay que ser cauteloso a la hora de agregar nuevos árboles. Sin embargo, el <em>boosting</em> aprende iterativamente de los errores en árboles anteriores, pudiendo llevar a sobreajustar el modelo. Aunque este enfoque produce predicciones más precisas, muchas veces mejores a la mayoría de algoritmos, puede llevar a ajustar las observaciones atípicas. Es por esto que el <em>random forest</em> es una técnica más recomendada cuando se trabaja con conjuntos de datos muy complejos con un gran número de observaciones atípicas.</p>
<p>Otra de las grandes desventajas del <em>boosting</em> es que su tiempo de procesamiento es muy elevado, puesto que su entrenamiento sigue una lógica secuencial. En el proceso de entrenamiento, un árbol debe esperar a que el inmediatamente anterior sea entrenado, para iniciar su entrenamiento, y esto limita la escalabilidad del modelo. Mientras tanto, un <em>random forest</em> entrena los árboles en paralelo, lo que hace que su tiempo de procesamiento sea más rápido.</p>
<p>Tanto los algoritmos de <em>boosting</em> como a los de <em>bagging</em> presentan el inconveniente de la dificultad de interpretación que tienen respecto a los árboles de decisión. En este aspecto estos algoritmos de ensamblado se pueden considerar como de caja negra.</p>
</div>
<div id="gradient-boosting-gb" class="section level2" number="29.3">
<h2>
<span class="header-section-number">29.3</span> Gradient Boosting (GB)<a class="anchor" aria-label="anchor" href="#gradient-boosting-gb"><i class="fas fa-link"></i></a>
</h2>
<p>Uno de los algoritmos de <em>boosting</em> más conocidos es el <strong>gradient boosting</strong> . Mientras que el <em>random forest</em> seleccionaba combinaciones aleatorias de variables en cada proceso de construcción de un árbol, el <em>gradient boosting</em> selecciona variables que mejoren la precisión con cada nuevo árbol. Por lo tanto, la construcción del modelo es secuencial, puesto que cada nuevo árbol se construye utilizando información derivada del árbol anterior y, en consecuencia, la construcción de estos árboles no son independientes. En cada iteración se registran los errores cometidos en los datos de entrenamiento y se tienen en cuenta para la siguiente ronda de entrenamiento. Además, se incorporan ponderaciones, como se observa en la Fig. <a href="cap-boosting-xgboost.html#fig:boosting">29.1</a> a los datos basándose en los resultados de la iteración anterior. Las ponderaciones más altas se aplicarán a las observaciones que fueron erróneamente clasificadas, y no se dará tanta atención a las bien clasificadas. Este proceso se repite hasta que se llega a un nivel bajo de error. El resultado final se obtiene a través de la media ponderada de las predicciones de los árboles de decisión.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:boosting"></span>
<img src="img/boosting.png" alt="Ejemplo de Boosting." width="70%"><p class="caption">
Figura 29.1: Ejemplo de Boosting.
</p>
</div>
<p>Matemáticamente, un algoritmo <em>gradient boosting</em> para clasificación sigue los pasos que a continuación se detallan. Sea un problema de clasificación binaria y, asumiendo que se tienen <span class="math inline">\(K\)</span> árboles de decisión de clasificación, la predicción del modelo ensamblado se obtiene utilizando la función sigmoidal, como en la regresión logística (Cap. <a href="cap-glm.html#cap-glm">16</a>), tal que:</p>
<span class="math display">\[\begin{equation}
P(y=1|x,f)=\frac{1}{1+e^{-f(x)}}
\end{equation}\]</span>
<p>Donde <span class="math inline">\(f(x)=\sum_{\kappa=1}^{K}{f_{\kappa}(x)}\)</span> y <span class="math inline">\(f_m\)</span> es un árbol de decisión. De nuevo, como en la regresión logística, se aplica el principio de máxima verosimilitud tratando de hallar una <span class="math inline">\(f\)</span> que maximice <span class="math inline">\(\mathcal{L}_f = \sum_{i=1}^{N}{\ln(P(y_i=1|x_i,f))}\)</span></p>
<p>El algoritmo, en origen, es un modelo constante de la forma <span class="math inline">\(f=f_0=\frac{p}{1-p}\)</span> donde <span class="math inline">\(p=\frac{1}{N}\sum^{N}_{i=1}\)</span>. Tras cada iteración se añade un nuevo árbol <span class="math inline">\(f_\kappa\)</span> al modelo. Para encontrar el mejor árbol <span class="math inline">\(f_\kappa\)</span>, la primera derivada parcial <span class="math inline">\(g_i\)</span> del modelo actual se obtiene para <span class="math inline">\(i=1,\dots,N\)</span>:</p>
<span class="math display">\[\begin{equation}
g_i = \frac{\delta\mathcal{L}_f}{\delta f}
\end{equation}\]</span>
<p>Donde <span class="math inline">\(f\)</span> es el modelo de clasificación ensamblado construido en la iteración previa. Se necesita obtener las derivadas de <span class="math inline">\(\ln(P(y_i=1|x_i,f))\)</span> con respecto a <span class="math inline">\(f\)</span> para todo <span class="math inline">\(i\)</span> para poder calcular <span class="math inline">\(g_i\)</span>. Nótese que:</p>
<span class="math display">\[\begin{equation}
\ln(P(y_i=1|x_i,f))=\ln(\frac{1}{1+e^{-f(x_i)}})
\end{equation}\]</span>
<p>Y, por tanto, la derivada respecto a <span class="math inline">\(f\)</span> es igual a:</p>
<span class="math display">\[\begin{equation}
\frac{\delta \ln(P(y_i=1|x_i,f))}{\delta f} = \frac{1}{e^{f(x_i)}+1}
\end{equation}\]</span>
<p>Después, se reemplaza en el conjunto de entrenamiento la categoría original <span class="math inline">\(y_i\)</span> por su correspondiente derivada parcial <span class="math inline">\(g_i\)</span> y se construye un nuevo modelo <span class="math inline">\(f_\kappa\)</span> utilizando el conjunto de entrenamiento transformado. Tras esto, se obtiene la actualización óptima (<span class="math inline">\(\rho_\kappa\)</span>) como:</p>
<span class="math display">\[\begin{equation}
\rho_\kappa = \arg \max\limits_{\rho}{\mathcal{L}_{f+\rho f_\kappa}}
\end{equation}\]</span>
<p>Al terminar la iteración <span class="math inline">\(\kappa\)</span>, se actualiza el modelo ensamblado <span class="math inline">\(f\)</span> añadiendo el nuevo árbol <span class="math inline">\(f_\kappa\)</span>:</p>
<span class="math display">\[\begin{equation}
f\leftarrow f+\alpha\rho_\kappa f_\kappa
\end{equation}\]</span>
<p>Se itera hasta que <span class="math inline">\(\kappa=K\)</span>, entonces el proceso se detiene y se obtiene el modelo ensamblado final <span class="math inline">\(f\)</span>.</p>
<div id="hiperparámetros-del-modelo-gradient-boosting" class="section level3" number="29.3.1">
<h3>
<span class="header-section-number">29.3.1</span> Hiperparámetros del modelo <em>gradient boosting</em><a class="anchor" aria-label="anchor" href="#hiperpar%C3%A1metros-del-modelo-gradient-boosting"><i class="fas fa-link"></i></a>
</h3>
<p>Un modelo de <em>gradient boosting</em> tiene dos tipos de hiperparámetros:</p>
<ul>
<li>Hiperparámetros de <em>boosting</em>.</li>
<li>Hiperparámetros del árbol.</li>
</ul>
<div id="hiperparámetros-de-boosting" class="section level4" number="29.3.1.1">
<h4>
<span class="header-section-number">29.3.1.1</span> Hiperparámetros de <em>boosting</em><a class="anchor" aria-label="anchor" href="#hiperpar%C3%A1metros-de-boosting"><i class="fas fa-link"></i></a>
</h4>
<p>Los hiperparámetros de <em>boosting</em> son principalmente dos: el número de árboles y la tasa de aprendizaje.</p>
<p>El primero indica el número de árboles a construir y que, como se ha comentado, es importante optimizar para evitar el sobreajuste del modelo. A diferencia de los modelos <em>random forest</em> o <em>bagging</em>, en el <em>boosting</em> los árboles crecen en secuencia para que cada árbol corrija los errores del anterior. El número de árboles necesarios para que el modelo sea buen predictor puede verse incrementado en función de los valores que tomen los otros hiperparámetros.</p>
<p>La tasa de aprendizaje es el hiperparámetro con el que se determina la contribución de cada árbol en el resultado final y controla la rapidez con la que el algoritmo avanza por el descenso del gradiente, es decir, la velocidad a la que aprende. Este hiperparámetro toma valores entre 0 y 1, aunque los valores habituales oscilan entre 0,001 y 0,3. El modelo es más robusto a las características específicas de cada árbol, permitiendo una buena generalización, cuando la tasa de aprendizaje toma valores bajos. Estos valores también facilitan la parada temprana antes del sobreajuste del modelo. Sin embargo, utilizar estos valores vuelve al modelo más exigente computacionalmente y dificulta alcanzar el modelo óptimo con un número fijo de árboles. En resumen, cuanto menor sea este valor, más preciso puede ser el modelo, pero también requerirá más árboles en la secuencia.</p>
</div>
<div id="hiperparámetros-de-árbol" class="section level4" number="29.3.1.2">
<h4>
<span class="header-section-number">29.3.1.2</span> Hiperparámetros de árbol<a class="anchor" aria-label="anchor" href="#hiperpar%C3%A1metros-de-%C3%A1rbol"><i class="fas fa-link"></i></a>
</h4>
<p>Los principales hiperparámetros de árbol son: la profundidad del árbol y el número mínimo de observaciones en nodos terminales, como se vio en el Cap. <a href="cap-arboles.html#cap-arboles">24</a>.</p>
<p>El primer hiperparámetro controla la profundidad de los árboles individuales. Los valores habituales de profundidad oscilan entre 3 y 8. Los árboles de menor profundidad son eficientes computacionalmente, pero menos precisos. Sin embargo, los árboles de mayor profundidad permiten que el algoritmo capture interacciones únicas, aunque aumentan el riesgo de sobreajuste.</p>
<p>El segundo hiperparámetro, además de controlar el número mínimo de observaciones en los nodos terminales, controla la complejidad de cada árbol. Los valores típicos de este hiperparámetro suelen estar entre 5 y 15. Los valores más altos ayudan a evitar que un modelo aprenda relaciones que pueden ser muy específicas de la muestra particular seleccionada para entrenar el árbol, evitando así el sobreajuste. Sin embargo, los valores más pequeños pueden ayudar con clases desbalanceadas en problemas de clasificación.</p>
</div>
</div>
<div id="estrategia-de-ajuste-de-hiperparámetros" class="section level3" number="29.3.2">
<h3>
<span class="header-section-number">29.3.2</span> Estrategia de ajuste de hiperparámetros<a class="anchor" aria-label="anchor" href="#estrategia-de-ajuste-de-hiperpar%C3%A1metros"><i class="fas fa-link"></i></a>
</h3>
<p>A diferencia del <em>random forest</em>, los modelos <em>gradient boosting</em> pueden variar mucho en su precisión de acuerdo a su configuración de hiperparámetros. Por ello, el ajuste puede requerir seguir una estrategia. Un buen enfoque para esto es:</p>
<ul>
<li>Elegir una tasa de aprendizaje relativamente alta. El valor predeterminado es 0,1 y generalmente funciona. Sin embargo, para la mayoría de problemas funcionan valores entre 0,05 y 0,2.</li>
<li>Determinar el número óptimo de árboles para la tasa de aprendizaje elegida.</li>
<li>Ajustar los hiperparámetros del árbol y la tasa de aprendizaje y evaluar la velocidad frente al rendimiento.</li>
<li>Ajustar los hiperparámetros específicos del árbol para determinar la tasa de aprendizaje.</li>
<li>Una vez que se ajustan los parámetros específicos del árbol, se reduce la tasa de aprendizaje para evaluar cualquier mejora en la precisión.</li>
<li>Utilizar la configuración final de hiperparámetros y aumentar los procedimientos de validación cruzada para obtener estimaciones más robustas. Si se utiliza validación cruzada en los pasos anteriores, entonces este paso no es necesario.</li>
</ul>
</div>
<div id="procedimiento-con-r-la-función-gbm" class="section level3" number="29.3.3">
<h3>
<span class="header-section-number">29.3.3</span> Procedimiento con R: la función <code>gbm()</code><a class="anchor" aria-label="anchor" href="#procedimiento-con-r-la-funci%C3%B3n-gbm"><i class="fas fa-link"></i></a>
</h3>
<p>En el paquete <code>gbm</code> de <strong>R</strong> se encuentra la función con el mismo nombre <code><a href="https://rdrr.io/pkg/gbm/man/gbm.html">gbm()</a></code> que se utiliza para entrenar un modelo <em>gradient boosting</em>:</p>
<div class="sourceCode" id="cb404"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">gbm</span><span class="op">(</span><span class="va">formula</span>,data<span class="op">=</span><span class="va">...</span>, <span class="va">...</span><span class="op">)</span></span></code></pre></div>
<ul>
<li>
<code>formula</code>: Refleja la relación entre la variable dependiente <span class="math inline">\(Y\)</span> y los predictores tal que <span class="math inline">\(Y \sim X_1 + ... + X_p\)</span>.</li>
<li>
<code>data</code>: Conjunto de datos con el que entrenar el árbol de acuerdo a la fórmula indicada.</li>
</ul>
</div>
<div id="aplicación-del-modelo-gradient-boosting-en-r" class="section level3" number="29.3.4">
<h3>
<span class="header-section-number">29.3.4</span> Aplicación del modelo <em>gradient boosting</em> en R<a class="anchor" aria-label="anchor" href="#aplicaci%C3%B3n-del-modelo-gradient-boosting-en-r"><i class="fas fa-link"></i></a>
</h3>
<p>A través de los datos de compras <code>dp_entr</code> incluidos en el paquete <code>CDR</code> se va a aplicar el modelo <em>gradient boosting</em> para clasificar qué clientes van a comprar un nuevo producto (<em>tensiómetro digital</em>) y quienes no. Se entrena el modelo utilizando el conjunto de datos de entrenamiento sin transformar (en su escala original). Así, en lugar de tener las variables categóricas transformadas mediante one-hot-encoding se usan en su escala original, como ocurre con el caso de la variable que mide el nivel educativo.</p>
<div class="sourceCode" id="cb405"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st">"CDR"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/topepo/caret/">"caret"</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/gbm-developers/gbm">"gbm"</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="http://had.co.nz/reshape">"reshape"</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="st"><a href="https://ggplot2.tidyverse.org">"ggplot2"</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">dp_entr</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb406"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># se determina la semilla aleatoria</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">101</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># se entrena el modelo</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/train.html">train</a></span><span class="op">(</span><span class="va">CLS_PRO_pro13</span> <span class="op">~</span> <span class="va">.</span>, </span>
<span>            data<span class="op">=</span><span class="va">dp_entr</span>, </span>
<span>            method<span class="op">=</span><span class="st">"gbm"</span>, </span>
<span>            metric<span class="op">=</span><span class="st">"Accuracy"</span>,</span>
<span>            trControl <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/trainControl.html">trainControl</a></span><span class="op">(</span>classProbs <span class="op">=</span> <span class="cn">TRUE</span>, </span>
<span>                                     method <span class="op">=</span> <span class="st">"cv"</span>, number <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span>            <span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb407"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb407-1"><a href="cap-boosting-xgboost.html#cb407-1" tabindex="-1"></a>model</span>
<span id="cb407-2"><a href="cap-boosting-xgboost.html#cb407-2" tabindex="-1"></a></span>
<span id="cb407-3"><a href="cap-boosting-xgboost.html#cb407-3" tabindex="-1"></a>Stochastic Gradient Boosting </span>
<span id="cb407-4"><a href="cap-boosting-xgboost.html#cb407-4" tabindex="-1"></a></span>
<span id="cb407-5"><a href="cap-boosting-xgboost.html#cb407-5" tabindex="-1"></a><span class="dv">558</span> samples</span>
<span id="cb407-6"><a href="cap-boosting-xgboost.html#cb407-6" tabindex="-1"></a> <span class="dv">17</span> predictor</span>
<span id="cb407-7"><a href="cap-boosting-xgboost.html#cb407-7" tabindex="-1"></a>  <span class="dv">2</span> classes<span class="sc">:</span> <span class="st">'S'</span>, <span class="st">'N'</span> </span>
<span id="cb407-8"><a href="cap-boosting-xgboost.html#cb407-8" tabindex="-1"></a></span>
<span id="cb407-9"><a href="cap-boosting-xgboost.html#cb407-9" tabindex="-1"></a>No pre<span class="sc">-</span>processing</span>
<span id="cb407-10"><a href="cap-boosting-xgboost.html#cb407-10" tabindex="-1"></a>Resampling<span class="sc">:</span> Cross<span class="sc">-</span><span class="fu">Validated</span> (<span class="dv">10</span> fold) </span>
<span id="cb407-11"><a href="cap-boosting-xgboost.html#cb407-11" tabindex="-1"></a>Summary of sample sizes<span class="sc">:</span> <span class="dv">502</span>, <span class="dv">502</span>, <span class="dv">502</span>, <span class="dv">503</span>, <span class="dv">503</span>, <span class="dv">502</span>, ... </span>
<span id="cb407-12"><a href="cap-boosting-xgboost.html#cb407-12" tabindex="-1"></a>Resampling results across tuning parameters<span class="sc">:</span></span>
<span id="cb407-13"><a href="cap-boosting-xgboost.html#cb407-13" tabindex="-1"></a></span>
<span id="cb407-14"><a href="cap-boosting-xgboost.html#cb407-14" tabindex="-1"></a>  interaction.depth  n.trees  Accuracy   Kappa    </span>
<span id="cb407-15"><a href="cap-boosting-xgboost.html#cb407-15" tabindex="-1"></a>  <span class="dv">1</span>                   <span class="dv">50</span>      <span class="fl">0.8564610</span>  <span class="fl">0.7130031</span></span>
<span id="cb407-16"><a href="cap-boosting-xgboost.html#cb407-16" tabindex="-1"></a>  <span class="dv">1</span>                  <span class="dv">100</span>      <span class="fl">0.8690909</span>  <span class="fl">0.7383556</span></span>
<span id="cb407-17"><a href="cap-boosting-xgboost.html#cb407-17" tabindex="-1"></a>  <span class="dv">1</span>                  <span class="dv">150</span>      <span class="fl">0.8762338</span>  <span class="fl">0.7526413</span></span>
<span id="cb407-18"><a href="cap-boosting-xgboost.html#cb407-18" tabindex="-1"></a>  <span class="dv">2</span>                   <span class="dv">50</span>      <span class="fl">0.8690909</span>  <span class="fl">0.7382344</span></span>
<span id="cb407-19"><a href="cap-boosting-xgboost.html#cb407-19" tabindex="-1"></a>  <span class="dv">2</span>                  <span class="dv">100</span>      <span class="fl">0.8762338</span>  <span class="fl">0.7526413</span></span>
<span id="cb407-20"><a href="cap-boosting-xgboost.html#cb407-20" tabindex="-1"></a>  <span class="dv">2</span>                  <span class="dv">150</span>      <span class="fl">0.8799026</span>  <span class="fl">0.7599227</span></span>
<span id="cb407-21"><a href="cap-boosting-xgboost.html#cb407-21" tabindex="-1"></a>  <span class="dv">3</span>                   <span class="dv">50</span>      <span class="fl">0.8763636</span>  <span class="fl">0.7528004</span></span>
<span id="cb407-22"><a href="cap-boosting-xgboost.html#cb407-22" tabindex="-1"></a>  <span class="dv">3</span>                  <span class="dv">100</span>      <span class="fl">0.8781494</span>  <span class="fl">0.7563575</span></span>
<span id="cb407-23"><a href="cap-boosting-xgboost.html#cb407-23" tabindex="-1"></a>  <span class="dv">3</span>                  <span class="dv">150</span>      <span class="fl">0.8835390</span>  <span class="fl">0.7671499</span></span>
<span id="cb407-24"><a href="cap-boosting-xgboost.html#cb407-24" tabindex="-1"></a></span>
<span id="cb407-25"><a href="cap-boosting-xgboost.html#cb407-25" tabindex="-1"></a>Tuning parameter <span class="st">'shrinkage'</span> was held constant at a value of <span class="fl">0.1</span></span>
<span id="cb407-26"><a href="cap-boosting-xgboost.html#cb407-26" tabindex="-1"></a>Tuning</span>
<span id="cb407-27"><a href="cap-boosting-xgboost.html#cb407-27" tabindex="-1"></a> parameter <span class="st">'n.minobsinnode'</span> was held constant at a value of <span class="dv">10</span></span>
<span id="cb407-28"><a href="cap-boosting-xgboost.html#cb407-28" tabindex="-1"></a>Accuracy was used to select the optimal model using the largest value.</span>
<span id="cb407-29"><a href="cap-boosting-xgboost.html#cb407-29" tabindex="-1"></a>The final values used <span class="cf">for</span> the model were n.trees <span class="ot">=</span> <span class="dv">150</span>, interaction.depth <span class="ot">=</span></span>
<span id="cb407-30"><a href="cap-boosting-xgboost.html#cb407-30" tabindex="-1"></a> <span class="dv">3</span>, shrinkage <span class="ot">=</span> <span class="fl">0.1</span> and n.minobsinnode <span class="ot">=</span> <span class="fl">10.</span></span></code></pre></div>
<p>El modelo resultante del proceso de entrenamiento es un <em>gradient boosting</em> que ha ajustado los hiperparámetros a 150 árboles y una profundidad igual a 3. Además, los valores tanto del número mínimo de observaciones en nodos como de la tasa de aprendizaje, toman los valores por defecto de 10 y 0,1, respectivamente. Los resultados en el proceso de validación cruzada se muestran en la Fig. <a href="cap-boosting-xgboost.html#fig:GBMBOXPLOT">29.2</a>, en el que se observa como la precisión oscila entre el 84% y el 93% en las iteraciones.</p>
<div class="sourceCode" id="cb408"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/reshape/man/melt-24.html">melt</a></span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">resample</span><span class="op">[</span>,<span class="op">-</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">variable</span>, y <span class="op">=</span> <span class="va">value</span>, fill<span class="op">=</span><span class="va">variable</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_boxplot.html">geom_boxplot</a></span><span class="op">(</span>show.legend<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:GBMBOXPLOT"></span>
<img src="img/boxplotgbm.png" alt="Resultados del modelo GB obtenidos durante el proceso de validación cruzada." width="60%"><p class="caption">
Figura 29.2: Resultados del modelo GB obtenidos durante el proceso de validación cruzada.
</p>
</div>
</div>
<div id="gradient-boosting-con-ajuste-automático" class="section level3" number="29.3.5">
<h3>
<span class="header-section-number">29.3.5</span> <em>Gradient Boosting</em> con ajuste automático<a class="anchor" aria-label="anchor" href="#gradient-boosting-con-ajuste-autom%C3%A1tico"><i class="fas fa-link"></i></a>
</h3>
<p>Se repite el procedimiento para el ejemplo anterior. Sin embargo, en este ejemplo se ajustan de forma automática los hiperparámetros más relevantes de dicho algoritmo para mejorar los resultados respecto al modelo anterior. Se observa como los hiperparámetros a ajustar para el método <code>gbm</code> son: el número de árboles, la profundidad, la tasa de aprendizaje y el número de observaciones en un nodo.</p>
<div class="sourceCode" id="cb409"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb409-1"><a href="cap-boosting-xgboost.html#cb409-1" tabindex="-1"></a><span class="fu">modelLookup</span>(<span class="st">"gbm"</span>)</span>
<span id="cb409-2"><a href="cap-boosting-xgboost.html#cb409-2" tabindex="-1"></a>model         parameter                   label forReg forClass probModel</span>
<span id="cb409-3"><a href="cap-boosting-xgboost.html#cb409-3" tabindex="-1"></a><span class="dv">1</span>   gbm           n.trees   <span class="co"># Boosting Iterations   TRUE     TRUE      TRUE</span></span>
<span id="cb409-4"><a href="cap-boosting-xgboost.html#cb409-4" tabindex="-1"></a><span class="dv">2</span>   gbm interaction.depth          Max Tree Depth   <span class="cn">TRUE</span>     <span class="cn">TRUE</span>      <span class="cn">TRUE</span></span>
<span id="cb409-5"><a href="cap-boosting-xgboost.html#cb409-5" tabindex="-1"></a><span class="dv">3</span>   gbm         shrinkage               Shrinkage   <span class="cn">TRUE</span>     <span class="cn">TRUE</span>      <span class="cn">TRUE</span></span>
<span id="cb409-6"><a href="cap-boosting-xgboost.html#cb409-6" tabindex="-1"></a><span class="dv">4</span>   gbm    n.minobsinnode Min. Terminal Node Size   <span class="cn">TRUE</span>     <span class="cn">TRUE</span>      <span class="cn">TRUE</span></span></code></pre></div>
<p>Siguiendo la estrategia descrita se definen rangos de posibles valores para los principales hiperparámetros a optimizar.</p>
<div class="sourceCode" id="cb410"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Se especifica un rango de valores posibles para los hiperparámetros</span></span>
<span><span class="va">tuneGrid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/expand.grid.html">expand.grid</a></span><span class="op">(</span>interaction.depth <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span>,<span class="fl">6</span>,<span class="fl">8</span><span class="op">)</span>,</span>
<span>                        n.trees <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">10</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">ncol</a></span><span class="op">(</span><span class="va">dp_entr</span><span class="op">)</span>,<span class="fl">300</span>,<span class="fl">500</span><span class="op">)</span>, </span>
<span>                        shrinkage <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.05</span>,<span class="fl">0.1</span>,<span class="fl">0.2</span><span class="op">)</span>, </span>
<span>                        n.minobsinnode <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">5</span>,<span class="fl">10</span>,<span class="fl">15</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Esta red de posibles valores para los hiperparámetros del modelo se incorporan a la función de entrenamiento. Cuanto más exhaustivo sea el ajuste de estos valores, mayor será el tiempo de ajuste del modelo. La red presentada está formada por 81 combinaciones de los posibles cuatro hiperparámetros.</p>
<div class="sourceCode" id="cb411"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># se fija la semilla aleatoria</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">101</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># se entrena el modelo</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/train.html">train</a></span><span class="op">(</span><span class="va">CLS_PRO_pro13</span><span class="op">~</span><span class="va">.</span>, </span>
<span>            data<span class="op">=</span><span class="va">dp_entr</span>, </span>
<span>            method<span class="op">=</span><span class="st">"gbm"</span>, </span>
<span>            metric<span class="op">=</span><span class="st">"Accuracy"</span>,</span>
<span>            trControl<span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/caret/man/trainControl.html">trainControl</a></span><span class="op">(</span>classProbs <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                                   method<span class="op">=</span><span class="st">"cv"</span>, number<span class="op">=</span><span class="fl">10</span><span class="op">)</span>,</span>
<span>            tuneGrid<span class="op">=</span><span class="va">tuneGrid</span><span class="op">)</span></span></code></pre></div>
<p>El modelo que mejores resultados proporciona es aquel que ajusta los hiperparámetros a los siguientes valores: 180 árboles, una profundidad igual a 6, una tasa de aprendizaje de 0.05 y un tamaño mínimo de los nodos de 10 observaciones.</p>
<div class="sourceCode" id="cb412"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb412-1"><a href="cap-boosting-xgboost.html#cb412-1" tabindex="-1"></a>model<span class="sc">$</span>bestTune</span>
<span id="cb412-2"><a href="cap-boosting-xgboost.html#cb412-2" tabindex="-1"></a>   n.trees interaction.depth shrinkage n.minobsinnode</span>
<span id="cb412-3"><a href="cap-boosting-xgboost.html#cb412-3" tabindex="-1"></a><span class="dv">13</span>     <span class="dv">180</span>                 <span class="dv">6</span>      <span class="fl">0.05</span>             <span class="dv">10</span></span></code></pre></div>
<p>En la Fig. <a href="cap-boosting-xgboost.html#fig:modelgbmboxplot">29.3</a> se muestran los resultados obtenidos durante el proceso de validación cruzada. Se puede ver que los resultados son similares a los del modelo anterior, aunque hay diferencias importantes. En primer lugar, se alcanza un valor máximo de precisión mayor al anterior, pues en este caso la precisión oscila entre el 84% y el 95%. En segundo lugar, vemos que el valor mediano de la precisión ha subido del 87.5% del modelo anterior hasta el 90% de este modelo. Por último, que el rendimiento haya variado tan poco desde el modelo por defecto a un modelo en el que se ha intentado ajustar los hiperparámetros, confirma lo ya expuesto sobre el buen rendimiento de un modelo de <em>gradient boosting</em> con los parámetros por defecto.</p>
<div class="sourceCode" id="cb413"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/reshape/man/melt-24.html">melt</a></span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">resample</span><span class="op">[</span>,<span class="op">-</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">variable</span>, y <span class="op">=</span> <span class="va">value</span>, fill<span class="op">=</span><span class="va">variable</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_boxplot.html">geom_boxplot</a></span><span class="op">(</span>show.legend<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:modelgbmboxplot"></span>
<img src="img/boxplottunedgbm.png" alt="Resultados del modelo GB con ajuste autmático obtenidos durante el proceso de validación cruzada." width="60%"><p class="caption">
Figura 29.3: Resultados del modelo GB con ajuste autmático obtenidos durante el proceso de validación cruzada.
</p>
</div>
</div>
</div>
<div id="extreme-gradient-boosting-xgb" class="section level2" number="29.4">
<h2>
<span class="header-section-number">29.4</span> eXtreme Gradient Boosting (XGB)<a class="anchor" aria-label="anchor" href="#extreme-gradient-boosting-xgb"><i class="fas fa-link"></i></a>
</h2>
<p>El <em>eXtreme Gradient Boosting</em> es una implementación eficiente y escalable del modelo <em>gradient boosting</em>. Este modelo, abreviado como XGBoost, es un paquete de código abierto en C++, Java, Python <span class="citation">(<a href="referncias.html#ref-wade2020hands">Wade 2020</a>)</span>, R, Julia, Perl y Scala. En R el modelo se incluye dentro del paquete <code>xgboost</code> <span class="citation">(<a href="referncias.html#ref-chen2015xgboost">T. Chen et al. 2015</a>)</span>. El paquete incluye un procedimiento para la solución eficiente de modelos lineales y un algoritmo de aprendizaje de árboles.</p>
<p>El paquete es compatible con funciones objetivo de regresión, clasificación y ranking. Además, tiene varias características importantes:</p>
<ol style="list-style-type: decimal">
<li><p>Velocidad: <code>xgboost</code> puede realizar automáticamente cálculos paralelos. Por lo general, es 10 veces más rápido que el modelo <em>gradient boosting</em>.</p></li>
<li><p>Tipo de entrada: <code>xgboost</code> toma varios tipos de datos de entrada en <code>R</code>:</p></li>
</ol>
<ul>
<li>Matriz densa (<code>matrix</code>)</li>
<li>Matriz dispersa (<code>Matrix::dgCMatrix</code>)</li>
<li>Archivo de datos locales</li>
<li>Un tipo de datos propio del paquete: <code>xgb.DMatrix</code>
</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li><p>Dispersión: <code>xgboost</code> acepta datos de entrada dispersos para los modelos incluidos.</p></li>
<li><p>Personalización: <code>xgboost</code> admite tanto funciones de objetivo y funciones de evaluación personalizadas.</p></li>
<li><p>Rendimiento: <code>xgboost</code> alcanza generalmente una mayor precisión.</p></li>
</ol>
<div id="hiperparámetros-del-modelo-xgboost" class="section level3" number="29.4.1">
<h3>
<span class="header-section-number">29.4.1</span> Hiperparámetros del modelo XGBoost<a class="anchor" aria-label="anchor" href="#hiperpar%C3%A1metros-del-modelo-xgboost"><i class="fas fa-link"></i></a>
</h3>
<p>El modelo XGBoost proporciona los hiperparámetros que ya incluía el modelo <em>gradient boosting</em> referentes tanto al <em>boosting</em> como a los árboles. Sin embargo, <code>xgboost</code> también proporciona hiperparámetros adicionales que pueden ayudar a reducir las posibilidades de sobreajuste, lo que lleva a una menor variabilidad de predicción y, por lo tanto, a una mayor precisión. Estos hiperparámetros son: la regularización y el dropout.</p>
<p>Los parámetros de regularización se incluyen para ayudar a evitar el sobreajuste y reducir la complejidad del modelo. Existen tres hiperparámetros que tienen esta funcionalidad: gamma (<span class="math inline">\(\gamma\)</span>), alpha (<span class="math inline">\(\alpha\)</span>) y lambda (<span class="math inline">\(\lambda\)</span>). Gamma es un hiperparámetro de pseudo-regularización conocido como multiplicador Lagrangiano y controla la complejidad de un árbol dado. Este hiperparámetro establece que para hacer una partición adicional en un nodo es necesaria una reducción de pérdida mínima especificada por <code>gamma</code>. Al especificarlo, el modelo XGBoost hace crecer los árboles hasta una profundidad máxima establecida, pero en un paso de poda eliminará las divisiones que no cumplan con la regularización <span class="math inline">\(\gamma\)</span>. Este hiperparámetro toma valores entre 0 e infinito (<span class="math inline">\(\infty\)</span>), siguiendo la regla de que a mayor valor, mayor será la regularización. Los otros hiperparámetros de regularización, <span class="math inline">\(\alpha\)</span> y <span class="math inline">\(\lambda\)</span>, son más clásicos. Mientras que <span class="math inline">\(\alpha\)</span> proporciona una regularización <span class="math inline">\(L_1\)</span>, <span class="math inline">\(\lambda\)</span> proporciona una regularización <span class="math inline">\(L_2\)</span>. Estos parámetros de regularización establecen un límite a cómo de extremos pueden llegar a ser los pesos de los nodos en un árbol. Sus valores se encuentran, al igual que los de <span class="math inline">\(\gamma\)</span>, entre 0 y <span class="math inline">\(\infty\)</span>.</p>
<p>El dropout es un enfoque alternativo para reducir el sobreajuste. Cuando se entrena un modelo de <em>gradient boosting</em>, los primeros árboles tienden a dominar el rendimiento del modelo, mientras que los que se agregan después suelen mejorar la predicción solo para un pequeño grupo de variables. Esto puede llevar a que se incremente el riesgo de sobreajuste. Con el dropout, se descartan árboles aleatoriamente en el proceso de entrenamiento.</p>
<p>En su implementación en R, el modelo XGBoost incluye principalmente los siguientes parámetros para ser optimizados: número de iteraciones, profundidad máxima de los árboles, tasa de aprendizaje y la regularización <span class="math inline">\(\gamma\)</span>.</p>
<div class="sourceCode" id="cb414"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb414-1"><a href="cap-boosting-xgboost.html#cb414-1" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">modelLookup</span>(<span class="st">"xgbTree"</span>),<span class="dv">4</span>)</span>
<span id="cb414-2"><a href="cap-boosting-xgboost.html#cb414-2" tabindex="-1"></a>    model parameter                  label forReg forClass probModel</span>
<span id="cb414-3"><a href="cap-boosting-xgboost.html#cb414-3" tabindex="-1"></a><span class="dv">1</span> xgbTree   nrounds  <span class="co"># Boosting Iterations   TRUE     TRUE      TRUE</span></span>
<span id="cb414-4"><a href="cap-boosting-xgboost.html#cb414-4" tabindex="-1"></a><span class="dv">2</span> xgbTree max_depth         Max Tree Depth   <span class="cn">TRUE</span>     <span class="cn">TRUE</span>      <span class="cn">TRUE</span></span>
<span id="cb414-5"><a href="cap-boosting-xgboost.html#cb414-5" tabindex="-1"></a><span class="dv">3</span> xgbTree       eta              Shrinkage   <span class="cn">TRUE</span>     <span class="cn">TRUE</span>      <span class="cn">TRUE</span></span>
<span id="cb414-6"><a href="cap-boosting-xgboost.html#cb414-6" tabindex="-1"></a><span class="dv">4</span> xgbTree     gamma Minimum Loss Reduction   <span class="cn">TRUE</span>     <span class="cn">TRUE</span>      <span class="cn">TRUE</span></span></code></pre></div>
</div>
<div id="procedimiento-con-r-la-función-xgboost" class="section level3" number="29.4.2">
<h3>
<span class="header-section-number">29.4.2</span> Procedimiento con R: la función <code>xgboost()</code><a class="anchor" aria-label="anchor" href="#procedimiento-con-r-la-funci%C3%B3n-xgboost"><i class="fas fa-link"></i></a>
</h3>
<p>En el paquete <code>xgboost</code> de <strong>R</strong> se encuentra la función <code>xgboost()</code> que se utiliza para entrenar un modelo <em>extreme gradient boosting</em>:</p>
<div class="sourceCode" id="cb415"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">xgboost</span><span class="op">(</span>data <span class="op">=</span> <span class="va">...</span>, label <span class="op">=</span> <span class="va">...</span>, <span class="va">...</span><span class="op">)</span></span></code></pre></div>
<ul>
<li>
<code>data</code>: Conjunto de datos con el que entrenar el modelo.</li>
<li>
<code>label</code>: Vector con la variable respuesta.</li>
</ul>
</div>
<div id="aplicación-del-modelo-xgboost-en-r" class="section level3" number="29.4.3">
<h3>
<span class="header-section-number">29.4.3</span> Aplicación del modelo XGBoost en R<a class="anchor" aria-label="anchor" href="#aplicaci%C3%B3n-del-modelo-xgboost-en-r"><i class="fas fa-link"></i></a>
</h3>
<p>Se entrena este modelo utilizando el conjunto de entrenamiento sin transformar (en su escala original). Se continúa así el ejemplo expuesto durante la aplicación del modelo <em>gradient boosting</em> sin y con ajuste automático de sus hiperparámetros. Se repite el procedimiento de entrenar el modelo para los hiperparámetros por defecto que proporciona R.</p>
<div class="sourceCode" id="cb416"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># se determina la semilla aleatoria</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">101</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># se entrena el modelo</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/train.html">train</a></span><span class="op">(</span><span class="va">CLS_PRO_pro13</span><span class="op">~</span><span class="va">.</span>,</span>
<span>               data<span class="op">=</span><span class="va">dp_entr</span>,</span>
<span>               method<span class="op">=</span><span class="st">"xgbTree"</span>,</span>
<span>               metric<span class="op">=</span><span class="st">"Accuracy"</span>,</span>
<span>               trControl<span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/caret/man/trainControl.html">trainControl</a></span><span class="op">(</span>classProbs <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                                      method <span class="op">=</span> <span class="st">"cv"</span>,</span>
<span>                                      number<span class="op">=</span><span class="fl">10</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Por defecto, el entrenamiento establece valores constantes para la regularización <span class="math inline">\(\gamma\)</span> (0) y para el tamaño mínimo del nodo (1). En cambio, ajusta los hiperparámetros del modelo dentro de los valores por defecto de la función. Así, el modelo XGBoost resultante tiene 50 iteraciones, una profundidad máxima igual a 2 y una tasa de aprendizaje de 0,3. Los resultados de la validación cruzada muestran que la precisión obtenida oscila entre el 85% y el 95%, resultado similar al del <em>gradient boosting</em> con hiperparámetros ajustados. Sin embargo, el valor mediano de la precisión es del 88%, ligeramente inferior a la observada en el modelo <em>gradient boosting</em> con ajuste automático.</p>
<div class="sourceCode" id="cb417"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/reshape/man/melt-24.html">melt</a></span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">resample</span><span class="op">[</span>,<span class="op">-</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">variable</span>, y <span class="op">=</span> <span class="va">value</span>, fill<span class="op">=</span><span class="va">variable</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_boxplot.html">geom_boxplot</a></span><span class="op">(</span>show.legend<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:XGBRESULTS"></span>
<img src="img/boxplotxgbm.png" alt="Resultados del modelo durante la validación cruzada." width="60%"><p class="caption">
Figura 29.4: Resultados del modelo durante la validación cruzada.
</p>
</div>
</div>
<div id="xgboost-con-ajuste-automático" class="section level3" number="29.4.4">
<h3>
<span class="header-section-number">29.4.4</span> XGBoost con ajuste automático<a class="anchor" aria-label="anchor" href="#xgboost-con-ajuste-autom%C3%A1tico"><i class="fas fa-link"></i></a>
</h3>
<p>Se continúa el ejemplo aplicado a los datos sobre compra de un nuevo producto por parte de los clientes utilizando un modelo XGBoost en R. Sin embargo, se quieren mejorar los resultados obtenidos, y por ello ajustan automáticamente los hiperparámetros más relevantes de dicho algoritmo generando una red de posibles valores para dichos hiperparámetros. Por motivos computacionales, ésta no se hace excesivamente exhaustiva para evitar largos tiempos de entrenamiento. Si se dispone de tiempo suficiente para el entrenamiento, es aconsejable tratar de estudiar más valores para los hiperparámetros a optimizar.</p>
<div class="sourceCode" id="cb418"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Se especifica un rango de valores típicos para los hiperparámetros</span></span>
<span><span class="va">tuneGrid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/expand.grid.html">expand.grid</a></span> <span class="op">(</span>nrounds<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">50</span>,<span class="fl">100</span>,<span class="fl">500</span><span class="op">)</span>, </span>
<span>                         max_depth <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">4</span>,<span class="fl">8</span><span class="op">)</span>,</span>
<span>                         eta <span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.05</span>,<span class="fl">0.1</span>,<span class="fl">0.2</span>,<span class="fl">0.3</span><span class="op">)</span>,</span>
<span>                         gamma<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">0.5</span>,<span class="fl">5</span><span class="op">)</span>,</span>
<span>                         colsample_bytree<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.8</span><span class="op">)</span>,</span>
<span>                         min_child_weight<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span>,</span>
<span>                         subsample<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.5</span><span class="op">)</span><span class="op">)</span> </span></code></pre></div>
<div class="sourceCode" id="cb419"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># se determina la semilla aleatoria</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">101</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># se entrena el modelo</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/train.html">train</a></span><span class="op">(</span><span class="va">CLS_PRO_pro13</span><span class="op">~</span><span class="va">.</span>, </span>
<span>               data<span class="op">=</span><span class="va">dp_entr</span>, </span>
<span>               method<span class="op">=</span><span class="st">"xgbTree"</span>, </span>
<span>               metric<span class="op">=</span><span class="st">"Accuracy"</span>,</span>
<span>               trControl<span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/caret/man/trainControl.html">trainControl</a></span><span class="op">(</span>classProbs <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                                      method <span class="op">=</span> <span class="st">"cv"</span>,</span>
<span>                                      number <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>,</span>
<span>               tuneGrid<span class="op">=</span><span class="va">tuneGrid</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb420"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb420-1"><a href="cap-boosting-xgboost.html#cb420-1" tabindex="-1"></a>model<span class="sc">$</span>bestTune[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>]</span>
<span id="cb420-2"><a href="cap-boosting-xgboost.html#cb420-2" tabindex="-1"></a>   nrounds max_depth eta gamma</span>
<span id="cb420-3"><a href="cap-boosting-xgboost.html#cb420-3" tabindex="-1"></a><span class="dv">71</span>     <span class="dv">100</span>         <span class="dv">4</span> <span class="fl">0.2</span>     <span class="dv">5</span></span></code></pre></div>
<p>El modelo resultante establece que se utilicen 100 iteraciones, que los árboles tengan una profundidad máxima de 4, una tasa de aprendizaje del 0,2 y que la regularización <span class="math inline">\(\gamma\)</span> tome el valor 5.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:xgb-tuned-RESULTS"></span>
<img src="img/boxplottunedxgbm.png" alt="Resultados del modelo durante la validación cruzada." width="60%"><p class="caption">
Figura 29.5: Resultados del modelo durante la validación cruzada.
</p>
</div>
<p>Los resultados obtenidos durante la validación cruzada muestran que la precisión es muy similar a la del modelo por defecto, al encontrarse entre el 85% y el 95%. Sin embargo, se observa en el valor mediano de la precisión una ligera mejoría, al aumentar hasta el 90%.</p>
</div>
<div id="resumen-26" class="section level3 unnumbered infobox_resume">
<h3>Resumen<a class="anchor" aria-label="anchor" href="#resumen-26"><i class="fas fa-link"></i></a>
</h3>
<p>En este capítulo se introduce al lector en el algoritmo de aprendizaje supervisado conocido como <em>gradient boosting</em>, en concreto:</p>
<ul>
<li>Se presenta el otro paradigma principal de aprendizaje ensamblado: el <em>boosting</em>.</li>
<li>Se explica el modelo basado en este paradigma, el <em>gradient boosting</em>, así como sus diferencias con el <em>random forest</em> (basado en <em>bagging</em>).</li>
<li>Se exponen los hiperparámetros más relevantes a la hora de optimizar un modelo de <em>gradient boosting</em>.</li>
<li>Se presenta el <em>eXtreme gradient boosting</em>, una implementación eficiente y escalable del modelo <em>gradient boosting</em>. Así como los hiperparámetros de regularización y otros parámetros importantes en esta implementación.</li>
<li>Se aplican ambos algoritmos en <code>R</code> en un caso práctico para la clasificación binaria de datos.</li>
</ul>
</div>

</div>
</div>



  <div class="chapter-nav">
<div class="prev"><a href="cap-bagg-rf.html"><span class="header-section-number">28</span> Métodos ensamblados: bagging y random forest</a></div>
<div class="next"><a href="jerarquico.html"><span class="header-section-number">30</span> Análisis cluster: clusterización jerárquica</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="Índice capítulo"><h2>Índice capítulo</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#cap-boosting-xgboost"><span class="header-section-number">29</span> Boosting y el algoritmo XGBoost</a></li>
<li><a class="nav-link" href="#m%C3%A9todos-ensamblados-bagging-vs-boosting"><span class="header-section-number">29.1</span> Métodos ensamblados: bagging vs boosting</a></li>
<li><a class="nav-link" href="#qu%C3%A9-es-el-boosting"><span class="header-section-number">29.2</span> ¿Qué es el boosting?</a></li>
<li>
<a class="nav-link" href="#gradient-boosting-gb"><span class="header-section-number">29.3</span> Gradient Boosting (GB)</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#hiperpar%C3%A1metros-del-modelo-gradient-boosting"><span class="header-section-number">29.3.1</span> Hiperparámetros del modelo gradient boosting</a></li>
<li><a class="nav-link" href="#estrategia-de-ajuste-de-hiperpar%C3%A1metros"><span class="header-section-number">29.3.2</span> Estrategia de ajuste de hiperparámetros</a></li>
<li><a class="nav-link" href="#procedimiento-con-r-la-funci%C3%B3n-gbm"><span class="header-section-number">29.3.3</span> Procedimiento con R: la función gbm()</a></li>
<li><a class="nav-link" href="#aplicaci%C3%B3n-del-modelo-gradient-boosting-en-r"><span class="header-section-number">29.3.4</span> Aplicación del modelo gradient boosting en R</a></li>
<li><a class="nav-link" href="#gradient-boosting-con-ajuste-autom%C3%A1tico"><span class="header-section-number">29.3.5</span> Gradient Boosting con ajuste automático</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#extreme-gradient-boosting-xgb"><span class="header-section-number">29.4</span> eXtreme Gradient Boosting (XGB)</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#hiperpar%C3%A1metros-del-modelo-xgboost"><span class="header-section-number">29.4.1</span> Hiperparámetros del modelo XGBoost</a></li>
<li><a class="nav-link" href="#procedimiento-con-r-la-funci%C3%B3n-xgboost"><span class="header-section-number">29.4.2</span> Procedimiento con R: la función xgboost()</a></li>
<li><a class="nav-link" href="#aplicaci%C3%B3n-del-modelo-xgboost-en-r"><span class="header-section-number">29.4.3</span> Aplicación del modelo XGBoost en R</a></li>
<li><a class="nav-link" href="#xgboost-con-ajuste-autom%C3%A1tico"><span class="header-section-number">29.4.4</span> XGBoost con ajuste automático</a></li>
<li><a class="nav-link" href="#resumen-26">Resumen</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Fundamentos de ciencia de datos con R</strong>" coordinado por <a href="https://blog.uclm.es/gemafaviles/" class="text-light">Gema Fernández-Avilés y José-María Montero</a>. Generado por última vez el día 2023-06-16.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>Este libro ha sido generado con el paquete de R <a class="text-light" href="https://bookdown.org">bookdown</a>.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
